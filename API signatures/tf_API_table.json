[
  {
    "tf.debugging.Assert": "tf.debugging.Assert(\n    condition, data, summarize=None, name=None\n)\n"
  },
  {
    "tf.CriticalSection": "tf.CriticalSection(\n    name=None, shared_name=None, critical_section_def=None, import_scope=None\n)\n"
  },
  {
    "tf.DeviceSpec": "tf.DeviceSpec(\n    job=None, replica=None, task=None, device_type=None, device_index=None\n)\n"
  },
  {
    "tf.GradientTape": "tf.GradientTape(\n    persistent=False, watch_accessed_variables=True\n)\n"
  },
  {
    "tf.IndexedSlices": "tf.IndexedSlices(\n    values, indices, dense_shape=None\n)\n"
  },
  {
    "tf.IndexedSlicesSpec": "tf.IndexedSlicesSpec(\n    shape=None,\n    dtype=tf.dtypes.float32,\n    indices_dtype=tf.dtypes.int64,\n    dense_shape_dtype=None,\n    indices_shape=None\n)\n"
  },
  {
    "tf.Module": "tf.Module(\n    name=None\n)\n"
  },
  {
    "tf.Operation": "tf.Operation(\n    node_def,\n    g,\n    inputs=None,\n    output_types=None,\n    control_inputs=None,\n    input_types=None,\n    original_op=None,\n    op_def=None\n)\n"
  },
  {
    "tf.OptionalSpec": "tf.OptionalSpec(\n    element_spec\n)\n"
  },
  {
    "tf.ragged.constant": "tf.ragged.constant([[0], [1, 2]]).shape"
  },
  {
    "tf.RaggedTensorSpec": "tf.RaggedTensorSpec(\n    shape=None,\n    dtype=tf.dtypes.float32,\n    ragged_rank=None,\n    row_splits_dtype=tf.dtypes.int64,\n    flat_values_spec=None\n)\n"
  },
  {
    "tf.RegisterGradient": "tf.RegisterGradient(\n    op_type\n)\n"
  },
  {
    "tf.sparse.SparseTensor": "tf.sparse.SparseTensor(\n    indices, values, dense_shape\n)\n"
  },
  {
    "tf.SparseTensorSpec": "tf.SparseTensorSpec(\n    shape=None,\n    dtype=tf.dtypes.float32\n)\n"
  },
  {
    "tf.Tensor": "tf.Tensor(\n    op, value_index, dtype\n)\n"
  },
  {
    "tf.TensorArray": "tf.TensorArray(\n    dtype,\n    size=None,\n    dynamic_size=None,\n    clear_after_read=None,\n    tensor_array_name=None,\n    handle=None,\n    flow=None,\n    infer_shape=True,\n    element_shape=None,\n    colocate_with_first_write_call=True,\n    name=None\n)\n"
  },
  {
    "tf.TensorArraySpec": "tf.TensorArraySpec(\n    element_shape=None,\n    dtype=tf.dtypes.float32,\n    dynamic_size=False,\n    infer_shape=True\n)\n"
  },
  {
    "tf.TensorShape": "tf.TensorShape(\n    dims\n)\n"
  },
  {
    "tf.TensorSpec": "tf.TensorSpec(\n    shape,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.Variable": "tf.Variable(\n    initial_value=None,\n    trainable=None,\n    validate_shape=True,\n    caching_device=None,\n    name=None,\n    variable_def=None,\n    dtype=None,\n    import_scope=None,\n    constraint=None,\n    synchronization=tf.VariableSynchronization.AUTO,\n    aggregation=tf.compat.v1.VariableAggregation.NONE,\n    shape=None,\n    experimental_enable_variable_lifting=True\n)\n"
  },
  {
    "tf.Variable.SaveSliceInfo": "tf.Variable.SaveSliceInfo(\n    full_name=None,\n    full_shape=None,\n    var_offset=None,\n    var_shape=None,\n    save_slice_info_def=None,\n    import_scope=None\n)\n"
  },
  {
    "tf.math.abs": "tf.math.abs(\n    x, name=None\n)\n"
  },
  {
    "tf.math.acos": "tf.math.acos(\n    x, name=None\n)\n"
  },
  {
    "tf.math.acosh": "tf.math.acosh(\n    x, name=None\n)\n"
  },
  {
    "tf.math.add": "tf.math.add(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.add_n": "tf.math.add_n(\n    inputs, name=None\n)\n"
  },
  {
    "tf.approx_top_k": "tf.approx_top_k(\n    input,\n    k,\n    reduction_dimension=-1,\n    recall_target=0.95,\n    is_max_k=True,\n    reduction_input_size_override=-1,\n    aggregate_to_topk=True,\n    name=None\n)\n"
  },
  {
    "tf.math.argmax": "tf.math.argmax(\n    input,\n    axis=None,\n    output_type=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.math.argmin": "tf.math.argmin(\n    input,\n    axis=None,\n    output_type=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.argsort": "tf.argsort(\n    values, axis=-1, direction='ASCENDING', stable=False, name=None\n)\n"
  },
  {
    "tf.dtypes.as_dtype": "tf.dtypes.as_dtype(\n    type_value\n)\n"
  },
  {
    "tf.strings.as_string": "tf.strings.as_string(\n    input,\n    precision=-1,\n    scientific=False,\n    shortest=False,\n    width=-1,\n    fill='',\n    name=None\n)\n"
  },
  {
    "tf.math.asin": "tf.math.asin(\n    x, name=None\n)\n"
  },
  {
    "tf.math.asinh": "tf.math.asinh(\n    x, name=None\n)\n"
  },
  {
    "tf.debugging.assert_equal": "tf.debugging.assert_equal(\n    x, y, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_greater": "tf.debugging.assert_greater(\n    x, y, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_less": "tf.debugging.assert_less(\n    x, y, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_rank": "tf.debugging.assert_rank(\n    x, rank, message=None, name=None\n)\n"
  },
  {
    "tf.math.atan": "tf.math.atan(\n    x, name=None\n)\n"
  },
  {
    "tf.math.atan2": "tf.math.atan2(\n    y, x, name=None\n)\n"
  },
  {
    "tf.math.atanh": "tf.math.atanh(\n    x, name=None\n)\n"
  },
  {
    "tf.audio.decode_wav": "tf.audio.decode_wav(\n    contents, desired_channels=-1, desired_samples=-1, name=None\n)\n"
  },
  {
    "tf.audio.encode_wav": "tf.audio.encode_wav(\n    audio, sample_rate, name=None\n)\n"
  },
  {
    "tf.autodiff.ForwardAccumulator": "tf.autodiff.ForwardAccumulator(\n    primals, tangents\n)\n"
  },
  {
    "tf.GradientTape": "tf.GradientTape(\n    persistent=False, watch_accessed_variables=True\n)\n"
  },
  {
    "tf.autograph.experimental.do_not_convert": "tf.autograph.experimental.do_not_convert(\n    func=None\n)\n"
  },
  {
    "tf.autograph.experimental.set_loop_options": "tf.autograph.experimental.set_loop_options(\n    parallel_iterations=UNSPECIFIED,\n    swap_memory=UNSPECIFIED,\n    maximum_iterations=UNSPECIFIED,\n    shape_invariants=UNSPECIFIED\n)\n"
  },
  {
    "tf.autograph.set_verbosity": "tf.autograph.set_verbosity(\n    level, alsologtostdout=False\n)\n"
  },
  {
    "tf.autograph.to_code": "tf.autograph.to_code(\n    entity, recursive=True, experimental_optional_features=None\n)\n"
  },
  {
    "tf.autograph.to_graph": "tf.autograph.to_graph(\n    entity, recursive=True, experimental_optional_features=None\n)\n"
  },
  {
    "tf.autograph.trace": "tf.autograph.trace(\n    *args\n)\n"
  },
  {
    "tf.batch_to_space": "tf.batch_to_space(\n    input, block_shape, crops, name=None\n)\n"
  },
  {
    "tf.bitcast": "tf.bitcast(\n    input, type, name=None\n)\n"
  },
  {
    "tf.bitwise.bitwise_and": "tf.bitwise.bitwise_and(\n    x, y, name=None\n)\n"
  },
  {
    "tf.bitwise.bitwise_or": "tf.bitwise.bitwise_or(\n    x, y, name=None\n)\n"
  },
  {
    "tf.bitwise.bitwise_xor": "tf.bitwise.bitwise_xor(\n    x, y, name=None\n)\n"
  },
  {
    "tf.bitwise.invert": "tf.bitwise.invert(\n    x, name=None\n)\n"
  },
  {
    "tf.bitwise.left_shift": "tf.bitwise.left_shift(\n    x, y, name=None\n)\n"
  },
  {
    "tf.bitwise.right_shift": "tf.bitwise.right_shift(\n    x, y, name=None\n)\n"
  },
  {
    "tf.boolean_mask": "tf.boolean_mask(\n    tensor, mask, axis=None, name='boolean_mask'\n)\n"
  },
  {
    "tf.broadcast_dynamic_shape": "tf.broadcast_dynamic_shape(\n    shape_x, shape_y\n)\n"
  },
  {
    "tf.broadcast_static_shape": "tf.broadcast_static_shape(\n    shape_x, shape_y\n)\n"
  },
  {
    "tf.broadcast_to": "tf.broadcast_to(\n    input, shape, name=None\n)\n"
  },
  {
    "tf.case": "tf.case(\n    pred_fn_pairs,\n    default=None,\n    exclusive=False,\n    strict=False,\n    name='case'\n)\n"
  },
  {
    "tf.cast": "tf.cast(\n    x, dtype, name=None\n)\n"
  },
  {
    "tf.clip_by_global_norm": "tf.clip_by_global_norm(\n    t_list, clip_norm, use_norm=None, name=None\n)\n"
  },
  {
    "tf.clip_by_norm": "tf.clip_by_norm(\n    t, clip_norm, axes=None, name=None\n)\n"
  },
  {
    "tf.clip_by_value": "tf.clip_by_value(\n    t, clip_value_min, clip_value_max, name=None\n)\n"
  },
  {
    "tf.compat.as_bytes": "tf.compat.as_bytes(\n    bytes_or_text, encoding='utf-8'\n)\n"
  },
  {
    "tf.compat.as_str": "tf.compat.as_str(\n    bytes_or_text, encoding='utf-8'\n)\n"
  },
  {
    "tf.compat.as_str_any": "tf.compat.as_str_any(\n    value, encoding='utf-8'\n)\n"
  },
  {
    "tf.compat.as_text": "tf.compat.as_text(\n    bytes_or_text, encoding='utf-8'\n)\n"
  },
  {
    "tf.compat.dimension_at_index": "tf.compat.dimension_at_index(\n    shape, index\n)\n"
  },
  {
    "tf.compat.dimension_value": "tf.compat.dimension_value(\n    dimension\n)\n"
  },
  {
    "tf.compat.forward_compatible": "tf.compat.forward_compatible(\n    year, month, day\n)\n"
  },
  {
    "tf.compat.path_to_str": "tf.compat.path_to_str(\n    path\n)\n"
  },
  {
    "tf.dtypes.complex": "tf.dtypes.complex(\n    real, imag, name=None\n)\n"
  },
  {
    "tf.concat": "tf.concat(\n    values, axis, name='concat'\n)\n"
  },
  {
    "tf.cond": "tf.cond(\n    pred, true_fn=None, false_fn=None, name=None\n)\n"
  },
  {
    "tf.config.LogicalDevice": "tf.config.LogicalDevice(\n    name, device_type\n)\n"
  },
  {
    "tf.config.LogicalDeviceConfiguration": "tf.config.LogicalDeviceConfiguration(\n    memory_limit=None,\n    experimental_priority=None,\n    experimental_device_ordinal=None\n)\n"
  },
  {
    "tf.config.PhysicalDevice": "tf.config.PhysicalDevice(\n    name, device_type\n)\n"
  },
  {
    "tf.config.LogicalDeviceConfiguration": "tf.config.LogicalDeviceConfiguration(\n    memory_limit=None,\n    experimental_priority=None,\n    experimental_device_ordinal=None\n)\n"
  },
  {
    "tf.config.experimental.enable_tensor_float_32_execution": "tf.config.experimental.enable_tensor_float_32_execution(\n    enabled\n)\n"
  },
  {
    "tf.config.experimental.get_device_details": "tf.config.experimental.get_device_details(\n    device\n)\n"
  },
  {
    "tf.config.experimental.get_memory_growth": "tf.config.experimental.get_memory_growth(\n    device\n)\n"
  },
  {
    "tf.config.experimental.get_memory_info": "tf.config.experimental.get_memory_info(\n    device\n)\n"
  },
  {
    "tf.config.experimental.get_memory_usage": "tf.config.experimental.get_memory_usage(\n    device\n)\n"
  },
  {
    "tf.config.get_logical_device_configuration": "tf.config.get_logical_device_configuration(\n    device\n)\n"
  },
  {
    "tf.config.get_visible_devices": "tf.config.get_visible_devices(\n    device_type=None\n)\n"
  },
  {
    "tf.config.list_logical_devices": "tf.config.list_logical_devices(\n    device_type=None\n)\n"
  },
  {
    "tf.config.list_physical_devices": "tf.config.list_physical_devices(\n    device_type=None\n)\n"
  },
  {
    "tf.config.experimental.reset_memory_stats": "tf.config.experimental.reset_memory_stats(\n    device\n)\n"
  },
  {
    "tf.config.experimental.set_device_policy": "tf.config.experimental.set_device_policy(\n    device_policy\n)\n"
  },
  {
    "tf.config.experimental.set_memory_growth": "tf.config.experimental.set_memory_growth(\n    device, enable\n)\n"
  },
  {
    "tf.config.experimental.set_synchronous_execution": "tf.config.experimental.set_synchronous_execution(\n    enable\n)\n"
  },
  {
    "tf.config.set_logical_device_configuration": "tf.config.set_logical_device_configuration(\n    device, logical_devices\n)\n"
  },
  {
    "tf.config.set_visible_devices": "tf.config.set_visible_devices(\n    devices, device_type=None\n)\n"
  },
  {
    "tf.config.experimental_connect_to_cluster": "tf.config.experimental_connect_to_cluster(\n    cluster_spec_or_resolver,\n    job_name='localhost',\n    task_index=0,\n    protocol=None,\n    make_master_device_default=True,\n    cluster_device_filters=None\n)\n"
  },
  {
    "tf.config.experimental_connect_to_host": "tf.config.experimental_connect_to_host(\n    remote_host=None, job_name='worker'\n)\n"
  },
  {
    "tf.config.experimental_run_functions_eagerly": "tf.config.experimental_run_functions_eagerly(\n    run_eagerly\n)\n"
  },
  {
    "tf.config.get_logical_device_configuration": "tf.config.get_logical_device_configuration(\n    device\n)\n"
  },
  {
    "tf.config.get_visible_devices": "tf.config.get_visible_devices(\n    device_type=None\n)\n"
  },
  {
    "tf.config.list_logical_devices": "tf.config.list_logical_devices(\n    device_type=None\n)\n"
  },
  {
    "tf.config.list_physical_devices": "tf.config.list_physical_devices(\n    device_type=None\n)\n"
  },
  {
    "tf.config.optimizer.get_jit": "tf.config.optimizer.get_jit() -> str\n"
  },
  {
    "tf.config.optimizer.set_experimental_options": "tf.config.optimizer.set_experimental_options(\n    options\n)\n"
  },
  {
    "tf.config.optimizer.set_jit": "tf.config.optimizer.set_jit(\n    enabled: Union[bool, str]\n)\n"
  },
  {
    "tf.config.run_functions_eagerly": "tf.config.run_functions_eagerly(\n    run_eagerly\n)\n"
  },
  {
    "tf.config.set_logical_device_configuration": "tf.config.set_logical_device_configuration(\n    device, logical_devices\n)\n"
  },
  {
    "tf.config.set_soft_device_placement": "tf.config.set_soft_device_placement(\n    enabled\n)\n"
  },
  {
    "tf.config.set_visible_devices": "tf.config.set_visible_devices(\n    devices, device_type=None\n)\n"
  },
  {
    "tf.config.threading.set_inter_op_parallelism_threads": "tf.config.threading.set_inter_op_parallelism_threads(\n    num_threads\n)\n"
  },
  {
    "tf.config.threading.set_intra_op_parallelism_threads": "tf.config.threading.set_intra_op_parallelism_threads(\n    num_threads\n)\n"
  },
  {
    "tf.constant": "tf.constant(\n    value, dtype=None, shape=None, name='Const'\n)\n"
  },
  {
    "tf.constant_initializer": "tf.constant_initializer(\n    value=0\n)\n"
  },
  {
    "tf.control_dependencies": "tf.control_dependencies(\n    control_inputs\n)\n"
  },
  {
    "tf.convert_to_tensor": "tf.convert_to_tensor(\n    value, dtype=None, dtype_hint=None, name=None\n)\n"
  },
  {
    "tf.math.cos": "tf.math.cos(\n    x, name=None\n)\n"
  },
  {
    "tf.math.cosh": "tf.math.cosh(\n    x, name=None\n)\n"
  },
  {
    "tf.math.cumsum": "tf.math.cumsum(\n    x, axis=0, exclusive=False, reverse=False, name=None\n)\n"
  },
  {
    "tf.custom_gradient": "tf.custom_gradient(\n    f=None\n)\n"
  },
  {
    "tf.data.Dataset": "tf.data.Dataset(\n    variant_tensor\n)\n"
  },
  {
    "tf.data.DatasetSpec": "tf.data.DatasetSpec(\n    element_spec, dataset_shape=()\n)\n"
  },
  {
    "tf.data.FixedLengthRecordDataset": "tf.data.FixedLengthRecordDataset(\n    filenames,\n    record_bytes,\n    header_bytes=None,\n    footer_bytes=None,\n    buffer_size=None,\n    compression_type=None,\n    num_parallel_reads=None,\n    name=None\n)\n"
  },
  {
    "tf.data.IteratorSpec": "tf.data.IteratorSpec(\n    element_spec\n)\n"
  },
  {
    "tf.data.TFRecordDataset": "tf.data.TFRecordDataset(\n    filenames,\n    compression_type=None,\n    buffer_size=None,\n    num_parallel_reads=None,\n    name=None\n)\n"
  },
  {
    "tf.data.TextLineDataset": "tf.data.TextLineDataset(\n    filenames,\n    compression_type=None,\n    buffer_size=None,\n    num_parallel_reads=None,\n    name=None\n)\n"
  },
  {
    "tf.data.experimental.CheckpointInputPipelineHook": "tf.data.experimental.CheckpointInputPipelineHook(\n    estimator, external_state_policy=None\n)\n"
  },
  {
    "tf.data.experimental.Counter": "tf.data.experimental.Counter(\n    start=0,\n    step=1,\n    dtype=tf.dtypes.int64\n)\n"
  },
  {
    "tf.data.experimental.CsvDataset": "tf.data.experimental.CsvDataset(\n    filenames,\n    record_defaults,\n    compression_type=None,\n    buffer_size=None,\n    header=False,\n    field_delim=',',\n    use_quote_delim=True,\n    na_value='',\n    select_cols=None,\n    exclude_cols=None\n)\n"
  },
  {
    "tf.data.experimental.DatasetInitializer": "tf.data.experimental.DatasetInitializer(\n    dataset\n)\n"
  },
  {
    "tf.data.experimental.RandomDataset": "tf.data.experimental.RandomDataset(\n    seed=None, name=None\n)\n"
  },
  {
    "tf.data.experimental.Reducer": "tf.data.experimental.Reducer(\n    init_func, reduce_func, finalize_func\n)\n"
  },
  {
    "tf.data.experimental.SqlDataset": "tf.data.experimental.SqlDataset(\n    driver_name, data_source_name, query, output_types\n)\n"
  },
  {
    "tf.data.experimental.TFRecordWriter": "tf.data.experimental.TFRecordWriter(\n    filename, compression_type=None\n)\n"
  },
  {
    "tf.data.experimental.assert_cardinality": "tf.data.experimental.assert_cardinality(\n    expected_cardinality\n)\n"
  },
  {
    "tf.data.experimental.bucket_by_sequence_length": "tf.data.experimental.bucket_by_sequence_length(\n    element_length_func,\n    bucket_boundaries,\n    bucket_batch_sizes,\n    padded_shapes=None,\n    padding_values=None,\n    pad_to_bucket_boundary=False,\n    no_padding=False,\n    drop_remainder=False\n)\n"
  },
  {
    "tf.data.experimental.cardinality": "tf.data.experimental.cardinality(\n    dataset\n)\n"
  },
  {
    "tf.data.experimental.choose_from_datasets": "tf.data.experimental.choose_from_datasets(\n    datasets, choice_dataset, stop_on_empty_dataset=False\n)\n"
  },
  {
    "tf.data.experimental.copy_to_device": "tf.data.experimental.copy_to_device(\n    target_device, source_device='/cpu:0'\n)\n"
  },
  {
    "tf.data.experimental.dense_to_ragged_batch": "tf.data.experimental.dense_to_ragged_batch(\n    batch_size,\n    drop_remainder=False,\n    row_splits_dtype=tf.dtypes.int64\n)\n"
  },
  {
    "tf.data.experimental.dense_to_sparse_batch": "tf.data.experimental.dense_to_sparse_batch(\n    batch_size, row_shape\n)\n"
  },
  {
    "tf.data.experimental.enumerate_dataset": "tf.data.experimental.enumerate_dataset(\n    start=0\n)\n"
  },
  {
    "tf.data.experimental.from_list": "tf.data.experimental.from_list(\n    elements, name=None\n)\n"
  },
  {
    "tf.data.experimental.from_variant": "tf.data.experimental.from_variant(\n    variant, structure\n)\n"
  },
  {
    "tf.data.experimental.get_next_as_optional": "tf.data.experimental.get_next_as_optional(\n    iterator\n)\n"
  },
  {
    "tf.data.experimental.get_single_element": "tf.data.experimental.get_single_element(\n    dataset\n)\n"
  },
  {
    "tf.data.experimental.get_structure": "tf.data.experimental.get_structure(\n    dataset_or_iterator\n)\n"
  },
  {
    "tf.data.experimental.group_by_reducer": "tf.data.experimental.group_by_reducer(\n    key_func, reducer\n)\n"
  },
  {
    "tf.data.experimental.group_by_window": "tf.data.experimental.group_by_window(\n    key_func, reduce_func, window_size=None, window_size_func=None\n)\n"
  },
  {
    "tf.data.experimental.ignore_errors": "tf.data.experimental.ignore_errors(\n    log_warning=False\n)\n"
  },
  {
    "tf.data.experimental.index_table_from_dataset": "tf.data.experimental.index_table_from_dataset(\n    dataset=None,\n    num_oov_buckets=0,\n    vocab_size=None,\n    default_value=-1,\n    hasher_spec=lookup_ops.FastHashSpec,\n    key_dtype=tf.dtypes.string,\n    name=None\n)\n"
  },
  {
    "tf.data.experimental.load": "tf.data.experimental.load(\n    path, element_spec=None, compression=None, reader_func=None\n)\n"
  },
  {
    "tf.data.experimental.make_batched_features_dataset": "tf.data.experimental.make_batched_features_dataset(\n    file_pattern,\n    batch_size,\n    features,\n    reader=None,\n    label_key=None,\n    reader_args=None,\n    num_epochs=None,\n    shuffle=True,\n    shuffle_buffer_size=10000,\n    shuffle_seed=None,\n    prefetch_buffer_size=None,\n    reader_num_threads=None,\n    parser_num_threads=None,\n    sloppy_ordering=False,\n    drop_final_batch=False\n)\n"
  },
  {
    "tf.data.experimental.make_csv_dataset": "tf.data.experimental.make_csv_dataset(\n    file_pattern,\n    batch_size,\n    column_names=None,\n    column_defaults=None,\n    label_name=None,\n    select_columns=None,\n    field_delim=',',\n    use_quote_delim=True,\n    na_value='',\n    header=True,\n    num_epochs=None,\n    shuffle=True,\n    shuffle_buffer_size=10000,\n    shuffle_seed=None,\n    prefetch_buffer_size=None,\n    num_parallel_reads=None,\n    sloppy=False,\n    num_rows_for_inference=100,\n    compression_type=None,\n    ignore_errors=False,\n    encoding='utf-8'\n)\n"
  },
  {
    "tf.data.experimental.make_saveable_from_iterator": "tf.data.experimental.make_saveable_from_iterator(\n    iterator, external_state_policy=None\n)\n"
  },
  {
    "tf.data.experimental.map_and_batch": "tf.data.experimental.map_and_batch(\n    map_func,\n    batch_size,\n    num_parallel_batches=None,\n    drop_remainder=False,\n    num_parallel_calls=None\n)\n"
  },
  {
    "tf.data.experimental.parallel_interleave": "tf.data.experimental.parallel_interleave(\n    map_func,\n    cycle_length,\n    block_length=1,\n    sloppy=False,\n    buffer_output_elements=None,\n    prefetch_input_elements=None\n)\n"
  },
  {
    "tf.data.experimental.parse_example_dataset": "tf.data.experimental.parse_example_dataset(\n    features, num_parallel_calls=1, deterministic=None\n)\n"
  },
  {
    "tf.data.experimental.prefetch_to_device": "tf.data.experimental.prefetch_to_device(\n    device, buffer_size=None\n)\n"
  },
  {
    "tf.data.experimental.rejection_resample": "tf.data.experimental.rejection_resample(\n    class_func, target_dist, initial_dist=None, seed=None\n)\n"
  },
  {
    "tf.data.experimental.sample_from_datasets": "tf.data.experimental.sample_from_datasets(\n    datasets, weights=None, seed=None, stop_on_empty_dataset=False\n)\n"
  },
  {
    "tf.data.experimental.save": "tf.data.experimental.save(\n    dataset, path, compression=None, shard_func=None, checkpoint_args=None\n)\n"
  },
  {
    "tf.data.experimental.scan": "tf.data.experimental.scan(\n    initial_state, scan_func\n)\n"
  },
  {
    "tf.data.experimental.service.CrossTrainerCache": "tf.data.experimental.service.CrossTrainerCache(\n    trainer_id\n)\n"
  },
  {
    "tf.data.experimental.service.DispatchServer": "tf.data.experimental.service.DispatchServer(\n    config=None, start=True\n)\n"
  },
  {
    "tf.data.experimental.service.DispatcherConfig": "tf.data.experimental.service.DispatcherConfig(\n    port=0,\n    protocol=None,\n    work_dir=None,\n    fault_tolerant_mode=False,\n    worker_addresses=None,\n    job_gc_check_interval_ms=None,\n    job_gc_timeout_ms=None\n)\n"
  },
  {
    "tf.data.experimental.service.WorkerConfig": "tf.data.experimental.service.WorkerConfig(\n    dispatcher_address,\n    worker_address=None,\n    port=0,\n    protocol=None,\n    heartbeat_interval_ms=None,\n    dispatcher_timeout_ms=None,\n    data_transfer_protocol=None,\n    data_transfer_address=None\n)\n"
  },
  {
    "tf.data.experimental.service.WorkerServer": "tf.data.experimental.service.WorkerServer(\n    config, start=True\n)\n"
  },
  {
    "tf.data.experimental.service.distribute": "tf.data.experimental.service.distribute(\n    processing_mode,\n    service,\n    job_name=None,\n    consumer_index=None,\n    num_consumers=None,\n    max_outstanding_requests=None,\n    data_transfer_protocol=None,\n    compression='AUTO',\n    cross_trainer_cache=None,\n    target_workers='AUTO'\n)\n"
  },
  {
    "tf.data.experimental.service.from_dataset_id": "tf.data.experimental.service.from_dataset_id(\n    processing_mode,\n    service,\n    dataset_id,\n    element_spec=None,\n    job_name=None,\n    consumer_index=None,\n    num_consumers=None,\n    max_outstanding_requests=None,\n    data_transfer_protocol=None,\n    cross_trainer_cache=None,\n    target_workers='AUTO'\n)\n"
  },
  {
    "tf.data.experimental.service.register_dataset": "tf.data.experimental.service.register_dataset(\n    service, dataset, compression='AUTO', dataset_id=None\n)\n"
  },
  {
    "tf.data.experimental.shuffle_and_repeat": "tf.data.experimental.shuffle_and_repeat(\n    buffer_size, count=None, seed=None\n)\n"
  },
  {
    "tf.data.experimental.snapshot": "tf.data.experimental.snapshot(\n    path, compression='AUTO', reader_func=None, shard_func=None\n)\n"
  },
  {
    "tf.data.experimental.table_from_dataset": "tf.data.experimental.table_from_dataset(\n    dataset=None,\n    num_oov_buckets=0,\n    vocab_size=None,\n    default_value=None,\n    hasher_spec=lookup_ops.FastHashSpec,\n    key_dtype=tf.dtypes.string,\n    name=None\n)\n"
  },
  {
    "tf.data.experimental.take_while": "tf.data.experimental.take_while(\n    predicate\n)\n"
  },
  {
    "tf.data.experimental.to_variant": "tf.data.experimental.to_variant(\n    dataset\n)\n"
  },
  {
    "tf.debugging.Assert": "tf.debugging.Assert(\n    condition, data, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_all_finite": "tf.debugging.assert_all_finite(\n    x, message, name=None\n)\n"
  },
  {
    "tf.debugging.assert_equal": "tf.debugging.assert_equal(\n    x, y, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_greater": "tf.debugging.assert_greater(\n    x, y, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_greater_equal": "tf.debugging.assert_greater_equal(\n    x, y, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_integer": "tf.debugging.assert_integer(\n    x, message=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_less": "tf.debugging.assert_less(\n    x, y, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_less_equal": "tf.debugging.assert_less_equal(\n    x, y, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_near": "tf.debugging.assert_near(\n    x, y, rtol=None, atol=None, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_negative": "tf.debugging.assert_negative(\n    x, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_non_negative": "tf.debugging.assert_non_negative(\n    x, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_non_positive": "tf.debugging.assert_non_positive(\n    x, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_none_equal": "tf.debugging.assert_none_equal(\n    x, y, summarize=None, message=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_positive": "tf.debugging.assert_positive(\n    x, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_proper_iterable": "tf.debugging.assert_proper_iterable(\n    values\n)\n"
  },
  {
    "tf.debugging.assert_rank": "tf.debugging.assert_rank(\n    x, rank, message=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_rank_at_least": "tf.debugging.assert_rank_at_least(\n    x, rank, message=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_rank_in": "tf.debugging.assert_rank_in(\n    x, ranks, message=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_same_float_dtype": "tf.debugging.assert_same_float_dtype(\n    tensors=None, dtype=None\n)\n"
  },
  {
    "tf.debugging.assert_scalar": "tf.debugging.assert_scalar(\n    tensor, message=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_shapes": "tf.debugging.assert_shapes(\n    shapes, data=None, summarize=None, message=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_type": "tf.debugging.assert_type(\n    tensor, tf_type, message=None, name=None\n)\n"
  },
  {
    "tf.debugging.check_numerics": "tf.debugging.check_numerics(\n    tensor, message, name=None\n)\n"
  },
  {
    "tf.debugging.enable_check_numerics": "tf.debugging.enable_check_numerics(\n    stack_height_limit=30, path_length_limit=50\n)\n"
  },
  {
    "tf.debugging.experimental.enable_dump_debug_info": "tf.debugging.experimental.enable_dump_debug_info(\n    dump_root,\n    tensor_debug_mode=DEFAULT_TENSOR_DEBUG_MODE,\n    circular_buffer_size=1000,\n    op_regex=None,\n    tensor_dtypes=None\n)\n"
  },
  {
    "tf.debugging.is_numeric_tensor": "tf.debugging.is_numeric_tensor(\n    tensor\n)\n"
  },
  {
    "tf.debugging.set_log_device_placement": "tf.debugging.set_log_device_placement(\n    enabled\n)\n"
  },
  {
    "tf.device": "tf.device(\n    device_name\n)\n"
  },
  {
    "tf.distribute.HierarchicalCopyAllReduce": "tf.distribute.HierarchicalCopyAllReduce(\n    num_packs=1\n)\n"
  },
  {
    "tf.distribute.InputContext": "tf.distribute.InputContext(\n    num_input_pipelines=1, input_pipeline_id=0, num_replicas_in_sync=1\n)\n"
  },
  {
    "tf.distribute.InputOptions": "tf.distribute.InputOptions(\n    experimental_fetch_to_device=None,\n    experimental_replication_mode=tf.distribute.InputReplicationMode.PER_WORKER,\n    experimental_place_dataset_on_device=False,\n    experimental_per_replica_buffer_size=1\n)\n"
  },
  {
    "tf.distribute.MirroredStrategy": "tf.distribute.MirroredStrategy(\n    devices=None, cross_device_ops=None\n)\n"
  },
  {
    "tf.distribute.MultiWorkerMirroredStrategy": "tf.distribute.MultiWorkerMirroredStrategy(\n    cluster_resolver=None, communication_options=None\n)\n"
  },
  {
    "tf.distribute.NcclAllReduce": "tf.distribute.NcclAllReduce(\n    num_packs=1\n)\n"
  },
  {
    "tf.distribute.OneDeviceStrategy": "tf.distribute.OneDeviceStrategy(\n    device\n)\n"
  },
  {
    "tf.distribute.experimental.ParameterServerStrategy": "tf.distribute.experimental.ParameterServerStrategy(\n    cluster_resolver, variable_partitioner=None\n)\n"
  },
  {
    "tf.distribute.ReductionToOneDevice": "tf.distribute.ReductionToOneDevice(\n    reduce_to_device=None, accumulation_fn=None\n)\n"
  },
  {
    "tf.distribute.ReplicaContext": "tf.distribute.ReplicaContext(\n    strategy, replica_id_in_sync_group\n)\n"
  },
  {
    "tf.distribute.RunOptions": "tf.distribute.RunOptions(\n    experimental_enable_dynamic_batch_size=True,\n    experimental_bucketizing_dynamic_shape=False,\n    experimental_xla_options=None\n)\n"
  },
  {
    "tf.distribute.Server": "tf.distribute.Server(\n    server_or_cluster_def,\n    job_name=None,\n    task_index=None,\n    protocol=None,\n    config=None,\n    start=True\n)\n"
  },
  {
    "tf.distribute.Strategy": "tf.distribute.Strategy(\n    extended\n)\n"
  },
  {
    "tf.distribute.StrategyExtended": "tf.distribute.StrategyExtended(\n    container_strategy\n)\n"
  },
  {
    "tf.distribute.TPUStrategy": "tf.distribute.TPUStrategy(\n    tpu_cluster_resolver=None,\n    experimental_device_assignment=None,\n    experimental_spmd_xla_partitioning=False\n)\n"
  },
  {
    "tf.distribute.cluster_resolver.GCEClusterResolver": "tf.distribute.cluster_resolver.GCEClusterResolver(\n    project,\n    zone,\n    instance_group,\n    port,\n    task_type='worker',\n    task_id=0,\n    rpc_layer='grpc',\n    credentials='default',\n    service=None\n)\n"
  },
  {
    "tf.distribute.cluster_resolver.KubernetesClusterResolver": "tf.distribute.cluster_resolver.KubernetesClusterResolver(\n    job_to_label_mapping=None,\n    tf_server_port=8470,\n    rpc_layer='grpc',\n    override_client=None\n)\n"
  },
  {
    "tf.distribute.cluster_resolver.SimpleClusterResolver": "tf.distribute.cluster_resolver.SimpleClusterResolver(\n    cluster_spec,\n    master='',\n    task_type=None,\n    task_id=None,\n    environment='',\n    num_accelerators=None,\n    rpc_layer=None\n)\n"
  },
  {
    "tf.distribute.cluster_resolver.SlurmClusterResolver": "tf.distribute.cluster_resolver.SlurmClusterResolver(\n    jobs=None,\n    port_base=8888,\n    gpus_per_node=None,\n    gpus_per_task=None,\n    tasks_per_node=None,\n    auto_set_gpu=True,\n    rpc_layer='grpc'\n)\n"
  },
  {
    "tf.distribute.cluster_resolver.TFConfigClusterResolver": "tf.distribute.cluster_resolver.TFConfigClusterResolver(\n    task_type=None, task_id=None, rpc_layer=None, environment=None\n)\n"
  },
  {
    "tf.distribute.cluster_resolver.TPUClusterResolver": "tf.distribute.cluster_resolver.TPUClusterResolver(\n    tpu=None,\n    zone=None,\n    project=None,\n    job_name='worker',\n    coordinator_name=None,\n    coordinator_address=None,\n    credentials='default',\n    service=None,\n    discovery_url=None\n)\n"
  },
  {
    "tf.distribute.cluster_resolver.UnionResolver": "tf.distribute.cluster_resolver.UnionResolver(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.distribute.experimental.coordinator.ClusterCoordinator": "tf.distribute.experimental.coordinator.ClusterCoordinator(\n    strategy\n)\n"
  },
  {
    "tf.distribute.experimental.coordinator.PerWorkerValues": "tf.distribute.experimental.coordinator.PerWorkerValues(\n    values\n)\n"
  },
  {
    "tf.distribute.experimental.CentralStorageStrategy": "tf.distribute.experimental.CentralStorageStrategy(\n    compute_devices=None, parameter_device=None\n)\n"
  },
  {
    "tf.distribute.experimental.CollectiveHints": "tf.distribute.experimental.CollectiveHints(\n    bytes_per_pack=0, timeout_seconds=None\n)\n"
  },
  {
    "tf.distribute.experimental.CommunicationOptions": "tf.distribute.experimental.CommunicationOptions(\n    bytes_per_pack=0,\n    timeout_seconds=None,\n    implementation=tf.distribute.experimental.CollectiveCommunication.AUTO\n)\n"
  },
  {
    "tf.distribute.experimental.MultiWorkerMirroredStrategy": "tf.distribute.experimental.MultiWorkerMirroredStrategy(\n    communication=tf.distribute.experimental.CollectiveCommunication.AUTO,\n    cluster_resolver=None\n)\n"
  },
  {
    "tf.distribute.experimental.ParameterServerStrategy": "tf.distribute.experimental.ParameterServerStrategy(\n    cluster_resolver, variable_partitioner=None\n)\n"
  },
  {
    "tf.distribute.experimental.PreemptionCheckpointHandler": "tf.distribute.experimental.PreemptionCheckpointHandler(\n    cluster_resolver,\n    checkpoint_or_checkpoint_manager,\n    checkpoint_dir=None,\n    termination_config=None\n)\n"
  },
  {
    "tf.distribute.experimental.TPUStrategy": "tf.distribute.experimental.TPUStrategy(\n    tpu_cluster_resolver=None, device_assignment=None\n)\n"
  },
  {
    "tf.distribute.experimental.TerminationConfig": "tf.distribute.experimental.TerminationConfig(\n    termination_watcher_fn=None, exit_fn=None, grace_period=None\n)\n"
  },
  {
    "tf.distribute.experimental.ValueContext": "tf.distribute.experimental.ValueContext(\n    replica_id_in_sync_group=0, num_replicas_in_sync=1\n)\n"
  },
  {
    "tf.distribute.experimental.coordinator.ClusterCoordinator": "tf.distribute.experimental.coordinator.ClusterCoordinator(\n    strategy\n)\n"
  },
  {
    "tf.distribute.experimental.coordinator.PerWorkerValues": "tf.distribute.experimental.coordinator.PerWorkerValues(\n    values\n)\n"
  },
  {
    "tf.distribute.experimental.partitioners.FixedShardsPartitioner": "tf.distribute.experimental.partitioners.FixedShardsPartitioner(\n    num_shards\n)\n"
  },
  {
    "tf.distribute.experimental.partitioners.MaxSizePartitioner": "tf.distribute.experimental.partitioners.MaxSizePartitioner(\n    max_shard_bytes, max_shards=None, bytes_per_string=16\n)\n"
  },
  {
    "tf.distribute.experimental.partitioners.MinSizePartitioner": "tf.distribute.experimental.partitioners.MinSizePartitioner(\n    min_shard_bytes=(256 << 10), max_shards=1, bytes_per_string=16\n)\n"
  },
  {
    "tf.distribute.experimental_set_strategy": "tf.distribute.experimental_set_strategy(\n    strategy\n)\n"
  },
  {
    "tf.math.divide": "tf.math.divide(\n    x, y, name=None\n)\n"
  },
  {
    "tf.dtypes.as_dtype": "tf.dtypes.as_dtype(\n    type_value\n)\n"
  },
  {
    "tf.cast": "tf.cast(\n    x, dtype, name=None\n)\n"
  },
  {
    "tf.dtypes.complex": "tf.dtypes.complex(\n    real, imag, name=None\n)\n"
  },
  {
    "tf.dtypes.saturate_cast": "tf.dtypes.saturate_cast(\n    value, dtype, name=None\n)\n"
  },
  {
    "tf.dynamic_partition": "tf.dynamic_partition(\n    data, partitions, num_partitions, name=None\n)\n"
  },
  {
    "tf.dynamic_stitch": "tf.dynamic_stitch(\n    indices, data, name=None\n)\n"
  },
  {
    "tf.edit_distance": "tf.edit_distance(\n    hypothesis, truth, normalize=True, name='edit_distance'\n)\n"
  },
  {
    "tf.linalg.eig": "tf.linalg.eig(\n    tensor, name=None\n)\n"
  },
  {
    "tf.linalg.eigvals": "tf.linalg.eigvals(\n    tensor, name=None\n)\n"
  },
  {
    "tf.einsum": "tf.einsum(\n    equation, *inputs, **kwargs\n)\n"
  },
  {
    "tf.ensure_shape": "tf.ensure_shape(\n    x, shape, name=None\n)\n"
  },
  {
    "tf.math.equal": "tf.math.equal(\n    x, y, name=None\n)\n"
  },
  {
    "tf.errors.AbortedError": "tf.errors.AbortedError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.AlreadyExistsError": "tf.errors.AlreadyExistsError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.CancelledError": "tf.errors.CancelledError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.DataLossError": "tf.errors.DataLossError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.DeadlineExceededError": "tf.errors.DeadlineExceededError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.FailedPreconditionError": "tf.errors.FailedPreconditionError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.InternalError": "tf.errors.InternalError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.InvalidArgumentError": "tf.errors.InvalidArgumentError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.NotFoundError": "tf.errors.NotFoundError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.OpError": "tf.errors.OpError(\n    node_def, op, message, error_code, *args\n)\n"
  },
  {
    "tf.errors.OperatorNotAllowedInGraphError": "tf.errors.OperatorNotAllowedInGraphError(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.errors.OutOfRangeError": "tf.errors.OutOfRangeError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.PermissionDeniedError": "tf.errors.PermissionDeniedError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.ResourceExhaustedError": "tf.errors.ResourceExhaustedError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.UnauthenticatedError": "tf.errors.UnauthenticatedError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.UnavailableError": "tf.errors.UnavailableError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.UnimplementedError": "tf.errors.UnimplementedError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.UnknownError": "tf.errors.UnknownError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.estimator.BaselineClassifier": "tf.estimator.BaselineClassifier(\n    model_dir=None,\n    n_classes=2,\n    weight_column=None,\n    label_vocabulary=None,\n    optimizer='Ftrl',\n    config=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE\n)\n"
  },
  {
    "tf.estimator.BaselineEstimator": "tf.estimator.BaselineEstimator(\n    head, model_dir=None, optimizer='Ftrl', config=None\n)\n"
  },
  {
    "tf.estimator.BaselineRegressor": "tf.estimator.BaselineRegressor(\n    model_dir=None,\n    label_dimension=1,\n    weight_column=None,\n    optimizer='Ftrl',\n    config=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE\n)\n"
  },
  {
    "tf.estimator.BestExporter": "tf.estimator.BestExporter(\n    name='best_exporter',\n    serving_input_receiver_fn=None,\n    event_file_pattern='eval/*.tfevents.*',\n    compare_fn=_loss_smaller,\n    assets_extra=None,\n    as_text=False,\n    exports_to_keep=5\n)\n"
  },
  {
    "tf.estimator.BinaryClassHead": "tf.estimator.BinaryClassHead(\n    weight_column=None,\n    thresholds=None,\n    label_vocabulary=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    loss_fn=None,\n    name=None\n)\n"
  },
  {
    "tf.estimator.CheckpointSaverHook": "tf.estimator.CheckpointSaverHook(\n    checkpoint_dir,\n    save_secs=None,\n    save_steps=None,\n    saver=None,\n    checkpoint_basename='model.ckpt',\n    scaffold=None,\n    listeners=None,\n    save_graph_def=True\n)\n"
  },
  {
    "tf.estimator.DNNClassifier": "tf.estimator.DNNClassifier(\n    hidden_units,\n    feature_columns,\n    model_dir=None,\n    n_classes=2,\n    weight_column=None,\n    label_vocabulary=None,\n    optimizer='Adagrad',\n    activation_fn=tf.nn.relu,\n    dropout=None,\n    config=None,\n    warm_start_from=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    batch_norm=False\n)\n"
  },
  {
    "tf.estimator.DNNEstimator": "tf.estimator.DNNEstimator(\n    head,\n    hidden_units,\n    feature_columns,\n    model_dir=None,\n    optimizer='Adagrad',\n    activation_fn=tf.nn.relu,\n    dropout=None,\n    config=None,\n    warm_start_from=None,\n    batch_norm=False\n)\n"
  },
  {
    "tf.estimator.DNNLinearCombinedClassifier": "tf.estimator.DNNLinearCombinedClassifier(\n    model_dir=None,\n    linear_feature_columns=None,\n    linear_optimizer='Ftrl',\n    dnn_feature_columns=None,\n    dnn_optimizer='Adagrad',\n    dnn_hidden_units=None,\n    dnn_activation_fn=tf.nn.relu,\n    dnn_dropout=None,\n    n_classes=2,\n    weight_column=None,\n    label_vocabulary=None,\n    config=None,\n    warm_start_from=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    batch_norm=False,\n    linear_sparse_combiner='sum'\n)\n"
  },
  {
    "tf.estimator.DNNLinearCombinedEstimator": "tf.estimator.DNNLinearCombinedEstimator(\n    head,\n    model_dir=None,\n    linear_feature_columns=None,\n    linear_optimizer='Ftrl',\n    dnn_feature_columns=None,\n    dnn_optimizer='Adagrad',\n    dnn_hidden_units=None,\n    dnn_activation_fn=tf.nn.relu,\n    dnn_dropout=None,\n    config=None,\n    batch_norm=False,\n    linear_sparse_combiner='sum'\n)\n"
  },
  {
    "tf.estimator.DNNLinearCombinedRegressor": "tf.estimator.DNNLinearCombinedRegressor(\n    model_dir=None,\n    linear_feature_columns=None,\n    linear_optimizer='Ftrl',\n    dnn_feature_columns=None,\n    dnn_optimizer='Adagrad',\n    dnn_hidden_units=None,\n    dnn_activation_fn=tf.nn.relu,\n    dnn_dropout=None,\n    label_dimension=1,\n    weight_column=None,\n    config=None,\n    warm_start_from=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    batch_norm=False,\n    linear_sparse_combiner='sum'\n)\n"
  },
  {
    "tf.estimator.DNNRegressor": "tf.estimator.DNNRegressor(\n    hidden_units,\n    feature_columns,\n    model_dir=None,\n    label_dimension=1,\n    weight_column=None,\n    optimizer='Adagrad',\n    activation_fn=tf.nn.relu,\n    dropout=None,\n    config=None,\n    warm_start_from=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    batch_norm=False\n)\n"
  },
  {
    "tf.estimator.Estimator": "tf.estimator.Estimator(\n    model_fn, model_dir=None, config=None, params=None, warm_start_from=None\n)\n"
  },
  {
    "tf.estimator.EstimatorSpec": "tf.estimator.EstimatorSpec(\n    mode,\n    predictions=None,\n    loss=None,\n    train_op=None,\n    eval_metric_ops=None,\n    export_outputs=None,\n    training_chief_hooks=None,\n    training_hooks=None,\n    scaffold=None,\n    evaluation_hooks=None,\n    prediction_hooks=None\n)\n"
  },
  {
    "tf.estimator.EvalSpec": "tf.estimator.EvalSpec(\n    input_fn,\n    steps=100,\n    name=None,\n    hooks=None,\n    exporters=None,\n    start_delay_secs=120,\n    throttle_secs=600\n)\n"
  },
  {
    "tf.estimator.FeedFnHook": "tf.estimator.FeedFnHook(\n    feed_fn\n)\n"
  },
  {
    "tf.estimator.FinalExporter": "tf.estimator.FinalExporter(\n    name, serving_input_receiver_fn, assets_extra=None, as_text=False\n)\n"
  },
  {
    "tf.estimator.FinalOpsHook": "tf.estimator.FinalOpsHook(\n    final_ops, final_ops_feed_dict=None\n)\n"
  },
  {
    "tf.estimator.GlobalStepWaiterHook": "tf.estimator.GlobalStepWaiterHook(\n    wait_until_step\n)\n"
  },
  {
    "tf.estimator.LatestExporter": "tf.estimator.LatestExporter(\n    name,\n    serving_input_receiver_fn,\n    assets_extra=None,\n    as_text=False,\n    exports_to_keep=5\n)\n"
  },
  {
    "tf.estimator.LinearClassifier": "tf.estimator.LinearClassifier(\n    feature_columns,\n    model_dir=None,\n    n_classes=2,\n    weight_column=None,\n    label_vocabulary=None,\n    optimizer='Ftrl',\n    config=None,\n    warm_start_from=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    sparse_combiner='sum'\n)\n"
  },
  {
    "tf.estimator.LinearEstimator": "tf.estimator.LinearEstimator(\n    head,\n    feature_columns,\n    model_dir=None,\n    optimizer='Ftrl',\n    config=None,\n    sparse_combiner='sum',\n    warm_start_from=None\n)\n"
  },
  {
    "tf.estimator.LinearRegressor": "tf.estimator.LinearRegressor(\n    feature_columns,\n    model_dir=None,\n    label_dimension=1,\n    weight_column=None,\n    optimizer='Ftrl',\n    config=None,\n    warm_start_from=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    sparse_combiner='sum'\n)\n"
  },
  {
    "tf.estimator.LoggingTensorHook": "tf.estimator.LoggingTensorHook(\n    tensors, every_n_iter=None, every_n_secs=None, at_end=False, formatter=None\n)\n"
  },
  {
    "tf.estimator.LogisticRegressionHead": "tf.estimator.LogisticRegressionHead(\n    weight_column=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    name=None\n)\n"
  },
  {
    "tf.estimator.MultiClassHead": "tf.estimator.MultiClassHead(\n    n_classes,\n    weight_column=None,\n    label_vocabulary=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    loss_fn=None,\n    name=None\n)\n"
  },
  {
    "tf.estimator.MultiHead": "tf.estimator.MultiHead(\n    heads, head_weights=None\n)\n"
  },
  {
    "tf.estimator.MultiLabelHead": "tf.estimator.MultiLabelHead(\n    n_classes,\n    weight_column=None,\n    thresholds=None,\n    label_vocabulary=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    loss_fn=None,\n    classes_for_class_based_metrics=None,\n    name=None\n)\n"
  },
  {
    "tf.estimator.NanLossDuringTrainingError": "tf.estimator.NanLossDuringTrainingError(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.estimator.NanTensorHook": "tf.estimator.NanTensorHook(\n    loss_tensor, fail_on_nan_loss=True\n)\n"
  },
  {
    "tf.estimator.PoissonRegressionHead": "tf.estimator.PoissonRegressionHead(\n    label_dimension=1,\n    weight_column=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    compute_full_loss=True,\n    name=None\n)\n"
  },
  {
    "tf.estimator.ProfilerHook": "tf.estimator.ProfilerHook(\n    save_steps=None,\n    save_secs=None,\n    output_dir='',\n    show_dataflow=True,\n    show_memory=False\n)\n"
  },
  {
    "tf.estimator.RegressionHead": "tf.estimator.RegressionHead(\n    label_dimension=1,\n    weight_column=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    loss_fn=None,\n    inverse_link_fn=None,\n    name=None\n)\n"
  },
  {
    "tf.estimator.RunConfig": "tf.estimator.RunConfig(\n    model_dir=None,\n    tf_random_seed=None,\n    save_summary_steps=100,\n    save_checkpoints_steps=_USE_DEFAULT,\n    save_checkpoints_secs=_USE_DEFAULT,\n    session_config=None,\n    keep_checkpoint_max=5,\n    keep_checkpoint_every_n_hours=10000,\n    log_step_count_steps=100,\n    train_distribute=None,\n    device_fn=None,\n    protocol=None,\n    eval_distribute=None,\n    experimental_distribute=None,\n    experimental_max_worker_delay_secs=None,\n    session_creation_timeout_secs=7200,\n    checkpoint_save_graph_def=True\n)\n"
  },
  {
    "tf.estimator.SecondOrStepTimer": "tf.estimator.SecondOrStepTimer(\n    every_secs=None, every_steps=None\n)\n"
  },
  {
    "tf.estimator.SessionRunArgs": "tf.estimator.SessionRunArgs(\n    fetches, feed_dict=None, options=None\n)\n"
  },
  {
    "tf.estimator.SessionRunContext": "tf.estimator.SessionRunContext(\n    original_args, session\n)\n"
  },
  {
    "tf.estimator.SessionRunValues": "tf.estimator.SessionRunValues(\n    results, options, run_metadata\n)\n"
  },
  {
    "tf.estimator.StepCounterHook": "tf.estimator.StepCounterHook(\n    every_n_steps=100, every_n_secs=None, output_dir=None, summary_writer=None\n)\n"
  },
  {
    "tf.estimator.StopAtStepHook": "tf.estimator.StopAtStepHook(\n    num_steps=None, last_step=None\n)\n"
  },
  {
    "tf.estimator.SummarySaverHook": "tf.estimator.SummarySaverHook(\n    save_steps=None,\n    save_secs=None,\n    output_dir=None,\n    summary_writer=None,\n    scaffold=None,\n    summary_op=None\n)\n"
  },
  {
    "tf.estimator.TrainSpec": "tf.estimator.TrainSpec(\n    input_fn, max_steps=None, hooks=None, saving_listeners=None\n)\n"
  },
  {
    "tf.estimator.WarmStartSettings": "tf.estimator.WarmStartSettings(\n    ckpt_to_initialize_from,\n    vars_to_warm_start='.*',\n    var_name_to_vocab_info=None,\n    var_name_to_prev_var_name=None\n)\n"
  },
  {
    "tf.estimator.add_metrics": "tf.estimator.add_metrics(\n    estimator, metric_fn\n)\n"
  },
  {
    "tf.estimator.classifier_parse_example_spec": "tf.estimator.classifier_parse_example_spec(\n    feature_columns,\n    label_key,\n    label_dtype=tf.dtypes.int64,\n    label_default=None,\n    weight_column=None\n)\n"
  },
  {
    "tf.estimator.experimental.InMemoryEvaluatorHook": "tf.estimator.experimental.InMemoryEvaluatorHook(\n    estimator, input_fn, steps=None, hooks=None, name=None, every_n_iter=100\n)\n"
  },
  {
    "tf.estimator.experimental.LinearSDCA": "tf.estimator.experimental.LinearSDCA(\n    example_id_column,\n    num_loss_partitions=1,\n    num_table_shards=None,\n    symmetric_l1_regularization=0.0,\n    symmetric_l2_regularization=1.0,\n    adaptive=False\n)\n"
  },
  {
    "tf.estimator.experimental.RNNClassifier": "tf.estimator.experimental.RNNClassifier(\n    sequence_feature_columns,\n    context_feature_columns=None,\n    units=None,\n    cell_type=USE_DEFAULT,\n    rnn_cell_fn=None,\n    return_sequences=False,\n    model_dir=None,\n    n_classes=2,\n    weight_column=None,\n    label_vocabulary=None,\n    optimizer='Adagrad',\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    sequence_mask='sequence_mask',\n    config=None\n)\n"
  },
  {
    "tf.estimator.experimental.RNNEstimator": "tf.estimator.experimental.RNNEstimator(\n    head,\n    sequence_feature_columns,\n    context_feature_columns=None,\n    units=None,\n    cell_type=USE_DEFAULT,\n    rnn_cell_fn=None,\n    return_sequences=False,\n    model_dir=None,\n    optimizer='Adagrad',\n    config=None\n)\n"
  },
  {
    "tf.estimator.experimental.build_raw_supervised_input_receiver_fn": "tf.estimator.experimental.build_raw_supervised_input_receiver_fn(\n    features, labels, default_batch_size=None\n)\n"
  },
  {
    "tf.estimator.experimental.call_logit_fn": "tf.estimator.experimental.call_logit_fn(\n    logit_fn, features, mode, params, config\n)\n"
  },
  {
    "tf.estimator.experimental.make_early_stopping_hook": "tf.estimator.experimental.make_early_stopping_hook(\n    estimator, should_stop_fn, run_every_secs=60, run_every_steps=None\n)\n"
  },
  {
    "tf.estimator.experimental.make_stop_at_checkpoint_step_hook": "tf.estimator.experimental.make_stop_at_checkpoint_step_hook(\n    estimator, last_step, wait_after_file_check_secs=30\n)\n"
  },
  {
    "tf.estimator.experimental.stop_if_higher_hook": "tf.estimator.experimental.stop_if_higher_hook(\n    estimator,\n    metric_name,\n    threshold,\n    eval_dir=None,\n    min_steps=0,\n    run_every_secs=60,\n    run_every_steps=None\n)\n"
  },
  {
    "tf.estimator.experimental.stop_if_lower_hook": "tf.estimator.experimental.stop_if_lower_hook(\n    estimator,\n    metric_name,\n    threshold,\n    eval_dir=None,\n    min_steps=0,\n    run_every_secs=60,\n    run_every_steps=None\n)\n"
  },
  {
    "tf.estimator.experimental.stop_if_no_decrease_hook": "tf.estimator.experimental.stop_if_no_decrease_hook(\n    estimator,\n    metric_name,\n    max_steps_without_decrease,\n    eval_dir=None,\n    min_steps=0,\n    run_every_secs=60,\n    run_every_steps=None\n)\n"
  },
  {
    "tf.estimator.experimental.stop_if_no_increase_hook": "tf.estimator.experimental.stop_if_no_increase_hook(\n    estimator,\n    metric_name,\n    max_steps_without_increase,\n    eval_dir=None,\n    min_steps=0,\n    run_every_secs=60,\n    run_every_steps=None\n)\n"
  },
  {
    "tf.estimator.export.ClassificationOutput": "tf.estimator.export.ClassificationOutput(\n    scores=None, classes=None\n)\n"
  },
  {
    "tf.estimator.export.EvalOutput": "tf.estimator.export.EvalOutput(\n    loss=None, predictions=None, metrics=None\n)\n"
  },
  {
    "tf.estimator.export.PredictOutput": "tf.estimator.export.PredictOutput(\n    outputs\n)\n"
  },
  {
    "tf.estimator.export.RegressionOutput": "tf.estimator.export.RegressionOutput(\n    value\n)\n"
  },
  {
    "tf.estimator.export.ServingInputReceiver": "tf.estimator.export.ServingInputReceiver(\n    features, receiver_tensors, receiver_tensors_alternatives=None\n)\n"
  },
  {
    "tf.estimator.export.TensorServingInputReceiver": "tf.estimator.export.TensorServingInputReceiver(\n    features, receiver_tensors, receiver_tensors_alternatives=None\n)\n"
  },
  {
    "tf.estimator.export.build_parsing_serving_input_receiver_fn": "tf.estimator.export.build_parsing_serving_input_receiver_fn(\n    feature_spec, default_batch_size=None\n)\n"
  },
  {
    "tf.estimator.export.build_raw_serving_input_receiver_fn": "tf.estimator.export.build_raw_serving_input_receiver_fn(\n    features, default_batch_size=None\n)\n"
  },
  {
    "tf.estimator.regressor_parse_example_spec": "tf.estimator.regressor_parse_example_spec(\n    feature_columns,\n    label_key,\n    label_dtype=tf.dtypes.float32,\n    label_default=None,\n    label_dimension=1,\n    weight_column=None\n)\n"
  },
  {
    "tf.estimator.train_and_evaluate": "tf.estimator.train_and_evaluate(\n    estimator, train_spec, eval_spec\n)\n"
  },
  {
    "tf.math.exp": "tf.math.exp(\n    x, name=None\n)\n"
  },
  {
    "tf.expand_dims": "tf.expand_dims(\n    input, axis, name=None\n)\n"
  },
  {
    "tf.experimental.BatchableExtensionType": "tf.experimental.BatchableExtensionType(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.DynamicRaggedShape": "tf.experimental.DynamicRaggedShape(\n    row_partitions: Sequence[tf.experimental.RowPartition],\n    inner_shape: tf.types.experimental.TensorLike,\n    dtype: Optional[tf.dtypes.DType] = None,\n    validate: bool = False,\n    static_inner_shape: ... = None\n)\n"
  },
  {
    "tf.experimental.DynamicRaggedShape.Spec": "tf.experimental.DynamicRaggedShape.Spec(\n    row_partitions: Tuple[RowPartitionSpec, ...],\n    static_inner_shape: tf.TensorShape,\n    dtype: tf.dtypes.DType\n)\n"
  },
  {
    "tf.experimental.ExtensionType": "tf.experimental.ExtensionType(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.RowPartition": "tf.experimental.RowPartition(\n    row_splits,\n    row_lengths=None,\n    value_rowids=None,\n    nrows=None,\n    uniform_row_length=None,\n    nvals=None,\n    internal=False\n)\n"
  },
  {
    "tf.experimental.StructuredTensor": "tf.experimental.StructuredTensor(\n    fields: Mapping[str, _FieldValue],\n    ragged_shape: tf.experimental.DynamicRaggedShape\n)\n"
  },
  {
    "tf.experimental.StructuredTensor": "tf.experimental.StructuredTensor(\n    fields: Mapping[str, _FieldValue],\n    ragged_shape: tf.experimental.DynamicRaggedShape\n)\n"
  },
  {
    "tf.experimental.StructuredTensor.Spec": "tf.experimental.StructuredTensor.Spec(\n    _fields, _ragged_shape\n)\n"
  },
  {
    "tf.experimental.dispatch_for_api": "tf.experimental.dispatch_for_api(\n    api, *signatures\n)\n"
  },
  {
    "tf.experimental.dispatch_for_binary_elementwise_apis": "tf.experimental.dispatch_for_binary_elementwise_apis(\n    x_type, y_type\n)\n"
  },
  {
    "tf.experimental.dispatch_for_binary_elementwise_assert_apis": "tf.experimental.dispatch_for_binary_elementwise_assert_apis(\n    x_type, y_type\n)\n"
  },
  {
    "tf.experimental.dispatch_for_unary_elementwise_apis": "tf.experimental.dispatch_for_unary_elementwise_apis(\n    x_type\n)\n"
  },
  {
    "tf.experimental.dlpack.from_dlpack": "tf.experimental.dlpack.from_dlpack(\n    dlcapsule\n)\n"
  },
  {
    "tf.experimental.dlpack.to_dlpack": "tf.experimental.dlpack.to_dlpack(\n    tf_tensor\n)\n"
  },
  {
    "tf.experimental.dtensor.DTensorCheckpoint": "tf.experimental.dtensor.DTensorCheckpoint(\n    mesh: tf.experimental.dtensor.Mesh,\n    root=None,\n    **kwargs\n)\n"
  },
  {
    "tf.experimental.dtensor.DTensorDataset": "tf.experimental.dtensor.DTensorDataset(\n    dataset: tf.data.Dataset,\n    *,\n    mesh: tf.experimental.dtensor.Mesh,\n    layouts: Any,\n    global_batch_size: int,\n    dataset_already_batched: bool = False,\n    batch_dim: Optional[str] = None,\n    prefetch: Optional[int] = None,\n    tf_data_service_config: Optional[TFDataServiceConfig] = None\n)\n"
  },
  {
    "tf.experimental.dtensor.DVariable": "tf.experimental.dtensor.DVariable(\n    initial_value, *args, dtype=None, **kwargs\n)\n"
  },
  {
    "tf.Variable.SaveSliceInfo": "tf.Variable.SaveSliceInfo(\n    full_name=None,\n    full_shape=None,\n    var_offset=None,\n    var_shape=None,\n    save_slice_info_def=None,\n    import_scope=None\n)\n"
  },
  {
    "tf.experimental.dtensor.Layout": "tf.experimental.dtensor.Layout(\n    sharding_specs: List[str],\n    mesh: tf.experimental.dtensor.Mesh\n)\n"
  },
  {
    "tf.experimental.dtensor.Mesh": "tf.experimental.dtensor.Mesh(\n    dim_names: List[str],\n    global_device_ids: np.ndarray,\n    local_device_ids: List[int],\n    local_devices: List[tf.compat.v1.DeviceSpec],\n    mesh_name: str = '',\n    global_devices: Optional[List[tf_device.DeviceSpec]] = None\n)\n"
  },
  {
    "tf.experimental.dtensor.barrier": "tf.experimental.dtensor.barrier(\n    mesh: tf.experimental.dtensor.Mesh,\n    barrier_name: Optional[str] = None\n)\n"
  },
  {
    "tf.experimental.dtensor.call_with_layout": "tf.experimental.dtensor.call_with_layout(\n    fn: Callable[..., Any],\n    layout: Optional[tf.experimental.dtensor.Layout],\n    *args,\n    **kwargs\n) -> Any\n"
  },
  {
    "tf.experimental.dtensor.check_layout": "tf.experimental.dtensor.check_layout(\n    tensor: tf.Tensor,\n    layout: tf.experimental.dtensor.Layout\n) -> None\n"
  },
  {
    "tf.experimental.dtensor.client_id": "tf.experimental.dtensor.client_id() -> int\n"
  },
  {
    "tf.experimental.dtensor.copy_to_mesh": "tf.experimental.dtensor.copy_to_mesh(\n    tensor: Any,\n    layout: tf.experimental.dtensor.Layout,\n    source_layout: Optional[tf.experimental.dtensor.Layout] = None\n) -> tf.Tensor\n"
  },
  {
    "tf.experimental.dtensor.create_distributed_mesh": "tf.experimental.dtensor.create_distributed_mesh(\n    mesh_dims: List[Tuple[str, int]],\n    mesh_name: str = '',\n    local_devices: Optional[List[str]] = None,\n    device_type: Optional[str] = None\n) -> tf.experimental.dtensor.Mesh\n"
  },
  {
    "tf.experimental.dtensor.create_mesh": "tf.experimental.dtensor.create_mesh(\n    mesh_dims: Optional[List[Tuple[str, int]]] = None,\n    mesh_name: str = '',\n    devices: Optional[List[str]] = None,\n    device_type: Optional[str] = None\n) -> tf.experimental.dtensor.Mesh\n"
  },
  {
    "tf.experimental.dtensor.create_tpu_mesh": "tf.experimental.dtensor.create_tpu_mesh(\n    mesh_dim_names: List[str],\n    mesh_shape: List[int],\n    mesh_name: str,\n    ring_dims: Optional[int] = None,\n    ring_axes: Optional[List[str]] = None,\n    ring_bounds: Optional[List[int]] = None,\n    can_split_host_across_rings: bool = True,\n    build_ring_across_rings: bool = False,\n    rotate_ring_across_rings: bool = False\n) -> tf.experimental.dtensor.Mesh\n"
  },
  {
    "tf.experimental.dtensor.device_name": "tf.experimental.dtensor.device_name() -> str\n"
  },
  {
    "tf.experimental.dtensor.enable_save_as_bf16": "tf.experimental.dtensor.enable_save_as_bf16(\n    variables: List[tf.Variable]\n)\n"
  },
  {
    "tf.experimental.dtensor.fetch_layout": "tf.experimental.dtensor.fetch_layout(\n    tensor: tf.Tensor\n) -> tf.experimental.dtensor.Layout\n"
  },
  {
    "tf.experimental.dtensor.full_job_name": "tf.experimental.dtensor.full_job_name(\n    task_id: Optional[int] = None\n) -> str\n"
  },
  {
    "tf.experimental.dtensor.heartbeat_enabled": "tf.experimental.dtensor.heartbeat_enabled() -> bool\n"
  },
  {
    "tf.experimental.dtensor.initialize_accelerator_system": "tf.experimental.dtensor.initialize_accelerator_system(\n    device_type: Optional[str] = None,\n    enable_coordination_service: Optional[bool] = False\n) -> str\n"
  },
  {
    "tf.experimental.dtensor.initialize_accelerator_system": "tf.experimental.dtensor.initialize_accelerator_system(\n    device_type: Optional[str] = None,\n    enable_coordination_service: Optional[bool] = False\n) -> str\n"
  },
  {
    "tf.experimental.dtensor.initialize_accelerator_system": "tf.experimental.dtensor.initialize_accelerator_system(\n    device_type: Optional[str] = None,\n    enable_coordination_service: Optional[bool] = False\n) -> str\n"
  },
  {
    "tf.experimental.dtensor.job_name": "tf.experimental.dtensor.job_name() -> str\n"
  },
  {
    "tf.experimental.dtensor.jobs": "tf.experimental.dtensor.jobs() -> List[str]\n"
  },
  {
    "tf.experimental.dtensor.local_devices": "tf.experimental.dtensor.local_devices(\n    device_type: str, for_client_id: Optional[int] = None\n) -> List[tf.compat.v1.DeviceSpec]\n"
  },
  {
    "tf.experimental.dtensor.name_based_restore": "tf.experimental.dtensor.name_based_restore(\n    mesh: tf.experimental.dtensor.Mesh,\n    checkpoint_prefix: str,\n    name_tensor_dict: Dict[str, Union[ops.Tensor, tf_variables.Variable]]\n)\n"
  },
  {
    "tf.experimental.dtensor.name_based_save": "tf.experimental.dtensor.name_based_save(\n    mesh: tf.experimental.dtensor.Mesh,\n    checkpoint_prefix: Union[str, tf.Tensor],\n    name_tensor_dict: Dict[str, Union[ops.Tensor, tf_variables.Variable]]\n)\n"
  },
  {
    "tf.experimental.dtensor.num_clients": "tf.experimental.dtensor.num_clients() -> int\n"
  },
  {
    "tf.experimental.dtensor.num_global_devices": "tf.experimental.dtensor.num_global_devices(\n    device_type: str\n) -> int\n"
  },
  {
    "tf.experimental.dtensor.num_local_devices": "tf.experimental.dtensor.num_local_devices(\n    device_type: str\n) -> int\n"
  },
  {
    "tf.experimental.dtensor.pack": "tf.experimental.dtensor.pack(\n    tensors: Sequence[Any],\n    layout: tf.experimental.dtensor.Layout\n) -> Any\n"
  },
  {
    "tf.experimental.dtensor.preferred_device_type": "tf.experimental.dtensor.preferred_device_type() -> str\n"
  },
  {
    "tf.experimental.dtensor.relayout": "tf.experimental.dtensor.relayout(\n    tensor: tf.Tensor,\n    layout: tf.experimental.dtensor.Layout\n) -> tf.Tensor\n"
  },
  {
    "tf.experimental.dtensor.sharded_save": "tf.experimental.dtensor.sharded_save(\n    mesh: tf.experimental.dtensor.Mesh,\n    file_prefix: Union[str, tf.Tensor],\n    tensor_names: Union[List[str], tf.Tensor],\n    shape_and_slices: Union[List[str], tf.Tensor],\n    tensors: List[Union[ops.Tensor, tf_variables.Variable]]\n)\n"
  },
  {
    "tf.experimental.dtensor.shutdown_accelerator_system": "tf.experimental.dtensor.shutdown_accelerator_system() -> None\n"
  },
  {
    "tf.experimental.dtensor.shutdown_accelerator_system": "tf.experimental.dtensor.shutdown_accelerator_system() -> None\n"
  },
  {
    "tf.experimental.dtensor.unpack": "tf.experimental.dtensor.unpack(\n    tensor: Any\n) -> Sequence[Any]\n"
  },
  {
    "tf.experimental.numpy.abs": "tf.experimental.numpy.abs(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.absolute": "tf.experimental.numpy.absolute(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.add": "tf.experimental.numpy.add(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.all": "tf.experimental.numpy.all(\n    a, axis=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.allclose": "tf.experimental.numpy.allclose(\n    a, b, rtol=1e-05, atol=1e-08, equal_nan=False\n)\n"
  },
  {
    "tf.experimental.numpy.amax": "tf.experimental.numpy.amax(\n    a, axis=None, out=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.amin": "tf.experimental.numpy.amin(\n    a, axis=None, out=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.angle": "tf.experimental.numpy.angle(\n    z, deg=False\n)\n"
  },
  {
    "tf.experimental.numpy.any": "tf.experimental.numpy.any(\n    a, axis=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.append": "tf.experimental.numpy.append(\n    arr, values, axis=None\n)\n"
  },
  {
    "tf.experimental.numpy.arange": "tf.experimental.numpy.arange(\n    start, stop=None, step=1, dtype=None\n)\n"
  },
  {
    "tf.experimental.numpy.arccos": "tf.experimental.numpy.arccos(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.arccosh": "tf.experimental.numpy.arccosh(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.arcsin": "tf.experimental.numpy.arcsin(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.arcsinh": "tf.experimental.numpy.arcsinh(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.arctan": "tf.experimental.numpy.arctan(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.arctan2": "tf.experimental.numpy.arctan2(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.arctanh": "tf.experimental.numpy.arctanh(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.argmax": "tf.experimental.numpy.argmax(\n    a, axis=None\n)\n"
  },
  {
    "tf.experimental.numpy.argmin": "tf.experimental.numpy.argmin(\n    a, axis=None\n)\n"
  },
  {
    "tf.experimental.numpy.argsort": "tf.experimental.numpy.argsort(\n    a, axis=-1, kind='quicksort', order=None\n)\n"
  },
  {
    "tf.experimental.numpy.around": "tf.experimental.numpy.around(\n    a, decimals=0\n)\n"
  },
  {
    "tf.experimental.numpy.array": "tf.experimental.numpy.array(\n    val, dtype=None, copy=True, ndmin=0\n)\n"
  },
  {
    "tf.experimental.numpy.array_equal": "tf.experimental.numpy.array_equal(\n    a1, a2\n)\n"
  },
  {
    "tf.experimental.numpy.asanyarray": "tf.experimental.numpy.asanyarray(\n    a, dtype=None\n)\n"
  },
  {
    "tf.experimental.numpy.asarray": "tf.experimental.numpy.asarray(\n    a, dtype=None\n)\n"
  },
  {
    "tf.experimental.numpy.ascontiguousarray": "tf.experimental.numpy.ascontiguousarray(\n    a, dtype=None\n)\n"
  },
  {
    "tf.experimental.numpy.atleast_1d": "tf.experimental.numpy.atleast_1d(\n    *arys\n)\n"
  },
  {
    "tf.experimental.numpy.atleast_2d": "tf.experimental.numpy.atleast_2d(\n    *arys\n)\n"
  },
  {
    "tf.experimental.numpy.atleast_3d": "tf.experimental.numpy.atleast_3d(\n    *arys\n)\n"
  },
  {
    "tf.experimental.numpy.average": "tf.experimental.numpy.average(\n    a, axis=None, weights=None, returned=False\n)\n"
  },
  {
    "tf.experimental.numpy.bitwise_and": "tf.experimental.numpy.bitwise_and(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.bitwise_not": "tf.experimental.numpy.bitwise_not(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.bitwise_or": "tf.experimental.numpy.bitwise_or(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.bitwise_xor": "tf.experimental.numpy.bitwise_xor(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.bool_": "tf.experimental.numpy.bool_(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.broadcast_arrays": "tf.experimental.numpy.broadcast_arrays(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.broadcast_to": "tf.experimental.numpy.broadcast_to(\n    array, shape\n)\n"
  },
  {
    "tf.experimental.numpy.cbrt": "tf.experimental.numpy.cbrt(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.ceil": "tf.experimental.numpy.ceil(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.clip": "tf.experimental.numpy.clip(\n    a, a_min, a_max\n)\n"
  },
  {
    "tf.experimental.numpy.complex128": "tf.experimental.numpy.complex128(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.complex64": "tf.experimental.numpy.complex64(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.complex128": "tf.experimental.numpy.complex128(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.compress": "tf.experimental.numpy.compress(\n    condition, a, axis=None\n)\n"
  },
  {
    "tf.experimental.numpy.concatenate": "tf.experimental.numpy.concatenate(\n    arys, axis=0\n)\n"
  },
  {
    "tf.experimental.numpy.conj": "tf.experimental.numpy.conj(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.conjugate": "tf.experimental.numpy.conjugate(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.copy": "tf.experimental.numpy.copy(\n    a\n)\n"
  },
  {
    "tf.experimental.numpy.cos": "tf.experimental.numpy.cos(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.cosh": "tf.experimental.numpy.cosh(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.count_nonzero": "tf.experimental.numpy.count_nonzero(\n    a, axis=None\n)\n"
  },
  {
    "tf.experimental.numpy.cross": "tf.experimental.numpy.cross(\n    a, b, axisa=-1, axisb=-1, axisc=-1, axis=None\n)\n"
  },
  {
    "tf.experimental.numpy.cumprod": "tf.experimental.numpy.cumprod(\n    a, axis=None, dtype=None\n)\n"
  },
  {
    "tf.experimental.numpy.cumsum": "tf.experimental.numpy.cumsum(\n    a, axis=None, dtype=None\n)\n"
  },
  {
    "tf.experimental.numpy.deg2rad": "tf.experimental.numpy.deg2rad(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.diag": "tf.experimental.numpy.diag(\n    v, k=0\n)\n"
  },
  {
    "tf.experimental.numpy.diag_indices": "tf.experimental.numpy.diag_indices(\n    n, ndim=2\n)\n"
  },
  {
    "tf.experimental.numpy.diagflat": "tf.experimental.numpy.diagflat(\n    v, k=0\n)\n"
  },
  {
    "tf.experimental.numpy.diagonal": "tf.experimental.numpy.diagonal(\n    a, offset=0, axis1=0, axis2=1\n)\n"
  },
  {
    "tf.experimental.numpy.diff": "tf.experimental.numpy.diff(\n    a, n=1, axis=-1\n)\n"
  },
  {
    "tf.experimental.numpy.divide": "tf.experimental.numpy.divide(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.divmod": "tf.experimental.numpy.divmod(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.dot": "tf.experimental.numpy.dot(\n    a, b\n)\n"
  },
  {
    "tf.experimental.numpy.dsplit": "tf.experimental.numpy.dsplit(\n    ary, indices_or_sections\n)\n"
  },
  {
    "tf.experimental.numpy.dstack": "tf.experimental.numpy.dstack(\n    tup\n)\n"
  },
  {
    "tf.experimental.numpy.einsum": "tf.experimental.numpy.einsum(\n    subscripts, *operands, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.empty": "tf.experimental.numpy.empty(\n    shape, dtype=float\n)\n"
  },
  {
    "tf.experimental.numpy.empty_like": "tf.experimental.numpy.empty_like(\n    a, dtype=None\n)\n"
  },
  {
    "tf.experimental.numpy.equal": "tf.experimental.numpy.equal(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.exp": "tf.experimental.numpy.exp(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.exp2": "tf.experimental.numpy.exp2(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.expand_dims": "tf.experimental.numpy.expand_dims(\n    a, axis\n)\n"
  },
  {
    "tf.experimental.numpy.experimental_enable_numpy_behavior": "tf.experimental.numpy.experimental_enable_numpy_behavior(\n    prefer_float32=False\n)\n"
  },
  {
    "tf.experimental.numpy.expm1": "tf.experimental.numpy.expm1(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.eye": "tf.experimental.numpy.eye(\n    N, M=None, k=0, dtype=float\n)\n"
  },
  {
    "tf.experimental.numpy.fabs": "tf.experimental.numpy.fabs(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.finfo": "tf.experimental.numpy.finfo(\n    dtype\n)\n"
  },
  {
    "tf.experimental.numpy.fix": "tf.experimental.numpy.fix(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.flip": "tf.experimental.numpy.flip(\n    m, axis=None\n)\n"
  },
  {
    "tf.experimental.numpy.fliplr": "tf.experimental.numpy.fliplr(\n    m\n)\n"
  },
  {
    "tf.experimental.numpy.flipud": "tf.experimental.numpy.flipud(\n    m\n)\n"
  },
  {
    "tf.experimental.numpy.float16": "tf.experimental.numpy.float16(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.float32": "tf.experimental.numpy.float32(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.float64": "tf.experimental.numpy.float64(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.float64": "tf.experimental.numpy.float64(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.float_power": "tf.experimental.numpy.float_power(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.floor": "tf.experimental.numpy.floor(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.floor_divide": "tf.experimental.numpy.floor_divide(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.full": "tf.experimental.numpy.full(\n    shape, fill_value, dtype=None\n)\n"
  },
  {
    "tf.experimental.numpy.full_like": "tf.experimental.numpy.full_like(\n    a, fill_value, dtype=None, order='K', subok=True, shape=None\n)\n"
  },
  {
    "tf.experimental.numpy.gcd": "tf.experimental.numpy.gcd(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.geomspace": "tf.experimental.numpy.geomspace(\n    start, stop, num=50, endpoint=True, dtype=None, axis=0\n)\n"
  },
  {
    "tf.experimental.numpy.greater": "tf.experimental.numpy.greater(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.greater_equal": "tf.experimental.numpy.greater_equal(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.heaviside": "tf.experimental.numpy.heaviside(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.hsplit": "tf.experimental.numpy.hsplit(\n    ary, indices_or_sections\n)\n"
  },
  {
    "tf.experimental.numpy.hstack": "tf.experimental.numpy.hstack(\n    tup\n)\n"
  },
  {
    "tf.experimental.numpy.hypot": "tf.experimental.numpy.hypot(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.identity": "tf.experimental.numpy.identity(\n    n, dtype=float\n)\n"
  },
  {
    "tf.experimental.numpy.iinfo": "tf.experimental.numpy.iinfo(\n    int_type\n)\n"
  },
  {
    "tf.experimental.numpy.imag": "tf.experimental.numpy.imag(\n    val\n)\n"
  },
  {
    "tf.experimental.numpy.inner": "tf.experimental.numpy.inner(\n    a, b\n)\n"
  },
  {
    "tf.experimental.numpy.int16": "tf.experimental.numpy.int16(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.int32": "tf.experimental.numpy.int32(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.int64": "tf.experimental.numpy.int64(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.int8": "tf.experimental.numpy.int8(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.int64": "tf.experimental.numpy.int64(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.isclose": "tf.experimental.numpy.isclose(\n    a, b, rtol=1e-05, atol=1e-08, equal_nan=False\n)\n"
  },
  {
    "tf.experimental.numpy.iscomplex": "tf.experimental.numpy.iscomplex(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.iscomplexobj": "tf.experimental.numpy.iscomplexobj(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.isfinite": "tf.experimental.numpy.isfinite(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.isinf": "tf.experimental.numpy.isinf(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.isnan": "tf.experimental.numpy.isnan(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.isneginf": "tf.experimental.numpy.isneginf(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.isposinf": "tf.experimental.numpy.isposinf(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.isreal": "tf.experimental.numpy.isreal(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.isrealobj": "tf.experimental.numpy.isrealobj(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.isscalar": "tf.experimental.numpy.isscalar(\n    num\n)\n"
  },
  {
    "tf.experimental.numpy.issubdtype": "tf.experimental.numpy.issubdtype(\n    arg1, arg2\n)\n"
  },
  {
    "tf.experimental.numpy.ix_": "tf.experimental.numpy.ix_(\n    *args\n)\n"
  },
  {
    "tf.experimental.numpy.kron": "tf.experimental.numpy.kron(\n    a, b\n)\n"
  },
  {
    "tf.experimental.numpy.lcm": "tf.experimental.numpy.lcm(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.less": "tf.experimental.numpy.less(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.less_equal": "tf.experimental.numpy.less_equal(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.linspace": "tf.experimental.numpy.linspace(\n    start, stop, num=50, endpoint=True, retstep=False, dtype=float, axis=0\n)\n"
  },
  {
    "tf.experimental.numpy.log": "tf.experimental.numpy.log(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.log10": "tf.experimental.numpy.log10(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.log1p": "tf.experimental.numpy.log1p(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.log2": "tf.experimental.numpy.log2(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.logaddexp": "tf.experimental.numpy.logaddexp(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.logaddexp2": "tf.experimental.numpy.logaddexp2(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.logical_and": "tf.experimental.numpy.logical_and(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.logical_not": "tf.experimental.numpy.logical_not(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.logical_or": "tf.experimental.numpy.logical_or(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.logical_xor": "tf.experimental.numpy.logical_xor(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.logspace": "tf.experimental.numpy.logspace(\n    start, stop, num=50, endpoint=True, base=10.0, dtype=None, axis=0\n)\n"
  },
  {
    "tf.experimental.numpy.matmul": "tf.experimental.numpy.matmul(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.max": "tf.experimental.numpy.max(\n    a, axis=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.maximum": "tf.experimental.numpy.maximum(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.mean": "tf.experimental.numpy.mean(\n    a, axis=None, dtype=None, out=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.meshgrid": "tf.experimental.numpy.meshgrid(\n    *xi, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.min": "tf.experimental.numpy.min(\n    a, axis=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.minimum": "tf.experimental.numpy.minimum(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.mod": "tf.experimental.numpy.mod(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.moveaxis": "tf.experimental.numpy.moveaxis(\n    a, source, destination\n)\n"
  },
  {
    "tf.experimental.numpy.multiply": "tf.experimental.numpy.multiply(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.nanmean": "tf.experimental.numpy.nanmean(\n    a, axis=None, dtype=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.nanprod": "tf.experimental.numpy.nanprod(\n    a, axis=None, dtype=None, keepdims=False\n)\n"
  },
  {
    "tf.experimental.numpy.nansum": "tf.experimental.numpy.nansum(\n    a, axis=None, dtype=None, keepdims=False\n)\n"
  },
  {
    "tf.Tensor": "tf.Tensor(\n    op, value_index, dtype\n)\n"
  },
  {
    "tf.experimental.numpy.ndim": "tf.experimental.numpy.ndim(\n    a\n)\n"
  },
  {
    "tf.experimental.numpy.negative": "tf.experimental.numpy.negative(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.nextafter": "tf.experimental.numpy.nextafter(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.nonzero": "tf.experimental.numpy.nonzero(\n    a\n)\n"
  },
  {
    "tf.experimental.numpy.not_equal": "tf.experimental.numpy.not_equal(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.object_": "tf.experimental.numpy.object_(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.ones": "tf.experimental.numpy.ones(\n    shape, dtype=float\n)\n"
  },
  {
    "tf.experimental.numpy.ones_like": "tf.experimental.numpy.ones_like(\n    a, dtype=None\n)\n"
  },
  {
    "tf.experimental.numpy.outer": "tf.experimental.numpy.outer(\n    a, b\n)\n"
  },
  {
    "tf.experimental.numpy.pad": "tf.experimental.numpy.pad(\n    array, pad_width, mode, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.polyval": "tf.experimental.numpy.polyval(\n    p, x\n)\n"
  },
  {
    "tf.experimental.numpy.positive": "tf.experimental.numpy.positive(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.power": "tf.experimental.numpy.power(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.prod": "tf.experimental.numpy.prod(\n    a, axis=None, dtype=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.promote_types": "tf.experimental.numpy.promote_types(\n    type1, type2\n)\n"
  },
  {
    "tf.experimental.numpy.ptp": "tf.experimental.numpy.ptp(\n    a, axis=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.rad2deg": "tf.experimental.numpy.rad2deg(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.random.poisson": "tf.experimental.numpy.random.poisson(\n    lam=1.0, size=None\n)\n"
  },
  {
    "tf.experimental.numpy.random.rand": "tf.experimental.numpy.random.rand(\n    *size\n)\n"
  },
  {
    "tf.experimental.numpy.random.randint": "tf.experimental.numpy.random.randint(\n    low,\n    high=None,\n    size=None,\n    dtype=tf.experimental.numpy.int64\n)\n"
  },
  {
    "tf.experimental.numpy.random.randn": "tf.experimental.numpy.random.randn(\n    *args\n)\n"
  },
  {
    "tf.experimental.numpy.random.random": "tf.experimental.numpy.random.random(\n    size=None\n)\n"
  },
  {
    "tf.experimental.numpy.random.seed": "tf.experimental.numpy.random.seed(\n    s\n)\n"
  },
  {
    "tf.experimental.numpy.random.standard_normal": "tf.experimental.numpy.random.standard_normal(\n    size=None\n)\n"
  },
  {
    "tf.experimental.numpy.random.uniform": "tf.experimental.numpy.random.uniform(\n    low=0.0, high=1.0, size=None\n)\n"
  },
  {
    "tf.experimental.numpy.ravel": "tf.experimental.numpy.ravel(\n    a\n)\n"
  },
  {
    "tf.experimental.numpy.real": "tf.experimental.numpy.real(\n    val\n)\n"
  },
  {
    "tf.experimental.numpy.reciprocal": "tf.experimental.numpy.reciprocal(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.remainder": "tf.experimental.numpy.remainder(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.repeat": "tf.experimental.numpy.repeat(\n    a, repeats, axis=None\n)\n"
  },
  {
    "tf.experimental.numpy.reshape": "tf.experimental.numpy.reshape(\n    a, newshape, order='C'\n)\n"
  },
  {
    "tf.experimental.numpy.result_type": "tf.experimental.numpy.result_type(\n    *arrays_and_dtypes\n)\n"
  },
  {
    "tf.experimental.numpy.roll": "tf.experimental.numpy.roll(\n    a, shift, axis=None\n)\n"
  },
  {
    "tf.experimental.numpy.rot90": "tf.experimental.numpy.rot90(\n    m, k=1, axes=(0, 1)\n)\n"
  },
  {
    "tf.experimental.numpy.round": "tf.experimental.numpy.round(\n    a, decimals=0\n)\n"
  },
  {
    "tf.experimental.numpy.select": "tf.experimental.numpy.select(\n    condlist, choicelist, default=0\n)\n"
  },
  {
    "tf.experimental.numpy.shape": "tf.experimental.numpy.shape(\n    a\n)\n"
  },
  {
    "tf.experimental.numpy.sign": "tf.experimental.numpy.sign(\n    x, out=None, where=None, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.signbit": "tf.experimental.numpy.signbit(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.sin": "tf.experimental.numpy.sin(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.sinc": "tf.experimental.numpy.sinc(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.sinh": "tf.experimental.numpy.sinh(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.size": "tf.experimental.numpy.size(\n    x, axis=None\n)\n"
  },
  {
    "tf.experimental.numpy.sort": "tf.experimental.numpy.sort(\n    a, axis=-1, kind='quicksort', order=None\n)\n"
  },
  {
    "tf.experimental.numpy.split": "tf.experimental.numpy.split(\n    ary, indices_or_sections, axis=0\n)\n"
  },
  {
    "tf.experimental.numpy.sqrt": "tf.experimental.numpy.sqrt(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.square": "tf.experimental.numpy.square(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.squeeze": "tf.experimental.numpy.squeeze(\n    a, axis=None\n)\n"
  },
  {
    "tf.experimental.numpy.stack": "tf.experimental.numpy.stack(\n    arrays, axis=0\n)\n"
  },
  {
    "tf.experimental.numpy.std": "tf.experimental.numpy.std(\n    a, axis=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.string_": "tf.experimental.numpy.string_(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.subtract": "tf.experimental.numpy.subtract(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.sum": "tf.experimental.numpy.sum(\n    a, axis=None, dtype=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.swapaxes": "tf.experimental.numpy.swapaxes(\n    a, axis1, axis2\n)\n"
  },
  {
    "tf.experimental.numpy.take": "tf.experimental.numpy.take(\n    a, indices, axis=None, out=None, mode='clip'\n)\n"
  },
  {
    "tf.experimental.numpy.take_along_axis": "tf.experimental.numpy.take_along_axis(\n    arr, indices, axis\n)\n"
  },
  {
    "tf.experimental.numpy.tan": "tf.experimental.numpy.tan(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.tanh": "tf.experimental.numpy.tanh(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.tensordot": "tf.experimental.numpy.tensordot(\n    a, b, axes=2\n)\n"
  },
  {
    "tf.experimental.numpy.tile": "tf.experimental.numpy.tile(\n    a, reps\n)\n"
  },
  {
    "tf.experimental.numpy.trace": "tf.experimental.numpy.trace(\n    a, offset=0, axis1=0, axis2=1, dtype=None\n)\n"
  },
  {
    "tf.experimental.numpy.transpose": "tf.experimental.numpy.transpose(\n    a, axes=None\n)\n"
  },
  {
    "tf.experimental.numpy.tri": "tf.experimental.numpy.tri(\n    N, M=None, k=0, dtype=None\n)\n"
  },
  {
    "tf.experimental.numpy.tril": "tf.experimental.numpy.tril(\n    m, k=0\n)\n"
  },
  {
    "tf.experimental.numpy.triu": "tf.experimental.numpy.triu(\n    m, k=0\n)\n"
  },
  {
    "tf.experimental.numpy.true_divide": "tf.experimental.numpy.true_divide(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.uint16": "tf.experimental.numpy.uint16(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.uint32": "tf.experimental.numpy.uint32(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.uint64": "tf.experimental.numpy.uint64(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.uint8": "tf.experimental.numpy.uint8(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.unicode_": "tf.experimental.numpy.unicode_(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.vander": "tf.experimental.numpy.vander(\n    x, N=None, increasing=False\n)\n"
  },
  {
    "tf.experimental.numpy.var": "tf.experimental.numpy.var(\n    a, axis=None, dtype=None, out=None, ddof=0, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.vdot": "tf.experimental.numpy.vdot(\n    a, b\n)\n"
  },
  {
    "tf.experimental.numpy.vsplit": "tf.experimental.numpy.vsplit(\n    ary, indices_or_sections\n)\n"
  },
  {
    "tf.experimental.numpy.vstack": "tf.experimental.numpy.vstack(\n    tup\n)\n"
  },
  {
    "tf.experimental.numpy.where": "tf.experimental.numpy.where(\n    condition, x=None, y=None\n)\n"
  },
  {
    "tf.experimental.numpy.zeros": "tf.experimental.numpy.zeros(\n    shape, dtype=float\n)\n"
  },
  {
    "tf.experimental.numpy.zeros_like": "tf.experimental.numpy.zeros_like(\n    a, dtype=None\n)\n"
  },
  {
    "tf.experimental.register_filesystem_plugin": "tf.experimental.register_filesystem_plugin(\n    plugin_location\n)\n"
  },
  {
    "tf.experimental.tensorrt.ConversionParams": "tf.experimental.tensorrt.ConversionParams(\n    max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,\n    precision_mode=TrtPrecisionMode.FP32,\n    minimum_segment_size=3,\n    maximum_cached_engines=1,\n    use_calibration=True,\n    allow_build_at_runtime=True\n)\n"
  },
  {
    "tf.experimental.tensorrt.Converter": "tf.experimental.tensorrt.Converter(\n    input_saved_model_dir=None,\n    input_saved_model_tags=None,\n    input_saved_model_signature_key=None,\n    use_dynamic_shape=None,\n    dynamic_shape_profile_strategy=None,\n    max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,\n    precision_mode=TrtPrecisionMode.FP32,\n    minimum_segment_size=3,\n    maximum_cached_engines=1,\n    use_calibration=True,\n    allow_build_at_runtime=True,\n    conversion_params=None\n)\n"
  },
  {
    "tf.experimental.unregister_dispatch_for": "tf.experimental.unregister_dispatch_for(\n    dispatch_target\n)\n"
  },
  {
    "tf.extract_volume_patches": "tf.extract_volume_patches(\n    input, ksizes, strides, padding, name=None\n)\n"
  },
  {
    "tf.eye": "tf.eye(\n    num_rows,\n    num_columns=None,\n    batch_shape=None,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.feature_column.bucketized_column": "tf.feature_column.bucketized_column(\n    source_column, boundaries\n)\n"
  },
  {
    "tf.feature_column.categorical_column_with_hash_bucket": "tf.feature_column.categorical_column_with_hash_bucket(\n    key,\n    hash_bucket_size,\n    dtype=tf.dtypes.string\n)\n"
  },
  {
    "tf.feature_column.categorical_column_with_identity": "tf.feature_column.categorical_column_with_identity(\n    key, num_buckets, default_value=None\n)\n"
  },
  {
    "tf.feature_column.categorical_column_with_vocabulary_file": "tf.feature_column.categorical_column_with_vocabulary_file(\n    key,\n    vocabulary_file,\n    vocabulary_size=None,\n    dtype=tf.dtypes.string,\n    default_value=None,\n    num_oov_buckets=0,\n    file_format=None\n)\n"
  },
  {
    "tf.feature_column.categorical_column_with_vocabulary_list": "tf.feature_column.categorical_column_with_vocabulary_list(\n    key, vocabulary_list, dtype=None, default_value=-1, num_oov_buckets=0\n)\n"
  },
  {
    "tf.feature_column.crossed_column": "tf.feature_column.crossed_column(\n    keys, hash_bucket_size, hash_key=None\n)\n"
  },
  {
    "tf.feature_column.embedding_column": "tf.feature_column.embedding_column(\n    categorical_column,\n    dimension,\n    combiner='mean',\n    initializer=None,\n    ckpt_to_load_from=None,\n    tensor_name_in_ckpt=None,\n    max_norm=None,\n    trainable=True,\n    use_safe_embedding_lookup=True\n)\n"
  },
  {
    "tf.feature_column.indicator_column": "tf.feature_column.indicator_column(\n    categorical_column\n)\n"
  },
  {
    "tf.feature_column.make_parse_example_spec": "tf.feature_column.make_parse_example_spec(\n    feature_columns\n)\n"
  },
  {
    "tf.feature_column.numeric_column": "tf.feature_column.numeric_column(\n    key,\n    shape=(1,),\n    default_value=None,\n    dtype=tf.dtypes.float32,\n    normalizer_fn=None\n)\n"
  },
  {
    "tf.feature_column.sequence_categorical_column_with_hash_bucket": "tf.feature_column.sequence_categorical_column_with_hash_bucket(\n    key,\n    hash_bucket_size,\n    dtype=tf.dtypes.string\n)\n"
  },
  {
    "tf.feature_column.sequence_categorical_column_with_identity": "tf.feature_column.sequence_categorical_column_with_identity(\n    key, num_buckets, default_value=None\n)\n"
  },
  {
    "tf.feature_column.sequence_categorical_column_with_vocabulary_file": "tf.feature_column.sequence_categorical_column_with_vocabulary_file(\n    key,\n    vocabulary_file,\n    vocabulary_size=None,\n    num_oov_buckets=0,\n    default_value=None,\n    dtype=tf.dtypes.string\n)\n"
  },
  {
    "tf.feature_column.sequence_categorical_column_with_vocabulary_list": "tf.feature_column.sequence_categorical_column_with_vocabulary_list(\n    key, vocabulary_list, dtype=None, default_value=-1, num_oov_buckets=0\n)\n"
  },
  {
    "tf.feature_column.sequence_numeric_column": "tf.feature_column.sequence_numeric_column(\n    key,\n    shape=(1,),\n    default_value=0.0,\n    dtype=tf.dtypes.float32,\n    normalizer_fn=None\n)\n"
  },
  {
    "tf.feature_column.shared_embeddings": "tf.feature_column.shared_embeddings(\n    categorical_columns,\n    dimension,\n    combiner='mean',\n    initializer=None,\n    shared_embedding_collection_name=None,\n    ckpt_to_load_from=None,\n    tensor_name_in_ckpt=None,\n    max_norm=None,\n    trainable=True,\n    use_safe_embedding_lookup=True\n)\n"
  },
  {
    "tf.feature_column.weighted_categorical_column": "tf.feature_column.weighted_categorical_column(\n    categorical_column,\n    weight_feature_key,\n    dtype=tf.dtypes.float32\n)\n"
  },
  {
    "tf.fill": "tf.fill(\n    dims, value, name=None\n)\n"
  },
  {
    "tf.fingerprint": "tf.fingerprint(\n    data, method='farmhash64', name=None\n)\n"
  },
  {
    "tf.math.floor": "tf.math.floor(\n    x, name=None\n)\n"
  },
  {
    "tf.foldl": "tf.foldl(\n    fn,\n    elems,\n    initializer=None,\n    parallel_iterations=10,\n    back_prop=True,\n    swap_memory=False,\n    name=None\n)\n"
  },
  {
    "tf.foldr": "tf.foldr(\n    fn,\n    elems,\n    initializer=None,\n    parallel_iterations=10,\n    back_prop=True,\n    swap_memory=False,\n    name=None\n)\n"
  },
  {
    "tf.function": "tf.function(\n    func=None,\n    input_signature=None,\n    autograph=True,\n    jit_compile=None,\n    reduce_retracing=False,\n    experimental_implements=None,\n    experimental_autograph_options=None,\n    experimental_relax_shapes=None,\n    experimental_compile=None,\n    experimental_follow_type_hints=None\n) -> tf.types.experimental.GenericFunction\n"
  },
  {
    "tf.gather": "tf.gather(\n    params, indices, validate_indices=None, axis=None, batch_dims=0, name=None\n)\n"
  },
  {
    "tf.gather_nd": "tf.gather_nd(\n    params, indices, batch_dims=0, name=None\n)\n"
  },
  {
    "tf.get_static_value": "tf.get_static_value(\n    tensor, partial=False\n)\n"
  },
  {
    "tf.grad_pass_through": "tf.grad_pass_through(\n    f\n)\n"
  },
  {
    "tf.gradients": "tf.gradients(\n    ys,\n    xs,\n    grad_ys=None,\n    name='gradients',\n    gate_gradients=False,\n    aggregation_method=None,\n    stop_gradients=None,\n    unconnected_gradients=tf.UnconnectedGradients.NONE\n)\n"
  },
  {
    "tf.graph_util.import_graph_def": "tf.graph_util.import_graph_def(\n    graph_def,\n    input_map=None,\n    return_elements=None,\n    name=None,\n    op_dict=None,\n    producer_op_list=None\n)\n"
  },
  {
    "tf.math.greater": "tf.math.greater(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.greater_equal": "tf.math.greater_equal(\n    x, y, name=None\n)\n"
  },
  {
    "tf.group": "tf.group(\n    *inputs, **kwargs\n)\n"
  },
  {
    "tf.guarantee_const": "tf.guarantee_const(\n    input, name=None\n)\n"
  },
  {
    "tf.hessians": "tf.hessians(\n    ys,\n    xs,\n    gate_gradients=False,\n    aggregation_method=None,\n    name='hessians'\n)\n"
  },
  {
    "tf.histogram_fixed_width": "tf.histogram_fixed_width(\n    values,\n    value_range,\n    nbins=100,\n    dtype=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.histogram_fixed_width_bins": "tf.histogram_fixed_width_bins(\n    values,\n    value_range,\n    nbins=100,\n    dtype=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.identity": "tf.identity(\n    input, name=None\n)\n"
  },
  {
    "tf.identity_n": "tf.identity_n(\n    input, name=None\n)\n"
  },
  {
    "tf.image.adjust_brightness": "tf.image.adjust_brightness(\n    image, delta\n)\n"
  },
  {
    "tf.image.adjust_contrast": "tf.image.adjust_contrast(\n    images, contrast_factor\n)\n"
  },
  {
    "tf.image.adjust_gamma": "tf.image.adjust_gamma(\n    image, gamma=1, gain=1\n)\n"
  },
  {
    "tf.image.adjust_hue": "tf.image.adjust_hue(\n    image, delta, name=None\n)\n"
  },
  {
    "tf.image.adjust_jpeg_quality": "tf.image.adjust_jpeg_quality(\n    image, jpeg_quality, name=None\n)\n"
  },
  {
    "tf.image.adjust_saturation": "tf.image.adjust_saturation(\n    image, saturation_factor, name=None\n)\n"
  },
  {
    "tf.image.central_crop": "tf.image.central_crop(\n    image, central_fraction\n)\n"
  },
  {
    "tf.image.combined_non_max_suppression": "tf.image.combined_non_max_suppression(\n    boxes,\n    scores,\n    max_output_size_per_class,\n    max_total_size,\n    iou_threshold=0.5,\n    score_threshold=float('-inf'),\n    pad_per_class=False,\n    clip_boxes=True,\n    name=None\n)\n"
  },
  {
    "tf.image.convert_image_dtype": "tf.image.convert_image_dtype(\n    image, dtype, saturate=False, name=None\n)\n"
  },
  {
    "tf.image.crop_and_resize": "tf.image.crop_and_resize(\n    image,\n    boxes,\n    box_indices,\n    crop_size,\n    method='bilinear',\n    extrapolation_value=0.0,\n    name=None\n)\n"
  },
  {
    "tf.image.crop_to_bounding_box": "tf.image.crop_to_bounding_box(\n    image, offset_height, offset_width, target_height, target_width\n)\n"
  },
  {
    "tf.io.decode_and_crop_jpeg": "tf.io.decode_and_crop_jpeg(\n    contents,\n    crop_window,\n    channels=0,\n    ratio=1,\n    fancy_upscaling=True,\n    try_recover_truncated=False,\n    acceptable_fraction=1,\n    dct_method='',\n    name=None\n)\n"
  },
  {
    "tf.io.decode_bmp": "tf.io.decode_bmp(\n    contents, channels=0, name=None\n)\n"
  },
  {
    "tf.io.decode_gif": "tf.io.decode_gif(\n    contents, name=None\n)\n"
  },
  {
    "tf.io.decode_image": "tf.io.decode_image(\n    contents,\n    channels=None,\n    dtype=tf.dtypes.uint8,\n    name=None,\n    expand_animations=True\n)\n"
  },
  {
    "tf.io.decode_jpeg": "tf.io.decode_jpeg(\n    contents,\n    channels=0,\n    ratio=1,\n    fancy_upscaling=True,\n    try_recover_truncated=False,\n    acceptable_fraction=1,\n    dct_method='',\n    name=None\n)\n"
  },
  {
    "tf.io.decode_png": "tf.io.decode_png(\n    contents,\n    channels=0,\n    dtype=tf.dtypes.uint8,\n    name=None\n)\n"
  },
  {
    "tf.image.draw_bounding_boxes": "tf.image.draw_bounding_boxes(\n    images, boxes, colors, name=None\n)\n"
  },
  {
    "tf.io.encode_jpeg": "tf.io.encode_jpeg(\n    image,\n    format='',\n    quality=95,\n    progressive=False,\n    optimize_size=False,\n    chroma_downsampling=True,\n    density_unit='in',\n    x_density=300,\n    y_density=300,\n    xmp_metadata='',\n    name=None\n)\n"
  },
  {
    "tf.io.encode_png": "tf.io.encode_png(\n    image, compression=-1, name=None\n)\n"
  },
  {
    "tf.image.extract_glimpse": "tf.image.extract_glimpse(\n    input,\n    size,\n    offsets,\n    centered=True,\n    normalized=True,\n    noise='uniform',\n    name=None\n)\n"
  },
  {
    "tf.io.extract_jpeg_shape": "tf.io.extract_jpeg_shape(\n    contents,\n    output_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.image.extract_patches": "tf.image.extract_patches(\n    images, sizes, strides, rates, padding, name=None\n)\n"
  },
  {
    "tf.image.flip_left_right": "tf.image.flip_left_right(\n    image\n)\n"
  },
  {
    "tf.image.flip_up_down": "tf.image.flip_up_down(\n    image\n)\n"
  },
  {
    "tf.image.generate_bounding_box_proposals": "tf.image.generate_bounding_box_proposals(\n    scores,\n    bbox_deltas,\n    image_info,\n    anchors,\n    nms_threshold=0.7,\n    pre_nms_topn=6000,\n    min_size=16,\n    post_nms_topn=300,\n    name=None\n)\n"
  },
  {
    "tf.image.grayscale_to_rgb": "tf.image.grayscale_to_rgb(\n    images, name=None\n)\n"
  },
  {
    "tf.image.hsv_to_rgb": "tf.image.hsv_to_rgb(\n    images, name=None\n)\n"
  },
  {
    "tf.image.image_gradients": "tf.image.image_gradients(\n    image\n)\n"
  },
  {
    "tf.io.is_jpeg": "tf.io.is_jpeg(\n    contents, name=None\n)\n"
  },
  {
    "tf.image.non_max_suppression": "tf.image.non_max_suppression(\n    boxes,\n    scores,\n    max_output_size,\n    iou_threshold=0.5,\n    score_threshold=float('-inf'),\n    name=None\n)\n"
  },
  {
    "tf.image.non_max_suppression_overlaps": "tf.image.non_max_suppression_overlaps(\n    overlaps,\n    scores,\n    max_output_size,\n    overlap_threshold=0.5,\n    score_threshold=float('-inf'),\n    name=None\n)\n"
  },
  {
    "tf.image.non_max_suppression_padded": "tf.image.non_max_suppression_padded(\n    boxes,\n    scores,\n    max_output_size,\n    iou_threshold=0.5,\n    score_threshold=float('-inf'),\n    pad_to_max_output_size=False,\n    name=None,\n    sorted_input=False,\n    canonicalized_coordinates=False,\n    tile_size=512\n)\n"
  },
  {
    "tf.image.non_max_suppression_with_scores": "tf.image.non_max_suppression_with_scores(\n    boxes,\n    scores,\n    max_output_size,\n    iou_threshold=0.5,\n    score_threshold=float('-inf'),\n    soft_nms_sigma=0.0,\n    name=None\n)\n"
  },
  {
    "tf.image.pad_to_bounding_box": "tf.image.pad_to_bounding_box(\n    image, offset_height, offset_width, target_height, target_width\n)\n"
  },
  {
    "tf.image.per_image_standardization": "tf.image.per_image_standardization(\n    image\n)\n"
  },
  {
    "tf.image.psnr": "tf.image.psnr(\n    a, b, max_val, name=None\n)\n"
  },
  {
    "tf.image.random_brightness": "tf.image.random_brightness(\n    image, max_delta, seed=None\n)\n"
  },
  {
    "tf.image.random_contrast": "tf.image.random_contrast(\n    image, lower, upper, seed=None\n)\n"
  },
  {
    "tf.image.random_crop": "tf.image.random_crop(\n    value, size, seed=None, name=None\n)\n"
  },
  {
    "tf.image.random_flip_left_right": "tf.image.random_flip_left_right(\n    image, seed=None\n)\n"
  },
  {
    "tf.image.random_flip_up_down": "tf.image.random_flip_up_down(\n    image, seed=None\n)\n"
  },
  {
    "tf.image.random_hue": "tf.image.random_hue(\n    image, max_delta, seed=None\n)\n"
  },
  {
    "tf.image.random_jpeg_quality": "tf.image.random_jpeg_quality(\n    image, min_jpeg_quality, max_jpeg_quality, seed=None\n)\n"
  },
  {
    "tf.image.random_saturation": "tf.image.random_saturation(\n    image, lower, upper, seed=None\n)\n"
  },
  {
    "tf.image.resize": "tf.image.resize(\n    images,\n    size,\n    method=ResizeMethod.BILINEAR,\n    preserve_aspect_ratio=False,\n    antialias=False,\n    name=None\n)\n"
  },
  {
    "tf.image.resize_with_crop_or_pad": "tf.image.resize_with_crop_or_pad(\n    image, target_height, target_width\n)\n"
  },
  {
    "tf.image.resize_with_pad": "tf.image.resize_with_pad(\n    image,\n    target_height,\n    target_width,\n    method=ResizeMethod.BILINEAR,\n    antialias=False\n)\n"
  },
  {
    "tf.image.rgb_to_grayscale": "tf.image.rgb_to_grayscale(\n    images, name=None\n)\n"
  },
  {
    "tf.image.rgb_to_hsv": "tf.image.rgb_to_hsv(\n    images, name=None\n)\n"
  },
  {
    "tf.image.rgb_to_yiq": "tf.image.rgb_to_yiq(\n    images\n)\n"
  },
  {
    "tf.image.rgb_to_yuv": "tf.image.rgb_to_yuv(\n    images\n)\n"
  },
  {
    "tf.image.rot90": "tf.image.rot90(\n    image, k=1, name=None\n)\n"
  },
  {
    "tf.image.sample_distorted_bounding_box": "tf.image.sample_distorted_bounding_box(\n    image_size,\n    bounding_boxes,\n    seed=0,\n    min_object_covered=0.1,\n    aspect_ratio_range=None,\n    area_range=None,\n    max_attempts=None,\n    use_image_if_no_bounding_boxes=None,\n    name=None\n)\n"
  },
  {
    "tf.image.sobel_edges": "tf.image.sobel_edges(\n    image\n)\n"
  },
  {
    "tf.image.ssim": "tf.image.ssim(\n    img1,\n    img2,\n    max_val,\n    filter_size=11,\n    filter_sigma=1.5,\n    k1=0.01,\n    k2=0.03,\n    return_index_map=False\n)\n"
  },
  {
    "tf.image.ssim_multiscale": "tf.image.ssim_multiscale(\n    img1,\n    img2,\n    max_val,\n    power_factors=_MSSSIM_WEIGHTS,\n    filter_size=11,\n    filter_sigma=1.5,\n    k1=0.01,\n    k2=0.03\n)\n"
  },
  {
    "tf.image.stateless_random_brightness": "tf.image.stateless_random_brightness(\n    image, max_delta, seed\n)\n"
  },
  {
    "tf.image.stateless_random_contrast": "tf.image.stateless_random_contrast(\n    image, lower, upper, seed\n)\n"
  },
  {
    "tf.image.stateless_random_crop": "tf.image.stateless_random_crop(\n    value, size, seed, name=None\n)\n"
  },
  {
    "tf.image.stateless_random_flip_left_right": "tf.image.stateless_random_flip_left_right(\n    image, seed\n)\n"
  },
  {
    "tf.image.stateless_random_flip_up_down": "tf.image.stateless_random_flip_up_down(\n    image, seed\n)\n"
  },
  {
    "tf.image.stateless_random_hue": "tf.image.stateless_random_hue(\n    image, max_delta, seed\n)\n"
  },
  {
    "tf.image.stateless_random_jpeg_quality": "tf.image.stateless_random_jpeg_quality(\n    image, min_jpeg_quality, max_jpeg_quality, seed\n)\n"
  },
  {
    "tf.image.stateless_random_saturation": "tf.image.stateless_random_saturation(\n    image, lower, upper, seed=None\n)\n"
  },
  {
    "tf.image.stateless_sample_distorted_bounding_box": "tf.image.stateless_sample_distorted_bounding_box(\n    image_size,\n    bounding_boxes,\n    seed,\n    min_object_covered=0.1,\n    aspect_ratio_range=None,\n    area_range=None,\n    max_attempts=None,\n    use_image_if_no_bounding_boxes=None,\n    name=None\n)\n"
  },
  {
    "tf.image.total_variation": "tf.image.total_variation(\n    images, name=None\n)\n"
  },
  {
    "tf.image.transpose": "tf.image.transpose(\n    image, name=None\n)\n"
  },
  {
    "tf.image.yiq_to_rgb": "tf.image.yiq_to_rgb(\n    images\n)\n"
  },
  {
    "tf.image.yuv_to_rgb": "tf.image.yuv_to_rgb(\n    images\n)\n"
  },
  {
    "tf.graph_util.import_graph_def": "tf.graph_util.import_graph_def(\n    graph_def,\n    input_map=None,\n    return_elements=None,\n    name=None,\n    op_dict=None,\n    producer_op_list=None\n)\n"
  },
  {
    "tf.keras.initializers.Constant": "tf.keras.initializers.Constant(\n    value=0\n)\n"
  },
  {
    "tf.keras.initializers.GlorotNormal": "tf.keras.initializers.GlorotNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.GlorotUniform": "tf.keras.initializers.GlorotUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.HeNormal": "tf.keras.initializers.HeNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.HeUniform": "tf.keras.initializers.HeUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.Identity": "tf.keras.initializers.Identity(\n    gain=1.0\n)\n"
  },
  {
    "tf.keras.initializers.LecunNormal": "tf.keras.initializers.LecunNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.LecunUniform": "tf.keras.initializers.LecunUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.Orthogonal": "tf.keras.initializers.Orthogonal(\n    gain=1.0, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.RandomNormal": "tf.keras.initializers.RandomNormal(\n    mean=0.0, stddev=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.RandomUniform": "tf.keras.initializers.RandomUniform(\n    minval=-0.05, maxval=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.TruncatedNormal": "tf.keras.initializers.TruncatedNormal(\n    mean=0.0, stddev=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.VarianceScaling": "tf.keras.initializers.VarianceScaling(\n    scale=1.0,\n    mode='fan_in',\n    distribution='truncated_normal',\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.Constant": "tf.keras.initializers.Constant(\n    value=0\n)\n"
  },
  {
    "tf.keras.initializers.deserialize": "tf.keras.initializers.deserialize(\n    config, custom_objects=None\n)\n"
  },
  {
    "tf.keras.initializers.get": "tf.keras.initializers.get(\n    identifier\n)\n"
  },
  {
    "tf.keras.initializers.GlorotNormal": "tf.keras.initializers.GlorotNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.GlorotUniform": "tf.keras.initializers.GlorotUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.HeNormal": "tf.keras.initializers.HeNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.HeUniform": "tf.keras.initializers.HeUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.Identity": "tf.keras.initializers.Identity(\n    gain=1.0\n)\n"
  },
  {
    "tf.keras.initializers.LecunNormal": "tf.keras.initializers.LecunNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.LecunUniform": "tf.keras.initializers.LecunUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.Orthogonal": "tf.keras.initializers.Orthogonal(\n    gain=1.0, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.RandomNormal": "tf.keras.initializers.RandomNormal(\n    mean=0.0, stddev=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.RandomUniform": "tf.keras.initializers.RandomUniform(\n    minval=-0.05, maxval=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.serialize": "tf.keras.initializers.serialize(\n    initializer\n)\n"
  },
  {
    "tf.keras.initializers.TruncatedNormal": "tf.keras.initializers.TruncatedNormal(\n    mean=0.0, stddev=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.VarianceScaling": "tf.keras.initializers.VarianceScaling(\n    scale=1.0,\n    mode='fan_in',\n    distribution='truncated_normal',\n    seed=None\n)\n"
  },
  {
    "tf.io.FixedLenFeature": "tf.io.FixedLenFeature(\n    shape, dtype, default_value=None\n)\n"
  },
  {
    "tf.io.FixedLenSequenceFeature": "tf.io.FixedLenSequenceFeature(\n    shape, dtype, allow_missing=False, default_value=None\n)\n"
  },
  {
    "tf.io.RaggedFeature": "tf.io.RaggedFeature(\n    dtype,\n    value_key=None,\n    partitions=(),\n    row_splits_dtype=tf.dtypes.int32,\n    validate=False\n)\n"
  },
  {
    "tf.io.RaggedFeature.RowLengths": "tf.io.RaggedFeature.RowLengths(\n    key\n)\n"
  },
  {
    "tf.io.RaggedFeature.RowLimits": "tf.io.RaggedFeature.RowLimits(\n    key\n)\n"
  },
  {
    "tf.io.RaggedFeature.RowSplits": "tf.io.RaggedFeature.RowSplits(\n    key\n)\n"
  },
  {
    "tf.io.RaggedFeature.RowStarts": "tf.io.RaggedFeature.RowStarts(\n    key\n)\n"
  },
  {
    "tf.io.RaggedFeature.UniformRowLength": "tf.io.RaggedFeature.UniformRowLength(\n    length\n)\n"
  },
  {
    "tf.io.RaggedFeature.ValueRowIds": "tf.io.RaggedFeature.ValueRowIds(\n    key\n)\n"
  },
  {
    "tf.io.SparseFeature": "tf.io.SparseFeature(\n    index_key, value_key, dtype, size, already_sorted=False\n)\n"
  },
  {
    "tf.io.TFRecordOptions": "tf.io.TFRecordOptions(\n    compression_type=None,\n    flush_mode=None,\n    input_buffer_size=None,\n    output_buffer_size=None,\n    window_bits=None,\n    compression_level=None,\n    compression_method=None,\n    mem_level=None,\n    compression_strategy=None\n)\n"
  },
  {
    "tf.io.TFRecordWriter": "tf.io.TFRecordWriter(\n    path, options=None\n)\n"
  },
  {
    "tf.io.VarLenFeature": "tf.io.VarLenFeature(\n    dtype\n)\n"
  },
  {
    "tf.io.decode_and_crop_jpeg": "tf.io.decode_and_crop_jpeg(\n    contents,\n    crop_window,\n    channels=0,\n    ratio=1,\n    fancy_upscaling=True,\n    try_recover_truncated=False,\n    acceptable_fraction=1,\n    dct_method='',\n    name=None\n)\n"
  },
  {
    "tf.io.decode_base64": "tf.io.decode_base64(\n    input, name=None\n)\n"
  },
  {
    "tf.io.decode_bmp": "tf.io.decode_bmp(\n    contents, channels=0, name=None\n)\n"
  },
  {
    "tf.io.decode_compressed": "tf.io.decode_compressed(\n    bytes, compression_type='', name=None\n)\n"
  },
  {
    "tf.io.decode_csv": "tf.io.decode_csv(\n    records,\n    record_defaults,\n    field_delim=',',\n    use_quote_delim=True,\n    na_value='',\n    select_cols=None,\n    name=None\n)\n"
  },
  {
    "tf.io.decode_gif": "tf.io.decode_gif(\n    contents, name=None\n)\n"
  },
  {
    "tf.io.decode_image": "tf.io.decode_image(\n    contents,\n    channels=None,\n    dtype=tf.dtypes.uint8,\n    name=None,\n    expand_animations=True\n)\n"
  },
  {
    "tf.io.decode_jpeg": "tf.io.decode_jpeg(\n    contents,\n    channels=0,\n    ratio=1,\n    fancy_upscaling=True,\n    try_recover_truncated=False,\n    acceptable_fraction=1,\n    dct_method='',\n    name=None\n)\n"
  },
  {
    "tf.io.decode_json_example": "tf.io.decode_json_example(\n    json_examples, name=None\n)\n"
  },
  {
    "tf.io.decode_png": "tf.io.decode_png(\n    contents,\n    channels=0,\n    dtype=tf.dtypes.uint8,\n    name=None\n)\n"
  },
  {
    "tf.io.decode_proto": "tf.io.decode_proto(\n    bytes,\n    message_type,\n    field_names,\n    output_types,\n    descriptor_source='local://',\n    message_format='binary',\n    sanitize=False,\n    name=None\n)\n"
  },
  {
    "tf.io.decode_raw": "tf.io.decode_raw(\n    input_bytes, out_type, little_endian=True, fixed_length=None, name=None\n)\n"
  },
  {
    "tf.io.deserialize_many_sparse": "tf.io.deserialize_many_sparse(\n    serialized_sparse, dtype, rank=None, name=None\n)\n"
  },
  {
    "tf.io.encode_base64": "tf.io.encode_base64(\n    input, pad=False, name=None\n)\n"
  },
  {
    "tf.io.encode_jpeg": "tf.io.encode_jpeg(\n    image,\n    format='',\n    quality=95,\n    progressive=False,\n    optimize_size=False,\n    chroma_downsampling=True,\n    density_unit='in',\n    x_density=300,\n    y_density=300,\n    xmp_metadata='',\n    name=None\n)\n"
  },
  {
    "tf.io.encode_png": "tf.io.encode_png(\n    image, compression=-1, name=None\n)\n"
  },
  {
    "tf.io.encode_proto": "tf.io.encode_proto(\n    sizes,\n    values,\n    field_names,\n    message_type,\n    descriptor_source='local://',\n    name=None\n)\n"
  },
  {
    "tf.io.extract_jpeg_shape": "tf.io.extract_jpeg_shape(\n    contents,\n    output_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.io.gfile.GFile": "tf.io.gfile.GFile(\n    name, mode='r'\n)\n"
  },
  {
    "tf.io.gfile.copy": "tf.io.gfile.copy(\n    src, dst, overwrite=False\n)\n"
  },
  {
    "tf.io.gfile.exists": "tf.io.gfile.exists(\n    path\n)\n"
  },
  {
    "tf.io.gfile.glob": "tf.io.gfile.glob(\n    pattern\n)\n"
  },
  {
    "tf.io.gfile.isdir": "tf.io.gfile.isdir(\n    path\n)\n"
  },
  {
    "tf.io.gfile.join": "tf.io.gfile.join(\n    path, *paths\n)\n"
  },
  {
    "tf.io.gfile.listdir": "tf.io.gfile.listdir(\n    path\n)\n"
  },
  {
    "tf.io.gfile.makedirs": "tf.io.gfile.makedirs(\n    path\n)\n"
  },
  {
    "tf.io.gfile.mkdir": "tf.io.gfile.mkdir(\n    path\n)\n"
  },
  {
    "tf.io.gfile.remove": "tf.io.gfile.remove(\n    path\n)\n"
  },
  {
    "tf.io.gfile.rename": "tf.io.gfile.rename(\n    src, dst, overwrite=False\n)\n"
  },
  {
    "tf.io.gfile.rmtree": "tf.io.gfile.rmtree(\n    path\n)\n"
  },
  {
    "tf.io.gfile.stat": "tf.io.gfile.stat(\n    path\n)\n"
  },
  {
    "tf.io.gfile.walk": "tf.io.gfile.walk(\n    top, topdown=True, onerror=None\n)\n"
  },
  {
    "tf.io.is_jpeg": "tf.io.is_jpeg(\n    contents, name=None\n)\n"
  },
  {
    "tf.io.match_filenames_once": "tf.io.match_filenames_once(\n    pattern, name=None\n)\n"
  },
  {
    "tf.io.matching_files": "tf.io.matching_files(\n    pattern, name=None\n)\n"
  },
  {
    "tf.io.parse_example": "tf.io.parse_example(\n    serialized, features, example_names=None, name=None\n)\n"
  },
  {
    "tf.io.parse_sequence_example": "tf.io.parse_sequence_example(\n    serialized,\n    context_features=None,\n    sequence_features=None,\n    example_names=None,\n    name=None\n)\n"
  },
  {
    "tf.io.parse_single_example": "tf.io.parse_single_example(\n    serialized, features, example_names=None, name=None\n)\n"
  },
  {
    "tf.io.parse_single_sequence_example": "tf.io.parse_single_sequence_example(\n    serialized,\n    context_features=None,\n    sequence_features=None,\n    example_name=None,\n    name=None\n)\n"
  },
  {
    "tf.io.parse_tensor": "tf.io.parse_tensor(\n    serialized, out_type, name=None\n)\n"
  },
  {
    "tf.io.read_file": "tf.io.read_file(\n    filename, name=None\n)\n"
  },
  {
    "tf.io.serialize_many_sparse": "tf.io.serialize_many_sparse(\n    sp_input,\n    out_type=tf.dtypes.string,\n    name=None\n)\n"
  },
  {
    "tf.io.serialize_sparse": "tf.io.serialize_sparse(\n    sp_input,\n    out_type=tf.dtypes.string,\n    name=None\n)\n"
  },
  {
    "tf.io.serialize_tensor": "tf.io.serialize_tensor(\n    tensor, name=None\n)\n"
  },
  {
    "tf.io.write_file": "tf.io.write_file(\n    filename, contents, name=None\n)\n"
  },
  {
    "tf.io.write_graph": "tf.io.write_graph(\n    graph_or_graph_def, logdir, name, as_text=True\n)\n"
  },
  {
    "tf.is_tensor": "tf.is_tensor(\n    x\n)\n"
  },
  {
    "tf.keras.Input": "tf.keras.Input(\n    shape=None,\n    batch_size=None,\n    name=None,\n    dtype=None,\n    sparse=None,\n    tensor=None,\n    ragged=None,\n    type_spec=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.Model": "tf.keras.Model(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.keras.Sequential": "tf.keras.Sequential(\n    layers=None, name=None\n)\n"
  },
  {
    "tf.keras.activations.deserialize": "tf.keras.activations.deserialize(\n    name, custom_objects=None\n)\n"
  },
  {
    "tf.keras.activations.elu": "tf.keras.activations.elu(\n    x, alpha=1.0\n)\n"
  },
  {
    "tf.keras.activations.exponential": "tf.keras.activations.exponential(\n    x\n)\n"
  },
  {
    "tf.keras.activations.gelu": "tf.keras.activations.gelu(\n    x, approximate=False\n)\n"
  },
  {
    "tf.keras.activations.get": "tf.keras.activations.get(\n    identifier\n)\n"
  },
  {
    "tf.keras.activations.hard_sigmoid": "tf.keras.activations.hard_sigmoid(\n    x\n)\n"
  },
  {
    "tf.keras.activations.linear": "tf.keras.activations.linear(\n    x\n)\n"
  },
  {
    "tf.keras.activations.relu": "tf.keras.activations.relu(\n    x, alpha=0.0, max_value=None, threshold=0.0\n)\n"
  },
  {
    "tf.keras.activations.selu": "tf.keras.activations.selu(\n    x\n)\n"
  },
  {
    "tf.keras.activations.serialize": "tf.keras.activations.serialize(\n    activation\n)\n"
  },
  {
    "tf.keras.activations.sigmoid": "tf.keras.activations.sigmoid(\n    x\n)\n"
  },
  {
    "tf.keras.activations.softmax": "tf.keras.activations.softmax(\n    x, axis=-1\n)\n"
  },
  {
    "tf.keras.activations.softplus": "tf.keras.activations.softplus(\n    x\n)\n"
  },
  {
    "tf.keras.activations.softsign": "tf.keras.activations.softsign(\n    x\n)\n"
  },
  {
    "tf.keras.activations.swish": "tf.keras.activations.swish(\n    x\n)\n"
  },
  {
    "tf.keras.activations.tanh": "tf.keras.activations.tanh(\n    x\n)\n"
  },
  {
    "tf.keras.applications.convnext.ConvNeXtBase": "tf.keras.applications.convnext.ConvNeXtBase(\n    model_name='convnext_base',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.convnext.ConvNeXtLarge": "tf.keras.applications.convnext.ConvNeXtLarge(\n    model_name='convnext_large',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.convnext.ConvNeXtSmall": "tf.keras.applications.convnext.ConvNeXtSmall(\n    model_name='convnext_small',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.convnext.ConvNeXtTiny": "tf.keras.applications.convnext.ConvNeXtTiny(\n    model_name='convnext_tiny',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.convnext.ConvNeXtXLarge": "tf.keras.applications.convnext.ConvNeXtXLarge(\n    model_name='convnext_xlarge',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.densenet.DenseNet121": "tf.keras.applications.densenet.DenseNet121(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.densenet.DenseNet169": "tf.keras.applications.densenet.DenseNet169(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.densenet.DenseNet201": "tf.keras.applications.densenet.DenseNet201(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB0": "tf.keras.applications.efficientnet.EfficientNetB0(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB1": "tf.keras.applications.efficientnet.EfficientNetB1(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB2": "tf.keras.applications.efficientnet.EfficientNetB2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB3": "tf.keras.applications.efficientnet.EfficientNetB3(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB4": "tf.keras.applications.efficientnet.EfficientNetB4(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB5": "tf.keras.applications.efficientnet.EfficientNetB5(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB6": "tf.keras.applications.efficientnet.EfficientNetB6(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB7": "tf.keras.applications.efficientnet.EfficientNetB7(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2B0": "tf.keras.applications.efficientnet_v2.EfficientNetV2B0(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2B1": "tf.keras.applications.efficientnet_v2.EfficientNetV2B1(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2B2": "tf.keras.applications.efficientnet_v2.EfficientNetV2B2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2B3": "tf.keras.applications.efficientnet_v2.EfficientNetV2B3(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2L": "tf.keras.applications.efficientnet_v2.EfficientNetV2L(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2M": "tf.keras.applications.efficientnet_v2.EfficientNetV2M(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2S": "tf.keras.applications.efficientnet_v2.EfficientNetV2S(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.inception_resnet_v2.InceptionResNetV2": "tf.keras.applications.inception_resnet_v2.InceptionResNetV2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.inception_v3.InceptionV3": "tf.keras.applications.inception_v3.InceptionV3(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.mobilenet.MobileNet": "tf.keras.applications.mobilenet.MobileNet(\n    input_shape=None,\n    alpha=1.0,\n    depth_multiplier=1,\n    dropout=0.001,\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.mobilenet_v2.MobileNetV2": "tf.keras.applications.mobilenet_v2.MobileNetV2(\n    input_shape=None,\n    alpha=1.0,\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.MobileNetV3Large": "tf.keras.applications.MobileNetV3Large(\n    input_shape=None,\n    alpha=1.0,\n    minimalistic=False,\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    classes=1000,\n    pooling=None,\n    dropout_rate=0.2,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.MobileNetV3Small": "tf.keras.applications.MobileNetV3Small(\n    input_shape=None,\n    alpha=1.0,\n    minimalistic=False,\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    classes=1000,\n    pooling=None,\n    dropout_rate=0.2,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.nasnet.NASNetLarge": "tf.keras.applications.nasnet.NASNetLarge(\n    input_shape=None,\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.nasnet.NASNetMobile": "tf.keras.applications.nasnet.NASNetMobile(\n    input_shape=None,\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX002": "tf.keras.applications.regnet.RegNetX002(\n    model_name='regnetx002',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX004": "tf.keras.applications.regnet.RegNetX004(\n    model_name='regnetx004',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX006": "tf.keras.applications.regnet.RegNetX006(\n    model_name='regnetx006',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX008": "tf.keras.applications.regnet.RegNetX008(\n    model_name='regnetx008',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX016": "tf.keras.applications.regnet.RegNetX016(\n    model_name='regnetx016',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX032": "tf.keras.applications.regnet.RegNetX032(\n    model_name='regnetx032',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX040": "tf.keras.applications.regnet.RegNetX040(\n    model_name='regnetx040',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX064": "tf.keras.applications.regnet.RegNetX064(\n    model_name='regnetx064',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX080": "tf.keras.applications.regnet.RegNetX080(\n    model_name='regnetx080',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX120": "tf.keras.applications.regnet.RegNetX120(\n    model_name='regnetx120',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX160": "tf.keras.applications.regnet.RegNetX160(\n    model_name='regnetx160',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX320": "tf.keras.applications.regnet.RegNetX320(\n    model_name='regnetx320',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY002": "tf.keras.applications.regnet.RegNetY002(\n    model_name='regnety002',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY004": "tf.keras.applications.regnet.RegNetY004(\n    model_name='regnety004',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY006": "tf.keras.applications.regnet.RegNetY006(\n    model_name='regnety006',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY008": "tf.keras.applications.regnet.RegNetY008(\n    model_name='regnety008',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY016": "tf.keras.applications.regnet.RegNetY016(\n    model_name='regnety016',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY032": "tf.keras.applications.regnet.RegNetY032(\n    model_name='regnety032',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY040": "tf.keras.applications.regnet.RegNetY040(\n    model_name='regnety040',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY064": "tf.keras.applications.regnet.RegNetY064(\n    model_name='regnety064',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY080": "tf.keras.applications.regnet.RegNetY080(\n    model_name='regnety080',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY120": "tf.keras.applications.regnet.RegNetY120(\n    model_name='regnety120',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY160": "tf.keras.applications.regnet.RegNetY160(\n    model_name='regnety160',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY320": "tf.keras.applications.regnet.RegNetY320(\n    model_name='regnety320',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.resnet.ResNet101": "tf.keras.applications.resnet.ResNet101(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.resnet_v2.ResNet101V2": "tf.keras.applications.resnet_v2.ResNet101V2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.resnet.ResNet152": "tf.keras.applications.resnet.ResNet152(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.resnet_v2.ResNet152V2": "tf.keras.applications.resnet_v2.ResNet152V2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.resnet50.ResNet50": "tf.keras.applications.resnet50.ResNet50(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.resnet_v2.ResNet50V2": "tf.keras.applications.resnet_v2.ResNet50V2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS101": "tf.keras.applications.resnet_rs.ResNetRS101(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS152": "tf.keras.applications.resnet_rs.ResNetRS152(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS200": "tf.keras.applications.resnet_rs.ResNetRS200(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS270": "tf.keras.applications.resnet_rs.ResNetRS270(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS350": "tf.keras.applications.resnet_rs.ResNetRS350(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS420": "tf.keras.applications.resnet_rs.ResNetRS420(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS50": "tf.keras.applications.resnet_rs.ResNetRS50(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.vgg16.VGG16": "tf.keras.applications.vgg16.VGG16(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.vgg19.VGG19": "tf.keras.applications.vgg19.VGG19(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.xception.Xception": "tf.keras.applications.xception.Xception(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.convnext.ConvNeXtBase": "tf.keras.applications.convnext.ConvNeXtBase(\n    model_name='convnext_base',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.convnext.ConvNeXtLarge": "tf.keras.applications.convnext.ConvNeXtLarge(\n    model_name='convnext_large',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.convnext.ConvNeXtSmall": "tf.keras.applications.convnext.ConvNeXtSmall(\n    model_name='convnext_small',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.convnext.ConvNeXtTiny": "tf.keras.applications.convnext.ConvNeXtTiny(\n    model_name='convnext_tiny',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.convnext.ConvNeXtXLarge": "tf.keras.applications.convnext.ConvNeXtXLarge(\n    model_name='convnext_xlarge',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.convnext.decode_predictions": "tf.keras.applications.convnext.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.convnext.preprocess_input": "tf.keras.applications.convnext.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.densenet.DenseNet121": "tf.keras.applications.densenet.DenseNet121(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.densenet.DenseNet169": "tf.keras.applications.densenet.DenseNet169(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.densenet.DenseNet201": "tf.keras.applications.densenet.DenseNet201(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.densenet.decode_predictions": "tf.keras.applications.densenet.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.densenet.preprocess_input": "tf.keras.applications.densenet.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB0": "tf.keras.applications.efficientnet.EfficientNetB0(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB1": "tf.keras.applications.efficientnet.EfficientNetB1(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB2": "tf.keras.applications.efficientnet.EfficientNetB2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB3": "tf.keras.applications.efficientnet.EfficientNetB3(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB4": "tf.keras.applications.efficientnet.EfficientNetB4(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB5": "tf.keras.applications.efficientnet.EfficientNetB5(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB6": "tf.keras.applications.efficientnet.EfficientNetB6(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB7": "tf.keras.applications.efficientnet.EfficientNetB7(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.decode_predictions": "tf.keras.applications.efficientnet.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.preprocess_input": "tf.keras.applications.efficientnet.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2B0": "tf.keras.applications.efficientnet_v2.EfficientNetV2B0(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2B1": "tf.keras.applications.efficientnet_v2.EfficientNetV2B1(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2B2": "tf.keras.applications.efficientnet_v2.EfficientNetV2B2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2B3": "tf.keras.applications.efficientnet_v2.EfficientNetV2B3(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2L": "tf.keras.applications.efficientnet_v2.EfficientNetV2L(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2M": "tf.keras.applications.efficientnet_v2.EfficientNetV2M(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2S": "tf.keras.applications.efficientnet_v2.EfficientNetV2S(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.decode_predictions": "tf.keras.applications.efficientnet_v2.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.preprocess_input": "tf.keras.applications.efficientnet_v2.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.imagenet_utils.decode_predictions": "tf.keras.applications.imagenet_utils.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.imagenet_utils.preprocess_input": "tf.keras.applications.imagenet_utils.preprocess_input(\n    x, data_format=None, mode='caffe'\n)\n"
  },
  {
    "tf.keras.applications.inception_resnet_v2.InceptionResNetV2": "tf.keras.applications.inception_resnet_v2.InceptionResNetV2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.inception_resnet_v2.decode_predictions": "tf.keras.applications.inception_resnet_v2.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.inception_resnet_v2.preprocess_input": "tf.keras.applications.inception_resnet_v2.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.inception_v3.InceptionV3": "tf.keras.applications.inception_v3.InceptionV3(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.inception_v3.decode_predictions": "tf.keras.applications.inception_v3.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.inception_v3.preprocess_input": "tf.keras.applications.inception_v3.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.mobilenet.MobileNet": "tf.keras.applications.mobilenet.MobileNet(\n    input_shape=None,\n    alpha=1.0,\n    depth_multiplier=1,\n    dropout=0.001,\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.mobilenet.decode_predictions": "tf.keras.applications.mobilenet.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.mobilenet.preprocess_input": "tf.keras.applications.mobilenet.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.mobilenet_v2.MobileNetV2": "tf.keras.applications.mobilenet_v2.MobileNetV2(\n    input_shape=None,\n    alpha=1.0,\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.mobilenet_v2.decode_predictions": "tf.keras.applications.mobilenet_v2.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.mobilenet_v2.preprocess_input": "tf.keras.applications.mobilenet_v2.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.mobilenet_v3.decode_predictions": "tf.keras.applications.mobilenet_v3.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.mobilenet_v3.preprocess_input": "tf.keras.applications.mobilenet_v3.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.nasnet.NASNetLarge": "tf.keras.applications.nasnet.NASNetLarge(\n    input_shape=None,\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.nasnet.NASNetMobile": "tf.keras.applications.nasnet.NASNetMobile(\n    input_shape=None,\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.nasnet.decode_predictions": "tf.keras.applications.nasnet.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.nasnet.preprocess_input": "tf.keras.applications.nasnet.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX002": "tf.keras.applications.regnet.RegNetX002(\n    model_name='regnetx002',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX004": "tf.keras.applications.regnet.RegNetX004(\n    model_name='regnetx004',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX006": "tf.keras.applications.regnet.RegNetX006(\n    model_name='regnetx006',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX008": "tf.keras.applications.regnet.RegNetX008(\n    model_name='regnetx008',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX016": "tf.keras.applications.regnet.RegNetX016(\n    model_name='regnetx016',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX032": "tf.keras.applications.regnet.RegNetX032(\n    model_name='regnetx032',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX040": "tf.keras.applications.regnet.RegNetX040(\n    model_name='regnetx040',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX064": "tf.keras.applications.regnet.RegNetX064(\n    model_name='regnetx064',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX080": "tf.keras.applications.regnet.RegNetX080(\n    model_name='regnetx080',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX120": "tf.keras.applications.regnet.RegNetX120(\n    model_name='regnetx120',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX160": "tf.keras.applications.regnet.RegNetX160(\n    model_name='regnetx160',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX320": "tf.keras.applications.regnet.RegNetX320(\n    model_name='regnetx320',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY002": "tf.keras.applications.regnet.RegNetY002(\n    model_name='regnety002',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY004": "tf.keras.applications.regnet.RegNetY004(\n    model_name='regnety004',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY006": "tf.keras.applications.regnet.RegNetY006(\n    model_name='regnety006',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY008": "tf.keras.applications.regnet.RegNetY008(\n    model_name='regnety008',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY016": "tf.keras.applications.regnet.RegNetY016(\n    model_name='regnety016',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY032": "tf.keras.applications.regnet.RegNetY032(\n    model_name='regnety032',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY040": "tf.keras.applications.regnet.RegNetY040(\n    model_name='regnety040',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY064": "tf.keras.applications.regnet.RegNetY064(\n    model_name='regnety064',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY080": "tf.keras.applications.regnet.RegNetY080(\n    model_name='regnety080',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY120": "tf.keras.applications.regnet.RegNetY120(\n    model_name='regnety120',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY160": "tf.keras.applications.regnet.RegNetY160(\n    model_name='regnety160',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY320": "tf.keras.applications.regnet.RegNetY320(\n    model_name='regnety320',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.decode_predictions": "tf.keras.applications.regnet.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.regnet.preprocess_input": "tf.keras.applications.regnet.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.resnet.ResNet101": "tf.keras.applications.resnet.ResNet101(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.resnet.ResNet152": "tf.keras.applications.resnet.ResNet152(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.resnet50.ResNet50": "tf.keras.applications.resnet50.ResNet50(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.resnet50.decode_predictions": "tf.keras.applications.resnet50.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.resnet50.preprocess_input": "tf.keras.applications.resnet50.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.resnet50.ResNet50": "tf.keras.applications.resnet50.ResNet50(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.resnet50.decode_predictions": "tf.keras.applications.resnet50.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.resnet50.preprocess_input": "tf.keras.applications.resnet50.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS101": "tf.keras.applications.resnet_rs.ResNetRS101(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS152": "tf.keras.applications.resnet_rs.ResNetRS152(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS200": "tf.keras.applications.resnet_rs.ResNetRS200(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS270": "tf.keras.applications.resnet_rs.ResNetRS270(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS350": "tf.keras.applications.resnet_rs.ResNetRS350(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS420": "tf.keras.applications.resnet_rs.ResNetRS420(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS50": "tf.keras.applications.resnet_rs.ResNetRS50(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.decode_predictions": "tf.keras.applications.resnet_rs.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.preprocess_input": "tf.keras.applications.resnet_rs.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.resnet_v2.ResNet101V2": "tf.keras.applications.resnet_v2.ResNet101V2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.resnet_v2.ResNet152V2": "tf.keras.applications.resnet_v2.ResNet152V2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.resnet_v2.ResNet50V2": "tf.keras.applications.resnet_v2.ResNet50V2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.resnet_v2.decode_predictions": "tf.keras.applications.resnet_v2.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.resnet_v2.preprocess_input": "tf.keras.applications.resnet_v2.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.vgg16.VGG16": "tf.keras.applications.vgg16.VGG16(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.vgg16.decode_predictions": "tf.keras.applications.vgg16.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.vgg16.preprocess_input": "tf.keras.applications.vgg16.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.vgg19.VGG19": "tf.keras.applications.vgg19.VGG19(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.vgg19.decode_predictions": "tf.keras.applications.vgg19.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.vgg19.preprocess_input": "tf.keras.applications.vgg19.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.xception.Xception": "tf.keras.applications.xception.Xception(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.xception.decode_predictions": "tf.keras.applications.xception.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.xception.preprocess_input": "tf.keras.applications.xception.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.backend.get_uid": "tf.keras.backend.get_uid(\n    prefix=''\n)\n"
  },
  {
    "tf.keras.backend.is_keras_tensor": "tf.keras.backend.is_keras_tensor(\n    x\n)\n"
  },
  {
    "tf.keras.backend.rnn": "tf.keras.backend.rnn(\n    step_function,\n    inputs,\n    initial_states,\n    go_backwards=False,\n    mask=None,\n    constants=None,\n    unroll=False,\n    input_length=None,\n    time_major=False,\n    zero_output_for_mask=False,\n    return_all_outputs=True\n)\n"
  },
  {
    "tf.keras.backend.set_epsilon": "tf.keras.backend.set_epsilon(\n    value\n)\n"
  },
  {
    "tf.keras.backend.set_floatx": "tf.keras.backend.set_floatx(\n    value\n)\n"
  },
  {
    "tf.keras.backend.set_image_data_format": "tf.keras.backend.set_image_data_format(\n    data_format\n)\n"
  },
  {
    "tf.keras.callbacks.BackupAndRestore": "tf.keras.callbacks.BackupAndRestore(\n    backup_dir,\n    save_freq='epoch',\n    delete_checkpoint=True,\n    save_before_preemption=False\n)\n"
  },
  {
    "tf.keras.callbacks.BaseLogger": "tf.keras.callbacks.BaseLogger(\n    stateful_metrics=None\n)\n"
  },
  {
    "tf.keras.callbacks.CSVLogger": "tf.keras.callbacks.CSVLogger(\n    filename, separator=',', append=False\n)\n"
  },
  {
    "tf.keras.callbacks.CallbackList": "tf.keras.callbacks.CallbackList(\n    callbacks=None, add_history=False, add_progbar=False, model=None, **params\n)\n"
  },
  {
    "tf.keras.callbacks.EarlyStopping": "tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    min_delta=0,\n    patience=0,\n    verbose=0,\n    mode='auto',\n    baseline=None,\n    restore_best_weights=False,\n    start_from_epoch=0\n)\n"
  },
  {
    "tf.keras.callbacks.LambdaCallback": "tf.keras.callbacks.LambdaCallback(\n    on_epoch_begin=None,\n    on_epoch_end=None,\n    on_batch_begin=None,\n    on_batch_end=None,\n    on_train_begin=None,\n    on_train_end=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.callbacks.LearningRateScheduler": "tf.keras.callbacks.LearningRateScheduler(\n    schedule, verbose=0\n)\n"
  },
  {
    "tf.keras.callbacks.ModelCheckpoint": "tf.keras.callbacks.ModelCheckpoint(\n    filepath,\n    monitor: str = 'val_loss',\n    verbose: int = 0,\n    save_best_only: bool = False,\n    save_weights_only: bool = False,\n    mode: str = 'auto',\n    save_freq='epoch',\n    options=None,\n    initial_value_threshold=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.callbacks.ProgbarLogger": "tf.keras.callbacks.ProgbarLogger(\n    count_mode: str = 'samples', stateful_metrics=None\n)\n"
  },
  {
    "tf.keras.callbacks.ReduceLROnPlateau": "tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.1,\n    patience=10,\n    verbose=0,\n    mode='auto',\n    min_delta=0.0001,\n    cooldown=0,\n    min_lr=0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.callbacks.RemoteMonitor": "tf.keras.callbacks.RemoteMonitor(\n    root='http://localhost:9000',\n    path='/publish/epoch/end/',\n    field='data',\n    headers=None,\n    send_as_json=False\n)\n"
  },
  {
    "tf.keras.callbacks.TensorBoard": "tf.keras.callbacks.TensorBoard(\n    log_dir='logs',\n    histogram_freq=0,\n    write_graph=True,\n    write_images=False,\n    write_steps_per_second=False,\n    update_freq='epoch',\n    profile_batch=0,\n    embeddings_freq=0,\n    embeddings_metadata=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.callbacks.experimental.BackupAndRestore": "tf.keras.callbacks.experimental.BackupAndRestore(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.keras.constraints.MaxNorm": "tf.keras.constraints.MaxNorm(\n    max_value=2, axis=0\n)\n"
  },
  {
    "tf.keras.constraints.MinMaxNorm": "tf.keras.constraints.MinMaxNorm(\n    min_value=0.0, max_value=1.0, rate=1.0, axis=0\n)\n"
  },
  {
    "tf.keras.constraints.UnitNorm": "tf.keras.constraints.UnitNorm(\n    axis=0\n)\n"
  },
  {
    "tf.keras.constraints.deserialize": "tf.keras.constraints.deserialize(\n    config, custom_objects=None\n)\n"
  },
  {
    "tf.keras.constraints.get": "tf.keras.constraints.get(\n    identifier\n)\n"
  },
  {
    "tf.keras.constraints.MaxNorm": "tf.keras.constraints.MaxNorm(\n    max_value=2, axis=0\n)\n"
  },
  {
    "tf.keras.constraints.MinMaxNorm": "tf.keras.constraints.MinMaxNorm(\n    min_value=0.0, max_value=1.0, rate=1.0, axis=0\n)\n"
  },
  {
    "tf.keras.constraints.serialize": "tf.keras.constraints.serialize(\n    constraint\n)\n"
  },
  {
    "tf.keras.constraints.UnitNorm": "tf.keras.constraints.UnitNorm(\n    axis=0\n)\n"
  },
  {
    "tf.keras.datasets.boston_housing.load_data": "tf.keras.datasets.boston_housing.load_data(\n    path='boston_housing.npz', test_split=0.2, seed=113\n)\n"
  },
  {
    "tf.keras.datasets.cifar100.load_data": "tf.keras.datasets.cifar100.load_data(\n    label_mode='fine'\n)\n"
  },
  {
    "tf.keras.datasets.imdb.get_word_index": "tf.keras.datasets.imdb.get_word_index(\n    path='imdb_word_index.json'\n)\n"
  },
  {
    "tf.keras.datasets.imdb.load_data": "tf.keras.datasets.imdb.load_data(\n    path='imdb.npz',\n    num_words=None,\n    skip_top=0,\n    maxlen=None,\n    seed=113,\n    start_char=1,\n    oov_char=2,\n    index_from=3,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.datasets.mnist.load_data": "tf.keras.datasets.mnist.load_data(\n    path='mnist.npz'\n)\n"
  },
  {
    "tf.keras.datasets.reuters.get_word_index": "tf.keras.datasets.reuters.get_word_index(\n    path='reuters_word_index.json'\n)\n"
  },
  {
    "tf.keras.datasets.reuters.load_data": "tf.keras.datasets.reuters.load_data(\n    path='reuters.npz',\n    num_words=None,\n    skip_top=0,\n    maxlen=None,\n    test_split=0.2,\n    seed=113,\n    start_char=1,\n    oov_char=2,\n    index_from=3,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.dtensor.experimental.LayoutMap": "tf.keras.dtensor.experimental.LayoutMap(\n    mesh=None\n)\n"
  },
  {
    "tf.keras.dtensor.experimental.optimizers.Adadelta": "tf.keras.dtensor.experimental.optimizers.Adadelta(\n    learning_rate=0.001,\n    rho=0.95,\n    epsilon=1e-07,\n    gradients_clip_option=None,\n    ema_option=None,\n    name='Adadelta',\n    mesh=None\n)\n"
  },
  {
    "tf.keras.dtensor.experimental.optimizers.Adagrad": "tf.keras.dtensor.experimental.optimizers.Adagrad(\n    learning_rate=0.001,\n    initial_accumulator_value=0.1,\n    epsilon=1e-07,\n    gradients_clip_option=None,\n    ema_option=None,\n    name='Adagrad',\n    mesh=None\n)\n"
  },
  {
    "tf.keras.dtensor.experimental.optimizers.Adam": "tf.keras.dtensor.experimental.optimizers.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    gradients_clip_option=None,\n    ema_option=None,\n    name='Adam',\n    mesh=None\n)\n"
  },
  {
    "tf.keras.dtensor.experimental.optimizers.AdamW": "tf.keras.dtensor.experimental.optimizers.AdamW(\n    learning_rate=0.001,\n    weight_decay=0.004,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    name='AdamW',\n    mesh=None\n)\n"
  },
  {
    "tf.keras.dtensor.experimental.optimizers.RMSprop": "tf.keras.dtensor.experimental.optimizers.RMSprop(\n    learning_rate=0.001,\n    rho=0.9,\n    momentum=0.0,\n    epsilon=1e-07,\n    centered=False,\n    gradients_clip_option=None,\n    ema_option=None,\n    jit_compile=False,\n    name='RMSprop',\n    mesh=None\n)\n"
  },
  {
    "tf.keras.dtensor.experimental.optimizers.SGD": "tf.keras.dtensor.experimental.optimizers.SGD(\n    learning_rate=0.01,\n    momentum=0.0,\n    nesterov=False,\n    amsgrad=False,\n    gradients_clip_option=None,\n    ema_option=None,\n    jit_compile=False,\n    name='SGD',\n    mesh=None\n)\n"
  },
  {
    "tf.keras.estimator.model_to_estimator": "tf.keras.estimator.model_to_estimator(\n    keras_model=None,\n    keras_model_path=None,\n    custom_objects=None,\n    model_dir=None,\n    config=None,\n    checkpoint_format='checkpoint',\n    metric_names_map=None,\n    export_outputs=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.CosineDecay": "tf.keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate, decay_steps, alpha=0.0, name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.CosineDecayRestarts": "tf.keras.optimizers.schedules.CosineDecayRestarts(\n    initial_learning_rate,\n    first_decay_steps,\n    t_mul=2.0,\n    m_mul=1.0,\n    alpha=0.0,\n    name=None\n)\n"
  },
  {
    "tf.keras.experimental.LinearModel": "tf.keras.experimental.LinearModel(\n    units=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='zeros',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.experimental.SequenceFeatures": "tf.keras.experimental.SequenceFeatures(\n    feature_columns, trainable=True, name=None, **kwargs\n)\n"
  },
  {
    "tf.keras.experimental.SidecarEvaluator": "tf.keras.experimental.SidecarEvaluator(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.keras.experimental.WideDeepModel": "tf.keras.experimental.WideDeepModel(\n    linear_model, dnn_model, activation=None, **kwargs\n)\n"
  },
  {
    "tf.keras.initializers.Constant": "tf.keras.initializers.Constant(\n    value=0\n)\n"
  },
  {
    "tf.keras.initializers.GlorotNormal": "tf.keras.initializers.GlorotNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.GlorotUniform": "tf.keras.initializers.GlorotUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.HeNormal": "tf.keras.initializers.HeNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.HeUniform": "tf.keras.initializers.HeUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.Identity": "tf.keras.initializers.Identity(\n    gain=1.0\n)\n"
  },
  {
    "tf.keras.initializers.LecunNormal": "tf.keras.initializers.LecunNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.LecunUniform": "tf.keras.initializers.LecunUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.Orthogonal": "tf.keras.initializers.Orthogonal(\n    gain=1.0, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.RandomNormal": "tf.keras.initializers.RandomNormal(\n    mean=0.0, stddev=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.RandomUniform": "tf.keras.initializers.RandomUniform(\n    minval=-0.05, maxval=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.TruncatedNormal": "tf.keras.initializers.TruncatedNormal(\n    mean=0.0, stddev=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.VarianceScaling": "tf.keras.initializers.VarianceScaling(\n    scale=1.0,\n    mode='fan_in',\n    distribution='truncated_normal',\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.Constant": "tf.keras.initializers.Constant(\n    value=0\n)\n"
  },
  {
    "tf.keras.initializers.deserialize": "tf.keras.initializers.deserialize(\n    config, custom_objects=None\n)\n"
  },
  {
    "tf.keras.initializers.get": "tf.keras.initializers.get(\n    identifier\n)\n"
  },
  {
    "tf.keras.initializers.GlorotNormal": "tf.keras.initializers.GlorotNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.GlorotUniform": "tf.keras.initializers.GlorotUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.HeNormal": "tf.keras.initializers.HeNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.HeUniform": "tf.keras.initializers.HeUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.Identity": "tf.keras.initializers.Identity(\n    gain=1.0\n)\n"
  },
  {
    "tf.keras.initializers.LecunNormal": "tf.keras.initializers.LecunNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.LecunUniform": "tf.keras.initializers.LecunUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.Orthogonal": "tf.keras.initializers.Orthogonal(\n    gain=1.0, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.RandomNormal": "tf.keras.initializers.RandomNormal(\n    mean=0.0, stddev=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.RandomUniform": "tf.keras.initializers.RandomUniform(\n    minval=-0.05, maxval=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.serialize": "tf.keras.initializers.serialize(\n    initializer\n)\n"
  },
  {
    "tf.keras.initializers.TruncatedNormal": "tf.keras.initializers.TruncatedNormal(\n    mean=0.0, stddev=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.VarianceScaling": "tf.keras.initializers.VarianceScaling(\n    scale=1.0,\n    mode='fan_in',\n    distribution='truncated_normal',\n    seed=None\n)\n"
  },
  {
    "tf.keras.layers.AbstractRNNCell": "tf.keras.layers.AbstractRNNCell(\n    trainable=True, name=None, dtype=None, dynamic=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Activation": "tf.keras.layers.Activation(\n    activation, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.ActivityRegularization": "tf.keras.layers.ActivityRegularization(\n    l1=0.0, l2=0.0, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Add": "tf.keras.layers.Add(\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.AdditiveAttention": "tf.keras.layers.AdditiveAttention(\n    use_scale=True, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.AlphaDropout": "tf.keras.layers.AlphaDropout(\n    rate, noise_shape=None, seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Attention": "tf.keras.layers.Attention(\n    use_scale=False, score_mode='dot', **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Average": "tf.keras.layers.Average(\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.AveragePooling1D": "tf.keras.layers.AveragePooling1D(\n    pool_size=2,\n    strides=None,\n    padding='valid',\n    data_format='channels_last',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.AveragePooling2D": "tf.keras.layers.AveragePooling2D(\n    pool_size=(2, 2),\n    strides=None,\n    padding='valid',\n    data_format=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.AveragePooling3D": "tf.keras.layers.AveragePooling3D(\n    pool_size=(2, 2, 2),\n    strides=None,\n    padding='valid',\n    data_format=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.AveragePooling1D": "tf.keras.layers.AveragePooling1D(\n    pool_size=2,\n    strides=None,\n    padding='valid',\n    data_format='channels_last',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.AveragePooling2D": "tf.keras.layers.AveragePooling2D(\n    pool_size=(2, 2),\n    strides=None,\n    padding='valid',\n    data_format=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.AveragePooling3D": "tf.keras.layers.AveragePooling3D(\n    pool_size=(2, 2, 2),\n    strides=None,\n    padding='valid',\n    data_format=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.BatchNormalization": "tf.keras.layers.BatchNormalization(\n    axis=-1,\n    momentum=0.99,\n    epsilon=0.001,\n    center=True,\n    scale=True,\n    beta_initializer='zeros',\n    gamma_initializer='ones',\n    moving_mean_initializer='zeros',\n    moving_variance_initializer='ones',\n    beta_regularizer=None,\n    gamma_regularizer=None,\n    beta_constraint=None,\n    gamma_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Bidirectional": "tf.keras.layers.Bidirectional(\n    layer,\n    merge_mode='concat',\n    weights=None,\n    backward_layer=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.CategoryEncoding": "tf.keras.layers.CategoryEncoding(\n    num_tokens=None, output_mode='multi_hot', sparse=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.CenterCrop": "tf.keras.layers.CenterCrop(\n    height, width, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Concatenate": "tf.keras.layers.Concatenate(\n    axis=-1, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv1D": "tf.keras.layers.Conv1D(\n    filters,\n    kernel_size,\n    strides=1,\n    padding='valid',\n    data_format='channels_last',\n    dilation_rate=1,\n    groups=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv1DTranspose": "tf.keras.layers.Conv1DTranspose(\n    filters,\n    kernel_size,\n    strides=1,\n    padding='valid',\n    output_padding=None,\n    data_format=None,\n    dilation_rate=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv2D": "tf.keras.layers.Conv2D(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding='valid',\n    data_format=None,\n    dilation_rate=(1, 1),\n    groups=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv2DTranspose": "tf.keras.layers.Conv2DTranspose(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding='valid',\n    output_padding=None,\n    data_format=None,\n    dilation_rate=(1, 1),\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv3D": "tf.keras.layers.Conv3D(\n    filters,\n    kernel_size,\n    strides=(1, 1, 1),\n    padding='valid',\n    data_format=None,\n    dilation_rate=(1, 1, 1),\n    groups=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv3DTranspose": "tf.keras.layers.Conv3DTranspose(\n    filters,\n    kernel_size,\n    strides=(1, 1, 1),\n    padding='valid',\n    output_padding=None,\n    data_format=None,\n    dilation_rate=(1, 1, 1),\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.ConvLSTM1D": "tf.keras.layers.ConvLSTM1D(\n    filters,\n    kernel_size,\n    strides=1,\n    padding='valid',\n    data_format=None,\n    dilation_rate=1,\n    activation='tanh',\n    recurrent_activation='hard_sigmoid',\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    recurrent_initializer='orthogonal',\n    bias_initializer='zeros',\n    unit_forget_bias=True,\n    kernel_regularizer=None,\n    recurrent_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    recurrent_constraint=None,\n    bias_constraint=None,\n    return_sequences=False,\n    return_state=False,\n    go_backwards=False,\n    stateful=False,\n    dropout=0.0,\n    recurrent_dropout=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.ConvLSTM2D": "tf.keras.layers.ConvLSTM2D(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding='valid',\n    data_format=None,\n    dilation_rate=(1, 1),\n    activation='tanh',\n    recurrent_activation='hard_sigmoid',\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    recurrent_initializer='orthogonal',\n    bias_initializer='zeros',\n    unit_forget_bias=True,\n    kernel_regularizer=None,\n    recurrent_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    recurrent_constraint=None,\n    bias_constraint=None,\n    return_sequences=False,\n    return_state=False,\n    go_backwards=False,\n    stateful=False,\n    dropout=0.0,\n    recurrent_dropout=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.ConvLSTM3D": "tf.keras.layers.ConvLSTM3D(\n    filters,\n    kernel_size,\n    strides=(1, 1, 1),\n    padding='valid',\n    data_format=None,\n    dilation_rate=(1, 1, 1),\n    activation='tanh',\n    recurrent_activation='hard_sigmoid',\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    recurrent_initializer='orthogonal',\n    bias_initializer='zeros',\n    unit_forget_bias=True,\n    kernel_regularizer=None,\n    recurrent_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    recurrent_constraint=None,\n    bias_constraint=None,\n    return_sequences=False,\n    return_state=False,\n    go_backwards=False,\n    stateful=False,\n    dropout=0.0,\n    recurrent_dropout=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv1D": "tf.keras.layers.Conv1D(\n    filters,\n    kernel_size,\n    strides=1,\n    padding='valid',\n    data_format='channels_last',\n    dilation_rate=1,\n    groups=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv1DTranspose": "tf.keras.layers.Conv1DTranspose(\n    filters,\n    kernel_size,\n    strides=1,\n    padding='valid',\n    output_padding=None,\n    data_format=None,\n    dilation_rate=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv2D": "tf.keras.layers.Conv2D(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding='valid',\n    data_format=None,\n    dilation_rate=(1, 1),\n    groups=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv2DTranspose": "tf.keras.layers.Conv2DTranspose(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding='valid',\n    output_padding=None,\n    data_format=None,\n    dilation_rate=(1, 1),\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv3D": "tf.keras.layers.Conv3D(\n    filters,\n    kernel_size,\n    strides=(1, 1, 1),\n    padding='valid',\n    data_format=None,\n    dilation_rate=(1, 1, 1),\n    groups=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv3DTranspose": "tf.keras.layers.Conv3DTranspose(\n    filters,\n    kernel_size,\n    strides=(1, 1, 1),\n    padding='valid',\n    output_padding=None,\n    data_format=None,\n    dilation_rate=(1, 1, 1),\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Cropping1D": "tf.keras.layers.Cropping1D(\n    cropping=(1, 1), **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Cropping2D": "tf.keras.layers.Cropping2D(\n    cropping=((0, 0), (0, 0)), data_format=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Cropping3D": "tf.keras.layers.Cropping3D(\n    cropping=((1, 1), (1, 1), (1, 1)), data_format=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Dense": "tf.keras.layers.Dense(\n    units,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.DenseFeatures": "tf.keras.layers.DenseFeatures(\n    feature_columns, trainable=True, name=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.DepthwiseConv1D": "tf.keras.layers.DepthwiseConv1D(\n    kernel_size,\n    strides=1,\n    padding='valid',\n    depth_multiplier=1,\n    data_format=None,\n    dilation_rate=1,\n    activation=None,\n    use_bias=True,\n    depthwise_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    depthwise_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    depthwise_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.DepthwiseConv2D": "tf.keras.layers.DepthwiseConv2D(\n    kernel_size,\n    strides=(1, 1),\n    padding='valid',\n    depth_multiplier=1,\n    data_format=None,\n    dilation_rate=(1, 1),\n    activation=None,\n    use_bias=True,\n    depthwise_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    depthwise_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    depthwise_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Discretization": "tf.keras.layers.Discretization(\n    bin_boundaries=None,\n    num_bins=None,\n    epsilon=0.01,\n    output_mode='int',\n    sparse=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Dot": "tf.keras.layers.Dot(\n    axes, normalize=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Dropout": "tf.keras.layers.Dropout(\n    rate, noise_shape=None, seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.ELU": "tf.keras.layers.ELU(\n    alpha=1.0, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.EinsumDense": "tf.keras.layers.EinsumDense(\n    equation,\n    output_shape,\n    activation=None,\n    bias_axes=None,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Embedding": "tf.keras.layers.Embedding(\n    input_dim,\n    output_dim,\n    embeddings_initializer='uniform',\n    embeddings_regularizer=None,\n    activity_regularizer=None,\n    embeddings_constraint=None,\n    mask_zero=False,\n    input_length=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Flatten": "tf.keras.layers.Flatten(\n    data_format=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GRU": "tf.keras.layers.GRU(\n    units,\n    activation='tanh',\n    recurrent_activation='sigmoid',\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    recurrent_initializer='orthogonal',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    recurrent_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    recurrent_constraint=None,\n    bias_constraint=None,\n    dropout=0.0,\n    recurrent_dropout=0.0,\n    return_sequences=False,\n    return_state=False,\n    go_backwards=False,\n    stateful=False,\n    unroll=False,\n    time_major=False,\n    reset_after=True,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GRUCell": "tf.keras.layers.GRUCell(\n    units,\n    activation='tanh',\n    recurrent_activation='sigmoid',\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    recurrent_initializer='orthogonal',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    recurrent_regularizer=None,\n    bias_regularizer=None,\n    kernel_constraint=None,\n    recurrent_constraint=None,\n    bias_constraint=None,\n    dropout=0.0,\n    recurrent_dropout=0.0,\n    reset_after=True,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GaussianDropout": "tf.keras.layers.GaussianDropout(\n    rate, seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GaussianNoise": "tf.keras.layers.GaussianNoise(\n    stddev, seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalAveragePooling1D": "tf.keras.layers.GlobalAveragePooling1D(\n    data_format='channels_last', **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalAveragePooling2D": "tf.keras.layers.GlobalAveragePooling2D(\n    data_format=None, keepdims=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalAveragePooling3D": "tf.keras.layers.GlobalAveragePooling3D(\n    data_format=None, keepdims=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalAveragePooling1D": "tf.keras.layers.GlobalAveragePooling1D(\n    data_format='channels_last', **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalAveragePooling2D": "tf.keras.layers.GlobalAveragePooling2D(\n    data_format=None, keepdims=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalAveragePooling3D": "tf.keras.layers.GlobalAveragePooling3D(\n    data_format=None, keepdims=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalMaxPool1D": "tf.keras.layers.GlobalMaxPool1D(\n    data_format='channels_last', keepdims=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalMaxPool2D": "tf.keras.layers.GlobalMaxPool2D(\n    data_format=None, keepdims=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalMaxPool3D": "tf.keras.layers.GlobalMaxPool3D(\n    data_format=None, keepdims=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalMaxPool1D": "tf.keras.layers.GlobalMaxPool1D(\n    data_format='channels_last', keepdims=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalMaxPool2D": "tf.keras.layers.GlobalMaxPool2D(\n    data_format=None, keepdims=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalMaxPool3D": "tf.keras.layers.GlobalMaxPool3D(\n    data_format=None, keepdims=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GroupNormalization": "tf.keras.layers.GroupNormalization(\n    groups=32,\n    axis=-1,\n    epsilon=0.001,\n    center=True,\n    scale=True,\n    beta_initializer='zeros',\n    gamma_initializer='ones',\n    beta_regularizer=None,\n    gamma_regularizer=None,\n    beta_constraint=None,\n    gamma_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Hashing": "tf.keras.layers.Hashing(\n    num_bins,\n    mask_value=None,\n    salt=None,\n    output_mode='int',\n    sparse=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.Input": "tf.keras.Input(\n    shape=None,\n    batch_size=None,\n    name=None,\n    dtype=None,\n    sparse=None,\n    tensor=None,\n    ragged=None,\n    type_spec=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.InputLayer": "tf.keras.layers.InputLayer(\n    input_shape=None,\n    batch_size=None,\n    dtype=None,\n    input_tensor=None,\n    sparse=None,\n    name=None,\n    ragged=None,\n    type_spec=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.InputSpec": "tf.keras.layers.InputSpec(\n    dtype=None,\n    shape=None,\n    ndim=None,\n    max_ndim=None,\n    min_ndim=None,\n    axes=None,\n    allow_last_axis_squeeze=False,\n    name=None\n)\n"
  },
  {
    "tf.keras.layers.IntegerLookup": "tf.keras.layers.IntegerLookup(\n    max_tokens=None,\n    num_oov_indices=1,\n    mask_token=None,\n    oov_token=-1,\n    vocabulary=None,\n    vocabulary_dtype='int64',\n    idf_weights=None,\n    invert=False,\n    output_mode='int',\n    sparse=False,\n    pad_to_max_tokens=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.LSTM": "tf.keras.layers.LSTM(\n    units,\n    activation='tanh',\n    recurrent_activation='sigmoid',\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    recurrent_initializer='orthogonal',\n    bias_initializer='zeros',\n    unit_forget_bias=True,\n    kernel_regularizer=None,\n    recurrent_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    recurrent_constraint=None,\n    bias_constraint=None,\n    dropout=0.0,\n    recurrent_dropout=0.0,\n    return_sequences=False,\n    return_state=False,\n    go_backwards=False,\n    stateful=False,\n    time_major=False,\n    unroll=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.LSTMCell": "tf.keras.layers.LSTMCell(\n    units,\n    activation='tanh',\n    recurrent_activation='sigmoid',\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    recurrent_initializer='orthogonal',\n    bias_initializer='zeros',\n    unit_forget_bias=True,\n    kernel_regularizer=None,\n    recurrent_regularizer=None,\n    bias_regularizer=None,\n    kernel_constraint=None,\n    recurrent_constraint=None,\n    bias_constraint=None,\n    dropout=0.0,\n    recurrent_dropout=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Lambda": "tf.keras.layers.Lambda(\n    function, output_shape=None, mask=None, arguments=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Layer": "tf.keras.layers.Layer(\n    trainable=True, name=None, dtype=None, dynamic=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.LayerNormalization": "tf.keras.layers.LayerNormalization(\n    axis=-1,\n    epsilon=0.001,\n    center=True,\n    scale=True,\n    beta_initializer='zeros',\n    gamma_initializer='ones',\n    beta_regularizer=None,\n    gamma_regularizer=None,\n    beta_constraint=None,\n    gamma_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.LeakyReLU": "tf.keras.layers.LeakyReLU(\n    alpha=0.3, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.LocallyConnected1D": "tf.keras.layers.LocallyConnected1D(\n    filters,\n    kernel_size,\n    strides=1,\n    padding='valid',\n    data_format=None,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    implementation=1,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.LocallyConnected2D": "tf.keras.layers.LocallyConnected2D(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding='valid',\n    data_format=None,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    implementation=1,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Masking": "tf.keras.layers.Masking(\n    mask_value=0.0, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.MaxPool1D": "tf.keras.layers.MaxPool1D(\n    pool_size=2,\n    strides=None,\n    padding='valid',\n    data_format='channels_last',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.MaxPool2D": "tf.keras.layers.MaxPool2D(\n    pool_size=(2, 2),\n    strides=None,\n    padding='valid',\n    data_format=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.MaxPool3D": "tf.keras.layers.MaxPool3D(\n    pool_size=(2, 2, 2),\n    strides=None,\n    padding='valid',\n    data_format=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.MaxPool1D": "tf.keras.layers.MaxPool1D(\n    pool_size=2,\n    strides=None,\n    padding='valid',\n    data_format='channels_last',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.MaxPool2D": "tf.keras.layers.MaxPool2D(\n    pool_size=(2, 2),\n    strides=None,\n    padding='valid',\n    data_format=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.MaxPool3D": "tf.keras.layers.MaxPool3D(\n    pool_size=(2, 2, 2),\n    strides=None,\n    padding='valid',\n    data_format=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Maximum": "tf.keras.layers.Maximum(\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Minimum": "tf.keras.layers.Minimum(\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.MultiHeadAttention": "tf.keras.layers.MultiHeadAttention(\n    num_heads,\n    key_dim,\n    value_dim=None,\n    dropout=0.0,\n    use_bias=True,\n    output_shape=None,\n    attention_axes=None,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Multiply": "tf.keras.layers.Multiply(\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Normalization": "tf.keras.layers.Normalization(\n    axis=-1, mean=None, variance=None, invert=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.PReLU": "tf.keras.layers.PReLU(\n    alpha_initializer='zeros',\n    alpha_regularizer=None,\n    alpha_constraint=None,\n    shared_axes=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Permute": "tf.keras.layers.Permute(\n    dims, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RNN": "tf.keras.layers.RNN(\n    cell,\n    return_sequences=False,\n    return_state=False,\n    go_backwards=False,\n    stateful=False,\n    unroll=False,\n    time_major=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomBrightness": "tf.keras.layers.RandomBrightness(\n    factor, value_range=(0, 255), seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomContrast": "tf.keras.layers.RandomContrast(\n    factor, seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomCrop": "tf.keras.layers.RandomCrop(\n    height, width, seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomFlip": "tf.keras.layers.RandomFlip(\n    mode=HORIZONTAL_AND_VERTICAL, seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomHeight": "tf.keras.layers.RandomHeight(\n    factor, interpolation='bilinear', seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomRotation": "tf.keras.layers.RandomRotation(\n    factor,\n    fill_mode='reflect',\n    interpolation='bilinear',\n    seed=None,\n    fill_value=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomTranslation": "tf.keras.layers.RandomTranslation(\n    height_factor,\n    width_factor,\n    fill_mode='reflect',\n    interpolation='bilinear',\n    seed=None,\n    fill_value=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomWidth": "tf.keras.layers.RandomWidth(\n    factor, interpolation='bilinear', seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomZoom": "tf.keras.layers.RandomZoom(\n    height_factor,\n    width_factor=None,\n    fill_mode='reflect',\n    interpolation='bilinear',\n    seed=None,\n    fill_value=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.ReLU": "tf.keras.layers.ReLU(\n    max_value=None, negative_slope=0.0, threshold=0.0, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RepeatVector": "tf.keras.layers.RepeatVector(\n    n, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Rescaling": "tf.keras.layers.Rescaling(\n    scale, offset=0.0, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Reshape": "tf.keras.layers.Reshape(\n    target_shape, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Resizing": "tf.keras.layers.Resizing(\n    height,\n    width,\n    interpolation='bilinear',\n    crop_to_aspect_ratio=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.SeparableConv1D": "tf.keras.layers.SeparableConv1D(\n    filters,\n    kernel_size,\n    strides=1,\n    padding='valid',\n    data_format=None,\n    dilation_rate=1,\n    depth_multiplier=1,\n    activation=None,\n    use_bias=True,\n    depthwise_initializer='glorot_uniform',\n    pointwise_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    depthwise_regularizer=None,\n    pointwise_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    depthwise_constraint=None,\n    pointwise_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.SeparableConv2D": "tf.keras.layers.SeparableConv2D(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding='valid',\n    data_format=None,\n    dilation_rate=(1, 1),\n    depth_multiplier=1,\n    activation=None,\n    use_bias=True,\n    depthwise_initializer='glorot_uniform',\n    pointwise_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    depthwise_regularizer=None,\n    pointwise_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    depthwise_constraint=None,\n    pointwise_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.SeparableConv1D": "tf.keras.layers.SeparableConv1D(\n    filters,\n    kernel_size,\n    strides=1,\n    padding='valid',\n    data_format=None,\n    dilation_rate=1,\n    depth_multiplier=1,\n    activation=None,\n    use_bias=True,\n    depthwise_initializer='glorot_uniform',\n    pointwise_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    depthwise_regularizer=None,\n    pointwise_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    depthwise_constraint=None,\n    pointwise_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.SeparableConv2D": "tf.keras.layers.SeparableConv2D(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding='valid',\n    data_format=None,\n    dilation_rate=(1, 1),\n    depth_multiplier=1,\n    activation=None,\n    use_bias=True,\n    depthwise_initializer='glorot_uniform',\n    pointwise_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    depthwise_regularizer=None,\n    pointwise_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    depthwise_constraint=None,\n    pointwise_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.SimpleRNN": "tf.keras.layers.SimpleRNN(\n    units,\n    activation='tanh',\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    recurrent_initializer='orthogonal',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    recurrent_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    recurrent_constraint=None,\n    bias_constraint=None,\n    dropout=0.0,\n    recurrent_dropout=0.0,\n    return_sequences=False,\n    return_state=False,\n    go_backwards=False,\n    stateful=False,\n    unroll=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.SimpleRNNCell": "tf.keras.layers.SimpleRNNCell(\n    units,\n    activation='tanh',\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    recurrent_initializer='orthogonal',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    recurrent_regularizer=None,\n    bias_regularizer=None,\n    kernel_constraint=None,\n    recurrent_constraint=None,\n    bias_constraint=None,\n    dropout=0.0,\n    recurrent_dropout=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Softmax": "tf.keras.layers.Softmax(\n    axis=-1, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.SpatialDropout1D": "tf.keras.layers.SpatialDropout1D(\n    rate, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.SpatialDropout2D": "tf.keras.layers.SpatialDropout2D(\n    rate, data_format=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.SpatialDropout3D": "tf.keras.layers.SpatialDropout3D(\n    rate, data_format=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.StackedRNNCells": "tf.keras.layers.StackedRNNCells(\n    cells, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.StringLookup": "tf.keras.layers.StringLookup(\n    max_tokens=None,\n    num_oov_indices=1,\n    mask_token=None,\n    oov_token='[UNK]',\n    vocabulary=None,\n    idf_weights=None,\n    encoding='utf-8',\n    invert=False,\n    output_mode='int',\n    sparse=False,\n    pad_to_max_tokens=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Subtract": "tf.keras.layers.Subtract(\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.TextVectorization": "tf.keras.layers.TextVectorization(\n    max_tokens=None,\n    standardize='lower_and_strip_punctuation',\n    split='whitespace',\n    ngrams=None,\n    output_mode='int',\n    output_sequence_length=None,\n    pad_to_max_tokens=False,\n    vocabulary=None,\n    idf_weights=None,\n    sparse=False,\n    ragged=False,\n    encoding='utf-8',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.ThresholdedReLU": "tf.keras.layers.ThresholdedReLU(\n    theta=1.0, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.TimeDistributed": "tf.keras.layers.TimeDistributed(\n    layer, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.UnitNormalization": "tf.keras.layers.UnitNormalization(\n    axis=-1, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.UpSampling1D": "tf.keras.layers.UpSampling1D(\n    size=2, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.UpSampling2D": "tf.keras.layers.UpSampling2D(\n    size=(2, 2), data_format=None, interpolation='nearest', **kwargs\n)\n"
  },
  {
    "tf.keras.layers.UpSampling3D": "tf.keras.layers.UpSampling3D(\n    size=(2, 2, 2), data_format=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Wrapper": "tf.keras.layers.Wrapper(\n    layer, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.ZeroPadding1D": "tf.keras.layers.ZeroPadding1D(\n    padding=1, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.ZeroPadding2D": "tf.keras.layers.ZeroPadding2D(\n    padding=(1, 1), data_format=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.ZeroPadding3D": "tf.keras.layers.ZeroPadding3D(\n    padding=(1, 1, 1), data_format=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.add": "tf.keras.layers.add(\n    inputs, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.average": "tf.keras.layers.average(\n    inputs, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.concatenate": "tf.keras.layers.concatenate(\n    inputs, axis=-1, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.deserialize": "tf.keras.layers.deserialize(\n    config, custom_objects=None\n)\n"
  },
  {
    "tf.keras.layers.dot": "tf.keras.layers.dot(\n    inputs, axes, normalize=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.EinsumDense": "tf.keras.layers.EinsumDense(\n    equation,\n    output_shape,\n    activation=None,\n    bias_axes=None,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.experimental.RandomFourierFeatures": "tf.keras.layers.experimental.RandomFourierFeatures(\n    output_dim,\n    kernel_initializer='gaussian',\n    scale=None,\n    trainable=False,\n    name=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.experimental.SyncBatchNormalization": "tf.keras.layers.experimental.SyncBatchNormalization(\n    axis=-1,\n    momentum=0.99,\n    epsilon=0.001,\n    center=True,\n    scale=True,\n    beta_initializer='zeros',\n    gamma_initializer='ones',\n    moving_mean_initializer='zeros',\n    moving_variance_initializer='ones',\n    beta_regularizer=None,\n    gamma_regularizer=None,\n    beta_constraint=None,\n    gamma_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.CategoryEncoding": "tf.keras.layers.CategoryEncoding(\n    num_tokens=None, output_mode='multi_hot', sparse=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.CenterCrop": "tf.keras.layers.CenterCrop(\n    height, width, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Discretization": "tf.keras.layers.Discretization(\n    bin_boundaries=None,\n    num_bins=None,\n    epsilon=0.01,\n    output_mode='int',\n    sparse=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.experimental.preprocessing.HashedCrossing": "tf.keras.layers.experimental.preprocessing.HashedCrossing(\n    num_bins, output_mode='int', sparse=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Hashing": "tf.keras.layers.Hashing(\n    num_bins,\n    mask_value=None,\n    salt=None,\n    output_mode='int',\n    sparse=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.IntegerLookup": "tf.keras.layers.IntegerLookup(\n    max_tokens=None,\n    num_oov_indices=1,\n    mask_token=None,\n    oov_token=-1,\n    vocabulary=None,\n    vocabulary_dtype='int64',\n    idf_weights=None,\n    invert=False,\n    output_mode='int',\n    sparse=False,\n    pad_to_max_tokens=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Normalization": "tf.keras.layers.Normalization(\n    axis=-1, mean=None, variance=None, invert=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.experimental.preprocessing.PreprocessingLayer": "tf.keras.layers.experimental.preprocessing.PreprocessingLayer(\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomContrast": "tf.keras.layers.RandomContrast(\n    factor, seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomCrop": "tf.keras.layers.RandomCrop(\n    height, width, seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomFlip": "tf.keras.layers.RandomFlip(\n    mode=HORIZONTAL_AND_VERTICAL, seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomHeight": "tf.keras.layers.RandomHeight(\n    factor, interpolation='bilinear', seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomRotation": "tf.keras.layers.RandomRotation(\n    factor,\n    fill_mode='reflect',\n    interpolation='bilinear',\n    seed=None,\n    fill_value=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomTranslation": "tf.keras.layers.RandomTranslation(\n    height_factor,\n    width_factor,\n    fill_mode='reflect',\n    interpolation='bilinear',\n    seed=None,\n    fill_value=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomWidth": "tf.keras.layers.RandomWidth(\n    factor, interpolation='bilinear', seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomZoom": "tf.keras.layers.RandomZoom(\n    height_factor,\n    width_factor=None,\n    fill_mode='reflect',\n    interpolation='bilinear',\n    seed=None,\n    fill_value=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Rescaling": "tf.keras.layers.Rescaling(\n    scale, offset=0.0, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Resizing": "tf.keras.layers.Resizing(\n    height,\n    width,\n    interpolation='bilinear',\n    crop_to_aspect_ratio=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.StringLookup": "tf.keras.layers.StringLookup(\n    max_tokens=None,\n    num_oov_indices=1,\n    mask_token=None,\n    oov_token='[UNK]',\n    vocabulary=None,\n    idf_weights=None,\n    encoding='utf-8',\n    invert=False,\n    output_mode='int',\n    sparse=False,\n    pad_to_max_tokens=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.TextVectorization": "tf.keras.layers.TextVectorization(\n    max_tokens=None,\n    standardize='lower_and_strip_punctuation',\n    split='whitespace',\n    ngrams=None,\n    output_mode='int',\n    output_sequence_length=None,\n    pad_to_max_tokens=False,\n    vocabulary=None,\n    idf_weights=None,\n    sparse=False,\n    ragged=False,\n    encoding='utf-8',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.maximum": "tf.keras.layers.maximum(\n    inputs, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.minimum": "tf.keras.layers.minimum(\n    inputs, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.multiply": "tf.keras.layers.multiply(\n    inputs, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.serialize": "tf.keras.layers.serialize(\n    layer\n)\n"
  },
  {
    "tf.keras.layers.subtract": "tf.keras.layers.subtract(\n    inputs, **kwargs\n)\n"
  },
  {
    "tf.keras.losses.BinaryCrossentropy": "tf.keras.losses.BinaryCrossentropy(\n    from_logits=False,\n    label_smoothing=0.0,\n    axis=-1,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='binary_crossentropy'\n)\n"
  },
  {
    "tf.keras.losses.BinaryFocalCrossentropy": "tf.keras.losses.BinaryFocalCrossentropy(\n    apply_class_balancing=False,\n    alpha=0.25,\n    gamma=2.0,\n    from_logits=False,\n    label_smoothing=0.0,\n    axis=-1,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='binary_focal_crossentropy'\n)\n"
  },
  {
    "tf.keras.losses.CategoricalCrossentropy": "tf.keras.losses.CategoricalCrossentropy(\n    from_logits=False,\n    label_smoothing=0.0,\n    axis=-1,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='categorical_crossentropy'\n)\n"
  },
  {
    "tf.keras.losses.CategoricalHinge": "tf.keras.losses.CategoricalHinge(\n    reduction=losses_utils.ReductionV2.AUTO, name='categorical_hinge'\n)\n"
  },
  {
    "tf.keras.losses.CosineSimilarity": "tf.keras.losses.CosineSimilarity(\n    axis=-1,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='cosine_similarity'\n)\n"
  },
  {
    "tf.keras.losses.Hinge": "tf.keras.losses.Hinge(\n    reduction=losses_utils.ReductionV2.AUTO, name='hinge'\n)\n"
  },
  {
    "tf.keras.losses.Huber": "tf.keras.losses.Huber(\n    delta=1.0,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='huber_loss'\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.KLDivergence": "tf.keras.losses.KLDivergence(\n    reduction=losses_utils.ReductionV2.AUTO, name='kl_divergence'\n)\n"
  },
  {
    "tf.keras.losses.LogCosh": "tf.keras.losses.LogCosh(\n    reduction=losses_utils.ReductionV2.AUTO, name='log_cosh'\n)\n"
  },
  {
    "tf.keras.losses.Loss": "tf.keras.losses.Loss(\n    reduction=losses_utils.ReductionV2.AUTO, name=None\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.MeanAbsoluteError": "tf.keras.losses.MeanAbsoluteError(\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='mean_absolute_error'\n)\n"
  },
  {
    "tf.keras.losses.MeanAbsolutePercentageError": "tf.keras.losses.MeanAbsolutePercentageError(\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='mean_absolute_percentage_error'\n)\n"
  },
  {
    "tf.keras.losses.MeanSquaredError": "tf.keras.losses.MeanSquaredError(\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='mean_squared_error'\n)\n"
  },
  {
    "tf.keras.losses.MeanSquaredLogarithmicError": "tf.keras.losses.MeanSquaredLogarithmicError(\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='mean_squared_logarithmic_error'\n)\n"
  },
  {
    "tf.keras.losses.Poisson": "tf.keras.losses.Poisson(\n    reduction=losses_utils.ReductionV2.AUTO, name='poisson'\n)\n"
  },
  {
    "tf.keras.losses.SparseCategoricalCrossentropy": "tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=False,\n    ignore_class=None,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='sparse_categorical_crossentropy'\n)\n"
  },
  {
    "tf.keras.losses.SquaredHinge": "tf.keras.losses.SquaredHinge(\n    reduction=losses_utils.ReductionV2.AUTO, name='squared_hinge'\n)\n"
  },
  {
    "tf.keras.metrics.binary_crossentropy": "tf.keras.metrics.binary_crossentropy(\n    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.binary_focal_crossentropy": "tf.keras.metrics.binary_focal_crossentropy(\n    y_true,\n    y_pred,\n    apply_class_balancing=False,\n    alpha=0.25,\n    gamma=2.0,\n    from_logits=False,\n    label_smoothing=0.0,\n    axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.categorical_crossentropy": "tf.keras.metrics.categorical_crossentropy(\n    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n)\n"
  },
  {
    "tf.keras.losses.categorical_hinge": "tf.keras.losses.categorical_hinge(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.cosine_similarity": "tf.keras.losses.cosine_similarity(\n    y_true, y_pred, axis=-1\n)\n"
  },
  {
    "tf.keras.losses.deserialize": "tf.keras.losses.deserialize(\n    name, custom_objects=None\n)\n"
  },
  {
    "tf.keras.losses.get": "tf.keras.losses.get(\n    identifier\n)\n"
  },
  {
    "tf.keras.metrics.hinge": "tf.keras.metrics.hinge(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.huber": "tf.keras.losses.huber(\n    y_true, y_pred, delta=1.0\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.log_cosh": "tf.keras.losses.log_cosh(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.log_cosh": "tf.keras.losses.log_cosh(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.poisson": "tf.keras.metrics.poisson(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.serialize": "tf.keras.losses.serialize(\n    loss\n)\n"
  },
  {
    "tf.keras.metrics.sparse_categorical_crossentropy": "tf.keras.metrics.sparse_categorical_crossentropy(\n    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None\n)\n"
  },
  {
    "tf.keras.metrics.squared_hinge": "tf.keras.metrics.squared_hinge(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.AUC": "tf.keras.metrics.AUC(\n    num_thresholds=200,\n    curve='ROC',\n    summation_method='interpolation',\n    name=None,\n    dtype=None,\n    thresholds=None,\n    multi_label=False,\n    num_labels=None,\n    label_weights=None,\n    from_logits=False\n)\n"
  },
  {
    "tf.keras.metrics.Accuracy": "tf.keras.metrics.Accuracy(\n    name='accuracy', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.BinaryAccuracy": "tf.keras.metrics.BinaryAccuracy(\n    name='binary_accuracy', dtype=None, threshold=0.5\n)\n"
  },
  {
    "tf.keras.metrics.BinaryCrossentropy": "tf.keras.metrics.BinaryCrossentropy(\n    name='binary_crossentropy',\n    dtype=None,\n    from_logits=False,\n    label_smoothing=0\n)\n"
  },
  {
    "tf.keras.metrics.BinaryIoU": "tf.keras.metrics.BinaryIoU(\n    target_class_ids: Union[List[int], Tuple[int, ...]] = (0, 1),\n    threshold=0.5,\n    name=None,\n    dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.CategoricalAccuracy": "tf.keras.metrics.CategoricalAccuracy(\n    name='categorical_accuracy', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.CategoricalCrossentropy": "tf.keras.metrics.CategoricalCrossentropy(\n    name='categorical_crossentropy',\n    dtype=None,\n    from_logits=False,\n    label_smoothing=0,\n    axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.CategoricalHinge": "tf.keras.metrics.CategoricalHinge(\n    name='categorical_hinge', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.CosineSimilarity": "tf.keras.metrics.CosineSimilarity(\n    name='cosine_similarity', dtype=None, axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.FalseNegatives": "tf.keras.metrics.FalseNegatives(\n    thresholds=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.FalsePositives": "tf.keras.metrics.FalsePositives(\n    thresholds=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.Hinge": "tf.keras.metrics.Hinge(\n    name='hinge', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.IoU": "tf.keras.metrics.IoU(\n    num_classes: int,\n    target_class_ids: Union[List[int], Tuple[int, ...]],\n    name: Optional[str] = None,\n    dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n    ignore_class: Optional[int] = None,\n    sparse_y_true: bool = True,\n    sparse_y_pred: bool = True,\n    axis: int = -1\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.KLDivergence": "tf.keras.metrics.KLDivergence(\n    name='kullback_leibler_divergence', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.LogCoshError": "tf.keras.metrics.LogCoshError(\n    name='logcosh', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.Mean": "tf.keras.metrics.Mean(\n    name='mean', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanAbsoluteError": "tf.keras.metrics.MeanAbsoluteError(\n    name='mean_absolute_error', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanAbsolutePercentageError": "tf.keras.metrics.MeanAbsolutePercentageError(\n    name='mean_absolute_percentage_error', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanIoU": "tf.keras.metrics.MeanIoU(\n    num_classes: int,\n    name: Optional[str] = None,\n    dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n    ignore_class: Optional[int] = None,\n    sparse_y_true: bool = True,\n    sparse_y_pred: bool = True,\n    axis: int = -1\n)\n"
  },
  {
    "tf.keras.metrics.MeanMetricWrapper": "tf.keras.metrics.MeanMetricWrapper(\n    fn, name=None, dtype=None, **kwargs\n)\n"
  },
  {
    "tf.keras.metrics.MeanRelativeError": "tf.keras.metrics.MeanRelativeError(\n    normalizer, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanSquaredError": "tf.keras.metrics.MeanSquaredError(\n    name='mean_squared_error', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanSquaredLogarithmicError": "tf.keras.metrics.MeanSquaredLogarithmicError(\n    name='mean_squared_logarithmic_error', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanTensor": "tf.keras.metrics.MeanTensor(\n    name='mean_tensor', dtype=None, shape=None\n)\n"
  },
  {
    "tf.keras.metrics.Metric": "tf.keras.metrics.Metric(\n    name=None, dtype=None, **kwargs\n)\n"
  },
  {
    "tf.keras.metrics.OneHotIoU": "tf.keras.metrics.OneHotIoU(\n    num_classes: int,\n    target_class_ids: Union[List[int], Tuple[int, ...]],\n    name=None,\n    dtype=None,\n    ignore_class: Optional[int] = None,\n    sparse_y_pred: bool = False,\n    axis: int = -1\n)\n"
  },
  {
    "tf.keras.metrics.OneHotMeanIoU": "tf.keras.metrics.OneHotMeanIoU(\n    num_classes: int,\n    name: str = None,\n    dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n    ignore_class: Optional[int] = None,\n    sparse_y_pred: bool = False,\n    axis: int = -1\n)\n"
  },
  {
    "tf.keras.metrics.Poisson": "tf.keras.metrics.Poisson(\n    name='poisson', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.Precision": "tf.keras.metrics.Precision(\n    thresholds=None, top_k=None, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.PrecisionAtRecall": "tf.keras.metrics.PrecisionAtRecall(\n    recall, num_thresholds=200, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.Recall": "tf.keras.metrics.Recall(\n    thresholds=None, top_k=None, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.RecallAtPrecision": "tf.keras.metrics.RecallAtPrecision(\n    precision, num_thresholds=200, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.RootMeanSquaredError": "tf.keras.metrics.RootMeanSquaredError(\n    name='root_mean_squared_error', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.SensitivityAtSpecificity": "tf.keras.metrics.SensitivityAtSpecificity(\n    specificity, num_thresholds=200, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.SparseCategoricalAccuracy": "tf.keras.metrics.SparseCategoricalAccuracy(\n    name='sparse_categorical_accuracy', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.SparseCategoricalCrossentropy": "tf.keras.metrics.SparseCategoricalCrossentropy(\n    name: str = 'sparse_categorical_crossentropy',\n    dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n    from_logits: bool = False,\n    ignore_class: Optional[int] = None,\n    axis: int = -1\n)\n"
  },
  {
    "tf.keras.metrics.SparseTopKCategoricalAccuracy": "tf.keras.metrics.SparseTopKCategoricalAccuracy(\n    k=5, name='sparse_top_k_categorical_accuracy', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.SpecificityAtSensitivity": "tf.keras.metrics.SpecificityAtSensitivity(\n    sensitivity, num_thresholds=200, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.SquaredHinge": "tf.keras.metrics.SquaredHinge(\n    name='squared_hinge', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.Sum": "tf.keras.metrics.Sum(\n    name='sum', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.TopKCategoricalAccuracy": "tf.keras.metrics.TopKCategoricalAccuracy(\n    k=5, name='top_k_categorical_accuracy', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.TrueNegatives": "tf.keras.metrics.TrueNegatives(\n    thresholds=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.TruePositives": "tf.keras.metrics.TruePositives(\n    thresholds=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.binary_accuracy": "tf.keras.metrics.binary_accuracy(\n    y_true, y_pred, threshold=0.5\n)\n"
  },
  {
    "tf.keras.metrics.binary_crossentropy": "tf.keras.metrics.binary_crossentropy(\n    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.binary_focal_crossentropy": "tf.keras.metrics.binary_focal_crossentropy(\n    y_true,\n    y_pred,\n    apply_class_balancing=False,\n    alpha=0.25,\n    gamma=2.0,\n    from_logits=False,\n    label_smoothing=0.0,\n    axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.categorical_accuracy": "tf.keras.metrics.categorical_accuracy(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.categorical_crossentropy": "tf.keras.metrics.categorical_crossentropy(\n    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.deserialize": "tf.keras.metrics.deserialize(\n    config, custom_objects=None\n)\n"
  },
  {
    "tf.keras.metrics.get": "tf.keras.metrics.get(\n    identifier\n)\n"
  },
  {
    "tf.keras.metrics.hinge": "tf.keras.metrics.hinge(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.log_cosh": "tf.keras.losses.log_cosh(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.log_cosh": "tf.keras.losses.log_cosh(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.poisson": "tf.keras.metrics.poisson(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.serialize": "tf.keras.metrics.serialize(\n    metric\n)\n"
  },
  {
    "tf.keras.metrics.sparse_categorical_accuracy": "tf.keras.metrics.sparse_categorical_accuracy(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.sparse_categorical_crossentropy": "tf.keras.metrics.sparse_categorical_crossentropy(\n    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None\n)\n"
  },
  {
    "tf.keras.metrics.sparse_top_k_categorical_accuracy": "tf.keras.metrics.sparse_top_k_categorical_accuracy(\n    y_true, y_pred, k=5\n)\n"
  },
  {
    "tf.keras.metrics.squared_hinge": "tf.keras.metrics.squared_hinge(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.top_k_categorical_accuracy": "tf.keras.metrics.top_k_categorical_accuracy(\n    y_true, y_pred, k=5\n)\n"
  },
  {
    "tf.keras.mixed_precision.Policy": "tf.keras.mixed_precision.Policy(\n    name\n)\n"
  },
  {
    "tf.keras.mixed_precision.set_global_policy": "tf.keras.mixed_precision.set_global_policy(\n    policy\n)\n"
  },
  {
    "tf.keras.Model": "tf.keras.Model(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.keras.Sequential": "tf.keras.Sequential(\n    layers=None, name=None\n)\n"
  },
  {
    "tf.keras.models.clone_model": "tf.keras.models.clone_model(\n    model, input_tensors=None, clone_function=None\n)\n"
  },
  {
    "tf.keras.models.experimental.SharpnessAwareMinimization": "tf.keras.models.experimental.SharpnessAwareMinimization(\n    model, rho=0.05, num_batch_splits=None, name=None\n)\n"
  },
  {
    "tf.keras.models.load_model": "tf.keras.models.load_model(\n    filepath, custom_objects=None, compile=True, options=None\n)\n"
  },
  {
    "tf.keras.models.model_from_config": "tf.keras.models.model_from_config(\n    config, custom_objects=None\n)\n"
  },
  {
    "tf.keras.models.model_from_json": "tf.keras.models.model_from_json(\n    json_string, custom_objects=None\n)\n"
  },
  {
    "tf.keras.models.model_from_yaml": "tf.keras.models.model_from_yaml(\n    yaml_string, custom_objects=None\n)\n"
  },
  {
    "tf.keras.models.save_model": "tf.keras.models.save_model(\n    model,\n    filepath,\n    overwrite=True,\n    include_optimizer=True,\n    save_format=None,\n    signatures=None,\n    options=None,\n    save_traces=True\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adadelta": "tf.keras.optimizers.experimental.Adadelta(\n    learning_rate=0.001,\n    rho=0.95,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adadelta',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adagrad": "tf.keras.optimizers.experimental.Adagrad(\n    learning_rate=0.001,\n    initial_accumulator_value=0.1,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adagrad',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.Adam": "tf.keras.optimizers.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adamax": "tf.keras.optimizers.experimental.Adamax(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adamax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Ftrl": "tf.keras.optimizers.experimental.Ftrl(\n    learning_rate=0.001,\n    learning_rate_power=-0.5,\n    initial_accumulator_value=0.1,\n    l1_regularization_strength=0.0,\n    l2_regularization_strength=0.0,\n    l2_shrinkage_regularization_strength=0.0,\n    beta=0.0,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Ftrl',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Nadam": "tf.keras.optimizers.experimental.Nadam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Nadam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.Optimizer": "tf.keras.optimizers.Optimizer(\n    name,\n    weight_decay=0,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.RMSprop": "tf.keras.optimizers.experimental.RMSprop(\n    learning_rate=0.001,\n    rho=0.9,\n    momentum=0.0,\n    epsilon=1e-07,\n    centered=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=100,\n    jit_compile=True,\n    name='RMSprop',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.SGD": "tf.keras.optimizers.experimental.SGD(\n    learning_rate=0.01,\n    momentum=0.0,\n    nesterov=False,\n    amsgrad=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='SGD',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.deserialize": "tf.keras.optimizers.deserialize(\n    config, custom_objects=None, **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adadelta": "tf.keras.optimizers.experimental.Adadelta(\n    learning_rate=0.001,\n    rho=0.95,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adadelta',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adafactor": "tf.keras.optimizers.experimental.Adafactor(\n    learning_rate=0.001,\n    beta_2_decay=-0.8,\n    epsilon_1=1e-30,\n    epsilon_2=0.001,\n    clip_threshold=1.0,\n    relative_step=True,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adafactor',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adagrad": "tf.keras.optimizers.experimental.Adagrad(\n    learning_rate=0.001,\n    initial_accumulator_value=0.1,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adagrad',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.Adam": "tf.keras.optimizers.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.AdamW": "tf.keras.optimizers.experimental.AdamW(\n    learning_rate=0.001,\n    weight_decay=0.004,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='AdamW',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adamax": "tf.keras.optimizers.experimental.Adamax(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adamax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Ftrl": "tf.keras.optimizers.experimental.Ftrl(\n    learning_rate=0.001,\n    learning_rate_power=-0.5,\n    initial_accumulator_value=0.1,\n    l1_regularization_strength=0.0,\n    l2_regularization_strength=0.0,\n    l2_shrinkage_regularization_strength=0.0,\n    beta=0.0,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Ftrl',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Nadam": "tf.keras.optimizers.experimental.Nadam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Nadam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.Optimizer": "tf.keras.optimizers.Optimizer(\n    name,\n    weight_decay=0,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.RMSprop": "tf.keras.optimizers.experimental.RMSprop(\n    learning_rate=0.001,\n    rho=0.9,\n    momentum=0.0,\n    epsilon=1e-07,\n    centered=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=100,\n    jit_compile=True,\n    name='RMSprop',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.SGD": "tf.keras.optimizers.experimental.SGD(\n    learning_rate=0.01,\n    momentum=0.0,\n    nesterov=False,\n    amsgrad=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='SGD',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.get": "tf.keras.optimizers.get(\n    identifier, **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Adadelta": "tf.keras.optimizers.legacy.Adadelta(\n    learning_rate=0.001,\n    rho=0.95,\n    epsilon=1e-07,\n    name='Adadelta',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Adagrad": "tf.keras.optimizers.legacy.Adagrad(\n    learning_rate=0.001,\n    initial_accumulator_value=0.1,\n    epsilon=1e-07,\n    name='Adagrad',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Adam": "tf.keras.optimizers.legacy.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    name='Adam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Adamax": "tf.keras.optimizers.legacy.Adamax(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    name='Adamax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Ftrl": "tf.keras.optimizers.legacy.Ftrl(\n    learning_rate=0.001,\n    learning_rate_power=-0.5,\n    initial_accumulator_value=0.1,\n    l1_regularization_strength=0.0,\n    l2_regularization_strength=0.0,\n    name='Ftrl',\n    l2_shrinkage_regularization_strength=0.0,\n    beta=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Nadam": "tf.keras.optimizers.legacy.Nadam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    name='Nadam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Optimizer": "tf.keras.optimizers.legacy.Optimizer(\n    name, gradient_aggregator=None, gradient_transformers=None, **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.RMSprop": "tf.keras.optimizers.legacy.RMSprop(\n    learning_rate=0.001,\n    rho=0.9,\n    momentum=0.0,\n    epsilon=1e-07,\n    centered=False,\n    name='RMSprop',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.SGD": "tf.keras.optimizers.legacy.SGD(\n    learning_rate=0.01,\n    momentum=0.0,\n    nesterov=False,\n    name='SGD',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.CosineDecay": "tf.keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate, decay_steps, alpha=0.0, name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.CosineDecayRestarts": "tf.keras.optimizers.schedules.CosineDecayRestarts(\n    initial_learning_rate,\n    first_decay_steps,\n    t_mul=2.0,\n    m_mul=1.0,\n    alpha=0.0,\n    name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.ExponentialDecay": "tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate, decay_steps, decay_rate, staircase=False, name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.InverseTimeDecay": "tf.keras.optimizers.schedules.InverseTimeDecay(\n    initial_learning_rate, decay_steps, decay_rate, staircase=False, name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.PiecewiseConstantDecay": "tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n    boundaries, values, name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.PolynomialDecay": "tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate,\n    decay_steps,\n    end_learning_rate=0.0001,\n    power=1.0,\n    cycle=False,\n    name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.deserialize": "tf.keras.optimizers.schedules.deserialize(\n    config, custom_objects=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.serialize": "tf.keras.optimizers.schedules.serialize(\n    learning_rate_schedule\n)\n"
  },
  {
    "tf.keras.optimizers.serialize": "tf.keras.optimizers.serialize(\n    optimizer\n)\n"
  },
  {
    "tf.keras.preprocessing.image.DirectoryIterator": "tf.keras.preprocessing.image.DirectoryIterator(\n    directory,\n    image_data_generator,\n    target_size=(256, 256),\n    color_mode='rgb',\n    classes=None,\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=None,\n    data_format=None,\n    save_to_dir=None,\n    save_prefix='',\n    save_format='png',\n    follow_links=False,\n    subset=None,\n    interpolation='nearest',\n    keep_aspect_ratio=False,\n    dtype=None\n)\n"
  },
  {
    "tf.keras.preprocessing.image.ImageDataGenerator": "tf.keras.preprocessing.image.ImageDataGenerator(\n    featurewise_center=False,\n    samplewise_center=False,\n    featurewise_std_normalization=False,\n    samplewise_std_normalization=False,\n    zca_whitening=False,\n    zca_epsilon=1e-06,\n    rotation_range=0,\n    width_shift_range=0.0,\n    height_shift_range=0.0,\n    brightness_range=None,\n    shear_range=0.0,\n    zoom_range=0.0,\n    channel_shift_range=0.0,\n    fill_mode='nearest',\n    cval=0.0,\n    horizontal_flip=False,\n    vertical_flip=False,\n    rescale=None,\n    preprocessing_function=None,\n    data_format=None,\n    validation_split=0.0,\n    interpolation_order=1,\n    dtype=None\n)\n"
  },
  {
    "tf.keras.preprocessing.image.Iterator": "tf.keras.preprocessing.image.Iterator(\n    n, batch_size, shuffle, seed\n)\n"
  },
  {
    "tf.keras.preprocessing.image.NumpyArrayIterator": "tf.keras.preprocessing.image.NumpyArrayIterator(\n    x,\n    y,\n    image_data_generator,\n    batch_size=32,\n    shuffle=False,\n    sample_weight=None,\n    seed=None,\n    data_format=None,\n    save_to_dir=None,\n    save_prefix='',\n    save_format='png',\n    subset=None,\n    ignore_class_split=False,\n    dtype=None\n)\n"
  },
  {
    "tf.keras.preprocessing.image.apply_affine_transform": "tf.keras.preprocessing.image.apply_affine_transform(\n    x,\n    theta=0,\n    tx=0,\n    ty=0,\n    shear=0,\n    zx=1,\n    zy=1,\n    row_axis=1,\n    col_axis=2,\n    channel_axis=0,\n    fill_mode='nearest',\n    cval=0.0,\n    order=1\n)\n"
  },
  {
    "tf.keras.preprocessing.image.apply_brightness_shift": "tf.keras.preprocessing.image.apply_brightness_shift(\n    x, brightness, scale=True\n)\n"
  },
  {
    "tf.keras.preprocessing.image.apply_channel_shift": "tf.keras.preprocessing.image.apply_channel_shift(\n    x, intensity, channel_axis=0\n)\n"
  },
  {
    "tf.keras.utils.array_to_img": "tf.keras.utils.array_to_img(\n    x, data_format=None, scale=True, dtype=None\n)\n"
  },
  {
    "tf.keras.utils.img_to_array": "tf.keras.utils.img_to_array(\n    img, data_format=None, dtype=None\n)\n"
  },
  {
    "tf.keras.utils.load_img": "tf.keras.utils.load_img(\n    path,\n    grayscale=False,\n    color_mode='rgb',\n    target_size=None,\n    interpolation='nearest',\n    keep_aspect_ratio=False\n)\n"
  },
  {
    "tf.keras.preprocessing.image.random_brightness": "tf.keras.preprocessing.image.random_brightness(\n    x, brightness_range, scale=True\n)\n"
  },
  {
    "tf.keras.preprocessing.image.random_channel_shift": "tf.keras.preprocessing.image.random_channel_shift(\n    x, intensity_range, channel_axis=0\n)\n"
  },
  {
    "tf.keras.preprocessing.image.random_rotation": "tf.keras.preprocessing.image.random_rotation(\n    x,\n    rg,\n    row_axis=1,\n    col_axis=2,\n    channel_axis=0,\n    fill_mode='nearest',\n    cval=0.0,\n    interpolation_order=1\n)\n"
  },
  {
    "tf.keras.preprocessing.image.random_shear": "tf.keras.preprocessing.image.random_shear(\n    x,\n    intensity,\n    row_axis=1,\n    col_axis=2,\n    channel_axis=0,\n    fill_mode='nearest',\n    cval=0.0,\n    interpolation_order=1\n)\n"
  },
  {
    "tf.keras.preprocessing.image.random_shift": "tf.keras.preprocessing.image.random_shift(\n    x,\n    wrg,\n    hrg,\n    row_axis=1,\n    col_axis=2,\n    channel_axis=0,\n    fill_mode='nearest',\n    cval=0.0,\n    interpolation_order=1\n)\n"
  },
  {
    "tf.keras.preprocessing.image.random_zoom": "tf.keras.preprocessing.image.random_zoom(\n    x,\n    zoom_range,\n    row_axis=1,\n    col_axis=2,\n    channel_axis=0,\n    fill_mode='nearest',\n    cval=0.0,\n    interpolation_order=1\n)\n"
  },
  {
    "tf.keras.utils.save_img": "tf.keras.utils.save_img(\n    path, x, data_format=None, file_format=None, scale=True, **kwargs\n)\n"
  },
  {
    "tf.keras.preprocessing.image.smart_resize": "tf.keras.preprocessing.image.smart_resize(\n    x, size, interpolation='bilinear'\n)\n"
  },
  {
    "tf.keras.utils.image_dataset_from_directory": "tf.keras.utils.image_dataset_from_directory(\n    directory,\n    labels='inferred',\n    label_mode='int',\n    class_names=None,\n    color_mode='rgb',\n    batch_size=32,\n    image_size=(256, 256),\n    shuffle=True,\n    seed=None,\n    validation_split=None,\n    subset=None,\n    interpolation='bilinear',\n    follow_links=False,\n    crop_to_aspect_ratio=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.preprocessing.sequence.TimeseriesGenerator": "tf.keras.preprocessing.sequence.TimeseriesGenerator(\n    data,\n    targets,\n    length,\n    sampling_rate=1,\n    stride=1,\n    start_index=0,\n    end_index=None,\n    shuffle=False,\n    reverse=False,\n    batch_size=128\n)\n"
  },
  {
    "tf.keras.preprocessing.sequence.make_sampling_table": "tf.keras.preprocessing.sequence.make_sampling_table(\n    size, sampling_factor=1e-05\n)\n"
  },
  {
    "tf.keras.utils.pad_sequences": "tf.keras.utils.pad_sequences(\n    sequences,\n    maxlen=None,\n    dtype='int32',\n    padding='pre',\n    truncating='pre',\n    value=0.0\n)\n"
  },
  {
    "tf.keras.preprocessing.sequence.skipgrams": "tf.keras.preprocessing.sequence.skipgrams(\n    sequence,\n    vocabulary_size,\n    window_size=4,\n    negative_samples=1.0,\n    shuffle=True,\n    categorical=False,\n    sampling_table=None,\n    seed=None\n)\n"
  },
  {
    "tf.keras.preprocessing.text.Tokenizer": "tf.keras.preprocessing.text.Tokenizer(\n    num_words=None,\n    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n    lower=True,\n    split=' ',\n    char_level=False,\n    oov_token=None,\n    analyzer=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.preprocessing.text.hashing_trick": "tf.keras.preprocessing.text.hashing_trick(\n    text,\n    n,\n    hash_function=None,\n    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n    lower=True,\n    split=' ',\n    analyzer=None\n)\n"
  },
  {
    "tf.keras.preprocessing.text.one_hot": "tf.keras.preprocessing.text.one_hot(\n    input_text,\n    n,\n    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n    lower=True,\n    split=' ',\n    analyzer=None\n)\n"
  },
  {
    "tf.keras.preprocessing.text.text_to_word_sequence": "tf.keras.preprocessing.text.text_to_word_sequence(\n    input_text,\n    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n    lower=True,\n    split=' '\n)\n"
  },
  {
    "tf.keras.preprocessing.text.tokenizer_from_json": "tf.keras.preprocessing.text.tokenizer_from_json(\n    json_string\n)\n"
  },
  {
    "tf.keras.utils.text_dataset_from_directory": "tf.keras.utils.text_dataset_from_directory(\n    directory,\n    labels='inferred',\n    label_mode='int',\n    class_names=None,\n    batch_size=32,\n    max_length=None,\n    shuffle=True,\n    seed=None,\n    validation_split=None,\n    subset=None,\n    follow_links=False\n)\n"
  },
  {
    "tf.keras.utils.timeseries_dataset_from_array": "tf.keras.utils.timeseries_dataset_from_array(\n    data,\n    targets,\n    sequence_length,\n    sequence_stride=1,\n    sampling_rate=1,\n    batch_size=128,\n    shuffle=False,\n    seed=None,\n    start_index=None,\n    end_index=None\n)\n"
  },
  {
    "tf.keras.regularizers.L1": "tf.keras.regularizers.L1(\n    l1=0.01, **kwargs\n)\n"
  },
  {
    "tf.keras.regularizers.L1L2": "tf.keras.regularizers.L1L2(\n    l1=0.0, l2=0.0\n)\n"
  },
  {
    "tf.keras.regularizers.L2": "tf.keras.regularizers.L2(\n    l2=0.01, **kwargs\n)\n"
  },
  {
    "tf.keras.regularizers.OrthogonalRegularizer": "tf.keras.regularizers.OrthogonalRegularizer(\n    factor=0.01, mode='rows'\n)\n"
  },
  {
    "tf.keras.regularizers.deserialize": "tf.keras.regularizers.deserialize(\n    config, custom_objects=None\n)\n"
  },
  {
    "tf.keras.regularizers.get": "tf.keras.regularizers.get(\n    identifier\n)\n"
  },
  {
    "tf.keras.regularizers.L1": "tf.keras.regularizers.L1(\n    l1=0.01, **kwargs\n)\n"
  },
  {
    "tf.keras.regularizers.l1_l2": "tf.keras.regularizers.l1_l2(\n    l1=0.01, l2=0.01\n)\n"
  },
  {
    "tf.keras.regularizers.L2": "tf.keras.regularizers.L2(\n    l2=0.01, **kwargs\n)\n"
  },
  {
    "tf.keras.regularizers.OrthogonalRegularizer": "tf.keras.regularizers.OrthogonalRegularizer(\n    factor=0.01, mode='rows'\n)\n"
  },
  {
    "tf.keras.regularizers.serialize": "tf.keras.regularizers.serialize(\n    regularizer\n)\n"
  },
  {
    "tf.keras.utils.custom_object_scope": "tf.keras.utils.custom_object_scope(\n    *args\n)\n"
  },
  {
    "tf.keras.utils.GeneratorEnqueuer": "tf.keras.utils.GeneratorEnqueuer(\n    generator, use_multiprocessing=False, random_seed=None\n)\n"
  },
  {
    "tf.keras.utils.OrderedEnqueuer": "tf.keras.utils.OrderedEnqueuer(\n    sequence, use_multiprocessing=False, shuffle=False\n)\n"
  },
  {
    "tf.keras.utils.Progbar": "tf.keras.utils.Progbar(\n    target,\n    width=30,\n    verbose=1,\n    interval=0.05,\n    stateful_metrics=None,\n    unit_name='step'\n)\n"
  },
  {
    "tf.keras.utils.SequenceEnqueuer": "tf.keras.utils.SequenceEnqueuer(\n    sequence, use_multiprocessing=False\n)\n"
  },
  {
    "tf.keras.utils.SidecarEvaluator": "tf.keras.utils.SidecarEvaluator(\n    model,\n    data,\n    checkpoint_dir,\n    steps=None,\n    max_evaluations=None,\n    callbacks=None\n)\n"
  },
  {
    "tf.keras.utils.array_to_img": "tf.keras.utils.array_to_img(\n    x, data_format=None, scale=True, dtype=None\n)\n"
  },
  {
    "tf.keras.utils.audio_dataset_from_directory": "tf.keras.utils.audio_dataset_from_directory(\n    directory,\n    labels='inferred',\n    label_mode='int',\n    class_names=None,\n    batch_size=32,\n    sampling_rate=None,\n    output_sequence_length=None,\n    ragged=False,\n    shuffle=True,\n    seed=None,\n    validation_split=None,\n    subset=None,\n    follow_links=False\n)\n"
  },
  {
    "tf.keras.utils.custom_object_scope": "tf.keras.utils.custom_object_scope(\n    *args\n)\n"
  },
  {
    "tf.keras.utils.deserialize_keras_object": "tf.keras.utils.deserialize_keras_object(\n    identifier,\n    module_objects=None,\n    custom_objects=None,\n    printable_module_name='object'\n)\n"
  },
  {
    "tf.keras.utils.experimental.DatasetCreator": "tf.keras.utils.experimental.DatasetCreator(\n    dataset_fn, input_options=None\n)\n"
  },
  {
    "tf.keras.utils.get_file": "tf.keras.utils.get_file(\n    fname=None,\n    origin=None,\n    untar=False,\n    md5_hash=None,\n    file_hash=None,\n    cache_subdir='datasets',\n    hash_algorithm='auto',\n    extract=False,\n    archive_format='auto',\n    cache_dir=None\n)\n"
  },
  {
    "tf.keras.utils.get_registered_name": "tf.keras.utils.get_registered_name(\n    obj\n)\n"
  },
  {
    "tf.keras.utils.get_registered_object": "tf.keras.utils.get_registered_object(\n    name, custom_objects=None, module_objects=None\n)\n"
  },
  {
    "tf.keras.utils.get_source_inputs": "tf.keras.utils.get_source_inputs(\n    tensor, layer=None, node_index=None\n)\n"
  },
  {
    "tf.keras.utils.image_dataset_from_directory": "tf.keras.utils.image_dataset_from_directory(\n    directory,\n    labels='inferred',\n    label_mode='int',\n    class_names=None,\n    color_mode='rgb',\n    batch_size=32,\n    image_size=(256, 256),\n    shuffle=True,\n    seed=None,\n    validation_split=None,\n    subset=None,\n    interpolation='bilinear',\n    follow_links=False,\n    crop_to_aspect_ratio=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.utils.img_to_array": "tf.keras.utils.img_to_array(\n    img, data_format=None, dtype=None\n)\n"
  },
  {
    "tf.keras.utils.load_img": "tf.keras.utils.load_img(\n    path,\n    grayscale=False,\n    color_mode='rgb',\n    target_size=None,\n    interpolation='nearest',\n    keep_aspect_ratio=False\n)\n"
  },
  {
    "tf.keras.utils.model_to_dot": "tf.keras.utils.model_to_dot(\n    model,\n    show_shapes=False,\n    show_dtype=False,\n    show_layer_names=True,\n    rankdir='TB',\n    expand_nested=False,\n    dpi=96,\n    subgraph=False,\n    layer_range=None,\n    show_layer_activations=False\n)\n"
  },
  {
    "tf.keras.utils.normalize": "tf.keras.utils.normalize(\n    x, axis=-1, order=2\n)\n"
  },
  {
    "tf.keras.utils.pack_x_y_sample_weight": "tf.keras.utils.pack_x_y_sample_weight(\n    x, y=None, sample_weight=None\n)\n"
  },
  {
    "tf.keras.utils.pad_sequences": "tf.keras.utils.pad_sequences(\n    sequences,\n    maxlen=None,\n    dtype='int32',\n    padding='pre',\n    truncating='pre',\n    value=0.0\n)\n"
  },
  {
    "tf.keras.utils.plot_model": "tf.keras.utils.plot_model(\n    model,\n    to_file='model.png',\n    show_shapes=False,\n    show_dtype=False,\n    show_layer_names=True,\n    rankdir='TB',\n    expand_nested=False,\n    dpi=96,\n    layer_range=None,\n    show_layer_activations=False\n)\n"
  },
  {
    "tf.keras.utils.register_keras_serializable": "tf.keras.utils.register_keras_serializable(\n    package='Custom', name=None\n)\n"
  },
  {
    "tf.keras.utils.save_img": "tf.keras.utils.save_img(\n    path, x, data_format=None, file_format=None, scale=True, **kwargs\n)\n"
  },
  {
    "tf.keras.utils.serialize_keras_object": "tf.keras.utils.serialize_keras_object(\n    instance\n)\n"
  },
  {
    "tf.keras.utils.set_random_seed": "tf.keras.utils.set_random_seed(\n    seed\n)\n"
  },
  {
    "tf.keras.utils.split_dataset": "tf.keras.utils.split_dataset(\n    dataset, left_size=None, right_size=None, shuffle=False, seed=None\n)\n"
  },
  {
    "tf.keras.utils.text_dataset_from_directory": "tf.keras.utils.text_dataset_from_directory(\n    directory,\n    labels='inferred',\n    label_mode='int',\n    class_names=None,\n    batch_size=32,\n    max_length=None,\n    shuffle=True,\n    seed=None,\n    validation_split=None,\n    subset=None,\n    follow_links=False\n)\n"
  },
  {
    "tf.keras.utils.timeseries_dataset_from_array": "tf.keras.utils.timeseries_dataset_from_array(\n    data,\n    targets,\n    sequence_length,\n    sequence_stride=1,\n    sampling_rate=1,\n    batch_size=128,\n    shuffle=False,\n    seed=None,\n    start_index=None,\n    end_index=None\n)\n"
  },
  {
    "tf.keras.utils.to_categorical": "tf.keras.utils.to_categorical(\n    y, num_classes=None, dtype='float32'\n)\n"
  },
  {
    "tf.keras.utils.unpack_x_y_sample_weight": "tf.keras.utils.unpack_x_y_sample_weight(\n    data\n)\n"
  },
  {
    "tf.keras.utils.warmstart_embedding_matrix": "tf.keras.utils.warmstart_embedding_matrix(\n    base_vocabulary,\n    new_vocabulary,\n    base_embeddings,\n    new_embeddings_initializer='uniform'\n)\n"
  },
  {
    "tf.math.less": "tf.math.less(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.less_equal": "tf.math.less_equal(\n    x, y, name=None\n)\n"
  },
  {
    "tf.linalg.LinearOperator": "tf.linalg.LinearOperator(\n    dtype,\n    graph_parents=None,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name=None,\n    parameters=None\n)\n"
  },
  {
    "tf.linalg.LinearOperatorAdjoint": "tf.linalg.LinearOperatorAdjoint(\n    operator,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name=None\n)\n"
  },
  {
    "tf.linalg.LinearOperatorBlockDiag": "tf.linalg.LinearOperatorBlockDiag(\n    operators,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=True,\n    name=None\n)\n"
  },
  {
    "tf.linalg.LinearOperatorBlockLowerTriangular": "tf.linalg.LinearOperatorBlockLowerTriangular(\n    operators,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name='LinearOperatorBlockLowerTriangular'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorCirculant": "tf.linalg.LinearOperatorCirculant(\n    spectrum,\n    input_output_dtype=tf.dtypes.complex64,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=True,\n    name='LinearOperatorCirculant'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorCirculant2D": "tf.linalg.LinearOperatorCirculant2D(\n    spectrum,\n    input_output_dtype=tf.dtypes.complex64,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=True,\n    name='LinearOperatorCirculant2D'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorCirculant3D": "tf.linalg.LinearOperatorCirculant3D(\n    spectrum,\n    input_output_dtype=tf.dtypes.complex64,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=True,\n    name='LinearOperatorCirculant3D'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorComposition": "tf.linalg.LinearOperatorComposition(\n    operators,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name=None\n)\n"
  },
  {
    "tf.linalg.LinearOperatorDiag": "tf.linalg.LinearOperatorDiag(\n    diag,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name='LinearOperatorDiag'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorFullMatrix": "tf.linalg.LinearOperatorFullMatrix(\n    matrix,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name='LinearOperatorFullMatrix'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorHouseholder": "tf.linalg.LinearOperatorHouseholder(\n    reflection_axis,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name='LinearOperatorHouseholder'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorIdentity": "tf.linalg.LinearOperatorIdentity(\n    num_rows,\n    batch_shape=None,\n    dtype=None,\n    is_non_singular=True,\n    is_self_adjoint=True,\n    is_positive_definite=True,\n    is_square=True,\n    assert_proper_shapes=False,\n    name='LinearOperatorIdentity'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorInversion": "tf.linalg.LinearOperatorInversion(\n    operator,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name=None\n)\n"
  },
  {
    "tf.linalg.LinearOperatorKronecker": "tf.linalg.LinearOperatorKronecker(\n    operators,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name=None\n)\n"
  },
  {
    "tf.linalg.LinearOperatorLowRankUpdate": "tf.linalg.LinearOperatorLowRankUpdate(\n    base_operator,\n    u,\n    diag_update=None,\n    v=None,\n    is_diag_update_positive=None,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name='LinearOperatorLowRankUpdate'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorLowerTriangular": "tf.linalg.LinearOperatorLowerTriangular(\n    tril,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name='LinearOperatorLowerTriangular'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorPermutation": "tf.linalg.LinearOperatorPermutation(\n    perm,\n    dtype=tf.dtypes.float32,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name='LinearOperatorPermutation'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorScaledIdentity": "tf.linalg.LinearOperatorScaledIdentity(\n    num_rows,\n    multiplier,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=True,\n    assert_proper_shapes=False,\n    name='LinearOperatorScaledIdentity'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorToeplitz": "tf.linalg.LinearOperatorToeplitz(\n    col,\n    row,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name='LinearOperatorToeplitz'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorTridiag": "tf.linalg.LinearOperatorTridiag(\n    diagonals,\n    diagonals_format=_COMPACT,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name='LinearOperatorTridiag'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorZeros": "tf.linalg.LinearOperatorZeros(\n    num_rows,\n    num_columns=None,\n    batch_shape=None,\n    dtype=None,\n    is_non_singular=False,\n    is_self_adjoint=True,\n    is_positive_definite=False,\n    is_square=True,\n    assert_proper_shapes=False,\n    name='LinearOperatorZeros'\n)\n"
  },
  {
    "tf.linalg.adjoint": "tf.linalg.adjoint(\n    matrix, name=None\n)\n"
  },
  {
    "tf.linalg.band_part": "tf.linalg.band_part(\n    input, num_lower, num_upper, name=None\n)\n"
  },
  {
    "tf.linalg.banded_triangular_solve": "tf.linalg.banded_triangular_solve(\n    bands, rhs, lower=True, adjoint=False, name=None\n)\n"
  },
  {
    "tf.linalg.cholesky": "tf.linalg.cholesky(\n    input, name=None\n)\n"
  },
  {
    "tf.linalg.cholesky_solve": "tf.linalg.cholesky_solve(\n    chol, rhs, name=None\n)\n"
  },
  {
    "tf.linalg.cross": "tf.linalg.cross(\n    a, b, name=None\n)\n"
  },
  {
    "tf.linalg.det": "tf.linalg.det(\n    input, name=None\n)\n"
  },
  {
    "tf.linalg.diag": "tf.linalg.diag(\n    diagonal,\n    name='diag',\n    k=0,\n    num_rows=-1,\n    num_cols=-1,\n    padding_value=0,\n    align='RIGHT_LEFT'\n)\n"
  },
  {
    "tf.linalg.diag_part": "tf.linalg.diag_part(\n    input,\n    name='diag_part',\n    k=0,\n    padding_value=0,\n    align='RIGHT_LEFT'\n)\n"
  },
  {
    "tf.linalg.eig": "tf.linalg.eig(\n    tensor, name=None\n)\n"
  },
  {
    "tf.linalg.eigh": "tf.linalg.eigh(\n    tensor, name=None\n)\n"
  },
  {
    "tf.linalg.eigh_tridiagonal": "tf.linalg.eigh_tridiagonal(\n    alpha,\n    beta,\n    eigvals_only=True,\n    select='a',\n    select_range=None,\n    tol=None,\n    name=None\n)\n"
  },
  {
    "tf.linalg.eigvals": "tf.linalg.eigvals(\n    tensor, name=None\n)\n"
  },
  {
    "tf.linalg.eigvalsh": "tf.linalg.eigvalsh(\n    tensor, name=None\n)\n"
  },
  {
    "tf.einsum": "tf.einsum(\n    equation, *inputs, **kwargs\n)\n"
  },
  {
    "tf.linalg.experimental.conjugate_gradient": "tf.linalg.experimental.conjugate_gradient(\n    operator,\n    rhs,\n    preconditioner=None,\n    x=None,\n    tol=1e-05,\n    max_iter=20,\n    name='conjugate_gradient'\n)\n"
  },
  {
    "tf.linalg.expm": "tf.linalg.expm(\n    input, name=None\n)\n"
  },
  {
    "tf.eye": "tf.eye(\n    num_rows,\n    num_columns=None,\n    batch_shape=None,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.linalg.global_norm": "tf.linalg.global_norm(\n    t_list, name=None\n)\n"
  },
  {
    "tf.linalg.inv": "tf.linalg.inv(\n    input, adjoint=False, name=None\n)\n"
  },
  {
    "tf.math.l2_normalize": "tf.math.l2_normalize(\n    x, axis=None, epsilon=1e-12, name=None, dim=None\n)\n"
  },
  {
    "tf.linalg.logdet": "tf.linalg.logdet(\n    matrix, name=None\n)\n"
  },
  {
    "tf.linalg.logm": "tf.linalg.logm(\n    input, name=None\n)\n"
  },
  {
    "tf.linalg.lstsq": "tf.linalg.lstsq(\n    matrix, rhs, l2_regularizer=0.0, fast=True, name=None\n)\n"
  },
  {
    "tf.linalg.lu": "tf.linalg.lu(\n    input,\n    output_idx_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.linalg.lu_matrix_inverse": "tf.linalg.lu_matrix_inverse(\n    lower_upper, perm, validate_args=False, name=None\n)\n"
  },
  {
    "tf.linalg.lu_reconstruct": "tf.linalg.lu_reconstruct(\n    lower_upper, perm, validate_args=False, name=None\n)\n"
  },
  {
    "tf.linalg.lu_solve": "tf.linalg.lu_solve(\n    lower_upper, perm, rhs, validate_args=False, name=None\n)\n"
  },
  {
    "tf.linalg.matmul": "tf.linalg.matmul(\n    a,\n    b,\n    transpose_a=False,\n    transpose_b=False,\n    adjoint_a=False,\n    adjoint_b=False,\n    a_is_sparse=False,\n    b_is_sparse=False,\n    output_type=None,\n    name=None\n)\n"
  },
  {
    "tf.linalg.matrix_rank": "tf.linalg.matrix_rank(\n    a, tol=None, validate_args=False, name=None\n)\n"
  },
  {
    "tf.linalg.matrix_transpose": "tf.linalg.matrix_transpose(\n    a, name='matrix_transpose', conjugate=False\n)\n"
  },
  {
    "tf.linalg.matvec": "tf.linalg.matvec(\n    a,\n    b,\n    transpose_a=False,\n    adjoint_a=False,\n    a_is_sparse=False,\n    b_is_sparse=False,\n    name=None\n)\n"
  },
  {
    "tf.norm": "tf.norm(\n    tensor, ord='euclidean', axis=None, keepdims=None, name=None\n)\n"
  },
  {
    "tf.linalg.normalize": "tf.linalg.normalize(\n    tensor, ord='euclidean', axis=None, name=None\n)\n"
  },
  {
    "tf.linalg.pinv": "tf.linalg.pinv(\n    a, rcond=None, validate_args=False, name=None\n)\n"
  },
  {
    "tf.linalg.qr": "tf.linalg.qr(\n    input, full_matrices=False, name=None\n)\n"
  },
  {
    "tf.linalg.set_diag": "tf.linalg.set_diag(\n    input,\n    diagonal,\n    name='set_diag',\n    k=0,\n    align='RIGHT_LEFT'\n)\n"
  },
  {
    "tf.linalg.slogdet": "tf.linalg.slogdet(\n    input, name=None\n)\n"
  },
  {
    "tf.linalg.solve": "tf.linalg.solve(\n    matrix, rhs, adjoint=False, name=None\n)\n"
  },
  {
    "tf.linalg.sqrtm": "tf.linalg.sqrtm(\n    input, name=None\n)\n"
  },
  {
    "tf.linalg.svd": "tf.linalg.svd(\n    tensor, full_matrices=False, compute_uv=True, name=None\n)\n"
  },
  {
    "tf.linalg.tensor_diag": "tf.linalg.tensor_diag(\n    diagonal, name=None\n)\n"
  },
  {
    "tf.linalg.tensor_diag_part": "tf.linalg.tensor_diag_part(\n    input, name=None\n)\n"
  },
  {
    "tf.tensordot": "tf.tensordot(\n    a, b, axes, name=None\n)\n"
  },
  {
    "tf.linalg.trace": "tf.linalg.trace(\n    x, name=None\n)\n"
  },
  {
    "tf.linalg.triangular_solve": "tf.linalg.triangular_solve(\n    matrix, rhs, lower=True, adjoint=False, name=None\n)\n"
  },
  {
    "tf.linalg.tridiagonal_matmul": "tf.linalg.tridiagonal_matmul(\n    diagonals, rhs, diagonals_format='compact', name=None\n)\n"
  },
  {
    "tf.linalg.tridiagonal_solve": "tf.linalg.tridiagonal_solve(\n    diagonals,\n    rhs,\n    diagonals_format='compact',\n    transpose_rhs=False,\n    conjugate_rhs=False,\n    name=None,\n    partial_pivoting=True,\n    perturb_singular=False\n)\n"
  },
  {
    "tf.linspace": "tf.linspace(\n    start, stop, num, name=None, axis=0\n)\n"
  },
  {
    "tf.lite.Interpreter": "tf.lite.Interpreter(\n    model_path=None,\n    model_content=None,\n    experimental_delegates=None,\n    num_threads=None,\n    experimental_op_resolver_type=tf.lite.experimental.OpResolverType.AUTO,\n    experimental_preserve_all_tensors=False\n)\n"
  },
  {
    "tf.lite.RepresentativeDataset": "tf.lite.RepresentativeDataset(\n    input_gen\n)\n"
  },
  {
    "tf.lite.TFLiteConverter": "tf.lite.TFLiteConverter(\n    funcs, trackable_obj=None\n)\n"
  },
  {
    "tf.lite.TargetSpec": "tf.lite.TargetSpec(\n    supported_ops=None,\n    supported_types=None,\n    experimental_select_user_tf_ops=None,\n    experimental_supported_backends=None\n)\n"
  },
  {
    "tf.lite.experimental.QuantizationDebugOptions": "tf.lite.experimental.QuantizationDebugOptions(\n    layer_debug_metrics: Optional[Mapping[str, Callable[[np.ndarray], float]]] = None,\n    model_debug_metrics: Optional[Mapping[str, Callable[[Sequence[np.ndarray], Sequence[np.ndarray]],\n        float]]] = None,\n    layer_direct_compare_metrics: Optional[Mapping[str, Callable[[Sequence[np.ndarray], Sequence[np.ndarray],\n        float, int], float]]] = None,\n    denylisted_ops: Optional[List[str]] = None,\n    denylisted_nodes: Optional[List[str]] = None,\n    fully_quantize: bool = False\n) -> None\n"
  },
  {
    "tf.lite.experimental.QuantizationDebugger": "tf.lite.experimental.QuantizationDebugger(\n    quant_debug_model_path: Optional[str] = None,\n    quant_debug_model_content: Optional[bytes] = None,\n    float_model_path: Optional[str] = None,\n    float_model_content: Optional[bytes] = None,\n    debug_dataset: Optional[Callable[[], Iterable[Sequence[np.ndarray]]]] = None,\n    debug_options: Optional[tf.lite.experimental.QuantizationDebugOptions] = None,\n    converter: Optional[TFLiteConverter] = None\n) -> None\n"
  },
  {
    "tf.lite.experimental.authoring.compatible": "tf.lite.experimental.authoring.compatible(\n    target=None, converter_target_spec=None, **kwargs\n)\n"
  },
  {
    "tf.lite.experimental.load_delegate": "tf.lite.experimental.load_delegate(\n    library, options=None\n)\n"
  },
  {
    "tf.load_library": "tf.load_library(\n    library_location\n)\n"
  },
  {
    "tf.load_op_library": "tf.load_op_library(\n    library_filename\n)\n"
  },
  {
    "tf.math.logical_and": "tf.math.logical_and(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.logical_not": "tf.math.logical_not(\n    x, name=None\n)\n"
  },
  {
    "tf.math.logical_or": "tf.math.logical_or(\n    x, y, name=None\n)\n"
  },
  {
    "tf.lookup.KeyValueTensorInitializer": "tf.lookup.KeyValueTensorInitializer(\n    keys, values, key_dtype=None, value_dtype=None, name=None\n)\n"
  },
  {
    "tf.lookup.StaticHashTable": "tf.lookup.StaticHashTable(\n    initializer, default_value, name=None, experimental_is_anonymous=False\n)\n"
  },
  {
    "tf.lookup.StaticVocabularyTable": "tf.lookup.StaticVocabularyTable(\n    initializer,\n    num_oov_buckets,\n    lookup_key_dtype=None,\n    name=None,\n    experimental_is_anonymous=False\n)\n"
  },
  {
    "tf.lookup.TextFileInitializer": "tf.lookup.TextFileInitializer(\n    filename,\n    key_dtype,\n    key_index,\n    value_dtype,\n    value_index,\n    vocab_size=None,\n    delimiter='\\t',\n    name=None,\n    value_index_offset=0\n)\n"
  },
  {
    "tf.lookup.experimental.DenseHashTable": "tf.lookup.experimental.DenseHashTable(\n    key_dtype,\n    value_dtype,\n    default_value,\n    empty_key,\n    deleted_key,\n    initial_num_buckets=None,\n    name='MutableDenseHashTable',\n    checkpoint=True,\n    experimental_is_anonymous=False\n)\n"
  },
  {
    "tf.lookup.experimental.MutableHashTable": "tf.lookup.experimental.MutableHashTable(\n    key_dtype,\n    value_dtype,\n    default_value,\n    name='MutableHashTable',\n    checkpoint=True,\n    experimental_is_anonymous=False\n)\n"
  },
  {
    "tf.keras.losses.BinaryCrossentropy": "tf.keras.losses.BinaryCrossentropy(\n    from_logits=False,\n    label_smoothing=0.0,\n    axis=-1,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='binary_crossentropy'\n)\n"
  },
  {
    "tf.keras.losses.BinaryFocalCrossentropy": "tf.keras.losses.BinaryFocalCrossentropy(\n    apply_class_balancing=False,\n    alpha=0.25,\n    gamma=2.0,\n    from_logits=False,\n    label_smoothing=0.0,\n    axis=-1,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='binary_focal_crossentropy'\n)\n"
  },
  {
    "tf.keras.losses.CategoricalCrossentropy": "tf.keras.losses.CategoricalCrossentropy(\n    from_logits=False,\n    label_smoothing=0.0,\n    axis=-1,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='categorical_crossentropy'\n)\n"
  },
  {
    "tf.keras.losses.CategoricalHinge": "tf.keras.losses.CategoricalHinge(\n    reduction=losses_utils.ReductionV2.AUTO, name='categorical_hinge'\n)\n"
  },
  {
    "tf.keras.losses.CosineSimilarity": "tf.keras.losses.CosineSimilarity(\n    axis=-1,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='cosine_similarity'\n)\n"
  },
  {
    "tf.keras.losses.Hinge": "tf.keras.losses.Hinge(\n    reduction=losses_utils.ReductionV2.AUTO, name='hinge'\n)\n"
  },
  {
    "tf.keras.losses.Huber": "tf.keras.losses.Huber(\n    delta=1.0,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='huber_loss'\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.KLDivergence": "tf.keras.losses.KLDivergence(\n    reduction=losses_utils.ReductionV2.AUTO, name='kl_divergence'\n)\n"
  },
  {
    "tf.keras.losses.LogCosh": "tf.keras.losses.LogCosh(\n    reduction=losses_utils.ReductionV2.AUTO, name='log_cosh'\n)\n"
  },
  {
    "tf.keras.losses.Loss": "tf.keras.losses.Loss(\n    reduction=losses_utils.ReductionV2.AUTO, name=None\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.MeanAbsoluteError": "tf.keras.losses.MeanAbsoluteError(\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='mean_absolute_error'\n)\n"
  },
  {
    "tf.keras.losses.MeanAbsolutePercentageError": "tf.keras.losses.MeanAbsolutePercentageError(\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='mean_absolute_percentage_error'\n)\n"
  },
  {
    "tf.keras.losses.MeanSquaredError": "tf.keras.losses.MeanSquaredError(\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='mean_squared_error'\n)\n"
  },
  {
    "tf.keras.losses.MeanSquaredLogarithmicError": "tf.keras.losses.MeanSquaredLogarithmicError(\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='mean_squared_logarithmic_error'\n)\n"
  },
  {
    "tf.keras.losses.Poisson": "tf.keras.losses.Poisson(\n    reduction=losses_utils.ReductionV2.AUTO, name='poisson'\n)\n"
  },
  {
    "tf.keras.losses.SparseCategoricalCrossentropy": "tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=False,\n    ignore_class=None,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='sparse_categorical_crossentropy'\n)\n"
  },
  {
    "tf.keras.losses.SquaredHinge": "tf.keras.losses.SquaredHinge(\n    reduction=losses_utils.ReductionV2.AUTO, name='squared_hinge'\n)\n"
  },
  {
    "tf.keras.metrics.binary_crossentropy": "tf.keras.metrics.binary_crossentropy(\n    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.binary_focal_crossentropy": "tf.keras.metrics.binary_focal_crossentropy(\n    y_true,\n    y_pred,\n    apply_class_balancing=False,\n    alpha=0.25,\n    gamma=2.0,\n    from_logits=False,\n    label_smoothing=0.0,\n    axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.categorical_crossentropy": "tf.keras.metrics.categorical_crossentropy(\n    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n)\n"
  },
  {
    "tf.keras.losses.categorical_hinge": "tf.keras.losses.categorical_hinge(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.cosine_similarity": "tf.keras.losses.cosine_similarity(\n    y_true, y_pred, axis=-1\n)\n"
  },
  {
    "tf.keras.losses.deserialize": "tf.keras.losses.deserialize(\n    name, custom_objects=None\n)\n"
  },
  {
    "tf.keras.losses.get": "tf.keras.losses.get(\n    identifier\n)\n"
  },
  {
    "tf.keras.metrics.hinge": "tf.keras.metrics.hinge(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.huber": "tf.keras.losses.huber(\n    y_true, y_pred, delta=1.0\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.log_cosh": "tf.keras.losses.log_cosh(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.log_cosh": "tf.keras.losses.log_cosh(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.poisson": "tf.keras.metrics.poisson(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.serialize": "tf.keras.losses.serialize(\n    loss\n)\n"
  },
  {
    "tf.keras.metrics.sparse_categorical_crossentropy": "tf.keras.metrics.sparse_categorical_crossentropy(\n    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None\n)\n"
  },
  {
    "tf.keras.metrics.squared_hinge": "tf.keras.metrics.squared_hinge(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.make_ndarray": "tf.make_ndarray(\n    tensor\n)\n"
  },
  {
    "tf.make_tensor_proto": "tf.make_tensor_proto(\n    values, dtype=None, shape=None, verify_shape=False, allow_broadcast=False\n)\n"
  },
  {
    "tf.map_fn": "tf.map_fn(\n    fn,\n    elems,\n    dtype=None,\n    parallel_iterations=None,\n    back_prop=True,\n    swap_memory=False,\n    infer_shape=True,\n    name=None,\n    fn_output_signature=None\n)\n"
  },
  {
    "tf.math.abs": "tf.math.abs(\n    x, name=None\n)\n"
  },
  {
    "tf.math.accumulate_n": "tf.math.accumulate_n(\n    inputs, shape=None, tensor_dtype=None, name=None\n)\n"
  },
  {
    "tf.math.acos": "tf.math.acos(\n    x, name=None\n)\n"
  },
  {
    "tf.math.acosh": "tf.math.acosh(\n    x, name=None\n)\n"
  },
  {
    "tf.math.add": "tf.math.add(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.add_n": "tf.math.add_n(\n    inputs, name=None\n)\n"
  },
  {
    "tf.math.angle": "tf.math.angle(\n    input, name=None\n)\n"
  },
  {
    "tf.math.approx_max_k": "tf.math.approx_max_k(\n    operand,\n    k,\n    reduction_dimension=-1,\n    recall_target=0.95,\n    reduction_input_size_override=-1,\n    aggregate_to_topk=True,\n    name=None\n)\n"
  },
  {
    "tf.math.approx_min_k": "tf.math.approx_min_k(\n    operand,\n    k,\n    reduction_dimension=-1,\n    recall_target=0.95,\n    reduction_input_size_override=-1,\n    aggregate_to_topk=True,\n    name=None\n)\n"
  },
  {
    "tf.math.argmax": "tf.math.argmax(\n    input,\n    axis=None,\n    output_type=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.math.argmin": "tf.math.argmin(\n    input,\n    axis=None,\n    output_type=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.math.asin": "tf.math.asin(\n    x, name=None\n)\n"
  },
  {
    "tf.math.asinh": "tf.math.asinh(\n    x, name=None\n)\n"
  },
  {
    "tf.math.atan": "tf.math.atan(\n    x, name=None\n)\n"
  },
  {
    "tf.math.atan2": "tf.math.atan2(\n    y, x, name=None\n)\n"
  },
  {
    "tf.math.atanh": "tf.math.atanh(\n    x, name=None\n)\n"
  },
  {
    "tf.math.bessel_i0": "tf.math.bessel_i0(\n    x, name=None\n)\n"
  },
  {
    "tf.math.bessel_i0e": "tf.math.bessel_i0e(\n    x, name=None\n)\n"
  },
  {
    "tf.math.bessel_i1": "tf.math.bessel_i1(\n    x, name=None\n)\n"
  },
  {
    "tf.math.bessel_i1e": "tf.math.bessel_i1e(\n    x, name=None\n)\n"
  },
  {
    "tf.math.betainc": "tf.math.betainc(\n    a, b, x, name=None\n)\n"
  },
  {
    "tf.math.bincount": "tf.math.bincount(\n    arr,\n    weights=None,\n    minlength=None,\n    maxlength=None,\n    dtype=tf.dtypes.int32,\n    name=None,\n    axis=None,\n    binary_output=False\n)\n"
  },
  {
    "tf.math.ceil": "tf.math.ceil(\n    x, name=None\n)\n"
  },
  {
    "tf.math.confusion_matrix": "tf.math.confusion_matrix(\n    labels,\n    predictions,\n    num_classes=None,\n    weights=None,\n    dtype=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.math.conj": "tf.math.conj(\n    x, name=None\n)\n"
  },
  {
    "tf.math.cos": "tf.math.cos(\n    x, name=None\n)\n"
  },
  {
    "tf.math.cosh": "tf.math.cosh(\n    x, name=None\n)\n"
  },
  {
    "tf.math.count_nonzero": "tf.math.count_nonzero(\n    input,\n    axis=None,\n    keepdims=None,\n    dtype=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.math.cumprod": "tf.math.cumprod(\n    x, axis=0, exclusive=False, reverse=False, name=None\n)\n"
  },
  {
    "tf.math.cumsum": "tf.math.cumsum(\n    x, axis=0, exclusive=False, reverse=False, name=None\n)\n"
  },
  {
    "tf.math.cumulative_logsumexp": "tf.math.cumulative_logsumexp(\n    x, axis=0, exclusive=False, reverse=False, name=None\n)\n"
  },
  {
    "tf.math.digamma": "tf.math.digamma(\n    x, name=None\n)\n"
  },
  {
    "tf.math.divide": "tf.math.divide(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.divide_no_nan": "tf.math.divide_no_nan(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.equal": "tf.math.equal(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.erf": "tf.math.erf(\n    x, name=None\n)\n"
  },
  {
    "tf.math.erfc": "tf.math.erfc(\n    x, name=None\n)\n"
  },
  {
    "tf.math.erfcinv": "tf.math.erfcinv(\n    x, name=None\n)\n"
  },
  {
    "tf.math.erfinv": "tf.math.erfinv(\n    x, name=None\n)\n"
  },
  {
    "tf.math.exp": "tf.math.exp(\n    x, name=None\n)\n"
  },
  {
    "tf.math.expm1": "tf.math.expm1(\n    x, name=None\n)\n"
  },
  {
    "tf.math.floor": "tf.math.floor(\n    x, name=None\n)\n"
  },
  {
    "tf.math.floordiv": "tf.math.floordiv(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.floormod": "tf.math.floormod(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.greater": "tf.math.greater(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.greater_equal": "tf.math.greater_equal(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.igamma": "tf.math.igamma(\n    a, x, name=None\n)\n"
  },
  {
    "tf.math.igammac": "tf.math.igammac(\n    a, x, name=None\n)\n"
  },
  {
    "tf.math.imag": "tf.math.imag(\n    input, name=None\n)\n"
  },
  {
    "tf.math.in_top_k": "tf.math.in_top_k(\n    targets, predictions, k, name=None\n)\n"
  },
  {
    "tf.math.invert_permutation": "tf.math.invert_permutation(\n    x, name=None\n)\n"
  },
  {
    "tf.math.is_finite": "tf.math.is_finite(\n    x, name=None\n)\n"
  },
  {
    "tf.math.is_inf": "tf.math.is_inf(\n    x, name=None\n)\n"
  },
  {
    "tf.math.is_nan": "tf.math.is_nan(\n    x, name=None\n)\n"
  },
  {
    "tf.math.is_non_decreasing": "tf.math.is_non_decreasing(\n    x, name=None\n)\n"
  },
  {
    "tf.math.is_strictly_increasing": "tf.math.is_strictly_increasing(\n    x, name=None\n)\n"
  },
  {
    "tf.math.l2_normalize": "tf.math.l2_normalize(\n    x, axis=None, epsilon=1e-12, name=None, dim=None\n)\n"
  },
  {
    "tf.math.lbeta": "tf.math.lbeta(\n    x, name=None\n)\n"
  },
  {
    "tf.math.less": "tf.math.less(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.less_equal": "tf.math.less_equal(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.lgamma": "tf.math.lgamma(\n    x, name=None\n)\n"
  },
  {
    "tf.math.log": "tf.math.log(\n    x, name=None\n)\n"
  },
  {
    "tf.math.log1p": "tf.math.log1p(\n    x, name=None\n)\n"
  },
  {
    "tf.math.log_sigmoid": "tf.math.log_sigmoid(\n    x, name=None\n)\n"
  },
  {
    "tf.nn.log_softmax": "tf.nn.log_softmax(\n    logits, axis=None, name=None\n)\n"
  },
  {
    "tf.math.logical_and": "tf.math.logical_and(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.logical_not": "tf.math.logical_not(\n    x, name=None\n)\n"
  },
  {
    "tf.math.logical_or": "tf.math.logical_or(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.logical_xor": "tf.math.logical_xor(\n    x, y, name='LogicalXor'\n)\n"
  },
  {
    "tf.math.maximum": "tf.math.maximum(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.minimum": "tf.math.minimum(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.floormod": "tf.math.floormod(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.multiply": "tf.math.multiply(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.multiply_no_nan": "tf.math.multiply_no_nan(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.ndtri": "tf.math.ndtri(\n    x, name=None\n)\n"
  },
  {
    "tf.math.negative": "tf.math.negative(\n    x, name=None\n)\n"
  },
  {
    "tf.math.nextafter": "tf.math.nextafter(\n    x1, x2, name=None\n)\n"
  },
  {
    "tf.math.not_equal": "tf.math.not_equal(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.polygamma": "tf.math.polygamma(\n    a, x, name=None\n)\n"
  },
  {
    "tf.math.polyval": "tf.math.polyval(\n    coeffs, x, name=None\n)\n"
  },
  {
    "tf.math.pow": "tf.math.pow(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.real": "tf.math.real(\n    input, name=None\n)\n"
  },
  {
    "tf.math.reciprocal": "tf.math.reciprocal(\n    x, name=None\n)\n"
  },
  {
    "tf.math.reciprocal_no_nan": "tf.math.reciprocal_no_nan(\n    x, name=None\n)\n"
  },
  {
    "tf.math.reduce_all": "tf.math.reduce_all(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_any": "tf.math.reduce_any(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_euclidean_norm": "tf.math.reduce_euclidean_norm(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_logsumexp": "tf.math.reduce_logsumexp(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_max": "tf.math.reduce_max(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_mean": "tf.math.reduce_mean(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_min": "tf.math.reduce_min(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_prod": "tf.math.reduce_prod(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_std": "tf.math.reduce_std(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_sum": "tf.math.reduce_sum(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_variance": "tf.math.reduce_variance(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.rint": "tf.math.rint(\n    x, name=None\n)\n"
  },
  {
    "tf.math.round": "tf.math.round(\n    x, name=None\n)\n"
  },
  {
    "tf.math.rsqrt": "tf.math.rsqrt(\n    x, name=None\n)\n"
  },
  {
    "tf.math.scalar_mul": "tf.math.scalar_mul(\n    scalar, x, name=None\n)\n"
  },
  {
    "tf.math.segment_max": "tf.math.segment_max(\n    data, segment_ids, name=None\n)\n"
  },
  {
    "tf.math.segment_mean": "tf.math.segment_mean(\n    data, segment_ids, name=None\n)\n"
  },
  {
    "tf.math.segment_min": "tf.math.segment_min(\n    data, segment_ids, name=None\n)\n"
  },
  {
    "tf.math.segment_prod": "tf.math.segment_prod(\n    data, segment_ids, name=None\n)\n"
  },
  {
    "tf.math.segment_sum": "tf.math.segment_sum(\n    data, segment_ids, name=None\n)\n"
  },
  {
    "tf.math.sigmoid": "tf.math.sigmoid(\n    x, name=None\n)\n"
  },
  {
    "tf.math.sign": "tf.math.sign(\n    x, name=None\n)\n"
  },
  {
    "tf.math.sin": "tf.math.sin(\n    x, name=None\n)\n"
  },
  {
    "tf.math.sinh": "tf.math.sinh(\n    x, name=None\n)\n"
  },
  {
    "tf.math.sobol_sample": "tf.math.sobol_sample(\n    dim,\n    num_results,\n    skip=0,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.nn.softmax": "tf.nn.softmax(\n    logits, axis=None, name=None\n)\n"
  },
  {
    "tf.math.softplus": "tf.math.softplus(\n    features, name=None\n)\n"
  },
  {
    "tf.nn.softsign": "tf.nn.softsign(\n    features, name=None\n)\n"
  },
  {
    "tf.math.bessel_i0": "tf.math.bessel_i0(\n    x, name=None\n)\n"
  },
  {
    "tf.math.bessel_i0e": "tf.math.bessel_i0e(\n    x, name=None\n)\n"
  },
  {
    "tf.math.bessel_i1": "tf.math.bessel_i1(\n    x, name=None\n)\n"
  },
  {
    "tf.math.bessel_i1e": "tf.math.bessel_i1e(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.bessel_j0": "tf.math.special.bessel_j0(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.bessel_j1": "tf.math.special.bessel_j1(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.bessel_k0": "tf.math.special.bessel_k0(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.bessel_k0e": "tf.math.special.bessel_k0e(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.bessel_k1": "tf.math.special.bessel_k1(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.bessel_k1e": "tf.math.special.bessel_k1e(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.bessel_y0": "tf.math.special.bessel_y0(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.bessel_y1": "tf.math.special.bessel_y1(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.dawsn": "tf.math.special.dawsn(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.expint": "tf.math.special.expint(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.fresnel_cos": "tf.math.special.fresnel_cos(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.fresnel_sin": "tf.math.special.fresnel_sin(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.spence": "tf.math.special.spence(\n    x, name=None\n)\n"
  },
  {
    "tf.math.sqrt": "tf.math.sqrt(\n    x, name=None\n)\n"
  },
  {
    "tf.math.square": "tf.math.square(\n    x, name=None\n)\n"
  },
  {
    "tf.math.squared_difference": "tf.math.squared_difference(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.subtract": "tf.math.subtract(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.tan": "tf.math.tan(\n    x, name=None\n)\n"
  },
  {
    "tf.math.tanh": "tf.math.tanh(\n    x, name=None\n)\n"
  },
  {
    "tf.math.top_k": "tf.math.top_k(\n    input, k=1, sorted=True, name=None\n)\n"
  },
  {
    "tf.math.truediv": "tf.math.truediv(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.unsorted_segment_max": "tf.math.unsorted_segment_max(\n    data, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.math.unsorted_segment_mean": "tf.math.unsorted_segment_mean(\n    data, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.math.unsorted_segment_min": "tf.math.unsorted_segment_min(\n    data, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.math.unsorted_segment_prod": "tf.math.unsorted_segment_prod(\n    data, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.math.unsorted_segment_sqrt_n": "tf.math.unsorted_segment_sqrt_n(\n    data, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.math.unsorted_segment_sum": "tf.math.unsorted_segment_sum(\n    data, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.math.xdivy": "tf.math.xdivy(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.xlog1py": "tf.math.xlog1py(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.xlogy": "tf.math.xlogy(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.zero_fraction": "tf.math.zero_fraction(\n    value, name=None\n)\n"
  },
  {
    "tf.math.zeta": "tf.math.zeta(\n    x, q, name=None\n)\n"
  },
  {
    "tf.linalg.matmul": "tf.linalg.matmul(\n    a,\n    b,\n    transpose_a=False,\n    transpose_b=False,\n    adjoint_a=False,\n    adjoint_b=False,\n    a_is_sparse=False,\n    b_is_sparse=False,\n    output_type=None,\n    name=None\n)\n"
  },
  {
    "tf.linalg.sqrtm": "tf.linalg.sqrtm(\n    input, name=None\n)\n"
  },
  {
    "tf.math.maximum": "tf.math.maximum(\n    x, y, name=None\n)\n"
  },
  {
    "tf.meshgrid": "tf.meshgrid(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.keras.metrics.AUC": "tf.keras.metrics.AUC(\n    num_thresholds=200,\n    curve='ROC',\n    summation_method='interpolation',\n    name=None,\n    dtype=None,\n    thresholds=None,\n    multi_label=False,\n    num_labels=None,\n    label_weights=None,\n    from_logits=False\n)\n"
  },
  {
    "tf.keras.metrics.Accuracy": "tf.keras.metrics.Accuracy(\n    name='accuracy', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.BinaryAccuracy": "tf.keras.metrics.BinaryAccuracy(\n    name='binary_accuracy', dtype=None, threshold=0.5\n)\n"
  },
  {
    "tf.keras.metrics.BinaryCrossentropy": "tf.keras.metrics.BinaryCrossentropy(\n    name='binary_crossentropy',\n    dtype=None,\n    from_logits=False,\n    label_smoothing=0\n)\n"
  },
  {
    "tf.keras.metrics.BinaryIoU": "tf.keras.metrics.BinaryIoU(\n    target_class_ids: Union[List[int], Tuple[int, ...]] = (0, 1),\n    threshold=0.5,\n    name=None,\n    dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.CategoricalAccuracy": "tf.keras.metrics.CategoricalAccuracy(\n    name='categorical_accuracy', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.CategoricalCrossentropy": "tf.keras.metrics.CategoricalCrossentropy(\n    name='categorical_crossentropy',\n    dtype=None,\n    from_logits=False,\n    label_smoothing=0,\n    axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.CategoricalHinge": "tf.keras.metrics.CategoricalHinge(\n    name='categorical_hinge', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.CosineSimilarity": "tf.keras.metrics.CosineSimilarity(\n    name='cosine_similarity', dtype=None, axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.FalseNegatives": "tf.keras.metrics.FalseNegatives(\n    thresholds=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.FalsePositives": "tf.keras.metrics.FalsePositives(\n    thresholds=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.Hinge": "tf.keras.metrics.Hinge(\n    name='hinge', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.IoU": "tf.keras.metrics.IoU(\n    num_classes: int,\n    target_class_ids: Union[List[int], Tuple[int, ...]],\n    name: Optional[str] = None,\n    dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n    ignore_class: Optional[int] = None,\n    sparse_y_true: bool = True,\n    sparse_y_pred: bool = True,\n    axis: int = -1\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.KLDivergence": "tf.keras.metrics.KLDivergence(\n    name='kullback_leibler_divergence', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.LogCoshError": "tf.keras.metrics.LogCoshError(\n    name='logcosh', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.Mean": "tf.keras.metrics.Mean(\n    name='mean', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanAbsoluteError": "tf.keras.metrics.MeanAbsoluteError(\n    name='mean_absolute_error', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanAbsolutePercentageError": "tf.keras.metrics.MeanAbsolutePercentageError(\n    name='mean_absolute_percentage_error', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanIoU": "tf.keras.metrics.MeanIoU(\n    num_classes: int,\n    name: Optional[str] = None,\n    dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n    ignore_class: Optional[int] = None,\n    sparse_y_true: bool = True,\n    sparse_y_pred: bool = True,\n    axis: int = -1\n)\n"
  },
  {
    "tf.keras.metrics.MeanMetricWrapper": "tf.keras.metrics.MeanMetricWrapper(\n    fn, name=None, dtype=None, **kwargs\n)\n"
  },
  {
    "tf.keras.metrics.MeanRelativeError": "tf.keras.metrics.MeanRelativeError(\n    normalizer, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanSquaredError": "tf.keras.metrics.MeanSquaredError(\n    name='mean_squared_error', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanSquaredLogarithmicError": "tf.keras.metrics.MeanSquaredLogarithmicError(\n    name='mean_squared_logarithmic_error', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanTensor": "tf.keras.metrics.MeanTensor(\n    name='mean_tensor', dtype=None, shape=None\n)\n"
  },
  {
    "tf.keras.metrics.Metric": "tf.keras.metrics.Metric(\n    name=None, dtype=None, **kwargs\n)\n"
  },
  {
    "tf.keras.metrics.OneHotIoU": "tf.keras.metrics.OneHotIoU(\n    num_classes: int,\n    target_class_ids: Union[List[int], Tuple[int, ...]],\n    name=None,\n    dtype=None,\n    ignore_class: Optional[int] = None,\n    sparse_y_pred: bool = False,\n    axis: int = -1\n)\n"
  },
  {
    "tf.keras.metrics.OneHotMeanIoU": "tf.keras.metrics.OneHotMeanIoU(\n    num_classes: int,\n    name: str = None,\n    dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n    ignore_class: Optional[int] = None,\n    sparse_y_pred: bool = False,\n    axis: int = -1\n)\n"
  },
  {
    "tf.keras.metrics.Poisson": "tf.keras.metrics.Poisson(\n    name='poisson', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.Precision": "tf.keras.metrics.Precision(\n    thresholds=None, top_k=None, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.PrecisionAtRecall": "tf.keras.metrics.PrecisionAtRecall(\n    recall, num_thresholds=200, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.Recall": "tf.keras.metrics.Recall(\n    thresholds=None, top_k=None, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.RecallAtPrecision": "tf.keras.metrics.RecallAtPrecision(\n    precision, num_thresholds=200, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.RootMeanSquaredError": "tf.keras.metrics.RootMeanSquaredError(\n    name='root_mean_squared_error', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.SensitivityAtSpecificity": "tf.keras.metrics.SensitivityAtSpecificity(\n    specificity, num_thresholds=200, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.SparseCategoricalAccuracy": "tf.keras.metrics.SparseCategoricalAccuracy(\n    name='sparse_categorical_accuracy', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.SparseCategoricalCrossentropy": "tf.keras.metrics.SparseCategoricalCrossentropy(\n    name: str = 'sparse_categorical_crossentropy',\n    dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n    from_logits: bool = False,\n    ignore_class: Optional[int] = None,\n    axis: int = -1\n)\n"
  },
  {
    "tf.keras.metrics.SparseTopKCategoricalAccuracy": "tf.keras.metrics.SparseTopKCategoricalAccuracy(\n    k=5, name='sparse_top_k_categorical_accuracy', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.SpecificityAtSensitivity": "tf.keras.metrics.SpecificityAtSensitivity(\n    sensitivity, num_thresholds=200, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.SquaredHinge": "tf.keras.metrics.SquaredHinge(\n    name='squared_hinge', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.Sum": "tf.keras.metrics.Sum(\n    name='sum', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.TopKCategoricalAccuracy": "tf.keras.metrics.TopKCategoricalAccuracy(\n    k=5, name='top_k_categorical_accuracy', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.TrueNegatives": "tf.keras.metrics.TrueNegatives(\n    thresholds=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.TruePositives": "tf.keras.metrics.TruePositives(\n    thresholds=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.binary_accuracy": "tf.keras.metrics.binary_accuracy(\n    y_true, y_pred, threshold=0.5\n)\n"
  },
  {
    "tf.keras.metrics.binary_crossentropy": "tf.keras.metrics.binary_crossentropy(\n    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.binary_focal_crossentropy": "tf.keras.metrics.binary_focal_crossentropy(\n    y_true,\n    y_pred,\n    apply_class_balancing=False,\n    alpha=0.25,\n    gamma=2.0,\n    from_logits=False,\n    label_smoothing=0.0,\n    axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.categorical_accuracy": "tf.keras.metrics.categorical_accuracy(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.categorical_crossentropy": "tf.keras.metrics.categorical_crossentropy(\n    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.deserialize": "tf.keras.metrics.deserialize(\n    config, custom_objects=None\n)\n"
  },
  {
    "tf.keras.metrics.get": "tf.keras.metrics.get(\n    identifier\n)\n"
  },
  {
    "tf.keras.metrics.hinge": "tf.keras.metrics.hinge(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.log_cosh": "tf.keras.losses.log_cosh(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.log_cosh": "tf.keras.losses.log_cosh(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.poisson": "tf.keras.metrics.poisson(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.serialize": "tf.keras.metrics.serialize(\n    metric\n)\n"
  },
  {
    "tf.keras.metrics.sparse_categorical_accuracy": "tf.keras.metrics.sparse_categorical_accuracy(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.sparse_categorical_crossentropy": "tf.keras.metrics.sparse_categorical_crossentropy(\n    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None\n)\n"
  },
  {
    "tf.keras.metrics.sparse_top_k_categorical_accuracy": "tf.keras.metrics.sparse_top_k_categorical_accuracy(\n    y_true, y_pred, k=5\n)\n"
  },
  {
    "tf.keras.metrics.squared_hinge": "tf.keras.metrics.squared_hinge(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.top_k_categorical_accuracy": "tf.keras.metrics.top_k_categorical_accuracy(\n    y_true, y_pred, k=5\n)\n"
  },
  {
    "tf.math.minimum": "tf.math.minimum(\n    x, y, name=None\n)\n"
  },
  {
    "tf.mlir.experimental.convert_function": "tf.mlir.experimental.convert_function(\n    concrete_function,\n    pass_pipeline='tf-standard-pipeline',\n    show_debug_info=False\n)\n"
  },
  {
    "tf.mlir.experimental.convert_graph_def": "tf.mlir.experimental.convert_graph_def(\n    graph_def,\n    pass_pipeline='tf-standard-pipeline',\n    show_debug_info=False\n)\n"
  },
  {
    "tf.math.multiply": "tf.math.multiply(\n    x, y, name=None\n)\n"
  },
  {
    "tf.name_scope": "tf.name_scope(\n    name\n)\n"
  },
  {
    "tf.math.negative": "tf.math.negative(\n    x, name=None\n)\n"
  },
  {
    "tf.nest.assert_same_structure": "tf.nest.assert_same_structure(\n    nest1, nest2, check_types=True, expand_composites=False\n)\n"
  },
  {
    "tf.nest.flatten": "tf.nest.flatten(\n    structure, expand_composites=False\n)\n"
  },
  {
    "tf.nest.is_nested": "tf.nest.is_nested(\n    seq\n)\n"
  },
  {
    "tf.nest.map_structure": "tf.nest.map_structure(\n    func, *structure, **kwargs\n)\n"
  },
  {
    "tf.nest.pack_sequence_as": "tf.nest.pack_sequence_as(\n    structure, flat_sequence, expand_composites=False\n)\n"
  },
  {
    "tf.nn.RNNCellDeviceWrapper": "tf.nn.RNNCellDeviceWrapper(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.nn.RNNCellDropoutWrapper": "tf.nn.RNNCellDropoutWrapper(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.nn.RNNCellResidualWrapper": "tf.nn.RNNCellResidualWrapper(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.random.all_candidate_sampler": "tf.random.all_candidate_sampler(\n    true_classes, num_true, num_sampled, unique, seed=None, name=None\n)\n"
  },
  {
    "tf.math.approx_max_k": "tf.math.approx_max_k(\n    operand,\n    k,\n    reduction_dimension=-1,\n    recall_target=0.95,\n    reduction_input_size_override=-1,\n    aggregate_to_topk=True,\n    name=None\n)\n"
  },
  {
    "tf.math.approx_min_k": "tf.math.approx_min_k(\n    operand,\n    k,\n    reduction_dimension=-1,\n    recall_target=0.95,\n    reduction_input_size_override=-1,\n    aggregate_to_topk=True,\n    name=None\n)\n"
  },
  {
    "tf.nn.atrous_conv2d": "tf.nn.atrous_conv2d(\n    value, filters, rate, padding, name=None\n)\n"
  },
  {
    "tf.nn.atrous_conv2d_transpose": "tf.nn.atrous_conv2d_transpose(\n    value, filters, output_shape, rate, padding, name=None\n)\n"
  },
  {
    "tf.nn.avg_pool": "tf.nn.avg_pool(\n    input, ksize, strides, padding, data_format=None, name=None\n)\n"
  },
  {
    "tf.nn.avg_pool1d": "tf.nn.avg_pool1d(\n    input, ksize, strides, padding, data_format='NWC', name=None\n)\n"
  },
  {
    "tf.nn.avg_pool2d": "tf.nn.avg_pool2d(\n    input, ksize, strides, padding, data_format='NHWC', name=None\n)\n"
  },
  {
    "tf.nn.avg_pool3d": "tf.nn.avg_pool3d(\n    input, ksize, strides, padding, data_format='NDHWC', name=None\n)\n"
  },
  {
    "tf.nn.batch_norm_with_global_normalization": "tf.nn.batch_norm_with_global_normalization(\n    input,\n    mean,\n    variance,\n    beta,\n    gamma,\n    variance_epsilon,\n    scale_after_normalization,\n    name=None\n)\n"
  },
  {
    "tf.nn.batch_normalization": "tf.nn.batch_normalization(\n    x, mean, variance, offset, scale, variance_epsilon, name=None\n)\n"
  },
  {
    "tf.nn.bias_add": "tf.nn.bias_add(\n    value, bias, data_format=None, name=None\n)\n"
  },
  {
    "tf.nn.collapse_repeated": "tf.nn.collapse_repeated(\n    labels, seq_length, name=None\n)\n"
  },
  {
    "tf.nn.compute_accidental_hits": "tf.nn.compute_accidental_hits(\n    true_classes, sampled_candidates, num_true, seed=None, name=None\n)\n"
  },
  {
    "tf.nn.compute_average_loss": "tf.nn.compute_average_loss(\n    per_example_loss, sample_weight=None, global_batch_size=None\n)\n"
  },
  {
    "tf.nn.conv1d": "tf.nn.conv1d(\n    input,\n    filters,\n    stride,\n    padding,\n    data_format='NWC',\n    dilations=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.conv1d_transpose": "tf.nn.conv1d_transpose(\n    input,\n    filters,\n    output_shape,\n    strides,\n    padding='SAME',\n    data_format='NWC',\n    dilations=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.conv2d": "tf.nn.conv2d(\n    input,\n    filters,\n    strides,\n    padding,\n    data_format='NHWC',\n    dilations=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.conv2d_transpose": "tf.nn.conv2d_transpose(\n    input,\n    filters,\n    output_shape,\n    strides,\n    padding='SAME',\n    data_format='NHWC',\n    dilations=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.conv3d": "tf.nn.conv3d(\n    input,\n    filters,\n    strides,\n    padding,\n    data_format='NDHWC',\n    dilations=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.conv3d_transpose": "tf.nn.conv3d_transpose(\n    input,\n    filters,\n    output_shape,\n    strides,\n    padding='SAME',\n    data_format='NDHWC',\n    dilations=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.conv_transpose": "tf.nn.conv_transpose(\n    input,\n    filters,\n    output_shape,\n    strides,\n    padding='SAME',\n    data_format=None,\n    dilations=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.convolution": "tf.nn.convolution(\n    input,\n    filters,\n    strides=None,\n    padding='VALID',\n    data_format=None,\n    dilations=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.crelu": "tf.nn.crelu(\n    features, axis=-1, name=None\n)\n"
  },
  {
    "tf.nn.ctc_beam_search_decoder": "tf.nn.ctc_beam_search_decoder(\n    inputs, sequence_length, beam_width=100, top_paths=1\n)\n"
  },
  {
    "tf.nn.ctc_greedy_decoder": "tf.nn.ctc_greedy_decoder(\n    inputs, sequence_length, merge_repeated=True, blank_index=None\n)\n"
  },
  {
    "tf.nn.ctc_loss": "tf.nn.ctc_loss(\n    labels,\n    logits,\n    label_length,\n    logit_length,\n    logits_time_major=True,\n    unique=None,\n    blank_index=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.ctc_unique_labels": "tf.nn.ctc_unique_labels(\n    labels, name=None\n)\n"
  },
  {
    "tf.nn.depth_to_space": "tf.nn.depth_to_space(\n    input, block_size, data_format='NHWC', name=None\n)\n"
  },
  {
    "tf.nn.depthwise_conv2d": "tf.nn.depthwise_conv2d(\n    input,\n    filter,\n    strides,\n    padding,\n    data_format=None,\n    dilations=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.depthwise_conv2d_backprop_filter": "tf.nn.depthwise_conv2d_backprop_filter(\n    input,\n    filter_sizes,\n    out_backprop,\n    strides,\n    padding,\n    data_format='NHWC',\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.nn.depthwise_conv2d_backprop_input": "tf.nn.depthwise_conv2d_backprop_input(\n    input_sizes,\n    filter,\n    out_backprop,\n    strides,\n    padding,\n    data_format='NHWC',\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.nn.dilation2d": "tf.nn.dilation2d(\n    input, filters, strides, padding, data_format, dilations, name=None\n)\n"
  },
  {
    "tf.nn.dropout": "tf.nn.dropout(\n    x, rate, noise_shape=None, seed=None, name=None\n)\n"
  },
  {
    "tf.nn.elu": "tf.nn.elu(\n    features, name=None\n)\n"
  },
  {
    "tf.nn.embedding_lookup": "tf.nn.embedding_lookup(\n    params, ids, max_norm=None, name=None\n)\n"
  },
  {
    "tf.nn.embedding_lookup_sparse": "tf.nn.embedding_lookup_sparse(\n    params, sp_ids, sp_weights, combiner=None, max_norm=None, name=None\n)\n"
  },
  {
    "tf.nn.erosion2d": "tf.nn.erosion2d(\n    value, filters, strides, padding, data_format, dilations, name=None\n)\n"
  },
  {
    "tf.nn.experimental.stateless_dropout": "tf.nn.experimental.stateless_dropout(\n    x, rate, seed, rng_alg=None, noise_shape=None, name=None\n)\n"
  },
  {
    "tf.random.fixed_unigram_candidate_sampler": "tf.random.fixed_unigram_candidate_sampler(\n    true_classes,\n    num_true,\n    num_sampled,\n    unique,\n    range_max,\n    vocab_file='',\n    distortion=1.0,\n    num_reserved_ids=0,\n    num_shards=1,\n    shard=0,\n    unigrams=(),\n    seed=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.fractional_avg_pool": "tf.nn.fractional_avg_pool(\n    value,\n    pooling_ratio,\n    pseudo_random=False,\n    overlapping=False,\n    seed=0,\n    name=None\n)\n"
  },
  {
    "tf.nn.fractional_max_pool": "tf.nn.fractional_max_pool(\n    value,\n    pooling_ratio,\n    pseudo_random=False,\n    overlapping=False,\n    seed=0,\n    name=None\n)\n"
  },
  {
    "tf.nn.gelu": "tf.nn.gelu(\n    features, approximate=False, name=None\n)\n"
  },
  {
    "tf.math.in_top_k": "tf.math.in_top_k(\n    targets, predictions, k, name=None\n)\n"
  },
  {
    "tf.nn.isotonic_regression": "tf.nn.isotonic_regression(\n    inputs, decreasing=True, axis=-1\n)\n"
  },
  {
    "tf.nn.l2_loss": "tf.nn.l2_loss(\n    t, name=None\n)\n"
  },
  {
    "tf.math.l2_normalize": "tf.math.l2_normalize(\n    x, axis=None, epsilon=1e-12, name=None, dim=None\n)\n"
  },
  {
    "tf.nn.leaky_relu": "tf.nn.leaky_relu(\n    features, alpha=0.2, name=None\n)\n"
  },
  {
    "tf.random.learned_unigram_candidate_sampler": "tf.random.learned_unigram_candidate_sampler(\n    true_classes,\n    num_true,\n    num_sampled,\n    unique,\n    range_max,\n    seed=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.local_response_normalization": "tf.nn.local_response_normalization(\n    input, depth_radius=5, bias=1, alpha=1, beta=0.5, name=None\n)\n"
  },
  {
    "tf.nn.log_poisson_loss": "tf.nn.log_poisson_loss(\n    targets, log_input, compute_full_loss=False, name=None\n)\n"
  },
  {
    "tf.nn.log_softmax": "tf.nn.log_softmax(\n    logits, axis=None, name=None\n)\n"
  },
  {
    "tf.nn.local_response_normalization": "tf.nn.local_response_normalization(\n    input, depth_radius=5, bias=1, alpha=1, beta=0.5, name=None\n)\n"
  },
  {
    "tf.nn.max_pool": "tf.nn.max_pool(\n    input, ksize, strides, padding, data_format=None, name=None\n)\n"
  },
  {
    "tf.nn.max_pool1d": "tf.nn.max_pool1d(\n    input, ksize, strides, padding, data_format='NWC', name=None\n)\n"
  },
  {
    "tf.nn.max_pool2d": "tf.nn.max_pool2d(\n    input, ksize, strides, padding, data_format='NHWC', name=None\n)\n"
  },
  {
    "tf.nn.max_pool3d": "tf.nn.max_pool3d(\n    input, ksize, strides, padding, data_format='NDHWC', name=None\n)\n"
  },
  {
    "tf.nn.max_pool_with_argmax": "tf.nn.max_pool_with_argmax(\n    input,\n    ksize,\n    strides,\n    padding,\n    data_format='NHWC',\n    output_dtype=tf.dtypes.int64,\n    include_batch_in_index=False,\n    name=None\n)\n"
  },
  {
    "tf.nn.moments": "tf.nn.moments(\n    x, axes, shift=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.nn.nce_loss": "tf.nn.nce_loss(\n    weights,\n    biases,\n    labels,\n    inputs,\n    num_sampled,\n    num_classes,\n    num_true=1,\n    sampled_values=None,\n    remove_accidental_hits=False,\n    name='nce_loss'\n)\n"
  },
  {
    "tf.nn.normalize_moments": "tf.nn.normalize_moments(\n    counts, mean_ss, variance_ss, shift, name=None\n)\n"
  },
  {
    "tf.nn.pool": "tf.nn.pool(\n    input,\n    window_shape,\n    pooling_type,\n    strides=None,\n    padding='VALID',\n    data_format=None,\n    dilations=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.relu": "tf.nn.relu(\n    features, name=None\n)\n"
  },
  {
    "tf.nn.relu6": "tf.nn.relu6(\n    features, name=None\n)\n"
  },
  {
    "tf.nn.safe_embedding_lookup_sparse": "tf.nn.safe_embedding_lookup_sparse(\n    embedding_weights,\n    sparse_ids,\n    sparse_weights=None,\n    combiner='mean',\n    default_id=None,\n    max_norm=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.sampled_softmax_loss": "tf.nn.sampled_softmax_loss(\n    weights,\n    biases,\n    labels,\n    inputs,\n    num_sampled,\n    num_classes,\n    num_true=1,\n    sampled_values=None,\n    remove_accidental_hits=True,\n    seed=None,\n    name='sampled_softmax_loss'\n)\n"
  },
  {
    "tf.nn.scale_regularization_loss": "tf.nn.scale_regularization_loss(\n    regularization_loss\n)\n"
  },
  {
    "tf.nn.selu": "tf.nn.selu(\n    features, name=None\n)\n"
  },
  {
    "tf.nn.separable_conv2d": "tf.nn.separable_conv2d(\n    input,\n    depthwise_filter,\n    pointwise_filter,\n    strides,\n    padding,\n    data_format=None,\n    dilations=None,\n    name=None\n)\n"
  },
  {
    "tf.math.sigmoid": "tf.math.sigmoid(\n    x, name=None\n)\n"
  },
  {
    "tf.nn.sigmoid_cross_entropy_with_logits": "tf.nn.sigmoid_cross_entropy_with_logits(\n    labels=None, logits=None, name=None\n)\n"
  },
  {
    "tf.nn.silu": "tf.nn.silu(\n    features, beta=1.0\n)\n"
  },
  {
    "tf.nn.softmax": "tf.nn.softmax(\n    logits, axis=None, name=None\n)\n"
  },
  {
    "tf.nn.softmax_cross_entropy_with_logits": "tf.nn.softmax_cross_entropy_with_logits(\n    labels, logits, axis=-1, name=None\n)\n"
  },
  {
    "tf.math.softplus": "tf.math.softplus(\n    features, name=None\n)\n"
  },
  {
    "tf.nn.softsign": "tf.nn.softsign(\n    features, name=None\n)\n"
  },
  {
    "tf.space_to_batch": "tf.space_to_batch(\n    input, block_shape, paddings, name=None\n)\n"
  },
  {
    "tf.nn.space_to_depth": "tf.nn.space_to_depth(\n    input, block_size, data_format='NHWC', name=None\n)\n"
  },
  {
    "tf.nn.sparse_softmax_cross_entropy_with_logits": "tf.nn.sparse_softmax_cross_entropy_with_logits(\n    labels, logits, name=None\n)\n"
  },
  {
    "tf.nn.sufficient_statistics": "tf.nn.sufficient_statistics(\n    x, axes, shift=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.nn.silu": "tf.nn.silu(\n    features, beta=1.0\n)\n"
  },
  {
    "tf.math.tanh": "tf.math.tanh(\n    x, name=None\n)\n"
  },
  {
    "tf.math.top_k": "tf.math.top_k(\n    input, k=1, sorted=True, name=None\n)\n"
  },
  {
    "tf.nn.weighted_cross_entropy_with_logits": "tf.nn.weighted_cross_entropy_with_logits(\n    labels, logits, pos_weight, name=None\n)\n"
  },
  {
    "tf.nn.weighted_moments": "tf.nn.weighted_moments(\n    x, axes, frequency_weights, keepdims=False, name=None\n)\n"
  },
  {
    "tf.nn.with_space_to_batch": "tf.nn.with_space_to_batch(\n    input,\n    dilation_rate,\n    padding,\n    op,\n    filter_shape=None,\n    spatial_dims=None,\n    data_format=None\n)\n"
  },
  {
    "tf.math.zero_fraction": "tf.math.zero_fraction(\n    value, name=None\n)\n"
  },
  {
    "tf.no_gradient": "tf.no_gradient(\n    op_type\n)\n"
  },
  {
    "tf.no_op": "tf.no_op(\n    name=None\n)\n"
  },
  {
    "tf.nondifferentiable_batch_function": "tf.nondifferentiable_batch_function(\n    num_batch_threads,\n    max_batch_size,\n    batch_timeout_micros,\n    allowed_batch_sizes=None,\n    max_enqueued_batches=10,\n    autograph=True,\n    enable_large_batch_splitting=True\n)\n"
  },
  {
    "tf.norm": "tf.norm(\n    tensor, ord='euclidean', axis=None, keepdims=None, name=None\n)\n"
  },
  {
    "tf.math.not_equal": "tf.math.not_equal(\n    x, y, name=None\n)\n"
  },
  {
    "tf.numpy_function": "tf.numpy_function(\n    func, inp, Tout, stateful=True, name=None\n)\n"
  },
  {
    "tf.one_hot": "tf.one_hot(\n    indices,\n    depth,\n    on_value=None,\n    off_value=None,\n    axis=None,\n    dtype=None,\n    name=None\n)\n"
  },
  {
    "tf.ones": "tf.ones(\n    shape,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.ones_like": "tf.ones_like(\n    input, dtype=None, name=None\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adadelta": "tf.keras.optimizers.experimental.Adadelta(\n    learning_rate=0.001,\n    rho=0.95,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adadelta',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adagrad": "tf.keras.optimizers.experimental.Adagrad(\n    learning_rate=0.001,\n    initial_accumulator_value=0.1,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adagrad',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.Adam": "tf.keras.optimizers.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adamax": "tf.keras.optimizers.experimental.Adamax(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adamax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Ftrl": "tf.keras.optimizers.experimental.Ftrl(\n    learning_rate=0.001,\n    learning_rate_power=-0.5,\n    initial_accumulator_value=0.1,\n    l1_regularization_strength=0.0,\n    l2_regularization_strength=0.0,\n    l2_shrinkage_regularization_strength=0.0,\n    beta=0.0,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Ftrl',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Nadam": "tf.keras.optimizers.experimental.Nadam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Nadam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.Optimizer": "tf.keras.optimizers.Optimizer(\n    name,\n    weight_decay=0,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.RMSprop": "tf.keras.optimizers.experimental.RMSprop(\n    learning_rate=0.001,\n    rho=0.9,\n    momentum=0.0,\n    epsilon=1e-07,\n    centered=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=100,\n    jit_compile=True,\n    name='RMSprop',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.SGD": "tf.keras.optimizers.experimental.SGD(\n    learning_rate=0.01,\n    momentum=0.0,\n    nesterov=False,\n    amsgrad=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='SGD',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.deserialize": "tf.keras.optimizers.deserialize(\n    config, custom_objects=None, **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adadelta": "tf.keras.optimizers.experimental.Adadelta(\n    learning_rate=0.001,\n    rho=0.95,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adadelta',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adafactor": "tf.keras.optimizers.experimental.Adafactor(\n    learning_rate=0.001,\n    beta_2_decay=-0.8,\n    epsilon_1=1e-30,\n    epsilon_2=0.001,\n    clip_threshold=1.0,\n    relative_step=True,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adafactor',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adagrad": "tf.keras.optimizers.experimental.Adagrad(\n    learning_rate=0.001,\n    initial_accumulator_value=0.1,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adagrad',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.Adam": "tf.keras.optimizers.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.AdamW": "tf.keras.optimizers.experimental.AdamW(\n    learning_rate=0.001,\n    weight_decay=0.004,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='AdamW',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adamax": "tf.keras.optimizers.experimental.Adamax(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adamax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Ftrl": "tf.keras.optimizers.experimental.Ftrl(\n    learning_rate=0.001,\n    learning_rate_power=-0.5,\n    initial_accumulator_value=0.1,\n    l1_regularization_strength=0.0,\n    l2_regularization_strength=0.0,\n    l2_shrinkage_regularization_strength=0.0,\n    beta=0.0,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Ftrl',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Nadam": "tf.keras.optimizers.experimental.Nadam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Nadam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.Optimizer": "tf.keras.optimizers.Optimizer(\n    name,\n    weight_decay=0,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.RMSprop": "tf.keras.optimizers.experimental.RMSprop(\n    learning_rate=0.001,\n    rho=0.9,\n    momentum=0.0,\n    epsilon=1e-07,\n    centered=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=100,\n    jit_compile=True,\n    name='RMSprop',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.SGD": "tf.keras.optimizers.experimental.SGD(\n    learning_rate=0.01,\n    momentum=0.0,\n    nesterov=False,\n    amsgrad=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='SGD',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.get": "tf.keras.optimizers.get(\n    identifier, **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Adadelta": "tf.keras.optimizers.legacy.Adadelta(\n    learning_rate=0.001,\n    rho=0.95,\n    epsilon=1e-07,\n    name='Adadelta',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Adagrad": "tf.keras.optimizers.legacy.Adagrad(\n    learning_rate=0.001,\n    initial_accumulator_value=0.1,\n    epsilon=1e-07,\n    name='Adagrad',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Adam": "tf.keras.optimizers.legacy.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    name='Adam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Adamax": "tf.keras.optimizers.legacy.Adamax(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    name='Adamax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Ftrl": "tf.keras.optimizers.legacy.Ftrl(\n    learning_rate=0.001,\n    learning_rate_power=-0.5,\n    initial_accumulator_value=0.1,\n    l1_regularization_strength=0.0,\n    l2_regularization_strength=0.0,\n    name='Ftrl',\n    l2_shrinkage_regularization_strength=0.0,\n    beta=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Nadam": "tf.keras.optimizers.legacy.Nadam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    name='Nadam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Optimizer": "tf.keras.optimizers.legacy.Optimizer(\n    name, gradient_aggregator=None, gradient_transformers=None, **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.RMSprop": "tf.keras.optimizers.legacy.RMSprop(\n    learning_rate=0.001,\n    rho=0.9,\n    momentum=0.0,\n    epsilon=1e-07,\n    centered=False,\n    name='RMSprop',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.SGD": "tf.keras.optimizers.legacy.SGD(\n    learning_rate=0.01,\n    momentum=0.0,\n    nesterov=False,\n    name='SGD',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.CosineDecay": "tf.keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate, decay_steps, alpha=0.0, name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.CosineDecayRestarts": "tf.keras.optimizers.schedules.CosineDecayRestarts(\n    initial_learning_rate,\n    first_decay_steps,\n    t_mul=2.0,\n    m_mul=1.0,\n    alpha=0.0,\n    name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.ExponentialDecay": "tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate, decay_steps, decay_rate, staircase=False, name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.InverseTimeDecay": "tf.keras.optimizers.schedules.InverseTimeDecay(\n    initial_learning_rate, decay_steps, decay_rate, staircase=False, name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.PiecewiseConstantDecay": "tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n    boundaries, values, name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.PolynomialDecay": "tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate,\n    decay_steps,\n    end_learning_rate=0.0001,\n    power=1.0,\n    cycle=False,\n    name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.deserialize": "tf.keras.optimizers.schedules.deserialize(\n    config, custom_objects=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.serialize": "tf.keras.optimizers.schedules.serialize(\n    learning_rate_schedule\n)\n"
  },
  {
    "tf.keras.optimizers.serialize": "tf.keras.optimizers.serialize(\n    optimizer\n)\n"
  },
  {
    "tf.pad": "tf.pad(\n    tensor, paddings, mode='CONSTANT', constant_values=0, name=None\n)\n"
  },
  {
    "tf.parallel_stack": "tf.parallel_stack(\n    values, name='parallel_stack'\n)\n"
  },
  {
    "tf.math.pow": "tf.math.pow(\n    x, y, name=None\n)\n"
  },
  {
    "tf.print": "tf.print(\n    *inputs, **kwargs\n)\n"
  },
  {
    "tf.profiler.experimental.Profile": "tf.profiler.experimental.Profile(\n    logdir, options=None\n)\n"
  },
  {
    "tf.profiler.experimental.ProfilerOptions": "tf.profiler.experimental.ProfilerOptions(\n    host_tracer_level=2,\n    python_tracer_level=0,\n    device_tracer_level=1,\n    delay_ms=None\n)\n"
  },
  {
    "tf.profiler.experimental.Trace": "tf.profiler.experimental.Trace(\n    name, **kwargs\n)\n"
  },
  {
    "tf.profiler.experimental.client.monitor": "tf.profiler.experimental.client.monitor(\n    service_addr, duration_ms, level=1\n)\n"
  },
  {
    "tf.profiler.experimental.client.trace": "tf.profiler.experimental.client.trace(\n    service_addr,\n    logdir,\n    duration_ms,\n    worker_list='',\n    num_tracing_attempts=3,\n    options=None\n)\n"
  },
  {
    "tf.profiler.experimental.server.start": "tf.profiler.experimental.server.start(\n    port\n)\n"
  },
  {
    "tf.profiler.experimental.start": "tf.profiler.experimental.start(\n    logdir, options=None\n)\n"
  },
  {
    "tf.profiler.experimental.stop": "tf.profiler.experimental.stop(\n    save=True\n)\n"
  },
  {
    "tf.py_function": "tf.py_function(\n    func, inp, Tout, name=None\n)\n"
  },
  {
    "tf.quantization.dequantize": "tf.quantization.dequantize(\n    input,\n    min_range,\n    max_range,\n    mode='MIN_COMBINED',\n    name=None,\n    axis=None,\n    narrow_range=False,\n    dtype=tf.dtypes.float32\n)\n"
  },
  {
    "tf.quantization.fake_quant_with_min_max_args": "tf.quantization.fake_quant_with_min_max_args(\n    inputs, min=-6, max=6, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.quantization.fake_quant_with_min_max_args_gradient": "tf.quantization.fake_quant_with_min_max_args_gradient(\n    gradients, inputs, min=-6, max=6, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.quantization.fake_quant_with_min_max_vars": "tf.quantization.fake_quant_with_min_max_vars(\n    inputs, min, max, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.quantization.fake_quant_with_min_max_vars_gradient": "tf.quantization.fake_quant_with_min_max_vars_gradient(\n    gradients, inputs, min, max, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.quantization.fake_quant_with_min_max_vars_per_channel": "tf.quantization.fake_quant_with_min_max_vars_per_channel(\n    inputs, min, max, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient": "tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient(\n    gradients, inputs, min, max, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.quantization.quantize": "tf.quantization.quantize(\n    input,\n    min_range,\n    max_range,\n    T,\n    mode='MIN_COMBINED',\n    round_mode='HALF_AWAY_FROM_ZERO',\n    name=None,\n    narrow_range=False,\n    axis=None,\n    ensure_minimum_range=0.01\n)\n"
  },
  {
    "tf.quantization.quantize_and_dequantize": "tf.quantization.quantize_and_dequantize(\n    input,\n    input_min,\n    input_max,\n    signed_input=True,\n    num_bits=8,\n    range_given=False,\n    round_mode='HALF_TO_EVEN',\n    name=None,\n    narrow_range=False,\n    axis=None\n)\n"
  },
  {
    "tf.quantization.quantize_and_dequantize_v2": "tf.quantization.quantize_and_dequantize_v2(\n    input,\n    input_min,\n    input_max,\n    signed_input=True,\n    num_bits=8,\n    range_given=False,\n    round_mode='HALF_TO_EVEN',\n    name=None,\n    narrow_range=False,\n    axis=None\n)\n"
  },
  {
    "tf.quantization.quantized_concat": "tf.quantization.quantized_concat(\n    concat_dim, values, input_mins, input_maxes, name=None\n)\n"
  },
  {
    "tf.queue.FIFOQueue": "tf.queue.FIFOQueue(\n    capacity,\n    dtypes,\n    shapes=None,\n    names=None,\n    shared_name=None,\n    name='fifo_queue'\n)\n"
  },
  {
    "tf.queue.PaddingFIFOQueue": "tf.queue.PaddingFIFOQueue(\n    capacity,\n    dtypes,\n    shapes,\n    names=None,\n    shared_name=None,\n    name='padding_fifo_queue'\n)\n"
  },
  {
    "tf.queue.PriorityQueue": "tf.queue.PriorityQueue(\n    capacity,\n    types,\n    shapes=None,\n    names=None,\n    shared_name=None,\n    name='priority_queue'\n)\n"
  },
  {
    "tf.queue.QueueBase": "tf.queue.QueueBase(\n    dtypes, shapes, names, queue_ref\n)\n"
  },
  {
    "tf.queue.RandomShuffleQueue": "tf.queue.RandomShuffleQueue(\n    capacity,\n    min_after_dequeue,\n    dtypes,\n    shapes=None,\n    names=None,\n    seed=None,\n    shared_name=None,\n    name='random_shuffle_queue'\n)\n"
  },
  {
    "tf.ragged.boolean_mask": "tf.ragged.boolean_mask(\n    data, mask, name=None\n)\n"
  },
  {
    "tf.ragged.constant": "tf.ragged.constant(\n    pylist,\n    dtype=None,\n    ragged_rank=None,\n    inner_shape=None,\n    name=None,\n    row_splits_dtype=tf.dtypes.int64\n)\n"
  },
  {
    "tf.ragged.cross": "tf.ragged.cross(\n    inputs, name=None\n)\n"
  },
  {
    "tf.ragged.cross_hashed": "tf.ragged.cross_hashed(\n    inputs, num_buckets=0, hash_key=None, name=None\n)\n"
  },
  {
    "tf.ragged.map_flat_values": "tf.ragged.map_flat_values(\n    op, *args, **kwargs\n)\n"
  },
  {
    "tf.ragged.range": "tf.ragged.range(\n    starts,\n    limits=None,\n    deltas=1,\n    dtype=None,\n    name=None,\n    row_splits_dtype=tf.dtypes.int64\n)\n"
  },
  {
    "tf.ragged.row_splits_to_segment_ids": "tf.ragged.row_splits_to_segment_ids(\n    splits, name=None, out_type=None\n)\n"
  },
  {
    "tf.ragged.segment_ids_to_row_splits": "tf.ragged.segment_ids_to_row_splits(\n    segment_ids, num_segments=None, out_type=None, name=None\n)\n"
  },
  {
    "tf.ragged.stack": "tf.ragged.stack(\n    values: typing.List[ragged_tensor.RaggedOrDense], axis=0, name=None\n)\n"
  },
  {
    "tf.ragged.stack_dynamic_partitions": "tf.ragged.stack_dynamic_partitions(\n    data, partitions, num_partitions, name=None\n)\n"
  },
  {
    "tf.random.Generator": "tf.random.Generator(\n    copy_from=None, state=None, alg=None\n)\n"
  },
  {
    "tf.random.all_candidate_sampler": "tf.random.all_candidate_sampler(\n    true_classes, num_true, num_sampled, unique, seed=None, name=None\n)\n"
  },
  {
    "tf.random.categorical": "tf.random.categorical(\n    logits, num_samples, dtype=None, seed=None, name=None\n)\n"
  },
  {
    "tf.random.create_rng_state": "tf.random.create_rng_state(\n    seed, alg\n)\n"
  },
  {
    "tf.random.Generator": "tf.random.Generator(\n    copy_from=None, state=None, alg=None\n)\n"
  },
  {
    "tf.random.create_rng_state": "tf.random.create_rng_state(\n    seed, alg\n)\n"
  },
  {
    "tf.random.experimental.index_shuffle": "tf.random.experimental.index_shuffle(\n    index, seed, max_index\n)\n"
  },
  {
    "tf.random.set_global_generator": "tf.random.set_global_generator(\n    generator\n)\n"
  },
  {
    "tf.random.experimental.stateless_fold_in": "tf.random.experimental.stateless_fold_in(\n    seed, data, alg='auto_select'\n)\n"
  },
  {
    "tf.random.experimental.stateless_shuffle": "tf.random.experimental.stateless_shuffle(\n    value, seed, alg='auto_select', name=None\n)\n"
  },
  {
    "tf.random.experimental.stateless_split": "tf.random.experimental.stateless_split(\n    seed, num=2, alg='auto_select'\n)\n"
  },
  {
    "tf.random.fixed_unigram_candidate_sampler": "tf.random.fixed_unigram_candidate_sampler(\n    true_classes,\n    num_true,\n    num_sampled,\n    unique,\n    range_max,\n    vocab_file='',\n    distortion=1.0,\n    num_reserved_ids=0,\n    num_shards=1,\n    shard=0,\n    unigrams=(),\n    seed=None,\n    name=None\n)\n"
  },
  {
    "tf.random.gamma": "tf.random.gamma(\n    shape,\n    alpha,\n    beta=None,\n    dtype=tf.dtypes.float32,\n    seed=None,\n    name=None\n)\n"
  },
  {
    "tf.random.learned_unigram_candidate_sampler": "tf.random.learned_unigram_candidate_sampler(\n    true_classes,\n    num_true,\n    num_sampled,\n    unique,\n    range_max,\n    seed=None,\n    name=None\n)\n"
  },
  {
    "tf.random.log_uniform_candidate_sampler": "tf.random.log_uniform_candidate_sampler(\n    true_classes,\n    num_true,\n    num_sampled,\n    unique,\n    range_max,\n    seed=None,\n    name=None\n)\n"
  },
  {
    "tf.random.normal": "tf.random.normal(\n    shape,\n    mean=0.0,\n    stddev=1.0,\n    dtype=tf.dtypes.float32,\n    seed=None,\n    name=None\n)\n"
  },
  {
    "tf.random.poisson": "tf.random.poisson(\n    shape,\n    lam,\n    dtype=tf.dtypes.float32,\n    seed=None,\n    name=None\n)\n"
  },
  {
    "tf.random.set_global_generator": "tf.random.set_global_generator(\n    generator\n)\n"
  },
  {
    "tf.random.set_seed": "tf.random.set_seed(\n    seed\n)\n"
  },
  {
    "tf.random.shuffle": "tf.random.shuffle(\n    value, seed=None, name=None\n)\n"
  },
  {
    "tf.random.stateless_binomial": "tf.random.stateless_binomial(\n    shape,\n    seed,\n    counts,\n    probs,\n    output_dtype=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.random.stateless_categorical": "tf.random.stateless_categorical(\n    logits,\n    num_samples,\n    seed,\n    dtype=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.random.stateless_gamma": "tf.random.stateless_gamma(\n    shape,\n    seed,\n    alpha,\n    beta=None,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.random.stateless_normal": "tf.random.stateless_normal(\n    shape,\n    seed,\n    mean=0.0,\n    stddev=1.0,\n    dtype=tf.dtypes.float32,\n    name=None,\n    alg='auto_select'\n)\n"
  },
  {
    "tf.random.stateless_parameterized_truncated_normal": "tf.random.stateless_parameterized_truncated_normal(\n    shape, seed, means=0.0, stddevs=1.0, minvals=-2.0, maxvals=2.0, name=None\n)\n"
  },
  {
    "tf.random.stateless_poisson": "tf.random.stateless_poisson(\n    shape,\n    seed,\n    lam,\n    dtype=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.random.stateless_truncated_normal": "tf.random.stateless_truncated_normal(\n    shape,\n    seed,\n    mean=0.0,\n    stddev=1.0,\n    dtype=tf.dtypes.float32,\n    name=None,\n    alg='auto_select'\n)\n"
  },
  {
    "tf.random.stateless_uniform": "tf.random.stateless_uniform(\n    shape,\n    seed,\n    minval=0,\n    maxval=None,\n    dtype=tf.dtypes.float32,\n    name=None,\n    alg='auto_select'\n)\n"
  },
  {
    "tf.random.truncated_normal": "tf.random.truncated_normal(\n    shape,\n    mean=0.0,\n    stddev=1.0,\n    dtype=tf.dtypes.float32,\n    seed=None,\n    name=None\n)\n"
  },
  {
    "tf.random.uniform": "tf.random.uniform(\n    shape,\n    minval=0,\n    maxval=None,\n    dtype=tf.dtypes.float32,\n    seed=None,\n    name=None\n)\n"
  },
  {
    "tf.random.uniform_candidate_sampler": "tf.random.uniform_candidate_sampler(\n    true_classes,\n    num_true,\n    num_sampled,\n    unique,\n    range_max,\n    seed=None,\n    name=None\n)\n"
  },
  {
    "tf.random_index_shuffle": "tf.random_index_shuffle(\n    index, seed, max_index, rounds=4, name=None\n)\n"
  },
  {
    "tf.random_normal_initializer": "tf.random_normal_initializer(\n    mean=0.0, stddev=0.05, seed=None\n)\n"
  },
  {
    "tf.random_uniform_initializer": "tf.random_uniform_initializer(\n    minval=-0.05, maxval=0.05, seed=None\n)\n"
  },
  {
    "tf.rank": "tf.rank(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.Abort": "tf.raw_ops.Abort(\n    error_msg='', exit_without_error=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.Abs": "tf.raw_ops.Abs(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.AccumulateNV2": "tf.raw_ops.AccumulateNV2(\n    inputs, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.AccumulatorApplyGradient": "tf.raw_ops.AccumulatorApplyGradient(\n    handle, local_step, gradient, name=None\n)\n"
  },
  {
    "tf.raw_ops.AccumulatorNumAccumulated": "tf.raw_ops.AccumulatorNumAccumulated(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.AccumulatorSetGlobalStep": "tf.raw_ops.AccumulatorSetGlobalStep(\n    handle, new_global_step, name=None\n)\n"
  },
  {
    "tf.raw_ops.AccumulatorTakeGradient": "tf.raw_ops.AccumulatorTakeGradient(\n    handle, num_required, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.Acos": "tf.raw_ops.Acos(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Acosh": "tf.raw_ops.Acosh(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Add": "tf.raw_ops.Add(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.AddManySparseToTensorsMap": "tf.raw_ops.AddManySparseToTensorsMap(\n    sparse_indices,\n    sparse_values,\n    sparse_shape,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.AddN": "tf.raw_ops.AddN(\n    inputs, name=None\n)\n"
  },
  {
    "tf.raw_ops.AddSparseToTensorsMap": "tf.raw_ops.AddSparseToTensorsMap(\n    sparse_indices,\n    sparse_values,\n    sparse_shape,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.AddV2": "tf.raw_ops.AddV2(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.AdjustContrast": "tf.raw_ops.AdjustContrast(\n    images, contrast_factor, min_value, max_value, name=None\n)\n"
  },
  {
    "tf.raw_ops.AdjustContrastv2": "tf.raw_ops.AdjustContrastv2(\n    images, contrast_factor, name=None\n)\n"
  },
  {
    "tf.raw_ops.AdjustHue": "tf.raw_ops.AdjustHue(\n    images, delta, name=None\n)\n"
  },
  {
    "tf.raw_ops.AdjustSaturation": "tf.raw_ops.AdjustSaturation(\n    images, scale, name=None\n)\n"
  },
  {
    "tf.raw_ops.All": "tf.raw_ops.All(\n    input, axis, keep_dims=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.AllCandidateSampler": "tf.raw_ops.AllCandidateSampler(\n    true_classes, num_true, num_sampled, unique, seed=0, seed2=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.AllToAll": "tf.raw_ops.AllToAll(\n    input,\n    group_assignment,\n    concat_dimension,\n    split_dimension,\n    split_count,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Angle": "tf.raw_ops.Angle(\n    input,\n    Tout=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousHashTable": "tf.raw_ops.AnonymousHashTable(\n    key_dtype, value_dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousIterator": "tf.raw_ops.AnonymousIterator(\n    output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousIteratorV2": "tf.raw_ops.AnonymousIteratorV2(\n    output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousIteratorV3": "tf.raw_ops.AnonymousIteratorV3(\n    output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousMemoryCache": "tf.raw_ops.AnonymousMemoryCache(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousMultiDeviceIterator": "tf.raw_ops.AnonymousMultiDeviceIterator(\n    devices, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousMultiDeviceIteratorV3": "tf.raw_ops.AnonymousMultiDeviceIteratorV3(\n    devices, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousMutableDenseHashTable": "tf.raw_ops.AnonymousMutableDenseHashTable(\n    empty_key,\n    deleted_key,\n    value_dtype,\n    value_shape=[],\n    initial_num_buckets=131072,\n    max_load_factor=0.8,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousMutableHashTable": "tf.raw_ops.AnonymousMutableHashTable(\n    key_dtype, value_dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousMutableHashTableOfTensors": "tf.raw_ops.AnonymousMutableHashTableOfTensors(\n    key_dtype, value_dtype, value_shape=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousRandomSeedGenerator": "tf.raw_ops.AnonymousRandomSeedGenerator(\n    seed, seed2, name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousSeedGenerator": "tf.raw_ops.AnonymousSeedGenerator(\n    seed, seed2, reshuffle, name=None\n)\n"
  },
  {
    "tf.raw_ops.Any": "tf.raw_ops.Any(\n    input, axis, keep_dims=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyAdaMax": "tf.raw_ops.ApplyAdaMax(\n    var,\n    m,\n    v,\n    beta1_power,\n    lr,\n    beta1,\n    beta2,\n    epsilon,\n    grad,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyAdadelta": "tf.raw_ops.ApplyAdadelta(\n    var,\n    accum,\n    accum_update,\n    lr,\n    rho,\n    epsilon,\n    grad,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyAdagrad": "tf.raw_ops.ApplyAdagrad(\n    var, accum, lr, grad, use_locking=False, update_slots=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyAdagradDA": "tf.raw_ops.ApplyAdagradDA(\n    var,\n    gradient_accumulator,\n    gradient_squared_accumulator,\n    grad,\n    lr,\n    l1,\n    l2,\n    global_step,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyAdagradV2": "tf.raw_ops.ApplyAdagradV2(\n    var,\n    accum,\n    lr,\n    epsilon,\n    grad,\n    use_locking=False,\n    update_slots=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyAdam": "tf.raw_ops.ApplyAdam(\n    var,\n    m,\n    v,\n    beta1_power,\n    beta2_power,\n    lr,\n    beta1,\n    beta2,\n    epsilon,\n    grad,\n    use_locking=False,\n    use_nesterov=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyAddSign": "tf.raw_ops.ApplyAddSign(\n    var, m, lr, alpha, sign_decay, beta, grad, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyCenteredRMSProp": "tf.raw_ops.ApplyCenteredRMSProp(\n    var,\n    mg,\n    ms,\n    mom,\n    lr,\n    rho,\n    momentum,\n    epsilon,\n    grad,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyFtrl": "tf.raw_ops.ApplyFtrl(\n    var,\n    accum,\n    linear,\n    grad,\n    lr,\n    l1,\n    l2,\n    lr_power,\n    use_locking=False,\n    multiply_linear_by_lr=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyFtrlV2": "tf.raw_ops.ApplyFtrlV2(\n    var,\n    accum,\n    linear,\n    grad,\n    lr,\n    l1,\n    l2,\n    l2_shrinkage,\n    lr_power,\n    use_locking=False,\n    multiply_linear_by_lr=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyGradientDescent": "tf.raw_ops.ApplyGradientDescent(\n    var, alpha, delta, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyMomentum": "tf.raw_ops.ApplyMomentum(\n    var,\n    accum,\n    lr,\n    grad,\n    momentum,\n    use_locking=False,\n    use_nesterov=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyPowerSign": "tf.raw_ops.ApplyPowerSign(\n    var, m, lr, logbase, sign_decay, beta, grad, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyProximalAdagrad": "tf.raw_ops.ApplyProximalAdagrad(\n    var, accum, lr, l1, l2, grad, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyProximalGradientDescent": "tf.raw_ops.ApplyProximalGradientDescent(\n    var, alpha, l1, l2, delta, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyRMSProp": "tf.raw_ops.ApplyRMSProp(\n    var,\n    ms,\n    mom,\n    lr,\n    rho,\n    momentum,\n    epsilon,\n    grad,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ApproxTopK": "tf.raw_ops.ApproxTopK(\n    input,\n    k,\n    reduction_dimension=-1,\n    recall_target=0.95,\n    is_max_k=True,\n    reduction_input_size_override=-1,\n    aggregate_to_topk=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ApproximateEqual": "tf.raw_ops.ApproximateEqual(\n    x, y, tolerance=1e-05, name=None\n)\n"
  },
  {
    "tf.raw_ops.ArgMax": "tf.raw_ops.ArgMax(\n    input,\n    dimension,\n    output_type=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ArgMin": "tf.raw_ops.ArgMin(\n    input,\n    dimension,\n    output_type=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.AsString": "tf.raw_ops.AsString(\n    input,\n    precision=-1,\n    scientific=False,\n    shortest=False,\n    width=-1,\n    fill='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Asin": "tf.raw_ops.Asin(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Asinh": "tf.raw_ops.Asinh(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Assert": "tf.raw_ops.Assert(\n    condition, data, summarize=3, name=None\n)\n"
  },
  {
    "tf.raw_ops.AssertCardinalityDataset": "tf.raw_ops.AssertCardinalityDataset(\n    input_dataset, cardinality, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.AssertNextDataset": "tf.raw_ops.AssertNextDataset(\n    input_dataset, transformations, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.AssertPrevDataset": "tf.raw_ops.AssertPrevDataset(\n    input_dataset, transformations, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.Assign": "tf.raw_ops.Assign(\n    ref, value, validate_shape=True, use_locking=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.AssignAdd": "tf.raw_ops.AssignAdd(\n    ref, value, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.AssignAddVariableOp": "tf.raw_ops.AssignAddVariableOp(\n    resource, value, name=None\n)\n"
  },
  {
    "tf.raw_ops.AssignSub": "tf.raw_ops.AssignSub(\n    ref, value, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.AssignSubVariableOp": "tf.raw_ops.AssignSubVariableOp(\n    resource, value, name=None\n)\n"
  },
  {
    "tf.raw_ops.AssignVariableOp": "tf.raw_ops.AssignVariableOp(\n    resource, value, validate_shape=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.AssignVariableXlaConcatND": "tf.raw_ops.AssignVariableXlaConcatND(\n    resource, inputs, num_concats, paddings=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.Atan": "tf.raw_ops.Atan(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Atan2": "tf.raw_ops.Atan2(\n    y, x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Atanh": "tf.raw_ops.Atanh(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.AudioSpectrogram": "tf.raw_ops.AudioSpectrogram(\n    input, window_size, stride, magnitude_squared=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.AudioSummary": "tf.raw_ops.AudioSummary(\n    tag, tensor, sample_rate, max_outputs=3, name=None\n)\n"
  },
  {
    "tf.raw_ops.AudioSummaryV2": "tf.raw_ops.AudioSummaryV2(\n    tag, tensor, sample_rate, max_outputs=3, name=None\n)\n"
  },
  {
    "tf.raw_ops.AutoShardDataset": "tf.raw_ops.AutoShardDataset(\n    input_dataset,\n    num_workers,\n    index,\n    output_types,\n    output_shapes,\n    auto_shard_policy=0,\n    num_replicas=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.AvgPool": "tf.raw_ops.AvgPool(\n    value, ksize, strides, padding, data_format='NHWC', name=None\n)\n"
  },
  {
    "tf.raw_ops.AvgPool3D": "tf.raw_ops.AvgPool3D(\n    input, ksize, strides, padding, data_format='NDHWC', name=None\n)\n"
  },
  {
    "tf.raw_ops.AvgPool3DGrad": "tf.raw_ops.AvgPool3DGrad(\n    orig_input_shape,\n    grad,\n    ksize,\n    strides,\n    padding,\n    data_format='NDHWC',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.AvgPoolGrad": "tf.raw_ops.AvgPoolGrad(\n    orig_input_shape,\n    grad,\n    ksize,\n    strides,\n    padding,\n    data_format='NHWC',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BandedTriangularSolve": "tf.raw_ops.BandedTriangularSolve(\n    matrix, rhs, lower=True, adjoint=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.Barrier": "tf.raw_ops.Barrier(\n    component_types,\n    shapes=[],\n    capacity=-1,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BarrierClose": "tf.raw_ops.BarrierClose(\n    handle, cancel_pending_enqueues=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.BarrierIncompleteSize": "tf.raw_ops.BarrierIncompleteSize(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.BarrierInsertMany": "tf.raw_ops.BarrierInsertMany(\n    handle, keys, values, component_index, name=None\n)\n"
  },
  {
    "tf.raw_ops.BarrierReadySize": "tf.raw_ops.BarrierReadySize(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.BarrierTakeMany": "tf.raw_ops.BarrierTakeMany(\n    handle,\n    num_elements,\n    component_types,\n    allow_small_batch=False,\n    wait_for_incomplete=False,\n    timeout_ms=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Batch": "tf.raw_ops.Batch(\n    in_tensors,\n    num_batch_threads,\n    max_batch_size,\n    batch_timeout_micros,\n    grad_timeout_micros,\n    max_enqueued_batches=10,\n    allowed_batch_sizes=[],\n    container='',\n    shared_name='',\n    batching_queue='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchCholesky": "tf.raw_ops.BatchCholesky(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchCholeskyGrad": "tf.raw_ops.BatchCholeskyGrad(\n    l, grad, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchDataset": "tf.raw_ops.BatchDataset(\n    input_dataset,\n    batch_size,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchDatasetV2": "tf.raw_ops.BatchDatasetV2(\n    input_dataset,\n    batch_size,\n    drop_remainder,\n    output_types,\n    output_shapes,\n    parallel_copy=False,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchFFT": "tf.raw_ops.BatchFFT(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchFFT2D": "tf.raw_ops.BatchFFT2D(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchFFT3D": "tf.raw_ops.BatchFFT3D(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchFunction": "tf.raw_ops.BatchFunction(\n    in_tensors,\n    captured_tensors,\n    f,\n    num_batch_threads,\n    max_batch_size,\n    batch_timeout_micros,\n    Tout,\n    max_enqueued_batches=10,\n    allowed_batch_sizes=[],\n    container='',\n    shared_name='',\n    batching_queue='',\n    enable_large_batch_splitting=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchIFFT": "tf.raw_ops.BatchIFFT(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchIFFT2D": "tf.raw_ops.BatchIFFT2D(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchIFFT3D": "tf.raw_ops.BatchIFFT3D(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatMul": "tf.raw_ops.BatchMatMul(\n    x, y, adj_x=False, adj_y=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatMulV2": "tf.raw_ops.BatchMatMulV2(\n    x, y, adj_x=False, adj_y=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatMulV3": "tf.raw_ops.BatchMatMulV3(\n    x, y, Tout, adj_x=False, adj_y=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatrixBandPart": "tf.raw_ops.BatchMatrixBandPart(\n    input, num_lower, num_upper, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatrixDeterminant": "tf.raw_ops.BatchMatrixDeterminant(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatrixDiag": "tf.raw_ops.BatchMatrixDiag(\n    diagonal, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatrixDiagPart": "tf.raw_ops.BatchMatrixDiagPart(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatrixInverse": "tf.raw_ops.BatchMatrixInverse(\n    input, adjoint=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatrixSetDiag": "tf.raw_ops.BatchMatrixSetDiag(\n    input, diagonal, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatrixSolve": "tf.raw_ops.BatchMatrixSolve(\n    matrix, rhs, adjoint=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatrixSolveLs": "tf.raw_ops.BatchMatrixSolveLs(\n    matrix, rhs, l2_regularizer, fast=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatrixTriangularSolve": "tf.raw_ops.BatchMatrixTriangularSolve(\n    matrix, rhs, lower=True, adjoint=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchNormWithGlobalNormalization": "tf.raw_ops.BatchNormWithGlobalNormalization(\n    t,\n    m,\n    v,\n    beta,\n    gamma,\n    variance_epsilon,\n    scale_after_normalization,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchNormWithGlobalNormalizationGrad": "tf.raw_ops.BatchNormWithGlobalNormalizationGrad(\n    t,\n    m,\n    v,\n    gamma,\n    backprop,\n    variance_epsilon,\n    scale_after_normalization,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchSelfAdjointEig": "tf.raw_ops.BatchSelfAdjointEig(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchSelfAdjointEigV2": "tf.raw_ops.BatchSelfAdjointEigV2(\n    input, compute_v=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchSvd": "tf.raw_ops.BatchSvd(\n    input, compute_uv=True, full_matrices=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchToSpace": "tf.raw_ops.BatchToSpace(\n    input, crops, block_size, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchToSpaceND": "tf.raw_ops.BatchToSpaceND(\n    input, block_shape, crops, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselI0": "tf.raw_ops.BesselI0(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselI0e": "tf.raw_ops.BesselI0e(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselI1": "tf.raw_ops.BesselI1(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselI1e": "tf.raw_ops.BesselI1e(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselJ0": "tf.raw_ops.BesselJ0(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselJ1": "tf.raw_ops.BesselJ1(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselK0": "tf.raw_ops.BesselK0(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselK0e": "tf.raw_ops.BesselK0e(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselK1": "tf.raw_ops.BesselK1(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselK1e": "tf.raw_ops.BesselK1e(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselY0": "tf.raw_ops.BesselY0(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselY1": "tf.raw_ops.BesselY1(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Betainc": "tf.raw_ops.Betainc(\n    a, b, x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BiasAdd": "tf.raw_ops.BiasAdd(\n    value, bias, data_format='NHWC', name=None\n)\n"
  },
  {
    "tf.raw_ops.BiasAddGrad": "tf.raw_ops.BiasAddGrad(\n    out_backprop, data_format='NHWC', name=None\n)\n"
  },
  {
    "tf.raw_ops.BiasAddV1": "tf.raw_ops.BiasAddV1(\n    value, bias, name=None\n)\n"
  },
  {
    "tf.raw_ops.Bincount": "tf.raw_ops.Bincount(\n    arr, size, weights, name=None\n)\n"
  },
  {
    "tf.raw_ops.Bitcast": "tf.raw_ops.Bitcast(\n    input, type, name=None\n)\n"
  },
  {
    "tf.raw_ops.BitwiseAnd": "tf.raw_ops.BitwiseAnd(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.BitwiseOr": "tf.raw_ops.BitwiseOr(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.BitwiseXor": "tf.raw_ops.BitwiseXor(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.BlockLSTM": "tf.raw_ops.BlockLSTM(\n    seq_len_max,\n    x,\n    cs_prev,\n    h_prev,\n    w,\n    wci,\n    wcf,\n    wco,\n    b,\n    forget_bias=1,\n    cell_clip=3,\n    use_peephole=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BlockLSTMGrad": "tf.raw_ops.BlockLSTMGrad(\n    seq_len_max,\n    x,\n    cs_prev,\n    h_prev,\n    w,\n    wci,\n    wcf,\n    wco,\n    b,\n    i,\n    cs,\n    f,\n    o,\n    ci,\n    co,\n    h,\n    cs_grad,\n    h_grad,\n    use_peephole,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BlockLSTMGradV2": "tf.raw_ops.BlockLSTMGradV2(\n    seq_len_max,\n    x,\n    cs_prev,\n    h_prev,\n    w,\n    wci,\n    wcf,\n    wco,\n    b,\n    i,\n    cs,\n    f,\n    o,\n    ci,\n    co,\n    h,\n    cs_grad,\n    h_grad,\n    use_peephole,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BlockLSTMV2": "tf.raw_ops.BlockLSTMV2(\n    seq_len_max,\n    x,\n    cs_prev,\n    h_prev,\n    w,\n    wci,\n    wcf,\n    wco,\n    b,\n    cell_clip=0,\n    use_peephole=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesAggregateStats": "tf.raw_ops.BoostedTreesAggregateStats(\n    node_ids, gradients, hessians, feature, max_splits, num_buckets, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesBucketize": "tf.raw_ops.BoostedTreesBucketize(\n    float_values, bucket_boundaries, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesCalculateBestFeatureSplit": "tf.raw_ops.BoostedTreesCalculateBestFeatureSplit(\n    node_id_range,\n    stats_summary,\n    l1,\n    l2,\n    tree_complexity,\n    min_node_weight,\n    logits_dimension,\n    split_type='inequality',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesCalculateBestFeatureSplitV2": "tf.raw_ops.BoostedTreesCalculateBestFeatureSplitV2(\n    node_id_range,\n    stats_summaries_list,\n    split_types,\n    candidate_feature_ids,\n    l1,\n    l2,\n    tree_complexity,\n    min_node_weight,\n    logits_dimension,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesCalculateBestGainsPerFeature": "tf.raw_ops.BoostedTreesCalculateBestGainsPerFeature(\n    node_id_range,\n    stats_summary_list,\n    l1,\n    l2,\n    tree_complexity,\n    min_node_weight,\n    max_splits,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesCenterBias": "tf.raw_ops.BoostedTreesCenterBias(\n    tree_ensemble_handle, mean_gradients, mean_hessians, l1, l2, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesCreateEnsemble": "tf.raw_ops.BoostedTreesCreateEnsemble(\n    tree_ensemble_handle, stamp_token, tree_ensemble_serialized, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesCreateQuantileStreamResource": "tf.raw_ops.BoostedTreesCreateQuantileStreamResource(\n    quantile_stream_resource_handle,\n    epsilon,\n    num_streams,\n    max_elements=1099511627776,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesDeserializeEnsemble": "tf.raw_ops.BoostedTreesDeserializeEnsemble(\n    tree_ensemble_handle, stamp_token, tree_ensemble_serialized, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesEnsembleResourceHandleOp": "tf.raw_ops.BoostedTreesEnsembleResourceHandleOp(\n    container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesExampleDebugOutputs": "tf.raw_ops.BoostedTreesExampleDebugOutputs(\n    tree_ensemble_handle, bucketized_features, logits_dimension, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesFlushQuantileSummaries": "tf.raw_ops.BoostedTreesFlushQuantileSummaries(\n    quantile_stream_resource_handle, num_features, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesGetEnsembleStates": "tf.raw_ops.BoostedTreesGetEnsembleStates(\n    tree_ensemble_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesMakeQuantileSummaries": "tf.raw_ops.BoostedTreesMakeQuantileSummaries(\n    float_values, example_weights, epsilon, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesMakeStatsSummary": "tf.raw_ops.BoostedTreesMakeStatsSummary(\n    node_ids,\n    gradients,\n    hessians,\n    bucketized_features_list,\n    max_splits,\n    num_buckets,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesPredict": "tf.raw_ops.BoostedTreesPredict(\n    tree_ensemble_handle, bucketized_features, logits_dimension, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesQuantileStreamResourceAddSummaries": "tf.raw_ops.BoostedTreesQuantileStreamResourceAddSummaries(\n    quantile_stream_resource_handle, summaries, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesQuantileStreamResourceDeserialize": "tf.raw_ops.BoostedTreesQuantileStreamResourceDeserialize(\n    quantile_stream_resource_handle, bucket_boundaries, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesQuantileStreamResourceFlush": "tf.raw_ops.BoostedTreesQuantileStreamResourceFlush(\n    quantile_stream_resource_handle,\n    num_buckets,\n    generate_quantiles=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesQuantileStreamResourceGetBucketBoundaries": "tf.raw_ops.BoostedTreesQuantileStreamResourceGetBucketBoundaries(\n    quantile_stream_resource_handle, num_features, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesQuantileStreamResourceHandleOp": "tf.raw_ops.BoostedTreesQuantileStreamResourceHandleOp(\n    container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesSerializeEnsemble": "tf.raw_ops.BoostedTreesSerializeEnsemble(\n    tree_ensemble_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesSparseAggregateStats": "tf.raw_ops.BoostedTreesSparseAggregateStats(\n    node_ids,\n    gradients,\n    hessians,\n    feature_indices,\n    feature_values,\n    feature_shape,\n    max_splits,\n    num_buckets,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesSparseCalculateBestFeatureSplit": "tf.raw_ops.BoostedTreesSparseCalculateBestFeatureSplit(\n    node_id_range,\n    stats_summary_indices,\n    stats_summary_values,\n    stats_summary_shape,\n    l1,\n    l2,\n    tree_complexity,\n    min_node_weight,\n    logits_dimension,\n    split_type='inequality',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesTrainingPredict": "tf.raw_ops.BoostedTreesTrainingPredict(\n    tree_ensemble_handle,\n    cached_tree_ids,\n    cached_node_ids,\n    bucketized_features,\n    logits_dimension,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesUpdateEnsemble": "tf.raw_ops.BoostedTreesUpdateEnsemble(\n    tree_ensemble_handle,\n    feature_ids,\n    node_ids,\n    gains,\n    thresholds,\n    left_node_contribs,\n    right_node_contribs,\n    max_depth,\n    learning_rate,\n    pruning_mode,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesUpdateEnsembleV2": "tf.raw_ops.BoostedTreesUpdateEnsembleV2(\n    tree_ensemble_handle,\n    feature_ids,\n    dimension_ids,\n    node_ids,\n    gains,\n    thresholds,\n    left_node_contribs,\n    right_node_contribs,\n    split_types,\n    max_depth,\n    learning_rate,\n    pruning_mode,\n    logits_dimension=1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BroadcastArgs": "tf.raw_ops.BroadcastArgs(\n    s0, s1, name=None\n)\n"
  },
  {
    "tf.raw_ops.BroadcastGradientArgs": "tf.raw_ops.BroadcastGradientArgs(\n    s0, s1, name=None\n)\n"
  },
  {
    "tf.raw_ops.BroadcastTo": "tf.raw_ops.BroadcastTo(\n    input, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.Bucketize": "tf.raw_ops.Bucketize(\n    input, boundaries, name=None\n)\n"
  },
  {
    "tf.raw_ops.BytesProducedStatsDataset": "tf.raw_ops.BytesProducedStatsDataset(\n    input_dataset, tag, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.CSRSparseMatrixComponents": "tf.raw_ops.CSRSparseMatrixComponents(\n    csr_sparse_matrix, index, type, name=None\n)\n"
  },
  {
    "tf.raw_ops.CSRSparseMatrixToDense": "tf.raw_ops.CSRSparseMatrixToDense(\n    sparse_input, type, name=None\n)\n"
  },
  {
    "tf.raw_ops.CSRSparseMatrixToSparseTensor": "tf.raw_ops.CSRSparseMatrixToSparseTensor(\n    sparse_matrix, type, name=None\n)\n"
  },
  {
    "tf.raw_ops.CSVDataset": "tf.raw_ops.CSVDataset(\n    filenames,\n    compression_type,\n    buffer_size,\n    header,\n    field_delim,\n    use_quote_delim,\n    na_value,\n    select_cols,\n    record_defaults,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CSVDatasetV2": "tf.raw_ops.CSVDatasetV2(\n    filenames,\n    compression_type,\n    buffer_size,\n    header,\n    field_delim,\n    use_quote_delim,\n    na_value,\n    select_cols,\n    record_defaults,\n    exclude_cols,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CTCBeamSearchDecoder": "tf.raw_ops.CTCBeamSearchDecoder(\n    inputs,\n    sequence_length,\n    beam_width,\n    top_paths,\n    merge_repeated=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CTCGreedyDecoder": "tf.raw_ops.CTCGreedyDecoder(\n    inputs, sequence_length, merge_repeated=False, blank_index=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.CTCLoss": "tf.raw_ops.CTCLoss(\n    inputs,\n    labels_indices,\n    labels_values,\n    sequence_length,\n    preprocess_collapse_repeated=False,\n    ctc_merge_repeated=True,\n    ignore_longer_outputs_than_inputs=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CTCLossV2": "tf.raw_ops.CTCLossV2(\n    inputs,\n    labels_indices,\n    labels_values,\n    sequence_length,\n    preprocess_collapse_repeated=False,\n    ctc_merge_repeated=True,\n    ignore_longer_outputs_than_inputs=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CacheDataset": "tf.raw_ops.CacheDataset(\n    input_dataset,\n    filename,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CacheDatasetV2": "tf.raw_ops.CacheDatasetV2(\n    input_dataset,\n    filename,\n    cache,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Case": "tf.raw_ops.Case(\n    branch_index, input, Tout, branches, output_shapes=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.Cast": "tf.raw_ops.Cast(\n    x, DstT, Truncate=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.Ceil": "tf.raw_ops.Ceil(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.CheckNumerics": "tf.raw_ops.CheckNumerics(\n    tensor, message, name=None\n)\n"
  },
  {
    "tf.raw_ops.CheckNumericsV2": "tf.raw_ops.CheckNumericsV2(\n    tensor, message, name=None\n)\n"
  },
  {
    "tf.raw_ops.Cholesky": "tf.raw_ops.Cholesky(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.CholeskyGrad": "tf.raw_ops.CholeskyGrad(\n    l, grad, name=None\n)\n"
  },
  {
    "tf.raw_ops.ChooseFastestBranchDataset": "tf.raw_ops.ChooseFastestBranchDataset(\n    input_dataset,\n    ratio_numerator,\n    ratio_denominator,\n    other_arguments,\n    num_elements_per_branch,\n    branches,\n    other_arguments_lengths,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ChooseFastestDataset": "tf.raw_ops.ChooseFastestDataset(\n    input_datasets, num_experiments, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ClipByValue": "tf.raw_ops.ClipByValue(\n    t, clip_value_min, clip_value_max, name=None\n)\n"
  },
  {
    "tf.raw_ops.CloseSummaryWriter": "tf.raw_ops.CloseSummaryWriter(\n    writer, name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveAllToAllV3": "tf.raw_ops.CollectiveAllToAllV3(\n    input, communicator, group_assignment, timeout_seconds=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveAssignGroupV2": "tf.raw_ops.CollectiveAssignGroupV2(\n    group_assignment, device_index, base_key, name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveBcastRecv": "tf.raw_ops.CollectiveBcastRecv(\n    T,\n    group_size,\n    group_key,\n    instance_key,\n    shape,\n    communication_hint='auto',\n    timeout_seconds=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveBcastRecvV2": "tf.raw_ops.CollectiveBcastRecvV2(\n    group_size,\n    group_key,\n    instance_key,\n    shape,\n    T,\n    communication_hint='auto',\n    timeout_seconds=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveBcastSend": "tf.raw_ops.CollectiveBcastSend(\n    input,\n    group_size,\n    group_key,\n    instance_key,\n    shape,\n    communication_hint='auto',\n    timeout_seconds=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveBcastSendV2": "tf.raw_ops.CollectiveBcastSendV2(\n    input,\n    group_size,\n    group_key,\n    instance_key,\n    communication_hint='auto',\n    timeout_seconds=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveGather": "tf.raw_ops.CollectiveGather(\n    input,\n    group_size,\n    group_key,\n    instance_key,\n    shape,\n    communication_hint='auto',\n    timeout_seconds=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveGatherV2": "tf.raw_ops.CollectiveGatherV2(\n    input,\n    group_size,\n    group_key,\n    instance_key,\n    ordering_token,\n    communication_hint='auto',\n    timeout_seconds=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveInitializeCommunicator": "tf.raw_ops.CollectiveInitializeCommunicator(\n    group_key,\n    rank,\n    group_size,\n    communication_hint='auto',\n    timeout_seconds=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectivePermute": "tf.raw_ops.CollectivePermute(\n    input, source_target_pairs, name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveReduce": "tf.raw_ops.CollectiveReduce(\n    input,\n    group_size,\n    group_key,\n    instance_key,\n    merge_op,\n    final_op,\n    subdiv_offsets,\n    wait_for=[],\n    communication_hint='auto',\n    timeout_seconds=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveReduceV2": "tf.raw_ops.CollectiveReduceV2(\n    input,\n    group_size,\n    group_key,\n    instance_key,\n    ordering_token,\n    merge_op,\n    final_op,\n    communication_hint='auto',\n    timeout_seconds=0,\n    max_subdivs_per_device=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveReduceV3": "tf.raw_ops.CollectiveReduceV3(\n    input,\n    communicator,\n    group_assignment,\n    reduction,\n    timeout_seconds=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CombinedNonMaxSuppression": "tf.raw_ops.CombinedNonMaxSuppression(\n    boxes,\n    scores,\n    max_output_size_per_class,\n    max_total_size,\n    iou_threshold,\n    score_threshold,\n    pad_per_class=False,\n    clip_boxes=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Complex": "tf.raw_ops.Complex(\n    real,\n    imag,\n    Tout=tf.dtypes.complex64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ComplexAbs": "tf.raw_ops.ComplexAbs(\n    x,\n    Tout=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CompositeTensorVariantFromComponents": "tf.raw_ops.CompositeTensorVariantFromComponents(\n    components, metadata, name=None\n)\n"
  },
  {
    "tf.raw_ops.CompositeTensorVariantToComponents": "tf.raw_ops.CompositeTensorVariantToComponents(\n    encoded, metadata, Tcomponents, name=None\n)\n"
  },
  {
    "tf.raw_ops.CompressElement": "tf.raw_ops.CompressElement(\n    components, name=None\n)\n"
  },
  {
    "tf.raw_ops.ComputeAccidentalHits": "tf.raw_ops.ComputeAccidentalHits(\n    true_classes, sampled_candidates, num_true, seed=0, seed2=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.ComputeBatchSize": "tf.raw_ops.ComputeBatchSize(\n    input_dataset, name=None\n)\n"
  },
  {
    "tf.raw_ops.Concat": "tf.raw_ops.Concat(\n    concat_dim, values, name=None\n)\n"
  },
  {
    "tf.raw_ops.ConcatOffset": "tf.raw_ops.ConcatOffset(\n    concat_dim, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.ConcatV2": "tf.raw_ops.ConcatV2(\n    values, axis, name=None\n)\n"
  },
  {
    "tf.raw_ops.ConcatenateDataset": "tf.raw_ops.ConcatenateDataset(\n    input_dataset,\n    another_dataset,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ConditionalAccumulator": "tf.raw_ops.ConditionalAccumulator(\n    dtype,\n    shape,\n    container='',\n    shared_name='',\n    reduction_type='MEAN',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ConfigureDistributedTPU": "tf.raw_ops.ConfigureDistributedTPU(\n    embedding_config='',\n    tpu_embedding_config='',\n    is_global_init=False,\n    enable_whole_mesh_compilations=False,\n    compilation_failure_closes_chips=True,\n    tpu_cancellation_closes_chips=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ConfigureTPUEmbedding": "tf.raw_ops.ConfigureTPUEmbedding(\n    config, name=None\n)\n"
  },
  {
    "tf.raw_ops.Conj": "tf.raw_ops.Conj(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.ConjugateTranspose": "tf.raw_ops.ConjugateTranspose(\n    x, perm, name=None\n)\n"
  },
  {
    "tf.raw_ops.Const": "tf.raw_ops.Const(\n    value, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.ConsumeMutexLock": "tf.raw_ops.ConsumeMutexLock(\n    mutex_lock, name=None\n)\n"
  },
  {
    "tf.raw_ops.ControlTrigger": "tf.raw_ops.ControlTrigger(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Conv2D": "tf.raw_ops.Conv2D(\n    input,\n    filter,\n    strides,\n    padding,\n    use_cudnn_on_gpu=True,\n    explicit_paddings=[],\n    data_format='NHWC',\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Conv2DBackpropFilter": "tf.raw_ops.Conv2DBackpropFilter(\n    input,\n    filter_sizes,\n    out_backprop,\n    strides,\n    padding,\n    use_cudnn_on_gpu=True,\n    explicit_paddings=[],\n    data_format='NHWC',\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Conv2DBackpropInput": "tf.raw_ops.Conv2DBackpropInput(\n    input_sizes,\n    filter,\n    out_backprop,\n    strides,\n    padding,\n    use_cudnn_on_gpu=True,\n    explicit_paddings=[],\n    data_format='NHWC',\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Conv3D": "tf.raw_ops.Conv3D(\n    input,\n    filter,\n    strides,\n    padding,\n    data_format='NDHWC',\n    dilations=[1, 1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Conv3DBackpropFilter": "tf.raw_ops.Conv3DBackpropFilter(\n    input,\n    filter,\n    out_backprop,\n    strides,\n    padding,\n    dilations=[1, 1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Conv3DBackpropFilterV2": "tf.raw_ops.Conv3DBackpropFilterV2(\n    input,\n    filter_sizes,\n    out_backprop,\n    strides,\n    padding,\n    data_format='NDHWC',\n    dilations=[1, 1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Conv3DBackpropInput": "tf.raw_ops.Conv3DBackpropInput(\n    input,\n    filter,\n    out_backprop,\n    strides,\n    padding,\n    dilations=[1, 1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Conv3DBackpropInputV2": "tf.raw_ops.Conv3DBackpropInputV2(\n    input_sizes,\n    filter,\n    out_backprop,\n    strides,\n    padding,\n    data_format='NDHWC',\n    dilations=[1, 1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Copy": "tf.raw_ops.Copy(\n    input, tensor_name='', debug_ops_spec=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.CopyHost": "tf.raw_ops.CopyHost(\n    input, tensor_name='', debug_ops_spec=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.Cos": "tf.raw_ops.Cos(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Cosh": "tf.raw_ops.Cosh(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.CountUpTo": "tf.raw_ops.CountUpTo(\n    ref, limit, name=None\n)\n"
  },
  {
    "tf.raw_ops.CreateSummaryDbWriter": "tf.raw_ops.CreateSummaryDbWriter(\n    writer, db_uri, experiment_name, run_name, user_name, name=None\n)\n"
  },
  {
    "tf.raw_ops.CreateSummaryFileWriter": "tf.raw_ops.CreateSummaryFileWriter(\n    writer, logdir, max_queue, flush_millis, filename_suffix, name=None\n)\n"
  },
  {
    "tf.raw_ops.CropAndResize": "tf.raw_ops.CropAndResize(\n    image,\n    boxes,\n    box_ind,\n    crop_size,\n    method='bilinear',\n    extrapolation_value=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CropAndResizeGradBoxes": "tf.raw_ops.CropAndResizeGradBoxes(\n    grads, image, boxes, box_ind, method='bilinear', name=None\n)\n"
  },
  {
    "tf.raw_ops.CropAndResizeGradImage": "tf.raw_ops.CropAndResizeGradImage(\n    grads,\n    boxes,\n    box_ind,\n    image_size,\n    T,\n    method='bilinear',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Cross": "tf.raw_ops.Cross(\n    a, b, name=None\n)\n"
  },
  {
    "tf.raw_ops.CrossReplicaSum": "tf.raw_ops.CrossReplicaSum(\n    input, group_assignment, name=None\n)\n"
  },
  {
    "tf.raw_ops.CudnnRNN": "tf.raw_ops.CudnnRNN(\n    input,\n    input_h,\n    input_c,\n    params,\n    rnn_mode='lstm',\n    input_mode='linear_input',\n    direction='unidirectional',\n    dropout=0,\n    seed=0,\n    seed2=0,\n    is_training=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CudnnRNNBackprop": "tf.raw_ops.CudnnRNNBackprop(\n    input,\n    input_h,\n    input_c,\n    params,\n    output,\n    output_h,\n    output_c,\n    output_backprop,\n    output_h_backprop,\n    output_c_backprop,\n    reserve_space,\n    rnn_mode='lstm',\n    input_mode='linear_input',\n    direction='unidirectional',\n    dropout=0,\n    seed=0,\n    seed2=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CudnnRNNBackpropV2": "tf.raw_ops.CudnnRNNBackpropV2(\n    input,\n    input_h,\n    input_c,\n    params,\n    output,\n    output_h,\n    output_c,\n    output_backprop,\n    output_h_backprop,\n    output_c_backprop,\n    reserve_space,\n    host_reserved,\n    rnn_mode='lstm',\n    input_mode='linear_input',\n    direction='unidirectional',\n    dropout=0,\n    seed=0,\n    seed2=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CudnnRNNBackpropV3": "tf.raw_ops.CudnnRNNBackpropV3(\n    input,\n    input_h,\n    input_c,\n    params,\n    sequence_lengths,\n    output,\n    output_h,\n    output_c,\n    output_backprop,\n    output_h_backprop,\n    output_c_backprop,\n    reserve_space,\n    host_reserved,\n    rnn_mode='lstm',\n    input_mode='linear_input',\n    direction='unidirectional',\n    dropout=0,\n    seed=0,\n    seed2=0,\n    num_proj=0,\n    time_major=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CudnnRNNCanonicalToParams": "tf.raw_ops.CudnnRNNCanonicalToParams(\n    num_layers,\n    num_units,\n    input_size,\n    weights,\n    biases,\n    rnn_mode='lstm',\n    input_mode='linear_input',\n    direction='unidirectional',\n    dropout=0,\n    seed=0,\n    seed2=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CudnnRNNCanonicalToParamsV2": "tf.raw_ops.CudnnRNNCanonicalToParamsV2(\n    num_layers,\n    num_units,\n    input_size,\n    weights,\n    biases,\n    rnn_mode='lstm',\n    input_mode='linear_input',\n    direction='unidirectional',\n    dropout=0,\n    seed=0,\n    seed2=0,\n    num_proj=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CudnnRNNParamsSize": "tf.raw_ops.CudnnRNNParamsSize(\n    num_layers,\n    num_units,\n    input_size,\n    T,\n    S,\n    rnn_mode='lstm',\n    input_mode='linear_input',\n    direction='unidirectional',\n    dropout=0,\n    seed=0,\n    seed2=0,\n    num_proj=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CudnnRNNParamsToCanonical": "tf.raw_ops.CudnnRNNParamsToCanonical(\n    num_layers,\n    num_units,\n    input_size,\n    params,\n    num_params,\n    rnn_mode='lstm',\n    input_mode='linear_input',\n    direction='unidirectional',\n    dropout=0,\n    seed=0,\n    seed2=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CudnnRNNParamsToCanonicalV2": "tf.raw_ops.CudnnRNNParamsToCanonicalV2(\n    num_layers,\n    num_units,\n    input_size,\n    params,\n    num_params_weights,\n    num_params_biases,\n    rnn_mode='lstm',\n    input_mode='linear_input',\n    direction='unidirectional',\n    dropout=0,\n    seed=0,\n    seed2=0,\n    num_proj=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CudnnRNNV2": "tf.raw_ops.CudnnRNNV2(\n    input,\n    input_h,\n    input_c,\n    params,\n    rnn_mode='lstm',\n    input_mode='linear_input',\n    direction='unidirectional',\n    dropout=0,\n    seed=0,\n    seed2=0,\n    is_training=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CudnnRNNV3": "tf.raw_ops.CudnnRNNV3(\n    input,\n    input_h,\n    input_c,\n    params,\n    sequence_lengths,\n    rnn_mode='lstm',\n    input_mode='linear_input',\n    direction='unidirectional',\n    dropout=0,\n    seed=0,\n    seed2=0,\n    num_proj=0,\n    is_training=True,\n    time_major=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Cumprod": "tf.raw_ops.Cumprod(\n    x, axis, exclusive=False, reverse=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.Cumsum": "tf.raw_ops.Cumsum(\n    x, axis, exclusive=False, reverse=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.CumulativeLogsumexp": "tf.raw_ops.CumulativeLogsumexp(\n    x, axis, exclusive=False, reverse=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.DataFormatDimMap": "tf.raw_ops.DataFormatDimMap(\n    x, src_format='NHWC', dst_format='NCHW', name=None\n)\n"
  },
  {
    "tf.raw_ops.DataFormatVecPermute": "tf.raw_ops.DataFormatVecPermute(\n    x, src_format='NHWC', dst_format='NCHW', name=None\n)\n"
  },
  {
    "tf.raw_ops.DataServiceDataset": "tf.raw_ops.DataServiceDataset(\n    dataset_id,\n    processing_mode,\n    address,\n    protocol,\n    job_name,\n    max_outstanding_requests,\n    iteration_counter,\n    output_types,\n    output_shapes,\n    task_refresh_interval_hint_ms=-1,\n    data_transfer_protocol='',\n    target_workers='AUTO',\n    cross_trainer_cache_options='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DataServiceDatasetV2": "tf.raw_ops.DataServiceDatasetV2(\n    dataset_id,\n    processing_mode,\n    address,\n    protocol,\n    job_name,\n    consumer_index,\n    num_consumers,\n    max_outstanding_requests,\n    iteration_counter,\n    output_types,\n    output_shapes,\n    task_refresh_interval_hint_ms=-1,\n    data_transfer_protocol='',\n    target_workers='AUTO',\n    cross_trainer_cache_options='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DataServiceDatasetV3": "tf.raw_ops.DataServiceDatasetV3(\n    dataset_id,\n    processing_mode,\n    address,\n    protocol,\n    job_name,\n    consumer_index,\n    num_consumers,\n    max_outstanding_requests,\n    iteration_counter,\n    output_types,\n    output_shapes,\n    uncompress_fn,\n    task_refresh_interval_hint_ms=-1,\n    data_transfer_protocol='',\n    target_workers='AUTO',\n    uncompress=False,\n    cross_trainer_cache_options='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DataServiceDatasetV4": "tf.raw_ops.DataServiceDatasetV4(\n    dataset_id,\n    processing_mode,\n    address,\n    protocol,\n    job_name,\n    consumer_index,\n    num_consumers,\n    max_outstanding_requests,\n    iteration_counter,\n    output_types,\n    output_shapes,\n    uncompress_fn,\n    task_refresh_interval_hint_ms=-1,\n    data_transfer_protocol='',\n    target_workers='AUTO',\n    uncompress=False,\n    cross_trainer_cache_options='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DatasetCardinality": "tf.raw_ops.DatasetCardinality(\n    input_dataset, name=None\n)\n"
  },
  {
    "tf.raw_ops.DatasetFromGraph": "tf.raw_ops.DatasetFromGraph(\n    graph_def, name=None\n)\n"
  },
  {
    "tf.raw_ops.DatasetToGraph": "tf.raw_ops.DatasetToGraph(\n    input_dataset,\n    stateful_whitelist=[],\n    allow_stateful=False,\n    strip_device_assignment=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DatasetToGraphV2": "tf.raw_ops.DatasetToGraphV2(\n    input_dataset,\n    external_state_policy=0,\n    strip_device_assignment=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DatasetToSingleElement": "tf.raw_ops.DatasetToSingleElement(\n    dataset, output_types, output_shapes, metadata='', name=None\n)\n"
  },
  {
    "tf.raw_ops.DatasetToTFRecord": "tf.raw_ops.DatasetToTFRecord(\n    input_dataset, filename, compression_type, name=None\n)\n"
  },
  {
    "tf.raw_ops.Dawsn": "tf.raw_ops.Dawsn(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.DebugGradientIdentity": "tf.raw_ops.DebugGradientIdentity(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.DebugGradientRefIdentity": "tf.raw_ops.DebugGradientRefIdentity(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.DebugIdentity": "tf.raw_ops.DebugIdentity(\n    input,\n    device_name='',\n    tensor_name='',\n    debug_urls=[],\n    gated_grpc=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DebugIdentityV2": "tf.raw_ops.DebugIdentityV2(\n    input,\n    tfdbg_context_id='',\n    op_name='',\n    output_slot=-1,\n    tensor_debug_mode=-1,\n    debug_urls=[],\n    circular_buffer_size=1000,\n    tfdbg_run_id='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DebugNanCount": "tf.raw_ops.DebugNanCount(\n    input,\n    device_name='',\n    tensor_name='',\n    debug_urls=[],\n    gated_grpc=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DebugNumericSummary": "tf.raw_ops.DebugNumericSummary(\n    input,\n    device_name='',\n    tensor_name='',\n    debug_urls=[],\n    lower_bound=float('-inf'),\n    upper_bound=float('inf'),\n    mute_if_healthy=False,\n    gated_grpc=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DebugNumericSummaryV2": "tf.raw_ops.DebugNumericSummaryV2(\n    input,\n    output_dtype=tf.dtypes.float32,\n    tensor_debug_mode=-1,\n    tensor_id=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeAndCropJpeg": "tf.raw_ops.DecodeAndCropJpeg(\n    contents,\n    crop_window,\n    channels=0,\n    ratio=1,\n    fancy_upscaling=True,\n    try_recover_truncated=False,\n    acceptable_fraction=1,\n    dct_method='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeBase64": "tf.raw_ops.DecodeBase64(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeBmp": "tf.raw_ops.DecodeBmp(\n    contents, channels=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeCSV": "tf.raw_ops.DecodeCSV(\n    records,\n    record_defaults,\n    field_delim=',',\n    use_quote_delim=True,\n    na_value='',\n    select_cols=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeCompressed": "tf.raw_ops.DecodeCompressed(\n    bytes, compression_type='', name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeGif": "tf.raw_ops.DecodeGif(\n    contents, name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeImage": "tf.raw_ops.DecodeImage(\n    contents,\n    channels=0,\n    dtype=tf.dtypes.uint8,\n    expand_animations=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeJSONExample": "tf.raw_ops.DecodeJSONExample(\n    json_examples, name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeJpeg": "tf.raw_ops.DecodeJpeg(\n    contents,\n    channels=0,\n    ratio=1,\n    fancy_upscaling=True,\n    try_recover_truncated=False,\n    acceptable_fraction=1,\n    dct_method='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodePaddedRaw": "tf.raw_ops.DecodePaddedRaw(\n    input_bytes, fixed_length, out_type, little_endian=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodePng": "tf.raw_ops.DecodePng(\n    contents,\n    channels=0,\n    dtype=tf.dtypes.uint8,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeProtoV2": "tf.raw_ops.DecodeProtoV2(\n    bytes,\n    message_type,\n    field_names,\n    output_types,\n    descriptor_source='local://',\n    message_format='binary',\n    sanitize=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeRaw": "tf.raw_ops.DecodeRaw(\n    bytes, out_type, little_endian=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeWav": "tf.raw_ops.DecodeWav(\n    contents, desired_channels=-1, desired_samples=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.DeepCopy": "tf.raw_ops.DeepCopy(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.DeleteIterator": "tf.raw_ops.DeleteIterator(\n    handle, deleter, name=None\n)\n"
  },
  {
    "tf.raw_ops.DeleteMemoryCache": "tf.raw_ops.DeleteMemoryCache(\n    handle, deleter, name=None\n)\n"
  },
  {
    "tf.raw_ops.DeleteMultiDeviceIterator": "tf.raw_ops.DeleteMultiDeviceIterator(\n    multi_device_iterator, iterators, deleter, name=None\n)\n"
  },
  {
    "tf.raw_ops.DeleteRandomSeedGenerator": "tf.raw_ops.DeleteRandomSeedGenerator(\n    handle, deleter, name=None\n)\n"
  },
  {
    "tf.raw_ops.DeleteSeedGenerator": "tf.raw_ops.DeleteSeedGenerator(\n    handle, deleter, name=None\n)\n"
  },
  {
    "tf.raw_ops.DeleteSessionTensor": "tf.raw_ops.DeleteSessionTensor(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.DenseBincount": "tf.raw_ops.DenseBincount(\n    input, size, weights, binary_output=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.DenseCountSparseOutput": "tf.raw_ops.DenseCountSparseOutput(\n    values, weights, binary_output, minlength=-1, maxlength=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.DenseToCSRSparseMatrix": "tf.raw_ops.DenseToCSRSparseMatrix(\n    dense_input, indices, name=None\n)\n"
  },
  {
    "tf.raw_ops.DenseToDenseSetOperation": "tf.raw_ops.DenseToDenseSetOperation(\n    set1, set2, set_operation, validate_indices=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.DenseToSparseBatchDataset": "tf.raw_ops.DenseToSparseBatchDataset(\n    input_dataset,\n    batch_size,\n    row_shape,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DenseToSparseSetOperation": "tf.raw_ops.DenseToSparseSetOperation(\n    set1,\n    set2_indices,\n    set2_values,\n    set2_shape,\n    set_operation,\n    validate_indices=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DepthToSpace": "tf.raw_ops.DepthToSpace(\n    input, block_size, data_format='NHWC', name=None\n)\n"
  },
  {
    "tf.raw_ops.DepthwiseConv2dNative": "tf.raw_ops.DepthwiseConv2dNative(\n    input,\n    filter,\n    strides,\n    padding,\n    explicit_paddings=[],\n    data_format='NHWC',\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DepthwiseConv2dNativeBackpropFilter": "tf.raw_ops.DepthwiseConv2dNativeBackpropFilter(\n    input,\n    filter_sizes,\n    out_backprop,\n    strides,\n    padding,\n    explicit_paddings=[],\n    data_format='NHWC',\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DepthwiseConv2dNativeBackpropInput": "tf.raw_ops.DepthwiseConv2dNativeBackpropInput(\n    input_sizes,\n    filter,\n    out_backprop,\n    strides,\n    padding,\n    explicit_paddings=[],\n    data_format='NHWC',\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Dequantize": "tf.raw_ops.Dequantize(\n    input,\n    min_range,\n    max_range,\n    mode='MIN_COMBINED',\n    narrow_range=False,\n    axis=-1,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DeserializeIterator": "tf.raw_ops.DeserializeIterator(\n    resource_handle, serialized, name=None\n)\n"
  },
  {
    "tf.raw_ops.DeserializeManySparse": "tf.raw_ops.DeserializeManySparse(\n    serialized_sparse, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.DeserializeSparse": "tf.raw_ops.DeserializeSparse(\n    serialized_sparse, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.DestroyResourceOp": "tf.raw_ops.DestroyResourceOp(\n    resource, ignore_lookup_error=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.DestroyTemporaryVariable": "tf.raw_ops.DestroyTemporaryVariable(\n    ref, var_name, name=None\n)\n"
  },
  {
    "tf.raw_ops.DeviceIndex": "tf.raw_ops.DeviceIndex(\n    device_names, name=None\n)\n"
  },
  {
    "tf.raw_ops.Diag": "tf.raw_ops.Diag(\n    diagonal, name=None\n)\n"
  },
  {
    "tf.raw_ops.DiagPart": "tf.raw_ops.DiagPart(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.Digamma": "tf.raw_ops.Digamma(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Dilation2D": "tf.raw_ops.Dilation2D(\n    input, filter, strides, rates, padding, name=None\n)\n"
  },
  {
    "tf.raw_ops.Dilation2DBackpropFilter": "tf.raw_ops.Dilation2DBackpropFilter(\n    input, filter, out_backprop, strides, rates, padding, name=None\n)\n"
  },
  {
    "tf.raw_ops.Dilation2DBackpropInput": "tf.raw_ops.Dilation2DBackpropInput(\n    input, filter, out_backprop, strides, rates, padding, name=None\n)\n"
  },
  {
    "tf.raw_ops.DirectedInterleaveDataset": "tf.raw_ops.DirectedInterleaveDataset(\n    selector_input_dataset,\n    data_input_datasets,\n    output_types,\n    output_shapes,\n    stop_on_empty_dataset=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DisableCopyOnRead": "tf.raw_ops.DisableCopyOnRead(\n    resource, name=None\n)\n"
  },
  {
    "tf.raw_ops.Div": "tf.raw_ops.Div(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.DivNoNan": "tf.raw_ops.DivNoNan(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.DrawBoundingBoxes": "tf.raw_ops.DrawBoundingBoxes(\n    images, boxes, name=None\n)\n"
  },
  {
    "tf.raw_ops.DrawBoundingBoxesV2": "tf.raw_ops.DrawBoundingBoxesV2(\n    images, boxes, colors, name=None\n)\n"
  },
  {
    "tf.raw_ops.DummyIterationCounter": "tf.raw_ops.DummyIterationCounter(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DummyMemoryCache": "tf.raw_ops.DummyMemoryCache(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DummySeedGenerator": "tf.raw_ops.DummySeedGenerator(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DynamicEnqueueTPUEmbeddingArbitraryTensorBatch": "tf.raw_ops.DynamicEnqueueTPUEmbeddingArbitraryTensorBatch(\n    sample_indices_or_row_splits,\n    embedding_indices,\n    aggregation_weights,\n    mode_override,\n    device_ordinal,\n    combiners=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DynamicPartition": "tf.raw_ops.DynamicPartition(\n    data, partitions, num_partitions, name=None\n)\n"
  },
  {
    "tf.raw_ops.DynamicStitch": "tf.raw_ops.DynamicStitch(\n    indices, data, name=None\n)\n"
  },
  {
    "tf.raw_ops.EagerPyFunc": "tf.raw_ops.EagerPyFunc(\n    input, token, Tout, is_async=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.EditDistance": "tf.raw_ops.EditDistance(\n    hypothesis_indices,\n    hypothesis_values,\n    hypothesis_shape,\n    truth_indices,\n    truth_values,\n    truth_shape,\n    normalize=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Eig": "tf.raw_ops.Eig(\n    input, Tout, compute_v=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.Einsum": "tf.raw_ops.Einsum(\n    inputs, equation, name=None\n)\n"
  },
  {
    "tf.raw_ops.Elu": "tf.raw_ops.Elu(\n    features, name=None\n)\n"
  },
  {
    "tf.raw_ops.EluGrad": "tf.raw_ops.EluGrad(\n    gradients, outputs, name=None\n)\n"
  },
  {
    "tf.raw_ops.Empty": "tf.raw_ops.Empty(\n    shape, dtype, init=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.EmptyTensorList": "tf.raw_ops.EmptyTensorList(\n    element_shape, max_num_elements, element_dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.EncodeBase64": "tf.raw_ops.EncodeBase64(\n    input, pad=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.EncodeJpeg": "tf.raw_ops.EncodeJpeg(\n    image,\n    format='',\n    quality=95,\n    progressive=False,\n    optimize_size=False,\n    chroma_downsampling=True,\n    density_unit='in',\n    x_density=300,\n    y_density=300,\n    xmp_metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.EncodeJpegVariableQuality": "tf.raw_ops.EncodeJpegVariableQuality(\n    images, quality, name=None\n)\n"
  },
  {
    "tf.raw_ops.EncodePng": "tf.raw_ops.EncodePng(\n    image, compression=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.EncodeProto": "tf.raw_ops.EncodeProto(\n    sizes,\n    values,\n    field_names,\n    message_type,\n    descriptor_source='local://',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.EncodeWav": "tf.raw_ops.EncodeWav(\n    audio, sample_rate, name=None\n)\n"
  },
  {
    "tf.raw_ops.EnqueueTPUEmbeddingArbitraryTensorBatch": "tf.raw_ops.EnqueueTPUEmbeddingArbitraryTensorBatch(\n    sample_indices_or_row_splits,\n    embedding_indices,\n    aggregation_weights,\n    mode_override,\n    device_ordinal=-1,\n    combiners=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.EnqueueTPUEmbeddingIntegerBatch": "tf.raw_ops.EnqueueTPUEmbeddingIntegerBatch(\n    batch, mode_override, device_ordinal=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.EnqueueTPUEmbeddingRaggedTensorBatch": "tf.raw_ops.EnqueueTPUEmbeddingRaggedTensorBatch(\n    sample_splits,\n    embedding_indices,\n    aggregation_weights,\n    mode_override,\n    table_ids,\n    device_ordinal=-1,\n    combiners=[],\n    max_sequence_lengths=[],\n    num_features=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.EnqueueTPUEmbeddingSparseBatch": "tf.raw_ops.EnqueueTPUEmbeddingSparseBatch(\n    sample_indices,\n    embedding_indices,\n    aggregation_weights,\n    mode_override,\n    device_ordinal=-1,\n    combiners=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.EnqueueTPUEmbeddingSparseTensorBatch": "tf.raw_ops.EnqueueTPUEmbeddingSparseTensorBatch(\n    sample_indices,\n    embedding_indices,\n    aggregation_weights,\n    mode_override,\n    table_ids,\n    device_ordinal=-1,\n    combiners=[],\n    max_sequence_lengths=[],\n    num_features=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.EnsureShape": "tf.raw_ops.EnsureShape(\n    input, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.Enter": "tf.raw_ops.Enter(\n    data, frame_name, is_constant=False, parallel_iterations=10, name=None\n)\n"
  },
  {
    "tf.raw_ops.Equal": "tf.raw_ops.Equal(\n    x, y, incompatible_shape_error=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.Erf": "tf.raw_ops.Erf(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Erfc": "tf.raw_ops.Erfc(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Erfinv": "tf.raw_ops.Erfinv(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.EuclideanNorm": "tf.raw_ops.EuclideanNorm(\n    input, axis, keep_dims=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.Exit": "tf.raw_ops.Exit(\n    data, name=None\n)\n"
  },
  {
    "tf.raw_ops.Exp": "tf.raw_ops.Exp(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExpandDims": "tf.raw_ops.ExpandDims(\n    input, axis, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalAssertNextDataset": "tf.raw_ops.ExperimentalAssertNextDataset(\n    input_dataset, transformations, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalAutoShardDataset": "tf.raw_ops.ExperimentalAutoShardDataset(\n    input_dataset,\n    num_workers,\n    index,\n    output_types,\n    output_shapes,\n    auto_shard_policy=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalBytesProducedStatsDataset": "tf.raw_ops.ExperimentalBytesProducedStatsDataset(\n    input_dataset, tag, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalCSVDataset": "tf.raw_ops.ExperimentalCSVDataset(\n    filenames,\n    compression_type,\n    buffer_size,\n    header,\n    field_delim,\n    use_quote_delim,\n    na_value,\n    select_cols,\n    record_defaults,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalChooseFastestDataset": "tf.raw_ops.ExperimentalChooseFastestDataset(\n    input_datasets, num_experiments, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalDatasetCardinality": "tf.raw_ops.ExperimentalDatasetCardinality(\n    input_dataset, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalDatasetToTFRecord": "tf.raw_ops.ExperimentalDatasetToTFRecord(\n    input_dataset, filename, compression_type, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalDenseToSparseBatchDataset": "tf.raw_ops.ExperimentalDenseToSparseBatchDataset(\n    input_dataset,\n    batch_size,\n    row_shape,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalDirectedInterleaveDataset": "tf.raw_ops.ExperimentalDirectedInterleaveDataset(\n    selector_input_dataset,\n    data_input_datasets,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalGroupByReducerDataset": "tf.raw_ops.ExperimentalGroupByReducerDataset(\n    input_dataset,\n    key_func_other_arguments,\n    init_func_other_arguments,\n    reduce_func_other_arguments,\n    finalize_func_other_arguments,\n    key_func,\n    init_func,\n    reduce_func,\n    finalize_func,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalGroupByWindowDataset": "tf.raw_ops.ExperimentalGroupByWindowDataset(\n    input_dataset,\n    key_func_other_arguments,\n    reduce_func_other_arguments,\n    window_size_func_other_arguments,\n    key_func,\n    reduce_func,\n    window_size_func,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalIgnoreErrorsDataset": "tf.raw_ops.ExperimentalIgnoreErrorsDataset(\n    input_dataset, output_types, output_shapes, log_warning=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalIteratorGetDevice": "tf.raw_ops.ExperimentalIteratorGetDevice(\n    resource, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalLMDBDataset": "tf.raw_ops.ExperimentalLMDBDataset(\n    filenames, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalLatencyStatsDataset": "tf.raw_ops.ExperimentalLatencyStatsDataset(\n    input_dataset, tag, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalMapAndBatchDataset": "tf.raw_ops.ExperimentalMapAndBatchDataset(\n    input_dataset,\n    other_arguments,\n    batch_size,\n    num_parallel_calls,\n    drop_remainder,\n    f,\n    output_types,\n    output_shapes,\n    preserve_cardinality=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalMapDataset": "tf.raw_ops.ExperimentalMapDataset(\n    input_dataset,\n    other_arguments,\n    f,\n    output_types,\n    output_shapes,\n    use_inter_op_parallelism=True,\n    preserve_cardinality=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalMatchingFilesDataset": "tf.raw_ops.ExperimentalMatchingFilesDataset(\n    patterns, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalMaxIntraOpParallelismDataset": "tf.raw_ops.ExperimentalMaxIntraOpParallelismDataset(\n    input_dataset,\n    max_intra_op_parallelism,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalNonSerializableDataset": "tf.raw_ops.ExperimentalNonSerializableDataset(\n    input_dataset, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalParallelInterleaveDataset": "tf.raw_ops.ExperimentalParallelInterleaveDataset(\n    input_dataset,\n    other_arguments,\n    cycle_length,\n    block_length,\n    sloppy,\n    buffer_output_elements,\n    prefetch_input_elements,\n    f,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalParseExampleDataset": "tf.raw_ops.ExperimentalParseExampleDataset(\n    input_dataset,\n    num_parallel_calls,\n    dense_defaults,\n    sparse_keys,\n    dense_keys,\n    sparse_types,\n    dense_shapes,\n    output_types,\n    output_shapes,\n    sloppy=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalPrivateThreadPoolDataset": "tf.raw_ops.ExperimentalPrivateThreadPoolDataset(\n    input_dataset, num_threads, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalRandomDataset": "tf.raw_ops.ExperimentalRandomDataset(\n    seed, seed2, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalRebatchDataset": "tf.raw_ops.ExperimentalRebatchDataset(\n    input_dataset,\n    num_replicas,\n    output_types,\n    output_shapes,\n    use_fallback=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalScanDataset": "tf.raw_ops.ExperimentalScanDataset(\n    input_dataset,\n    initial_state,\n    other_arguments,\n    f,\n    output_types,\n    output_shapes,\n    preserve_cardinality=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalSetStatsAggregatorDataset": "tf.raw_ops.ExperimentalSetStatsAggregatorDataset(\n    input_dataset,\n    stats_aggregator,\n    tag,\n    counter_prefix,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalSleepDataset": "tf.raw_ops.ExperimentalSleepDataset(\n    input_dataset, sleep_microseconds, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalSlidingWindowDataset": "tf.raw_ops.ExperimentalSlidingWindowDataset(\n    input_dataset,\n    window_size,\n    window_shift,\n    window_stride,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalSqlDataset": "tf.raw_ops.ExperimentalSqlDataset(\n    driver_name,\n    data_source_name,\n    query,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalStatsAggregatorHandle": "tf.raw_ops.ExperimentalStatsAggregatorHandle(\n    container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalStatsAggregatorSummary": "tf.raw_ops.ExperimentalStatsAggregatorSummary(\n    iterator, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalTakeWhileDataset": "tf.raw_ops.ExperimentalTakeWhileDataset(\n    input_dataset,\n    other_arguments,\n    predicate,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalThreadPoolDataset": "tf.raw_ops.ExperimentalThreadPoolDataset(\n    input_dataset, thread_pool, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalThreadPoolHandle": "tf.raw_ops.ExperimentalThreadPoolHandle(\n    num_threads,\n    display_name,\n    max_intra_op_parallelism=1,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalUnbatchDataset": "tf.raw_ops.ExperimentalUnbatchDataset(\n    input_dataset, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalUniqueDataset": "tf.raw_ops.ExperimentalUniqueDataset(\n    input_dataset, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.Expint": "tf.raw_ops.Expint(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Expm1": "tf.raw_ops.Expm1(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExtractGlimpse": "tf.raw_ops.ExtractGlimpse(\n    input,\n    size,\n    offsets,\n    centered=True,\n    normalized=True,\n    uniform_noise=True,\n    noise='uniform',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExtractGlimpseV2": "tf.raw_ops.ExtractGlimpseV2(\n    input,\n    size,\n    offsets,\n    centered=True,\n    normalized=True,\n    uniform_noise=True,\n    noise='uniform',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExtractImagePatches": "tf.raw_ops.ExtractImagePatches(\n    images, ksizes, strides, rates, padding, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExtractJpegShape": "tf.raw_ops.ExtractJpegShape(\n    contents,\n    output_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExtractVolumePatches": "tf.raw_ops.ExtractVolumePatches(\n    input, ksizes, strides, padding, name=None\n)\n"
  },
  {
    "tf.raw_ops.FFT": "tf.raw_ops.FFT(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.FFT2D": "tf.raw_ops.FFT2D(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.FFT3D": "tf.raw_ops.FFT3D(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.FIFOQueue": "tf.raw_ops.FIFOQueue(\n    component_types,\n    shapes=[],\n    capacity=-1,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FIFOQueueV2": "tf.raw_ops.FIFOQueueV2(\n    component_types,\n    shapes=[],\n    capacity=-1,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Fact": "tf.raw_ops.Fact(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FakeParam": "tf.raw_ops.FakeParam(\n    dtype, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.FakeQuantWithMinMaxArgs": "tf.raw_ops.FakeQuantWithMinMaxArgs(\n    inputs, min=-6, max=6, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.FakeQuantWithMinMaxArgsGradient": "tf.raw_ops.FakeQuantWithMinMaxArgsGradient(\n    gradients, inputs, min=-6, max=6, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.FakeQuantWithMinMaxVars": "tf.raw_ops.FakeQuantWithMinMaxVars(\n    inputs, min, max, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.FakeQuantWithMinMaxVarsGradient": "tf.raw_ops.FakeQuantWithMinMaxVarsGradient(\n    gradients, inputs, min, max, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel": "tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel(\n    inputs, min, max, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.FakeQuantWithMinMaxVarsPerChannelGradient": "tf.raw_ops.FakeQuantWithMinMaxVarsPerChannelGradient(\n    gradients, inputs, min, max, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.FakeQueue": "tf.raw_ops.FakeQueue(\n    resource, name=None\n)\n"
  },
  {
    "tf.raw_ops.Fill": "tf.raw_ops.Fill(\n    dims, value, name=None\n)\n"
  },
  {
    "tf.raw_ops.FilterByLastComponentDataset": "tf.raw_ops.FilterByLastComponentDataset(\n    input_dataset, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.FilterDataset": "tf.raw_ops.FilterDataset(\n    input_dataset,\n    other_arguments,\n    predicate,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FinalizeDataset": "tf.raw_ops.FinalizeDataset(\n    input_dataset,\n    output_types,\n    output_shapes,\n    has_captured_ref=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Fingerprint": "tf.raw_ops.Fingerprint(\n    data, method, name=None\n)\n"
  },
  {
    "tf.raw_ops.FixedLengthRecordDataset": "tf.raw_ops.FixedLengthRecordDataset(\n    filenames,\n    header_bytes,\n    record_bytes,\n    footer_bytes,\n    buffer_size,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FixedLengthRecordDatasetV2": "tf.raw_ops.FixedLengthRecordDatasetV2(\n    filenames,\n    header_bytes,\n    record_bytes,\n    footer_bytes,\n    buffer_size,\n    compression_type,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FixedLengthRecordReader": "tf.raw_ops.FixedLengthRecordReader(\n    record_bytes,\n    header_bytes=0,\n    footer_bytes=0,\n    hop_bytes=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FixedLengthRecordReaderV2": "tf.raw_ops.FixedLengthRecordReaderV2(\n    record_bytes,\n    header_bytes=0,\n    footer_bytes=0,\n    hop_bytes=0,\n    container='',\n    shared_name='',\n    encoding='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FixedUnigramCandidateSampler": "tf.raw_ops.FixedUnigramCandidateSampler(\n    true_classes,\n    num_true,\n    num_sampled,\n    unique,\n    range_max,\n    vocab_file='',\n    distortion=1,\n    num_reserved_ids=0,\n    num_shards=1,\n    shard=0,\n    unigrams=[],\n    seed=0,\n    seed2=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FlatMapDataset": "tf.raw_ops.FlatMapDataset(\n    input_dataset,\n    other_arguments,\n    f,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Floor": "tf.raw_ops.Floor(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.FloorDiv": "tf.raw_ops.FloorDiv(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.FloorMod": "tf.raw_ops.FloorMod(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.FlushSummaryWriter": "tf.raw_ops.FlushSummaryWriter(\n    writer, name=None\n)\n"
  },
  {
    "tf.raw_ops.For": "tf.raw_ops.For(\n    start, limit, delta, input, body, name=None\n)\n"
  },
  {
    "tf.raw_ops.FractionalAvgPool": "tf.raw_ops.FractionalAvgPool(\n    value,\n    pooling_ratio,\n    pseudo_random=False,\n    overlapping=False,\n    deterministic=False,\n    seed=0,\n    seed2=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FractionalAvgPoolGrad": "tf.raw_ops.FractionalAvgPoolGrad(\n    orig_input_tensor_shape,\n    out_backprop,\n    row_pooling_sequence,\n    col_pooling_sequence,\n    overlapping=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FractionalMaxPool": "tf.raw_ops.FractionalMaxPool(\n    value,\n    pooling_ratio,\n    pseudo_random=False,\n    overlapping=False,\n    deterministic=False,\n    seed=0,\n    seed2=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FractionalMaxPoolGrad": "tf.raw_ops.FractionalMaxPoolGrad(\n    orig_input,\n    orig_output,\n    out_backprop,\n    row_pooling_sequence,\n    col_pooling_sequence,\n    overlapping=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FresnelCos": "tf.raw_ops.FresnelCos(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.FresnelSin": "tf.raw_ops.FresnelSin(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.FusedBatchNorm": "tf.raw_ops.FusedBatchNorm(\n    x,\n    scale,\n    offset,\n    mean,\n    variance,\n    epsilon=0.0001,\n    exponential_avg_factor=1,\n    data_format='NHWC',\n    is_training=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FusedBatchNormGrad": "tf.raw_ops.FusedBatchNormGrad(\n    y_backprop,\n    x,\n    scale,\n    reserve_space_1,\n    reserve_space_2,\n    epsilon=0.0001,\n    data_format='NHWC',\n    is_training=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FusedBatchNormGradV2": "tf.raw_ops.FusedBatchNormGradV2(\n    y_backprop,\n    x,\n    scale,\n    reserve_space_1,\n    reserve_space_2,\n    epsilon=0.0001,\n    data_format='NHWC',\n    is_training=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FusedBatchNormGradV3": "tf.raw_ops.FusedBatchNormGradV3(\n    y_backprop,\n    x,\n    scale,\n    reserve_space_1,\n    reserve_space_2,\n    reserve_space_3,\n    epsilon=0.0001,\n    data_format='NHWC',\n    is_training=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FusedBatchNormV2": "tf.raw_ops.FusedBatchNormV2(\n    x,\n    scale,\n    offset,\n    mean,\n    variance,\n    epsilon=0.0001,\n    exponential_avg_factor=1,\n    data_format='NHWC',\n    is_training=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FusedBatchNormV3": "tf.raw_ops.FusedBatchNormV3(\n    x,\n    scale,\n    offset,\n    mean,\n    variance,\n    epsilon=0.0001,\n    exponential_avg_factor=1,\n    data_format='NHWC',\n    is_training=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FusedPadConv2D": "tf.raw_ops.FusedPadConv2D(\n    input, paddings, filter, mode, strides, padding, name=None\n)\n"
  },
  {
    "tf.raw_ops.FusedResizeAndPadConv2D": "tf.raw_ops.FusedResizeAndPadConv2D(\n    input,\n    size,\n    paddings,\n    filter,\n    mode,\n    strides,\n    padding,\n    resize_align_corners=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.GRUBlockCell": "tf.raw_ops.GRUBlockCell(\n    x, h_prev, w_ru, w_c, b_ru, b_c, name=None\n)\n"
  },
  {
    "tf.raw_ops.GRUBlockCellGrad": "tf.raw_ops.GRUBlockCellGrad(\n    x, h_prev, w_ru, w_c, b_ru, b_c, r, u, c, d_h, name=None\n)\n"
  },
  {
    "tf.raw_ops.Gather": "tf.raw_ops.Gather(\n    params, indices, validate_indices=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.GatherNd": "tf.raw_ops.GatherNd(\n    params, indices, name=None\n)\n"
  },
  {
    "tf.raw_ops.GatherV2": "tf.raw_ops.GatherV2(\n    params, indices, axis, batch_dims=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.GenerateBoundingBoxProposals": "tf.raw_ops.GenerateBoundingBoxProposals(\n    scores,\n    bbox_deltas,\n    image_info,\n    anchors,\n    nms_threshold,\n    pre_nms_topn,\n    min_size,\n    post_nms_topn=300,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.GenerateVocabRemapping": "tf.raw_ops.GenerateVocabRemapping(\n    new_vocab_file,\n    old_vocab_file,\n    new_vocab_offset,\n    num_new_vocab,\n    old_vocab_size=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.GeneratorDataset": "tf.raw_ops.GeneratorDataset(\n    init_func_other_args,\n    next_func_other_args,\n    finalize_func_other_args,\n    init_func,\n    next_func,\n    finalize_func,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.GetElementAtIndex": "tf.raw_ops.GetElementAtIndex(\n    dataset, index, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.GetOptions": "tf.raw_ops.GetOptions(\n    input_dataset, name=None\n)\n"
  },
  {
    "tf.raw_ops.GetSessionHandle": "tf.raw_ops.GetSessionHandle(\n    value, name=None\n)\n"
  },
  {
    "tf.raw_ops.GetSessionHandleV2": "tf.raw_ops.GetSessionHandleV2(\n    value, name=None\n)\n"
  },
  {
    "tf.raw_ops.GetSessionTensor": "tf.raw_ops.GetSessionTensor(\n    handle, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.Greater": "tf.raw_ops.Greater(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.GreaterEqual": "tf.raw_ops.GreaterEqual(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.GroupByReducerDataset": "tf.raw_ops.GroupByReducerDataset(\n    input_dataset,\n    key_func_other_arguments,\n    init_func_other_arguments,\n    reduce_func_other_arguments,\n    finalize_func_other_arguments,\n    key_func,\n    init_func,\n    reduce_func,\n    finalize_func,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.GroupByWindowDataset": "tf.raw_ops.GroupByWindowDataset(\n    input_dataset,\n    key_func_other_arguments,\n    reduce_func_other_arguments,\n    window_size_func_other_arguments,\n    key_func,\n    reduce_func,\n    window_size_func,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.GuaranteeConst": "tf.raw_ops.GuaranteeConst(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.HSVToRGB": "tf.raw_ops.HSVToRGB(\n    images, name=None\n)\n"
  },
  {
    "tf.raw_ops.HashTable": "tf.raw_ops.HashTable(\n    key_dtype,\n    value_dtype,\n    container='',\n    shared_name='',\n    use_node_name_sharing=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.HashTableV2": "tf.raw_ops.HashTableV2(\n    key_dtype,\n    value_dtype,\n    container='',\n    shared_name='',\n    use_node_name_sharing=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.HistogramFixedWidth": "tf.raw_ops.HistogramFixedWidth(\n    values,\n    value_range,\n    nbins,\n    dtype=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.HistogramSummary": "tf.raw_ops.HistogramSummary(\n    tag, values, name=None\n)\n"
  },
  {
    "tf.raw_ops.IFFT": "tf.raw_ops.IFFT(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.IFFT2D": "tf.raw_ops.IFFT2D(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.IFFT3D": "tf.raw_ops.IFFT3D(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.IRFFT": "tf.raw_ops.IRFFT(\n    input,\n    fft_length,\n    Treal=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.IRFFT2D": "tf.raw_ops.IRFFT2D(\n    input,\n    fft_length,\n    Treal=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.IRFFT3D": "tf.raw_ops.IRFFT3D(\n    input,\n    fft_length,\n    Treal=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Identity": "tf.raw_ops.Identity(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.IdentityN": "tf.raw_ops.IdentityN(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.IdentityReader": "tf.raw_ops.IdentityReader(\n    container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.IdentityReaderV2": "tf.raw_ops.IdentityReaderV2(\n    container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.If": "tf.raw_ops.If(\n    cond, input, Tout, then_branch, else_branch, output_shapes=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.Igamma": "tf.raw_ops.Igamma(\n    a, x, name=None\n)\n"
  },
  {
    "tf.raw_ops.IgammaGradA": "tf.raw_ops.IgammaGradA(\n    a, x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Igammac": "tf.raw_ops.Igammac(\n    a, x, name=None\n)\n"
  },
  {
    "tf.raw_ops.IgnoreErrorsDataset": "tf.raw_ops.IgnoreErrorsDataset(\n    input_dataset, output_types, output_shapes, log_warning=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.Imag": "tf.raw_ops.Imag(\n    input,\n    Tout=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ImageProjectiveTransformV2": "tf.raw_ops.ImageProjectiveTransformV2(\n    images,\n    transforms,\n    output_shape,\n    interpolation,\n    fill_mode='CONSTANT',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ImageProjectiveTransformV3": "tf.raw_ops.ImageProjectiveTransformV3(\n    images,\n    transforms,\n    output_shape,\n    fill_value,\n    interpolation,\n    fill_mode='CONSTANT',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ImageSummary": "tf.raw_ops.ImageSummary(\n    tag,\n    tensor,\n    max_images=3,\n    bad_color=_execute.make_tensor(\\n    'dtype: DT_UINT8 tensor_shape { dim { size: 4 } } int_val: 255 int_val: 0 int_val: 0 int_val: 255 '\\n    , 'bad_color'),\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ImmutableConst": "tf.raw_ops.ImmutableConst(\n    dtype, shape, memory_region_name, name=None\n)\n"
  },
  {
    "tf.raw_ops.ImportEvent": "tf.raw_ops.ImportEvent(\n    writer, event, name=None\n)\n"
  },
  {
    "tf.raw_ops.InTopK": "tf.raw_ops.InTopK(\n    predictions, targets, k, name=None\n)\n"
  },
  {
    "tf.raw_ops.InTopKV2": "tf.raw_ops.InTopKV2(\n    predictions, targets, k, name=None\n)\n"
  },
  {
    "tf.raw_ops.InfeedDequeue": "tf.raw_ops.InfeedDequeue(\n    dtype, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.InfeedDequeueTuple": "tf.raw_ops.InfeedDequeueTuple(\n    dtypes, shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.InfeedEnqueue": "tf.raw_ops.InfeedEnqueue(\n    input, shape=[], layout=[], device_ordinal=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.InfeedEnqueuePrelinearizedBuffer": "tf.raw_ops.InfeedEnqueuePrelinearizedBuffer(\n    input, device_ordinal=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.InfeedEnqueueTuple": "tf.raw_ops.InfeedEnqueueTuple(\n    inputs, shapes, layouts=[], device_ordinal=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.InitializeTable": "tf.raw_ops.InitializeTable(\n    table_handle, keys, values, name=None\n)\n"
  },
  {
    "tf.raw_ops.InitializeTableFromDataset": "tf.raw_ops.InitializeTableFromDataset(\n    table_handle, dataset, name=None\n)\n"
  },
  {
    "tf.raw_ops.InitializeTableFromTextFile": "tf.raw_ops.InitializeTableFromTextFile(\n    table_handle,\n    filename,\n    key_index,\n    value_index,\n    vocab_size=-1,\n    delimiter='\\t',\n    offset=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.InitializeTableFromTextFileV2": "tf.raw_ops.InitializeTableFromTextFileV2(\n    table_handle,\n    filename,\n    key_index,\n    value_index,\n    vocab_size=-1,\n    delimiter='\\t',\n    offset=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.InitializeTableV2": "tf.raw_ops.InitializeTableV2(\n    table_handle, keys, values, name=None\n)\n"
  },
  {
    "tf.raw_ops.InplaceAdd": "tf.raw_ops.InplaceAdd(\n    x, i, v, name=None\n)\n"
  },
  {
    "tf.raw_ops.InplaceSub": "tf.raw_ops.InplaceSub(\n    x, i, v, name=None\n)\n"
  },
  {
    "tf.raw_ops.InplaceUpdate": "tf.raw_ops.InplaceUpdate(\n    x, i, v, name=None\n)\n"
  },
  {
    "tf.raw_ops.InterleaveDataset": "tf.raw_ops.InterleaveDataset(\n    input_dataset,\n    other_arguments,\n    cycle_length,\n    block_length,\n    f,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Inv": "tf.raw_ops.Inv(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.InvGrad": "tf.raw_ops.InvGrad(\n    y, dy, name=None\n)\n"
  },
  {
    "tf.raw_ops.Invert": "tf.raw_ops.Invert(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.InvertPermutation": "tf.raw_ops.InvertPermutation(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.IsBoostedTreesEnsembleInitialized": "tf.raw_ops.IsBoostedTreesEnsembleInitialized(\n    tree_ensemble_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.IsBoostedTreesQuantileStreamResourceInitialized": "tf.raw_ops.IsBoostedTreesQuantileStreamResourceInitialized(\n    quantile_stream_resource_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.IsFinite": "tf.raw_ops.IsFinite(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.IsInf": "tf.raw_ops.IsInf(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.IsNan": "tf.raw_ops.IsNan(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.IsTPUEmbeddingInitialized": "tf.raw_ops.IsTPUEmbeddingInitialized(\n    config='', name=None\n)\n"
  },
  {
    "tf.raw_ops.IsVariableInitialized": "tf.raw_ops.IsVariableInitialized(\n    ref, name=None\n)\n"
  },
  {
    "tf.raw_ops.IsotonicRegression": "tf.raw_ops.IsotonicRegression(\n    input,\n    output_dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Iterator": "tf.raw_ops.Iterator(\n    shared_name, container, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.IteratorFromStringHandle": "tf.raw_ops.IteratorFromStringHandle(\n    string_handle, output_types=[], output_shapes=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.IteratorFromStringHandleV2": "tf.raw_ops.IteratorFromStringHandleV2(\n    string_handle, output_types=[], output_shapes=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.IteratorGetDevice": "tf.raw_ops.IteratorGetDevice(\n    resource, name=None\n)\n"
  },
  {
    "tf.raw_ops.IteratorGetNext": "tf.raw_ops.IteratorGetNext(\n    iterator, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.IteratorGetNextAsOptional": "tf.raw_ops.IteratorGetNextAsOptional(\n    iterator, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.IteratorGetNextSync": "tf.raw_ops.IteratorGetNextSync(\n    iterator, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.IteratorToStringHandle": "tf.raw_ops.IteratorToStringHandle(\n    resource_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.IteratorV2": "tf.raw_ops.IteratorV2(\n    shared_name, container, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.L2Loss": "tf.raw_ops.L2Loss(\n    t, name=None\n)\n"
  },
  {
    "tf.raw_ops.LMDBDataset": "tf.raw_ops.LMDBDataset(\n    filenames, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.LMDBReader": "tf.raw_ops.LMDBReader(\n    container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.LRN": "tf.raw_ops.LRN(\n    input, depth_radius=5, bias=1, alpha=1, beta=0.5, name=None\n)\n"
  },
  {
    "tf.raw_ops.LRNGrad": "tf.raw_ops.LRNGrad(\n    input_grads,\n    input_image,\n    output_image,\n    depth_radius=5,\n    bias=1,\n    alpha=1,\n    beta=0.5,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LSTMBlockCell": "tf.raw_ops.LSTMBlockCell(\n    x,\n    cs_prev,\n    h_prev,\n    w,\n    wci,\n    wcf,\n    wco,\n    b,\n    forget_bias=1,\n    cell_clip=3,\n    use_peephole=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LSTMBlockCellGrad": "tf.raw_ops.LSTMBlockCellGrad(\n    x,\n    cs_prev,\n    h_prev,\n    w,\n    wci,\n    wcf,\n    wco,\n    b,\n    i,\n    cs,\n    f,\n    o,\n    ci,\n    co,\n    cs_grad,\n    h_grad,\n    use_peephole,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LatencyStatsDataset": "tf.raw_ops.LatencyStatsDataset(\n    input_dataset, tag, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.LeakyRelu": "tf.raw_ops.LeakyRelu(\n    features, alpha=0.2, name=None\n)\n"
  },
  {
    "tf.raw_ops.LeakyReluGrad": "tf.raw_ops.LeakyReluGrad(\n    gradients, features, alpha=0.2, name=None\n)\n"
  },
  {
    "tf.raw_ops.LearnedUnigramCandidateSampler": "tf.raw_ops.LearnedUnigramCandidateSampler(\n    true_classes,\n    num_true,\n    num_sampled,\n    unique,\n    range_max,\n    seed=0,\n    seed2=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LeftShift": "tf.raw_ops.LeftShift(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.LegacyParallelInterleaveDatasetV2": "tf.raw_ops.LegacyParallelInterleaveDatasetV2(\n    input_dataset,\n    other_arguments,\n    cycle_length,\n    block_length,\n    buffer_output_elements,\n    prefetch_input_elements,\n    f,\n    output_types,\n    output_shapes,\n    deterministic='default',\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Less": "tf.raw_ops.Less(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.LessEqual": "tf.raw_ops.LessEqual(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.Lgamma": "tf.raw_ops.Lgamma(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.LinSpace": "tf.raw_ops.LinSpace(\n    start, stop, num, name=None\n)\n"
  },
  {
    "tf.raw_ops.ListDataset": "tf.raw_ops.ListDataset(\n    tensors, output_types, output_shapes, metadata='', name=None\n)\n"
  },
  {
    "tf.raw_ops.ListDiff": "tf.raw_ops.ListDiff(\n    x,\n    y,\n    out_idx=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadAndRemapMatrix": "tf.raw_ops.LoadAndRemapMatrix(\n    ckpt_path,\n    old_tensor_name,\n    row_remapping,\n    col_remapping,\n    initializing_values,\n    num_rows,\n    num_cols,\n    max_rows_in_memory=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadDataset": "tf.raw_ops.LoadDataset(\n    path,\n    reader_func_other_args,\n    output_types,\n    output_shapes,\n    reader_func,\n    compression='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingADAMParameters": "tf.raw_ops.LoadTPUEmbeddingADAMParameters(\n    parameters,\n    momenta,\n    velocities,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingAdadeltaParameters": "tf.raw_ops.LoadTPUEmbeddingAdadeltaParameters(\n    parameters,\n    accumulators,\n    updates,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingAdagradMomentumParameters": "tf.raw_ops.LoadTPUEmbeddingAdagradMomentumParameters(\n    parameters,\n    accumulators,\n    momenta,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingAdagradParameters": "tf.raw_ops.LoadTPUEmbeddingAdagradParameters(\n    parameters,\n    accumulators,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingCenteredRMSPropParameters": "tf.raw_ops.LoadTPUEmbeddingCenteredRMSPropParameters(\n    parameters,\n    ms,\n    mom,\n    mg,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingFTRLParameters": "tf.raw_ops.LoadTPUEmbeddingFTRLParameters(\n    parameters,\n    accumulators,\n    linears,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingFrequencyEstimatorParameters": "tf.raw_ops.LoadTPUEmbeddingFrequencyEstimatorParameters(\n    parameters,\n    last_hit_step,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingMDLAdagradLightParameters": "tf.raw_ops.LoadTPUEmbeddingMDLAdagradLightParameters(\n    parameters,\n    accumulators,\n    weights,\n    benefits,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingMomentumParameters": "tf.raw_ops.LoadTPUEmbeddingMomentumParameters(\n    parameters,\n    momenta,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingProximalAdagradParameters": "tf.raw_ops.LoadTPUEmbeddingProximalAdagradParameters(\n    parameters,\n    accumulators,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingProximalYogiParameters": "tf.raw_ops.LoadTPUEmbeddingProximalYogiParameters(\n    parameters,\n    v,\n    m,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingRMSPropParameters": "tf.raw_ops.LoadTPUEmbeddingRMSPropParameters(\n    parameters,\n    ms,\n    mom,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingStochasticGradientDescentParameters": "tf.raw_ops.LoadTPUEmbeddingStochasticGradientDescentParameters(\n    parameters,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Log": "tf.raw_ops.Log(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Log1p": "tf.raw_ops.Log1p(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.LogMatrixDeterminant": "tf.raw_ops.LogMatrixDeterminant(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.LogSoftmax": "tf.raw_ops.LogSoftmax(\n    logits, name=None\n)\n"
  },
  {
    "tf.raw_ops.LogUniformCandidateSampler": "tf.raw_ops.LogUniformCandidateSampler(\n    true_classes,\n    num_true,\n    num_sampled,\n    unique,\n    range_max,\n    seed=0,\n    seed2=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LogicalAnd": "tf.raw_ops.LogicalAnd(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.LogicalNot": "tf.raw_ops.LogicalNot(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.LogicalOr": "tf.raw_ops.LogicalOr(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.LookupTableExport": "tf.raw_ops.LookupTableExport(\n    table_handle, Tkeys, Tvalues, name=None\n)\n"
  },
  {
    "tf.raw_ops.LookupTableExportV2": "tf.raw_ops.LookupTableExportV2(\n    table_handle, Tkeys, Tvalues, name=None\n)\n"
  },
  {
    "tf.raw_ops.LookupTableFind": "tf.raw_ops.LookupTableFind(\n    table_handle, keys, default_value, name=None\n)\n"
  },
  {
    "tf.raw_ops.LookupTableFindV2": "tf.raw_ops.LookupTableFindV2(\n    table_handle, keys, default_value, name=None\n)\n"
  },
  {
    "tf.raw_ops.LookupTableImport": "tf.raw_ops.LookupTableImport(\n    table_handle, keys, values, name=None\n)\n"
  },
  {
    "tf.raw_ops.LookupTableImportV2": "tf.raw_ops.LookupTableImportV2(\n    table_handle, keys, values, name=None\n)\n"
  },
  {
    "tf.raw_ops.LookupTableInsert": "tf.raw_ops.LookupTableInsert(\n    table_handle, keys, values, name=None\n)\n"
  },
  {
    "tf.raw_ops.LookupTableInsertV2": "tf.raw_ops.LookupTableInsertV2(\n    table_handle, keys, values, name=None\n)\n"
  },
  {
    "tf.raw_ops.LookupTableRemoveV2": "tf.raw_ops.LookupTableRemoveV2(\n    table_handle, keys, name=None\n)\n"
  },
  {
    "tf.raw_ops.LookupTableSize": "tf.raw_ops.LookupTableSize(\n    table_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.LookupTableSizeV2": "tf.raw_ops.LookupTableSizeV2(\n    table_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.LoopCond": "tf.raw_ops.LoopCond(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.LowerBound": "tf.raw_ops.LowerBound(\n    sorted_inputs,\n    values,\n    out_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Lu": "tf.raw_ops.Lu(\n    input,\n    output_idx_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MakeIterator": "tf.raw_ops.MakeIterator(\n    dataset, iterator, name=None\n)\n"
  },
  {
    "tf.raw_ops.MapAndBatchDataset": "tf.raw_ops.MapAndBatchDataset(\n    input_dataset,\n    other_arguments,\n    batch_size,\n    num_parallel_calls,\n    drop_remainder,\n    f,\n    output_types,\n    output_shapes,\n    preserve_cardinality=False,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MapClear": "tf.raw_ops.MapClear(\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MapDataset": "tf.raw_ops.MapDataset(\n    input_dataset,\n    other_arguments,\n    f,\n    output_types,\n    output_shapes,\n    use_inter_op_parallelism=True,\n    preserve_cardinality=False,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MapDefun": "tf.raw_ops.MapDefun(\n    arguments,\n    captured_inputs,\n    output_types,\n    output_shapes,\n    f,\n    max_intra_op_parallelism=1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MapIncompleteSize": "tf.raw_ops.MapIncompleteSize(\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MapPeek": "tf.raw_ops.MapPeek(\n    key,\n    indices,\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MapSize": "tf.raw_ops.MapSize(\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MapStage": "tf.raw_ops.MapStage(\n    key,\n    indices,\n    values,\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MapUnstage": "tf.raw_ops.MapUnstage(\n    key,\n    indices,\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MapUnstageNoKey": "tf.raw_ops.MapUnstageNoKey(\n    indices,\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MatMul": "tf.raw_ops.MatMul(\n    a, b, transpose_a=False, transpose_b=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatchingFiles": "tf.raw_ops.MatchingFiles(\n    pattern, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatchingFilesDataset": "tf.raw_ops.MatchingFilesDataset(\n    patterns, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixBandPart": "tf.raw_ops.MatrixBandPart(\n    input, num_lower, num_upper, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixDeterminant": "tf.raw_ops.MatrixDeterminant(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixDiag": "tf.raw_ops.MatrixDiag(\n    diagonal, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixDiagPart": "tf.raw_ops.MatrixDiagPart(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixDiagPartV2": "tf.raw_ops.MatrixDiagPartV2(\n    input, k, padding_value, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixDiagPartV3": "tf.raw_ops.MatrixDiagPartV3(\n    input, k, padding_value, align='RIGHT_LEFT', name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixDiagV2": "tf.raw_ops.MatrixDiagV2(\n    diagonal, k, num_rows, num_cols, padding_value, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixDiagV3": "tf.raw_ops.MatrixDiagV3(\n    diagonal,\n    k,\n    num_rows,\n    num_cols,\n    padding_value,\n    align='RIGHT_LEFT',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixExponential": "tf.raw_ops.MatrixExponential(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixInverse": "tf.raw_ops.MatrixInverse(\n    input, adjoint=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixLogarithm": "tf.raw_ops.MatrixLogarithm(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixSetDiag": "tf.raw_ops.MatrixSetDiag(\n    input, diagonal, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixSetDiagV2": "tf.raw_ops.MatrixSetDiagV2(\n    input, diagonal, k, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixSetDiagV3": "tf.raw_ops.MatrixSetDiagV3(\n    input, diagonal, k, align='RIGHT_LEFT', name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixSolve": "tf.raw_ops.MatrixSolve(\n    matrix, rhs, adjoint=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixSolveLs": "tf.raw_ops.MatrixSolveLs(\n    matrix, rhs, l2_regularizer, fast=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixSquareRoot": "tf.raw_ops.MatrixSquareRoot(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixTriangularSolve": "tf.raw_ops.MatrixTriangularSolve(\n    matrix, rhs, lower=True, adjoint=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.Max": "tf.raw_ops.Max(\n    input, axis, keep_dims=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxIntraOpParallelismDataset": "tf.raw_ops.MaxIntraOpParallelismDataset(\n    input_dataset,\n    max_intra_op_parallelism,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPool": "tf.raw_ops.MaxPool(\n    input,\n    ksize,\n    strides,\n    padding,\n    explicit_paddings=[],\n    data_format='NHWC',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPool3D": "tf.raw_ops.MaxPool3D(\n    input, ksize, strides, padding, data_format='NDHWC', name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPool3DGrad": "tf.raw_ops.MaxPool3DGrad(\n    orig_input,\n    orig_output,\n    grad,\n    ksize,\n    strides,\n    padding,\n    data_format='NDHWC',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPool3DGradGrad": "tf.raw_ops.MaxPool3DGradGrad(\n    orig_input,\n    orig_output,\n    grad,\n    ksize,\n    strides,\n    padding,\n    data_format='NDHWC',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPoolGrad": "tf.raw_ops.MaxPoolGrad(\n    orig_input,\n    orig_output,\n    grad,\n    ksize,\n    strides,\n    padding,\n    explicit_paddings=[],\n    data_format='NHWC',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPoolGradGrad": "tf.raw_ops.MaxPoolGradGrad(\n    orig_input,\n    orig_output,\n    grad,\n    ksize,\n    strides,\n    padding,\n    data_format='NHWC',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPoolGradGradV2": "tf.raw_ops.MaxPoolGradGradV2(\n    orig_input,\n    orig_output,\n    grad,\n    ksize,\n    strides,\n    padding,\n    data_format='NHWC',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPoolGradGradWithArgmax": "tf.raw_ops.MaxPoolGradGradWithArgmax(\n    input,\n    grad,\n    argmax,\n    ksize,\n    strides,\n    padding,\n    include_batch_in_index=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPoolGradV2": "tf.raw_ops.MaxPoolGradV2(\n    orig_input,\n    orig_output,\n    grad,\n    ksize,\n    strides,\n    padding,\n    data_format='NHWC',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPoolGradWithArgmax": "tf.raw_ops.MaxPoolGradWithArgmax(\n    input,\n    grad,\n    argmax,\n    ksize,\n    strides,\n    padding,\n    include_batch_in_index=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPoolV2": "tf.raw_ops.MaxPoolV2(\n    input, ksize, strides, padding, data_format='NHWC', name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPoolWithArgmax": "tf.raw_ops.MaxPoolWithArgmax(\n    input,\n    ksize,\n    strides,\n    padding,\n    Targmax=tf.dtypes.int64,\n    include_batch_in_index=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Maximum": "tf.raw_ops.Maximum(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.Mean": "tf.raw_ops.Mean(\n    input, axis, keep_dims=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.Merge": "tf.raw_ops.Merge(\n    inputs, name=None\n)\n"
  },
  {
    "tf.raw_ops.MergeSummary": "tf.raw_ops.MergeSummary(\n    inputs, name=None\n)\n"
  },
  {
    "tf.raw_ops.MergeV2Checkpoints": "tf.raw_ops.MergeV2Checkpoints(\n    checkpoint_prefixes,\n    destination_prefix,\n    delete_old_dirs=True,\n    allow_missing_files=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Mfcc": "tf.raw_ops.Mfcc(\n    spectrogram,\n    sample_rate,\n    upper_frequency_limit=4000,\n    lower_frequency_limit=20,\n    filterbank_channel_count=40,\n    dct_coefficient_count=13,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Min": "tf.raw_ops.Min(\n    input, axis, keep_dims=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.Minimum": "tf.raw_ops.Minimum(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.MirrorPad": "tf.raw_ops.MirrorPad(\n    input, paddings, mode, name=None\n)\n"
  },
  {
    "tf.raw_ops.MirrorPadGrad": "tf.raw_ops.MirrorPadGrad(\n    input, paddings, mode, name=None\n)\n"
  },
  {
    "tf.raw_ops.Mod": "tf.raw_ops.Mod(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.ModelDataset": "tf.raw_ops.ModelDataset(\n    input_dataset,\n    output_types,\n    output_shapes,\n    algorithm=0,\n    cpu_budget=0,\n    ram_budget=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Mul": "tf.raw_ops.Mul(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.MulNoNan": "tf.raw_ops.MulNoNan(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.MultiDeviceIterator": "tf.raw_ops.MultiDeviceIterator(\n    devices, shared_name, container, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.MultiDeviceIteratorFromStringHandle": "tf.raw_ops.MultiDeviceIteratorFromStringHandle(\n    string_handle, output_types=[], output_shapes=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.MultiDeviceIteratorGetNextFromShard": "tf.raw_ops.MultiDeviceIteratorGetNextFromShard(\n    multi_device_iterator,\n    shard_num,\n    incarnation_id,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MultiDeviceIteratorInit": "tf.raw_ops.MultiDeviceIteratorInit(\n    dataset, multi_device_iterator, max_buffer_size, name=None\n)\n"
  },
  {
    "tf.raw_ops.MultiDeviceIteratorToStringHandle": "tf.raw_ops.MultiDeviceIteratorToStringHandle(\n    multi_device_iterator, name=None\n)\n"
  },
  {
    "tf.raw_ops.Multinomial": "tf.raw_ops.Multinomial(\n    logits,\n    num_samples,\n    seed=0,\n    seed2=0,\n    output_dtype=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MutableDenseHashTable": "tf.raw_ops.MutableDenseHashTable(\n    empty_key,\n    value_dtype,\n    container='',\n    shared_name='',\n    use_node_name_sharing=False,\n    value_shape=[],\n    initial_num_buckets=131072,\n    max_load_factor=0.8,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MutableDenseHashTableV2": "tf.raw_ops.MutableDenseHashTableV2(\n    empty_key,\n    deleted_key,\n    value_dtype,\n    container='',\n    shared_name='',\n    use_node_name_sharing=False,\n    value_shape=[],\n    initial_num_buckets=131072,\n    max_load_factor=0.8,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MutableHashTable": "tf.raw_ops.MutableHashTable(\n    key_dtype,\n    value_dtype,\n    container='',\n    shared_name='',\n    use_node_name_sharing=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MutableHashTableOfTensors": "tf.raw_ops.MutableHashTableOfTensors(\n    key_dtype,\n    value_dtype,\n    container='',\n    shared_name='',\n    use_node_name_sharing=False,\n    value_shape=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MutableHashTableOfTensorsV2": "tf.raw_ops.MutableHashTableOfTensorsV2(\n    key_dtype,\n    value_dtype,\n    container='',\n    shared_name='',\n    use_node_name_sharing=False,\n    value_shape=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MutableHashTableV2": "tf.raw_ops.MutableHashTableV2(\n    key_dtype,\n    value_dtype,\n    container='',\n    shared_name='',\n    use_node_name_sharing=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MutexLock": "tf.raw_ops.MutexLock(\n    mutex, name=None\n)\n"
  },
  {
    "tf.raw_ops.MutexV2": "tf.raw_ops.MutexV2(\n    container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.NcclAllReduce": "tf.raw_ops.NcclAllReduce(\n    input, reduction, num_devices, shared_name, name=None\n)\n"
  },
  {
    "tf.raw_ops.NcclBroadcast": "tf.raw_ops.NcclBroadcast(\n    input, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.NcclReduce": "tf.raw_ops.NcclReduce(\n    input, reduction, name=None\n)\n"
  },
  {
    "tf.raw_ops.Ndtri": "tf.raw_ops.Ndtri(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Neg": "tf.raw_ops.Neg(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.NextAfter": "tf.raw_ops.NextAfter(\n    x1, x2, name=None\n)\n"
  },
  {
    "tf.raw_ops.NextIteration": "tf.raw_ops.NextIteration(\n    data, name=None\n)\n"
  },
  {
    "tf.raw_ops.NoOp": "tf.raw_ops.NoOp(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.NonDeterministicInts": "tf.raw_ops.NonDeterministicInts(\n    shape,\n    dtype=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.NonMaxSuppression": "tf.raw_ops.NonMaxSuppression(\n    boxes, scores, max_output_size, iou_threshold=0.5, name=None\n)\n"
  },
  {
    "tf.raw_ops.NonMaxSuppressionV2": "tf.raw_ops.NonMaxSuppressionV2(\n    boxes, scores, max_output_size, iou_threshold, name=None\n)\n"
  },
  {
    "tf.raw_ops.NonMaxSuppressionV3": "tf.raw_ops.NonMaxSuppressionV3(\n    boxes, scores, max_output_size, iou_threshold, score_threshold, name=None\n)\n"
  },
  {
    "tf.raw_ops.NonMaxSuppressionV4": "tf.raw_ops.NonMaxSuppressionV4(\n    boxes,\n    scores,\n    max_output_size,\n    iou_threshold,\n    score_threshold,\n    pad_to_max_output_size=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.NonMaxSuppressionV5": "tf.raw_ops.NonMaxSuppressionV5(\n    boxes,\n    scores,\n    max_output_size,\n    iou_threshold,\n    score_threshold,\n    soft_nms_sigma,\n    pad_to_max_output_size=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.NonMaxSuppressionWithOverlaps": "tf.raw_ops.NonMaxSuppressionWithOverlaps(\n    overlaps,\n    scores,\n    max_output_size,\n    overlap_threshold,\n    score_threshold,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.NonSerializableDataset": "tf.raw_ops.NonSerializableDataset(\n    input_dataset, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.NotEqual": "tf.raw_ops.NotEqual(\n    x, y, incompatible_shape_error=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.NthElement": "tf.raw_ops.NthElement(\n    input, n, reverse=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.OneHot": "tf.raw_ops.OneHot(\n    indices, depth, on_value, off_value, axis=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.OneShotIterator": "tf.raw_ops.OneShotIterator(\n    dataset_factory,\n    output_types,\n    output_shapes,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OnesLike": "tf.raw_ops.OnesLike(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.OptimizeDataset": "tf.raw_ops.OptimizeDataset(\n    input_dataset,\n    optimizations,\n    output_types,\n    output_shapes,\n    optimization_configs=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OptimizeDatasetV2": "tf.raw_ops.OptimizeDatasetV2(\n    input_dataset,\n    optimizations_enabled,\n    optimizations_disabled,\n    optimizations_default,\n    output_types,\n    output_shapes,\n    optimization_configs=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OptionalFromValue": "tf.raw_ops.OptionalFromValue(\n    components, name=None\n)\n"
  },
  {
    "tf.raw_ops.OptionalGetValue": "tf.raw_ops.OptionalGetValue(\n    optional, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.OptionalHasValue": "tf.raw_ops.OptionalHasValue(\n    optional, name=None\n)\n"
  },
  {
    "tf.raw_ops.OptionalNone": "tf.raw_ops.OptionalNone(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OptionsDataset": "tf.raw_ops.OptionsDataset(\n    input_dataset,\n    serialized_options,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OrderedMapClear": "tf.raw_ops.OrderedMapClear(\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OrderedMapIncompleteSize": "tf.raw_ops.OrderedMapIncompleteSize(\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OrderedMapPeek": "tf.raw_ops.OrderedMapPeek(\n    key,\n    indices,\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OrderedMapSize": "tf.raw_ops.OrderedMapSize(\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OrderedMapStage": "tf.raw_ops.OrderedMapStage(\n    key,\n    indices,\n    values,\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OrderedMapUnstage": "tf.raw_ops.OrderedMapUnstage(\n    key,\n    indices,\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OrderedMapUnstageNoKey": "tf.raw_ops.OrderedMapUnstageNoKey(\n    indices,\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OutfeedDequeue": "tf.raw_ops.OutfeedDequeue(\n    dtype, shape, device_ordinal=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.OutfeedDequeueTuple": "tf.raw_ops.OutfeedDequeueTuple(\n    dtypes, shapes, device_ordinal=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.OutfeedDequeueTupleV2": "tf.raw_ops.OutfeedDequeueTupleV2(\n    device_ordinal, dtypes, shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.OutfeedDequeueV2": "tf.raw_ops.OutfeedDequeueV2(\n    device_ordinal, dtype, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.OutfeedEnqueue": "tf.raw_ops.OutfeedEnqueue(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.OutfeedEnqueueTuple": "tf.raw_ops.OutfeedEnqueueTuple(\n    inputs, name=None\n)\n"
  },
  {
    "tf.raw_ops.Pack": "tf.raw_ops.Pack(\n    values, axis=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.Pad": "tf.raw_ops.Pad(\n    input, paddings, name=None\n)\n"
  },
  {
    "tf.raw_ops.PadV2": "tf.raw_ops.PadV2(\n    input, paddings, constant_values, name=None\n)\n"
  },
  {
    "tf.raw_ops.PaddedBatchDataset": "tf.raw_ops.PaddedBatchDataset(\n    input_dataset,\n    batch_size,\n    padded_shapes,\n    padding_values,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.PaddedBatchDatasetV2": "tf.raw_ops.PaddedBatchDatasetV2(\n    input_dataset,\n    batch_size,\n    padded_shapes,\n    padding_values,\n    drop_remainder,\n    output_shapes,\n    parallel_copy=False,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.PaddingFIFOQueue": "tf.raw_ops.PaddingFIFOQueue(\n    component_types,\n    shapes=[],\n    capacity=-1,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.PaddingFIFOQueueV2": "tf.raw_ops.PaddingFIFOQueueV2(\n    component_types,\n    shapes=[],\n    capacity=-1,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParallelBatchDataset": "tf.raw_ops.ParallelBatchDataset(\n    input_dataset,\n    batch_size,\n    num_parallel_calls,\n    drop_remainder,\n    output_types,\n    output_shapes,\n    parallel_copy=False,\n    deterministic='default',\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParallelConcat": "tf.raw_ops.ParallelConcat(\n    values, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.ParallelDynamicStitch": "tf.raw_ops.ParallelDynamicStitch(\n    indices, data, name=None\n)\n"
  },
  {
    "tf.raw_ops.ParallelFilterDataset": "tf.raw_ops.ParallelFilterDataset(\n    input_dataset,\n    other_arguments,\n    num_parallel_calls,\n    predicate,\n    output_types,\n    output_shapes,\n    deterministic='default',\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParallelInterleaveDataset": "tf.raw_ops.ParallelInterleaveDataset(\n    input_dataset,\n    other_arguments,\n    cycle_length,\n    block_length,\n    sloppy,\n    buffer_output_elements,\n    prefetch_input_elements,\n    f,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParallelInterleaveDatasetV2": "tf.raw_ops.ParallelInterleaveDatasetV2(\n    input_dataset,\n    other_arguments,\n    cycle_length,\n    block_length,\n    num_parallel_calls,\n    f,\n    output_types,\n    output_shapes,\n    sloppy=False,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParallelInterleaveDatasetV3": "tf.raw_ops.ParallelInterleaveDatasetV3(\n    input_dataset,\n    other_arguments,\n    cycle_length,\n    block_length,\n    num_parallel_calls,\n    f,\n    output_types,\n    output_shapes,\n    deterministic='default',\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParallelInterleaveDatasetV4": "tf.raw_ops.ParallelInterleaveDatasetV4(\n    input_dataset,\n    other_arguments,\n    cycle_length,\n    block_length,\n    buffer_output_elements,\n    prefetch_input_elements,\n    num_parallel_calls,\n    f,\n    output_types,\n    output_shapes,\n    deterministic='default',\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParallelMapDataset": "tf.raw_ops.ParallelMapDataset(\n    input_dataset,\n    other_arguments,\n    num_parallel_calls,\n    f,\n    output_types,\n    output_shapes,\n    use_inter_op_parallelism=True,\n    sloppy=False,\n    preserve_cardinality=False,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParallelMapDatasetV2": "tf.raw_ops.ParallelMapDatasetV2(\n    input_dataset,\n    other_arguments,\n    num_parallel_calls,\n    f,\n    output_types,\n    output_shapes,\n    use_inter_op_parallelism=True,\n    deterministic='default',\n    preserve_cardinality=False,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParameterizedTruncatedNormal": "tf.raw_ops.ParameterizedTruncatedNormal(\n    shape, means, stdevs, minvals, maxvals, seed=0, seed2=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.ParseExample": "tf.raw_ops.ParseExample(\n    serialized,\n    names,\n    sparse_keys,\n    dense_keys,\n    dense_defaults,\n    sparse_types,\n    dense_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParseExampleDataset": "tf.raw_ops.ParseExampleDataset(\n    input_dataset,\n    num_parallel_calls,\n    dense_defaults,\n    sparse_keys,\n    dense_keys,\n    sparse_types,\n    dense_shapes,\n    output_types,\n    output_shapes,\n    sloppy=False,\n    ragged_keys=[],\n    ragged_value_types=[],\n    ragged_split_types=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParseExampleDatasetV2": "tf.raw_ops.ParseExampleDatasetV2(\n    input_dataset,\n    num_parallel_calls,\n    dense_defaults,\n    sparse_keys,\n    dense_keys,\n    sparse_types,\n    dense_shapes,\n    output_types,\n    output_shapes,\n    deterministic='default',\n    ragged_keys=[],\n    ragged_value_types=[],\n    ragged_split_types=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParseExampleV2": "tf.raw_ops.ParseExampleV2(\n    serialized,\n    names,\n    sparse_keys,\n    dense_keys,\n    ragged_keys,\n    dense_defaults,\n    num_sparse,\n    sparse_types,\n    ragged_value_types,\n    ragged_split_types,\n    dense_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParseSequenceExample": "tf.raw_ops.ParseSequenceExample(\n    serialized,\n    debug_name,\n    context_dense_defaults,\n    feature_list_dense_missing_assumed_empty,\n    context_sparse_keys,\n    context_dense_keys,\n    feature_list_sparse_keys,\n    feature_list_dense_keys,\n    Ncontext_sparse=0,\n    Ncontext_dense=0,\n    Nfeature_list_sparse=0,\n    Nfeature_list_dense=0,\n    context_sparse_types=[],\n    feature_list_dense_types=[],\n    context_dense_shapes=[],\n    feature_list_sparse_types=[],\n    feature_list_dense_shapes=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParseSequenceExampleV2": "tf.raw_ops.ParseSequenceExampleV2(\n    serialized,\n    debug_name,\n    context_sparse_keys,\n    context_dense_keys,\n    context_ragged_keys,\n    feature_list_sparse_keys,\n    feature_list_dense_keys,\n    feature_list_ragged_keys,\n    feature_list_dense_missing_assumed_empty,\n    context_dense_defaults,\n    Ncontext_sparse=0,\n    context_sparse_types=[],\n    context_ragged_value_types=[],\n    context_ragged_split_types=[],\n    context_dense_shapes=[],\n    Nfeature_list_sparse=0,\n    Nfeature_list_dense=0,\n    feature_list_dense_types=[],\n    feature_list_sparse_types=[],\n    feature_list_ragged_value_types=[],\n    feature_list_ragged_split_types=[],\n    feature_list_dense_shapes=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParseSingleExample": "tf.raw_ops.ParseSingleExample(\n    serialized,\n    dense_defaults,\n    num_sparse,\n    sparse_keys,\n    dense_keys,\n    sparse_types,\n    dense_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParseSingleSequenceExample": "tf.raw_ops.ParseSingleSequenceExample(\n    serialized,\n    feature_list_dense_missing_assumed_empty,\n    context_sparse_keys,\n    context_dense_keys,\n    feature_list_sparse_keys,\n    feature_list_dense_keys,\n    context_dense_defaults,\n    debug_name,\n    context_sparse_types=[],\n    feature_list_dense_types=[],\n    context_dense_shapes=[],\n    feature_list_sparse_types=[],\n    feature_list_dense_shapes=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParseTensor": "tf.raw_ops.ParseTensor(\n    serialized, out_type, name=None\n)\n"
  },
  {
    "tf.raw_ops.PartitionedCall": "tf.raw_ops.PartitionedCall(\n    args,\n    Tout,\n    f,\n    config='',\n    config_proto='',\n    executor_type='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Placeholder": "tf.raw_ops.Placeholder(\n    dtype, shape=None, name=None\n)\n"
  },
  {
    "tf.raw_ops.PlaceholderV2": "tf.raw_ops.PlaceholderV2(\n    dtype, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.PlaceholderWithDefault": "tf.raw_ops.PlaceholderWithDefault(\n    input, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.Polygamma": "tf.raw_ops.Polygamma(\n    a, x, name=None\n)\n"
  },
  {
    "tf.raw_ops.PopulationCount": "tf.raw_ops.PopulationCount(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Pow": "tf.raw_ops.Pow(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.PrefetchDataset": "tf.raw_ops.PrefetchDataset(\n    input_dataset,\n    buffer_size,\n    output_types,\n    output_shapes,\n    slack_period=0,\n    legacy_autotune=True,\n    buffer_size_min=0,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Prelinearize": "tf.raw_ops.Prelinearize(\n    input, shape=[], layout=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.PrelinearizeTuple": "tf.raw_ops.PrelinearizeTuple(\n    inputs, shapes, layouts=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.PreventGradient": "tf.raw_ops.PreventGradient(\n    input, message='', name=None\n)\n"
  },
  {
    "tf.raw_ops.Print": "tf.raw_ops.Print(\n    input, data, message='', first_n=-1, summarize=3, name=None\n)\n"
  },
  {
    "tf.raw_ops.PrintV2": "tf.raw_ops.PrintV2(\n    input, output_stream='stderr', end='\\n', name=None\n)\n"
  },
  {
    "tf.raw_ops.PriorityQueue": "tf.raw_ops.PriorityQueue(\n    shapes,\n    component_types=[],\n    capacity=-1,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.PriorityQueueV2": "tf.raw_ops.PriorityQueueV2(\n    shapes,\n    component_types=[],\n    capacity=-1,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.PrivateThreadPoolDataset": "tf.raw_ops.PrivateThreadPoolDataset(\n    input_dataset, num_threads, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.Prod": "tf.raw_ops.Prod(\n    input, axis, keep_dims=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.PyFunc": "tf.raw_ops.PyFunc(\n    input, token, Tout, name=None\n)\n"
  },
  {
    "tf.raw_ops.PyFuncStateless": "tf.raw_ops.PyFuncStateless(\n    input, token, Tout, name=None\n)\n"
  },
  {
    "tf.raw_ops.Qr": "tf.raw_ops.Qr(\n    input, full_matrices=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizeAndDequantize": "tf.raw_ops.QuantizeAndDequantize(\n    input,\n    signed_input=True,\n    num_bits=8,\n    range_given=False,\n    input_min=0,\n    input_max=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizeAndDequantizeV2": "tf.raw_ops.QuantizeAndDequantizeV2(\n    input,\n    input_min,\n    input_max,\n    signed_input=True,\n    num_bits=8,\n    range_given=False,\n    round_mode='HALF_TO_EVEN',\n    narrow_range=False,\n    axis=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizeAndDequantizeV3": "tf.raw_ops.QuantizeAndDequantizeV3(\n    input,\n    input_min,\n    input_max,\n    num_bits,\n    signed_input=True,\n    range_given=True,\n    narrow_range=False,\n    axis=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizeAndDequantizeV4": "tf.raw_ops.QuantizeAndDequantizeV4(\n    input,\n    input_min,\n    input_max,\n    signed_input=True,\n    num_bits=8,\n    range_given=False,\n    round_mode='HALF_TO_EVEN',\n    narrow_range=False,\n    axis=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizeAndDequantizeV4Grad": "tf.raw_ops.QuantizeAndDequantizeV4Grad(\n    gradients, input, input_min, input_max, axis=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizeDownAndShrinkRange": "tf.raw_ops.QuantizeDownAndShrinkRange(\n    input, input_min, input_max, out_type, name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizeV2": "tf.raw_ops.QuantizeV2(\n    input,\n    min_range,\n    max_range,\n    T,\n    mode='MIN_COMBINED',\n    round_mode='HALF_AWAY_FROM_ZERO',\n    narrow_range=False,\n    axis=-1,\n    ensure_minimum_range=0.01,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedAdd": "tf.raw_ops.QuantizedAdd(\n    x,\n    y,\n    min_x,\n    max_x,\n    min_y,\n    max_y,\n    Toutput=tf.dtypes.qint32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedAvgPool": "tf.raw_ops.QuantizedAvgPool(\n    input, min_input, max_input, ksize, strides, padding, name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedBatchNormWithGlobalNormalization": "tf.raw_ops.QuantizedBatchNormWithGlobalNormalization(\n    t,\n    t_min,\n    t_max,\n    m,\n    m_min,\n    m_max,\n    v,\n    v_min,\n    v_max,\n    beta,\n    beta_min,\n    beta_max,\n    gamma,\n    gamma_min,\n    gamma_max,\n    out_type,\n    variance_epsilon,\n    scale_after_normalization,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedBiasAdd": "tf.raw_ops.QuantizedBiasAdd(\n    input, bias, min_input, max_input, min_bias, max_bias, out_type, name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConcat": "tf.raw_ops.QuantizedConcat(\n    concat_dim, values, input_mins, input_maxes, name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2D": "tf.raw_ops.QuantizedConv2D(\n    input,\n    filter,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    strides,\n    padding,\n    out_type=tf.dtypes.qint32,\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2DAndRelu": "tf.raw_ops.QuantizedConv2DAndRelu(\n    input,\n    filter,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    strides,\n    padding,\n    out_type=tf.dtypes.qint32,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2DAndReluAndRequantize": "tf.raw_ops.QuantizedConv2DAndReluAndRequantize(\n    input,\n    filter,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    min_freezed_output,\n    max_freezed_output,\n    strides,\n    padding,\n    out_type=tf.dtypes.quint8,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2DAndRequantize": "tf.raw_ops.QuantizedConv2DAndRequantize(\n    input,\n    filter,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    min_freezed_output,\n    max_freezed_output,\n    strides,\n    padding,\n    out_type=tf.dtypes.qint8,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2DPerChannel": "tf.raw_ops.QuantizedConv2DPerChannel(\n    input,\n    filter,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    strides,\n    padding,\n    out_type=tf.dtypes.qint32,\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2DWithBias": "tf.raw_ops.QuantizedConv2DWithBias(\n    input,\n    filter,\n    bias,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    strides,\n    padding,\n    out_type=tf.dtypes.qint32,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2DWithBiasAndRelu": "tf.raw_ops.QuantizedConv2DWithBiasAndRelu(\n    input,\n    filter,\n    bias,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    strides,\n    padding,\n    out_type=tf.dtypes.qint32,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2DWithBiasAndReluAndRequantize": "tf.raw_ops.QuantizedConv2DWithBiasAndReluAndRequantize(\n    input,\n    filter,\n    bias,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    min_freezed_output,\n    max_freezed_output,\n    strides,\n    padding,\n    out_type=tf.dtypes.quint8,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2DWithBiasAndRequantize": "tf.raw_ops.QuantizedConv2DWithBiasAndRequantize(\n    input,\n    filter,\n    bias,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    min_freezed_output,\n    max_freezed_output,\n    strides,\n    padding,\n    out_type=tf.dtypes.qint8,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2DWithBiasSignedSumAndReluAndRequantize": "tf.raw_ops.QuantizedConv2DWithBiasSignedSumAndReluAndRequantize(\n    input,\n    filter,\n    bias,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    min_freezed_output,\n    max_freezed_output,\n    summand,\n    min_summand,\n    max_summand,\n    strides,\n    padding,\n    out_type=tf.dtypes.quint8,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2DWithBiasSumAndRelu": "tf.raw_ops.QuantizedConv2DWithBiasSumAndRelu(\n    input,\n    filter,\n    bias,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    summand,\n    strides,\n    padding,\n    out_type=tf.dtypes.qint32,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2DWithBiasSumAndReluAndRequantize": "tf.raw_ops.QuantizedConv2DWithBiasSumAndReluAndRequantize(\n    input,\n    filter,\n    bias,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    min_freezed_output,\n    max_freezed_output,\n    summand,\n    min_summand,\n    max_summand,\n    strides,\n    padding,\n    out_type=tf.dtypes.quint8,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedDepthwiseConv2D": "tf.raw_ops.QuantizedDepthwiseConv2D(\n    input,\n    filter,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    strides,\n    padding,\n    out_type=tf.dtypes.qint32,\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedDepthwiseConv2DWithBias": "tf.raw_ops.QuantizedDepthwiseConv2DWithBias(\n    input,\n    filter,\n    bias,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    strides,\n    padding,\n    out_type=tf.dtypes.qint32,\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedDepthwiseConv2DWithBiasAndRelu": "tf.raw_ops.QuantizedDepthwiseConv2DWithBiasAndRelu(\n    input,\n    filter,\n    bias,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    strides,\n    padding,\n    out_type=tf.dtypes.qint32,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedDepthwiseConv2DWithBiasAndReluAndRequantize": "tf.raw_ops.QuantizedDepthwiseConv2DWithBiasAndReluAndRequantize(\n    input,\n    filter,\n    bias,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    min_freezed_output,\n    max_freezed_output,\n    strides,\n    padding,\n    out_type=tf.dtypes.quint8,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedInstanceNorm": "tf.raw_ops.QuantizedInstanceNorm(\n    x,\n    x_min,\n    x_max,\n    output_range_given=False,\n    given_y_min=0,\n    given_y_max=0,\n    variance_epsilon=1e-05,\n    min_separation=0.001,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedMatMul": "tf.raw_ops.QuantizedMatMul(\n    a,\n    b,\n    min_a,\n    max_a,\n    min_b,\n    max_b,\n    Toutput=tf.dtypes.qint32,\n    transpose_a=False,\n    transpose_b=False,\n    Tactivation=tf.dtypes.quint8,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedMatMulWithBias": "tf.raw_ops.QuantizedMatMulWithBias(\n    a,\n    b,\n    bias,\n    min_a,\n    max_a,\n    min_b,\n    max_b,\n    Toutput=tf.dtypes.qint32,\n    transpose_a=False,\n    transpose_b=False,\n    input_quant_mode='MIN_FIRST',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedMatMulWithBiasAndDequantize": "tf.raw_ops.QuantizedMatMulWithBiasAndDequantize(\n    a,\n    b,\n    bias,\n    min_a,\n    max_a,\n    min_b,\n    max_b,\n    min_freezed_output,\n    max_freezed_output,\n    Toutput,\n    transpose_a=False,\n    transpose_b=False,\n    input_quant_mode='MIN_FIRST',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedMatMulWithBiasAndRelu": "tf.raw_ops.QuantizedMatMulWithBiasAndRelu(\n    a,\n    b,\n    bias,\n    min_a,\n    max_a,\n    min_b,\n    max_b,\n    Toutput=tf.dtypes.qint32,\n    transpose_a=False,\n    transpose_b=False,\n    input_quant_mode='MIN_FIRST',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedMatMulWithBiasAndReluAndRequantize": "tf.raw_ops.QuantizedMatMulWithBiasAndReluAndRequantize(\n    a,\n    b,\n    bias,\n    min_a,\n    max_a,\n    min_b,\n    max_b,\n    min_freezed_output,\n    max_freezed_output,\n    Toutput=tf.dtypes.quint8,\n    transpose_a=False,\n    transpose_b=False,\n    input_quant_mode='MIN_FIRST',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedMatMulWithBiasAndRequantize": "tf.raw_ops.QuantizedMatMulWithBiasAndRequantize(\n    a,\n    b,\n    bias,\n    min_a,\n    max_a,\n    min_b,\n    max_b,\n    min_freezed_output,\n    max_freezed_output,\n    Toutput=tf.dtypes.quint8,\n    transpose_a=False,\n    transpose_b=False,\n    input_quant_mode='MIN_FIRST',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedMaxPool": "tf.raw_ops.QuantizedMaxPool(\n    input, min_input, max_input, ksize, strides, padding, name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedMul": "tf.raw_ops.QuantizedMul(\n    x,\n    y,\n    min_x,\n    max_x,\n    min_y,\n    max_y,\n    Toutput=tf.dtypes.qint32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedRelu": "tf.raw_ops.QuantizedRelu(\n    features,\n    min_features,\n    max_features,\n    out_type=tf.dtypes.quint8,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedRelu6": "tf.raw_ops.QuantizedRelu6(\n    features,\n    min_features,\n    max_features,\n    out_type=tf.dtypes.quint8,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedReluX": "tf.raw_ops.QuantizedReluX(\n    features,\n    max_value,\n    min_features,\n    max_features,\n    out_type=tf.dtypes.quint8,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedReshape": "tf.raw_ops.QuantizedReshape(\n    tensor, shape, input_min, input_max, name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedResizeBilinear": "tf.raw_ops.QuantizedResizeBilinear(\n    images,\n    size,\n    min,\n    max,\n    align_corners=False,\n    half_pixel_centers=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueClose": "tf.raw_ops.QueueClose(\n    handle, cancel_pending_enqueues=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueCloseV2": "tf.raw_ops.QueueCloseV2(\n    handle, cancel_pending_enqueues=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueDequeue": "tf.raw_ops.QueueDequeue(\n    handle, component_types, timeout_ms=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueDequeueMany": "tf.raw_ops.QueueDequeueMany(\n    handle, n, component_types, timeout_ms=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueDequeueManyV2": "tf.raw_ops.QueueDequeueManyV2(\n    handle, n, component_types, timeout_ms=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueDequeueUpTo": "tf.raw_ops.QueueDequeueUpTo(\n    handle, n, component_types, timeout_ms=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueDequeueUpToV2": "tf.raw_ops.QueueDequeueUpToV2(\n    handle, n, component_types, timeout_ms=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueDequeueV2": "tf.raw_ops.QueueDequeueV2(\n    handle, component_types, timeout_ms=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueEnqueue": "tf.raw_ops.QueueEnqueue(\n    handle, components, timeout_ms=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueEnqueueMany": "tf.raw_ops.QueueEnqueueMany(\n    handle, components, timeout_ms=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueEnqueueManyV2": "tf.raw_ops.QueueEnqueueManyV2(\n    handle, components, timeout_ms=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueEnqueueV2": "tf.raw_ops.QueueEnqueueV2(\n    handle, components, timeout_ms=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueIsClosed": "tf.raw_ops.QueueIsClosed(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueIsClosedV2": "tf.raw_ops.QueueIsClosedV2(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueSize": "tf.raw_ops.QueueSize(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueSizeV2": "tf.raw_ops.QueueSizeV2(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.RFFT": "tf.raw_ops.RFFT(\n    input,\n    fft_length,\n    Tcomplex=tf.dtypes.complex64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RFFT2D": "tf.raw_ops.RFFT2D(\n    input,\n    fft_length,\n    Tcomplex=tf.dtypes.complex64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RFFT3D": "tf.raw_ops.RFFT3D(\n    input,\n    fft_length,\n    Tcomplex=tf.dtypes.complex64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RGBToHSV": "tf.raw_ops.RGBToHSV(\n    images, name=None\n)\n"
  },
  {
    "tf.raw_ops.RaggedBincount": "tf.raw_ops.RaggedBincount(\n    splits, values, size, weights, binary_output=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.RaggedCountSparseOutput": "tf.raw_ops.RaggedCountSparseOutput(\n    splits,\n    values,\n    weights,\n    binary_output,\n    minlength=-1,\n    maxlength=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RaggedCross": "tf.raw_ops.RaggedCross(\n    ragged_values,\n    ragged_row_splits,\n    sparse_indices,\n    sparse_values,\n    sparse_shape,\n    dense_inputs,\n    input_order,\n    hashed_output,\n    num_buckets,\n    hash_key,\n    out_values_type,\n    out_row_splits_type,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RaggedGather": "tf.raw_ops.RaggedGather(\n    params_nested_splits,\n    params_dense_values,\n    indices,\n    OUTPUT_RAGGED_RANK,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RaggedRange": "tf.raw_ops.RaggedRange(\n    starts,\n    limits,\n    deltas,\n    Tsplits=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RaggedTensorFromVariant": "tf.raw_ops.RaggedTensorFromVariant(\n    encoded_ragged,\n    input_ragged_rank,\n    output_ragged_rank,\n    Tvalues,\n    Tsplits=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RaggedTensorToSparse": "tf.raw_ops.RaggedTensorToSparse(\n    rt_nested_splits, rt_dense_values, name=None\n)\n"
  },
  {
    "tf.raw_ops.RaggedTensorToTensor": "tf.raw_ops.RaggedTensorToTensor(\n    shape,\n    values,\n    default_value,\n    row_partition_tensors,\n    row_partition_types,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RaggedTensorToVariant": "tf.raw_ops.RaggedTensorToVariant(\n    rt_nested_splits, rt_dense_values, batched_input, name=None\n)\n"
  },
  {
    "tf.raw_ops.RaggedTensorToVariantGradient": "tf.raw_ops.RaggedTensorToVariantGradient(\n    encoded_ragged_grad, row_splits, dense_values_shape, Tvalues, name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomCrop": "tf.raw_ops.RandomCrop(\n    image, size, seed=0, seed2=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomDataset": "tf.raw_ops.RandomDataset(\n    seed, seed2, output_types, output_shapes, metadata='', name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomGamma": "tf.raw_ops.RandomGamma(\n    shape, alpha, seed=0, seed2=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomGammaGrad": "tf.raw_ops.RandomGammaGrad(\n    alpha, sample, name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomIndexShuffle": "tf.raw_ops.RandomIndexShuffle(\n    index, seed, max_index, rounds=4, name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomPoisson": "tf.raw_ops.RandomPoisson(\n    shape, rate, seed=0, seed2=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomPoissonV2": "tf.raw_ops.RandomPoissonV2(\n    shape,\n    rate,\n    seed=0,\n    seed2=0,\n    dtype=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomShuffle": "tf.raw_ops.RandomShuffle(\n    value, seed=0, seed2=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomShuffleQueue": "tf.raw_ops.RandomShuffleQueue(\n    component_types,\n    shapes=[],\n    capacity=-1,\n    min_after_dequeue=0,\n    seed=0,\n    seed2=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomShuffleQueueV2": "tf.raw_ops.RandomShuffleQueueV2(\n    component_types,\n    shapes=[],\n    capacity=-1,\n    min_after_dequeue=0,\n    seed=0,\n    seed2=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomStandardNormal": "tf.raw_ops.RandomStandardNormal(\n    shape, dtype, seed=0, seed2=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomUniform": "tf.raw_ops.RandomUniform(\n    shape, dtype, seed=0, seed2=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomUniformInt": "tf.raw_ops.RandomUniformInt(\n    shape, minval, maxval, seed=0, seed2=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.Range": "tf.raw_ops.Range(\n    start, limit, delta, name=None\n)\n"
  },
  {
    "tf.raw_ops.RangeDataset": "tf.raw_ops.RangeDataset(\n    start,\n    stop,\n    step,\n    output_types,\n    output_shapes,\n    metadata='',\n    replicate_on_split=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Rank": "tf.raw_ops.Rank(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReadFile": "tf.raw_ops.ReadFile(\n    filename, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReadVariableOp": "tf.raw_ops.ReadVariableOp(\n    resource, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReadVariableXlaSplitND": "tf.raw_ops.ReadVariableXlaSplitND(\n    resource, T, N, num_splits, paddings=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderNumRecordsProduced": "tf.raw_ops.ReaderNumRecordsProduced(\n    reader_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderNumRecordsProducedV2": "tf.raw_ops.ReaderNumRecordsProducedV2(\n    reader_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderNumWorkUnitsCompleted": "tf.raw_ops.ReaderNumWorkUnitsCompleted(\n    reader_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderNumWorkUnitsCompletedV2": "tf.raw_ops.ReaderNumWorkUnitsCompletedV2(\n    reader_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderRead": "tf.raw_ops.ReaderRead(\n    reader_handle, queue_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderReadUpTo": "tf.raw_ops.ReaderReadUpTo(\n    reader_handle, queue_handle, num_records, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderReadUpToV2": "tf.raw_ops.ReaderReadUpToV2(\n    reader_handle, queue_handle, num_records, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderReadV2": "tf.raw_ops.ReaderReadV2(\n    reader_handle, queue_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderReset": "tf.raw_ops.ReaderReset(\n    reader_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderResetV2": "tf.raw_ops.ReaderResetV2(\n    reader_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderRestoreState": "tf.raw_ops.ReaderRestoreState(\n    reader_handle, state, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderRestoreStateV2": "tf.raw_ops.ReaderRestoreStateV2(\n    reader_handle, state, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderSerializeState": "tf.raw_ops.ReaderSerializeState(\n    reader_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderSerializeStateV2": "tf.raw_ops.ReaderSerializeStateV2(\n    reader_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.Real": "tf.raw_ops.Real(\n    input,\n    Tout=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RealDiv": "tf.raw_ops.RealDiv(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.RebatchDataset": "tf.raw_ops.RebatchDataset(\n    input_dataset,\n    num_replicas,\n    output_types,\n    output_shapes,\n    use_fallback=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RebatchDatasetV2": "tf.raw_ops.RebatchDatasetV2(\n    input_dataset,\n    batch_sizes,\n    drop_remainder,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Reciprocal": "tf.raw_ops.Reciprocal(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReciprocalGrad": "tf.raw_ops.ReciprocalGrad(\n    y, dy, name=None\n)\n"
  },
  {
    "tf.raw_ops.RecordInput": "tf.raw_ops.RecordInput(\n    file_pattern,\n    file_random_seed=301,\n    file_shuffle_shift_ratio=0,\n    file_buffer_size=10000,\n    file_parallelism=16,\n    batch_size=32,\n    compression_type='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Recv": "tf.raw_ops.Recv(\n    tensor_type,\n    tensor_name,\n    send_device,\n    send_device_incarnation,\n    recv_device,\n    client_terminated=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RecvTPUEmbeddingActivations": "tf.raw_ops.RecvTPUEmbeddingActivations(\n    num_outputs, config, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReduceDataset": "tf.raw_ops.ReduceDataset(\n    input_dataset,\n    initial_state,\n    other_arguments,\n    f,\n    output_types,\n    output_shapes,\n    use_inter_op_parallelism=True,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ReduceJoin": "tf.raw_ops.ReduceJoin(\n    inputs,\n    reduction_indices,\n    keep_dims=False,\n    separator='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RefEnter": "tf.raw_ops.RefEnter(\n    data, frame_name, is_constant=False, parallel_iterations=10, name=None\n)\n"
  },
  {
    "tf.raw_ops.RefExit": "tf.raw_ops.RefExit(\n    data, name=None\n)\n"
  },
  {
    "tf.raw_ops.RefIdentity": "tf.raw_ops.RefIdentity(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.RefMerge": "tf.raw_ops.RefMerge(\n    inputs, name=None\n)\n"
  },
  {
    "tf.raw_ops.RefNextIteration": "tf.raw_ops.RefNextIteration(\n    data, name=None\n)\n"
  },
  {
    "tf.raw_ops.RefSelect": "tf.raw_ops.RefSelect(\n    index, inputs, name=None\n)\n"
  },
  {
    "tf.raw_ops.RefSwitch": "tf.raw_ops.RefSwitch(\n    data, pred, name=None\n)\n"
  },
  {
    "tf.raw_ops.RegexFullMatch": "tf.raw_ops.RegexFullMatch(\n    input, pattern, name=None\n)\n"
  },
  {
    "tf.raw_ops.RegexReplace": "tf.raw_ops.RegexReplace(\n    input, pattern, rewrite, replace_global=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.RegisterDataset": "tf.raw_ops.RegisterDataset(\n    dataset,\n    address,\n    protocol,\n    external_state_policy,\n    element_spec='',\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RegisterDatasetV2": "tf.raw_ops.RegisterDatasetV2(\n    dataset,\n    address,\n    protocol,\n    external_state_policy,\n    element_spec='',\n    requested_dataset_id='',\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Relu": "tf.raw_ops.Relu(\n    features, name=None\n)\n"
  },
  {
    "tf.raw_ops.Relu6": "tf.raw_ops.Relu6(\n    features, name=None\n)\n"
  },
  {
    "tf.raw_ops.Relu6Grad": "tf.raw_ops.Relu6Grad(\n    gradients, features, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReluGrad": "tf.raw_ops.ReluGrad(\n    gradients, features, name=None\n)\n"
  },
  {
    "tf.raw_ops.RemoteCall": "tf.raw_ops.RemoteCall(\n    target, args, Tout, f, name=None\n)\n"
  },
  {
    "tf.raw_ops.RepeatDataset": "tf.raw_ops.RepeatDataset(\n    input_dataset,\n    count,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RequantizationRange": "tf.raw_ops.RequantizationRange(\n    input, input_min, input_max, name=None\n)\n"
  },
  {
    "tf.raw_ops.RequantizationRangePerChannel": "tf.raw_ops.RequantizationRangePerChannel(\n    input, input_min, input_max, clip_value_max, name=None\n)\n"
  },
  {
    "tf.raw_ops.Requantize": "tf.raw_ops.Requantize(\n    input,\n    input_min,\n    input_max,\n    requested_output_min,\n    requested_output_max,\n    out_type,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RequantizePerChannel": "tf.raw_ops.RequantizePerChannel(\n    input,\n    input_min,\n    input_max,\n    requested_output_min,\n    requested_output_max,\n    out_type=tf.dtypes.quint8,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Reshape": "tf.raw_ops.Reshape(\n    tensor, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResizeArea": "tf.raw_ops.ResizeArea(\n    images, size, align_corners=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResizeBicubic": "tf.raw_ops.ResizeBicubic(\n    images, size, align_corners=False, half_pixel_centers=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResizeBicubicGrad": "tf.raw_ops.ResizeBicubicGrad(\n    grads,\n    original_image,\n    align_corners=False,\n    half_pixel_centers=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResizeBilinear": "tf.raw_ops.ResizeBilinear(\n    images, size, align_corners=False, half_pixel_centers=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResizeBilinearGrad": "tf.raw_ops.ResizeBilinearGrad(\n    grads,\n    original_image,\n    align_corners=False,\n    half_pixel_centers=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResizeNearestNeighbor": "tf.raw_ops.ResizeNearestNeighbor(\n    images, size, align_corners=False, half_pixel_centers=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResizeNearestNeighborGrad": "tf.raw_ops.ResizeNearestNeighborGrad(\n    grads, size, align_corners=False, half_pixel_centers=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceAccumulatorApplyGradient": "tf.raw_ops.ResourceAccumulatorApplyGradient(\n    handle, local_step, gradient, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceAccumulatorNumAccumulated": "tf.raw_ops.ResourceAccumulatorNumAccumulated(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceAccumulatorSetGlobalStep": "tf.raw_ops.ResourceAccumulatorSetGlobalStep(\n    handle, new_global_step, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceAccumulatorTakeGradient": "tf.raw_ops.ResourceAccumulatorTakeGradient(\n    handle, num_required, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyAdaMax": "tf.raw_ops.ResourceApplyAdaMax(\n    var,\n    m,\n    v,\n    beta1_power,\n    lr,\n    beta1,\n    beta2,\n    epsilon,\n    grad,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyAdadelta": "tf.raw_ops.ResourceApplyAdadelta(\n    var,\n    accum,\n    accum_update,\n    lr,\n    rho,\n    epsilon,\n    grad,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyAdagrad": "tf.raw_ops.ResourceApplyAdagrad(\n    var, accum, lr, grad, use_locking=False, update_slots=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyAdagradDA": "tf.raw_ops.ResourceApplyAdagradDA(\n    var,\n    gradient_accumulator,\n    gradient_squared_accumulator,\n    grad,\n    lr,\n    l1,\n    l2,\n    global_step,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyAdagradV2": "tf.raw_ops.ResourceApplyAdagradV2(\n    var,\n    accum,\n    lr,\n    epsilon,\n    grad,\n    use_locking=False,\n    update_slots=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyAdam": "tf.raw_ops.ResourceApplyAdam(\n    var,\n    m,\n    v,\n    beta1_power,\n    beta2_power,\n    lr,\n    beta1,\n    beta2,\n    epsilon,\n    grad,\n    use_locking=False,\n    use_nesterov=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyAdamWithAmsgrad": "tf.raw_ops.ResourceApplyAdamWithAmsgrad(\n    var,\n    m,\n    v,\n    vhat,\n    beta1_power,\n    beta2_power,\n    lr,\n    beta1,\n    beta2,\n    epsilon,\n    grad,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyAddSign": "tf.raw_ops.ResourceApplyAddSign(\n    var, m, lr, alpha, sign_decay, beta, grad, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyCenteredRMSProp": "tf.raw_ops.ResourceApplyCenteredRMSProp(\n    var,\n    mg,\n    ms,\n    mom,\n    lr,\n    rho,\n    momentum,\n    epsilon,\n    grad,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyFtrl": "tf.raw_ops.ResourceApplyFtrl(\n    var,\n    accum,\n    linear,\n    grad,\n    lr,\n    l1,\n    l2,\n    lr_power,\n    use_locking=False,\n    multiply_linear_by_lr=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyFtrlV2": "tf.raw_ops.ResourceApplyFtrlV2(\n    var,\n    accum,\n    linear,\n    grad,\n    lr,\n    l1,\n    l2,\n    l2_shrinkage,\n    lr_power,\n    use_locking=False,\n    multiply_linear_by_lr=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyGradientDescent": "tf.raw_ops.ResourceApplyGradientDescent(\n    var, alpha, delta, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyKerasMomentum": "tf.raw_ops.ResourceApplyKerasMomentum(\n    var,\n    accum,\n    lr,\n    grad,\n    momentum,\n    use_locking=False,\n    use_nesterov=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyMomentum": "tf.raw_ops.ResourceApplyMomentum(\n    var,\n    accum,\n    lr,\n    grad,\n    momentum,\n    use_locking=False,\n    use_nesterov=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyPowerSign": "tf.raw_ops.ResourceApplyPowerSign(\n    var, m, lr, logbase, sign_decay, beta, grad, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyProximalAdagrad": "tf.raw_ops.ResourceApplyProximalAdagrad(\n    var, accum, lr, l1, l2, grad, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyProximalGradientDescent": "tf.raw_ops.ResourceApplyProximalGradientDescent(\n    var, alpha, l1, l2, delta, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyRMSProp": "tf.raw_ops.ResourceApplyRMSProp(\n    var,\n    ms,\n    mom,\n    lr,\n    rho,\n    momentum,\n    epsilon,\n    grad,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceConditionalAccumulator": "tf.raw_ops.ResourceConditionalAccumulator(\n    dtype,\n    shape,\n    container='',\n    shared_name='',\n    reduction_type='MEAN',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceCountUpTo": "tf.raw_ops.ResourceCountUpTo(\n    resource, limit, T, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceGather": "tf.raw_ops.ResourceGather(\n    resource, indices, dtype, batch_dims=0, validate_indices=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceGatherNd": "tf.raw_ops.ResourceGatherNd(\n    resource, indices, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterAdd": "tf.raw_ops.ResourceScatterAdd(\n    resource, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterDiv": "tf.raw_ops.ResourceScatterDiv(\n    resource, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterMax": "tf.raw_ops.ResourceScatterMax(\n    resource, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterMin": "tf.raw_ops.ResourceScatterMin(\n    resource, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterMul": "tf.raw_ops.ResourceScatterMul(\n    resource, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterNdAdd": "tf.raw_ops.ResourceScatterNdAdd(\n    ref, indices, updates, use_locking=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterNdMax": "tf.raw_ops.ResourceScatterNdMax(\n    ref, indices, updates, use_locking=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterNdMin": "tf.raw_ops.ResourceScatterNdMin(\n    ref, indices, updates, use_locking=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterNdSub": "tf.raw_ops.ResourceScatterNdSub(\n    ref, indices, updates, use_locking=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterNdUpdate": "tf.raw_ops.ResourceScatterNdUpdate(\n    ref, indices, updates, use_locking=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterSub": "tf.raw_ops.ResourceScatterSub(\n    resource, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterUpdate": "tf.raw_ops.ResourceScatterUpdate(\n    resource, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyAdadelta": "tf.raw_ops.ResourceSparseApplyAdadelta(\n    var,\n    accum,\n    accum_update,\n    lr,\n    rho,\n    epsilon,\n    grad,\n    indices,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyAdagrad": "tf.raw_ops.ResourceSparseApplyAdagrad(\n    var,\n    accum,\n    lr,\n    grad,\n    indices,\n    use_locking=False,\n    update_slots=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyAdagradDA": "tf.raw_ops.ResourceSparseApplyAdagradDA(\n    var,\n    gradient_accumulator,\n    gradient_squared_accumulator,\n    grad,\n    indices,\n    lr,\n    l1,\n    l2,\n    global_step,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyAdagradV2": "tf.raw_ops.ResourceSparseApplyAdagradV2(\n    var,\n    accum,\n    lr,\n    epsilon,\n    grad,\n    indices,\n    use_locking=False,\n    update_slots=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyCenteredRMSProp": "tf.raw_ops.ResourceSparseApplyCenteredRMSProp(\n    var,\n    mg,\n    ms,\n    mom,\n    lr,\n    rho,\n    momentum,\n    epsilon,\n    grad,\n    indices,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyFtrl": "tf.raw_ops.ResourceSparseApplyFtrl(\n    var,\n    accum,\n    linear,\n    grad,\n    indices,\n    lr,\n    l1,\n    l2,\n    lr_power,\n    use_locking=False,\n    multiply_linear_by_lr=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyFtrlV2": "tf.raw_ops.ResourceSparseApplyFtrlV2(\n    var,\n    accum,\n    linear,\n    grad,\n    indices,\n    lr,\n    l1,\n    l2,\n    l2_shrinkage,\n    lr_power,\n    use_locking=False,\n    multiply_linear_by_lr=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyKerasMomentum": "tf.raw_ops.ResourceSparseApplyKerasMomentum(\n    var,\n    accum,\n    lr,\n    grad,\n    indices,\n    momentum,\n    use_locking=False,\n    use_nesterov=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyMomentum": "tf.raw_ops.ResourceSparseApplyMomentum(\n    var,\n    accum,\n    lr,\n    grad,\n    indices,\n    momentum,\n    use_locking=False,\n    use_nesterov=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyProximalAdagrad": "tf.raw_ops.ResourceSparseApplyProximalAdagrad(\n    var, accum, lr, l1, l2, grad, indices, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyProximalGradientDescent": "tf.raw_ops.ResourceSparseApplyProximalGradientDescent(\n    var, alpha, l1, l2, grad, indices, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyRMSProp": "tf.raw_ops.ResourceSparseApplyRMSProp(\n    var,\n    ms,\n    mom,\n    lr,\n    rho,\n    momentum,\n    epsilon,\n    grad,\n    indices,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceStridedSliceAssign": "tf.raw_ops.ResourceStridedSliceAssign(\n    ref,\n    begin,\n    end,\n    strides,\n    value,\n    begin_mask=0,\n    end_mask=0,\n    ellipsis_mask=0,\n    new_axis_mask=0,\n    shrink_axis_mask=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Restore": "tf.raw_ops.Restore(\n    file_pattern, tensor_name, dt, preferred_shard=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.RestoreSlice": "tf.raw_ops.RestoreSlice(\n    file_pattern,\n    tensor_name,\n    shape_and_slice,\n    dt,\n    preferred_shard=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RestoreV2": "tf.raw_ops.RestoreV2(\n    prefix, tensor_names, shape_and_slices, dtypes, name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingADAMParameters": "tf.raw_ops.RetrieveTPUEmbeddingADAMParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingAdadeltaParameters": "tf.raw_ops.RetrieveTPUEmbeddingAdadeltaParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingAdagradMomentumParameters": "tf.raw_ops.RetrieveTPUEmbeddingAdagradMomentumParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingAdagradParameters": "tf.raw_ops.RetrieveTPUEmbeddingAdagradParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingCenteredRMSPropParameters": "tf.raw_ops.RetrieveTPUEmbeddingCenteredRMSPropParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingFTRLParameters": "tf.raw_ops.RetrieveTPUEmbeddingFTRLParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingFrequencyEstimatorParameters": "tf.raw_ops.RetrieveTPUEmbeddingFrequencyEstimatorParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingMDLAdagradLightParameters": "tf.raw_ops.RetrieveTPUEmbeddingMDLAdagradLightParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingMomentumParameters": "tf.raw_ops.RetrieveTPUEmbeddingMomentumParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingProximalAdagradParameters": "tf.raw_ops.RetrieveTPUEmbeddingProximalAdagradParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingProximalYogiParameters": "tf.raw_ops.RetrieveTPUEmbeddingProximalYogiParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingRMSPropParameters": "tf.raw_ops.RetrieveTPUEmbeddingRMSPropParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingStochasticGradientDescentParameters": "tf.raw_ops.RetrieveTPUEmbeddingStochasticGradientDescentParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Reverse": "tf.raw_ops.Reverse(\n    tensor, dims, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReverseSequence": "tf.raw_ops.ReverseSequence(\n    input, seq_lengths, seq_dim, batch_dim=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReverseV2": "tf.raw_ops.ReverseV2(\n    tensor, axis, name=None\n)\n"
  },
  {
    "tf.raw_ops.RewriteDataset": "tf.raw_ops.RewriteDataset(\n    input_dataset, rewrite_name, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.RightShift": "tf.raw_ops.RightShift(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.Rint": "tf.raw_ops.Rint(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.RngReadAndSkip": "tf.raw_ops.RngReadAndSkip(\n    resource, alg, delta, name=None\n)\n"
  },
  {
    "tf.raw_ops.RngSkip": "tf.raw_ops.RngSkip(\n    resource, algorithm, delta, name=None\n)\n"
  },
  {
    "tf.raw_ops.Roll": "tf.raw_ops.Roll(\n    input, shift, axis, name=None\n)\n"
  },
  {
    "tf.raw_ops.Round": "tf.raw_ops.Round(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Rsqrt": "tf.raw_ops.Rsqrt(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.RsqrtGrad": "tf.raw_ops.RsqrtGrad(\n    y, dy, name=None\n)\n"
  },
  {
    "tf.raw_ops.SampleDistortedBoundingBox": "tf.raw_ops.SampleDistortedBoundingBox(\n    image_size,\n    bounding_boxes,\n    seed=0,\n    seed2=0,\n    min_object_covered=0.1,\n    aspect_ratio_range=[0.75, 1.33],\n    area_range=[0.05, 1],\n    max_attempts=100,\n    use_image_if_no_bounding_boxes=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SampleDistortedBoundingBoxV2": "tf.raw_ops.SampleDistortedBoundingBoxV2(\n    image_size,\n    bounding_boxes,\n    min_object_covered,\n    seed=0,\n    seed2=0,\n    aspect_ratio_range=[0.75, 1.33],\n    area_range=[0.05, 1],\n    max_attempts=100,\n    use_image_if_no_bounding_boxes=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SamplingDataset": "tf.raw_ops.SamplingDataset(\n    input_dataset, rate, seed, seed2, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.Save": "tf.raw_ops.Save(\n    filename, tensor_names, data, name=None\n)\n"
  },
  {
    "tf.raw_ops.SaveDataset": "tf.raw_ops.SaveDataset(\n    input_dataset,\n    path,\n    shard_func_other_args,\n    shard_func,\n    compression='',\n    use_shard_func=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SaveDatasetV2": "tf.raw_ops.SaveDatasetV2(\n    input_dataset,\n    path,\n    shard_func_other_args,\n    shard_func,\n    output_types,\n    output_shapes,\n    compression='',\n    use_shard_func=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SaveSlices": "tf.raw_ops.SaveSlices(\n    filename, tensor_names, shapes_and_slices, data, name=None\n)\n"
  },
  {
    "tf.raw_ops.SaveV2": "tf.raw_ops.SaveV2(\n    prefix, tensor_names, shape_and_slices, tensors, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScalarSummary": "tf.raw_ops.ScalarSummary(\n    tags, values, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScaleAndTranslate": "tf.raw_ops.ScaleAndTranslate(\n    images,\n    size,\n    scale,\n    translation,\n    kernel_type='lanczos3',\n    antialias=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ScaleAndTranslateGrad": "tf.raw_ops.ScaleAndTranslateGrad(\n    grads,\n    original_image,\n    scale,\n    translation,\n    kernel_type='lanczos3',\n    antialias=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ScanDataset": "tf.raw_ops.ScanDataset(\n    input_dataset,\n    initial_state,\n    other_arguments,\n    f,\n    output_types,\n    output_shapes,\n    preserve_cardinality=False,\n    use_default_device=True,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterAdd": "tf.raw_ops.ScatterAdd(\n    ref, indices, updates, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterDiv": "tf.raw_ops.ScatterDiv(\n    ref, indices, updates, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterMax": "tf.raw_ops.ScatterMax(\n    ref, indices, updates, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterMin": "tf.raw_ops.ScatterMin(\n    ref, indices, updates, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterMul": "tf.raw_ops.ScatterMul(\n    ref, indices, updates, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterNd": "tf.raw_ops.ScatterNd(\n    indices, updates, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterNdAdd": "tf.raw_ops.ScatterNdAdd(\n    ref, indices, updates, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterNdMax": "tf.raw_ops.ScatterNdMax(\n    ref, indices, updates, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterNdMin": "tf.raw_ops.ScatterNdMin(\n    ref, indices, updates, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterNdNonAliasingAdd": "tf.raw_ops.ScatterNdNonAliasingAdd(\n    input, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterNdSub": "tf.raw_ops.ScatterNdSub(\n    ref, indices, updates, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterNdUpdate": "tf.raw_ops.ScatterNdUpdate(\n    ref, indices, updates, use_locking=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterSub": "tf.raw_ops.ScatterSub(\n    ref, indices, updates, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterUpdate": "tf.raw_ops.ScatterUpdate(\n    ref, indices, updates, use_locking=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.SdcaFprint": "tf.raw_ops.SdcaFprint(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.SdcaOptimizer": "tf.raw_ops.SdcaOptimizer(\n    sparse_example_indices,\n    sparse_feature_indices,\n    sparse_feature_values,\n    dense_features,\n    example_weights,\n    example_labels,\n    sparse_indices,\n    sparse_weights,\n    dense_weights,\n    example_state_data,\n    loss_type,\n    l1,\n    l2,\n    num_loss_partitions,\n    num_inner_iterations,\n    adaptative=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SdcaOptimizerV2": "tf.raw_ops.SdcaOptimizerV2(\n    sparse_example_indices,\n    sparse_feature_indices,\n    sparse_feature_values,\n    dense_features,\n    example_weights,\n    example_labels,\n    sparse_indices,\n    sparse_weights,\n    dense_weights,\n    example_state_data,\n    loss_type,\n    l1,\n    l2,\n    num_loss_partitions,\n    num_inner_iterations,\n    adaptive=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SdcaShrinkL1": "tf.raw_ops.SdcaShrinkL1(\n    weights, l1, l2, name=None\n)\n"
  },
  {
    "tf.raw_ops.SegmentMax": "tf.raw_ops.SegmentMax(\n    data, segment_ids, name=None\n)\n"
  },
  {
    "tf.raw_ops.SegmentMean": "tf.raw_ops.SegmentMean(\n    data, segment_ids, name=None\n)\n"
  },
  {
    "tf.raw_ops.SegmentMin": "tf.raw_ops.SegmentMin(\n    data, segment_ids, name=None\n)\n"
  },
  {
    "tf.raw_ops.SegmentProd": "tf.raw_ops.SegmentProd(\n    data, segment_ids, name=None\n)\n"
  },
  {
    "tf.raw_ops.SegmentSum": "tf.raw_ops.SegmentSum(\n    data, segment_ids, name=None\n)\n"
  },
  {
    "tf.raw_ops.Select": "tf.raw_ops.Select(\n    condition, x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.SelectV2": "tf.raw_ops.SelectV2(\n    condition, t, e, name=None\n)\n"
  },
  {
    "tf.raw_ops.SelfAdjointEig": "tf.raw_ops.SelfAdjointEig(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.SelfAdjointEigV2": "tf.raw_ops.SelfAdjointEigV2(\n    input, compute_v=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.Selu": "tf.raw_ops.Selu(\n    features, name=None\n)\n"
  },
  {
    "tf.raw_ops.SeluGrad": "tf.raw_ops.SeluGrad(\n    gradients, outputs, name=None\n)\n"
  },
  {
    "tf.raw_ops.Send": "tf.raw_ops.Send(\n    tensor,\n    tensor_name,\n    send_device,\n    send_device_incarnation,\n    recv_device,\n    client_terminated=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SendTPUEmbeddingGradients": "tf.raw_ops.SendTPUEmbeddingGradients(\n    inputs, learning_rates, config, name=None\n)\n"
  },
  {
    "tf.raw_ops.SerializeIterator": "tf.raw_ops.SerializeIterator(\n    resource_handle, external_state_policy=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.SerializeManySparse": "tf.raw_ops.SerializeManySparse(\n    sparse_indices,\n    sparse_values,\n    sparse_shape,\n    out_type=tf.dtypes.string,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SerializeSparse": "tf.raw_ops.SerializeSparse(\n    sparse_indices,\n    sparse_values,\n    sparse_shape,\n    out_type=tf.dtypes.string,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SerializeTensor": "tf.raw_ops.SerializeTensor(\n    tensor, name=None\n)\n"
  },
  {
    "tf.raw_ops.SetSize": "tf.raw_ops.SetSize(\n    set_indices, set_values, set_shape, validate_indices=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.SetStatsAggregatorDataset": "tf.raw_ops.SetStatsAggregatorDataset(\n    input_dataset,\n    stats_aggregator,\n    tag,\n    counter_prefix,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Shape": "tf.raw_ops.Shape(\n    input,\n    out_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ShapeN": "tf.raw_ops.ShapeN(\n    input,\n    out_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ShardDataset": "tf.raw_ops.ShardDataset(\n    input_dataset,\n    num_shards,\n    index,\n    output_types,\n    output_shapes,\n    require_non_empty=False,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ShardedFilename": "tf.raw_ops.ShardedFilename(\n    basename, shard, num_shards, name=None\n)\n"
  },
  {
    "tf.raw_ops.ShardedFilespec": "tf.raw_ops.ShardedFilespec(\n    basename, num_shards, name=None\n)\n"
  },
  {
    "tf.raw_ops.ShuffleAndRepeatDataset": "tf.raw_ops.ShuffleAndRepeatDataset(\n    input_dataset,\n    buffer_size,\n    seed,\n    seed2,\n    count,\n    output_types,\n    output_shapes,\n    reshuffle_each_iteration=True,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ShuffleAndRepeatDatasetV2": "tf.raw_ops.ShuffleAndRepeatDatasetV2(\n    input_dataset,\n    buffer_size,\n    seed,\n    seed2,\n    count,\n    seed_generator,\n    output_types,\n    output_shapes,\n    reshuffle_each_iteration=True,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ShuffleDataset": "tf.raw_ops.ShuffleDataset(\n    input_dataset,\n    buffer_size,\n    seed,\n    seed2,\n    output_types,\n    output_shapes,\n    reshuffle_each_iteration=True,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ShuffleDatasetV2": "tf.raw_ops.ShuffleDatasetV2(\n    input_dataset,\n    buffer_size,\n    seed_generator,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ShuffleDatasetV3": "tf.raw_ops.ShuffleDatasetV3(\n    input_dataset,\n    buffer_size,\n    seed,\n    seed2,\n    seed_generator,\n    output_types,\n    output_shapes,\n    reshuffle_each_iteration=True,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ShutdownDistributedTPU": "tf.raw_ops.ShutdownDistributedTPU(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Sigmoid": "tf.raw_ops.Sigmoid(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.SigmoidGrad": "tf.raw_ops.SigmoidGrad(\n    y, dy, name=None\n)\n"
  },
  {
    "tf.raw_ops.Sign": "tf.raw_ops.Sign(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Sin": "tf.raw_ops.Sin(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Sinh": "tf.raw_ops.Sinh(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Size": "tf.raw_ops.Size(\n    input,\n    out_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SkipDataset": "tf.raw_ops.SkipDataset(\n    input_dataset,\n    count,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SleepDataset": "tf.raw_ops.SleepDataset(\n    input_dataset, sleep_microseconds, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.Slice": "tf.raw_ops.Slice(\n    input, begin, size, name=None\n)\n"
  },
  {
    "tf.raw_ops.SlidingWindowDataset": "tf.raw_ops.SlidingWindowDataset(\n    input_dataset,\n    window_size,\n    window_shift,\n    window_stride,\n    output_types,\n    output_shapes,\n    drop_remainder=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Snapshot": "tf.raw_ops.Snapshot(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.SnapshotDataset": "tf.raw_ops.SnapshotDataset(\n    input_dataset,\n    path,\n    output_types,\n    output_shapes,\n    compression='',\n    reader_path_prefix='',\n    writer_path_prefix='',\n    shard_size_bytes=10737418240,\n    pending_snapshot_expiry_seconds=86400,\n    num_reader_threads=1,\n    reader_buffer_size=1,\n    num_writer_threads=1,\n    writer_buffer_size=1,\n    shuffle_on_read=False,\n    seed=0,\n    seed2=0,\n    mode='auto',\n    snapshot_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SnapshotDatasetReader": "tf.raw_ops.SnapshotDatasetReader(\n    shard_dir,\n    start_index,\n    output_types,\n    output_shapes,\n    version,\n    compression='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SnapshotDatasetV2": "tf.raw_ops.SnapshotDatasetV2(\n    input_dataset,\n    path,\n    reader_func_other_args,\n    shard_func_other_args,\n    output_types,\n    output_shapes,\n    reader_func,\n    shard_func,\n    compression='',\n    reader_prefix='',\n    writer_prefix='',\n    hash_valid=False,\n    hash=0,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SnapshotNestedDatasetReader": "tf.raw_ops.SnapshotNestedDatasetReader(\n    inputs, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.SobolSample": "tf.raw_ops.SobolSample(\n    dim,\n    num_results,\n    skip,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Softmax": "tf.raw_ops.Softmax(\n    logits, name=None\n)\n"
  },
  {
    "tf.raw_ops.SoftmaxCrossEntropyWithLogits": "tf.raw_ops.SoftmaxCrossEntropyWithLogits(\n    features, labels, name=None\n)\n"
  },
  {
    "tf.raw_ops.Softplus": "tf.raw_ops.Softplus(\n    features, name=None\n)\n"
  },
  {
    "tf.raw_ops.SoftplusGrad": "tf.raw_ops.SoftplusGrad(\n    gradients, features, name=None\n)\n"
  },
  {
    "tf.raw_ops.Softsign": "tf.raw_ops.Softsign(\n    features, name=None\n)\n"
  },
  {
    "tf.raw_ops.SoftsignGrad": "tf.raw_ops.SoftsignGrad(\n    gradients, features, name=None\n)\n"
  },
  {
    "tf.raw_ops.SpaceToBatch": "tf.raw_ops.SpaceToBatch(\n    input, paddings, block_size, name=None\n)\n"
  },
  {
    "tf.raw_ops.SpaceToBatchND": "tf.raw_ops.SpaceToBatchND(\n    input, block_shape, paddings, name=None\n)\n"
  },
  {
    "tf.raw_ops.SpaceToDepth": "tf.raw_ops.SpaceToDepth(\n    input, block_size, data_format='NHWC', name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseAccumulatorApplyGradient": "tf.raw_ops.SparseAccumulatorApplyGradient(\n    handle,\n    local_step,\n    gradient_indices,\n    gradient_values,\n    gradient_shape,\n    has_known_shape,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseAccumulatorTakeGradient": "tf.raw_ops.SparseAccumulatorTakeGradient(\n    handle, num_required, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseAdd": "tf.raw_ops.SparseAdd(\n    a_indices,\n    a_values,\n    a_shape,\n    b_indices,\n    b_values,\n    b_shape,\n    thresh,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseAddGrad": "tf.raw_ops.SparseAddGrad(\n    backprop_val_grad, a_indices, b_indices, sum_indices, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseApplyAdadelta": "tf.raw_ops.SparseApplyAdadelta(\n    var,\n    accum,\n    accum_update,\n    lr,\n    rho,\n    epsilon,\n    grad,\n    indices,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseApplyAdagrad": "tf.raw_ops.SparseApplyAdagrad(\n    var,\n    accum,\n    lr,\n    grad,\n    indices,\n    use_locking=False,\n    update_slots=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseApplyAdagradDA": "tf.raw_ops.SparseApplyAdagradDA(\n    var,\n    gradient_accumulator,\n    gradient_squared_accumulator,\n    grad,\n    indices,\n    lr,\n    l1,\n    l2,\n    global_step,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseApplyAdagradV2": "tf.raw_ops.SparseApplyAdagradV2(\n    var,\n    accum,\n    lr,\n    epsilon,\n    grad,\n    indices,\n    use_locking=False,\n    update_slots=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseApplyCenteredRMSProp": "tf.raw_ops.SparseApplyCenteredRMSProp(\n    var,\n    mg,\n    ms,\n    mom,\n    lr,\n    rho,\n    momentum,\n    epsilon,\n    grad,\n    indices,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseApplyFtrl": "tf.raw_ops.SparseApplyFtrl(\n    var,\n    accum,\n    linear,\n    grad,\n    indices,\n    lr,\n    l1,\n    l2,\n    lr_power,\n    use_locking=False,\n    multiply_linear_by_lr=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseApplyFtrlV2": "tf.raw_ops.SparseApplyFtrlV2(\n    var,\n    accum,\n    linear,\n    grad,\n    indices,\n    lr,\n    l1,\n    l2,\n    l2_shrinkage,\n    lr_power,\n    use_locking=False,\n    multiply_linear_by_lr=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseApplyMomentum": "tf.raw_ops.SparseApplyMomentum(\n    var,\n    accum,\n    lr,\n    grad,\n    indices,\n    momentum,\n    use_locking=False,\n    use_nesterov=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseApplyProximalAdagrad": "tf.raw_ops.SparseApplyProximalAdagrad(\n    var, accum, lr, l1, l2, grad, indices, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseApplyProximalGradientDescent": "tf.raw_ops.SparseApplyProximalGradientDescent(\n    var, alpha, l1, l2, grad, indices, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseApplyRMSProp": "tf.raw_ops.SparseApplyRMSProp(\n    var,\n    ms,\n    mom,\n    lr,\n    rho,\n    momentum,\n    epsilon,\n    grad,\n    indices,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseBincount": "tf.raw_ops.SparseBincount(\n    indices, values, dense_shape, size, weights, binary_output=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseConcat": "tf.raw_ops.SparseConcat(\n    indices, values, shapes, concat_dim, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseConditionalAccumulator": "tf.raw_ops.SparseConditionalAccumulator(\n    dtype,\n    shape,\n    container='',\n    shared_name='',\n    reduction_type='MEAN',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseCountSparseOutput": "tf.raw_ops.SparseCountSparseOutput(\n    indices,\n    values,\n    dense_shape,\n    weights,\n    binary_output,\n    minlength=-1,\n    maxlength=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseCross": "tf.raw_ops.SparseCross(\n    indices,\n    values,\n    shapes,\n    dense_inputs,\n    hashed_output,\n    num_buckets,\n    hash_key,\n    out_type,\n    internal_type,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseCrossHashed": "tf.raw_ops.SparseCrossHashed(\n    indices,\n    values,\n    shapes,\n    dense_inputs,\n    num_buckets,\n    strong_hash,\n    salt,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseCrossV2": "tf.raw_ops.SparseCrossV2(\n    indices, values, shapes, dense_inputs, sep, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseDenseCwiseAdd": "tf.raw_ops.SparseDenseCwiseAdd(\n    sp_indices, sp_values, sp_shape, dense, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseDenseCwiseDiv": "tf.raw_ops.SparseDenseCwiseDiv(\n    sp_indices, sp_values, sp_shape, dense, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseDenseCwiseMul": "tf.raw_ops.SparseDenseCwiseMul(\n    sp_indices, sp_values, sp_shape, dense, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseFillEmptyRows": "tf.raw_ops.SparseFillEmptyRows(\n    indices, values, dense_shape, default_value, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseFillEmptyRowsGrad": "tf.raw_ops.SparseFillEmptyRowsGrad(\n    reverse_index_map, grad_values, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatMul": "tf.raw_ops.SparseMatMul(\n    a,\n    b,\n    transpose_a=False,\n    transpose_b=False,\n    a_is_sparse=False,\n    b_is_sparse=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatrixAdd": "tf.raw_ops.SparseMatrixAdd(\n    a, b, alpha, beta, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatrixMatMul": "tf.raw_ops.SparseMatrixMatMul(\n    a,\n    b,\n    transpose_a=False,\n    transpose_b=False,\n    adjoint_a=False,\n    adjoint_b=False,\n    transpose_output=False,\n    conjugate_output=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatrixMul": "tf.raw_ops.SparseMatrixMul(\n    a, b, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatrixNNZ": "tf.raw_ops.SparseMatrixNNZ(\n    sparse_matrix, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatrixOrderingAMD": "tf.raw_ops.SparseMatrixOrderingAMD(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatrixSoftmax": "tf.raw_ops.SparseMatrixSoftmax(\n    logits, type, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatrixSoftmaxGrad": "tf.raw_ops.SparseMatrixSoftmaxGrad(\n    softmax, grad_softmax, type, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatrixSparseCholesky": "tf.raw_ops.SparseMatrixSparseCholesky(\n    input, permutation, type, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatrixSparseMatMul": "tf.raw_ops.SparseMatrixSparseMatMul(\n    a,\n    b,\n    type,\n    transpose_a=False,\n    transpose_b=False,\n    adjoint_a=False,\n    adjoint_b=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatrixTranspose": "tf.raw_ops.SparseMatrixTranspose(\n    input, type, conjugate=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatrixZeros": "tf.raw_ops.SparseMatrixZeros(\n    dense_shape, type, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseReduceMax": "tf.raw_ops.SparseReduceMax(\n    input_indices,\n    input_values,\n    input_shape,\n    reduction_axes,\n    keep_dims=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseReduceMaxSparse": "tf.raw_ops.SparseReduceMaxSparse(\n    input_indices,\n    input_values,\n    input_shape,\n    reduction_axes,\n    keep_dims=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseReduceSum": "tf.raw_ops.SparseReduceSum(\n    input_indices,\n    input_values,\n    input_shape,\n    reduction_axes,\n    keep_dims=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseReduceSumSparse": "tf.raw_ops.SparseReduceSumSparse(\n    input_indices,\n    input_values,\n    input_shape,\n    reduction_axes,\n    keep_dims=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseReorder": "tf.raw_ops.SparseReorder(\n    input_indices, input_values, input_shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseReshape": "tf.raw_ops.SparseReshape(\n    input_indices, input_shape, new_shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSegmentMean": "tf.raw_ops.SparseSegmentMean(\n    data, indices, segment_ids, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSegmentMeanGrad": "tf.raw_ops.SparseSegmentMeanGrad(\n    grad, indices, segment_ids, output_dim0, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSegmentMeanWithNumSegments": "tf.raw_ops.SparseSegmentMeanWithNumSegments(\n    data, indices, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSegmentSqrtN": "tf.raw_ops.SparseSegmentSqrtN(\n    data, indices, segment_ids, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSegmentSqrtNGrad": "tf.raw_ops.SparseSegmentSqrtNGrad(\n    grad, indices, segment_ids, output_dim0, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSegmentSqrtNWithNumSegments": "tf.raw_ops.SparseSegmentSqrtNWithNumSegments(\n    data, indices, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSegmentSum": "tf.raw_ops.SparseSegmentSum(\n    data, indices, segment_ids, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSegmentSumGrad": "tf.raw_ops.SparseSegmentSumGrad(\n    grad, indices, segment_ids, output_dim0, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSegmentSumWithNumSegments": "tf.raw_ops.SparseSegmentSumWithNumSegments(\n    data, indices, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSlice": "tf.raw_ops.SparseSlice(\n    indices, values, shape, start, size, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSliceGrad": "tf.raw_ops.SparseSliceGrad(\n    backprop_val_grad, input_indices, input_start, output_indices, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSoftmax": "tf.raw_ops.SparseSoftmax(\n    sp_indices, sp_values, sp_shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSoftmaxCrossEntropyWithLogits": "tf.raw_ops.SparseSoftmaxCrossEntropyWithLogits(\n    features, labels, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSparseMaximum": "tf.raw_ops.SparseSparseMaximum(\n    a_indices, a_values, a_shape, b_indices, b_values, b_shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSparseMinimum": "tf.raw_ops.SparseSparseMinimum(\n    a_indices, a_values, a_shape, b_indices, b_values, b_shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSplit": "tf.raw_ops.SparseSplit(\n    split_dim, indices, values, shape, num_split, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseTensorDenseAdd": "tf.raw_ops.SparseTensorDenseAdd(\n    a_indices, a_values, a_shape, b, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseTensorDenseMatMul": "tf.raw_ops.SparseTensorDenseMatMul(\n    a_indices,\n    a_values,\n    a_shape,\n    b,\n    adjoint_a=False,\n    adjoint_b=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseTensorSliceDataset": "tf.raw_ops.SparseTensorSliceDataset(\n    indices, values, dense_shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseTensorToCSRSparseMatrix": "tf.raw_ops.SparseTensorToCSRSparseMatrix(\n    indices, values, dense_shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseToDense": "tf.raw_ops.SparseToDense(\n    sparse_indices,\n    output_shape,\n    sparse_values,\n    default_value,\n    validate_indices=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseToSparseSetOperation": "tf.raw_ops.SparseToSparseSetOperation(\n    set1_indices,\n    set1_values,\n    set1_shape,\n    set2_indices,\n    set2_values,\n    set2_shape,\n    set_operation,\n    validate_indices=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Spence": "tf.raw_ops.Spence(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Split": "tf.raw_ops.Split(\n    axis, value, num_split, name=None\n)\n"
  },
  {
    "tf.raw_ops.SplitV": "tf.raw_ops.SplitV(\n    value, size_splits, axis, num_split, name=None\n)\n"
  },
  {
    "tf.raw_ops.SqlDataset": "tf.raw_ops.SqlDataset(\n    driver_name,\n    data_source_name,\n    query,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Sqrt": "tf.raw_ops.Sqrt(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.SqrtGrad": "tf.raw_ops.SqrtGrad(\n    y, dy, name=None\n)\n"
  },
  {
    "tf.raw_ops.Square": "tf.raw_ops.Square(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.SquaredDifference": "tf.raw_ops.SquaredDifference(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.Squeeze": "tf.raw_ops.Squeeze(\n    input, axis=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.Stack": "tf.raw_ops.Stack(\n    elem_type, stack_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.StackClose": "tf.raw_ops.StackClose(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.StackCloseV2": "tf.raw_ops.StackCloseV2(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.StackPop": "tf.raw_ops.StackPop(\n    handle, elem_type, name=None\n)\n"
  },
  {
    "tf.raw_ops.StackPopV2": "tf.raw_ops.StackPopV2(\n    handle, elem_type, name=None\n)\n"
  },
  {
    "tf.raw_ops.StackPush": "tf.raw_ops.StackPush(\n    handle, elem, swap_memory=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.StackPushV2": "tf.raw_ops.StackPushV2(\n    handle, elem, swap_memory=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.StackV2": "tf.raw_ops.StackV2(\n    max_size, elem_type, stack_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.Stage": "tf.raw_ops.Stage(\n    values,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StageClear": "tf.raw_ops.StageClear(\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StagePeek": "tf.raw_ops.StagePeek(\n    index,\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StageSize": "tf.raw_ops.StageSize(\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatefulPartitionedCall": "tf.raw_ops.StatefulPartitionedCall(\n    args,\n    Tout,\n    f,\n    config='',\n    config_proto='',\n    executor_type='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatefulRandomBinomial": "tf.raw_ops.StatefulRandomBinomial(\n    resource,\n    algorithm,\n    shape,\n    counts,\n    probs,\n    dtype=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatefulStandardNormal": "tf.raw_ops.StatefulStandardNormal(\n    resource,\n    shape,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatefulStandardNormalV2": "tf.raw_ops.StatefulStandardNormalV2(\n    resource,\n    algorithm,\n    shape,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatefulTruncatedNormal": "tf.raw_ops.StatefulTruncatedNormal(\n    resource,\n    algorithm,\n    shape,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatefulUniform": "tf.raw_ops.StatefulUniform(\n    resource,\n    algorithm,\n    shape,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatefulUniformFullInt": "tf.raw_ops.StatefulUniformFullInt(\n    resource,\n    algorithm,\n    shape,\n    dtype=tf.dtypes.uint64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatefulUniformInt": "tf.raw_ops.StatefulUniformInt(\n    resource, algorithm, shape, minval, maxval, name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessCase": "tf.raw_ops.StatelessCase(\n    branch_index, input, Tout, branches, output_shapes=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessIf": "tf.raw_ops.StatelessIf(\n    cond, input, Tout, then_branch, else_branch, output_shapes=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessMultinomial": "tf.raw_ops.StatelessMultinomial(\n    logits,\n    num_samples,\n    seed,\n    output_dtype=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessParameterizedTruncatedNormal": "tf.raw_ops.StatelessParameterizedTruncatedNormal(\n    shape, seed, means, stddevs, minvals, maxvals, name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomBinomial": "tf.raw_ops.StatelessRandomBinomial(\n    shape,\n    seed,\n    counts,\n    probs,\n    dtype=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomGammaV2": "tf.raw_ops.StatelessRandomGammaV2(\n    shape, seed, alpha, name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomGetAlg": "tf.raw_ops.StatelessRandomGetAlg(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomGetKeyCounter": "tf.raw_ops.StatelessRandomGetKeyCounter(\n    seed, name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomGetKeyCounterAlg": "tf.raw_ops.StatelessRandomGetKeyCounterAlg(\n    seed, name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomNormal": "tf.raw_ops.StatelessRandomNormal(\n    shape,\n    seed,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomNormalV2": "tf.raw_ops.StatelessRandomNormalV2(\n    shape,\n    key,\n    counter,\n    alg,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomPoisson": "tf.raw_ops.StatelessRandomPoisson(\n    shape, seed, lam, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomUniform": "tf.raw_ops.StatelessRandomUniform(\n    shape,\n    seed,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomUniformFullInt": "tf.raw_ops.StatelessRandomUniformFullInt(\n    shape,\n    seed,\n    dtype=tf.dtypes.uint64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomUniformFullIntV2": "tf.raw_ops.StatelessRandomUniformFullIntV2(\n    shape,\n    key,\n    counter,\n    alg,\n    dtype=tf.dtypes.uint64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomUniformInt": "tf.raw_ops.StatelessRandomUniformInt(\n    shape, seed, minval, maxval, name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomUniformIntV2": "tf.raw_ops.StatelessRandomUniformIntV2(\n    shape, key, counter, alg, minval, maxval, name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomUniformV2": "tf.raw_ops.StatelessRandomUniformV2(\n    shape,\n    key,\n    counter,\n    alg,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessSampleDistortedBoundingBox": "tf.raw_ops.StatelessSampleDistortedBoundingBox(\n    image_size,\n    bounding_boxes,\n    min_object_covered,\n    seed,\n    aspect_ratio_range=[0.75, 1.33],\n    area_range=[0.05, 1],\n    max_attempts=100,\n    use_image_if_no_bounding_boxes=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessShuffle": "tf.raw_ops.StatelessShuffle(\n    value, key, counter, alg, name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessTruncatedNormal": "tf.raw_ops.StatelessTruncatedNormal(\n    shape,\n    seed,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessTruncatedNormalV2": "tf.raw_ops.StatelessTruncatedNormalV2(\n    shape,\n    key,\n    counter,\n    alg,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessWhile": "tf.raw_ops.StatelessWhile(\n    input, cond, body, output_shapes=[], parallel_iterations=10, name=None\n)\n"
  },
  {
    "tf.raw_ops.StaticRegexFullMatch": "tf.raw_ops.StaticRegexFullMatch(\n    input, pattern, name=None\n)\n"
  },
  {
    "tf.raw_ops.StaticRegexReplace": "tf.raw_ops.StaticRegexReplace(\n    input, pattern, rewrite, replace_global=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.StatsAggregatorHandle": "tf.raw_ops.StatsAggregatorHandle(\n    container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.StatsAggregatorHandleV2": "tf.raw_ops.StatsAggregatorHandleV2(\n    container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.StatsAggregatorSetSummaryWriter": "tf.raw_ops.StatsAggregatorSetSummaryWriter(\n    stats_aggregator, summary, name=None\n)\n"
  },
  {
    "tf.raw_ops.StatsAggregatorSummary": "tf.raw_ops.StatsAggregatorSummary(\n    iterator, name=None\n)\n"
  },
  {
    "tf.raw_ops.StopGradient": "tf.raw_ops.StopGradient(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.StridedSlice": "tf.raw_ops.StridedSlice(\n    input,\n    begin,\n    end,\n    strides,\n    begin_mask=0,\n    end_mask=0,\n    ellipsis_mask=0,\n    new_axis_mask=0,\n    shrink_axis_mask=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StridedSliceAssign": "tf.raw_ops.StridedSliceAssign(\n    ref,\n    begin,\n    end,\n    strides,\n    value,\n    begin_mask=0,\n    end_mask=0,\n    ellipsis_mask=0,\n    new_axis_mask=0,\n    shrink_axis_mask=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StridedSliceGrad": "tf.raw_ops.StridedSliceGrad(\n    shape,\n    begin,\n    end,\n    strides,\n    dy,\n    begin_mask=0,\n    end_mask=0,\n    ellipsis_mask=0,\n    new_axis_mask=0,\n    shrink_axis_mask=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StringFormat": "tf.raw_ops.StringFormat(\n    inputs,\n    template='%s',\n    placeholder='%s',\n    summarize=3,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StringJoin": "tf.raw_ops.StringJoin(\n    inputs, separator='', name=None\n)\n"
  },
  {
    "tf.raw_ops.StringLength": "tf.raw_ops.StringLength(\n    input, unit='BYTE', name=None\n)\n"
  },
  {
    "tf.raw_ops.StringLower": "tf.raw_ops.StringLower(\n    input, encoding='', name=None\n)\n"
  },
  {
    "tf.raw_ops.StringNGrams": "tf.raw_ops.StringNGrams(\n    data,\n    data_splits,\n    separator,\n    ngram_widths,\n    left_pad,\n    right_pad,\n    pad_width,\n    preserve_short_sequences,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StringSplit": "tf.raw_ops.StringSplit(\n    input, delimiter, skip_empty=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.StringSplitV2": "tf.raw_ops.StringSplitV2(\n    input, sep, maxsplit=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.StringStrip": "tf.raw_ops.StringStrip(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.StringToHashBucket": "tf.raw_ops.StringToHashBucket(\n    string_tensor, num_buckets, name=None\n)\n"
  },
  {
    "tf.raw_ops.StringToHashBucketFast": "tf.raw_ops.StringToHashBucketFast(\n    input, num_buckets, name=None\n)\n"
  },
  {
    "tf.raw_ops.StringToHashBucketStrong": "tf.raw_ops.StringToHashBucketStrong(\n    input, num_buckets, key, name=None\n)\n"
  },
  {
    "tf.raw_ops.StringToNumber": "tf.raw_ops.StringToNumber(\n    string_tensor,\n    out_type=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StringUpper": "tf.raw_ops.StringUpper(\n    input, encoding='', name=None\n)\n"
  },
  {
    "tf.raw_ops.Sub": "tf.raw_ops.Sub(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.Substr": "tf.raw_ops.Substr(\n    input, pos, len, unit='BYTE', name=None\n)\n"
  },
  {
    "tf.raw_ops.Sum": "tf.raw_ops.Sum(\n    input, axis, keep_dims=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.SummaryWriter": "tf.raw_ops.SummaryWriter(\n    shared_name='', container='', name=None\n)\n"
  },
  {
    "tf.raw_ops.Svd": "tf.raw_ops.Svd(\n    input, compute_uv=True, full_matrices=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.Switch": "tf.raw_ops.Switch(\n    data, pred, name=None\n)\n"
  },
  {
    "tf.raw_ops.SymbolicGradient": "tf.raw_ops.SymbolicGradient(\n    input, Tout, f, name=None\n)\n"
  },
  {
    "tf.raw_ops.TFRecordDataset": "tf.raw_ops.TFRecordDataset(\n    filenames, compression_type, buffer_size, metadata='', name=None\n)\n"
  },
  {
    "tf.raw_ops.TFRecordReader": "tf.raw_ops.TFRecordReader(\n    container='',\n    shared_name='',\n    compression_type='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TFRecordReaderV2": "tf.raw_ops.TFRecordReaderV2(\n    container='',\n    shared_name='',\n    compression_type='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TPUCompilationResult": "tf.raw_ops.TPUCompilationResult(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TPUEmbeddingActivations": "tf.raw_ops.TPUEmbeddingActivations(\n    embedding_variable, sliced_activations, table_id, lookup_id, name=None\n)\n"
  },
  {
    "tf.raw_ops.TPUOrdinalSelector": "tf.raw_ops.TPUOrdinalSelector(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TPUPartitionedCall": "tf.raw_ops.TPUPartitionedCall(\n    args, device_ordinal, Tout, f, autotuner_thresh=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.TPUPartitionedInput": "tf.raw_ops.TPUPartitionedInput(\n    inputs, partition_dim=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.TPUPartitionedOutput": "tf.raw_ops.TPUPartitionedOutput(\n    inputs, num_splits, partition_dim=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.TPUReplicateMetadata": "tf.raw_ops.TPUReplicateMetadata(\n    num_replicas,\n    num_cores_per_replica=1,\n    topology='',\n    use_tpu=True,\n    device_assignment=[],\n    computation_shape=[],\n    host_compute_core=[],\n    padding_map=[],\n    step_marker_location='STEP_MARK_AT_ENTRY',\n    allow_soft_placement=False,\n    use_spmd_for_xla_partitioning=False,\n    tpu_compile_options_proto='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TPUReplicatedInput": "tf.raw_ops.TPUReplicatedInput(\n    inputs, is_mirrored_variable=False, index=-1, is_packed=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.TPUReplicatedOutput": "tf.raw_ops.TPUReplicatedOutput(\n    input, num_replicas, name=None\n)\n"
  },
  {
    "tf.raw_ops.TakeDataset": "tf.raw_ops.TakeDataset(\n    input_dataset,\n    count,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TakeManySparseFromTensorsMap": "tf.raw_ops.TakeManySparseFromTensorsMap(\n    sparse_handles,\n    dtype,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TakeWhileDataset": "tf.raw_ops.TakeWhileDataset(\n    input_dataset,\n    other_arguments,\n    predicate,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Tan": "tf.raw_ops.Tan(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Tanh": "tf.raw_ops.Tanh(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.TanhGrad": "tf.raw_ops.TanhGrad(\n    y, dy, name=None\n)\n"
  },
  {
    "tf.raw_ops.TemporaryVariable": "tf.raw_ops.TemporaryVariable(\n    shape, dtype, var_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArray": "tf.raw_ops.TensorArray(\n    size,\n    dtype,\n    dynamic_size=False,\n    clear_after_read=True,\n    tensor_array_name='',\n    element_shape=None,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayClose": "tf.raw_ops.TensorArrayClose(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayCloseV2": "tf.raw_ops.TensorArrayCloseV2(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayCloseV3": "tf.raw_ops.TensorArrayCloseV3(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayConcat": "tf.raw_ops.TensorArrayConcat(\n    handle, flow_in, dtype, element_shape_except0=None, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayConcatV2": "tf.raw_ops.TensorArrayConcatV2(\n    handle, flow_in, dtype, element_shape_except0=None, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayConcatV3": "tf.raw_ops.TensorArrayConcatV3(\n    handle, flow_in, dtype, element_shape_except0=None, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayGather": "tf.raw_ops.TensorArrayGather(\n    handle, indices, flow_in, dtype, element_shape=None, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayGatherV2": "tf.raw_ops.TensorArrayGatherV2(\n    handle, indices, flow_in, dtype, element_shape=None, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayGatherV3": "tf.raw_ops.TensorArrayGatherV3(\n    handle, indices, flow_in, dtype, element_shape=None, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayGrad": "tf.raw_ops.TensorArrayGrad(\n    handle, flow_in, source, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayGradV2": "tf.raw_ops.TensorArrayGradV2(\n    handle, flow_in, source, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayGradV3": "tf.raw_ops.TensorArrayGradV3(\n    handle, flow_in, source, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayGradWithShape": "tf.raw_ops.TensorArrayGradWithShape(\n    handle, flow_in, shape_to_prepend, source, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayPack": "tf.raw_ops.TensorArrayPack(\n    handle, flow_in, dtype, element_shape=None, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayRead": "tf.raw_ops.TensorArrayRead(\n    handle, index, flow_in, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayReadV2": "tf.raw_ops.TensorArrayReadV2(\n    handle, index, flow_in, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayReadV3": "tf.raw_ops.TensorArrayReadV3(\n    handle, index, flow_in, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayScatter": "tf.raw_ops.TensorArrayScatter(\n    handle, indices, value, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayScatterV2": "tf.raw_ops.TensorArrayScatterV2(\n    handle, indices, value, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayScatterV3": "tf.raw_ops.TensorArrayScatterV3(\n    handle, indices, value, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArraySize": "tf.raw_ops.TensorArraySize(\n    handle, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArraySizeV2": "tf.raw_ops.TensorArraySizeV2(\n    handle, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArraySizeV3": "tf.raw_ops.TensorArraySizeV3(\n    handle, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArraySplit": "tf.raw_ops.TensorArraySplit(\n    handle, value, lengths, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArraySplitV2": "tf.raw_ops.TensorArraySplitV2(\n    handle, value, lengths, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArraySplitV3": "tf.raw_ops.TensorArraySplitV3(\n    handle, value, lengths, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayUnpack": "tf.raw_ops.TensorArrayUnpack(\n    handle, value, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayV2": "tf.raw_ops.TensorArrayV2(\n    size,\n    dtype,\n    element_shape=None,\n    dynamic_size=False,\n    clear_after_read=True,\n    tensor_array_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayV3": "tf.raw_ops.TensorArrayV3(\n    size,\n    dtype,\n    element_shape=None,\n    dynamic_size=False,\n    clear_after_read=True,\n    identical_element_shapes=False,\n    tensor_array_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayWrite": "tf.raw_ops.TensorArrayWrite(\n    handle, index, value, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayWriteV2": "tf.raw_ops.TensorArrayWriteV2(\n    handle, index, value, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayWriteV3": "tf.raw_ops.TensorArrayWriteV3(\n    handle, index, value, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorDataset": "tf.raw_ops.TensorDataset(\n    components, output_shapes, metadata='', name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListConcat": "tf.raw_ops.TensorListConcat(\n    input_handle, element_dtype, element_shape=None, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListConcatLists": "tf.raw_ops.TensorListConcatLists(\n    input_a, input_b, element_dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListConcatV2": "tf.raw_ops.TensorListConcatV2(\n    input_handle, element_shape, leading_dims, element_dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListElementShape": "tf.raw_ops.TensorListElementShape(\n    input_handle, shape_type, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListFromTensor": "tf.raw_ops.TensorListFromTensor(\n    tensor, element_shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListGather": "tf.raw_ops.TensorListGather(\n    input_handle, indices, element_shape, element_dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListGetItem": "tf.raw_ops.TensorListGetItem(\n    input_handle, index, element_shape, element_dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListLength": "tf.raw_ops.TensorListLength(\n    input_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListPopBack": "tf.raw_ops.TensorListPopBack(\n    input_handle, element_shape, element_dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListPushBack": "tf.raw_ops.TensorListPushBack(\n    input_handle, tensor, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListPushBackBatch": "tf.raw_ops.TensorListPushBackBatch(\n    input_handles, tensor, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListReserve": "tf.raw_ops.TensorListReserve(\n    element_shape, num_elements, element_dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListResize": "tf.raw_ops.TensorListResize(\n    input_handle, size, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListScatter": "tf.raw_ops.TensorListScatter(\n    tensor, indices, element_shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListScatterIntoExistingList": "tf.raw_ops.TensorListScatterIntoExistingList(\n    input_handle, tensor, indices, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListScatterV2": "tf.raw_ops.TensorListScatterV2(\n    tensor, indices, element_shape, num_elements, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListSetItem": "tf.raw_ops.TensorListSetItem(\n    input_handle, index, item, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListSplit": "tf.raw_ops.TensorListSplit(\n    tensor, element_shape, lengths, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListStack": "tf.raw_ops.TensorListStack(\n    input_handle, element_shape, element_dtype, num_elements=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorScatterAdd": "tf.raw_ops.TensorScatterAdd(\n    tensor, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorScatterMax": "tf.raw_ops.TensorScatterMax(\n    tensor, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorScatterMin": "tf.raw_ops.TensorScatterMin(\n    tensor, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorScatterSub": "tf.raw_ops.TensorScatterSub(\n    tensor, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorScatterUpdate": "tf.raw_ops.TensorScatterUpdate(\n    tensor, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorSliceDataset": "tf.raw_ops.TensorSliceDataset(\n    components,\n    output_shapes,\n    is_files=False,\n    metadata='',\n    replicate_on_split=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorStridedSliceUpdate": "tf.raw_ops.TensorStridedSliceUpdate(\n    input,\n    begin,\n    end,\n    strides,\n    value,\n    begin_mask=0,\n    end_mask=0,\n    ellipsis_mask=0,\n    new_axis_mask=0,\n    shrink_axis_mask=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorSummary": "tf.raw_ops.TensorSummary(\n    tensor,\n    description='',\n    labels=[],\n    display_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorSummaryV2": "tf.raw_ops.TensorSummaryV2(\n    tag, tensor, serialized_summary_metadata, name=None\n)\n"
  },
  {
    "tf.raw_ops.TextLineDataset": "tf.raw_ops.TextLineDataset(\n    filenames, compression_type, buffer_size, metadata='', name=None\n)\n"
  },
  {
    "tf.raw_ops.TextLineReader": "tf.raw_ops.TextLineReader(\n    skip_header_lines=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TextLineReaderV2": "tf.raw_ops.TextLineReaderV2(\n    skip_header_lines=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ThreadPoolDataset": "tf.raw_ops.ThreadPoolDataset(\n    input_dataset, thread_pool, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ThreadPoolHandle": "tf.raw_ops.ThreadPoolHandle(\n    num_threads,\n    display_name,\n    max_intra_op_parallelism=1,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ThreadUnsafeUnigramCandidateSampler": "tf.raw_ops.ThreadUnsafeUnigramCandidateSampler(\n    true_classes,\n    num_true,\n    num_sampled,\n    unique,\n    range_max,\n    seed=0,\n    seed2=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Tile": "tf.raw_ops.Tile(\n    input, multiples, name=None\n)\n"
  },
  {
    "tf.raw_ops.TileGrad": "tf.raw_ops.TileGrad(\n    input, multiples, name=None\n)\n"
  },
  {
    "tf.raw_ops.Timestamp": "tf.raw_ops.Timestamp(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ToBool": "tf.raw_ops.ToBool(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.TopK": "tf.raw_ops.TopK(\n    input, k, sorted=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.TopKV2": "tf.raw_ops.TopKV2(\n    input, k, sorted=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.Transpose": "tf.raw_ops.Transpose(\n    x, perm, name=None\n)\n"
  },
  {
    "tf.raw_ops.TridiagonalMatMul": "tf.raw_ops.TridiagonalMatMul(\n    superdiag, maindiag, subdiag, rhs, name=None\n)\n"
  },
  {
    "tf.raw_ops.TridiagonalSolve": "tf.raw_ops.TridiagonalSolve(\n    diagonals, rhs, partial_pivoting=True, perturb_singular=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.TruncateDiv": "tf.raw_ops.TruncateDiv(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.TruncateMod": "tf.raw_ops.TruncateMod(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.TruncatedNormal": "tf.raw_ops.TruncatedNormal(\n    shape, dtype, seed=0, seed2=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.Unbatch": "tf.raw_ops.Unbatch(\n    batched_tensor,\n    batch_index,\n    id,\n    timeout_micros,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UnbatchDataset": "tf.raw_ops.UnbatchDataset(\n    input_dataset,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UnbatchGrad": "tf.raw_ops.UnbatchGrad(\n    original_input,\n    batch_index,\n    grad,\n    id,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UncompressElement": "tf.raw_ops.UncompressElement(\n    compressed, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.UnicodeDecode": "tf.raw_ops.UnicodeDecode(\n    input,\n    input_encoding,\n    errors='replace',\n    replacement_char=65533,\n    replace_control_characters=False,\n    Tsplits=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UnicodeDecodeWithOffsets": "tf.raw_ops.UnicodeDecodeWithOffsets(\n    input,\n    input_encoding,\n    errors='replace',\n    replacement_char=65533,\n    replace_control_characters=False,\n    Tsplits=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UnicodeEncode": "tf.raw_ops.UnicodeEncode(\n    input_values,\n    input_splits,\n    output_encoding,\n    errors='replace',\n    replacement_char=65533,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UnicodeScript": "tf.raw_ops.UnicodeScript(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.UnicodeTranscode": "tf.raw_ops.UnicodeTranscode(\n    input,\n    input_encoding,\n    output_encoding,\n    errors='replace',\n    replacement_char=65533,\n    replace_control_characters=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UniformCandidateSampler": "tf.raw_ops.UniformCandidateSampler(\n    true_classes,\n    num_true,\n    num_sampled,\n    unique,\n    range_max,\n    seed=0,\n    seed2=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UniformDequantize": "tf.raw_ops.UniformDequantize(\n    input,\n    scales,\n    zero_points,\n    Tout,\n    quantization_min_val,\n    quantization_max_val,\n    quantization_axis=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UniformQuantize": "tf.raw_ops.UniformQuantize(\n    input,\n    scales,\n    zero_points,\n    Tout,\n    quantization_min_val,\n    quantization_max_val,\n    quantization_axis=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UniformQuantizedClipByValue": "tf.raw_ops.UniformQuantizedClipByValue(\n    operand,\n    min,\n    max,\n    scales,\n    zero_points,\n    quantization_min_val,\n    quantization_max_val,\n    quantization_axis=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UniformQuantizedDot": "tf.raw_ops.UniformQuantizedDot(\n    lhs,\n    rhs,\n    lhs_scales,\n    lhs_zero_points,\n    rhs_scales,\n    rhs_zero_points,\n    output_scales,\n    output_zero_points,\n    Tout,\n    lhs_quantization_min_val,\n    lhs_quantization_max_val,\n    rhs_quantization_min_val,\n    rhs_quantization_max_val,\n    output_quantization_min_val,\n    output_quantization_max_val,\n    lhs_quantization_axis=-1,\n    rhs_quantization_axis=-1,\n    output_quantization_axis=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UniformQuantizedDotHybrid": "tf.raw_ops.UniformQuantizedDotHybrid(\n    lhs,\n    rhs,\n    rhs_scales,\n    rhs_zero_points,\n    Tout,\n    rhs_quantization_min_val,\n    rhs_quantization_max_val,\n    rhs_quantization_axis=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UniformRequantize": "tf.raw_ops.UniformRequantize(\n    input,\n    input_scales,\n    input_zero_points,\n    output_scales,\n    output_zero_points,\n    Tout,\n    input_quantization_min_val,\n    input_quantization_max_val,\n    output_quantization_min_val,\n    output_quantization_max_val,\n    input_quantization_axis=-1,\n    output_quantization_axis=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Unique": "tf.raw_ops.Unique(\n    x,\n    out_idx=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UniqueDataset": "tf.raw_ops.UniqueDataset(\n    input_dataset,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UniqueV2": "tf.raw_ops.UniqueV2(\n    x,\n    axis,\n    out_idx=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UniqueWithCounts": "tf.raw_ops.UniqueWithCounts(\n    x,\n    out_idx=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UniqueWithCountsV2": "tf.raw_ops.UniqueWithCountsV2(\n    x,\n    axis,\n    out_idx=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Unpack": "tf.raw_ops.Unpack(\n    value, num, axis=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.UnravelIndex": "tf.raw_ops.UnravelIndex(\n    indices, dims, name=None\n)\n"
  },
  {
    "tf.raw_ops.UnsortedSegmentJoin": "tf.raw_ops.UnsortedSegmentJoin(\n    inputs, segment_ids, num_segments, separator='', name=None\n)\n"
  },
  {
    "tf.raw_ops.UnsortedSegmentMax": "tf.raw_ops.UnsortedSegmentMax(\n    data, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.raw_ops.UnsortedSegmentMin": "tf.raw_ops.UnsortedSegmentMin(\n    data, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.raw_ops.UnsortedSegmentProd": "tf.raw_ops.UnsortedSegmentProd(\n    data, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.raw_ops.UnsortedSegmentSum": "tf.raw_ops.UnsortedSegmentSum(\n    data, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.raw_ops.Unstage": "tf.raw_ops.Unstage(\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UnwrapDatasetVariant": "tf.raw_ops.UnwrapDatasetVariant(\n    input_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.UpperBound": "tf.raw_ops.UpperBound(\n    sorted_inputs,\n    values,\n    out_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.VarHandleOp": "tf.raw_ops.VarHandleOp(\n    dtype,\n    shape,\n    container='',\n    shared_name='',\n    allowed_devices=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.VarIsInitializedOp": "tf.raw_ops.VarIsInitializedOp(\n    resource, name=None\n)\n"
  },
  {
    "tf.raw_ops.Variable": "tf.raw_ops.Variable(\n    shape, dtype, container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.VariableShape": "tf.raw_ops.VariableShape(\n    input,\n    out_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.VariableV2": "tf.raw_ops.VariableV2(\n    shape, dtype, container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.Where": "tf.raw_ops.Where(\n    condition, name=None\n)\n"
  },
  {
    "tf.raw_ops.While": "tf.raw_ops.While(\n    input, cond, body, output_shapes=[], parallel_iterations=10, name=None\n)\n"
  },
  {
    "tf.raw_ops.WholeFileReader": "tf.raw_ops.WholeFileReader(\n    container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.WholeFileReaderV2": "tf.raw_ops.WholeFileReaderV2(\n    container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.WindowDataset": "tf.raw_ops.WindowDataset(\n    input_dataset,\n    size,\n    shift,\n    stride,\n    drop_remainder,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.WindowOp": "tf.raw_ops.WindowOp(\n    inputs, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.WorkerHeartbeat": "tf.raw_ops.WorkerHeartbeat(\n    request, name=None\n)\n"
  },
  {
    "tf.raw_ops.WrapDatasetVariant": "tf.raw_ops.WrapDatasetVariant(\n    input_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.WriteAudioSummary": "tf.raw_ops.WriteAudioSummary(\n    writer, step, tag, tensor, sample_rate, max_outputs=3, name=None\n)\n"
  },
  {
    "tf.raw_ops.WriteFile": "tf.raw_ops.WriteFile(\n    filename, contents, name=None\n)\n"
  },
  {
    "tf.raw_ops.WriteGraphSummary": "tf.raw_ops.WriteGraphSummary(\n    writer, step, tensor, name=None\n)\n"
  },
  {
    "tf.raw_ops.WriteHistogramSummary": "tf.raw_ops.WriteHistogramSummary(\n    writer, step, tag, values, name=None\n)\n"
  },
  {
    "tf.raw_ops.WriteImageSummary": "tf.raw_ops.WriteImageSummary(\n    writer, step, tag, tensor, bad_color, max_images=3, name=None\n)\n"
  },
  {
    "tf.raw_ops.WriteRawProtoSummary": "tf.raw_ops.WriteRawProtoSummary(\n    writer, step, tensor, name=None\n)\n"
  },
  {
    "tf.raw_ops.WriteScalarSummary": "tf.raw_ops.WriteScalarSummary(\n    writer, step, tag, value, name=None\n)\n"
  },
  {
    "tf.raw_ops.WriteSummary": "tf.raw_ops.WriteSummary(\n    writer, step, tensor, tag, summary_metadata, name=None\n)\n"
  },
  {
    "tf.raw_ops.Xdivy": "tf.raw_ops.Xdivy(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.XlaConcatND": "tf.raw_ops.XlaConcatND(\n    inputs, num_concats, paddings=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.XlaSplitND": "tf.raw_ops.XlaSplitND(\n    input, N, num_splits, paddings=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.Xlog1py": "tf.raw_ops.Xlog1py(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.Xlogy": "tf.raw_ops.Xlogy(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.ZerosLike": "tf.raw_ops.ZerosLike(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Zeta": "tf.raw_ops.Zeta(\n    x, q, name=None\n)\n"
  },
  {
    "tf.raw_ops.ZipDataset": "tf.raw_ops.ZipDataset(\n    input_datasets,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.realdiv": "tf.realdiv(\n    x, y, name=None\n)\n"
  },
  {
    "tf.recompute_grad": "tf.recompute_grad(\n    f\n)\n"
  },
  {
    "tf.math.reduce_all": "tf.math.reduce_all(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_any": "tf.math.reduce_any(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_logsumexp": "tf.math.reduce_logsumexp(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_max": "tf.math.reduce_max(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_mean": "tf.math.reduce_mean(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_min": "tf.math.reduce_min(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_prod": "tf.math.reduce_prod(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_sum": "tf.math.reduce_sum(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.register_tensor_conversion_function": "tf.register_tensor_conversion_function(\n    base_type, conversion_func, priority=100\n)\n"
  },
  {
    "tf.repeat": "tf.repeat(\n    input, repeats, axis=None, name=None\n)\n"
  },
  {
    "tf.required_space_to_batch_paddings": "tf.required_space_to_batch_paddings(\n    input_shape, block_shape, base_paddings=None, name=None\n)\n"
  },
  {
    "tf.reshape": "tf.reshape(\n    tensor, shape, name=None\n)\n"
  },
  {
    "tf.reverse": "tf.reverse(\n    tensor, axis, name=None\n)\n"
  },
  {
    "tf.reverse_sequence": "tf.reverse_sequence(\n    input, seq_lengths, seq_axis=None, batch_axis=None, name=None\n)\n"
  },
  {
    "tf.roll": "tf.roll(\n    input, shift, axis, name=None\n)\n"
  },
  {
    "tf.math.round": "tf.math.round(\n    x, name=None\n)\n"
  },
  {
    "tf.dtypes.saturate_cast": "tf.dtypes.saturate_cast(\n    value, dtype, name=None\n)\n"
  },
  {
    "tf.saved_model.Asset": "tf.saved_model.Asset(\n    path\n)\n"
  },
  {
    "tf.saved_model.LoadOptions": "tf.saved_model.LoadOptions(\n    allow_partial_checkpoint=False,\n    experimental_io_device=None,\n    experimental_skip_checkpoint=False,\n    experimental_variable_policy=None\n)\n"
  },
  {
    "tf.saved_model.SaveOptions": "tf.saved_model.SaveOptions(\n    namespace_whitelist=None,\n    save_debug_info=False,\n    function_aliases=None,\n    experimental_io_device=None,\n    experimental_variable_policy=None,\n    experimental_custom_gradients=True\n)\n"
  },
  {
    "tf.saved_model.contains_saved_model": "tf.saved_model.contains_saved_model(\n    export_dir\n)\n"
  },
  {
    "tf.saved_model.experimental.TrackableResource": "tf.saved_model.experimental.TrackableResource(\n    device=''\n)\n"
  },
  {
    "tf.saved_model.load": "tf.saved_model.load(\n    export_dir, tags=None, options=None\n)\n"
  },
  {
    "tf.saved_model.save": "tf.saved_model.save(\n    obj, export_dir, signatures=None, options=None\n)\n"
  },
  {
    "tf.math.scalar_mul": "tf.math.scalar_mul(\n    scalar, x, name=None\n)\n"
  },
  {
    "tf.scan": "tf.scan(\n    fn,\n    elems,\n    initializer=None,\n    parallel_iterations=10,\n    back_prop=True,\n    swap_memory=False,\n    infer_shape=True,\n    reverse=False,\n    name=None\n)\n"
  },
  {
    "tf.scatter_nd": "tf.scatter_nd(\n    indices, updates, shape, name=None\n)\n"
  },
  {
    "tf.searchsorted": "tf.searchsorted(\n    sorted_sequence,\n    values,\n    side='left',\n    out_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.sequence_mask": "tf.sequence_mask(\n    lengths,\n    maxlen=None,\n    dtype=tf.dtypes.bool,\n    name=None\n)\n"
  },
  {
    "tf.sets.difference": "tf.sets.difference(\n    a, b, aminusb=True, validate_indices=True\n)\n"
  },
  {
    "tf.sets.intersection": "tf.sets.intersection(\n    a, b, validate_indices=True\n)\n"
  },
  {
    "tf.sets.size": "tf.sets.size(\n    a, validate_indices=True\n)\n"
  },
  {
    "tf.sets.union": "tf.sets.union(\n    a, b, validate_indices=True\n)\n"
  },
  {
    "tf.shape": "tf.shape(\n    input,\n    out_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.shape_n": "tf.shape_n(\n    input,\n    out_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.math.sigmoid": "tf.math.sigmoid(\n    x, name=None\n)\n"
  },
  {
    "tf.math.sign": "tf.math.sign(\n    x, name=None\n)\n"
  },
  {
    "tf.signal.dct": "tf.signal.dct(\n    input, type=2, n=None, axis=-1, norm=None, name=None\n)\n"
  },
  {
    "tf.signal.fft": "tf.signal.fft(\n    input, name=None\n)\n"
  },
  {
    "tf.signal.fft2d": "tf.signal.fft2d(\n    input, name=None\n)\n"
  },
  {
    "tf.signal.fft3d": "tf.signal.fft3d(\n    input, name=None\n)\n"
  },
  {
    "tf.signal.fftshift": "tf.signal.fftshift(\n    x, axes=None, name=None\n)\n"
  },
  {
    "tf.signal.frame": "tf.signal.frame(\n    signal,\n    frame_length,\n    frame_step,\n    pad_end=False,\n    pad_value=0,\n    axis=-1,\n    name=None\n)\n"
  },
  {
    "tf.signal.hamming_window": "tf.signal.hamming_window(\n    window_length,\n    periodic=True,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.signal.hann_window": "tf.signal.hann_window(\n    window_length,\n    periodic=True,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.signal.idct": "tf.signal.idct(\n    input, type=2, n=None, axis=-1, norm=None, name=None\n)\n"
  },
  {
    "tf.signal.ifft": "tf.signal.ifft(\n    input, name=None\n)\n"
  },
  {
    "tf.signal.ifft2d": "tf.signal.ifft2d(\n    input, name=None\n)\n"
  },
  {
    "tf.signal.ifft3d": "tf.signal.ifft3d(\n    input, name=None\n)\n"
  },
  {
    "tf.signal.ifftshift": "tf.signal.ifftshift(\n    x, axes=None, name=None\n)\n"
  },
  {
    "tf.signal.inverse_mdct": "tf.signal.inverse_mdct(\n    mdcts,\n    window_fn=tf.signal.vorbis_window,\n    norm=None,\n    name=None\n)\n"
  },
  {
    "tf.signal.inverse_stft": "tf.signal.inverse_stft(\n    stfts,\n    frame_length,\n    frame_step,\n    fft_length=None,\n    window_fn=tf.signal.hann_window,\n    name=None\n)\n"
  },
  {
    "tf.signal.inverse_stft_window_fn": "tf.signal.inverse_stft_window_fn(\n    frame_step,\n    forward_window_fn=tf.signal.hann_window,\n    name=None\n)\n"
  },
  {
    "tf.signal.irfft": "tf.signal.irfft(\n    input_tensor, fft_length=None, name=None\n)\n"
  },
  {
    "tf.signal.irfft2d": "tf.signal.irfft2d(\n    input_tensor, fft_length=None, name=None\n)\n"
  },
  {
    "tf.signal.irfft3d": "tf.signal.irfft3d(\n    input_tensor, fft_length=None, name=None\n)\n"
  },
  {
    "tf.signal.kaiser_bessel_derived_window": "tf.signal.kaiser_bessel_derived_window(\n    window_length,\n    beta=12.0,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.signal.kaiser_window": "tf.signal.kaiser_window(\n    window_length,\n    beta=12.0,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.signal.linear_to_mel_weight_matrix": "tf.signal.linear_to_mel_weight_matrix(\n    num_mel_bins=20,\n    num_spectrogram_bins=129,\n    sample_rate=8000,\n    lower_edge_hertz=125.0,\n    upper_edge_hertz=3800.0,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.signal.mdct": "tf.signal.mdct(\n    signals,\n    frame_length,\n    window_fn=tf.signal.vorbis_window,\n    pad_end=False,\n    norm=None,\n    name=None\n)\n"
  },
  {
    "tf.signal.mfccs_from_log_mel_spectrograms": "tf.signal.mfccs_from_log_mel_spectrograms(\n    log_mel_spectrograms, name=None\n)\n"
  },
  {
    "tf.signal.overlap_and_add": "tf.signal.overlap_and_add(\n    signal, frame_step, name=None\n)\n"
  },
  {
    "tf.signal.rfft": "tf.signal.rfft(\n    input_tensor, fft_length=None, name=None\n)\n"
  },
  {
    "tf.signal.rfft2d": "tf.signal.rfft2d(\n    input_tensor, fft_length=None, name=None\n)\n"
  },
  {
    "tf.signal.rfft3d": "tf.signal.rfft3d(\n    input_tensor, fft_length=None, name=None\n)\n"
  },
  {
    "tf.signal.stft": "tf.signal.stft(\n    signals,\n    frame_length,\n    frame_step,\n    fft_length=None,\n    window_fn=tf.signal.hann_window,\n    pad_end=False,\n    name=None\n)\n"
  },
  {
    "tf.signal.vorbis_window": "tf.signal.vorbis_window(\n    window_length,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.math.sin": "tf.math.sin(\n    x, name=None\n)\n"
  },
  {
    "tf.math.sinh": "tf.math.sinh(\n    x, name=None\n)\n"
  },
  {
    "tf.size": "tf.size(\n    input,\n    out_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.slice": "tf.slice(\n    input_, begin, size, name=None\n)\n"
  },
  {
    "tf.sort": "tf.sort(\n    values, axis=-1, direction='ASCENDING', name=None\n)\n"
  },
  {
    "tf.space_to_batch": "tf.space_to_batch(\n    input, block_shape, paddings, name=None\n)\n"
  },
  {
    "tf.space_to_batch_nd": "tf.space_to_batch_nd(\n    input, block_shape, paddings, name=None\n)\n"
  },
  {
    "tf.sparse.SparseTensor": "tf.sparse.SparseTensor(\n    indices, values, dense_shape\n)\n"
  },
  {
    "tf.sparse.add": "tf.sparse.add(\n    a, b, threshold=0\n)\n"
  },
  {
    "tf.sparse.bincount": "tf.sparse.bincount(\n    values,\n    weights=None,\n    axis=0,\n    minlength=None,\n    maxlength=None,\n    binary_output=False,\n    name=None\n)\n"
  },
  {
    "tf.sparse.concat": "tf.sparse.concat(\n    axis, sp_inputs, expand_nonconcat_dims=False, name=None\n)\n"
  },
  {
    "tf.sparse.cross": "tf.sparse.cross(\n    inputs, name=None, separator=None\n)\n"
  },
  {
    "tf.sparse.cross_hashed": "tf.sparse.cross_hashed(\n    inputs, num_buckets=0, hash_key=None, name=None\n)\n"
  },
  {
    "tf.sparse.expand_dims": "tf.sparse.expand_dims(\n    sp_input, axis=None, name=None\n)\n"
  },
  {
    "tf.sparse.eye": "tf.sparse.eye(\n    num_rows,\n    num_columns=None,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.sparse.fill_empty_rows": "tf.sparse.fill_empty_rows(\n    sp_input, default_value, name=None\n)\n"
  },
  {
    "tf.sparse.from_dense": "tf.sparse.from_dense(\n    tensor, name=None\n)\n"
  },
  {
    "tf.sparse.map_values": "tf.sparse.map_values(\n    op, *args, **kwargs\n)\n"
  },
  {
    "tf.sparse.mask": "tf.sparse.mask(\n    a, mask_indices, name=None\n)\n"
  },
  {
    "tf.sparse.maximum": "tf.sparse.maximum(\n    sp_a, sp_b, name=None\n)\n"
  },
  {
    "tf.sparse.minimum": "tf.sparse.minimum(\n    sp_a, sp_b, name=None\n)\n"
  },
  {
    "tf.sparse.reduce_max": "tf.sparse.reduce_max(\n    sp_input, axis=None, keepdims=None, output_is_sparse=False, name=None\n)\n"
  },
  {
    "tf.sparse.reduce_sum": "tf.sparse.reduce_sum(\n    sp_input, axis=None, keepdims=None, output_is_sparse=False, name=None\n)\n"
  },
  {
    "tf.sparse.reorder": "tf.sparse.reorder(\n    sp_input, name=None\n)\n"
  },
  {
    "tf.sparse.reset_shape": "tf.sparse.reset_shape(\n    sp_input, new_shape=None\n)\n"
  },
  {
    "tf.sparse.reshape": "tf.sparse.reshape(\n    sp_input, shape, name=None\n)\n"
  },
  {
    "tf.sparse.retain": "tf.sparse.retain(\n    sp_input, to_retain\n)\n"
  },
  {
    "tf.sparse.segment_mean": "tf.sparse.segment_mean(\n    data, indices, segment_ids, num_segments=None, name=None\n)\n"
  },
  {
    "tf.sparse.segment_sqrt_n": "tf.sparse.segment_sqrt_n(\n    data, indices, segment_ids, num_segments=None, name=None\n)\n"
  },
  {
    "tf.sparse.segment_sum": "tf.sparse.segment_sum(\n    data, indices, segment_ids, num_segments=None, name=None\n)\n"
  },
  {
    "tf.sparse.slice": "tf.sparse.slice(\n    sp_input, start, size, name=None\n)\n"
  },
  {
    "tf.sparse.softmax": "tf.sparse.softmax(\n    sp_input, name=None\n)\n"
  },
  {
    "tf.sparse.sparse_dense_matmul": "tf.sparse.sparse_dense_matmul(\n    sp_a, b, adjoint_a=False, adjoint_b=False, name=None\n)\n"
  },
  {
    "tf.sparse.split": "tf.sparse.split(\n    sp_input=None, num_split=None, axis=None, name=None\n)\n"
  },
  {
    "tf.sparse.to_dense": "tf.sparse.to_dense(\n    sp_input, default_value=None, validate_indices=True, name=None\n)\n"
  },
  {
    "tf.sparse.to_indicator": "tf.sparse.to_indicator(\n    sp_input, vocab_size, name=None\n)\n"
  },
  {
    "tf.sparse.transpose": "tf.sparse.transpose(\n    sp_input, perm=None, name=None\n)\n"
  },
  {
    "tf.split": "tf.split(\n    value, num_or_size_splits, axis=0, num=None, name='split'\n)\n"
  },
  {
    "tf.math.sqrt": "tf.math.sqrt(\n    x, name=None\n)\n"
  },
  {
    "tf.math.square": "tf.math.square(\n    x, name=None\n)\n"
  },
  {
    "tf.squeeze": "tf.squeeze(\n    input, axis=None, name=None\n)\n"
  },
  {
    "tf.stack": "tf.stack(\n    values, axis=0, name='stack'\n)\n"
  },
  {
    "tf.stop_gradient": "tf.stop_gradient(\n    input, name=None\n)\n"
  },
  {
    "tf.strided_slice": "tf.strided_slice(\n    input_,\n    begin,\n    end,\n    strides=None,\n    begin_mask=0,\n    end_mask=0,\n    ellipsis_mask=0,\n    new_axis_mask=0,\n    shrink_axis_mask=0,\n    var=None,\n    name=None\n)\n"
  },
  {
    "tf.strings.as_string": "tf.strings.as_string(\n    input,\n    precision=-1,\n    scientific=False,\n    shortest=False,\n    width=-1,\n    fill='',\n    name=None\n)\n"
  },
  {
    "tf.strings.bytes_split": "tf.strings.bytes_split(\n    input, name=None\n)\n"
  },
  {
    "tf.strings.format": "tf.strings.format(\n    template, inputs, placeholder='{}', summarize=3, name=None\n)\n"
  },
  {
    "tf.strings.join": "tf.strings.join(\n    inputs, separator='', name=None\n)\n"
  },
  {
    "tf.strings.length": "tf.strings.length(\n    input, unit='BYTE', name=None\n)\n"
  },
  {
    "tf.strings.lower": "tf.strings.lower(\n    input, encoding='', name=None\n)\n"
  },
  {
    "tf.strings.ngrams": "tf.strings.ngrams(\n    data,\n    ngram_width,\n    separator=' ',\n    pad_values=None,\n    padding_width=None,\n    preserve_short_sequences=False,\n    name=None\n)\n"
  },
  {
    "tf.strings.reduce_join": "tf.strings.reduce_join(\n    inputs, axis=None, keepdims=False, separator='', name=None\n)\n"
  },
  {
    "tf.strings.regex_full_match": "tf.strings.regex_full_match(\n    input, pattern, name=None\n)\n"
  },
  {
    "tf.strings.regex_replace": "tf.strings.regex_replace(\n    input, pattern, rewrite, replace_global=True, name=None\n)\n"
  },
  {
    "tf.strings.split": "tf.strings.split(\n    input, sep=None, maxsplit=-1, name=None\n)\n"
  },
  {
    "tf.strings.strip": "tf.strings.strip(\n    input, name=None\n)\n"
  },
  {
    "tf.strings.substr": "tf.strings.substr(\n    input, pos, len, unit='BYTE', name=None\n)\n"
  },
  {
    "tf.strings.to_hash_bucket": "tf.strings.to_hash_bucket(\n    input, num_buckets, name=None\n)\n"
  },
  {
    "tf.strings.to_hash_bucket_fast": "tf.strings.to_hash_bucket_fast(\n    input, num_buckets, name=None\n)\n"
  },
  {
    "tf.strings.to_hash_bucket_strong": "tf.strings.to_hash_bucket_strong(\n    input, num_buckets, key, name=None\n)\n"
  },
  {
    "tf.strings.to_number": "tf.strings.to_number(\n    input,\n    out_type=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.strings.unicode_decode": "tf.strings.unicode_decode(\n    input,\n    input_encoding,\n    errors='replace',\n    replacement_char=65533,\n    replace_control_characters=False,\n    name=None\n)\n"
  },
  {
    "tf.strings.unicode_decode_with_offsets": "tf.strings.unicode_decode_with_offsets(\n    input,\n    input_encoding,\n    errors='replace',\n    replacement_char=65533,\n    replace_control_characters=False,\n    name=None\n)\n"
  },
  {
    "tf.strings.unicode_encode": "tf.strings.unicode_encode(\n    input,\n    output_encoding,\n    errors='replace',\n    replacement_char=65533,\n    name=None\n)\n"
  },
  {
    "tf.strings.unicode_script": "tf.strings.unicode_script(\n    input, name=None\n)\n"
  },
  {
    "tf.strings.unicode_split": "tf.strings.unicode_split(\n    input,\n    input_encoding,\n    errors='replace',\n    replacement_char=65533,\n    name=None\n)\n"
  },
  {
    "tf.strings.unicode_split_with_offsets": "tf.strings.unicode_split_with_offsets(\n    input,\n    input_encoding,\n    errors='replace',\n    replacement_char=65533,\n    name=None\n)\n"
  },
  {
    "tf.strings.unicode_transcode": "tf.strings.unicode_transcode(\n    input,\n    input_encoding,\n    output_encoding,\n    errors='replace',\n    replacement_char=65533,\n    replace_control_characters=False,\n    name=None\n)\n"
  },
  {
    "tf.strings.unsorted_segment_join": "tf.strings.unsorted_segment_join(\n    inputs, segment_ids, num_segments, separator='', name=None\n)\n"
  },
  {
    "tf.strings.upper": "tf.strings.upper(\n    input, encoding='', name=None\n)\n"
  },
  {
    "tf.math.subtract": "tf.math.subtract(\n    x, y, name=None\n)\n"
  },
  {
    "tf.summary.audio": "tf.summary.audio(\n    name,\n    data,\n    sample_rate,\n    step=None,\n    max_outputs=3,\n    encoding=None,\n    description=None\n)\n"
  },
  {
    "tf.summary.create_file_writer": "tf.summary.create_file_writer(\n    logdir,\n    max_queue=None,\n    flush_millis=None,\n    filename_suffix=None,\n    name=None,\n    experimental_trackable=False\n)\n"
  },
  {
    "tf.summary.flush": "tf.summary.flush(\n    writer=None, name=None\n)\n"
  },
  {
    "tf.summary.graph": "tf.summary.graph(\n    graph_data\n)\n"
  },
  {
    "tf.summary.histogram": "tf.summary.histogram(\n    name, data, step=None, buckets=None, description=None\n)\n"
  },
  {
    "tf.summary.image": "tf.summary.image(\n    name, data, step=None, max_outputs=3, description=None\n)\n"
  },
  {
    "tf.summary.scalar": "tf.summary.scalar(\n    name, data, step=None, description=None\n)\n"
  },
  {
    "tf.summary.text": "tf.summary.text(\n    name, data, step=None, description=None\n)\n"
  },
  {
    "tf.summary.trace_export": "tf.summary.trace_export(\n    name, step=None, profiler_outdir=None\n)\n"
  },
  {
    "tf.summary.trace_on": "tf.summary.trace_on(\n    graph=True, profiler=False\n)\n"
  },
  {
    "tf.summary.write": "tf.summary.write(\n    tag, tensor, step=None, metadata=None, name=None\n)\n"
  },
  {
    "tf.switch_case": "tf.switch_case(\n    branch_index, branch_fns, default=None, name='switch_case'\n)\n"
  },
  {
    "tf.math.tan": "tf.math.tan(\n    x, name=None\n)\n"
  },
  {
    "tf.math.tanh": "tf.math.tanh(\n    x, name=None\n)\n"
  },
  {
    "tf.tensor_scatter_nd_add": "tf.tensor_scatter_nd_add(\n    tensor, indices, updates, name=None\n)\n"
  },
  {
    "tf.tensor_scatter_nd_max": "tf.tensor_scatter_nd_max(\n    tensor, indices, updates, name=None\n)\n"
  },
  {
    "tf.tensor_scatter_nd_min": "tf.tensor_scatter_nd_min(\n    tensor, indices, updates, name=None\n)\n"
  },
  {
    "tf.tensor_scatter_nd_sub": "tf.tensor_scatter_nd_sub(\n    tensor, indices, updates, name=None\n)\n"
  },
  {
    "tf.tensor_scatter_nd_update": "tf.tensor_scatter_nd_update(\n    tensor, indices, updates, name=None\n)\n"
  },
  {
    "tf.tensordot": "tf.tensordot(\n    a, b, axes, name=None\n)\n"
  },
  {
    "tf.test.TestCase": "tf.test.TestCase(\n    methodName='runTest'\n)\n"
  },
  {
    "tf.test.TestCase.failureException": "tf.test.TestCase.failureException(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.test.assert_equal_graph_def": "tf.test.assert_equal_graph_def(\n    expected, actual\n)\n"
  },
  {
    "tf.test.compute_gradient": "tf.test.compute_gradient(\n    f, x, delta=None\n)\n"
  },
  {
    "tf.test.create_local_cluster": "tf.test.create_local_cluster(\n    num_workers,\n    num_ps,\n    protocol='grpc',\n    worker_config=None,\n    ps_config=None\n)\n"
  },
  {
    "tf.test.disable_with_predicate": "tf.test.disable_with_predicate(\n    pred, skip_message\n)\n"
  },
  {
    "tf.test.is_gpu_available": "tf.test.is_gpu_available(\n    cuda_only=False, min_cuda_compute_capability=None\n)\n"
  },
  {
    "tf.test.main": "tf.test.main(\n    argv=None\n)\n"
  },
  {
    "tf.test.with_eager_op_as_function": "tf.test.with_eager_op_as_function(\n    cls=None, only_as_function=False\n)\n"
  },
  {
    "tf.tile": "tf.tile(\n    input, multiples, name=None\n)\n"
  },
  {
    "tf.timestamp": "tf.timestamp(\n    name=None\n)\n"
  },
  {
    "tf.tpu.XLAOptions": "tf.tpu.XLAOptions(\n    use_spmd_for_xla_partitioning=True, enable_xla_dynamic_padder=True\n)\n"
  },
  {
    "tf.tpu.experimental.DeviceAssignment": "tf.tpu.experimental.DeviceAssignment(\n    topology: tf.tpu.experimental.Topology,\n    core_assignment: np.ndarray\n)\n"
  },
  {
    "tf.tpu.experimental.HardwareFeature": "tf.tpu.experimental.HardwareFeature(\n    tpu_hardware_feature_proto\n)\n"
  },
  {
    "tf.tpu.experimental.TPUSystemMetadata": "tf.tpu.experimental.TPUSystemMetadata(\n    num_cores, num_hosts, num_of_cores_per_host, topology, devices\n)\n"
  },
  {
    "tf.tpu.experimental.Topology": "tf.tpu.experimental.Topology(\n    serialized=None, mesh_shape=None, device_coordinates=None\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.Adagrad": "tf.tpu.experimental.embedding.Adagrad(\n    learning_rate: Union[float, Callable[[], float]] = 0.001,\n    initial_accumulator_value: float = 0.1,\n    use_gradient_accumulation: bool = True,\n    clip_weight_min: Optional[float] = None,\n    clip_weight_max: Optional[float] = None,\n    weight_decay_factor: Optional[float] = None,\n    multiply_weight_decay_factor_by_learning_rate: bool = None,\n    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,\n    clipvalue: Optional[ClipValueType] = None\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.AdagradMomentum": "tf.tpu.experimental.embedding.AdagradMomentum(\n    learning_rate: Union[float, Callable[[], float]] = 0.001,\n    momentum: float = 0.0,\n    use_nesterov: bool = False,\n    exponent: float = 2,\n    beta2: float = 1,\n    epsilon: float = 1e-10,\n    use_gradient_accumulation: bool = True,\n    clip_weight_min: Optional[float] = None,\n    clip_weight_max: Optional[float] = None,\n    weight_decay_factor: Optional[float] = None,\n    multiply_weight_decay_factor_by_learning_rate: bool = None,\n    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,\n    clipvalue: Optional[ClipValueType] = None\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.Adam": "tf.tpu.experimental.embedding.Adam(\n    learning_rate: Union[float, Callable[[], float]] = 0.001,\n    beta_1: float = 0.9,\n    beta_2: float = 0.999,\n    epsilon: float = 1e-07,\n    lazy_adam: bool = True,\n    sum_inside_sqrt: bool = True,\n    use_gradient_accumulation: bool = True,\n    clip_weight_min: Optional[float] = None,\n    clip_weight_max: Optional[float] = None,\n    weight_decay_factor: Optional[float] = None,\n    multiply_weight_decay_factor_by_learning_rate: bool = None,\n    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,\n    clipvalue: Optional[ClipValueType] = None\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.FTRL": "tf.tpu.experimental.embedding.FTRL(\n    learning_rate: Union[float, Callable[[], float]] = 0.001,\n    learning_rate_power: float = -0.5,\n    l1_regularization_strength: float = 0.0,\n    l2_regularization_strength: float = 0.0,\n    beta: float = 0.0,\n    initial_accumulator_value: float = 0.1,\n    use_gradient_accumulation: bool = True,\n    clip_weight_min: Optional[float] = None,\n    clip_weight_max: Optional[float] = None,\n    weight_decay_factor: Optional[float] = None,\n    multiply_weight_decay_factor_by_learning_rate: bool = None,\n    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,\n    clipvalue: Optional[ClipValueType] = None,\n    multiply_linear_by_learning_rate: bool = False,\n    allow_zero_accumulator: bool = False\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.FeatureConfig": "tf.tpu.experimental.embedding.FeatureConfig(\n    table: tf.tpu.experimental.embedding.TableConfig,\n    max_sequence_length: int = 0,\n    validate_weights_and_indices: bool = True,\n    output_shape: Optional[Union[List[int], tf.TensorShape]] = None,\n    name: Optional[Text] = None\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.QuantizationConfig": "tf.tpu.experimental.embedding.QuantizationConfig(\n    num_buckets: int, lower: float, upper: float\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.SGD": "tf.tpu.experimental.embedding.SGD(\n    learning_rate: Union[float, Callable[[], float]] = 0.01,\n    use_gradient_accumulation: bool = True,\n    clip_weight_min: Optional[float] = None,\n    clip_weight_max: Optional[float] = None,\n    weight_decay_factor: Optional[float] = None,\n    multiply_weight_decay_factor_by_learning_rate: bool = None,\n    clipvalue: Optional[ClipValueType] = None\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.TPUEmbedding": "tf.tpu.experimental.embedding.TPUEmbedding(\n    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable],\n    optimizer: Optional[tpu_embedding_v2_utils._Optimizer],\n    pipeline_execution_with_tensor_core: bool = False\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.TPUEmbeddingForServing": "tf.tpu.experimental.embedding.TPUEmbeddingForServing(\n    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable],\n    optimizer: Optional[tpu_embedding_v2_utils._Optimizer]\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.TPUEmbeddingV0": "tf.tpu.experimental.embedding.TPUEmbeddingV0(\n    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable],\n    optimizer: Optional[tpu_embedding_v2_utils._Optimizer]\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.TableConfig": "tf.tpu.experimental.embedding.TableConfig(\n    vocabulary_size: int,\n    dim: int,\n    initializer: Optional[Callable[[Any], None]] = None,\n    optimizer: Optional[_Optimizer] = None,\n    combiner: Text = 'mean',\n    name: Optional[Text] = None,\n    quantization_config: tf.tpu.experimental.embedding.QuantizationConfig = None\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.serving_embedding_lookup": "tf.tpu.experimental.embedding.serving_embedding_lookup(\n    inputs: Any,\n    weights: Optional[Any],\n    tables: Dict[tf.tpu.experimental.embedding.TableConfig, tf.Variable],\n    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable]\n) -> Any\n"
  },
  {
    "tf.tpu.experimental.initialize_tpu_system": "tf.tpu.experimental.initialize_tpu_system(\n    cluster_resolver=None\n)\n"
  },
  {
    "tf.tpu.experimental.shutdown_tpu_system": "tf.tpu.experimental.shutdown_tpu_system(\n    cluster_resolver=None\n)\n"
  },
  {
    "tf.io.parse_example": "tf.io.parse_example("
  },
  {
    "tf.train.Checkpoint": "tf.train.Checkpoint(\n    root=None, **kwargs\n)\n"
  },
  {
    "tf.train.CheckpointManager": "tf.train.CheckpointManager(\n    checkpoint,\n    directory,\n    max_to_keep,\n    keep_checkpoint_every_n_hours=None,\n    checkpoint_name='ckpt',\n    step_counter=None,\n    checkpoint_interval=None,\n    init_fn=None\n)\n"
  },
  {
    "tf.train.CheckpointOptions": "tf.train.CheckpointOptions(\n    experimental_io_device=None, experimental_enable_async_checkpoint=False\n)\n"
  },
  {
    "tf.train.CheckpointView": "tf.train.CheckpointView(\n    save_path\n)\n"
  },
  {
    "tf.train.ClusterSpec": "tf.train.ClusterSpec(\n    cluster\n)\n"
  },
  {
    "tf.train.Coordinator": "tf.train.Coordinator(\n    clean_stop_exception_types=None\n)\n"
  },
  {
    "tf.io.parse_example": "tf.io.parse_example("
  },
  {
    "tf.train.ExponentialMovingAverage": "tf.train.ExponentialMovingAverage(\n    decay,\n    num_updates=None,\n    zero_debias=False,\n    name='ExponentialMovingAverage'\n)\n"
  },
  {
    "tf.io.parse_example": "tf.io.parse_example("
  },
  {
    "tf.io.parse_example": "tf.io.parse_example("
  },
  {
    "tf.io.parse_example": "tf.io.parse_example("
  },
  {
    "tf.io.parse_example": "tf.io.parse_example("
  },
  {
    "tf.train.TrackableView": "tf.train.TrackableView(\n    root\n)\n"
  },
  {
    "tf.train.checkpoints_iterator": "tf.train.checkpoints_iterator(\n    checkpoint_dir, min_interval_secs=0, timeout=None, timeout_fn=None\n)\n"
  },
  {
    "tf.train.get_checkpoint_state": "tf.train.get_checkpoint_state(\n    checkpoint_dir, latest_filename=None\n)\n"
  },
  {
    "tf.train.latest_checkpoint": "tf.train.latest_checkpoint(\n    checkpoint_dir, latest_filename=None\n)\n"
  },
  {
    "tf.train.list_variables": "tf.train.list_variables(\n    ckpt_dir_or_file\n)\n"
  },
  {
    "tf.train.load_checkpoint": "tf.train.load_checkpoint(\n    ckpt_dir_or_file\n)\n"
  },
  {
    "tf.train.load_variable": "tf.train.load_variable(\n    ckpt_dir_or_file, name\n)\n"
  },
  {
    "tf.transpose": "tf.transpose(\n    a, perm=None, conjugate=False, name='transpose'\n)\n"
  },
  {
    "tf.math.truediv": "tf.math.truediv(\n    x, y, name=None\n)\n"
  },
  {
    "tf.truncatediv": "tf.truncatediv(\n    x, y, name=None\n)\n"
  },
  {
    "tf.truncatemod": "tf.truncatemod(\n    x, y, name=None\n)\n"
  },
  {
    "tf.tuple": "tf.tuple(\n    tensors, control_inputs=None, name=None\n)\n"
  },
  {
    "tf.type_spec_from_value": "tf.type_spec_from_value(\n    value\n) -> tf.TypeSpec\n"
  },
  {
    "tf.unique": "tf.unique(\n    x,\n    out_idx=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.unique_with_counts": "tf.unique_with_counts(\n    x,\n    out_idx=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.unravel_index": "tf.unravel_index(\n    indices, dims, name=None\n)\n"
  },
  {
    "tf.unstack": "tf.unstack(\n    value, num=None, axis=0, name='unstack'\n)\n"
  },
  {
    "tf.vectorized_map": "tf.vectorized_map(\n    fn, elems, fallback_to_while_loop=True, warn=True\n)\n"
  },
  {
    "tf.where": "tf.where(\n    condition, x=None, y=None, name=None\n)\n"
  },
  {
    "tf.while_loop": "tf.while_loop(\n    cond,\n    body,\n    loop_vars,\n    shape_invariants=None,\n    parallel_iterations=10,\n    back_prop=True,\n    swap_memory=False,\n    maximum_iterations=None,\n    name=None\n)\n"
  },
  {
    "tf.xla.experimental.compile": "tf.xla.experimental.compile(\n    computation, inputs=None\n)\n"
  },
  {
    "tf.zeros": "tf.zeros(\n    shape,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.zeros_like": "tf.zeros_like(\n    input, dtype=None, name=None\n)\n"
  },
  {
    "tf.debugging.Assert": "tf.debugging.Assert(\n    condition, data, summarize=None, name=None\n)\n"
  },
  {
    "tf.CriticalSection": "tf.CriticalSection(\n    name=None, shared_name=None, critical_section_def=None, import_scope=None\n)\n"
  },
  {
    "tf.DeviceSpec": "tf.DeviceSpec(\n    job=None, replica=None, task=None, device_type=None, device_index=None\n)\n"
  },
  {
    "tf.GradientTape": "tf.GradientTape(\n    persistent=False, watch_accessed_variables=True\n)\n"
  },
  {
    "tf.IndexedSlices": "tf.IndexedSlices(\n    values, indices, dense_shape=None\n)\n"
  },
  {
    "tf.IndexedSlicesSpec": "tf.IndexedSlicesSpec(\n    shape=None,\n    dtype=tf.dtypes.float32,\n    indices_dtype=tf.dtypes.int64,\n    dense_shape_dtype=None,\n    indices_shape=None\n)\n"
  },
  {
    "tf.Module": "tf.Module(\n    name=None\n)\n"
  },
  {
    "tf.Operation": "tf.Operation(\n    node_def,\n    g,\n    inputs=None,\n    output_types=None,\n    control_inputs=None,\n    input_types=None,\n    original_op=None,\n    op_def=None\n)\n"
  },
  {
    "tf.OptionalSpec": "tf.OptionalSpec(\n    element_spec\n)\n"
  },
  {
    "tf.ragged.constant": "tf.ragged.constant([[0], [1, 2]]).shape"
  },
  {
    "tf.RaggedTensorSpec": "tf.RaggedTensorSpec(\n    shape=None,\n    dtype=tf.dtypes.float32,\n    ragged_rank=None,\n    row_splits_dtype=tf.dtypes.int64,\n    flat_values_spec=None\n)\n"
  },
  {
    "tf.RegisterGradient": "tf.RegisterGradient(\n    op_type\n)\n"
  },
  {
    "tf.sparse.SparseTensor": "tf.sparse.SparseTensor(\n    indices, values, dense_shape\n)\n"
  },
  {
    "tf.SparseTensorSpec": "tf.SparseTensorSpec(\n    shape=None,\n    dtype=tf.dtypes.float32\n)\n"
  },
  {
    "tf.Tensor": "tf.Tensor(\n    op, value_index, dtype\n)\n"
  },
  {
    "tf.TensorArray": "tf.TensorArray(\n    dtype,\n    size=None,\n    dynamic_size=None,\n    clear_after_read=None,\n    tensor_array_name=None,\n    handle=None,\n    flow=None,\n    infer_shape=True,\n    element_shape=None,\n    colocate_with_first_write_call=True,\n    name=None\n)\n"
  },
  {
    "tf.TensorArraySpec": "tf.TensorArraySpec(\n    element_shape=None,\n    dtype=tf.dtypes.float32,\n    dynamic_size=False,\n    infer_shape=True\n)\n"
  },
  {
    "tf.TensorShape": "tf.TensorShape(\n    dims\n)\n"
  },
  {
    "tf.TensorSpec": "tf.TensorSpec(\n    shape,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.Variable": "tf.Variable(\n    initial_value=None,\n    trainable=None,\n    validate_shape=True,\n    caching_device=None,\n    name=None,\n    variable_def=None,\n    dtype=None,\n    import_scope=None,\n    constraint=None,\n    synchronization=tf.VariableSynchronization.AUTO,\n    aggregation=tf.compat.v1.VariableAggregation.NONE,\n    shape=None,\n    experimental_enable_variable_lifting=True\n)\n"
  },
  {
    "tf.Variable.SaveSliceInfo": "tf.Variable.SaveSliceInfo(\n    full_name=None,\n    full_shape=None,\n    var_offset=None,\n    var_shape=None,\n    save_slice_info_def=None,\n    import_scope=None\n)\n"
  },
  {
    "tf.math.abs": "tf.math.abs(\n    x, name=None\n)\n"
  },
  {
    "tf.math.acos": "tf.math.acos(\n    x, name=None\n)\n"
  },
  {
    "tf.math.acosh": "tf.math.acosh(\n    x, name=None\n)\n"
  },
  {
    "tf.math.add": "tf.math.add(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.add_n": "tf.math.add_n(\n    inputs, name=None\n)\n"
  },
  {
    "tf.approx_top_k": "tf.approx_top_k(\n    input,\n    k,\n    reduction_dimension=-1,\n    recall_target=0.95,\n    is_max_k=True,\n    reduction_input_size_override=-1,\n    aggregate_to_topk=True,\n    name=None\n)\n"
  },
  {
    "tf.math.argmax": "tf.math.argmax(\n    input,\n    axis=None,\n    output_type=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.math.argmin": "tf.math.argmin(\n    input,\n    axis=None,\n    output_type=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.argsort": "tf.argsort(\n    values, axis=-1, direction='ASCENDING', stable=False, name=None\n)\n"
  },
  {
    "tf.dtypes.as_dtype": "tf.dtypes.as_dtype(\n    type_value\n)\n"
  },
  {
    "tf.strings.as_string": "tf.strings.as_string(\n    input,\n    precision=-1,\n    scientific=False,\n    shortest=False,\n    width=-1,\n    fill='',\n    name=None\n)\n"
  },
  {
    "tf.math.asin": "tf.math.asin(\n    x, name=None\n)\n"
  },
  {
    "tf.math.asinh": "tf.math.asinh(\n    x, name=None\n)\n"
  },
  {
    "tf.debugging.assert_equal": "tf.debugging.assert_equal(\n    x, y, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_greater": "tf.debugging.assert_greater(\n    x, y, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_less": "tf.debugging.assert_less(\n    x, y, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_rank": "tf.debugging.assert_rank(\n    x, rank, message=None, name=None\n)\n"
  },
  {
    "tf.math.atan": "tf.math.atan(\n    x, name=None\n)\n"
  },
  {
    "tf.math.atan2": "tf.math.atan2(\n    y, x, name=None\n)\n"
  },
  {
    "tf.math.atanh": "tf.math.atanh(\n    x, name=None\n)\n"
  },
  {
    "tf.audio.decode_wav": "tf.audio.decode_wav(\n    contents, desired_channels=-1, desired_samples=-1, name=None\n)\n"
  },
  {
    "tf.audio.encode_wav": "tf.audio.encode_wav(\n    audio, sample_rate, name=None\n)\n"
  },
  {
    "tf.autodiff.ForwardAccumulator": "tf.autodiff.ForwardAccumulator(\n    primals, tangents\n)\n"
  },
  {
    "tf.GradientTape": "tf.GradientTape(\n    persistent=False, watch_accessed_variables=True\n)\n"
  },
  {
    "tf.autograph.experimental.do_not_convert": "tf.autograph.experimental.do_not_convert(\n    func=None\n)\n"
  },
  {
    "tf.autograph.experimental.set_loop_options": "tf.autograph.experimental.set_loop_options(\n    parallel_iterations=UNSPECIFIED,\n    swap_memory=UNSPECIFIED,\n    maximum_iterations=UNSPECIFIED,\n    shape_invariants=UNSPECIFIED\n)\n"
  },
  {
    "tf.autograph.set_verbosity": "tf.autograph.set_verbosity(\n    level, alsologtostdout=False\n)\n"
  },
  {
    "tf.autograph.to_code": "tf.autograph.to_code(\n    entity, recursive=True, experimental_optional_features=None\n)\n"
  },
  {
    "tf.autograph.to_graph": "tf.autograph.to_graph(\n    entity, recursive=True, experimental_optional_features=None\n)\n"
  },
  {
    "tf.autograph.trace": "tf.autograph.trace(\n    *args\n)\n"
  },
  {
    "tf.batch_to_space": "tf.batch_to_space(\n    input, block_shape, crops, name=None\n)\n"
  },
  {
    "tf.bitcast": "tf.bitcast(\n    input, type, name=None\n)\n"
  },
  {
    "tf.bitwise.bitwise_and": "tf.bitwise.bitwise_and(\n    x, y, name=None\n)\n"
  },
  {
    "tf.bitwise.bitwise_or": "tf.bitwise.bitwise_or(\n    x, y, name=None\n)\n"
  },
  {
    "tf.bitwise.bitwise_xor": "tf.bitwise.bitwise_xor(\n    x, y, name=None\n)\n"
  },
  {
    "tf.bitwise.invert": "tf.bitwise.invert(\n    x, name=None\n)\n"
  },
  {
    "tf.bitwise.left_shift": "tf.bitwise.left_shift(\n    x, y, name=None\n)\n"
  },
  {
    "tf.bitwise.right_shift": "tf.bitwise.right_shift(\n    x, y, name=None\n)\n"
  },
  {
    "tf.boolean_mask": "tf.boolean_mask(\n    tensor, mask, axis=None, name='boolean_mask'\n)\n"
  },
  {
    "tf.broadcast_dynamic_shape": "tf.broadcast_dynamic_shape(\n    shape_x, shape_y\n)\n"
  },
  {
    "tf.broadcast_static_shape": "tf.broadcast_static_shape(\n    shape_x, shape_y\n)\n"
  },
  {
    "tf.broadcast_to": "tf.broadcast_to(\n    input, shape, name=None\n)\n"
  },
  {
    "tf.case": "tf.case(\n    pred_fn_pairs,\n    default=None,\n    exclusive=False,\n    strict=False,\n    name='case'\n)\n"
  },
  {
    "tf.cast": "tf.cast(\n    x, dtype, name=None\n)\n"
  },
  {
    "tf.clip_by_global_norm": "tf.clip_by_global_norm(\n    t_list, clip_norm, use_norm=None, name=None\n)\n"
  },
  {
    "tf.clip_by_norm": "tf.clip_by_norm(\n    t, clip_norm, axes=None, name=None\n)\n"
  },
  {
    "tf.clip_by_value": "tf.clip_by_value(\n    t, clip_value_min, clip_value_max, name=None\n)\n"
  },
  {
    "tf.compat.as_bytes": "tf.compat.as_bytes(\n    bytes_or_text, encoding='utf-8'\n)\n"
  },
  {
    "tf.compat.as_str": "tf.compat.as_str(\n    bytes_or_text, encoding='utf-8'\n)\n"
  },
  {
    "tf.compat.as_str_any": "tf.compat.as_str_any(\n    value, encoding='utf-8'\n)\n"
  },
  {
    "tf.compat.as_text": "tf.compat.as_text(\n    bytes_or_text, encoding='utf-8'\n)\n"
  },
  {
    "tf.compat.dimension_at_index": "tf.compat.dimension_at_index(\n    shape, index\n)\n"
  },
  {
    "tf.compat.dimension_value": "tf.compat.dimension_value(\n    dimension\n)\n"
  },
  {
    "tf.compat.forward_compatible": "tf.compat.forward_compatible(\n    year, month, day\n)\n"
  },
  {
    "tf.compat.path_to_str": "tf.compat.path_to_str(\n    path\n)\n"
  },
  {
    "tf.dtypes.complex": "tf.dtypes.complex(\n    real, imag, name=None\n)\n"
  },
  {
    "tf.concat": "tf.concat(\n    values, axis, name='concat'\n)\n"
  },
  {
    "tf.cond": "tf.cond(\n    pred, true_fn=None, false_fn=None, name=None\n)\n"
  },
  {
    "tf.config.LogicalDevice": "tf.config.LogicalDevice(\n    name, device_type\n)\n"
  },
  {
    "tf.config.LogicalDeviceConfiguration": "tf.config.LogicalDeviceConfiguration(\n    memory_limit=None,\n    experimental_priority=None,\n    experimental_device_ordinal=None\n)\n"
  },
  {
    "tf.config.PhysicalDevice": "tf.config.PhysicalDevice(\n    name, device_type\n)\n"
  },
  {
    "tf.config.LogicalDeviceConfiguration": "tf.config.LogicalDeviceConfiguration(\n    memory_limit=None,\n    experimental_priority=None,\n    experimental_device_ordinal=None\n)\n"
  },
  {
    "tf.config.experimental.enable_tensor_float_32_execution": "tf.config.experimental.enable_tensor_float_32_execution(\n    enabled\n)\n"
  },
  {
    "tf.config.experimental.get_device_details": "tf.config.experimental.get_device_details(\n    device\n)\n"
  },
  {
    "tf.config.experimental.get_memory_growth": "tf.config.experimental.get_memory_growth(\n    device\n)\n"
  },
  {
    "tf.config.experimental.get_memory_info": "tf.config.experimental.get_memory_info(\n    device\n)\n"
  },
  {
    "tf.config.experimental.get_memory_usage": "tf.config.experimental.get_memory_usage(\n    device\n)\n"
  },
  {
    "tf.config.get_logical_device_configuration": "tf.config.get_logical_device_configuration(\n    device\n)\n"
  },
  {
    "tf.config.get_visible_devices": "tf.config.get_visible_devices(\n    device_type=None\n)\n"
  },
  {
    "tf.config.list_logical_devices": "tf.config.list_logical_devices(\n    device_type=None\n)\n"
  },
  {
    "tf.config.list_physical_devices": "tf.config.list_physical_devices(\n    device_type=None\n)\n"
  },
  {
    "tf.config.experimental.reset_memory_stats": "tf.config.experimental.reset_memory_stats(\n    device\n)\n"
  },
  {
    "tf.config.experimental.set_device_policy": "tf.config.experimental.set_device_policy(\n    device_policy\n)\n"
  },
  {
    "tf.config.experimental.set_memory_growth": "tf.config.experimental.set_memory_growth(\n    device, enable\n)\n"
  },
  {
    "tf.config.experimental.set_synchronous_execution": "tf.config.experimental.set_synchronous_execution(\n    enable\n)\n"
  },
  {
    "tf.config.set_logical_device_configuration": "tf.config.set_logical_device_configuration(\n    device, logical_devices\n)\n"
  },
  {
    "tf.config.set_visible_devices": "tf.config.set_visible_devices(\n    devices, device_type=None\n)\n"
  },
  {
    "tf.config.experimental_connect_to_cluster": "tf.config.experimental_connect_to_cluster(\n    cluster_spec_or_resolver,\n    job_name='localhost',\n    task_index=0,\n    protocol=None,\n    make_master_device_default=True,\n    cluster_device_filters=None\n)\n"
  },
  {
    "tf.config.experimental_connect_to_host": "tf.config.experimental_connect_to_host(\n    remote_host=None, job_name='worker'\n)\n"
  },
  {
    "tf.config.experimental_run_functions_eagerly": "tf.config.experimental_run_functions_eagerly(\n    run_eagerly\n)\n"
  },
  {
    "tf.config.get_logical_device_configuration": "tf.config.get_logical_device_configuration(\n    device\n)\n"
  },
  {
    "tf.config.get_visible_devices": "tf.config.get_visible_devices(\n    device_type=None\n)\n"
  },
  {
    "tf.config.list_logical_devices": "tf.config.list_logical_devices(\n    device_type=None\n)\n"
  },
  {
    "tf.config.list_physical_devices": "tf.config.list_physical_devices(\n    device_type=None\n)\n"
  },
  {
    "tf.config.optimizer.get_jit": "tf.config.optimizer.get_jit() -> str\n"
  },
  {
    "tf.config.optimizer.set_experimental_options": "tf.config.optimizer.set_experimental_options(\n    options\n)\n"
  },
  {
    "tf.config.optimizer.set_jit": "tf.config.optimizer.set_jit(\n    enabled: Union[bool, str]\n)\n"
  },
  {
    "tf.config.run_functions_eagerly": "tf.config.run_functions_eagerly(\n    run_eagerly\n)\n"
  },
  {
    "tf.config.set_logical_device_configuration": "tf.config.set_logical_device_configuration(\n    device, logical_devices\n)\n"
  },
  {
    "tf.config.set_soft_device_placement": "tf.config.set_soft_device_placement(\n    enabled\n)\n"
  },
  {
    "tf.config.set_visible_devices": "tf.config.set_visible_devices(\n    devices, device_type=None\n)\n"
  },
  {
    "tf.config.threading.set_inter_op_parallelism_threads": "tf.config.threading.set_inter_op_parallelism_threads(\n    num_threads\n)\n"
  },
  {
    "tf.config.threading.set_intra_op_parallelism_threads": "tf.config.threading.set_intra_op_parallelism_threads(\n    num_threads\n)\n"
  },
  {
    "tf.constant": "tf.constant(\n    value, dtype=None, shape=None, name='Const'\n)\n"
  },
  {
    "tf.constant_initializer": "tf.constant_initializer(\n    value=0\n)\n"
  },
  {
    "tf.control_dependencies": "tf.control_dependencies(\n    control_inputs\n)\n"
  },
  {
    "tf.convert_to_tensor": "tf.convert_to_tensor(\n    value, dtype=None, dtype_hint=None, name=None\n)\n"
  },
  {
    "tf.math.cos": "tf.math.cos(\n    x, name=None\n)\n"
  },
  {
    "tf.math.cosh": "tf.math.cosh(\n    x, name=None\n)\n"
  },
  {
    "tf.math.cumsum": "tf.math.cumsum(\n    x, axis=0, exclusive=False, reverse=False, name=None\n)\n"
  },
  {
    "tf.custom_gradient": "tf.custom_gradient(\n    f=None\n)\n"
  },
  {
    "tf.data.Dataset": "tf.data.Dataset(\n    variant_tensor\n)\n"
  },
  {
    "tf.data.DatasetSpec": "tf.data.DatasetSpec(\n    element_spec, dataset_shape=()\n)\n"
  },
  {
    "tf.data.FixedLengthRecordDataset": "tf.data.FixedLengthRecordDataset(\n    filenames,\n    record_bytes,\n    header_bytes=None,\n    footer_bytes=None,\n    buffer_size=None,\n    compression_type=None,\n    num_parallel_reads=None,\n    name=None\n)\n"
  },
  {
    "tf.data.IteratorSpec": "tf.data.IteratorSpec(\n    element_spec\n)\n"
  },
  {
    "tf.data.TFRecordDataset": "tf.data.TFRecordDataset(\n    filenames,\n    compression_type=None,\n    buffer_size=None,\n    num_parallel_reads=None,\n    name=None\n)\n"
  },
  {
    "tf.data.TextLineDataset": "tf.data.TextLineDataset(\n    filenames,\n    compression_type=None,\n    buffer_size=None,\n    num_parallel_reads=None,\n    name=None\n)\n"
  },
  {
    "tf.data.experimental.CheckpointInputPipelineHook": "tf.data.experimental.CheckpointInputPipelineHook(\n    estimator, external_state_policy=None\n)\n"
  },
  {
    "tf.data.experimental.Counter": "tf.data.experimental.Counter(\n    start=0,\n    step=1,\n    dtype=tf.dtypes.int64\n)\n"
  },
  {
    "tf.data.experimental.CsvDataset": "tf.data.experimental.CsvDataset(\n    filenames,\n    record_defaults,\n    compression_type=None,\n    buffer_size=None,\n    header=False,\n    field_delim=',',\n    use_quote_delim=True,\n    na_value='',\n    select_cols=None,\n    exclude_cols=None\n)\n"
  },
  {
    "tf.data.experimental.DatasetInitializer": "tf.data.experimental.DatasetInitializer(\n    dataset\n)\n"
  },
  {
    "tf.data.experimental.RandomDataset": "tf.data.experimental.RandomDataset(\n    seed=None, name=None\n)\n"
  },
  {
    "tf.data.experimental.Reducer": "tf.data.experimental.Reducer(\n    init_func, reduce_func, finalize_func\n)\n"
  },
  {
    "tf.data.experimental.SqlDataset": "tf.data.experimental.SqlDataset(\n    driver_name, data_source_name, query, output_types\n)\n"
  },
  {
    "tf.data.experimental.TFRecordWriter": "tf.data.experimental.TFRecordWriter(\n    filename, compression_type=None\n)\n"
  },
  {
    "tf.data.experimental.assert_cardinality": "tf.data.experimental.assert_cardinality(\n    expected_cardinality\n)\n"
  },
  {
    "tf.data.experimental.bucket_by_sequence_length": "tf.data.experimental.bucket_by_sequence_length(\n    element_length_func,\n    bucket_boundaries,\n    bucket_batch_sizes,\n    padded_shapes=None,\n    padding_values=None,\n    pad_to_bucket_boundary=False,\n    no_padding=False,\n    drop_remainder=False\n)\n"
  },
  {
    "tf.data.experimental.cardinality": "tf.data.experimental.cardinality(\n    dataset\n)\n"
  },
  {
    "tf.data.experimental.choose_from_datasets": "tf.data.experimental.choose_from_datasets(\n    datasets, choice_dataset, stop_on_empty_dataset=False\n)\n"
  },
  {
    "tf.data.experimental.copy_to_device": "tf.data.experimental.copy_to_device(\n    target_device, source_device='/cpu:0'\n)\n"
  },
  {
    "tf.data.experimental.dense_to_ragged_batch": "tf.data.experimental.dense_to_ragged_batch(\n    batch_size,\n    drop_remainder=False,\n    row_splits_dtype=tf.dtypes.int64\n)\n"
  },
  {
    "tf.data.experimental.dense_to_sparse_batch": "tf.data.experimental.dense_to_sparse_batch(\n    batch_size, row_shape\n)\n"
  },
  {
    "tf.data.experimental.enumerate_dataset": "tf.data.experimental.enumerate_dataset(\n    start=0\n)\n"
  },
  {
    "tf.data.experimental.from_list": "tf.data.experimental.from_list(\n    elements, name=None\n)\n"
  },
  {
    "tf.data.experimental.from_variant": "tf.data.experimental.from_variant(\n    variant, structure\n)\n"
  },
  {
    "tf.data.experimental.get_next_as_optional": "tf.data.experimental.get_next_as_optional(\n    iterator\n)\n"
  },
  {
    "tf.data.experimental.get_single_element": "tf.data.experimental.get_single_element(\n    dataset\n)\n"
  },
  {
    "tf.data.experimental.get_structure": "tf.data.experimental.get_structure(\n    dataset_or_iterator\n)\n"
  },
  {
    "tf.data.experimental.group_by_reducer": "tf.data.experimental.group_by_reducer(\n    key_func, reducer\n)\n"
  },
  {
    "tf.data.experimental.group_by_window": "tf.data.experimental.group_by_window(\n    key_func, reduce_func, window_size=None, window_size_func=None\n)\n"
  },
  {
    "tf.data.experimental.ignore_errors": "tf.data.experimental.ignore_errors(\n    log_warning=False\n)\n"
  },
  {
    "tf.data.experimental.index_table_from_dataset": "tf.data.experimental.index_table_from_dataset(\n    dataset=None,\n    num_oov_buckets=0,\n    vocab_size=None,\n    default_value=-1,\n    hasher_spec=lookup_ops.FastHashSpec,\n    key_dtype=tf.dtypes.string,\n    name=None\n)\n"
  },
  {
    "tf.data.experimental.load": "tf.data.experimental.load(\n    path, element_spec=None, compression=None, reader_func=None\n)\n"
  },
  {
    "tf.data.experimental.make_batched_features_dataset": "tf.data.experimental.make_batched_features_dataset(\n    file_pattern,\n    batch_size,\n    features,\n    reader=None,\n    label_key=None,\n    reader_args=None,\n    num_epochs=None,\n    shuffle=True,\n    shuffle_buffer_size=10000,\n    shuffle_seed=None,\n    prefetch_buffer_size=None,\n    reader_num_threads=None,\n    parser_num_threads=None,\n    sloppy_ordering=False,\n    drop_final_batch=False\n)\n"
  },
  {
    "tf.data.experimental.make_csv_dataset": "tf.data.experimental.make_csv_dataset(\n    file_pattern,\n    batch_size,\n    column_names=None,\n    column_defaults=None,\n    label_name=None,\n    select_columns=None,\n    field_delim=',',\n    use_quote_delim=True,\n    na_value='',\n    header=True,\n    num_epochs=None,\n    shuffle=True,\n    shuffle_buffer_size=10000,\n    shuffle_seed=None,\n    prefetch_buffer_size=None,\n    num_parallel_reads=None,\n    sloppy=False,\n    num_rows_for_inference=100,\n    compression_type=None,\n    ignore_errors=False,\n    encoding='utf-8'\n)\n"
  },
  {
    "tf.data.experimental.make_saveable_from_iterator": "tf.data.experimental.make_saveable_from_iterator(\n    iterator, external_state_policy=None\n)\n"
  },
  {
    "tf.data.experimental.map_and_batch": "tf.data.experimental.map_and_batch(\n    map_func,\n    batch_size,\n    num_parallel_batches=None,\n    drop_remainder=False,\n    num_parallel_calls=None\n)\n"
  },
  {
    "tf.data.experimental.parallel_interleave": "tf.data.experimental.parallel_interleave(\n    map_func,\n    cycle_length,\n    block_length=1,\n    sloppy=False,\n    buffer_output_elements=None,\n    prefetch_input_elements=None\n)\n"
  },
  {
    "tf.data.experimental.parse_example_dataset": "tf.data.experimental.parse_example_dataset(\n    features, num_parallel_calls=1, deterministic=None\n)\n"
  },
  {
    "tf.data.experimental.prefetch_to_device": "tf.data.experimental.prefetch_to_device(\n    device, buffer_size=None\n)\n"
  },
  {
    "tf.data.experimental.rejection_resample": "tf.data.experimental.rejection_resample(\n    class_func, target_dist, initial_dist=None, seed=None\n)\n"
  },
  {
    "tf.data.experimental.sample_from_datasets": "tf.data.experimental.sample_from_datasets(\n    datasets, weights=None, seed=None, stop_on_empty_dataset=False\n)\n"
  },
  {
    "tf.data.experimental.save": "tf.data.experimental.save(\n    dataset, path, compression=None, shard_func=None, checkpoint_args=None\n)\n"
  },
  {
    "tf.data.experimental.scan": "tf.data.experimental.scan(\n    initial_state, scan_func\n)\n"
  },
  {
    "tf.data.experimental.service.CrossTrainerCache": "tf.data.experimental.service.CrossTrainerCache(\n    trainer_id\n)\n"
  },
  {
    "tf.data.experimental.service.DispatchServer": "tf.data.experimental.service.DispatchServer(\n    config=None, start=True\n)\n"
  },
  {
    "tf.data.experimental.service.DispatcherConfig": "tf.data.experimental.service.DispatcherConfig(\n    port=0,\n    protocol=None,\n    work_dir=None,\n    fault_tolerant_mode=False,\n    worker_addresses=None,\n    job_gc_check_interval_ms=None,\n    job_gc_timeout_ms=None\n)\n"
  },
  {
    "tf.data.experimental.service.WorkerConfig": "tf.data.experimental.service.WorkerConfig(\n    dispatcher_address,\n    worker_address=None,\n    port=0,\n    protocol=None,\n    heartbeat_interval_ms=None,\n    dispatcher_timeout_ms=None,\n    data_transfer_protocol=None,\n    data_transfer_address=None\n)\n"
  },
  {
    "tf.data.experimental.service.WorkerServer": "tf.data.experimental.service.WorkerServer(\n    config, start=True\n)\n"
  },
  {
    "tf.data.experimental.service.distribute": "tf.data.experimental.service.distribute(\n    processing_mode,\n    service,\n    job_name=None,\n    consumer_index=None,\n    num_consumers=None,\n    max_outstanding_requests=None,\n    data_transfer_protocol=None,\n    compression='AUTO',\n    cross_trainer_cache=None,\n    target_workers='AUTO'\n)\n"
  },
  {
    "tf.data.experimental.service.from_dataset_id": "tf.data.experimental.service.from_dataset_id(\n    processing_mode,\n    service,\n    dataset_id,\n    element_spec=None,\n    job_name=None,\n    consumer_index=None,\n    num_consumers=None,\n    max_outstanding_requests=None,\n    data_transfer_protocol=None,\n    cross_trainer_cache=None,\n    target_workers='AUTO'\n)\n"
  },
  {
    "tf.data.experimental.service.register_dataset": "tf.data.experimental.service.register_dataset(\n    service, dataset, compression='AUTO', dataset_id=None\n)\n"
  },
  {
    "tf.data.experimental.shuffle_and_repeat": "tf.data.experimental.shuffle_and_repeat(\n    buffer_size, count=None, seed=None\n)\n"
  },
  {
    "tf.data.experimental.snapshot": "tf.data.experimental.snapshot(\n    path, compression='AUTO', reader_func=None, shard_func=None\n)\n"
  },
  {
    "tf.data.experimental.table_from_dataset": "tf.data.experimental.table_from_dataset(\n    dataset=None,\n    num_oov_buckets=0,\n    vocab_size=None,\n    default_value=None,\n    hasher_spec=lookup_ops.FastHashSpec,\n    key_dtype=tf.dtypes.string,\n    name=None\n)\n"
  },
  {
    "tf.data.experimental.take_while": "tf.data.experimental.take_while(\n    predicate\n)\n"
  },
  {
    "tf.data.experimental.to_variant": "tf.data.experimental.to_variant(\n    dataset\n)\n"
  },
  {
    "tf.debugging.Assert": "tf.debugging.Assert(\n    condition, data, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_all_finite": "tf.debugging.assert_all_finite(\n    x, message, name=None\n)\n"
  },
  {
    "tf.debugging.assert_equal": "tf.debugging.assert_equal(\n    x, y, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_greater": "tf.debugging.assert_greater(\n    x, y, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_greater_equal": "tf.debugging.assert_greater_equal(\n    x, y, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_integer": "tf.debugging.assert_integer(\n    x, message=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_less": "tf.debugging.assert_less(\n    x, y, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_less_equal": "tf.debugging.assert_less_equal(\n    x, y, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_near": "tf.debugging.assert_near(\n    x, y, rtol=None, atol=None, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_negative": "tf.debugging.assert_negative(\n    x, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_non_negative": "tf.debugging.assert_non_negative(\n    x, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_non_positive": "tf.debugging.assert_non_positive(\n    x, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_none_equal": "tf.debugging.assert_none_equal(\n    x, y, summarize=None, message=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_positive": "tf.debugging.assert_positive(\n    x, message=None, summarize=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_proper_iterable": "tf.debugging.assert_proper_iterable(\n    values\n)\n"
  },
  {
    "tf.debugging.assert_rank": "tf.debugging.assert_rank(\n    x, rank, message=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_rank_at_least": "tf.debugging.assert_rank_at_least(\n    x, rank, message=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_rank_in": "tf.debugging.assert_rank_in(\n    x, ranks, message=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_same_float_dtype": "tf.debugging.assert_same_float_dtype(\n    tensors=None, dtype=None\n)\n"
  },
  {
    "tf.debugging.assert_scalar": "tf.debugging.assert_scalar(\n    tensor, message=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_shapes": "tf.debugging.assert_shapes(\n    shapes, data=None, summarize=None, message=None, name=None\n)\n"
  },
  {
    "tf.debugging.assert_type": "tf.debugging.assert_type(\n    tensor, tf_type, message=None, name=None\n)\n"
  },
  {
    "tf.debugging.check_numerics": "tf.debugging.check_numerics(\n    tensor, message, name=None\n)\n"
  },
  {
    "tf.debugging.enable_check_numerics": "tf.debugging.enable_check_numerics(\n    stack_height_limit=30, path_length_limit=50\n)\n"
  },
  {
    "tf.debugging.experimental.enable_dump_debug_info": "tf.debugging.experimental.enable_dump_debug_info(\n    dump_root,\n    tensor_debug_mode=DEFAULT_TENSOR_DEBUG_MODE,\n    circular_buffer_size=1000,\n    op_regex=None,\n    tensor_dtypes=None\n)\n"
  },
  {
    "tf.debugging.is_numeric_tensor": "tf.debugging.is_numeric_tensor(\n    tensor\n)\n"
  },
  {
    "tf.debugging.set_log_device_placement": "tf.debugging.set_log_device_placement(\n    enabled\n)\n"
  },
  {
    "tf.device": "tf.device(\n    device_name\n)\n"
  },
  {
    "tf.distribute.HierarchicalCopyAllReduce": "tf.distribute.HierarchicalCopyAllReduce(\n    num_packs=1\n)\n"
  },
  {
    "tf.distribute.InputContext": "tf.distribute.InputContext(\n    num_input_pipelines=1, input_pipeline_id=0, num_replicas_in_sync=1\n)\n"
  },
  {
    "tf.distribute.InputOptions": "tf.distribute.InputOptions(\n    experimental_fetch_to_device=None,\n    experimental_replication_mode=tf.distribute.InputReplicationMode.PER_WORKER,\n    experimental_place_dataset_on_device=False,\n    experimental_per_replica_buffer_size=1\n)\n"
  },
  {
    "tf.distribute.MirroredStrategy": "tf.distribute.MirroredStrategy(\n    devices=None, cross_device_ops=None\n)\n"
  },
  {
    "tf.distribute.MultiWorkerMirroredStrategy": "tf.distribute.MultiWorkerMirroredStrategy(\n    cluster_resolver=None, communication_options=None\n)\n"
  },
  {
    "tf.distribute.NcclAllReduce": "tf.distribute.NcclAllReduce(\n    num_packs=1\n)\n"
  },
  {
    "tf.distribute.OneDeviceStrategy": "tf.distribute.OneDeviceStrategy(\n    device\n)\n"
  },
  {
    "tf.distribute.experimental.ParameterServerStrategy": "tf.distribute.experimental.ParameterServerStrategy(\n    cluster_resolver, variable_partitioner=None\n)\n"
  },
  {
    "tf.distribute.ReductionToOneDevice": "tf.distribute.ReductionToOneDevice(\n    reduce_to_device=None, accumulation_fn=None\n)\n"
  },
  {
    "tf.distribute.ReplicaContext": "tf.distribute.ReplicaContext(\n    strategy, replica_id_in_sync_group\n)\n"
  },
  {
    "tf.distribute.RunOptions": "tf.distribute.RunOptions(\n    experimental_enable_dynamic_batch_size=True,\n    experimental_bucketizing_dynamic_shape=False,\n    experimental_xla_options=None\n)\n"
  },
  {
    "tf.distribute.Server": "tf.distribute.Server(\n    server_or_cluster_def,\n    job_name=None,\n    task_index=None,\n    protocol=None,\n    config=None,\n    start=True\n)\n"
  },
  {
    "tf.distribute.Strategy": "tf.distribute.Strategy(\n    extended\n)\n"
  },
  {
    "tf.distribute.StrategyExtended": "tf.distribute.StrategyExtended(\n    container_strategy\n)\n"
  },
  {
    "tf.distribute.TPUStrategy": "tf.distribute.TPUStrategy(\n    tpu_cluster_resolver=None,\n    experimental_device_assignment=None,\n    experimental_spmd_xla_partitioning=False\n)\n"
  },
  {
    "tf.distribute.cluster_resolver.GCEClusterResolver": "tf.distribute.cluster_resolver.GCEClusterResolver(\n    project,\n    zone,\n    instance_group,\n    port,\n    task_type='worker',\n    task_id=0,\n    rpc_layer='grpc',\n    credentials='default',\n    service=None\n)\n"
  },
  {
    "tf.distribute.cluster_resolver.KubernetesClusterResolver": "tf.distribute.cluster_resolver.KubernetesClusterResolver(\n    job_to_label_mapping=None,\n    tf_server_port=8470,\n    rpc_layer='grpc',\n    override_client=None\n)\n"
  },
  {
    "tf.distribute.cluster_resolver.SimpleClusterResolver": "tf.distribute.cluster_resolver.SimpleClusterResolver(\n    cluster_spec,\n    master='',\n    task_type=None,\n    task_id=None,\n    environment='',\n    num_accelerators=None,\n    rpc_layer=None\n)\n"
  },
  {
    "tf.distribute.cluster_resolver.SlurmClusterResolver": "tf.distribute.cluster_resolver.SlurmClusterResolver(\n    jobs=None,\n    port_base=8888,\n    gpus_per_node=None,\n    gpus_per_task=None,\n    tasks_per_node=None,\n    auto_set_gpu=True,\n    rpc_layer='grpc'\n)\n"
  },
  {
    "tf.distribute.cluster_resolver.TFConfigClusterResolver": "tf.distribute.cluster_resolver.TFConfigClusterResolver(\n    task_type=None, task_id=None, rpc_layer=None, environment=None\n)\n"
  },
  {
    "tf.distribute.cluster_resolver.TPUClusterResolver": "tf.distribute.cluster_resolver.TPUClusterResolver(\n    tpu=None,\n    zone=None,\n    project=None,\n    job_name='worker',\n    coordinator_name=None,\n    coordinator_address=None,\n    credentials='default',\n    service=None,\n    discovery_url=None\n)\n"
  },
  {
    "tf.distribute.cluster_resolver.UnionResolver": "tf.distribute.cluster_resolver.UnionResolver(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.distribute.experimental.coordinator.ClusterCoordinator": "tf.distribute.experimental.coordinator.ClusterCoordinator(\n    strategy\n)\n"
  },
  {
    "tf.distribute.experimental.coordinator.PerWorkerValues": "tf.distribute.experimental.coordinator.PerWorkerValues(\n    values\n)\n"
  },
  {
    "tf.distribute.experimental.CentralStorageStrategy": "tf.distribute.experimental.CentralStorageStrategy(\n    compute_devices=None, parameter_device=None\n)\n"
  },
  {
    "tf.distribute.experimental.CollectiveHints": "tf.distribute.experimental.CollectiveHints(\n    bytes_per_pack=0, timeout_seconds=None\n)\n"
  },
  {
    "tf.distribute.experimental.CommunicationOptions": "tf.distribute.experimental.CommunicationOptions(\n    bytes_per_pack=0,\n    timeout_seconds=None,\n    implementation=tf.distribute.experimental.CollectiveCommunication.AUTO\n)\n"
  },
  {
    "tf.distribute.experimental.MultiWorkerMirroredStrategy": "tf.distribute.experimental.MultiWorkerMirroredStrategy(\n    communication=tf.distribute.experimental.CollectiveCommunication.AUTO,\n    cluster_resolver=None\n)\n"
  },
  {
    "tf.distribute.experimental.ParameterServerStrategy": "tf.distribute.experimental.ParameterServerStrategy(\n    cluster_resolver, variable_partitioner=None\n)\n"
  },
  {
    "tf.distribute.experimental.PreemptionCheckpointHandler": "tf.distribute.experimental.PreemptionCheckpointHandler(\n    cluster_resolver,\n    checkpoint_or_checkpoint_manager,\n    checkpoint_dir=None,\n    termination_config=None\n)\n"
  },
  {
    "tf.distribute.experimental.TPUStrategy": "tf.distribute.experimental.TPUStrategy(\n    tpu_cluster_resolver=None, device_assignment=None\n)\n"
  },
  {
    "tf.distribute.experimental.TerminationConfig": "tf.distribute.experimental.TerminationConfig(\n    termination_watcher_fn=None, exit_fn=None, grace_period=None\n)\n"
  },
  {
    "tf.distribute.experimental.ValueContext": "tf.distribute.experimental.ValueContext(\n    replica_id_in_sync_group=0, num_replicas_in_sync=1\n)\n"
  },
  {
    "tf.distribute.experimental.coordinator.ClusterCoordinator": "tf.distribute.experimental.coordinator.ClusterCoordinator(\n    strategy\n)\n"
  },
  {
    "tf.distribute.experimental.coordinator.PerWorkerValues": "tf.distribute.experimental.coordinator.PerWorkerValues(\n    values\n)\n"
  },
  {
    "tf.distribute.experimental.partitioners.FixedShardsPartitioner": "tf.distribute.experimental.partitioners.FixedShardsPartitioner(\n    num_shards\n)\n"
  },
  {
    "tf.distribute.experimental.partitioners.MaxSizePartitioner": "tf.distribute.experimental.partitioners.MaxSizePartitioner(\n    max_shard_bytes, max_shards=None, bytes_per_string=16\n)\n"
  },
  {
    "tf.distribute.experimental.partitioners.MinSizePartitioner": "tf.distribute.experimental.partitioners.MinSizePartitioner(\n    min_shard_bytes=(256 << 10), max_shards=1, bytes_per_string=16\n)\n"
  },
  {
    "tf.distribute.experimental_set_strategy": "tf.distribute.experimental_set_strategy(\n    strategy\n)\n"
  },
  {
    "tf.math.divide": "tf.math.divide(\n    x, y, name=None\n)\n"
  },
  {
    "tf.dtypes.as_dtype": "tf.dtypes.as_dtype(\n    type_value\n)\n"
  },
  {
    "tf.cast": "tf.cast(\n    x, dtype, name=None\n)\n"
  },
  {
    "tf.dtypes.complex": "tf.dtypes.complex(\n    real, imag, name=None\n)\n"
  },
  {
    "tf.dtypes.saturate_cast": "tf.dtypes.saturate_cast(\n    value, dtype, name=None\n)\n"
  },
  {
    "tf.dynamic_partition": "tf.dynamic_partition(\n    data, partitions, num_partitions, name=None\n)\n"
  },
  {
    "tf.dynamic_stitch": "tf.dynamic_stitch(\n    indices, data, name=None\n)\n"
  },
  {
    "tf.edit_distance": "tf.edit_distance(\n    hypothesis, truth, normalize=True, name='edit_distance'\n)\n"
  },
  {
    "tf.linalg.eig": "tf.linalg.eig(\n    tensor, name=None\n)\n"
  },
  {
    "tf.linalg.eigvals": "tf.linalg.eigvals(\n    tensor, name=None\n)\n"
  },
  {
    "tf.einsum": "tf.einsum(\n    equation, *inputs, **kwargs\n)\n"
  },
  {
    "tf.ensure_shape": "tf.ensure_shape(\n    x, shape, name=None\n)\n"
  },
  {
    "tf.math.equal": "tf.math.equal(\n    x, y, name=None\n)\n"
  },
  {
    "tf.errors.AbortedError": "tf.errors.AbortedError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.AlreadyExistsError": "tf.errors.AlreadyExistsError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.CancelledError": "tf.errors.CancelledError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.DataLossError": "tf.errors.DataLossError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.DeadlineExceededError": "tf.errors.DeadlineExceededError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.FailedPreconditionError": "tf.errors.FailedPreconditionError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.InternalError": "tf.errors.InternalError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.InvalidArgumentError": "tf.errors.InvalidArgumentError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.NotFoundError": "tf.errors.NotFoundError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.OpError": "tf.errors.OpError(\n    node_def, op, message, error_code, *args\n)\n"
  },
  {
    "tf.errors.OperatorNotAllowedInGraphError": "tf.errors.OperatorNotAllowedInGraphError(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.errors.OutOfRangeError": "tf.errors.OutOfRangeError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.PermissionDeniedError": "tf.errors.PermissionDeniedError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.ResourceExhaustedError": "tf.errors.ResourceExhaustedError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.UnauthenticatedError": "tf.errors.UnauthenticatedError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.UnavailableError": "tf.errors.UnavailableError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.UnimplementedError": "tf.errors.UnimplementedError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.errors.UnknownError": "tf.errors.UnknownError(\n    node_def, op, message, *args\n)\n"
  },
  {
    "tf.estimator.BaselineClassifier": "tf.estimator.BaselineClassifier(\n    model_dir=None,\n    n_classes=2,\n    weight_column=None,\n    label_vocabulary=None,\n    optimizer='Ftrl',\n    config=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE\n)\n"
  },
  {
    "tf.estimator.BaselineEstimator": "tf.estimator.BaselineEstimator(\n    head, model_dir=None, optimizer='Ftrl', config=None\n)\n"
  },
  {
    "tf.estimator.BaselineRegressor": "tf.estimator.BaselineRegressor(\n    model_dir=None,\n    label_dimension=1,\n    weight_column=None,\n    optimizer='Ftrl',\n    config=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE\n)\n"
  },
  {
    "tf.estimator.BestExporter": "tf.estimator.BestExporter(\n    name='best_exporter',\n    serving_input_receiver_fn=None,\n    event_file_pattern='eval/*.tfevents.*',\n    compare_fn=_loss_smaller,\n    assets_extra=None,\n    as_text=False,\n    exports_to_keep=5\n)\n"
  },
  {
    "tf.estimator.BinaryClassHead": "tf.estimator.BinaryClassHead(\n    weight_column=None,\n    thresholds=None,\n    label_vocabulary=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    loss_fn=None,\n    name=None\n)\n"
  },
  {
    "tf.estimator.CheckpointSaverHook": "tf.estimator.CheckpointSaverHook(\n    checkpoint_dir,\n    save_secs=None,\n    save_steps=None,\n    saver=None,\n    checkpoint_basename='model.ckpt',\n    scaffold=None,\n    listeners=None,\n    save_graph_def=True\n)\n"
  },
  {
    "tf.estimator.DNNClassifier": "tf.estimator.DNNClassifier(\n    hidden_units,\n    feature_columns,\n    model_dir=None,\n    n_classes=2,\n    weight_column=None,\n    label_vocabulary=None,\n    optimizer='Adagrad',\n    activation_fn=tf.nn.relu,\n    dropout=None,\n    config=None,\n    warm_start_from=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    batch_norm=False\n)\n"
  },
  {
    "tf.estimator.DNNEstimator": "tf.estimator.DNNEstimator(\n    head,\n    hidden_units,\n    feature_columns,\n    model_dir=None,\n    optimizer='Adagrad',\n    activation_fn=tf.nn.relu,\n    dropout=None,\n    config=None,\n    warm_start_from=None,\n    batch_norm=False\n)\n"
  },
  {
    "tf.estimator.DNNLinearCombinedClassifier": "tf.estimator.DNNLinearCombinedClassifier(\n    model_dir=None,\n    linear_feature_columns=None,\n    linear_optimizer='Ftrl',\n    dnn_feature_columns=None,\n    dnn_optimizer='Adagrad',\n    dnn_hidden_units=None,\n    dnn_activation_fn=tf.nn.relu,\n    dnn_dropout=None,\n    n_classes=2,\n    weight_column=None,\n    label_vocabulary=None,\n    config=None,\n    warm_start_from=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    batch_norm=False,\n    linear_sparse_combiner='sum'\n)\n"
  },
  {
    "tf.estimator.DNNLinearCombinedEstimator": "tf.estimator.DNNLinearCombinedEstimator(\n    head,\n    model_dir=None,\n    linear_feature_columns=None,\n    linear_optimizer='Ftrl',\n    dnn_feature_columns=None,\n    dnn_optimizer='Adagrad',\n    dnn_hidden_units=None,\n    dnn_activation_fn=tf.nn.relu,\n    dnn_dropout=None,\n    config=None,\n    batch_norm=False,\n    linear_sparse_combiner='sum'\n)\n"
  },
  {
    "tf.estimator.DNNLinearCombinedRegressor": "tf.estimator.DNNLinearCombinedRegressor(\n    model_dir=None,\n    linear_feature_columns=None,\n    linear_optimizer='Ftrl',\n    dnn_feature_columns=None,\n    dnn_optimizer='Adagrad',\n    dnn_hidden_units=None,\n    dnn_activation_fn=tf.nn.relu,\n    dnn_dropout=None,\n    label_dimension=1,\n    weight_column=None,\n    config=None,\n    warm_start_from=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    batch_norm=False,\n    linear_sparse_combiner='sum'\n)\n"
  },
  {
    "tf.estimator.DNNRegressor": "tf.estimator.DNNRegressor(\n    hidden_units,\n    feature_columns,\n    model_dir=None,\n    label_dimension=1,\n    weight_column=None,\n    optimizer='Adagrad',\n    activation_fn=tf.nn.relu,\n    dropout=None,\n    config=None,\n    warm_start_from=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    batch_norm=False\n)\n"
  },
  {
    "tf.estimator.Estimator": "tf.estimator.Estimator(\n    model_fn, model_dir=None, config=None, params=None, warm_start_from=None\n)\n"
  },
  {
    "tf.estimator.EstimatorSpec": "tf.estimator.EstimatorSpec(\n    mode,\n    predictions=None,\n    loss=None,\n    train_op=None,\n    eval_metric_ops=None,\n    export_outputs=None,\n    training_chief_hooks=None,\n    training_hooks=None,\n    scaffold=None,\n    evaluation_hooks=None,\n    prediction_hooks=None\n)\n"
  },
  {
    "tf.estimator.EvalSpec": "tf.estimator.EvalSpec(\n    input_fn,\n    steps=100,\n    name=None,\n    hooks=None,\n    exporters=None,\n    start_delay_secs=120,\n    throttle_secs=600\n)\n"
  },
  {
    "tf.estimator.FeedFnHook": "tf.estimator.FeedFnHook(\n    feed_fn\n)\n"
  },
  {
    "tf.estimator.FinalExporter": "tf.estimator.FinalExporter(\n    name, serving_input_receiver_fn, assets_extra=None, as_text=False\n)\n"
  },
  {
    "tf.estimator.FinalOpsHook": "tf.estimator.FinalOpsHook(\n    final_ops, final_ops_feed_dict=None\n)\n"
  },
  {
    "tf.estimator.GlobalStepWaiterHook": "tf.estimator.GlobalStepWaiterHook(\n    wait_until_step\n)\n"
  },
  {
    "tf.estimator.LatestExporter": "tf.estimator.LatestExporter(\n    name,\n    serving_input_receiver_fn,\n    assets_extra=None,\n    as_text=False,\n    exports_to_keep=5\n)\n"
  },
  {
    "tf.estimator.LinearClassifier": "tf.estimator.LinearClassifier(\n    feature_columns,\n    model_dir=None,\n    n_classes=2,\n    weight_column=None,\n    label_vocabulary=None,\n    optimizer='Ftrl',\n    config=None,\n    warm_start_from=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    sparse_combiner='sum'\n)\n"
  },
  {
    "tf.estimator.LinearEstimator": "tf.estimator.LinearEstimator(\n    head,\n    feature_columns,\n    model_dir=None,\n    optimizer='Ftrl',\n    config=None,\n    sparse_combiner='sum',\n    warm_start_from=None\n)\n"
  },
  {
    "tf.estimator.LinearRegressor": "tf.estimator.LinearRegressor(\n    feature_columns,\n    model_dir=None,\n    label_dimension=1,\n    weight_column=None,\n    optimizer='Ftrl',\n    config=None,\n    warm_start_from=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    sparse_combiner='sum'\n)\n"
  },
  {
    "tf.estimator.LoggingTensorHook": "tf.estimator.LoggingTensorHook(\n    tensors, every_n_iter=None, every_n_secs=None, at_end=False, formatter=None\n)\n"
  },
  {
    "tf.estimator.LogisticRegressionHead": "tf.estimator.LogisticRegressionHead(\n    weight_column=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    name=None\n)\n"
  },
  {
    "tf.estimator.MultiClassHead": "tf.estimator.MultiClassHead(\n    n_classes,\n    weight_column=None,\n    label_vocabulary=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    loss_fn=None,\n    name=None\n)\n"
  },
  {
    "tf.estimator.MultiHead": "tf.estimator.MultiHead(\n    heads, head_weights=None\n)\n"
  },
  {
    "tf.estimator.MultiLabelHead": "tf.estimator.MultiLabelHead(\n    n_classes,\n    weight_column=None,\n    thresholds=None,\n    label_vocabulary=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    loss_fn=None,\n    classes_for_class_based_metrics=None,\n    name=None\n)\n"
  },
  {
    "tf.estimator.NanLossDuringTrainingError": "tf.estimator.NanLossDuringTrainingError(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.estimator.NanTensorHook": "tf.estimator.NanTensorHook(\n    loss_tensor, fail_on_nan_loss=True\n)\n"
  },
  {
    "tf.estimator.PoissonRegressionHead": "tf.estimator.PoissonRegressionHead(\n    label_dimension=1,\n    weight_column=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    compute_full_loss=True,\n    name=None\n)\n"
  },
  {
    "tf.estimator.ProfilerHook": "tf.estimator.ProfilerHook(\n    save_steps=None,\n    save_secs=None,\n    output_dir='',\n    show_dataflow=True,\n    show_memory=False\n)\n"
  },
  {
    "tf.estimator.RegressionHead": "tf.estimator.RegressionHead(\n    label_dimension=1,\n    weight_column=None,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    loss_fn=None,\n    inverse_link_fn=None,\n    name=None\n)\n"
  },
  {
    "tf.estimator.RunConfig": "tf.estimator.RunConfig(\n    model_dir=None,\n    tf_random_seed=None,\n    save_summary_steps=100,\n    save_checkpoints_steps=_USE_DEFAULT,\n    save_checkpoints_secs=_USE_DEFAULT,\n    session_config=None,\n    keep_checkpoint_max=5,\n    keep_checkpoint_every_n_hours=10000,\n    log_step_count_steps=100,\n    train_distribute=None,\n    device_fn=None,\n    protocol=None,\n    eval_distribute=None,\n    experimental_distribute=None,\n    experimental_max_worker_delay_secs=None,\n    session_creation_timeout_secs=7200,\n    checkpoint_save_graph_def=True\n)\n"
  },
  {
    "tf.estimator.SecondOrStepTimer": "tf.estimator.SecondOrStepTimer(\n    every_secs=None, every_steps=None\n)\n"
  },
  {
    "tf.estimator.SessionRunArgs": "tf.estimator.SessionRunArgs(\n    fetches, feed_dict=None, options=None\n)\n"
  },
  {
    "tf.estimator.SessionRunContext": "tf.estimator.SessionRunContext(\n    original_args, session\n)\n"
  },
  {
    "tf.estimator.SessionRunValues": "tf.estimator.SessionRunValues(\n    results, options, run_metadata\n)\n"
  },
  {
    "tf.estimator.StepCounterHook": "tf.estimator.StepCounterHook(\n    every_n_steps=100, every_n_secs=None, output_dir=None, summary_writer=None\n)\n"
  },
  {
    "tf.estimator.StopAtStepHook": "tf.estimator.StopAtStepHook(\n    num_steps=None, last_step=None\n)\n"
  },
  {
    "tf.estimator.SummarySaverHook": "tf.estimator.SummarySaverHook(\n    save_steps=None,\n    save_secs=None,\n    output_dir=None,\n    summary_writer=None,\n    scaffold=None,\n    summary_op=None\n)\n"
  },
  {
    "tf.estimator.TrainSpec": "tf.estimator.TrainSpec(\n    input_fn, max_steps=None, hooks=None, saving_listeners=None\n)\n"
  },
  {
    "tf.estimator.WarmStartSettings": "tf.estimator.WarmStartSettings(\n    ckpt_to_initialize_from,\n    vars_to_warm_start='.*',\n    var_name_to_vocab_info=None,\n    var_name_to_prev_var_name=None\n)\n"
  },
  {
    "tf.estimator.add_metrics": "tf.estimator.add_metrics(\n    estimator, metric_fn\n)\n"
  },
  {
    "tf.estimator.classifier_parse_example_spec": "tf.estimator.classifier_parse_example_spec(\n    feature_columns,\n    label_key,\n    label_dtype=tf.dtypes.int64,\n    label_default=None,\n    weight_column=None\n)\n"
  },
  {
    "tf.estimator.experimental.InMemoryEvaluatorHook": "tf.estimator.experimental.InMemoryEvaluatorHook(\n    estimator, input_fn, steps=None, hooks=None, name=None, every_n_iter=100\n)\n"
  },
  {
    "tf.estimator.experimental.LinearSDCA": "tf.estimator.experimental.LinearSDCA(\n    example_id_column,\n    num_loss_partitions=1,\n    num_table_shards=None,\n    symmetric_l1_regularization=0.0,\n    symmetric_l2_regularization=1.0,\n    adaptive=False\n)\n"
  },
  {
    "tf.estimator.experimental.RNNClassifier": "tf.estimator.experimental.RNNClassifier(\n    sequence_feature_columns,\n    context_feature_columns=None,\n    units=None,\n    cell_type=USE_DEFAULT,\n    rnn_cell_fn=None,\n    return_sequences=False,\n    model_dir=None,\n    n_classes=2,\n    weight_column=None,\n    label_vocabulary=None,\n    optimizer='Adagrad',\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    sequence_mask='sequence_mask',\n    config=None\n)\n"
  },
  {
    "tf.estimator.experimental.RNNEstimator": "tf.estimator.experimental.RNNEstimator(\n    head,\n    sequence_feature_columns,\n    context_feature_columns=None,\n    units=None,\n    cell_type=USE_DEFAULT,\n    rnn_cell_fn=None,\n    return_sequences=False,\n    model_dir=None,\n    optimizer='Adagrad',\n    config=None\n)\n"
  },
  {
    "tf.estimator.experimental.build_raw_supervised_input_receiver_fn": "tf.estimator.experimental.build_raw_supervised_input_receiver_fn(\n    features, labels, default_batch_size=None\n)\n"
  },
  {
    "tf.estimator.experimental.call_logit_fn": "tf.estimator.experimental.call_logit_fn(\n    logit_fn, features, mode, params, config\n)\n"
  },
  {
    "tf.estimator.experimental.make_early_stopping_hook": "tf.estimator.experimental.make_early_stopping_hook(\n    estimator, should_stop_fn, run_every_secs=60, run_every_steps=None\n)\n"
  },
  {
    "tf.estimator.experimental.make_stop_at_checkpoint_step_hook": "tf.estimator.experimental.make_stop_at_checkpoint_step_hook(\n    estimator, last_step, wait_after_file_check_secs=30\n)\n"
  },
  {
    "tf.estimator.experimental.stop_if_higher_hook": "tf.estimator.experimental.stop_if_higher_hook(\n    estimator,\n    metric_name,\n    threshold,\n    eval_dir=None,\n    min_steps=0,\n    run_every_secs=60,\n    run_every_steps=None\n)\n"
  },
  {
    "tf.estimator.experimental.stop_if_lower_hook": "tf.estimator.experimental.stop_if_lower_hook(\n    estimator,\n    metric_name,\n    threshold,\n    eval_dir=None,\n    min_steps=0,\n    run_every_secs=60,\n    run_every_steps=None\n)\n"
  },
  {
    "tf.estimator.experimental.stop_if_no_decrease_hook": "tf.estimator.experimental.stop_if_no_decrease_hook(\n    estimator,\n    metric_name,\n    max_steps_without_decrease,\n    eval_dir=None,\n    min_steps=0,\n    run_every_secs=60,\n    run_every_steps=None\n)\n"
  },
  {
    "tf.estimator.experimental.stop_if_no_increase_hook": "tf.estimator.experimental.stop_if_no_increase_hook(\n    estimator,\n    metric_name,\n    max_steps_without_increase,\n    eval_dir=None,\n    min_steps=0,\n    run_every_secs=60,\n    run_every_steps=None\n)\n"
  },
  {
    "tf.estimator.export.ClassificationOutput": "tf.estimator.export.ClassificationOutput(\n    scores=None, classes=None\n)\n"
  },
  {
    "tf.estimator.export.EvalOutput": "tf.estimator.export.EvalOutput(\n    loss=None, predictions=None, metrics=None\n)\n"
  },
  {
    "tf.estimator.export.PredictOutput": "tf.estimator.export.PredictOutput(\n    outputs\n)\n"
  },
  {
    "tf.estimator.export.RegressionOutput": "tf.estimator.export.RegressionOutput(\n    value\n)\n"
  },
  {
    "tf.estimator.export.ServingInputReceiver": "tf.estimator.export.ServingInputReceiver(\n    features, receiver_tensors, receiver_tensors_alternatives=None\n)\n"
  },
  {
    "tf.estimator.export.TensorServingInputReceiver": "tf.estimator.export.TensorServingInputReceiver(\n    features, receiver_tensors, receiver_tensors_alternatives=None\n)\n"
  },
  {
    "tf.estimator.export.build_parsing_serving_input_receiver_fn": "tf.estimator.export.build_parsing_serving_input_receiver_fn(\n    feature_spec, default_batch_size=None\n)\n"
  },
  {
    "tf.estimator.export.build_raw_serving_input_receiver_fn": "tf.estimator.export.build_raw_serving_input_receiver_fn(\n    features, default_batch_size=None\n)\n"
  },
  {
    "tf.estimator.regressor_parse_example_spec": "tf.estimator.regressor_parse_example_spec(\n    feature_columns,\n    label_key,\n    label_dtype=tf.dtypes.float32,\n    label_default=None,\n    label_dimension=1,\n    weight_column=None\n)\n"
  },
  {
    "tf.estimator.train_and_evaluate": "tf.estimator.train_and_evaluate(\n    estimator, train_spec, eval_spec\n)\n"
  },
  {
    "tf.math.exp": "tf.math.exp(\n    x, name=None\n)\n"
  },
  {
    "tf.expand_dims": "tf.expand_dims(\n    input, axis, name=None\n)\n"
  },
  {
    "tf.experimental.BatchableExtensionType": "tf.experimental.BatchableExtensionType(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.DynamicRaggedShape": "tf.experimental.DynamicRaggedShape(\n    row_partitions: Sequence[tf.experimental.RowPartition],\n    inner_shape: tf.types.experimental.TensorLike,\n    dtype: Optional[tf.dtypes.DType] = None,\n    validate: bool = False,\n    static_inner_shape: ... = None\n)\n"
  },
  {
    "tf.experimental.DynamicRaggedShape.Spec": "tf.experimental.DynamicRaggedShape.Spec(\n    row_partitions: Tuple[RowPartitionSpec, ...],\n    static_inner_shape: tf.TensorShape,\n    dtype: tf.dtypes.DType\n)\n"
  },
  {
    "tf.experimental.ExtensionType": "tf.experimental.ExtensionType(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.RowPartition": "tf.experimental.RowPartition(\n    row_splits,\n    row_lengths=None,\n    value_rowids=None,\n    nrows=None,\n    uniform_row_length=None,\n    nvals=None,\n    internal=False\n)\n"
  },
  {
    "tf.experimental.StructuredTensor": "tf.experimental.StructuredTensor(\n    fields: Mapping[str, _FieldValue],\n    ragged_shape: tf.experimental.DynamicRaggedShape\n)\n"
  },
  {
    "tf.experimental.StructuredTensor": "tf.experimental.StructuredTensor(\n    fields: Mapping[str, _FieldValue],\n    ragged_shape: tf.experimental.DynamicRaggedShape\n)\n"
  },
  {
    "tf.experimental.StructuredTensor.Spec": "tf.experimental.StructuredTensor.Spec(\n    _fields, _ragged_shape\n)\n"
  },
  {
    "tf.experimental.dispatch_for_api": "tf.experimental.dispatch_for_api(\n    api, *signatures\n)\n"
  },
  {
    "tf.experimental.dispatch_for_binary_elementwise_apis": "tf.experimental.dispatch_for_binary_elementwise_apis(\n    x_type, y_type\n)\n"
  },
  {
    "tf.experimental.dispatch_for_binary_elementwise_assert_apis": "tf.experimental.dispatch_for_binary_elementwise_assert_apis(\n    x_type, y_type\n)\n"
  },
  {
    "tf.experimental.dispatch_for_unary_elementwise_apis": "tf.experimental.dispatch_for_unary_elementwise_apis(\n    x_type\n)\n"
  },
  {
    "tf.experimental.dlpack.from_dlpack": "tf.experimental.dlpack.from_dlpack(\n    dlcapsule\n)\n"
  },
  {
    "tf.experimental.dlpack.to_dlpack": "tf.experimental.dlpack.to_dlpack(\n    tf_tensor\n)\n"
  },
  {
    "tf.experimental.dtensor.DTensorCheckpoint": "tf.experimental.dtensor.DTensorCheckpoint(\n    mesh: tf.experimental.dtensor.Mesh,\n    root=None,\n    **kwargs\n)\n"
  },
  {
    "tf.experimental.dtensor.DTensorDataset": "tf.experimental.dtensor.DTensorDataset(\n    dataset: tf.data.Dataset,\n    *,\n    mesh: tf.experimental.dtensor.Mesh,\n    layouts: Any,\n    global_batch_size: int,\n    dataset_already_batched: bool = False,\n    batch_dim: Optional[str] = None,\n    prefetch: Optional[int] = None,\n    tf_data_service_config: Optional[TFDataServiceConfig] = None\n)\n"
  },
  {
    "tf.experimental.dtensor.DVariable": "tf.experimental.dtensor.DVariable(\n    initial_value, *args, dtype=None, **kwargs\n)\n"
  },
  {
    "tf.Variable.SaveSliceInfo": "tf.Variable.SaveSliceInfo(\n    full_name=None,\n    full_shape=None,\n    var_offset=None,\n    var_shape=None,\n    save_slice_info_def=None,\n    import_scope=None\n)\n"
  },
  {
    "tf.experimental.dtensor.Layout": "tf.experimental.dtensor.Layout(\n    sharding_specs: List[str],\n    mesh: tf.experimental.dtensor.Mesh\n)\n"
  },
  {
    "tf.experimental.dtensor.Mesh": "tf.experimental.dtensor.Mesh(\n    dim_names: List[str],\n    global_device_ids: np.ndarray,\n    local_device_ids: List[int],\n    local_devices: List[tf.compat.v1.DeviceSpec],\n    mesh_name: str = '',\n    global_devices: Optional[List[tf_device.DeviceSpec]] = None\n)\n"
  },
  {
    "tf.experimental.dtensor.barrier": "tf.experimental.dtensor.barrier(\n    mesh: tf.experimental.dtensor.Mesh,\n    barrier_name: Optional[str] = None\n)\n"
  },
  {
    "tf.experimental.dtensor.call_with_layout": "tf.experimental.dtensor.call_with_layout(\n    fn: Callable[..., Any],\n    layout: Optional[tf.experimental.dtensor.Layout],\n    *args,\n    **kwargs\n) -> Any\n"
  },
  {
    "tf.experimental.dtensor.check_layout": "tf.experimental.dtensor.check_layout(\n    tensor: tf.Tensor,\n    layout: tf.experimental.dtensor.Layout\n) -> None\n"
  },
  {
    "tf.experimental.dtensor.client_id": "tf.experimental.dtensor.client_id() -> int\n"
  },
  {
    "tf.experimental.dtensor.copy_to_mesh": "tf.experimental.dtensor.copy_to_mesh(\n    tensor: Any,\n    layout: tf.experimental.dtensor.Layout,\n    source_layout: Optional[tf.experimental.dtensor.Layout] = None\n) -> tf.Tensor\n"
  },
  {
    "tf.experimental.dtensor.create_distributed_mesh": "tf.experimental.dtensor.create_distributed_mesh(\n    mesh_dims: List[Tuple[str, int]],\n    mesh_name: str = '',\n    local_devices: Optional[List[str]] = None,\n    device_type: Optional[str] = None\n) -> tf.experimental.dtensor.Mesh\n"
  },
  {
    "tf.experimental.dtensor.create_mesh": "tf.experimental.dtensor.create_mesh(\n    mesh_dims: Optional[List[Tuple[str, int]]] = None,\n    mesh_name: str = '',\n    devices: Optional[List[str]] = None,\n    device_type: Optional[str] = None\n) -> tf.experimental.dtensor.Mesh\n"
  },
  {
    "tf.experimental.dtensor.create_tpu_mesh": "tf.experimental.dtensor.create_tpu_mesh(\n    mesh_dim_names: List[str],\n    mesh_shape: List[int],\n    mesh_name: str,\n    ring_dims: Optional[int] = None,\n    ring_axes: Optional[List[str]] = None,\n    ring_bounds: Optional[List[int]] = None,\n    can_split_host_across_rings: bool = True,\n    build_ring_across_rings: bool = False,\n    rotate_ring_across_rings: bool = False\n) -> tf.experimental.dtensor.Mesh\n"
  },
  {
    "tf.experimental.dtensor.device_name": "tf.experimental.dtensor.device_name() -> str\n"
  },
  {
    "tf.experimental.dtensor.enable_save_as_bf16": "tf.experimental.dtensor.enable_save_as_bf16(\n    variables: List[tf.Variable]\n)\n"
  },
  {
    "tf.experimental.dtensor.fetch_layout": "tf.experimental.dtensor.fetch_layout(\n    tensor: tf.Tensor\n) -> tf.experimental.dtensor.Layout\n"
  },
  {
    "tf.experimental.dtensor.full_job_name": "tf.experimental.dtensor.full_job_name(\n    task_id: Optional[int] = None\n) -> str\n"
  },
  {
    "tf.experimental.dtensor.heartbeat_enabled": "tf.experimental.dtensor.heartbeat_enabled() -> bool\n"
  },
  {
    "tf.experimental.dtensor.initialize_accelerator_system": "tf.experimental.dtensor.initialize_accelerator_system(\n    device_type: Optional[str] = None,\n    enable_coordination_service: Optional[bool] = False\n) -> str\n"
  },
  {
    "tf.experimental.dtensor.initialize_accelerator_system": "tf.experimental.dtensor.initialize_accelerator_system(\n    device_type: Optional[str] = None,\n    enable_coordination_service: Optional[bool] = False\n) -> str\n"
  },
  {
    "tf.experimental.dtensor.initialize_accelerator_system": "tf.experimental.dtensor.initialize_accelerator_system(\n    device_type: Optional[str] = None,\n    enable_coordination_service: Optional[bool] = False\n) -> str\n"
  },
  {
    "tf.experimental.dtensor.job_name": "tf.experimental.dtensor.job_name() -> str\n"
  },
  {
    "tf.experimental.dtensor.jobs": "tf.experimental.dtensor.jobs() -> List[str]\n"
  },
  {
    "tf.experimental.dtensor.local_devices": "tf.experimental.dtensor.local_devices(\n    device_type: str, for_client_id: Optional[int] = None\n) -> List[tf.compat.v1.DeviceSpec]\n"
  },
  {
    "tf.experimental.dtensor.name_based_restore": "tf.experimental.dtensor.name_based_restore(\n    mesh: tf.experimental.dtensor.Mesh,\n    checkpoint_prefix: str,\n    name_tensor_dict: Dict[str, Union[ops.Tensor, tf_variables.Variable]]\n)\n"
  },
  {
    "tf.experimental.dtensor.name_based_save": "tf.experimental.dtensor.name_based_save(\n    mesh: tf.experimental.dtensor.Mesh,\n    checkpoint_prefix: Union[str, tf.Tensor],\n    name_tensor_dict: Dict[str, Union[ops.Tensor, tf_variables.Variable]]\n)\n"
  },
  {
    "tf.experimental.dtensor.num_clients": "tf.experimental.dtensor.num_clients() -> int\n"
  },
  {
    "tf.experimental.dtensor.num_global_devices": "tf.experimental.dtensor.num_global_devices(\n    device_type: str\n) -> int\n"
  },
  {
    "tf.experimental.dtensor.num_local_devices": "tf.experimental.dtensor.num_local_devices(\n    device_type: str\n) -> int\n"
  },
  {
    "tf.experimental.dtensor.pack": "tf.experimental.dtensor.pack(\n    tensors: Sequence[Any],\n    layout: tf.experimental.dtensor.Layout\n) -> Any\n"
  },
  {
    "tf.experimental.dtensor.preferred_device_type": "tf.experimental.dtensor.preferred_device_type() -> str\n"
  },
  {
    "tf.experimental.dtensor.relayout": "tf.experimental.dtensor.relayout(\n    tensor: tf.Tensor,\n    layout: tf.experimental.dtensor.Layout\n) -> tf.Tensor\n"
  },
  {
    "tf.experimental.dtensor.sharded_save": "tf.experimental.dtensor.sharded_save(\n    mesh: tf.experimental.dtensor.Mesh,\n    file_prefix: Union[str, tf.Tensor],\n    tensor_names: Union[List[str], tf.Tensor],\n    shape_and_slices: Union[List[str], tf.Tensor],\n    tensors: List[Union[ops.Tensor, tf_variables.Variable]]\n)\n"
  },
  {
    "tf.experimental.dtensor.shutdown_accelerator_system": "tf.experimental.dtensor.shutdown_accelerator_system() -> None\n"
  },
  {
    "tf.experimental.dtensor.shutdown_accelerator_system": "tf.experimental.dtensor.shutdown_accelerator_system() -> None\n"
  },
  {
    "tf.experimental.dtensor.unpack": "tf.experimental.dtensor.unpack(\n    tensor: Any\n) -> Sequence[Any]\n"
  },
  {
    "tf.experimental.numpy.abs": "tf.experimental.numpy.abs(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.absolute": "tf.experimental.numpy.absolute(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.add": "tf.experimental.numpy.add(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.all": "tf.experimental.numpy.all(\n    a, axis=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.allclose": "tf.experimental.numpy.allclose(\n    a, b, rtol=1e-05, atol=1e-08, equal_nan=False\n)\n"
  },
  {
    "tf.experimental.numpy.amax": "tf.experimental.numpy.amax(\n    a, axis=None, out=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.amin": "tf.experimental.numpy.amin(\n    a, axis=None, out=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.angle": "tf.experimental.numpy.angle(\n    z, deg=False\n)\n"
  },
  {
    "tf.experimental.numpy.any": "tf.experimental.numpy.any(\n    a, axis=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.append": "tf.experimental.numpy.append(\n    arr, values, axis=None\n)\n"
  },
  {
    "tf.experimental.numpy.arange": "tf.experimental.numpy.arange(\n    start, stop=None, step=1, dtype=None\n)\n"
  },
  {
    "tf.experimental.numpy.arccos": "tf.experimental.numpy.arccos(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.arccosh": "tf.experimental.numpy.arccosh(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.arcsin": "tf.experimental.numpy.arcsin(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.arcsinh": "tf.experimental.numpy.arcsinh(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.arctan": "tf.experimental.numpy.arctan(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.arctan2": "tf.experimental.numpy.arctan2(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.arctanh": "tf.experimental.numpy.arctanh(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.argmax": "tf.experimental.numpy.argmax(\n    a, axis=None\n)\n"
  },
  {
    "tf.experimental.numpy.argmin": "tf.experimental.numpy.argmin(\n    a, axis=None\n)\n"
  },
  {
    "tf.experimental.numpy.argsort": "tf.experimental.numpy.argsort(\n    a, axis=-1, kind='quicksort', order=None\n)\n"
  },
  {
    "tf.experimental.numpy.around": "tf.experimental.numpy.around(\n    a, decimals=0\n)\n"
  },
  {
    "tf.experimental.numpy.array": "tf.experimental.numpy.array(\n    val, dtype=None, copy=True, ndmin=0\n)\n"
  },
  {
    "tf.experimental.numpy.array_equal": "tf.experimental.numpy.array_equal(\n    a1, a2\n)\n"
  },
  {
    "tf.experimental.numpy.asanyarray": "tf.experimental.numpy.asanyarray(\n    a, dtype=None\n)\n"
  },
  {
    "tf.experimental.numpy.asarray": "tf.experimental.numpy.asarray(\n    a, dtype=None\n)\n"
  },
  {
    "tf.experimental.numpy.ascontiguousarray": "tf.experimental.numpy.ascontiguousarray(\n    a, dtype=None\n)\n"
  },
  {
    "tf.experimental.numpy.atleast_1d": "tf.experimental.numpy.atleast_1d(\n    *arys\n)\n"
  },
  {
    "tf.experimental.numpy.atleast_2d": "tf.experimental.numpy.atleast_2d(\n    *arys\n)\n"
  },
  {
    "tf.experimental.numpy.atleast_3d": "tf.experimental.numpy.atleast_3d(\n    *arys\n)\n"
  },
  {
    "tf.experimental.numpy.average": "tf.experimental.numpy.average(\n    a, axis=None, weights=None, returned=False\n)\n"
  },
  {
    "tf.experimental.numpy.bitwise_and": "tf.experimental.numpy.bitwise_and(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.bitwise_not": "tf.experimental.numpy.bitwise_not(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.bitwise_or": "tf.experimental.numpy.bitwise_or(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.bitwise_xor": "tf.experimental.numpy.bitwise_xor(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.bool_": "tf.experimental.numpy.bool_(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.broadcast_arrays": "tf.experimental.numpy.broadcast_arrays(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.broadcast_to": "tf.experimental.numpy.broadcast_to(\n    array, shape\n)\n"
  },
  {
    "tf.experimental.numpy.cbrt": "tf.experimental.numpy.cbrt(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.ceil": "tf.experimental.numpy.ceil(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.clip": "tf.experimental.numpy.clip(\n    a, a_min, a_max\n)\n"
  },
  {
    "tf.experimental.numpy.complex128": "tf.experimental.numpy.complex128(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.complex64": "tf.experimental.numpy.complex64(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.complex128": "tf.experimental.numpy.complex128(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.compress": "tf.experimental.numpy.compress(\n    condition, a, axis=None\n)\n"
  },
  {
    "tf.experimental.numpy.concatenate": "tf.experimental.numpy.concatenate(\n    arys, axis=0\n)\n"
  },
  {
    "tf.experimental.numpy.conj": "tf.experimental.numpy.conj(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.conjugate": "tf.experimental.numpy.conjugate(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.copy": "tf.experimental.numpy.copy(\n    a\n)\n"
  },
  {
    "tf.experimental.numpy.cos": "tf.experimental.numpy.cos(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.cosh": "tf.experimental.numpy.cosh(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.count_nonzero": "tf.experimental.numpy.count_nonzero(\n    a, axis=None\n)\n"
  },
  {
    "tf.experimental.numpy.cross": "tf.experimental.numpy.cross(\n    a, b, axisa=-1, axisb=-1, axisc=-1, axis=None\n)\n"
  },
  {
    "tf.experimental.numpy.cumprod": "tf.experimental.numpy.cumprod(\n    a, axis=None, dtype=None\n)\n"
  },
  {
    "tf.experimental.numpy.cumsum": "tf.experimental.numpy.cumsum(\n    a, axis=None, dtype=None\n)\n"
  },
  {
    "tf.experimental.numpy.deg2rad": "tf.experimental.numpy.deg2rad(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.diag": "tf.experimental.numpy.diag(\n    v, k=0\n)\n"
  },
  {
    "tf.experimental.numpy.diag_indices": "tf.experimental.numpy.diag_indices(\n    n, ndim=2\n)\n"
  },
  {
    "tf.experimental.numpy.diagflat": "tf.experimental.numpy.diagflat(\n    v, k=0\n)\n"
  },
  {
    "tf.experimental.numpy.diagonal": "tf.experimental.numpy.diagonal(\n    a, offset=0, axis1=0, axis2=1\n)\n"
  },
  {
    "tf.experimental.numpy.diff": "tf.experimental.numpy.diff(\n    a, n=1, axis=-1\n)\n"
  },
  {
    "tf.experimental.numpy.divide": "tf.experimental.numpy.divide(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.divmod": "tf.experimental.numpy.divmod(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.dot": "tf.experimental.numpy.dot(\n    a, b\n)\n"
  },
  {
    "tf.experimental.numpy.dsplit": "tf.experimental.numpy.dsplit(\n    ary, indices_or_sections\n)\n"
  },
  {
    "tf.experimental.numpy.dstack": "tf.experimental.numpy.dstack(\n    tup\n)\n"
  },
  {
    "tf.experimental.numpy.einsum": "tf.experimental.numpy.einsum(\n    subscripts, *operands, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.empty": "tf.experimental.numpy.empty(\n    shape, dtype=float\n)\n"
  },
  {
    "tf.experimental.numpy.empty_like": "tf.experimental.numpy.empty_like(\n    a, dtype=None\n)\n"
  },
  {
    "tf.experimental.numpy.equal": "tf.experimental.numpy.equal(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.exp": "tf.experimental.numpy.exp(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.exp2": "tf.experimental.numpy.exp2(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.expand_dims": "tf.experimental.numpy.expand_dims(\n    a, axis\n)\n"
  },
  {
    "tf.experimental.numpy.experimental_enable_numpy_behavior": "tf.experimental.numpy.experimental_enable_numpy_behavior(\n    prefer_float32=False\n)\n"
  },
  {
    "tf.experimental.numpy.expm1": "tf.experimental.numpy.expm1(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.eye": "tf.experimental.numpy.eye(\n    N, M=None, k=0, dtype=float\n)\n"
  },
  {
    "tf.experimental.numpy.fabs": "tf.experimental.numpy.fabs(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.finfo": "tf.experimental.numpy.finfo(\n    dtype\n)\n"
  },
  {
    "tf.experimental.numpy.fix": "tf.experimental.numpy.fix(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.flip": "tf.experimental.numpy.flip(\n    m, axis=None\n)\n"
  },
  {
    "tf.experimental.numpy.fliplr": "tf.experimental.numpy.fliplr(\n    m\n)\n"
  },
  {
    "tf.experimental.numpy.flipud": "tf.experimental.numpy.flipud(\n    m\n)\n"
  },
  {
    "tf.experimental.numpy.float16": "tf.experimental.numpy.float16(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.float32": "tf.experimental.numpy.float32(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.float64": "tf.experimental.numpy.float64(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.float64": "tf.experimental.numpy.float64(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.float_power": "tf.experimental.numpy.float_power(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.floor": "tf.experimental.numpy.floor(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.floor_divide": "tf.experimental.numpy.floor_divide(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.full": "tf.experimental.numpy.full(\n    shape, fill_value, dtype=None\n)\n"
  },
  {
    "tf.experimental.numpy.full_like": "tf.experimental.numpy.full_like(\n    a, fill_value, dtype=None, order='K', subok=True, shape=None\n)\n"
  },
  {
    "tf.experimental.numpy.gcd": "tf.experimental.numpy.gcd(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.geomspace": "tf.experimental.numpy.geomspace(\n    start, stop, num=50, endpoint=True, dtype=None, axis=0\n)\n"
  },
  {
    "tf.experimental.numpy.greater": "tf.experimental.numpy.greater(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.greater_equal": "tf.experimental.numpy.greater_equal(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.heaviside": "tf.experimental.numpy.heaviside(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.hsplit": "tf.experimental.numpy.hsplit(\n    ary, indices_or_sections\n)\n"
  },
  {
    "tf.experimental.numpy.hstack": "tf.experimental.numpy.hstack(\n    tup\n)\n"
  },
  {
    "tf.experimental.numpy.hypot": "tf.experimental.numpy.hypot(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.identity": "tf.experimental.numpy.identity(\n    n, dtype=float\n)\n"
  },
  {
    "tf.experimental.numpy.iinfo": "tf.experimental.numpy.iinfo(\n    int_type\n)\n"
  },
  {
    "tf.experimental.numpy.imag": "tf.experimental.numpy.imag(\n    val\n)\n"
  },
  {
    "tf.experimental.numpy.inner": "tf.experimental.numpy.inner(\n    a, b\n)\n"
  },
  {
    "tf.experimental.numpy.int16": "tf.experimental.numpy.int16(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.int32": "tf.experimental.numpy.int32(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.int64": "tf.experimental.numpy.int64(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.int8": "tf.experimental.numpy.int8(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.int64": "tf.experimental.numpy.int64(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.isclose": "tf.experimental.numpy.isclose(\n    a, b, rtol=1e-05, atol=1e-08, equal_nan=False\n)\n"
  },
  {
    "tf.experimental.numpy.iscomplex": "tf.experimental.numpy.iscomplex(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.iscomplexobj": "tf.experimental.numpy.iscomplexobj(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.isfinite": "tf.experimental.numpy.isfinite(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.isinf": "tf.experimental.numpy.isinf(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.isnan": "tf.experimental.numpy.isnan(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.isneginf": "tf.experimental.numpy.isneginf(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.isposinf": "tf.experimental.numpy.isposinf(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.isreal": "tf.experimental.numpy.isreal(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.isrealobj": "tf.experimental.numpy.isrealobj(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.isscalar": "tf.experimental.numpy.isscalar(\n    num\n)\n"
  },
  {
    "tf.experimental.numpy.issubdtype": "tf.experimental.numpy.issubdtype(\n    arg1, arg2\n)\n"
  },
  {
    "tf.experimental.numpy.ix_": "tf.experimental.numpy.ix_(\n    *args\n)\n"
  },
  {
    "tf.experimental.numpy.kron": "tf.experimental.numpy.kron(\n    a, b\n)\n"
  },
  {
    "tf.experimental.numpy.lcm": "tf.experimental.numpy.lcm(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.less": "tf.experimental.numpy.less(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.less_equal": "tf.experimental.numpy.less_equal(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.linspace": "tf.experimental.numpy.linspace(\n    start, stop, num=50, endpoint=True, retstep=False, dtype=float, axis=0\n)\n"
  },
  {
    "tf.experimental.numpy.log": "tf.experimental.numpy.log(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.log10": "tf.experimental.numpy.log10(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.log1p": "tf.experimental.numpy.log1p(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.log2": "tf.experimental.numpy.log2(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.logaddexp": "tf.experimental.numpy.logaddexp(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.logaddexp2": "tf.experimental.numpy.logaddexp2(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.logical_and": "tf.experimental.numpy.logical_and(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.logical_not": "tf.experimental.numpy.logical_not(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.logical_or": "tf.experimental.numpy.logical_or(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.logical_xor": "tf.experimental.numpy.logical_xor(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.logspace": "tf.experimental.numpy.logspace(\n    start, stop, num=50, endpoint=True, base=10.0, dtype=None, axis=0\n)\n"
  },
  {
    "tf.experimental.numpy.matmul": "tf.experimental.numpy.matmul(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.max": "tf.experimental.numpy.max(\n    a, axis=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.maximum": "tf.experimental.numpy.maximum(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.mean": "tf.experimental.numpy.mean(\n    a, axis=None, dtype=None, out=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.meshgrid": "tf.experimental.numpy.meshgrid(\n    *xi, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.min": "tf.experimental.numpy.min(\n    a, axis=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.minimum": "tf.experimental.numpy.minimum(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.mod": "tf.experimental.numpy.mod(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.moveaxis": "tf.experimental.numpy.moveaxis(\n    a, source, destination\n)\n"
  },
  {
    "tf.experimental.numpy.multiply": "tf.experimental.numpy.multiply(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.nanmean": "tf.experimental.numpy.nanmean(\n    a, axis=None, dtype=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.nanprod": "tf.experimental.numpy.nanprod(\n    a, axis=None, dtype=None, keepdims=False\n)\n"
  },
  {
    "tf.experimental.numpy.nansum": "tf.experimental.numpy.nansum(\n    a, axis=None, dtype=None, keepdims=False\n)\n"
  },
  {
    "tf.Tensor": "tf.Tensor(\n    op, value_index, dtype\n)\n"
  },
  {
    "tf.experimental.numpy.ndim": "tf.experimental.numpy.ndim(\n    a\n)\n"
  },
  {
    "tf.experimental.numpy.negative": "tf.experimental.numpy.negative(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.nextafter": "tf.experimental.numpy.nextafter(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.nonzero": "tf.experimental.numpy.nonzero(\n    a\n)\n"
  },
  {
    "tf.experimental.numpy.not_equal": "tf.experimental.numpy.not_equal(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.object_": "tf.experimental.numpy.object_(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.ones": "tf.experimental.numpy.ones(\n    shape, dtype=float\n)\n"
  },
  {
    "tf.experimental.numpy.ones_like": "tf.experimental.numpy.ones_like(\n    a, dtype=None\n)\n"
  },
  {
    "tf.experimental.numpy.outer": "tf.experimental.numpy.outer(\n    a, b\n)\n"
  },
  {
    "tf.experimental.numpy.pad": "tf.experimental.numpy.pad(\n    array, pad_width, mode, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.polyval": "tf.experimental.numpy.polyval(\n    p, x\n)\n"
  },
  {
    "tf.experimental.numpy.positive": "tf.experimental.numpy.positive(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.power": "tf.experimental.numpy.power(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.prod": "tf.experimental.numpy.prod(\n    a, axis=None, dtype=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.promote_types": "tf.experimental.numpy.promote_types(\n    type1, type2\n)\n"
  },
  {
    "tf.experimental.numpy.ptp": "tf.experimental.numpy.ptp(\n    a, axis=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.rad2deg": "tf.experimental.numpy.rad2deg(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.random.poisson": "tf.experimental.numpy.random.poisson(\n    lam=1.0, size=None\n)\n"
  },
  {
    "tf.experimental.numpy.random.rand": "tf.experimental.numpy.random.rand(\n    *size\n)\n"
  },
  {
    "tf.experimental.numpy.random.randint": "tf.experimental.numpy.random.randint(\n    low,\n    high=None,\n    size=None,\n    dtype=tf.experimental.numpy.int64\n)\n"
  },
  {
    "tf.experimental.numpy.random.randn": "tf.experimental.numpy.random.randn(\n    *args\n)\n"
  },
  {
    "tf.experimental.numpy.random.random": "tf.experimental.numpy.random.random(\n    size=None\n)\n"
  },
  {
    "tf.experimental.numpy.random.seed": "tf.experimental.numpy.random.seed(\n    s\n)\n"
  },
  {
    "tf.experimental.numpy.random.standard_normal": "tf.experimental.numpy.random.standard_normal(\n    size=None\n)\n"
  },
  {
    "tf.experimental.numpy.random.uniform": "tf.experimental.numpy.random.uniform(\n    low=0.0, high=1.0, size=None\n)\n"
  },
  {
    "tf.experimental.numpy.ravel": "tf.experimental.numpy.ravel(\n    a\n)\n"
  },
  {
    "tf.experimental.numpy.real": "tf.experimental.numpy.real(\n    val\n)\n"
  },
  {
    "tf.experimental.numpy.reciprocal": "tf.experimental.numpy.reciprocal(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.remainder": "tf.experimental.numpy.remainder(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.repeat": "tf.experimental.numpy.repeat(\n    a, repeats, axis=None\n)\n"
  },
  {
    "tf.experimental.numpy.reshape": "tf.experimental.numpy.reshape(\n    a, newshape, order='C'\n)\n"
  },
  {
    "tf.experimental.numpy.result_type": "tf.experimental.numpy.result_type(\n    *arrays_and_dtypes\n)\n"
  },
  {
    "tf.experimental.numpy.roll": "tf.experimental.numpy.roll(\n    a, shift, axis=None\n)\n"
  },
  {
    "tf.experimental.numpy.rot90": "tf.experimental.numpy.rot90(\n    m, k=1, axes=(0, 1)\n)\n"
  },
  {
    "tf.experimental.numpy.round": "tf.experimental.numpy.round(\n    a, decimals=0\n)\n"
  },
  {
    "tf.experimental.numpy.select": "tf.experimental.numpy.select(\n    condlist, choicelist, default=0\n)\n"
  },
  {
    "tf.experimental.numpy.shape": "tf.experimental.numpy.shape(\n    a\n)\n"
  },
  {
    "tf.experimental.numpy.sign": "tf.experimental.numpy.sign(\n    x, out=None, where=None, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.signbit": "tf.experimental.numpy.signbit(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.sin": "tf.experimental.numpy.sin(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.sinc": "tf.experimental.numpy.sinc(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.sinh": "tf.experimental.numpy.sinh(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.size": "tf.experimental.numpy.size(\n    x, axis=None\n)\n"
  },
  {
    "tf.experimental.numpy.sort": "tf.experimental.numpy.sort(\n    a, axis=-1, kind='quicksort', order=None\n)\n"
  },
  {
    "tf.experimental.numpy.split": "tf.experimental.numpy.split(\n    ary, indices_or_sections, axis=0\n)\n"
  },
  {
    "tf.experimental.numpy.sqrt": "tf.experimental.numpy.sqrt(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.square": "tf.experimental.numpy.square(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.squeeze": "tf.experimental.numpy.squeeze(\n    a, axis=None\n)\n"
  },
  {
    "tf.experimental.numpy.stack": "tf.experimental.numpy.stack(\n    arrays, axis=0\n)\n"
  },
  {
    "tf.experimental.numpy.std": "tf.experimental.numpy.std(\n    a, axis=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.string_": "tf.experimental.numpy.string_(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.subtract": "tf.experimental.numpy.subtract(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.sum": "tf.experimental.numpy.sum(\n    a, axis=None, dtype=None, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.swapaxes": "tf.experimental.numpy.swapaxes(\n    a, axis1, axis2\n)\n"
  },
  {
    "tf.experimental.numpy.take": "tf.experimental.numpy.take(\n    a, indices, axis=None, out=None, mode='clip'\n)\n"
  },
  {
    "tf.experimental.numpy.take_along_axis": "tf.experimental.numpy.take_along_axis(\n    arr, indices, axis\n)\n"
  },
  {
    "tf.experimental.numpy.tan": "tf.experimental.numpy.tan(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.tanh": "tf.experimental.numpy.tanh(\n    x\n)\n"
  },
  {
    "tf.experimental.numpy.tensordot": "tf.experimental.numpy.tensordot(\n    a, b, axes=2\n)\n"
  },
  {
    "tf.experimental.numpy.tile": "tf.experimental.numpy.tile(\n    a, reps\n)\n"
  },
  {
    "tf.experimental.numpy.trace": "tf.experimental.numpy.trace(\n    a, offset=0, axis1=0, axis2=1, dtype=None\n)\n"
  },
  {
    "tf.experimental.numpy.transpose": "tf.experimental.numpy.transpose(\n    a, axes=None\n)\n"
  },
  {
    "tf.experimental.numpy.tri": "tf.experimental.numpy.tri(\n    N, M=None, k=0, dtype=None\n)\n"
  },
  {
    "tf.experimental.numpy.tril": "tf.experimental.numpy.tril(\n    m, k=0\n)\n"
  },
  {
    "tf.experimental.numpy.triu": "tf.experimental.numpy.triu(\n    m, k=0\n)\n"
  },
  {
    "tf.experimental.numpy.true_divide": "tf.experimental.numpy.true_divide(\n    x1, x2\n)\n"
  },
  {
    "tf.experimental.numpy.uint16": "tf.experimental.numpy.uint16(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.uint32": "tf.experimental.numpy.uint32(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.uint64": "tf.experimental.numpy.uint64(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.uint8": "tf.experimental.numpy.uint8(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.unicode_": "tf.experimental.numpy.unicode_(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.experimental.numpy.vander": "tf.experimental.numpy.vander(\n    x, N=None, increasing=False\n)\n"
  },
  {
    "tf.experimental.numpy.var": "tf.experimental.numpy.var(\n    a, axis=None, dtype=None, out=None, ddof=0, keepdims=None\n)\n"
  },
  {
    "tf.experimental.numpy.vdot": "tf.experimental.numpy.vdot(\n    a, b\n)\n"
  },
  {
    "tf.experimental.numpy.vsplit": "tf.experimental.numpy.vsplit(\n    ary, indices_or_sections\n)\n"
  },
  {
    "tf.experimental.numpy.vstack": "tf.experimental.numpy.vstack(\n    tup\n)\n"
  },
  {
    "tf.experimental.numpy.where": "tf.experimental.numpy.where(\n    condition, x=None, y=None\n)\n"
  },
  {
    "tf.experimental.numpy.zeros": "tf.experimental.numpy.zeros(\n    shape, dtype=float\n)\n"
  },
  {
    "tf.experimental.numpy.zeros_like": "tf.experimental.numpy.zeros_like(\n    a, dtype=None\n)\n"
  },
  {
    "tf.experimental.register_filesystem_plugin": "tf.experimental.register_filesystem_plugin(\n    plugin_location\n)\n"
  },
  {
    "tf.experimental.tensorrt.ConversionParams": "tf.experimental.tensorrt.ConversionParams(\n    max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,\n    precision_mode=TrtPrecisionMode.FP32,\n    minimum_segment_size=3,\n    maximum_cached_engines=1,\n    use_calibration=True,\n    allow_build_at_runtime=True\n)\n"
  },
  {
    "tf.experimental.tensorrt.Converter": "tf.experimental.tensorrt.Converter(\n    input_saved_model_dir=None,\n    input_saved_model_tags=None,\n    input_saved_model_signature_key=None,\n    use_dynamic_shape=None,\n    dynamic_shape_profile_strategy=None,\n    max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,\n    precision_mode=TrtPrecisionMode.FP32,\n    minimum_segment_size=3,\n    maximum_cached_engines=1,\n    use_calibration=True,\n    allow_build_at_runtime=True,\n    conversion_params=None\n)\n"
  },
  {
    "tf.experimental.unregister_dispatch_for": "tf.experimental.unregister_dispatch_for(\n    dispatch_target\n)\n"
  },
  {
    "tf.extract_volume_patches": "tf.extract_volume_patches(\n    input, ksizes, strides, padding, name=None\n)\n"
  },
  {
    "tf.eye": "tf.eye(\n    num_rows,\n    num_columns=None,\n    batch_shape=None,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.feature_column.bucketized_column": "tf.feature_column.bucketized_column(\n    source_column, boundaries\n)\n"
  },
  {
    "tf.feature_column.categorical_column_with_hash_bucket": "tf.feature_column.categorical_column_with_hash_bucket(\n    key,\n    hash_bucket_size,\n    dtype=tf.dtypes.string\n)\n"
  },
  {
    "tf.feature_column.categorical_column_with_identity": "tf.feature_column.categorical_column_with_identity(\n    key, num_buckets, default_value=None\n)\n"
  },
  {
    "tf.feature_column.categorical_column_with_vocabulary_file": "tf.feature_column.categorical_column_with_vocabulary_file(\n    key,\n    vocabulary_file,\n    vocabulary_size=None,\n    dtype=tf.dtypes.string,\n    default_value=None,\n    num_oov_buckets=0,\n    file_format=None\n)\n"
  },
  {
    "tf.feature_column.categorical_column_with_vocabulary_list": "tf.feature_column.categorical_column_with_vocabulary_list(\n    key, vocabulary_list, dtype=None, default_value=-1, num_oov_buckets=0\n)\n"
  },
  {
    "tf.feature_column.crossed_column": "tf.feature_column.crossed_column(\n    keys, hash_bucket_size, hash_key=None\n)\n"
  },
  {
    "tf.feature_column.embedding_column": "tf.feature_column.embedding_column(\n    categorical_column,\n    dimension,\n    combiner='mean',\n    initializer=None,\n    ckpt_to_load_from=None,\n    tensor_name_in_ckpt=None,\n    max_norm=None,\n    trainable=True,\n    use_safe_embedding_lookup=True\n)\n"
  },
  {
    "tf.feature_column.indicator_column": "tf.feature_column.indicator_column(\n    categorical_column\n)\n"
  },
  {
    "tf.feature_column.make_parse_example_spec": "tf.feature_column.make_parse_example_spec(\n    feature_columns\n)\n"
  },
  {
    "tf.feature_column.numeric_column": "tf.feature_column.numeric_column(\n    key,\n    shape=(1,),\n    default_value=None,\n    dtype=tf.dtypes.float32,\n    normalizer_fn=None\n)\n"
  },
  {
    "tf.feature_column.sequence_categorical_column_with_hash_bucket": "tf.feature_column.sequence_categorical_column_with_hash_bucket(\n    key,\n    hash_bucket_size,\n    dtype=tf.dtypes.string\n)\n"
  },
  {
    "tf.feature_column.sequence_categorical_column_with_identity": "tf.feature_column.sequence_categorical_column_with_identity(\n    key, num_buckets, default_value=None\n)\n"
  },
  {
    "tf.feature_column.sequence_categorical_column_with_vocabulary_file": "tf.feature_column.sequence_categorical_column_with_vocabulary_file(\n    key,\n    vocabulary_file,\n    vocabulary_size=None,\n    num_oov_buckets=0,\n    default_value=None,\n    dtype=tf.dtypes.string\n)\n"
  },
  {
    "tf.feature_column.sequence_categorical_column_with_vocabulary_list": "tf.feature_column.sequence_categorical_column_with_vocabulary_list(\n    key, vocabulary_list, dtype=None, default_value=-1, num_oov_buckets=0\n)\n"
  },
  {
    "tf.feature_column.sequence_numeric_column": "tf.feature_column.sequence_numeric_column(\n    key,\n    shape=(1,),\n    default_value=0.0,\n    dtype=tf.dtypes.float32,\n    normalizer_fn=None\n)\n"
  },
  {
    "tf.feature_column.shared_embeddings": "tf.feature_column.shared_embeddings(\n    categorical_columns,\n    dimension,\n    combiner='mean',\n    initializer=None,\n    shared_embedding_collection_name=None,\n    ckpt_to_load_from=None,\n    tensor_name_in_ckpt=None,\n    max_norm=None,\n    trainable=True,\n    use_safe_embedding_lookup=True\n)\n"
  },
  {
    "tf.feature_column.weighted_categorical_column": "tf.feature_column.weighted_categorical_column(\n    categorical_column,\n    weight_feature_key,\n    dtype=tf.dtypes.float32\n)\n"
  },
  {
    "tf.fill": "tf.fill(\n    dims, value, name=None\n)\n"
  },
  {
    "tf.fingerprint": "tf.fingerprint(\n    data, method='farmhash64', name=None\n)\n"
  },
  {
    "tf.math.floor": "tf.math.floor(\n    x, name=None\n)\n"
  },
  {
    "tf.foldl": "tf.foldl(\n    fn,\n    elems,\n    initializer=None,\n    parallel_iterations=10,\n    back_prop=True,\n    swap_memory=False,\n    name=None\n)\n"
  },
  {
    "tf.foldr": "tf.foldr(\n    fn,\n    elems,\n    initializer=None,\n    parallel_iterations=10,\n    back_prop=True,\n    swap_memory=False,\n    name=None\n)\n"
  },
  {
    "tf.function": "tf.function(\n    func=None,\n    input_signature=None,\n    autograph=True,\n    jit_compile=None,\n    reduce_retracing=False,\n    experimental_implements=None,\n    experimental_autograph_options=None,\n    experimental_relax_shapes=None,\n    experimental_compile=None,\n    experimental_follow_type_hints=None\n) -> tf.types.experimental.GenericFunction\n"
  },
  {
    "tf.gather": "tf.gather(\n    params, indices, validate_indices=None, axis=None, batch_dims=0, name=None\n)\n"
  },
  {
    "tf.gather_nd": "tf.gather_nd(\n    params, indices, batch_dims=0, name=None\n)\n"
  },
  {
    "tf.get_static_value": "tf.get_static_value(\n    tensor, partial=False\n)\n"
  },
  {
    "tf.grad_pass_through": "tf.grad_pass_through(\n    f\n)\n"
  },
  {
    "tf.gradients": "tf.gradients(\n    ys,\n    xs,\n    grad_ys=None,\n    name='gradients',\n    gate_gradients=False,\n    aggregation_method=None,\n    stop_gradients=None,\n    unconnected_gradients=tf.UnconnectedGradients.NONE\n)\n"
  },
  {
    "tf.graph_util.import_graph_def": "tf.graph_util.import_graph_def(\n    graph_def,\n    input_map=None,\n    return_elements=None,\n    name=None,\n    op_dict=None,\n    producer_op_list=None\n)\n"
  },
  {
    "tf.math.greater": "tf.math.greater(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.greater_equal": "tf.math.greater_equal(\n    x, y, name=None\n)\n"
  },
  {
    "tf.group": "tf.group(\n    *inputs, **kwargs\n)\n"
  },
  {
    "tf.guarantee_const": "tf.guarantee_const(\n    input, name=None\n)\n"
  },
  {
    "tf.hessians": "tf.hessians(\n    ys,\n    xs,\n    gate_gradients=False,\n    aggregation_method=None,\n    name='hessians'\n)\n"
  },
  {
    "tf.histogram_fixed_width": "tf.histogram_fixed_width(\n    values,\n    value_range,\n    nbins=100,\n    dtype=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.histogram_fixed_width_bins": "tf.histogram_fixed_width_bins(\n    values,\n    value_range,\n    nbins=100,\n    dtype=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.identity": "tf.identity(\n    input, name=None\n)\n"
  },
  {
    "tf.identity_n": "tf.identity_n(\n    input, name=None\n)\n"
  },
  {
    "tf.image.adjust_brightness": "tf.image.adjust_brightness(\n    image, delta\n)\n"
  },
  {
    "tf.image.adjust_contrast": "tf.image.adjust_contrast(\n    images, contrast_factor\n)\n"
  },
  {
    "tf.image.adjust_gamma": "tf.image.adjust_gamma(\n    image, gamma=1, gain=1\n)\n"
  },
  {
    "tf.image.adjust_hue": "tf.image.adjust_hue(\n    image, delta, name=None\n)\n"
  },
  {
    "tf.image.adjust_jpeg_quality": "tf.image.adjust_jpeg_quality(\n    image, jpeg_quality, name=None\n)\n"
  },
  {
    "tf.image.adjust_saturation": "tf.image.adjust_saturation(\n    image, saturation_factor, name=None\n)\n"
  },
  {
    "tf.image.central_crop": "tf.image.central_crop(\n    image, central_fraction\n)\n"
  },
  {
    "tf.image.combined_non_max_suppression": "tf.image.combined_non_max_suppression(\n    boxes,\n    scores,\n    max_output_size_per_class,\n    max_total_size,\n    iou_threshold=0.5,\n    score_threshold=float('-inf'),\n    pad_per_class=False,\n    clip_boxes=True,\n    name=None\n)\n"
  },
  {
    "tf.image.convert_image_dtype": "tf.image.convert_image_dtype(\n    image, dtype, saturate=False, name=None\n)\n"
  },
  {
    "tf.image.crop_and_resize": "tf.image.crop_and_resize(\n    image,\n    boxes,\n    box_indices,\n    crop_size,\n    method='bilinear',\n    extrapolation_value=0.0,\n    name=None\n)\n"
  },
  {
    "tf.image.crop_to_bounding_box": "tf.image.crop_to_bounding_box(\n    image, offset_height, offset_width, target_height, target_width\n)\n"
  },
  {
    "tf.io.decode_and_crop_jpeg": "tf.io.decode_and_crop_jpeg(\n    contents,\n    crop_window,\n    channels=0,\n    ratio=1,\n    fancy_upscaling=True,\n    try_recover_truncated=False,\n    acceptable_fraction=1,\n    dct_method='',\n    name=None\n)\n"
  },
  {
    "tf.io.decode_bmp": "tf.io.decode_bmp(\n    contents, channels=0, name=None\n)\n"
  },
  {
    "tf.io.decode_gif": "tf.io.decode_gif(\n    contents, name=None\n)\n"
  },
  {
    "tf.io.decode_image": "tf.io.decode_image(\n    contents,\n    channels=None,\n    dtype=tf.dtypes.uint8,\n    name=None,\n    expand_animations=True\n)\n"
  },
  {
    "tf.io.decode_jpeg": "tf.io.decode_jpeg(\n    contents,\n    channels=0,\n    ratio=1,\n    fancy_upscaling=True,\n    try_recover_truncated=False,\n    acceptable_fraction=1,\n    dct_method='',\n    name=None\n)\n"
  },
  {
    "tf.io.decode_png": "tf.io.decode_png(\n    contents,\n    channels=0,\n    dtype=tf.dtypes.uint8,\n    name=None\n)\n"
  },
  {
    "tf.image.draw_bounding_boxes": "tf.image.draw_bounding_boxes(\n    images, boxes, colors, name=None\n)\n"
  },
  {
    "tf.io.encode_jpeg": "tf.io.encode_jpeg(\n    image,\n    format='',\n    quality=95,\n    progressive=False,\n    optimize_size=False,\n    chroma_downsampling=True,\n    density_unit='in',\n    x_density=300,\n    y_density=300,\n    xmp_metadata='',\n    name=None\n)\n"
  },
  {
    "tf.io.encode_png": "tf.io.encode_png(\n    image, compression=-1, name=None\n)\n"
  },
  {
    "tf.image.extract_glimpse": "tf.image.extract_glimpse(\n    input,\n    size,\n    offsets,\n    centered=True,\n    normalized=True,\n    noise='uniform',\n    name=None\n)\n"
  },
  {
    "tf.io.extract_jpeg_shape": "tf.io.extract_jpeg_shape(\n    contents,\n    output_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.image.extract_patches": "tf.image.extract_patches(\n    images, sizes, strides, rates, padding, name=None\n)\n"
  },
  {
    "tf.image.flip_left_right": "tf.image.flip_left_right(\n    image\n)\n"
  },
  {
    "tf.image.flip_up_down": "tf.image.flip_up_down(\n    image\n)\n"
  },
  {
    "tf.image.generate_bounding_box_proposals": "tf.image.generate_bounding_box_proposals(\n    scores,\n    bbox_deltas,\n    image_info,\n    anchors,\n    nms_threshold=0.7,\n    pre_nms_topn=6000,\n    min_size=16,\n    post_nms_topn=300,\n    name=None\n)\n"
  },
  {
    "tf.image.grayscale_to_rgb": "tf.image.grayscale_to_rgb(\n    images, name=None\n)\n"
  },
  {
    "tf.image.hsv_to_rgb": "tf.image.hsv_to_rgb(\n    images, name=None\n)\n"
  },
  {
    "tf.image.image_gradients": "tf.image.image_gradients(\n    image\n)\n"
  },
  {
    "tf.io.is_jpeg": "tf.io.is_jpeg(\n    contents, name=None\n)\n"
  },
  {
    "tf.image.non_max_suppression": "tf.image.non_max_suppression(\n    boxes,\n    scores,\n    max_output_size,\n    iou_threshold=0.5,\n    score_threshold=float('-inf'),\n    name=None\n)\n"
  },
  {
    "tf.image.non_max_suppression_overlaps": "tf.image.non_max_suppression_overlaps(\n    overlaps,\n    scores,\n    max_output_size,\n    overlap_threshold=0.5,\n    score_threshold=float('-inf'),\n    name=None\n)\n"
  },
  {
    "tf.image.non_max_suppression_padded": "tf.image.non_max_suppression_padded(\n    boxes,\n    scores,\n    max_output_size,\n    iou_threshold=0.5,\n    score_threshold=float('-inf'),\n    pad_to_max_output_size=False,\n    name=None,\n    sorted_input=False,\n    canonicalized_coordinates=False,\n    tile_size=512\n)\n"
  },
  {
    "tf.image.non_max_suppression_with_scores": "tf.image.non_max_suppression_with_scores(\n    boxes,\n    scores,\n    max_output_size,\n    iou_threshold=0.5,\n    score_threshold=float('-inf'),\n    soft_nms_sigma=0.0,\n    name=None\n)\n"
  },
  {
    "tf.image.pad_to_bounding_box": "tf.image.pad_to_bounding_box(\n    image, offset_height, offset_width, target_height, target_width\n)\n"
  },
  {
    "tf.image.per_image_standardization": "tf.image.per_image_standardization(\n    image\n)\n"
  },
  {
    "tf.image.psnr": "tf.image.psnr(\n    a, b, max_val, name=None\n)\n"
  },
  {
    "tf.image.random_brightness": "tf.image.random_brightness(\n    image, max_delta, seed=None\n)\n"
  },
  {
    "tf.image.random_contrast": "tf.image.random_contrast(\n    image, lower, upper, seed=None\n)\n"
  },
  {
    "tf.image.random_crop": "tf.image.random_crop(\n    value, size, seed=None, name=None\n)\n"
  },
  {
    "tf.image.random_flip_left_right": "tf.image.random_flip_left_right(\n    image, seed=None\n)\n"
  },
  {
    "tf.image.random_flip_up_down": "tf.image.random_flip_up_down(\n    image, seed=None\n)\n"
  },
  {
    "tf.image.random_hue": "tf.image.random_hue(\n    image, max_delta, seed=None\n)\n"
  },
  {
    "tf.image.random_jpeg_quality": "tf.image.random_jpeg_quality(\n    image, min_jpeg_quality, max_jpeg_quality, seed=None\n)\n"
  },
  {
    "tf.image.random_saturation": "tf.image.random_saturation(\n    image, lower, upper, seed=None\n)\n"
  },
  {
    "tf.image.resize": "tf.image.resize(\n    images,\n    size,\n    method=ResizeMethod.BILINEAR,\n    preserve_aspect_ratio=False,\n    antialias=False,\n    name=None\n)\n"
  },
  {
    "tf.image.resize_with_crop_or_pad": "tf.image.resize_with_crop_or_pad(\n    image, target_height, target_width\n)\n"
  },
  {
    "tf.image.resize_with_pad": "tf.image.resize_with_pad(\n    image,\n    target_height,\n    target_width,\n    method=ResizeMethod.BILINEAR,\n    antialias=False\n)\n"
  },
  {
    "tf.image.rgb_to_grayscale": "tf.image.rgb_to_grayscale(\n    images, name=None\n)\n"
  },
  {
    "tf.image.rgb_to_hsv": "tf.image.rgb_to_hsv(\n    images, name=None\n)\n"
  },
  {
    "tf.image.rgb_to_yiq": "tf.image.rgb_to_yiq(\n    images\n)\n"
  },
  {
    "tf.image.rgb_to_yuv": "tf.image.rgb_to_yuv(\n    images\n)\n"
  },
  {
    "tf.image.rot90": "tf.image.rot90(\n    image, k=1, name=None\n)\n"
  },
  {
    "tf.image.sample_distorted_bounding_box": "tf.image.sample_distorted_bounding_box(\n    image_size,\n    bounding_boxes,\n    seed=0,\n    min_object_covered=0.1,\n    aspect_ratio_range=None,\n    area_range=None,\n    max_attempts=None,\n    use_image_if_no_bounding_boxes=None,\n    name=None\n)\n"
  },
  {
    "tf.image.sobel_edges": "tf.image.sobel_edges(\n    image\n)\n"
  },
  {
    "tf.image.ssim": "tf.image.ssim(\n    img1,\n    img2,\n    max_val,\n    filter_size=11,\n    filter_sigma=1.5,\n    k1=0.01,\n    k2=0.03,\n    return_index_map=False\n)\n"
  },
  {
    "tf.image.ssim_multiscale": "tf.image.ssim_multiscale(\n    img1,\n    img2,\n    max_val,\n    power_factors=_MSSSIM_WEIGHTS,\n    filter_size=11,\n    filter_sigma=1.5,\n    k1=0.01,\n    k2=0.03\n)\n"
  },
  {
    "tf.image.stateless_random_brightness": "tf.image.stateless_random_brightness(\n    image, max_delta, seed\n)\n"
  },
  {
    "tf.image.stateless_random_contrast": "tf.image.stateless_random_contrast(\n    image, lower, upper, seed\n)\n"
  },
  {
    "tf.image.stateless_random_crop": "tf.image.stateless_random_crop(\n    value, size, seed, name=None\n)\n"
  },
  {
    "tf.image.stateless_random_flip_left_right": "tf.image.stateless_random_flip_left_right(\n    image, seed\n)\n"
  },
  {
    "tf.image.stateless_random_flip_up_down": "tf.image.stateless_random_flip_up_down(\n    image, seed\n)\n"
  },
  {
    "tf.image.stateless_random_hue": "tf.image.stateless_random_hue(\n    image, max_delta, seed\n)\n"
  },
  {
    "tf.image.stateless_random_jpeg_quality": "tf.image.stateless_random_jpeg_quality(\n    image, min_jpeg_quality, max_jpeg_quality, seed\n)\n"
  },
  {
    "tf.image.stateless_random_saturation": "tf.image.stateless_random_saturation(\n    image, lower, upper, seed=None\n)\n"
  },
  {
    "tf.image.stateless_sample_distorted_bounding_box": "tf.image.stateless_sample_distorted_bounding_box(\n    image_size,\n    bounding_boxes,\n    seed,\n    min_object_covered=0.1,\n    aspect_ratio_range=None,\n    area_range=None,\n    max_attempts=None,\n    use_image_if_no_bounding_boxes=None,\n    name=None\n)\n"
  },
  {
    "tf.image.total_variation": "tf.image.total_variation(\n    images, name=None\n)\n"
  },
  {
    "tf.image.transpose": "tf.image.transpose(\n    image, name=None\n)\n"
  },
  {
    "tf.image.yiq_to_rgb": "tf.image.yiq_to_rgb(\n    images\n)\n"
  },
  {
    "tf.image.yuv_to_rgb": "tf.image.yuv_to_rgb(\n    images\n)\n"
  },
  {
    "tf.graph_util.import_graph_def": "tf.graph_util.import_graph_def(\n    graph_def,\n    input_map=None,\n    return_elements=None,\n    name=None,\n    op_dict=None,\n    producer_op_list=None\n)\n"
  },
  {
    "tf.keras.initializers.Constant": "tf.keras.initializers.Constant(\n    value=0\n)\n"
  },
  {
    "tf.keras.initializers.GlorotNormal": "tf.keras.initializers.GlorotNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.GlorotUniform": "tf.keras.initializers.GlorotUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.HeNormal": "tf.keras.initializers.HeNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.HeUniform": "tf.keras.initializers.HeUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.Identity": "tf.keras.initializers.Identity(\n    gain=1.0\n)\n"
  },
  {
    "tf.keras.initializers.LecunNormal": "tf.keras.initializers.LecunNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.LecunUniform": "tf.keras.initializers.LecunUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.Orthogonal": "tf.keras.initializers.Orthogonal(\n    gain=1.0, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.RandomNormal": "tf.keras.initializers.RandomNormal(\n    mean=0.0, stddev=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.RandomUniform": "tf.keras.initializers.RandomUniform(\n    minval=-0.05, maxval=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.TruncatedNormal": "tf.keras.initializers.TruncatedNormal(\n    mean=0.0, stddev=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.VarianceScaling": "tf.keras.initializers.VarianceScaling(\n    scale=1.0,\n    mode='fan_in',\n    distribution='truncated_normal',\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.Constant": "tf.keras.initializers.Constant(\n    value=0\n)\n"
  },
  {
    "tf.keras.initializers.deserialize": "tf.keras.initializers.deserialize(\n    config, custom_objects=None\n)\n"
  },
  {
    "tf.keras.initializers.get": "tf.keras.initializers.get(\n    identifier\n)\n"
  },
  {
    "tf.keras.initializers.GlorotNormal": "tf.keras.initializers.GlorotNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.GlorotUniform": "tf.keras.initializers.GlorotUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.HeNormal": "tf.keras.initializers.HeNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.HeUniform": "tf.keras.initializers.HeUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.Identity": "tf.keras.initializers.Identity(\n    gain=1.0\n)\n"
  },
  {
    "tf.keras.initializers.LecunNormal": "tf.keras.initializers.LecunNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.LecunUniform": "tf.keras.initializers.LecunUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.Orthogonal": "tf.keras.initializers.Orthogonal(\n    gain=1.0, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.RandomNormal": "tf.keras.initializers.RandomNormal(\n    mean=0.0, stddev=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.RandomUniform": "tf.keras.initializers.RandomUniform(\n    minval=-0.05, maxval=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.serialize": "tf.keras.initializers.serialize(\n    initializer\n)\n"
  },
  {
    "tf.keras.initializers.TruncatedNormal": "tf.keras.initializers.TruncatedNormal(\n    mean=0.0, stddev=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.VarianceScaling": "tf.keras.initializers.VarianceScaling(\n    scale=1.0,\n    mode='fan_in',\n    distribution='truncated_normal',\n    seed=None\n)\n"
  },
  {
    "tf.io.FixedLenFeature": "tf.io.FixedLenFeature(\n    shape, dtype, default_value=None\n)\n"
  },
  {
    "tf.io.FixedLenSequenceFeature": "tf.io.FixedLenSequenceFeature(\n    shape, dtype, allow_missing=False, default_value=None\n)\n"
  },
  {
    "tf.io.RaggedFeature": "tf.io.RaggedFeature(\n    dtype,\n    value_key=None,\n    partitions=(),\n    row_splits_dtype=tf.dtypes.int32,\n    validate=False\n)\n"
  },
  {
    "tf.io.RaggedFeature.RowLengths": "tf.io.RaggedFeature.RowLengths(\n    key\n)\n"
  },
  {
    "tf.io.RaggedFeature.RowLimits": "tf.io.RaggedFeature.RowLimits(\n    key\n)\n"
  },
  {
    "tf.io.RaggedFeature.RowSplits": "tf.io.RaggedFeature.RowSplits(\n    key\n)\n"
  },
  {
    "tf.io.RaggedFeature.RowStarts": "tf.io.RaggedFeature.RowStarts(\n    key\n)\n"
  },
  {
    "tf.io.RaggedFeature.UniformRowLength": "tf.io.RaggedFeature.UniformRowLength(\n    length\n)\n"
  },
  {
    "tf.io.RaggedFeature.ValueRowIds": "tf.io.RaggedFeature.ValueRowIds(\n    key\n)\n"
  },
  {
    "tf.io.SparseFeature": "tf.io.SparseFeature(\n    index_key, value_key, dtype, size, already_sorted=False\n)\n"
  },
  {
    "tf.io.TFRecordOptions": "tf.io.TFRecordOptions(\n    compression_type=None,\n    flush_mode=None,\n    input_buffer_size=None,\n    output_buffer_size=None,\n    window_bits=None,\n    compression_level=None,\n    compression_method=None,\n    mem_level=None,\n    compression_strategy=None\n)\n"
  },
  {
    "tf.io.TFRecordWriter": "tf.io.TFRecordWriter(\n    path, options=None\n)\n"
  },
  {
    "tf.io.VarLenFeature": "tf.io.VarLenFeature(\n    dtype\n)\n"
  },
  {
    "tf.io.decode_and_crop_jpeg": "tf.io.decode_and_crop_jpeg(\n    contents,\n    crop_window,\n    channels=0,\n    ratio=1,\n    fancy_upscaling=True,\n    try_recover_truncated=False,\n    acceptable_fraction=1,\n    dct_method='',\n    name=None\n)\n"
  },
  {
    "tf.io.decode_base64": "tf.io.decode_base64(\n    input, name=None\n)\n"
  },
  {
    "tf.io.decode_bmp": "tf.io.decode_bmp(\n    contents, channels=0, name=None\n)\n"
  },
  {
    "tf.io.decode_compressed": "tf.io.decode_compressed(\n    bytes, compression_type='', name=None\n)\n"
  },
  {
    "tf.io.decode_csv": "tf.io.decode_csv(\n    records,\n    record_defaults,\n    field_delim=',',\n    use_quote_delim=True,\n    na_value='',\n    select_cols=None,\n    name=None\n)\n"
  },
  {
    "tf.io.decode_gif": "tf.io.decode_gif(\n    contents, name=None\n)\n"
  },
  {
    "tf.io.decode_image": "tf.io.decode_image(\n    contents,\n    channels=None,\n    dtype=tf.dtypes.uint8,\n    name=None,\n    expand_animations=True\n)\n"
  },
  {
    "tf.io.decode_jpeg": "tf.io.decode_jpeg(\n    contents,\n    channels=0,\n    ratio=1,\n    fancy_upscaling=True,\n    try_recover_truncated=False,\n    acceptable_fraction=1,\n    dct_method='',\n    name=None\n)\n"
  },
  {
    "tf.io.decode_json_example": "tf.io.decode_json_example(\n    json_examples, name=None\n)\n"
  },
  {
    "tf.io.decode_png": "tf.io.decode_png(\n    contents,\n    channels=0,\n    dtype=tf.dtypes.uint8,\n    name=None\n)\n"
  },
  {
    "tf.io.decode_proto": "tf.io.decode_proto(\n    bytes,\n    message_type,\n    field_names,\n    output_types,\n    descriptor_source='local://',\n    message_format='binary',\n    sanitize=False,\n    name=None\n)\n"
  },
  {
    "tf.io.decode_raw": "tf.io.decode_raw(\n    input_bytes, out_type, little_endian=True, fixed_length=None, name=None\n)\n"
  },
  {
    "tf.io.deserialize_many_sparse": "tf.io.deserialize_many_sparse(\n    serialized_sparse, dtype, rank=None, name=None\n)\n"
  },
  {
    "tf.io.encode_base64": "tf.io.encode_base64(\n    input, pad=False, name=None\n)\n"
  },
  {
    "tf.io.encode_jpeg": "tf.io.encode_jpeg(\n    image,\n    format='',\n    quality=95,\n    progressive=False,\n    optimize_size=False,\n    chroma_downsampling=True,\n    density_unit='in',\n    x_density=300,\n    y_density=300,\n    xmp_metadata='',\n    name=None\n)\n"
  },
  {
    "tf.io.encode_png": "tf.io.encode_png(\n    image, compression=-1, name=None\n)\n"
  },
  {
    "tf.io.encode_proto": "tf.io.encode_proto(\n    sizes,\n    values,\n    field_names,\n    message_type,\n    descriptor_source='local://',\n    name=None\n)\n"
  },
  {
    "tf.io.extract_jpeg_shape": "tf.io.extract_jpeg_shape(\n    contents,\n    output_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.io.gfile.GFile": "tf.io.gfile.GFile(\n    name, mode='r'\n)\n"
  },
  {
    "tf.io.gfile.copy": "tf.io.gfile.copy(\n    src, dst, overwrite=False\n)\n"
  },
  {
    "tf.io.gfile.exists": "tf.io.gfile.exists(\n    path\n)\n"
  },
  {
    "tf.io.gfile.glob": "tf.io.gfile.glob(\n    pattern\n)\n"
  },
  {
    "tf.io.gfile.isdir": "tf.io.gfile.isdir(\n    path\n)\n"
  },
  {
    "tf.io.gfile.join": "tf.io.gfile.join(\n    path, *paths\n)\n"
  },
  {
    "tf.io.gfile.listdir": "tf.io.gfile.listdir(\n    path\n)\n"
  },
  {
    "tf.io.gfile.makedirs": "tf.io.gfile.makedirs(\n    path\n)\n"
  },
  {
    "tf.io.gfile.mkdir": "tf.io.gfile.mkdir(\n    path\n)\n"
  },
  {
    "tf.io.gfile.remove": "tf.io.gfile.remove(\n    path\n)\n"
  },
  {
    "tf.io.gfile.rename": "tf.io.gfile.rename(\n    src, dst, overwrite=False\n)\n"
  },
  {
    "tf.io.gfile.rmtree": "tf.io.gfile.rmtree(\n    path\n)\n"
  },
  {
    "tf.io.gfile.stat": "tf.io.gfile.stat(\n    path\n)\n"
  },
  {
    "tf.io.gfile.walk": "tf.io.gfile.walk(\n    top, topdown=True, onerror=None\n)\n"
  },
  {
    "tf.io.is_jpeg": "tf.io.is_jpeg(\n    contents, name=None\n)\n"
  },
  {
    "tf.io.match_filenames_once": "tf.io.match_filenames_once(\n    pattern, name=None\n)\n"
  },
  {
    "tf.io.matching_files": "tf.io.matching_files(\n    pattern, name=None\n)\n"
  },
  {
    "tf.io.parse_example": "tf.io.parse_example(\n    serialized, features, example_names=None, name=None\n)\n"
  },
  {
    "tf.io.parse_sequence_example": "tf.io.parse_sequence_example(\n    serialized,\n    context_features=None,\n    sequence_features=None,\n    example_names=None,\n    name=None\n)\n"
  },
  {
    "tf.io.parse_single_example": "tf.io.parse_single_example(\n    serialized, features, example_names=None, name=None\n)\n"
  },
  {
    "tf.io.parse_single_sequence_example": "tf.io.parse_single_sequence_example(\n    serialized,\n    context_features=None,\n    sequence_features=None,\n    example_name=None,\n    name=None\n)\n"
  },
  {
    "tf.io.parse_tensor": "tf.io.parse_tensor(\n    serialized, out_type, name=None\n)\n"
  },
  {
    "tf.io.read_file": "tf.io.read_file(\n    filename, name=None\n)\n"
  },
  {
    "tf.io.serialize_many_sparse": "tf.io.serialize_many_sparse(\n    sp_input,\n    out_type=tf.dtypes.string,\n    name=None\n)\n"
  },
  {
    "tf.io.serialize_sparse": "tf.io.serialize_sparse(\n    sp_input,\n    out_type=tf.dtypes.string,\n    name=None\n)\n"
  },
  {
    "tf.io.serialize_tensor": "tf.io.serialize_tensor(\n    tensor, name=None\n)\n"
  },
  {
    "tf.io.write_file": "tf.io.write_file(\n    filename, contents, name=None\n)\n"
  },
  {
    "tf.io.write_graph": "tf.io.write_graph(\n    graph_or_graph_def, logdir, name, as_text=True\n)\n"
  },
  {
    "tf.is_tensor": "tf.is_tensor(\n    x\n)\n"
  },
  {
    "tf.keras.Input": "tf.keras.Input(\n    shape=None,\n    batch_size=None,\n    name=None,\n    dtype=None,\n    sparse=None,\n    tensor=None,\n    ragged=None,\n    type_spec=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.Model": "tf.keras.Model(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.keras.Sequential": "tf.keras.Sequential(\n    layers=None, name=None\n)\n"
  },
  {
    "tf.keras.activations.deserialize": "tf.keras.activations.deserialize(\n    name, custom_objects=None\n)\n"
  },
  {
    "tf.keras.activations.elu": "tf.keras.activations.elu(\n    x, alpha=1.0\n)\n"
  },
  {
    "tf.keras.activations.exponential": "tf.keras.activations.exponential(\n    x\n)\n"
  },
  {
    "tf.keras.activations.gelu": "tf.keras.activations.gelu(\n    x, approximate=False\n)\n"
  },
  {
    "tf.keras.activations.get": "tf.keras.activations.get(\n    identifier\n)\n"
  },
  {
    "tf.keras.activations.hard_sigmoid": "tf.keras.activations.hard_sigmoid(\n    x\n)\n"
  },
  {
    "tf.keras.activations.linear": "tf.keras.activations.linear(\n    x\n)\n"
  },
  {
    "tf.keras.activations.relu": "tf.keras.activations.relu(\n    x, alpha=0.0, max_value=None, threshold=0.0\n)\n"
  },
  {
    "tf.keras.activations.selu": "tf.keras.activations.selu(\n    x\n)\n"
  },
  {
    "tf.keras.activations.serialize": "tf.keras.activations.serialize(\n    activation\n)\n"
  },
  {
    "tf.keras.activations.sigmoid": "tf.keras.activations.sigmoid(\n    x\n)\n"
  },
  {
    "tf.keras.activations.softmax": "tf.keras.activations.softmax(\n    x, axis=-1\n)\n"
  },
  {
    "tf.keras.activations.softplus": "tf.keras.activations.softplus(\n    x\n)\n"
  },
  {
    "tf.keras.activations.softsign": "tf.keras.activations.softsign(\n    x\n)\n"
  },
  {
    "tf.keras.activations.swish": "tf.keras.activations.swish(\n    x\n)\n"
  },
  {
    "tf.keras.activations.tanh": "tf.keras.activations.tanh(\n    x\n)\n"
  },
  {
    "tf.keras.applications.convnext.ConvNeXtBase": "tf.keras.applications.convnext.ConvNeXtBase(\n    model_name='convnext_base',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.convnext.ConvNeXtLarge": "tf.keras.applications.convnext.ConvNeXtLarge(\n    model_name='convnext_large',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.convnext.ConvNeXtSmall": "tf.keras.applications.convnext.ConvNeXtSmall(\n    model_name='convnext_small',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.convnext.ConvNeXtTiny": "tf.keras.applications.convnext.ConvNeXtTiny(\n    model_name='convnext_tiny',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.convnext.ConvNeXtXLarge": "tf.keras.applications.convnext.ConvNeXtXLarge(\n    model_name='convnext_xlarge',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.densenet.DenseNet121": "tf.keras.applications.densenet.DenseNet121(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.densenet.DenseNet169": "tf.keras.applications.densenet.DenseNet169(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.densenet.DenseNet201": "tf.keras.applications.densenet.DenseNet201(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB0": "tf.keras.applications.efficientnet.EfficientNetB0(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB1": "tf.keras.applications.efficientnet.EfficientNetB1(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB2": "tf.keras.applications.efficientnet.EfficientNetB2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB3": "tf.keras.applications.efficientnet.EfficientNetB3(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB4": "tf.keras.applications.efficientnet.EfficientNetB4(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB5": "tf.keras.applications.efficientnet.EfficientNetB5(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB6": "tf.keras.applications.efficientnet.EfficientNetB6(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB7": "tf.keras.applications.efficientnet.EfficientNetB7(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2B0": "tf.keras.applications.efficientnet_v2.EfficientNetV2B0(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2B1": "tf.keras.applications.efficientnet_v2.EfficientNetV2B1(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2B2": "tf.keras.applications.efficientnet_v2.EfficientNetV2B2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2B3": "tf.keras.applications.efficientnet_v2.EfficientNetV2B3(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2L": "tf.keras.applications.efficientnet_v2.EfficientNetV2L(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2M": "tf.keras.applications.efficientnet_v2.EfficientNetV2M(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2S": "tf.keras.applications.efficientnet_v2.EfficientNetV2S(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.inception_resnet_v2.InceptionResNetV2": "tf.keras.applications.inception_resnet_v2.InceptionResNetV2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.inception_v3.InceptionV3": "tf.keras.applications.inception_v3.InceptionV3(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.mobilenet.MobileNet": "tf.keras.applications.mobilenet.MobileNet(\n    input_shape=None,\n    alpha=1.0,\n    depth_multiplier=1,\n    dropout=0.001,\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.mobilenet_v2.MobileNetV2": "tf.keras.applications.mobilenet_v2.MobileNetV2(\n    input_shape=None,\n    alpha=1.0,\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.MobileNetV3Large": "tf.keras.applications.MobileNetV3Large(\n    input_shape=None,\n    alpha=1.0,\n    minimalistic=False,\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    classes=1000,\n    pooling=None,\n    dropout_rate=0.2,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.MobileNetV3Small": "tf.keras.applications.MobileNetV3Small(\n    input_shape=None,\n    alpha=1.0,\n    minimalistic=False,\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    classes=1000,\n    pooling=None,\n    dropout_rate=0.2,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.nasnet.NASNetLarge": "tf.keras.applications.nasnet.NASNetLarge(\n    input_shape=None,\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.nasnet.NASNetMobile": "tf.keras.applications.nasnet.NASNetMobile(\n    input_shape=None,\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX002": "tf.keras.applications.regnet.RegNetX002(\n    model_name='regnetx002',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX004": "tf.keras.applications.regnet.RegNetX004(\n    model_name='regnetx004',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX006": "tf.keras.applications.regnet.RegNetX006(\n    model_name='regnetx006',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX008": "tf.keras.applications.regnet.RegNetX008(\n    model_name='regnetx008',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX016": "tf.keras.applications.regnet.RegNetX016(\n    model_name='regnetx016',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX032": "tf.keras.applications.regnet.RegNetX032(\n    model_name='regnetx032',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX040": "tf.keras.applications.regnet.RegNetX040(\n    model_name='regnetx040',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX064": "tf.keras.applications.regnet.RegNetX064(\n    model_name='regnetx064',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX080": "tf.keras.applications.regnet.RegNetX080(\n    model_name='regnetx080',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX120": "tf.keras.applications.regnet.RegNetX120(\n    model_name='regnetx120',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX160": "tf.keras.applications.regnet.RegNetX160(\n    model_name='regnetx160',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX320": "tf.keras.applications.regnet.RegNetX320(\n    model_name='regnetx320',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY002": "tf.keras.applications.regnet.RegNetY002(\n    model_name='regnety002',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY004": "tf.keras.applications.regnet.RegNetY004(\n    model_name='regnety004',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY006": "tf.keras.applications.regnet.RegNetY006(\n    model_name='regnety006',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY008": "tf.keras.applications.regnet.RegNetY008(\n    model_name='regnety008',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY016": "tf.keras.applications.regnet.RegNetY016(\n    model_name='regnety016',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY032": "tf.keras.applications.regnet.RegNetY032(\n    model_name='regnety032',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY040": "tf.keras.applications.regnet.RegNetY040(\n    model_name='regnety040',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY064": "tf.keras.applications.regnet.RegNetY064(\n    model_name='regnety064',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY080": "tf.keras.applications.regnet.RegNetY080(\n    model_name='regnety080',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY120": "tf.keras.applications.regnet.RegNetY120(\n    model_name='regnety120',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY160": "tf.keras.applications.regnet.RegNetY160(\n    model_name='regnety160',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY320": "tf.keras.applications.regnet.RegNetY320(\n    model_name='regnety320',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.resnet.ResNet101": "tf.keras.applications.resnet.ResNet101(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.resnet_v2.ResNet101V2": "tf.keras.applications.resnet_v2.ResNet101V2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.resnet.ResNet152": "tf.keras.applications.resnet.ResNet152(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.resnet_v2.ResNet152V2": "tf.keras.applications.resnet_v2.ResNet152V2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.resnet50.ResNet50": "tf.keras.applications.resnet50.ResNet50(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.resnet_v2.ResNet50V2": "tf.keras.applications.resnet_v2.ResNet50V2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS101": "tf.keras.applications.resnet_rs.ResNetRS101(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS152": "tf.keras.applications.resnet_rs.ResNetRS152(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS200": "tf.keras.applications.resnet_rs.ResNetRS200(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS270": "tf.keras.applications.resnet_rs.ResNetRS270(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS350": "tf.keras.applications.resnet_rs.ResNetRS350(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS420": "tf.keras.applications.resnet_rs.ResNetRS420(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS50": "tf.keras.applications.resnet_rs.ResNetRS50(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.vgg16.VGG16": "tf.keras.applications.vgg16.VGG16(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.vgg19.VGG19": "tf.keras.applications.vgg19.VGG19(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.xception.Xception": "tf.keras.applications.xception.Xception(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.convnext.ConvNeXtBase": "tf.keras.applications.convnext.ConvNeXtBase(\n    model_name='convnext_base',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.convnext.ConvNeXtLarge": "tf.keras.applications.convnext.ConvNeXtLarge(\n    model_name='convnext_large',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.convnext.ConvNeXtSmall": "tf.keras.applications.convnext.ConvNeXtSmall(\n    model_name='convnext_small',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.convnext.ConvNeXtTiny": "tf.keras.applications.convnext.ConvNeXtTiny(\n    model_name='convnext_tiny',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.convnext.ConvNeXtXLarge": "tf.keras.applications.convnext.ConvNeXtXLarge(\n    model_name='convnext_xlarge',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.convnext.decode_predictions": "tf.keras.applications.convnext.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.convnext.preprocess_input": "tf.keras.applications.convnext.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.densenet.DenseNet121": "tf.keras.applications.densenet.DenseNet121(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.densenet.DenseNet169": "tf.keras.applications.densenet.DenseNet169(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.densenet.DenseNet201": "tf.keras.applications.densenet.DenseNet201(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.densenet.decode_predictions": "tf.keras.applications.densenet.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.densenet.preprocess_input": "tf.keras.applications.densenet.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB0": "tf.keras.applications.efficientnet.EfficientNetB0(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB1": "tf.keras.applications.efficientnet.EfficientNetB1(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB2": "tf.keras.applications.efficientnet.EfficientNetB2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB3": "tf.keras.applications.efficientnet.EfficientNetB3(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB4": "tf.keras.applications.efficientnet.EfficientNetB4(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB5": "tf.keras.applications.efficientnet.EfficientNetB5(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB6": "tf.keras.applications.efficientnet.EfficientNetB6(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.EfficientNetB7": "tf.keras.applications.efficientnet.EfficientNetB7(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.decode_predictions": "tf.keras.applications.efficientnet.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.efficientnet.preprocess_input": "tf.keras.applications.efficientnet.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2B0": "tf.keras.applications.efficientnet_v2.EfficientNetV2B0(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2B1": "tf.keras.applications.efficientnet_v2.EfficientNetV2B1(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2B2": "tf.keras.applications.efficientnet_v2.EfficientNetV2B2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2B3": "tf.keras.applications.efficientnet_v2.EfficientNetV2B3(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2L": "tf.keras.applications.efficientnet_v2.EfficientNetV2L(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2M": "tf.keras.applications.efficientnet_v2.EfficientNetV2M(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.EfficientNetV2S": "tf.keras.applications.efficientnet_v2.EfficientNetV2S(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.decode_predictions": "tf.keras.applications.efficientnet_v2.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.efficientnet_v2.preprocess_input": "tf.keras.applications.efficientnet_v2.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.imagenet_utils.decode_predictions": "tf.keras.applications.imagenet_utils.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.imagenet_utils.preprocess_input": "tf.keras.applications.imagenet_utils.preprocess_input(\n    x, data_format=None, mode='caffe'\n)\n"
  },
  {
    "tf.keras.applications.inception_resnet_v2.InceptionResNetV2": "tf.keras.applications.inception_resnet_v2.InceptionResNetV2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.inception_resnet_v2.decode_predictions": "tf.keras.applications.inception_resnet_v2.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.inception_resnet_v2.preprocess_input": "tf.keras.applications.inception_resnet_v2.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.inception_v3.InceptionV3": "tf.keras.applications.inception_v3.InceptionV3(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.inception_v3.decode_predictions": "tf.keras.applications.inception_v3.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.inception_v3.preprocess_input": "tf.keras.applications.inception_v3.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.mobilenet.MobileNet": "tf.keras.applications.mobilenet.MobileNet(\n    input_shape=None,\n    alpha=1.0,\n    depth_multiplier=1,\n    dropout=0.001,\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.mobilenet.decode_predictions": "tf.keras.applications.mobilenet.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.mobilenet.preprocess_input": "tf.keras.applications.mobilenet.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.mobilenet_v2.MobileNetV2": "tf.keras.applications.mobilenet_v2.MobileNetV2(\n    input_shape=None,\n    alpha=1.0,\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.mobilenet_v2.decode_predictions": "tf.keras.applications.mobilenet_v2.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.mobilenet_v2.preprocess_input": "tf.keras.applications.mobilenet_v2.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.mobilenet_v3.decode_predictions": "tf.keras.applications.mobilenet_v3.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.mobilenet_v3.preprocess_input": "tf.keras.applications.mobilenet_v3.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.nasnet.NASNetLarge": "tf.keras.applications.nasnet.NASNetLarge(\n    input_shape=None,\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.nasnet.NASNetMobile": "tf.keras.applications.nasnet.NASNetMobile(\n    input_shape=None,\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.nasnet.decode_predictions": "tf.keras.applications.nasnet.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.nasnet.preprocess_input": "tf.keras.applications.nasnet.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX002": "tf.keras.applications.regnet.RegNetX002(\n    model_name='regnetx002',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX004": "tf.keras.applications.regnet.RegNetX004(\n    model_name='regnetx004',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX006": "tf.keras.applications.regnet.RegNetX006(\n    model_name='regnetx006',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX008": "tf.keras.applications.regnet.RegNetX008(\n    model_name='regnetx008',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX016": "tf.keras.applications.regnet.RegNetX016(\n    model_name='regnetx016',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX032": "tf.keras.applications.regnet.RegNetX032(\n    model_name='regnetx032',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX040": "tf.keras.applications.regnet.RegNetX040(\n    model_name='regnetx040',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX064": "tf.keras.applications.regnet.RegNetX064(\n    model_name='regnetx064',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX080": "tf.keras.applications.regnet.RegNetX080(\n    model_name='regnetx080',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX120": "tf.keras.applications.regnet.RegNetX120(\n    model_name='regnetx120',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX160": "tf.keras.applications.regnet.RegNetX160(\n    model_name='regnetx160',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetX320": "tf.keras.applications.regnet.RegNetX320(\n    model_name='regnetx320',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY002": "tf.keras.applications.regnet.RegNetY002(\n    model_name='regnety002',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY004": "tf.keras.applications.regnet.RegNetY004(\n    model_name='regnety004',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY006": "tf.keras.applications.regnet.RegNetY006(\n    model_name='regnety006',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY008": "tf.keras.applications.regnet.RegNetY008(\n    model_name='regnety008',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY016": "tf.keras.applications.regnet.RegNetY016(\n    model_name='regnety016',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY032": "tf.keras.applications.regnet.RegNetY032(\n    model_name='regnety032',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY040": "tf.keras.applications.regnet.RegNetY040(\n    model_name='regnety040',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY064": "tf.keras.applications.regnet.RegNetY064(\n    model_name='regnety064',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY080": "tf.keras.applications.regnet.RegNetY080(\n    model_name='regnety080',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY120": "tf.keras.applications.regnet.RegNetY120(\n    model_name='regnety120',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY160": "tf.keras.applications.regnet.RegNetY160(\n    model_name='regnety160',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.RegNetY320": "tf.keras.applications.regnet.RegNetY320(\n    model_name='regnety320',\n    include_top=True,\n    include_preprocessing=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.regnet.decode_predictions": "tf.keras.applications.regnet.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.regnet.preprocess_input": "tf.keras.applications.regnet.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.resnet.ResNet101": "tf.keras.applications.resnet.ResNet101(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.resnet.ResNet152": "tf.keras.applications.resnet.ResNet152(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.resnet50.ResNet50": "tf.keras.applications.resnet50.ResNet50(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.resnet50.decode_predictions": "tf.keras.applications.resnet50.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.resnet50.preprocess_input": "tf.keras.applications.resnet50.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.resnet50.ResNet50": "tf.keras.applications.resnet50.ResNet50(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.applications.resnet50.decode_predictions": "tf.keras.applications.resnet50.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.resnet50.preprocess_input": "tf.keras.applications.resnet50.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS101": "tf.keras.applications.resnet_rs.ResNetRS101(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS152": "tf.keras.applications.resnet_rs.ResNetRS152(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS200": "tf.keras.applications.resnet_rs.ResNetRS200(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS270": "tf.keras.applications.resnet_rs.ResNetRS270(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS350": "tf.keras.applications.resnet_rs.ResNetRS350(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS420": "tf.keras.applications.resnet_rs.ResNetRS420(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.ResNetRS50": "tf.keras.applications.resnet_rs.ResNetRS50(\n    include_top=True,\n    weights='imagenet',\n    classes=1000,\n    input_shape=None,\n    input_tensor=None,\n    pooling=None,\n    classifier_activation='softmax',\n    include_preprocessing=True\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.decode_predictions": "tf.keras.applications.resnet_rs.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.resnet_rs.preprocess_input": "tf.keras.applications.resnet_rs.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.resnet_v2.ResNet101V2": "tf.keras.applications.resnet_v2.ResNet101V2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.resnet_v2.ResNet152V2": "tf.keras.applications.resnet_v2.ResNet152V2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.resnet_v2.ResNet50V2": "tf.keras.applications.resnet_v2.ResNet50V2(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.resnet_v2.decode_predictions": "tf.keras.applications.resnet_v2.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.resnet_v2.preprocess_input": "tf.keras.applications.resnet_v2.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.vgg16.VGG16": "tf.keras.applications.vgg16.VGG16(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.vgg16.decode_predictions": "tf.keras.applications.vgg16.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.vgg16.preprocess_input": "tf.keras.applications.vgg16.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.vgg19.VGG19": "tf.keras.applications.vgg19.VGG19(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.vgg19.decode_predictions": "tf.keras.applications.vgg19.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.vgg19.preprocess_input": "tf.keras.applications.vgg19.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.applications.xception.Xception": "tf.keras.applications.xception.Xception(\n    include_top=True,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation='softmax'\n)\n"
  },
  {
    "tf.keras.applications.xception.decode_predictions": "tf.keras.applications.xception.decode_predictions(\n    preds, top=5\n)\n"
  },
  {
    "tf.keras.applications.xception.preprocess_input": "tf.keras.applications.xception.preprocess_input(\n    x, data_format=None\n)\n"
  },
  {
    "tf.keras.backend.get_uid": "tf.keras.backend.get_uid(\n    prefix=''\n)\n"
  },
  {
    "tf.keras.backend.is_keras_tensor": "tf.keras.backend.is_keras_tensor(\n    x\n)\n"
  },
  {
    "tf.keras.backend.rnn": "tf.keras.backend.rnn(\n    step_function,\n    inputs,\n    initial_states,\n    go_backwards=False,\n    mask=None,\n    constants=None,\n    unroll=False,\n    input_length=None,\n    time_major=False,\n    zero_output_for_mask=False,\n    return_all_outputs=True\n)\n"
  },
  {
    "tf.keras.backend.set_epsilon": "tf.keras.backend.set_epsilon(\n    value\n)\n"
  },
  {
    "tf.keras.backend.set_floatx": "tf.keras.backend.set_floatx(\n    value\n)\n"
  },
  {
    "tf.keras.backend.set_image_data_format": "tf.keras.backend.set_image_data_format(\n    data_format\n)\n"
  },
  {
    "tf.keras.callbacks.BackupAndRestore": "tf.keras.callbacks.BackupAndRestore(\n    backup_dir,\n    save_freq='epoch',\n    delete_checkpoint=True,\n    save_before_preemption=False\n)\n"
  },
  {
    "tf.keras.callbacks.BaseLogger": "tf.keras.callbacks.BaseLogger(\n    stateful_metrics=None\n)\n"
  },
  {
    "tf.keras.callbacks.CSVLogger": "tf.keras.callbacks.CSVLogger(\n    filename, separator=',', append=False\n)\n"
  },
  {
    "tf.keras.callbacks.CallbackList": "tf.keras.callbacks.CallbackList(\n    callbacks=None, add_history=False, add_progbar=False, model=None, **params\n)\n"
  },
  {
    "tf.keras.callbacks.EarlyStopping": "tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    min_delta=0,\n    patience=0,\n    verbose=0,\n    mode='auto',\n    baseline=None,\n    restore_best_weights=False,\n    start_from_epoch=0\n)\n"
  },
  {
    "tf.keras.callbacks.LambdaCallback": "tf.keras.callbacks.LambdaCallback(\n    on_epoch_begin=None,\n    on_epoch_end=None,\n    on_batch_begin=None,\n    on_batch_end=None,\n    on_train_begin=None,\n    on_train_end=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.callbacks.LearningRateScheduler": "tf.keras.callbacks.LearningRateScheduler(\n    schedule, verbose=0\n)\n"
  },
  {
    "tf.keras.callbacks.ModelCheckpoint": "tf.keras.callbacks.ModelCheckpoint(\n    filepath,\n    monitor: str = 'val_loss',\n    verbose: int = 0,\n    save_best_only: bool = False,\n    save_weights_only: bool = False,\n    mode: str = 'auto',\n    save_freq='epoch',\n    options=None,\n    initial_value_threshold=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.callbacks.ProgbarLogger": "tf.keras.callbacks.ProgbarLogger(\n    count_mode: str = 'samples', stateful_metrics=None\n)\n"
  },
  {
    "tf.keras.callbacks.ReduceLROnPlateau": "tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.1,\n    patience=10,\n    verbose=0,\n    mode='auto',\n    min_delta=0.0001,\n    cooldown=0,\n    min_lr=0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.callbacks.RemoteMonitor": "tf.keras.callbacks.RemoteMonitor(\n    root='http://localhost:9000',\n    path='/publish/epoch/end/',\n    field='data',\n    headers=None,\n    send_as_json=False\n)\n"
  },
  {
    "tf.keras.callbacks.TensorBoard": "tf.keras.callbacks.TensorBoard(\n    log_dir='logs',\n    histogram_freq=0,\n    write_graph=True,\n    write_images=False,\n    write_steps_per_second=False,\n    update_freq='epoch',\n    profile_batch=0,\n    embeddings_freq=0,\n    embeddings_metadata=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.callbacks.experimental.BackupAndRestore": "tf.keras.callbacks.experimental.BackupAndRestore(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.keras.constraints.MaxNorm": "tf.keras.constraints.MaxNorm(\n    max_value=2, axis=0\n)\n"
  },
  {
    "tf.keras.constraints.MinMaxNorm": "tf.keras.constraints.MinMaxNorm(\n    min_value=0.0, max_value=1.0, rate=1.0, axis=0\n)\n"
  },
  {
    "tf.keras.constraints.UnitNorm": "tf.keras.constraints.UnitNorm(\n    axis=0\n)\n"
  },
  {
    "tf.keras.constraints.deserialize": "tf.keras.constraints.deserialize(\n    config, custom_objects=None\n)\n"
  },
  {
    "tf.keras.constraints.get": "tf.keras.constraints.get(\n    identifier\n)\n"
  },
  {
    "tf.keras.constraints.MaxNorm": "tf.keras.constraints.MaxNorm(\n    max_value=2, axis=0\n)\n"
  },
  {
    "tf.keras.constraints.MinMaxNorm": "tf.keras.constraints.MinMaxNorm(\n    min_value=0.0, max_value=1.0, rate=1.0, axis=0\n)\n"
  },
  {
    "tf.keras.constraints.serialize": "tf.keras.constraints.serialize(\n    constraint\n)\n"
  },
  {
    "tf.keras.constraints.UnitNorm": "tf.keras.constraints.UnitNorm(\n    axis=0\n)\n"
  },
  {
    "tf.keras.datasets.boston_housing.load_data": "tf.keras.datasets.boston_housing.load_data(\n    path='boston_housing.npz', test_split=0.2, seed=113\n)\n"
  },
  {
    "tf.keras.datasets.cifar100.load_data": "tf.keras.datasets.cifar100.load_data(\n    label_mode='fine'\n)\n"
  },
  {
    "tf.keras.datasets.imdb.get_word_index": "tf.keras.datasets.imdb.get_word_index(\n    path='imdb_word_index.json'\n)\n"
  },
  {
    "tf.keras.datasets.imdb.load_data": "tf.keras.datasets.imdb.load_data(\n    path='imdb.npz',\n    num_words=None,\n    skip_top=0,\n    maxlen=None,\n    seed=113,\n    start_char=1,\n    oov_char=2,\n    index_from=3,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.datasets.mnist.load_data": "tf.keras.datasets.mnist.load_data(\n    path='mnist.npz'\n)\n"
  },
  {
    "tf.keras.datasets.reuters.get_word_index": "tf.keras.datasets.reuters.get_word_index(\n    path='reuters_word_index.json'\n)\n"
  },
  {
    "tf.keras.datasets.reuters.load_data": "tf.keras.datasets.reuters.load_data(\n    path='reuters.npz',\n    num_words=None,\n    skip_top=0,\n    maxlen=None,\n    test_split=0.2,\n    seed=113,\n    start_char=1,\n    oov_char=2,\n    index_from=3,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.dtensor.experimental.LayoutMap": "tf.keras.dtensor.experimental.LayoutMap(\n    mesh=None\n)\n"
  },
  {
    "tf.keras.dtensor.experimental.optimizers.Adadelta": "tf.keras.dtensor.experimental.optimizers.Adadelta(\n    learning_rate=0.001,\n    rho=0.95,\n    epsilon=1e-07,\n    gradients_clip_option=None,\n    ema_option=None,\n    name='Adadelta',\n    mesh=None\n)\n"
  },
  {
    "tf.keras.dtensor.experimental.optimizers.Adagrad": "tf.keras.dtensor.experimental.optimizers.Adagrad(\n    learning_rate=0.001,\n    initial_accumulator_value=0.1,\n    epsilon=1e-07,\n    gradients_clip_option=None,\n    ema_option=None,\n    name='Adagrad',\n    mesh=None\n)\n"
  },
  {
    "tf.keras.dtensor.experimental.optimizers.Adam": "tf.keras.dtensor.experimental.optimizers.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    gradients_clip_option=None,\n    ema_option=None,\n    name='Adam',\n    mesh=None\n)\n"
  },
  {
    "tf.keras.dtensor.experimental.optimizers.AdamW": "tf.keras.dtensor.experimental.optimizers.AdamW(\n    learning_rate=0.001,\n    weight_decay=0.004,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    name='AdamW',\n    mesh=None\n)\n"
  },
  {
    "tf.keras.dtensor.experimental.optimizers.RMSprop": "tf.keras.dtensor.experimental.optimizers.RMSprop(\n    learning_rate=0.001,\n    rho=0.9,\n    momentum=0.0,\n    epsilon=1e-07,\n    centered=False,\n    gradients_clip_option=None,\n    ema_option=None,\n    jit_compile=False,\n    name='RMSprop',\n    mesh=None\n)\n"
  },
  {
    "tf.keras.dtensor.experimental.optimizers.SGD": "tf.keras.dtensor.experimental.optimizers.SGD(\n    learning_rate=0.01,\n    momentum=0.0,\n    nesterov=False,\n    amsgrad=False,\n    gradients_clip_option=None,\n    ema_option=None,\n    jit_compile=False,\n    name='SGD',\n    mesh=None\n)\n"
  },
  {
    "tf.keras.estimator.model_to_estimator": "tf.keras.estimator.model_to_estimator(\n    keras_model=None,\n    keras_model_path=None,\n    custom_objects=None,\n    model_dir=None,\n    config=None,\n    checkpoint_format='checkpoint',\n    metric_names_map=None,\n    export_outputs=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.CosineDecay": "tf.keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate, decay_steps, alpha=0.0, name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.CosineDecayRestarts": "tf.keras.optimizers.schedules.CosineDecayRestarts(\n    initial_learning_rate,\n    first_decay_steps,\n    t_mul=2.0,\n    m_mul=1.0,\n    alpha=0.0,\n    name=None\n)\n"
  },
  {
    "tf.keras.experimental.LinearModel": "tf.keras.experimental.LinearModel(\n    units=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='zeros',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.experimental.SequenceFeatures": "tf.keras.experimental.SequenceFeatures(\n    feature_columns, trainable=True, name=None, **kwargs\n)\n"
  },
  {
    "tf.keras.experimental.SidecarEvaluator": "tf.keras.experimental.SidecarEvaluator(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.keras.experimental.WideDeepModel": "tf.keras.experimental.WideDeepModel(\n    linear_model, dnn_model, activation=None, **kwargs\n)\n"
  },
  {
    "tf.keras.initializers.Constant": "tf.keras.initializers.Constant(\n    value=0\n)\n"
  },
  {
    "tf.keras.initializers.GlorotNormal": "tf.keras.initializers.GlorotNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.GlorotUniform": "tf.keras.initializers.GlorotUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.HeNormal": "tf.keras.initializers.HeNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.HeUniform": "tf.keras.initializers.HeUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.Identity": "tf.keras.initializers.Identity(\n    gain=1.0\n)\n"
  },
  {
    "tf.keras.initializers.LecunNormal": "tf.keras.initializers.LecunNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.LecunUniform": "tf.keras.initializers.LecunUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.Orthogonal": "tf.keras.initializers.Orthogonal(\n    gain=1.0, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.RandomNormal": "tf.keras.initializers.RandomNormal(\n    mean=0.0, stddev=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.RandomUniform": "tf.keras.initializers.RandomUniform(\n    minval=-0.05, maxval=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.TruncatedNormal": "tf.keras.initializers.TruncatedNormal(\n    mean=0.0, stddev=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.VarianceScaling": "tf.keras.initializers.VarianceScaling(\n    scale=1.0,\n    mode='fan_in',\n    distribution='truncated_normal',\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.Constant": "tf.keras.initializers.Constant(\n    value=0\n)\n"
  },
  {
    "tf.keras.initializers.deserialize": "tf.keras.initializers.deserialize(\n    config, custom_objects=None\n)\n"
  },
  {
    "tf.keras.initializers.get": "tf.keras.initializers.get(\n    identifier\n)\n"
  },
  {
    "tf.keras.initializers.GlorotNormal": "tf.keras.initializers.GlorotNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.GlorotUniform": "tf.keras.initializers.GlorotUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.HeNormal": "tf.keras.initializers.HeNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.HeUniform": "tf.keras.initializers.HeUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.Identity": "tf.keras.initializers.Identity(\n    gain=1.0\n)\n"
  },
  {
    "tf.keras.initializers.LecunNormal": "tf.keras.initializers.LecunNormal(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.LecunUniform": "tf.keras.initializers.LecunUniform(\n    seed=None\n)\n"
  },
  {
    "tf.keras.initializers.Orthogonal": "tf.keras.initializers.Orthogonal(\n    gain=1.0, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.RandomNormal": "tf.keras.initializers.RandomNormal(\n    mean=0.0, stddev=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.RandomUniform": "tf.keras.initializers.RandomUniform(\n    minval=-0.05, maxval=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.serialize": "tf.keras.initializers.serialize(\n    initializer\n)\n"
  },
  {
    "tf.keras.initializers.TruncatedNormal": "tf.keras.initializers.TruncatedNormal(\n    mean=0.0, stddev=0.05, seed=None\n)\n"
  },
  {
    "tf.keras.initializers.VarianceScaling": "tf.keras.initializers.VarianceScaling(\n    scale=1.0,\n    mode='fan_in',\n    distribution='truncated_normal',\n    seed=None\n)\n"
  },
  {
    "tf.keras.layers.AbstractRNNCell": "tf.keras.layers.AbstractRNNCell(\n    trainable=True, name=None, dtype=None, dynamic=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Activation": "tf.keras.layers.Activation(\n    activation, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.ActivityRegularization": "tf.keras.layers.ActivityRegularization(\n    l1=0.0, l2=0.0, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Add": "tf.keras.layers.Add(\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.AdditiveAttention": "tf.keras.layers.AdditiveAttention(\n    use_scale=True, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.AlphaDropout": "tf.keras.layers.AlphaDropout(\n    rate, noise_shape=None, seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Attention": "tf.keras.layers.Attention(\n    use_scale=False, score_mode='dot', **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Average": "tf.keras.layers.Average(\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.AveragePooling1D": "tf.keras.layers.AveragePooling1D(\n    pool_size=2,\n    strides=None,\n    padding='valid',\n    data_format='channels_last',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.AveragePooling2D": "tf.keras.layers.AveragePooling2D(\n    pool_size=(2, 2),\n    strides=None,\n    padding='valid',\n    data_format=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.AveragePooling3D": "tf.keras.layers.AveragePooling3D(\n    pool_size=(2, 2, 2),\n    strides=None,\n    padding='valid',\n    data_format=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.AveragePooling1D": "tf.keras.layers.AveragePooling1D(\n    pool_size=2,\n    strides=None,\n    padding='valid',\n    data_format='channels_last',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.AveragePooling2D": "tf.keras.layers.AveragePooling2D(\n    pool_size=(2, 2),\n    strides=None,\n    padding='valid',\n    data_format=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.AveragePooling3D": "tf.keras.layers.AveragePooling3D(\n    pool_size=(2, 2, 2),\n    strides=None,\n    padding='valid',\n    data_format=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.BatchNormalization": "tf.keras.layers.BatchNormalization(\n    axis=-1,\n    momentum=0.99,\n    epsilon=0.001,\n    center=True,\n    scale=True,\n    beta_initializer='zeros',\n    gamma_initializer='ones',\n    moving_mean_initializer='zeros',\n    moving_variance_initializer='ones',\n    beta_regularizer=None,\n    gamma_regularizer=None,\n    beta_constraint=None,\n    gamma_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Bidirectional": "tf.keras.layers.Bidirectional(\n    layer,\n    merge_mode='concat',\n    weights=None,\n    backward_layer=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.CategoryEncoding": "tf.keras.layers.CategoryEncoding(\n    num_tokens=None, output_mode='multi_hot', sparse=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.CenterCrop": "tf.keras.layers.CenterCrop(\n    height, width, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Concatenate": "tf.keras.layers.Concatenate(\n    axis=-1, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv1D": "tf.keras.layers.Conv1D(\n    filters,\n    kernel_size,\n    strides=1,\n    padding='valid',\n    data_format='channels_last',\n    dilation_rate=1,\n    groups=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv1DTranspose": "tf.keras.layers.Conv1DTranspose(\n    filters,\n    kernel_size,\n    strides=1,\n    padding='valid',\n    output_padding=None,\n    data_format=None,\n    dilation_rate=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv2D": "tf.keras.layers.Conv2D(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding='valid',\n    data_format=None,\n    dilation_rate=(1, 1),\n    groups=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv2DTranspose": "tf.keras.layers.Conv2DTranspose(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding='valid',\n    output_padding=None,\n    data_format=None,\n    dilation_rate=(1, 1),\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv3D": "tf.keras.layers.Conv3D(\n    filters,\n    kernel_size,\n    strides=(1, 1, 1),\n    padding='valid',\n    data_format=None,\n    dilation_rate=(1, 1, 1),\n    groups=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv3DTranspose": "tf.keras.layers.Conv3DTranspose(\n    filters,\n    kernel_size,\n    strides=(1, 1, 1),\n    padding='valid',\n    output_padding=None,\n    data_format=None,\n    dilation_rate=(1, 1, 1),\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.ConvLSTM1D": "tf.keras.layers.ConvLSTM1D(\n    filters,\n    kernel_size,\n    strides=1,\n    padding='valid',\n    data_format=None,\n    dilation_rate=1,\n    activation='tanh',\n    recurrent_activation='hard_sigmoid',\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    recurrent_initializer='orthogonal',\n    bias_initializer='zeros',\n    unit_forget_bias=True,\n    kernel_regularizer=None,\n    recurrent_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    recurrent_constraint=None,\n    bias_constraint=None,\n    return_sequences=False,\n    return_state=False,\n    go_backwards=False,\n    stateful=False,\n    dropout=0.0,\n    recurrent_dropout=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.ConvLSTM2D": "tf.keras.layers.ConvLSTM2D(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding='valid',\n    data_format=None,\n    dilation_rate=(1, 1),\n    activation='tanh',\n    recurrent_activation='hard_sigmoid',\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    recurrent_initializer='orthogonal',\n    bias_initializer='zeros',\n    unit_forget_bias=True,\n    kernel_regularizer=None,\n    recurrent_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    recurrent_constraint=None,\n    bias_constraint=None,\n    return_sequences=False,\n    return_state=False,\n    go_backwards=False,\n    stateful=False,\n    dropout=0.0,\n    recurrent_dropout=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.ConvLSTM3D": "tf.keras.layers.ConvLSTM3D(\n    filters,\n    kernel_size,\n    strides=(1, 1, 1),\n    padding='valid',\n    data_format=None,\n    dilation_rate=(1, 1, 1),\n    activation='tanh',\n    recurrent_activation='hard_sigmoid',\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    recurrent_initializer='orthogonal',\n    bias_initializer='zeros',\n    unit_forget_bias=True,\n    kernel_regularizer=None,\n    recurrent_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    recurrent_constraint=None,\n    bias_constraint=None,\n    return_sequences=False,\n    return_state=False,\n    go_backwards=False,\n    stateful=False,\n    dropout=0.0,\n    recurrent_dropout=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv1D": "tf.keras.layers.Conv1D(\n    filters,\n    kernel_size,\n    strides=1,\n    padding='valid',\n    data_format='channels_last',\n    dilation_rate=1,\n    groups=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv1DTranspose": "tf.keras.layers.Conv1DTranspose(\n    filters,\n    kernel_size,\n    strides=1,\n    padding='valid',\n    output_padding=None,\n    data_format=None,\n    dilation_rate=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv2D": "tf.keras.layers.Conv2D(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding='valid',\n    data_format=None,\n    dilation_rate=(1, 1),\n    groups=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv2DTranspose": "tf.keras.layers.Conv2DTranspose(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding='valid',\n    output_padding=None,\n    data_format=None,\n    dilation_rate=(1, 1),\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv3D": "tf.keras.layers.Conv3D(\n    filters,\n    kernel_size,\n    strides=(1, 1, 1),\n    padding='valid',\n    data_format=None,\n    dilation_rate=(1, 1, 1),\n    groups=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Conv3DTranspose": "tf.keras.layers.Conv3DTranspose(\n    filters,\n    kernel_size,\n    strides=(1, 1, 1),\n    padding='valid',\n    output_padding=None,\n    data_format=None,\n    dilation_rate=(1, 1, 1),\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Cropping1D": "tf.keras.layers.Cropping1D(\n    cropping=(1, 1), **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Cropping2D": "tf.keras.layers.Cropping2D(\n    cropping=((0, 0), (0, 0)), data_format=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Cropping3D": "tf.keras.layers.Cropping3D(\n    cropping=((1, 1), (1, 1), (1, 1)), data_format=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Dense": "tf.keras.layers.Dense(\n    units,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.DenseFeatures": "tf.keras.layers.DenseFeatures(\n    feature_columns, trainable=True, name=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.DepthwiseConv1D": "tf.keras.layers.DepthwiseConv1D(\n    kernel_size,\n    strides=1,\n    padding='valid',\n    depth_multiplier=1,\n    data_format=None,\n    dilation_rate=1,\n    activation=None,\n    use_bias=True,\n    depthwise_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    depthwise_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    depthwise_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.DepthwiseConv2D": "tf.keras.layers.DepthwiseConv2D(\n    kernel_size,\n    strides=(1, 1),\n    padding='valid',\n    depth_multiplier=1,\n    data_format=None,\n    dilation_rate=(1, 1),\n    activation=None,\n    use_bias=True,\n    depthwise_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    depthwise_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    depthwise_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Discretization": "tf.keras.layers.Discretization(\n    bin_boundaries=None,\n    num_bins=None,\n    epsilon=0.01,\n    output_mode='int',\n    sparse=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Dot": "tf.keras.layers.Dot(\n    axes, normalize=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Dropout": "tf.keras.layers.Dropout(\n    rate, noise_shape=None, seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.ELU": "tf.keras.layers.ELU(\n    alpha=1.0, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.EinsumDense": "tf.keras.layers.EinsumDense(\n    equation,\n    output_shape,\n    activation=None,\n    bias_axes=None,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Embedding": "tf.keras.layers.Embedding(\n    input_dim,\n    output_dim,\n    embeddings_initializer='uniform',\n    embeddings_regularizer=None,\n    activity_regularizer=None,\n    embeddings_constraint=None,\n    mask_zero=False,\n    input_length=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Flatten": "tf.keras.layers.Flatten(\n    data_format=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GRU": "tf.keras.layers.GRU(\n    units,\n    activation='tanh',\n    recurrent_activation='sigmoid',\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    recurrent_initializer='orthogonal',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    recurrent_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    recurrent_constraint=None,\n    bias_constraint=None,\n    dropout=0.0,\n    recurrent_dropout=0.0,\n    return_sequences=False,\n    return_state=False,\n    go_backwards=False,\n    stateful=False,\n    unroll=False,\n    time_major=False,\n    reset_after=True,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GRUCell": "tf.keras.layers.GRUCell(\n    units,\n    activation='tanh',\n    recurrent_activation='sigmoid',\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    recurrent_initializer='orthogonal',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    recurrent_regularizer=None,\n    bias_regularizer=None,\n    kernel_constraint=None,\n    recurrent_constraint=None,\n    bias_constraint=None,\n    dropout=0.0,\n    recurrent_dropout=0.0,\n    reset_after=True,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GaussianDropout": "tf.keras.layers.GaussianDropout(\n    rate, seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GaussianNoise": "tf.keras.layers.GaussianNoise(\n    stddev, seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalAveragePooling1D": "tf.keras.layers.GlobalAveragePooling1D(\n    data_format='channels_last', **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalAveragePooling2D": "tf.keras.layers.GlobalAveragePooling2D(\n    data_format=None, keepdims=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalAveragePooling3D": "tf.keras.layers.GlobalAveragePooling3D(\n    data_format=None, keepdims=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalAveragePooling1D": "tf.keras.layers.GlobalAveragePooling1D(\n    data_format='channels_last', **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalAveragePooling2D": "tf.keras.layers.GlobalAveragePooling2D(\n    data_format=None, keepdims=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalAveragePooling3D": "tf.keras.layers.GlobalAveragePooling3D(\n    data_format=None, keepdims=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalMaxPool1D": "tf.keras.layers.GlobalMaxPool1D(\n    data_format='channels_last', keepdims=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalMaxPool2D": "tf.keras.layers.GlobalMaxPool2D(\n    data_format=None, keepdims=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalMaxPool3D": "tf.keras.layers.GlobalMaxPool3D(\n    data_format=None, keepdims=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalMaxPool1D": "tf.keras.layers.GlobalMaxPool1D(\n    data_format='channels_last', keepdims=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalMaxPool2D": "tf.keras.layers.GlobalMaxPool2D(\n    data_format=None, keepdims=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GlobalMaxPool3D": "tf.keras.layers.GlobalMaxPool3D(\n    data_format=None, keepdims=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.GroupNormalization": "tf.keras.layers.GroupNormalization(\n    groups=32,\n    axis=-1,\n    epsilon=0.001,\n    center=True,\n    scale=True,\n    beta_initializer='zeros',\n    gamma_initializer='ones',\n    beta_regularizer=None,\n    gamma_regularizer=None,\n    beta_constraint=None,\n    gamma_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Hashing": "tf.keras.layers.Hashing(\n    num_bins,\n    mask_value=None,\n    salt=None,\n    output_mode='int',\n    sparse=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.Input": "tf.keras.Input(\n    shape=None,\n    batch_size=None,\n    name=None,\n    dtype=None,\n    sparse=None,\n    tensor=None,\n    ragged=None,\n    type_spec=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.InputLayer": "tf.keras.layers.InputLayer(\n    input_shape=None,\n    batch_size=None,\n    dtype=None,\n    input_tensor=None,\n    sparse=None,\n    name=None,\n    ragged=None,\n    type_spec=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.InputSpec": "tf.keras.layers.InputSpec(\n    dtype=None,\n    shape=None,\n    ndim=None,\n    max_ndim=None,\n    min_ndim=None,\n    axes=None,\n    allow_last_axis_squeeze=False,\n    name=None\n)\n"
  },
  {
    "tf.keras.layers.IntegerLookup": "tf.keras.layers.IntegerLookup(\n    max_tokens=None,\n    num_oov_indices=1,\n    mask_token=None,\n    oov_token=-1,\n    vocabulary=None,\n    vocabulary_dtype='int64',\n    idf_weights=None,\n    invert=False,\n    output_mode='int',\n    sparse=False,\n    pad_to_max_tokens=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.LSTM": "tf.keras.layers.LSTM(\n    units,\n    activation='tanh',\n    recurrent_activation='sigmoid',\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    recurrent_initializer='orthogonal',\n    bias_initializer='zeros',\n    unit_forget_bias=True,\n    kernel_regularizer=None,\n    recurrent_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    recurrent_constraint=None,\n    bias_constraint=None,\n    dropout=0.0,\n    recurrent_dropout=0.0,\n    return_sequences=False,\n    return_state=False,\n    go_backwards=False,\n    stateful=False,\n    time_major=False,\n    unroll=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.LSTMCell": "tf.keras.layers.LSTMCell(\n    units,\n    activation='tanh',\n    recurrent_activation='sigmoid',\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    recurrent_initializer='orthogonal',\n    bias_initializer='zeros',\n    unit_forget_bias=True,\n    kernel_regularizer=None,\n    recurrent_regularizer=None,\n    bias_regularizer=None,\n    kernel_constraint=None,\n    recurrent_constraint=None,\n    bias_constraint=None,\n    dropout=0.0,\n    recurrent_dropout=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Lambda": "tf.keras.layers.Lambda(\n    function, output_shape=None, mask=None, arguments=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Layer": "tf.keras.layers.Layer(\n    trainable=True, name=None, dtype=None, dynamic=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.LayerNormalization": "tf.keras.layers.LayerNormalization(\n    axis=-1,\n    epsilon=0.001,\n    center=True,\n    scale=True,\n    beta_initializer='zeros',\n    gamma_initializer='ones',\n    beta_regularizer=None,\n    gamma_regularizer=None,\n    beta_constraint=None,\n    gamma_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.LeakyReLU": "tf.keras.layers.LeakyReLU(\n    alpha=0.3, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.LocallyConnected1D": "tf.keras.layers.LocallyConnected1D(\n    filters,\n    kernel_size,\n    strides=1,\n    padding='valid',\n    data_format=None,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    implementation=1,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.LocallyConnected2D": "tf.keras.layers.LocallyConnected2D(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding='valid',\n    data_format=None,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    implementation=1,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Masking": "tf.keras.layers.Masking(\n    mask_value=0.0, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.MaxPool1D": "tf.keras.layers.MaxPool1D(\n    pool_size=2,\n    strides=None,\n    padding='valid',\n    data_format='channels_last',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.MaxPool2D": "tf.keras.layers.MaxPool2D(\n    pool_size=(2, 2),\n    strides=None,\n    padding='valid',\n    data_format=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.MaxPool3D": "tf.keras.layers.MaxPool3D(\n    pool_size=(2, 2, 2),\n    strides=None,\n    padding='valid',\n    data_format=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.MaxPool1D": "tf.keras.layers.MaxPool1D(\n    pool_size=2,\n    strides=None,\n    padding='valid',\n    data_format='channels_last',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.MaxPool2D": "tf.keras.layers.MaxPool2D(\n    pool_size=(2, 2),\n    strides=None,\n    padding='valid',\n    data_format=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.MaxPool3D": "tf.keras.layers.MaxPool3D(\n    pool_size=(2, 2, 2),\n    strides=None,\n    padding='valid',\n    data_format=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Maximum": "tf.keras.layers.Maximum(\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Minimum": "tf.keras.layers.Minimum(\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.MultiHeadAttention": "tf.keras.layers.MultiHeadAttention(\n    num_heads,\n    key_dim,\n    value_dim=None,\n    dropout=0.0,\n    use_bias=True,\n    output_shape=None,\n    attention_axes=None,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Multiply": "tf.keras.layers.Multiply(\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Normalization": "tf.keras.layers.Normalization(\n    axis=-1, mean=None, variance=None, invert=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.PReLU": "tf.keras.layers.PReLU(\n    alpha_initializer='zeros',\n    alpha_regularizer=None,\n    alpha_constraint=None,\n    shared_axes=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Permute": "tf.keras.layers.Permute(\n    dims, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RNN": "tf.keras.layers.RNN(\n    cell,\n    return_sequences=False,\n    return_state=False,\n    go_backwards=False,\n    stateful=False,\n    unroll=False,\n    time_major=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomBrightness": "tf.keras.layers.RandomBrightness(\n    factor, value_range=(0, 255), seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomContrast": "tf.keras.layers.RandomContrast(\n    factor, seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomCrop": "tf.keras.layers.RandomCrop(\n    height, width, seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomFlip": "tf.keras.layers.RandomFlip(\n    mode=HORIZONTAL_AND_VERTICAL, seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomHeight": "tf.keras.layers.RandomHeight(\n    factor, interpolation='bilinear', seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomRotation": "tf.keras.layers.RandomRotation(\n    factor,\n    fill_mode='reflect',\n    interpolation='bilinear',\n    seed=None,\n    fill_value=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomTranslation": "tf.keras.layers.RandomTranslation(\n    height_factor,\n    width_factor,\n    fill_mode='reflect',\n    interpolation='bilinear',\n    seed=None,\n    fill_value=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomWidth": "tf.keras.layers.RandomWidth(\n    factor, interpolation='bilinear', seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomZoom": "tf.keras.layers.RandomZoom(\n    height_factor,\n    width_factor=None,\n    fill_mode='reflect',\n    interpolation='bilinear',\n    seed=None,\n    fill_value=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.ReLU": "tf.keras.layers.ReLU(\n    max_value=None, negative_slope=0.0, threshold=0.0, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RepeatVector": "tf.keras.layers.RepeatVector(\n    n, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Rescaling": "tf.keras.layers.Rescaling(\n    scale, offset=0.0, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Reshape": "tf.keras.layers.Reshape(\n    target_shape, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Resizing": "tf.keras.layers.Resizing(\n    height,\n    width,\n    interpolation='bilinear',\n    crop_to_aspect_ratio=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.SeparableConv1D": "tf.keras.layers.SeparableConv1D(\n    filters,\n    kernel_size,\n    strides=1,\n    padding='valid',\n    data_format=None,\n    dilation_rate=1,\n    depth_multiplier=1,\n    activation=None,\n    use_bias=True,\n    depthwise_initializer='glorot_uniform',\n    pointwise_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    depthwise_regularizer=None,\n    pointwise_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    depthwise_constraint=None,\n    pointwise_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.SeparableConv2D": "tf.keras.layers.SeparableConv2D(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding='valid',\n    data_format=None,\n    dilation_rate=(1, 1),\n    depth_multiplier=1,\n    activation=None,\n    use_bias=True,\n    depthwise_initializer='glorot_uniform',\n    pointwise_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    depthwise_regularizer=None,\n    pointwise_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    depthwise_constraint=None,\n    pointwise_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.SeparableConv1D": "tf.keras.layers.SeparableConv1D(\n    filters,\n    kernel_size,\n    strides=1,\n    padding='valid',\n    data_format=None,\n    dilation_rate=1,\n    depth_multiplier=1,\n    activation=None,\n    use_bias=True,\n    depthwise_initializer='glorot_uniform',\n    pointwise_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    depthwise_regularizer=None,\n    pointwise_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    depthwise_constraint=None,\n    pointwise_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.SeparableConv2D": "tf.keras.layers.SeparableConv2D(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding='valid',\n    data_format=None,\n    dilation_rate=(1, 1),\n    depth_multiplier=1,\n    activation=None,\n    use_bias=True,\n    depthwise_initializer='glorot_uniform',\n    pointwise_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    depthwise_regularizer=None,\n    pointwise_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    depthwise_constraint=None,\n    pointwise_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.SimpleRNN": "tf.keras.layers.SimpleRNN(\n    units,\n    activation='tanh',\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    recurrent_initializer='orthogonal',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    recurrent_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    recurrent_constraint=None,\n    bias_constraint=None,\n    dropout=0.0,\n    recurrent_dropout=0.0,\n    return_sequences=False,\n    return_state=False,\n    go_backwards=False,\n    stateful=False,\n    unroll=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.SimpleRNNCell": "tf.keras.layers.SimpleRNNCell(\n    units,\n    activation='tanh',\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    recurrent_initializer='orthogonal',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    recurrent_regularizer=None,\n    bias_regularizer=None,\n    kernel_constraint=None,\n    recurrent_constraint=None,\n    bias_constraint=None,\n    dropout=0.0,\n    recurrent_dropout=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Softmax": "tf.keras.layers.Softmax(\n    axis=-1, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.SpatialDropout1D": "tf.keras.layers.SpatialDropout1D(\n    rate, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.SpatialDropout2D": "tf.keras.layers.SpatialDropout2D(\n    rate, data_format=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.SpatialDropout3D": "tf.keras.layers.SpatialDropout3D(\n    rate, data_format=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.StackedRNNCells": "tf.keras.layers.StackedRNNCells(\n    cells, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.StringLookup": "tf.keras.layers.StringLookup(\n    max_tokens=None,\n    num_oov_indices=1,\n    mask_token=None,\n    oov_token='[UNK]',\n    vocabulary=None,\n    idf_weights=None,\n    encoding='utf-8',\n    invert=False,\n    output_mode='int',\n    sparse=False,\n    pad_to_max_tokens=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Subtract": "tf.keras.layers.Subtract(\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.TextVectorization": "tf.keras.layers.TextVectorization(\n    max_tokens=None,\n    standardize='lower_and_strip_punctuation',\n    split='whitespace',\n    ngrams=None,\n    output_mode='int',\n    output_sequence_length=None,\n    pad_to_max_tokens=False,\n    vocabulary=None,\n    idf_weights=None,\n    sparse=False,\n    ragged=False,\n    encoding='utf-8',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.ThresholdedReLU": "tf.keras.layers.ThresholdedReLU(\n    theta=1.0, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.TimeDistributed": "tf.keras.layers.TimeDistributed(\n    layer, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.UnitNormalization": "tf.keras.layers.UnitNormalization(\n    axis=-1, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.UpSampling1D": "tf.keras.layers.UpSampling1D(\n    size=2, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.UpSampling2D": "tf.keras.layers.UpSampling2D(\n    size=(2, 2), data_format=None, interpolation='nearest', **kwargs\n)\n"
  },
  {
    "tf.keras.layers.UpSampling3D": "tf.keras.layers.UpSampling3D(\n    size=(2, 2, 2), data_format=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Wrapper": "tf.keras.layers.Wrapper(\n    layer, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.ZeroPadding1D": "tf.keras.layers.ZeroPadding1D(\n    padding=1, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.ZeroPadding2D": "tf.keras.layers.ZeroPadding2D(\n    padding=(1, 1), data_format=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.ZeroPadding3D": "tf.keras.layers.ZeroPadding3D(\n    padding=(1, 1, 1), data_format=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.add": "tf.keras.layers.add(\n    inputs, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.average": "tf.keras.layers.average(\n    inputs, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.concatenate": "tf.keras.layers.concatenate(\n    inputs, axis=-1, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.deserialize": "tf.keras.layers.deserialize(\n    config, custom_objects=None\n)\n"
  },
  {
    "tf.keras.layers.dot": "tf.keras.layers.dot(\n    inputs, axes, normalize=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.EinsumDense": "tf.keras.layers.EinsumDense(\n    equation,\n    output_shape,\n    activation=None,\n    bias_axes=None,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.experimental.RandomFourierFeatures": "tf.keras.layers.experimental.RandomFourierFeatures(\n    output_dim,\n    kernel_initializer='gaussian',\n    scale=None,\n    trainable=False,\n    name=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.experimental.SyncBatchNormalization": "tf.keras.layers.experimental.SyncBatchNormalization(\n    axis=-1,\n    momentum=0.99,\n    epsilon=0.001,\n    center=True,\n    scale=True,\n    beta_initializer='zeros',\n    gamma_initializer='ones',\n    moving_mean_initializer='zeros',\n    moving_variance_initializer='ones',\n    beta_regularizer=None,\n    gamma_regularizer=None,\n    beta_constraint=None,\n    gamma_constraint=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.CategoryEncoding": "tf.keras.layers.CategoryEncoding(\n    num_tokens=None, output_mode='multi_hot', sparse=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.CenterCrop": "tf.keras.layers.CenterCrop(\n    height, width, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Discretization": "tf.keras.layers.Discretization(\n    bin_boundaries=None,\n    num_bins=None,\n    epsilon=0.01,\n    output_mode='int',\n    sparse=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.experimental.preprocessing.HashedCrossing": "tf.keras.layers.experimental.preprocessing.HashedCrossing(\n    num_bins, output_mode='int', sparse=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Hashing": "tf.keras.layers.Hashing(\n    num_bins,\n    mask_value=None,\n    salt=None,\n    output_mode='int',\n    sparse=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.IntegerLookup": "tf.keras.layers.IntegerLookup(\n    max_tokens=None,\n    num_oov_indices=1,\n    mask_token=None,\n    oov_token=-1,\n    vocabulary=None,\n    vocabulary_dtype='int64',\n    idf_weights=None,\n    invert=False,\n    output_mode='int',\n    sparse=False,\n    pad_to_max_tokens=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Normalization": "tf.keras.layers.Normalization(\n    axis=-1, mean=None, variance=None, invert=False, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.experimental.preprocessing.PreprocessingLayer": "tf.keras.layers.experimental.preprocessing.PreprocessingLayer(\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomContrast": "tf.keras.layers.RandomContrast(\n    factor, seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomCrop": "tf.keras.layers.RandomCrop(\n    height, width, seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomFlip": "tf.keras.layers.RandomFlip(\n    mode=HORIZONTAL_AND_VERTICAL, seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomHeight": "tf.keras.layers.RandomHeight(\n    factor, interpolation='bilinear', seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomRotation": "tf.keras.layers.RandomRotation(\n    factor,\n    fill_mode='reflect',\n    interpolation='bilinear',\n    seed=None,\n    fill_value=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomTranslation": "tf.keras.layers.RandomTranslation(\n    height_factor,\n    width_factor,\n    fill_mode='reflect',\n    interpolation='bilinear',\n    seed=None,\n    fill_value=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomWidth": "tf.keras.layers.RandomWidth(\n    factor, interpolation='bilinear', seed=None, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.RandomZoom": "tf.keras.layers.RandomZoom(\n    height_factor,\n    width_factor=None,\n    fill_mode='reflect',\n    interpolation='bilinear',\n    seed=None,\n    fill_value=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Rescaling": "tf.keras.layers.Rescaling(\n    scale, offset=0.0, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.Resizing": "tf.keras.layers.Resizing(\n    height,\n    width,\n    interpolation='bilinear',\n    crop_to_aspect_ratio=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.StringLookup": "tf.keras.layers.StringLookup(\n    max_tokens=None,\n    num_oov_indices=1,\n    mask_token=None,\n    oov_token='[UNK]',\n    vocabulary=None,\n    idf_weights=None,\n    encoding='utf-8',\n    invert=False,\n    output_mode='int',\n    sparse=False,\n    pad_to_max_tokens=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.TextVectorization": "tf.keras.layers.TextVectorization(\n    max_tokens=None,\n    standardize='lower_and_strip_punctuation',\n    split='whitespace',\n    ngrams=None,\n    output_mode='int',\n    output_sequence_length=None,\n    pad_to_max_tokens=False,\n    vocabulary=None,\n    idf_weights=None,\n    sparse=False,\n    ragged=False,\n    encoding='utf-8',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.layers.maximum": "tf.keras.layers.maximum(\n    inputs, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.minimum": "tf.keras.layers.minimum(\n    inputs, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.multiply": "tf.keras.layers.multiply(\n    inputs, **kwargs\n)\n"
  },
  {
    "tf.keras.layers.serialize": "tf.keras.layers.serialize(\n    layer\n)\n"
  },
  {
    "tf.keras.layers.subtract": "tf.keras.layers.subtract(\n    inputs, **kwargs\n)\n"
  },
  {
    "tf.keras.losses.BinaryCrossentropy": "tf.keras.losses.BinaryCrossentropy(\n    from_logits=False,\n    label_smoothing=0.0,\n    axis=-1,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='binary_crossentropy'\n)\n"
  },
  {
    "tf.keras.losses.BinaryFocalCrossentropy": "tf.keras.losses.BinaryFocalCrossentropy(\n    apply_class_balancing=False,\n    alpha=0.25,\n    gamma=2.0,\n    from_logits=False,\n    label_smoothing=0.0,\n    axis=-1,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='binary_focal_crossentropy'\n)\n"
  },
  {
    "tf.keras.losses.CategoricalCrossentropy": "tf.keras.losses.CategoricalCrossentropy(\n    from_logits=False,\n    label_smoothing=0.0,\n    axis=-1,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='categorical_crossentropy'\n)\n"
  },
  {
    "tf.keras.losses.CategoricalHinge": "tf.keras.losses.CategoricalHinge(\n    reduction=losses_utils.ReductionV2.AUTO, name='categorical_hinge'\n)\n"
  },
  {
    "tf.keras.losses.CosineSimilarity": "tf.keras.losses.CosineSimilarity(\n    axis=-1,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='cosine_similarity'\n)\n"
  },
  {
    "tf.keras.losses.Hinge": "tf.keras.losses.Hinge(\n    reduction=losses_utils.ReductionV2.AUTO, name='hinge'\n)\n"
  },
  {
    "tf.keras.losses.Huber": "tf.keras.losses.Huber(\n    delta=1.0,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='huber_loss'\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.KLDivergence": "tf.keras.losses.KLDivergence(\n    reduction=losses_utils.ReductionV2.AUTO, name='kl_divergence'\n)\n"
  },
  {
    "tf.keras.losses.LogCosh": "tf.keras.losses.LogCosh(\n    reduction=losses_utils.ReductionV2.AUTO, name='log_cosh'\n)\n"
  },
  {
    "tf.keras.losses.Loss": "tf.keras.losses.Loss(\n    reduction=losses_utils.ReductionV2.AUTO, name=None\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.MeanAbsoluteError": "tf.keras.losses.MeanAbsoluteError(\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='mean_absolute_error'\n)\n"
  },
  {
    "tf.keras.losses.MeanAbsolutePercentageError": "tf.keras.losses.MeanAbsolutePercentageError(\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='mean_absolute_percentage_error'\n)\n"
  },
  {
    "tf.keras.losses.MeanSquaredError": "tf.keras.losses.MeanSquaredError(\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='mean_squared_error'\n)\n"
  },
  {
    "tf.keras.losses.MeanSquaredLogarithmicError": "tf.keras.losses.MeanSquaredLogarithmicError(\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='mean_squared_logarithmic_error'\n)\n"
  },
  {
    "tf.keras.losses.Poisson": "tf.keras.losses.Poisson(\n    reduction=losses_utils.ReductionV2.AUTO, name='poisson'\n)\n"
  },
  {
    "tf.keras.losses.SparseCategoricalCrossentropy": "tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=False,\n    ignore_class=None,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='sparse_categorical_crossentropy'\n)\n"
  },
  {
    "tf.keras.losses.SquaredHinge": "tf.keras.losses.SquaredHinge(\n    reduction=losses_utils.ReductionV2.AUTO, name='squared_hinge'\n)\n"
  },
  {
    "tf.keras.metrics.binary_crossentropy": "tf.keras.metrics.binary_crossentropy(\n    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.binary_focal_crossentropy": "tf.keras.metrics.binary_focal_crossentropy(\n    y_true,\n    y_pred,\n    apply_class_balancing=False,\n    alpha=0.25,\n    gamma=2.0,\n    from_logits=False,\n    label_smoothing=0.0,\n    axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.categorical_crossentropy": "tf.keras.metrics.categorical_crossentropy(\n    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n)\n"
  },
  {
    "tf.keras.losses.categorical_hinge": "tf.keras.losses.categorical_hinge(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.cosine_similarity": "tf.keras.losses.cosine_similarity(\n    y_true, y_pred, axis=-1\n)\n"
  },
  {
    "tf.keras.losses.deserialize": "tf.keras.losses.deserialize(\n    name, custom_objects=None\n)\n"
  },
  {
    "tf.keras.losses.get": "tf.keras.losses.get(\n    identifier\n)\n"
  },
  {
    "tf.keras.metrics.hinge": "tf.keras.metrics.hinge(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.huber": "tf.keras.losses.huber(\n    y_true, y_pred, delta=1.0\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.log_cosh": "tf.keras.losses.log_cosh(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.log_cosh": "tf.keras.losses.log_cosh(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.poisson": "tf.keras.metrics.poisson(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.serialize": "tf.keras.losses.serialize(\n    loss\n)\n"
  },
  {
    "tf.keras.metrics.sparse_categorical_crossentropy": "tf.keras.metrics.sparse_categorical_crossentropy(\n    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None\n)\n"
  },
  {
    "tf.keras.metrics.squared_hinge": "tf.keras.metrics.squared_hinge(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.AUC": "tf.keras.metrics.AUC(\n    num_thresholds=200,\n    curve='ROC',\n    summation_method='interpolation',\n    name=None,\n    dtype=None,\n    thresholds=None,\n    multi_label=False,\n    num_labels=None,\n    label_weights=None,\n    from_logits=False\n)\n"
  },
  {
    "tf.keras.metrics.Accuracy": "tf.keras.metrics.Accuracy(\n    name='accuracy', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.BinaryAccuracy": "tf.keras.metrics.BinaryAccuracy(\n    name='binary_accuracy', dtype=None, threshold=0.5\n)\n"
  },
  {
    "tf.keras.metrics.BinaryCrossentropy": "tf.keras.metrics.BinaryCrossentropy(\n    name='binary_crossentropy',\n    dtype=None,\n    from_logits=False,\n    label_smoothing=0\n)\n"
  },
  {
    "tf.keras.metrics.BinaryIoU": "tf.keras.metrics.BinaryIoU(\n    target_class_ids: Union[List[int], Tuple[int, ...]] = (0, 1),\n    threshold=0.5,\n    name=None,\n    dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.CategoricalAccuracy": "tf.keras.metrics.CategoricalAccuracy(\n    name='categorical_accuracy', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.CategoricalCrossentropy": "tf.keras.metrics.CategoricalCrossentropy(\n    name='categorical_crossentropy',\n    dtype=None,\n    from_logits=False,\n    label_smoothing=0,\n    axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.CategoricalHinge": "tf.keras.metrics.CategoricalHinge(\n    name='categorical_hinge', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.CosineSimilarity": "tf.keras.metrics.CosineSimilarity(\n    name='cosine_similarity', dtype=None, axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.FalseNegatives": "tf.keras.metrics.FalseNegatives(\n    thresholds=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.FalsePositives": "tf.keras.metrics.FalsePositives(\n    thresholds=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.Hinge": "tf.keras.metrics.Hinge(\n    name='hinge', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.IoU": "tf.keras.metrics.IoU(\n    num_classes: int,\n    target_class_ids: Union[List[int], Tuple[int, ...]],\n    name: Optional[str] = None,\n    dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n    ignore_class: Optional[int] = None,\n    sparse_y_true: bool = True,\n    sparse_y_pred: bool = True,\n    axis: int = -1\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.KLDivergence": "tf.keras.metrics.KLDivergence(\n    name='kullback_leibler_divergence', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.LogCoshError": "tf.keras.metrics.LogCoshError(\n    name='logcosh', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.Mean": "tf.keras.metrics.Mean(\n    name='mean', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanAbsoluteError": "tf.keras.metrics.MeanAbsoluteError(\n    name='mean_absolute_error', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanAbsolutePercentageError": "tf.keras.metrics.MeanAbsolutePercentageError(\n    name='mean_absolute_percentage_error', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanIoU": "tf.keras.metrics.MeanIoU(\n    num_classes: int,\n    name: Optional[str] = None,\n    dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n    ignore_class: Optional[int] = None,\n    sparse_y_true: bool = True,\n    sparse_y_pred: bool = True,\n    axis: int = -1\n)\n"
  },
  {
    "tf.keras.metrics.MeanMetricWrapper": "tf.keras.metrics.MeanMetricWrapper(\n    fn, name=None, dtype=None, **kwargs\n)\n"
  },
  {
    "tf.keras.metrics.MeanRelativeError": "tf.keras.metrics.MeanRelativeError(\n    normalizer, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanSquaredError": "tf.keras.metrics.MeanSquaredError(\n    name='mean_squared_error', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanSquaredLogarithmicError": "tf.keras.metrics.MeanSquaredLogarithmicError(\n    name='mean_squared_logarithmic_error', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanTensor": "tf.keras.metrics.MeanTensor(\n    name='mean_tensor', dtype=None, shape=None\n)\n"
  },
  {
    "tf.keras.metrics.Metric": "tf.keras.metrics.Metric(\n    name=None, dtype=None, **kwargs\n)\n"
  },
  {
    "tf.keras.metrics.OneHotIoU": "tf.keras.metrics.OneHotIoU(\n    num_classes: int,\n    target_class_ids: Union[List[int], Tuple[int, ...]],\n    name=None,\n    dtype=None,\n    ignore_class: Optional[int] = None,\n    sparse_y_pred: bool = False,\n    axis: int = -1\n)\n"
  },
  {
    "tf.keras.metrics.OneHotMeanIoU": "tf.keras.metrics.OneHotMeanIoU(\n    num_classes: int,\n    name: str = None,\n    dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n    ignore_class: Optional[int] = None,\n    sparse_y_pred: bool = False,\n    axis: int = -1\n)\n"
  },
  {
    "tf.keras.metrics.Poisson": "tf.keras.metrics.Poisson(\n    name='poisson', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.Precision": "tf.keras.metrics.Precision(\n    thresholds=None, top_k=None, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.PrecisionAtRecall": "tf.keras.metrics.PrecisionAtRecall(\n    recall, num_thresholds=200, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.Recall": "tf.keras.metrics.Recall(\n    thresholds=None, top_k=None, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.RecallAtPrecision": "tf.keras.metrics.RecallAtPrecision(\n    precision, num_thresholds=200, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.RootMeanSquaredError": "tf.keras.metrics.RootMeanSquaredError(\n    name='root_mean_squared_error', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.SensitivityAtSpecificity": "tf.keras.metrics.SensitivityAtSpecificity(\n    specificity, num_thresholds=200, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.SparseCategoricalAccuracy": "tf.keras.metrics.SparseCategoricalAccuracy(\n    name='sparse_categorical_accuracy', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.SparseCategoricalCrossentropy": "tf.keras.metrics.SparseCategoricalCrossentropy(\n    name: str = 'sparse_categorical_crossentropy',\n    dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n    from_logits: bool = False,\n    ignore_class: Optional[int] = None,\n    axis: int = -1\n)\n"
  },
  {
    "tf.keras.metrics.SparseTopKCategoricalAccuracy": "tf.keras.metrics.SparseTopKCategoricalAccuracy(\n    k=5, name='sparse_top_k_categorical_accuracy', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.SpecificityAtSensitivity": "tf.keras.metrics.SpecificityAtSensitivity(\n    sensitivity, num_thresholds=200, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.SquaredHinge": "tf.keras.metrics.SquaredHinge(\n    name='squared_hinge', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.Sum": "tf.keras.metrics.Sum(\n    name='sum', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.TopKCategoricalAccuracy": "tf.keras.metrics.TopKCategoricalAccuracy(\n    k=5, name='top_k_categorical_accuracy', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.TrueNegatives": "tf.keras.metrics.TrueNegatives(\n    thresholds=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.TruePositives": "tf.keras.metrics.TruePositives(\n    thresholds=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.binary_accuracy": "tf.keras.metrics.binary_accuracy(\n    y_true, y_pred, threshold=0.5\n)\n"
  },
  {
    "tf.keras.metrics.binary_crossentropy": "tf.keras.metrics.binary_crossentropy(\n    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.binary_focal_crossentropy": "tf.keras.metrics.binary_focal_crossentropy(\n    y_true,\n    y_pred,\n    apply_class_balancing=False,\n    alpha=0.25,\n    gamma=2.0,\n    from_logits=False,\n    label_smoothing=0.0,\n    axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.categorical_accuracy": "tf.keras.metrics.categorical_accuracy(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.categorical_crossentropy": "tf.keras.metrics.categorical_crossentropy(\n    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.deserialize": "tf.keras.metrics.deserialize(\n    config, custom_objects=None\n)\n"
  },
  {
    "tf.keras.metrics.get": "tf.keras.metrics.get(\n    identifier\n)\n"
  },
  {
    "tf.keras.metrics.hinge": "tf.keras.metrics.hinge(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.log_cosh": "tf.keras.losses.log_cosh(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.log_cosh": "tf.keras.losses.log_cosh(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.poisson": "tf.keras.metrics.poisson(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.serialize": "tf.keras.metrics.serialize(\n    metric\n)\n"
  },
  {
    "tf.keras.metrics.sparse_categorical_accuracy": "tf.keras.metrics.sparse_categorical_accuracy(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.sparse_categorical_crossentropy": "tf.keras.metrics.sparse_categorical_crossentropy(\n    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None\n)\n"
  },
  {
    "tf.keras.metrics.sparse_top_k_categorical_accuracy": "tf.keras.metrics.sparse_top_k_categorical_accuracy(\n    y_true, y_pred, k=5\n)\n"
  },
  {
    "tf.keras.metrics.squared_hinge": "tf.keras.metrics.squared_hinge(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.top_k_categorical_accuracy": "tf.keras.metrics.top_k_categorical_accuracy(\n    y_true, y_pred, k=5\n)\n"
  },
  {
    "tf.keras.mixed_precision.Policy": "tf.keras.mixed_precision.Policy(\n    name\n)\n"
  },
  {
    "tf.keras.mixed_precision.set_global_policy": "tf.keras.mixed_precision.set_global_policy(\n    policy\n)\n"
  },
  {
    "tf.keras.Model": "tf.keras.Model(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.keras.Sequential": "tf.keras.Sequential(\n    layers=None, name=None\n)\n"
  },
  {
    "tf.keras.models.clone_model": "tf.keras.models.clone_model(\n    model, input_tensors=None, clone_function=None\n)\n"
  },
  {
    "tf.keras.models.experimental.SharpnessAwareMinimization": "tf.keras.models.experimental.SharpnessAwareMinimization(\n    model, rho=0.05, num_batch_splits=None, name=None\n)\n"
  },
  {
    "tf.keras.models.load_model": "tf.keras.models.load_model(\n    filepath, custom_objects=None, compile=True, options=None\n)\n"
  },
  {
    "tf.keras.models.model_from_config": "tf.keras.models.model_from_config(\n    config, custom_objects=None\n)\n"
  },
  {
    "tf.keras.models.model_from_json": "tf.keras.models.model_from_json(\n    json_string, custom_objects=None\n)\n"
  },
  {
    "tf.keras.models.model_from_yaml": "tf.keras.models.model_from_yaml(\n    yaml_string, custom_objects=None\n)\n"
  },
  {
    "tf.keras.models.save_model": "tf.keras.models.save_model(\n    model,\n    filepath,\n    overwrite=True,\n    include_optimizer=True,\n    save_format=None,\n    signatures=None,\n    options=None,\n    save_traces=True\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adadelta": "tf.keras.optimizers.experimental.Adadelta(\n    learning_rate=0.001,\n    rho=0.95,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adadelta',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adagrad": "tf.keras.optimizers.experimental.Adagrad(\n    learning_rate=0.001,\n    initial_accumulator_value=0.1,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adagrad',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.Adam": "tf.keras.optimizers.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adamax": "tf.keras.optimizers.experimental.Adamax(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adamax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Ftrl": "tf.keras.optimizers.experimental.Ftrl(\n    learning_rate=0.001,\n    learning_rate_power=-0.5,\n    initial_accumulator_value=0.1,\n    l1_regularization_strength=0.0,\n    l2_regularization_strength=0.0,\n    l2_shrinkage_regularization_strength=0.0,\n    beta=0.0,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Ftrl',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Nadam": "tf.keras.optimizers.experimental.Nadam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Nadam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.Optimizer": "tf.keras.optimizers.Optimizer(\n    name,\n    weight_decay=0,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.RMSprop": "tf.keras.optimizers.experimental.RMSprop(\n    learning_rate=0.001,\n    rho=0.9,\n    momentum=0.0,\n    epsilon=1e-07,\n    centered=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=100,\n    jit_compile=True,\n    name='RMSprop',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.SGD": "tf.keras.optimizers.experimental.SGD(\n    learning_rate=0.01,\n    momentum=0.0,\n    nesterov=False,\n    amsgrad=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='SGD',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.deserialize": "tf.keras.optimizers.deserialize(\n    config, custom_objects=None, **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adadelta": "tf.keras.optimizers.experimental.Adadelta(\n    learning_rate=0.001,\n    rho=0.95,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adadelta',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adafactor": "tf.keras.optimizers.experimental.Adafactor(\n    learning_rate=0.001,\n    beta_2_decay=-0.8,\n    epsilon_1=1e-30,\n    epsilon_2=0.001,\n    clip_threshold=1.0,\n    relative_step=True,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adafactor',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adagrad": "tf.keras.optimizers.experimental.Adagrad(\n    learning_rate=0.001,\n    initial_accumulator_value=0.1,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adagrad',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.Adam": "tf.keras.optimizers.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.AdamW": "tf.keras.optimizers.experimental.AdamW(\n    learning_rate=0.001,\n    weight_decay=0.004,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='AdamW',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adamax": "tf.keras.optimizers.experimental.Adamax(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adamax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Ftrl": "tf.keras.optimizers.experimental.Ftrl(\n    learning_rate=0.001,\n    learning_rate_power=-0.5,\n    initial_accumulator_value=0.1,\n    l1_regularization_strength=0.0,\n    l2_regularization_strength=0.0,\n    l2_shrinkage_regularization_strength=0.0,\n    beta=0.0,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Ftrl',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Nadam": "tf.keras.optimizers.experimental.Nadam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Nadam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.Optimizer": "tf.keras.optimizers.Optimizer(\n    name,\n    weight_decay=0,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.RMSprop": "tf.keras.optimizers.experimental.RMSprop(\n    learning_rate=0.001,\n    rho=0.9,\n    momentum=0.0,\n    epsilon=1e-07,\n    centered=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=100,\n    jit_compile=True,\n    name='RMSprop',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.SGD": "tf.keras.optimizers.experimental.SGD(\n    learning_rate=0.01,\n    momentum=0.0,\n    nesterov=False,\n    amsgrad=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='SGD',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.get": "tf.keras.optimizers.get(\n    identifier, **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Adadelta": "tf.keras.optimizers.legacy.Adadelta(\n    learning_rate=0.001,\n    rho=0.95,\n    epsilon=1e-07,\n    name='Adadelta',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Adagrad": "tf.keras.optimizers.legacy.Adagrad(\n    learning_rate=0.001,\n    initial_accumulator_value=0.1,\n    epsilon=1e-07,\n    name='Adagrad',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Adam": "tf.keras.optimizers.legacy.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    name='Adam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Adamax": "tf.keras.optimizers.legacy.Adamax(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    name='Adamax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Ftrl": "tf.keras.optimizers.legacy.Ftrl(\n    learning_rate=0.001,\n    learning_rate_power=-0.5,\n    initial_accumulator_value=0.1,\n    l1_regularization_strength=0.0,\n    l2_regularization_strength=0.0,\n    name='Ftrl',\n    l2_shrinkage_regularization_strength=0.0,\n    beta=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Nadam": "tf.keras.optimizers.legacy.Nadam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    name='Nadam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Optimizer": "tf.keras.optimizers.legacy.Optimizer(\n    name, gradient_aggregator=None, gradient_transformers=None, **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.RMSprop": "tf.keras.optimizers.legacy.RMSprop(\n    learning_rate=0.001,\n    rho=0.9,\n    momentum=0.0,\n    epsilon=1e-07,\n    centered=False,\n    name='RMSprop',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.SGD": "tf.keras.optimizers.legacy.SGD(\n    learning_rate=0.01,\n    momentum=0.0,\n    nesterov=False,\n    name='SGD',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.CosineDecay": "tf.keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate, decay_steps, alpha=0.0, name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.CosineDecayRestarts": "tf.keras.optimizers.schedules.CosineDecayRestarts(\n    initial_learning_rate,\n    first_decay_steps,\n    t_mul=2.0,\n    m_mul=1.0,\n    alpha=0.0,\n    name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.ExponentialDecay": "tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate, decay_steps, decay_rate, staircase=False, name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.InverseTimeDecay": "tf.keras.optimizers.schedules.InverseTimeDecay(\n    initial_learning_rate, decay_steps, decay_rate, staircase=False, name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.PiecewiseConstantDecay": "tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n    boundaries, values, name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.PolynomialDecay": "tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate,\n    decay_steps,\n    end_learning_rate=0.0001,\n    power=1.0,\n    cycle=False,\n    name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.deserialize": "tf.keras.optimizers.schedules.deserialize(\n    config, custom_objects=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.serialize": "tf.keras.optimizers.schedules.serialize(\n    learning_rate_schedule\n)\n"
  },
  {
    "tf.keras.optimizers.serialize": "tf.keras.optimizers.serialize(\n    optimizer\n)\n"
  },
  {
    "tf.keras.preprocessing.image.DirectoryIterator": "tf.keras.preprocessing.image.DirectoryIterator(\n    directory,\n    image_data_generator,\n    target_size=(256, 256),\n    color_mode='rgb',\n    classes=None,\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=None,\n    data_format=None,\n    save_to_dir=None,\n    save_prefix='',\n    save_format='png',\n    follow_links=False,\n    subset=None,\n    interpolation='nearest',\n    keep_aspect_ratio=False,\n    dtype=None\n)\n"
  },
  {
    "tf.keras.preprocessing.image.ImageDataGenerator": "tf.keras.preprocessing.image.ImageDataGenerator(\n    featurewise_center=False,\n    samplewise_center=False,\n    featurewise_std_normalization=False,\n    samplewise_std_normalization=False,\n    zca_whitening=False,\n    zca_epsilon=1e-06,\n    rotation_range=0,\n    width_shift_range=0.0,\n    height_shift_range=0.0,\n    brightness_range=None,\n    shear_range=0.0,\n    zoom_range=0.0,\n    channel_shift_range=0.0,\n    fill_mode='nearest',\n    cval=0.0,\n    horizontal_flip=False,\n    vertical_flip=False,\n    rescale=None,\n    preprocessing_function=None,\n    data_format=None,\n    validation_split=0.0,\n    interpolation_order=1,\n    dtype=None\n)\n"
  },
  {
    "tf.keras.preprocessing.image.Iterator": "tf.keras.preprocessing.image.Iterator(\n    n, batch_size, shuffle, seed\n)\n"
  },
  {
    "tf.keras.preprocessing.image.NumpyArrayIterator": "tf.keras.preprocessing.image.NumpyArrayIterator(\n    x,\n    y,\n    image_data_generator,\n    batch_size=32,\n    shuffle=False,\n    sample_weight=None,\n    seed=None,\n    data_format=None,\n    save_to_dir=None,\n    save_prefix='',\n    save_format='png',\n    subset=None,\n    ignore_class_split=False,\n    dtype=None\n)\n"
  },
  {
    "tf.keras.preprocessing.image.apply_affine_transform": "tf.keras.preprocessing.image.apply_affine_transform(\n    x,\n    theta=0,\n    tx=0,\n    ty=0,\n    shear=0,\n    zx=1,\n    zy=1,\n    row_axis=1,\n    col_axis=2,\n    channel_axis=0,\n    fill_mode='nearest',\n    cval=0.0,\n    order=1\n)\n"
  },
  {
    "tf.keras.preprocessing.image.apply_brightness_shift": "tf.keras.preprocessing.image.apply_brightness_shift(\n    x, brightness, scale=True\n)\n"
  },
  {
    "tf.keras.preprocessing.image.apply_channel_shift": "tf.keras.preprocessing.image.apply_channel_shift(\n    x, intensity, channel_axis=0\n)\n"
  },
  {
    "tf.keras.utils.array_to_img": "tf.keras.utils.array_to_img(\n    x, data_format=None, scale=True, dtype=None\n)\n"
  },
  {
    "tf.keras.utils.img_to_array": "tf.keras.utils.img_to_array(\n    img, data_format=None, dtype=None\n)\n"
  },
  {
    "tf.keras.utils.load_img": "tf.keras.utils.load_img(\n    path,\n    grayscale=False,\n    color_mode='rgb',\n    target_size=None,\n    interpolation='nearest',\n    keep_aspect_ratio=False\n)\n"
  },
  {
    "tf.keras.preprocessing.image.random_brightness": "tf.keras.preprocessing.image.random_brightness(\n    x, brightness_range, scale=True\n)\n"
  },
  {
    "tf.keras.preprocessing.image.random_channel_shift": "tf.keras.preprocessing.image.random_channel_shift(\n    x, intensity_range, channel_axis=0\n)\n"
  },
  {
    "tf.keras.preprocessing.image.random_rotation": "tf.keras.preprocessing.image.random_rotation(\n    x,\n    rg,\n    row_axis=1,\n    col_axis=2,\n    channel_axis=0,\n    fill_mode='nearest',\n    cval=0.0,\n    interpolation_order=1\n)\n"
  },
  {
    "tf.keras.preprocessing.image.random_shear": "tf.keras.preprocessing.image.random_shear(\n    x,\n    intensity,\n    row_axis=1,\n    col_axis=2,\n    channel_axis=0,\n    fill_mode='nearest',\n    cval=0.0,\n    interpolation_order=1\n)\n"
  },
  {
    "tf.keras.preprocessing.image.random_shift": "tf.keras.preprocessing.image.random_shift(\n    x,\n    wrg,\n    hrg,\n    row_axis=1,\n    col_axis=2,\n    channel_axis=0,\n    fill_mode='nearest',\n    cval=0.0,\n    interpolation_order=1\n)\n"
  },
  {
    "tf.keras.preprocessing.image.random_zoom": "tf.keras.preprocessing.image.random_zoom(\n    x,\n    zoom_range,\n    row_axis=1,\n    col_axis=2,\n    channel_axis=0,\n    fill_mode='nearest',\n    cval=0.0,\n    interpolation_order=1\n)\n"
  },
  {
    "tf.keras.utils.save_img": "tf.keras.utils.save_img(\n    path, x, data_format=None, file_format=None, scale=True, **kwargs\n)\n"
  },
  {
    "tf.keras.preprocessing.image.smart_resize": "tf.keras.preprocessing.image.smart_resize(\n    x, size, interpolation='bilinear'\n)\n"
  },
  {
    "tf.keras.utils.image_dataset_from_directory": "tf.keras.utils.image_dataset_from_directory(\n    directory,\n    labels='inferred',\n    label_mode='int',\n    class_names=None,\n    color_mode='rgb',\n    batch_size=32,\n    image_size=(256, 256),\n    shuffle=True,\n    seed=None,\n    validation_split=None,\n    subset=None,\n    interpolation='bilinear',\n    follow_links=False,\n    crop_to_aspect_ratio=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.preprocessing.sequence.TimeseriesGenerator": "tf.keras.preprocessing.sequence.TimeseriesGenerator(\n    data,\n    targets,\n    length,\n    sampling_rate=1,\n    stride=1,\n    start_index=0,\n    end_index=None,\n    shuffle=False,\n    reverse=False,\n    batch_size=128\n)\n"
  },
  {
    "tf.keras.preprocessing.sequence.make_sampling_table": "tf.keras.preprocessing.sequence.make_sampling_table(\n    size, sampling_factor=1e-05\n)\n"
  },
  {
    "tf.keras.utils.pad_sequences": "tf.keras.utils.pad_sequences(\n    sequences,\n    maxlen=None,\n    dtype='int32',\n    padding='pre',\n    truncating='pre',\n    value=0.0\n)\n"
  },
  {
    "tf.keras.preprocessing.sequence.skipgrams": "tf.keras.preprocessing.sequence.skipgrams(\n    sequence,\n    vocabulary_size,\n    window_size=4,\n    negative_samples=1.0,\n    shuffle=True,\n    categorical=False,\n    sampling_table=None,\n    seed=None\n)\n"
  },
  {
    "tf.keras.preprocessing.text.Tokenizer": "tf.keras.preprocessing.text.Tokenizer(\n    num_words=None,\n    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n    lower=True,\n    split=' ',\n    char_level=False,\n    oov_token=None,\n    analyzer=None,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.preprocessing.text.hashing_trick": "tf.keras.preprocessing.text.hashing_trick(\n    text,\n    n,\n    hash_function=None,\n    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n    lower=True,\n    split=' ',\n    analyzer=None\n)\n"
  },
  {
    "tf.keras.preprocessing.text.one_hot": "tf.keras.preprocessing.text.one_hot(\n    input_text,\n    n,\n    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n    lower=True,\n    split=' ',\n    analyzer=None\n)\n"
  },
  {
    "tf.keras.preprocessing.text.text_to_word_sequence": "tf.keras.preprocessing.text.text_to_word_sequence(\n    input_text,\n    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n    lower=True,\n    split=' '\n)\n"
  },
  {
    "tf.keras.preprocessing.text.tokenizer_from_json": "tf.keras.preprocessing.text.tokenizer_from_json(\n    json_string\n)\n"
  },
  {
    "tf.keras.utils.text_dataset_from_directory": "tf.keras.utils.text_dataset_from_directory(\n    directory,\n    labels='inferred',\n    label_mode='int',\n    class_names=None,\n    batch_size=32,\n    max_length=None,\n    shuffle=True,\n    seed=None,\n    validation_split=None,\n    subset=None,\n    follow_links=False\n)\n"
  },
  {
    "tf.keras.utils.timeseries_dataset_from_array": "tf.keras.utils.timeseries_dataset_from_array(\n    data,\n    targets,\n    sequence_length,\n    sequence_stride=1,\n    sampling_rate=1,\n    batch_size=128,\n    shuffle=False,\n    seed=None,\n    start_index=None,\n    end_index=None\n)\n"
  },
  {
    "tf.keras.regularizers.L1": "tf.keras.regularizers.L1(\n    l1=0.01, **kwargs\n)\n"
  },
  {
    "tf.keras.regularizers.L1L2": "tf.keras.regularizers.L1L2(\n    l1=0.0, l2=0.0\n)\n"
  },
  {
    "tf.keras.regularizers.L2": "tf.keras.regularizers.L2(\n    l2=0.01, **kwargs\n)\n"
  },
  {
    "tf.keras.regularizers.OrthogonalRegularizer": "tf.keras.regularizers.OrthogonalRegularizer(\n    factor=0.01, mode='rows'\n)\n"
  },
  {
    "tf.keras.regularizers.deserialize": "tf.keras.regularizers.deserialize(\n    config, custom_objects=None\n)\n"
  },
  {
    "tf.keras.regularizers.get": "tf.keras.regularizers.get(\n    identifier\n)\n"
  },
  {
    "tf.keras.regularizers.L1": "tf.keras.regularizers.L1(\n    l1=0.01, **kwargs\n)\n"
  },
  {
    "tf.keras.regularizers.l1_l2": "tf.keras.regularizers.l1_l2(\n    l1=0.01, l2=0.01\n)\n"
  },
  {
    "tf.keras.regularizers.L2": "tf.keras.regularizers.L2(\n    l2=0.01, **kwargs\n)\n"
  },
  {
    "tf.keras.regularizers.OrthogonalRegularizer": "tf.keras.regularizers.OrthogonalRegularizer(\n    factor=0.01, mode='rows'\n)\n"
  },
  {
    "tf.keras.regularizers.serialize": "tf.keras.regularizers.serialize(\n    regularizer\n)\n"
  },
  {
    "tf.keras.utils.custom_object_scope": "tf.keras.utils.custom_object_scope(\n    *args\n)\n"
  },
  {
    "tf.keras.utils.GeneratorEnqueuer": "tf.keras.utils.GeneratorEnqueuer(\n    generator, use_multiprocessing=False, random_seed=None\n)\n"
  },
  {
    "tf.keras.utils.OrderedEnqueuer": "tf.keras.utils.OrderedEnqueuer(\n    sequence, use_multiprocessing=False, shuffle=False\n)\n"
  },
  {
    "tf.keras.utils.Progbar": "tf.keras.utils.Progbar(\n    target,\n    width=30,\n    verbose=1,\n    interval=0.05,\n    stateful_metrics=None,\n    unit_name='step'\n)\n"
  },
  {
    "tf.keras.utils.SequenceEnqueuer": "tf.keras.utils.SequenceEnqueuer(\n    sequence, use_multiprocessing=False\n)\n"
  },
  {
    "tf.keras.utils.SidecarEvaluator": "tf.keras.utils.SidecarEvaluator(\n    model,\n    data,\n    checkpoint_dir,\n    steps=None,\n    max_evaluations=None,\n    callbacks=None\n)\n"
  },
  {
    "tf.keras.utils.array_to_img": "tf.keras.utils.array_to_img(\n    x, data_format=None, scale=True, dtype=None\n)\n"
  },
  {
    "tf.keras.utils.audio_dataset_from_directory": "tf.keras.utils.audio_dataset_from_directory(\n    directory,\n    labels='inferred',\n    label_mode='int',\n    class_names=None,\n    batch_size=32,\n    sampling_rate=None,\n    output_sequence_length=None,\n    ragged=False,\n    shuffle=True,\n    seed=None,\n    validation_split=None,\n    subset=None,\n    follow_links=False\n)\n"
  },
  {
    "tf.keras.utils.custom_object_scope": "tf.keras.utils.custom_object_scope(\n    *args\n)\n"
  },
  {
    "tf.keras.utils.deserialize_keras_object": "tf.keras.utils.deserialize_keras_object(\n    identifier,\n    module_objects=None,\n    custom_objects=None,\n    printable_module_name='object'\n)\n"
  },
  {
    "tf.keras.utils.experimental.DatasetCreator": "tf.keras.utils.experimental.DatasetCreator(\n    dataset_fn, input_options=None\n)\n"
  },
  {
    "tf.keras.utils.get_file": "tf.keras.utils.get_file(\n    fname=None,\n    origin=None,\n    untar=False,\n    md5_hash=None,\n    file_hash=None,\n    cache_subdir='datasets',\n    hash_algorithm='auto',\n    extract=False,\n    archive_format='auto',\n    cache_dir=None\n)\n"
  },
  {
    "tf.keras.utils.get_registered_name": "tf.keras.utils.get_registered_name(\n    obj\n)\n"
  },
  {
    "tf.keras.utils.get_registered_object": "tf.keras.utils.get_registered_object(\n    name, custom_objects=None, module_objects=None\n)\n"
  },
  {
    "tf.keras.utils.get_source_inputs": "tf.keras.utils.get_source_inputs(\n    tensor, layer=None, node_index=None\n)\n"
  },
  {
    "tf.keras.utils.image_dataset_from_directory": "tf.keras.utils.image_dataset_from_directory(\n    directory,\n    labels='inferred',\n    label_mode='int',\n    class_names=None,\n    color_mode='rgb',\n    batch_size=32,\n    image_size=(256, 256),\n    shuffle=True,\n    seed=None,\n    validation_split=None,\n    subset=None,\n    interpolation='bilinear',\n    follow_links=False,\n    crop_to_aspect_ratio=False,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.utils.img_to_array": "tf.keras.utils.img_to_array(\n    img, data_format=None, dtype=None\n)\n"
  },
  {
    "tf.keras.utils.load_img": "tf.keras.utils.load_img(\n    path,\n    grayscale=False,\n    color_mode='rgb',\n    target_size=None,\n    interpolation='nearest',\n    keep_aspect_ratio=False\n)\n"
  },
  {
    "tf.keras.utils.model_to_dot": "tf.keras.utils.model_to_dot(\n    model,\n    show_shapes=False,\n    show_dtype=False,\n    show_layer_names=True,\n    rankdir='TB',\n    expand_nested=False,\n    dpi=96,\n    subgraph=False,\n    layer_range=None,\n    show_layer_activations=False\n)\n"
  },
  {
    "tf.keras.utils.normalize": "tf.keras.utils.normalize(\n    x, axis=-1, order=2\n)\n"
  },
  {
    "tf.keras.utils.pack_x_y_sample_weight": "tf.keras.utils.pack_x_y_sample_weight(\n    x, y=None, sample_weight=None\n)\n"
  },
  {
    "tf.keras.utils.pad_sequences": "tf.keras.utils.pad_sequences(\n    sequences,\n    maxlen=None,\n    dtype='int32',\n    padding='pre',\n    truncating='pre',\n    value=0.0\n)\n"
  },
  {
    "tf.keras.utils.plot_model": "tf.keras.utils.plot_model(\n    model,\n    to_file='model.png',\n    show_shapes=False,\n    show_dtype=False,\n    show_layer_names=True,\n    rankdir='TB',\n    expand_nested=False,\n    dpi=96,\n    layer_range=None,\n    show_layer_activations=False\n)\n"
  },
  {
    "tf.keras.utils.register_keras_serializable": "tf.keras.utils.register_keras_serializable(\n    package='Custom', name=None\n)\n"
  },
  {
    "tf.keras.utils.save_img": "tf.keras.utils.save_img(\n    path, x, data_format=None, file_format=None, scale=True, **kwargs\n)\n"
  },
  {
    "tf.keras.utils.serialize_keras_object": "tf.keras.utils.serialize_keras_object(\n    instance\n)\n"
  },
  {
    "tf.keras.utils.set_random_seed": "tf.keras.utils.set_random_seed(\n    seed\n)\n"
  },
  {
    "tf.keras.utils.split_dataset": "tf.keras.utils.split_dataset(\n    dataset, left_size=None, right_size=None, shuffle=False, seed=None\n)\n"
  },
  {
    "tf.keras.utils.text_dataset_from_directory": "tf.keras.utils.text_dataset_from_directory(\n    directory,\n    labels='inferred',\n    label_mode='int',\n    class_names=None,\n    batch_size=32,\n    max_length=None,\n    shuffle=True,\n    seed=None,\n    validation_split=None,\n    subset=None,\n    follow_links=False\n)\n"
  },
  {
    "tf.keras.utils.timeseries_dataset_from_array": "tf.keras.utils.timeseries_dataset_from_array(\n    data,\n    targets,\n    sequence_length,\n    sequence_stride=1,\n    sampling_rate=1,\n    batch_size=128,\n    shuffle=False,\n    seed=None,\n    start_index=None,\n    end_index=None\n)\n"
  },
  {
    "tf.keras.utils.to_categorical": "tf.keras.utils.to_categorical(\n    y, num_classes=None, dtype='float32'\n)\n"
  },
  {
    "tf.keras.utils.unpack_x_y_sample_weight": "tf.keras.utils.unpack_x_y_sample_weight(\n    data\n)\n"
  },
  {
    "tf.keras.utils.warmstart_embedding_matrix": "tf.keras.utils.warmstart_embedding_matrix(\n    base_vocabulary,\n    new_vocabulary,\n    base_embeddings,\n    new_embeddings_initializer='uniform'\n)\n"
  },
  {
    "tf.math.less": "tf.math.less(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.less_equal": "tf.math.less_equal(\n    x, y, name=None\n)\n"
  },
  {
    "tf.linalg.LinearOperator": "tf.linalg.LinearOperator(\n    dtype,\n    graph_parents=None,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name=None,\n    parameters=None\n)\n"
  },
  {
    "tf.linalg.LinearOperatorAdjoint": "tf.linalg.LinearOperatorAdjoint(\n    operator,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name=None\n)\n"
  },
  {
    "tf.linalg.LinearOperatorBlockDiag": "tf.linalg.LinearOperatorBlockDiag(\n    operators,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=True,\n    name=None\n)\n"
  },
  {
    "tf.linalg.LinearOperatorBlockLowerTriangular": "tf.linalg.LinearOperatorBlockLowerTriangular(\n    operators,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name='LinearOperatorBlockLowerTriangular'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorCirculant": "tf.linalg.LinearOperatorCirculant(\n    spectrum,\n    input_output_dtype=tf.dtypes.complex64,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=True,\n    name='LinearOperatorCirculant'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorCirculant2D": "tf.linalg.LinearOperatorCirculant2D(\n    spectrum,\n    input_output_dtype=tf.dtypes.complex64,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=True,\n    name='LinearOperatorCirculant2D'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorCirculant3D": "tf.linalg.LinearOperatorCirculant3D(\n    spectrum,\n    input_output_dtype=tf.dtypes.complex64,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=True,\n    name='LinearOperatorCirculant3D'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorComposition": "tf.linalg.LinearOperatorComposition(\n    operators,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name=None\n)\n"
  },
  {
    "tf.linalg.LinearOperatorDiag": "tf.linalg.LinearOperatorDiag(\n    diag,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name='LinearOperatorDiag'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorFullMatrix": "tf.linalg.LinearOperatorFullMatrix(\n    matrix,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name='LinearOperatorFullMatrix'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorHouseholder": "tf.linalg.LinearOperatorHouseholder(\n    reflection_axis,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name='LinearOperatorHouseholder'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorIdentity": "tf.linalg.LinearOperatorIdentity(\n    num_rows,\n    batch_shape=None,\n    dtype=None,\n    is_non_singular=True,\n    is_self_adjoint=True,\n    is_positive_definite=True,\n    is_square=True,\n    assert_proper_shapes=False,\n    name='LinearOperatorIdentity'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorInversion": "tf.linalg.LinearOperatorInversion(\n    operator,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name=None\n)\n"
  },
  {
    "tf.linalg.LinearOperatorKronecker": "tf.linalg.LinearOperatorKronecker(\n    operators,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name=None\n)\n"
  },
  {
    "tf.linalg.LinearOperatorLowRankUpdate": "tf.linalg.LinearOperatorLowRankUpdate(\n    base_operator,\n    u,\n    diag_update=None,\n    v=None,\n    is_diag_update_positive=None,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name='LinearOperatorLowRankUpdate'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorLowerTriangular": "tf.linalg.LinearOperatorLowerTriangular(\n    tril,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name='LinearOperatorLowerTriangular'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorPermutation": "tf.linalg.LinearOperatorPermutation(\n    perm,\n    dtype=tf.dtypes.float32,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name='LinearOperatorPermutation'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorScaledIdentity": "tf.linalg.LinearOperatorScaledIdentity(\n    num_rows,\n    multiplier,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=True,\n    assert_proper_shapes=False,\n    name='LinearOperatorScaledIdentity'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorToeplitz": "tf.linalg.LinearOperatorToeplitz(\n    col,\n    row,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name='LinearOperatorToeplitz'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorTridiag": "tf.linalg.LinearOperatorTridiag(\n    diagonals,\n    diagonals_format=_COMPACT,\n    is_non_singular=None,\n    is_self_adjoint=None,\n    is_positive_definite=None,\n    is_square=None,\n    name='LinearOperatorTridiag'\n)\n"
  },
  {
    "tf.linalg.LinearOperatorZeros": "tf.linalg.LinearOperatorZeros(\n    num_rows,\n    num_columns=None,\n    batch_shape=None,\n    dtype=None,\n    is_non_singular=False,\n    is_self_adjoint=True,\n    is_positive_definite=False,\n    is_square=True,\n    assert_proper_shapes=False,\n    name='LinearOperatorZeros'\n)\n"
  },
  {
    "tf.linalg.adjoint": "tf.linalg.adjoint(\n    matrix, name=None\n)\n"
  },
  {
    "tf.linalg.band_part": "tf.linalg.band_part(\n    input, num_lower, num_upper, name=None\n)\n"
  },
  {
    "tf.linalg.banded_triangular_solve": "tf.linalg.banded_triangular_solve(\n    bands, rhs, lower=True, adjoint=False, name=None\n)\n"
  },
  {
    "tf.linalg.cholesky": "tf.linalg.cholesky(\n    input, name=None\n)\n"
  },
  {
    "tf.linalg.cholesky_solve": "tf.linalg.cholesky_solve(\n    chol, rhs, name=None\n)\n"
  },
  {
    "tf.linalg.cross": "tf.linalg.cross(\n    a, b, name=None\n)\n"
  },
  {
    "tf.linalg.det": "tf.linalg.det(\n    input, name=None\n)\n"
  },
  {
    "tf.linalg.diag": "tf.linalg.diag(\n    diagonal,\n    name='diag',\n    k=0,\n    num_rows=-1,\n    num_cols=-1,\n    padding_value=0,\n    align='RIGHT_LEFT'\n)\n"
  },
  {
    "tf.linalg.diag_part": "tf.linalg.diag_part(\n    input,\n    name='diag_part',\n    k=0,\n    padding_value=0,\n    align='RIGHT_LEFT'\n)\n"
  },
  {
    "tf.linalg.eig": "tf.linalg.eig(\n    tensor, name=None\n)\n"
  },
  {
    "tf.linalg.eigh": "tf.linalg.eigh(\n    tensor, name=None\n)\n"
  },
  {
    "tf.linalg.eigh_tridiagonal": "tf.linalg.eigh_tridiagonal(\n    alpha,\n    beta,\n    eigvals_only=True,\n    select='a',\n    select_range=None,\n    tol=None,\n    name=None\n)\n"
  },
  {
    "tf.linalg.eigvals": "tf.linalg.eigvals(\n    tensor, name=None\n)\n"
  },
  {
    "tf.linalg.eigvalsh": "tf.linalg.eigvalsh(\n    tensor, name=None\n)\n"
  },
  {
    "tf.einsum": "tf.einsum(\n    equation, *inputs, **kwargs\n)\n"
  },
  {
    "tf.linalg.experimental.conjugate_gradient": "tf.linalg.experimental.conjugate_gradient(\n    operator,\n    rhs,\n    preconditioner=None,\n    x=None,\n    tol=1e-05,\n    max_iter=20,\n    name='conjugate_gradient'\n)\n"
  },
  {
    "tf.linalg.expm": "tf.linalg.expm(\n    input, name=None\n)\n"
  },
  {
    "tf.eye": "tf.eye(\n    num_rows,\n    num_columns=None,\n    batch_shape=None,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.linalg.global_norm": "tf.linalg.global_norm(\n    t_list, name=None\n)\n"
  },
  {
    "tf.linalg.inv": "tf.linalg.inv(\n    input, adjoint=False, name=None\n)\n"
  },
  {
    "tf.math.l2_normalize": "tf.math.l2_normalize(\n    x, axis=None, epsilon=1e-12, name=None, dim=None\n)\n"
  },
  {
    "tf.linalg.logdet": "tf.linalg.logdet(\n    matrix, name=None\n)\n"
  },
  {
    "tf.linalg.logm": "tf.linalg.logm(\n    input, name=None\n)\n"
  },
  {
    "tf.linalg.lstsq": "tf.linalg.lstsq(\n    matrix, rhs, l2_regularizer=0.0, fast=True, name=None\n)\n"
  },
  {
    "tf.linalg.lu": "tf.linalg.lu(\n    input,\n    output_idx_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.linalg.lu_matrix_inverse": "tf.linalg.lu_matrix_inverse(\n    lower_upper, perm, validate_args=False, name=None\n)\n"
  },
  {
    "tf.linalg.lu_reconstruct": "tf.linalg.lu_reconstruct(\n    lower_upper, perm, validate_args=False, name=None\n)\n"
  },
  {
    "tf.linalg.lu_solve": "tf.linalg.lu_solve(\n    lower_upper, perm, rhs, validate_args=False, name=None\n)\n"
  },
  {
    "tf.linalg.matmul": "tf.linalg.matmul(\n    a,\n    b,\n    transpose_a=False,\n    transpose_b=False,\n    adjoint_a=False,\n    adjoint_b=False,\n    a_is_sparse=False,\n    b_is_sparse=False,\n    output_type=None,\n    name=None\n)\n"
  },
  {
    "tf.linalg.matrix_rank": "tf.linalg.matrix_rank(\n    a, tol=None, validate_args=False, name=None\n)\n"
  },
  {
    "tf.linalg.matrix_transpose": "tf.linalg.matrix_transpose(\n    a, name='matrix_transpose', conjugate=False\n)\n"
  },
  {
    "tf.linalg.matvec": "tf.linalg.matvec(\n    a,\n    b,\n    transpose_a=False,\n    adjoint_a=False,\n    a_is_sparse=False,\n    b_is_sparse=False,\n    name=None\n)\n"
  },
  {
    "tf.norm": "tf.norm(\n    tensor, ord='euclidean', axis=None, keepdims=None, name=None\n)\n"
  },
  {
    "tf.linalg.normalize": "tf.linalg.normalize(\n    tensor, ord='euclidean', axis=None, name=None\n)\n"
  },
  {
    "tf.linalg.pinv": "tf.linalg.pinv(\n    a, rcond=None, validate_args=False, name=None\n)\n"
  },
  {
    "tf.linalg.qr": "tf.linalg.qr(\n    input, full_matrices=False, name=None\n)\n"
  },
  {
    "tf.linalg.set_diag": "tf.linalg.set_diag(\n    input,\n    diagonal,\n    name='set_diag',\n    k=0,\n    align='RIGHT_LEFT'\n)\n"
  },
  {
    "tf.linalg.slogdet": "tf.linalg.slogdet(\n    input, name=None\n)\n"
  },
  {
    "tf.linalg.solve": "tf.linalg.solve(\n    matrix, rhs, adjoint=False, name=None\n)\n"
  },
  {
    "tf.linalg.sqrtm": "tf.linalg.sqrtm(\n    input, name=None\n)\n"
  },
  {
    "tf.linalg.svd": "tf.linalg.svd(\n    tensor, full_matrices=False, compute_uv=True, name=None\n)\n"
  },
  {
    "tf.linalg.tensor_diag": "tf.linalg.tensor_diag(\n    diagonal, name=None\n)\n"
  },
  {
    "tf.linalg.tensor_diag_part": "tf.linalg.tensor_diag_part(\n    input, name=None\n)\n"
  },
  {
    "tf.tensordot": "tf.tensordot(\n    a, b, axes, name=None\n)\n"
  },
  {
    "tf.linalg.trace": "tf.linalg.trace(\n    x, name=None\n)\n"
  },
  {
    "tf.linalg.triangular_solve": "tf.linalg.triangular_solve(\n    matrix, rhs, lower=True, adjoint=False, name=None\n)\n"
  },
  {
    "tf.linalg.tridiagonal_matmul": "tf.linalg.tridiagonal_matmul(\n    diagonals, rhs, diagonals_format='compact', name=None\n)\n"
  },
  {
    "tf.linalg.tridiagonal_solve": "tf.linalg.tridiagonal_solve(\n    diagonals,\n    rhs,\n    diagonals_format='compact',\n    transpose_rhs=False,\n    conjugate_rhs=False,\n    name=None,\n    partial_pivoting=True,\n    perturb_singular=False\n)\n"
  },
  {
    "tf.linspace": "tf.linspace(\n    start, stop, num, name=None, axis=0\n)\n"
  },
  {
    "tf.lite.Interpreter": "tf.lite.Interpreter(\n    model_path=None,\n    model_content=None,\n    experimental_delegates=None,\n    num_threads=None,\n    experimental_op_resolver_type=tf.lite.experimental.OpResolverType.AUTO,\n    experimental_preserve_all_tensors=False\n)\n"
  },
  {
    "tf.lite.RepresentativeDataset": "tf.lite.RepresentativeDataset(\n    input_gen\n)\n"
  },
  {
    "tf.lite.TFLiteConverter": "tf.lite.TFLiteConverter(\n    funcs, trackable_obj=None\n)\n"
  },
  {
    "tf.lite.TargetSpec": "tf.lite.TargetSpec(\n    supported_ops=None,\n    supported_types=None,\n    experimental_select_user_tf_ops=None,\n    experimental_supported_backends=None\n)\n"
  },
  {
    "tf.lite.experimental.QuantizationDebugOptions": "tf.lite.experimental.QuantizationDebugOptions(\n    layer_debug_metrics: Optional[Mapping[str, Callable[[np.ndarray], float]]] = None,\n    model_debug_metrics: Optional[Mapping[str, Callable[[Sequence[np.ndarray], Sequence[np.ndarray]],\n        float]]] = None,\n    layer_direct_compare_metrics: Optional[Mapping[str, Callable[[Sequence[np.ndarray], Sequence[np.ndarray],\n        float, int], float]]] = None,\n    denylisted_ops: Optional[List[str]] = None,\n    denylisted_nodes: Optional[List[str]] = None,\n    fully_quantize: bool = False\n) -> None\n"
  },
  {
    "tf.lite.experimental.QuantizationDebugger": "tf.lite.experimental.QuantizationDebugger(\n    quant_debug_model_path: Optional[str] = None,\n    quant_debug_model_content: Optional[bytes] = None,\n    float_model_path: Optional[str] = None,\n    float_model_content: Optional[bytes] = None,\n    debug_dataset: Optional[Callable[[], Iterable[Sequence[np.ndarray]]]] = None,\n    debug_options: Optional[tf.lite.experimental.QuantizationDebugOptions] = None,\n    converter: Optional[TFLiteConverter] = None\n) -> None\n"
  },
  {
    "tf.lite.experimental.authoring.compatible": "tf.lite.experimental.authoring.compatible(\n    target=None, converter_target_spec=None, **kwargs\n)\n"
  },
  {
    "tf.lite.experimental.load_delegate": "tf.lite.experimental.load_delegate(\n    library, options=None\n)\n"
  },
  {
    "tf.load_library": "tf.load_library(\n    library_location\n)\n"
  },
  {
    "tf.load_op_library": "tf.load_op_library(\n    library_filename\n)\n"
  },
  {
    "tf.math.logical_and": "tf.math.logical_and(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.logical_not": "tf.math.logical_not(\n    x, name=None\n)\n"
  },
  {
    "tf.math.logical_or": "tf.math.logical_or(\n    x, y, name=None\n)\n"
  },
  {
    "tf.lookup.KeyValueTensorInitializer": "tf.lookup.KeyValueTensorInitializer(\n    keys, values, key_dtype=None, value_dtype=None, name=None\n)\n"
  },
  {
    "tf.lookup.StaticHashTable": "tf.lookup.StaticHashTable(\n    initializer, default_value, name=None, experimental_is_anonymous=False\n)\n"
  },
  {
    "tf.lookup.StaticVocabularyTable": "tf.lookup.StaticVocabularyTable(\n    initializer,\n    num_oov_buckets,\n    lookup_key_dtype=None,\n    name=None,\n    experimental_is_anonymous=False\n)\n"
  },
  {
    "tf.lookup.TextFileInitializer": "tf.lookup.TextFileInitializer(\n    filename,\n    key_dtype,\n    key_index,\n    value_dtype,\n    value_index,\n    vocab_size=None,\n    delimiter='\\t',\n    name=None,\n    value_index_offset=0\n)\n"
  },
  {
    "tf.lookup.experimental.DenseHashTable": "tf.lookup.experimental.DenseHashTable(\n    key_dtype,\n    value_dtype,\n    default_value,\n    empty_key,\n    deleted_key,\n    initial_num_buckets=None,\n    name='MutableDenseHashTable',\n    checkpoint=True,\n    experimental_is_anonymous=False\n)\n"
  },
  {
    "tf.lookup.experimental.MutableHashTable": "tf.lookup.experimental.MutableHashTable(\n    key_dtype,\n    value_dtype,\n    default_value,\n    name='MutableHashTable',\n    checkpoint=True,\n    experimental_is_anonymous=False\n)\n"
  },
  {
    "tf.keras.losses.BinaryCrossentropy": "tf.keras.losses.BinaryCrossentropy(\n    from_logits=False,\n    label_smoothing=0.0,\n    axis=-1,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='binary_crossentropy'\n)\n"
  },
  {
    "tf.keras.losses.BinaryFocalCrossentropy": "tf.keras.losses.BinaryFocalCrossentropy(\n    apply_class_balancing=False,\n    alpha=0.25,\n    gamma=2.0,\n    from_logits=False,\n    label_smoothing=0.0,\n    axis=-1,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='binary_focal_crossentropy'\n)\n"
  },
  {
    "tf.keras.losses.CategoricalCrossentropy": "tf.keras.losses.CategoricalCrossentropy(\n    from_logits=False,\n    label_smoothing=0.0,\n    axis=-1,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='categorical_crossentropy'\n)\n"
  },
  {
    "tf.keras.losses.CategoricalHinge": "tf.keras.losses.CategoricalHinge(\n    reduction=losses_utils.ReductionV2.AUTO, name='categorical_hinge'\n)\n"
  },
  {
    "tf.keras.losses.CosineSimilarity": "tf.keras.losses.CosineSimilarity(\n    axis=-1,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='cosine_similarity'\n)\n"
  },
  {
    "tf.keras.losses.Hinge": "tf.keras.losses.Hinge(\n    reduction=losses_utils.ReductionV2.AUTO, name='hinge'\n)\n"
  },
  {
    "tf.keras.losses.Huber": "tf.keras.losses.Huber(\n    delta=1.0,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='huber_loss'\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.KLDivergence": "tf.keras.losses.KLDivergence(\n    reduction=losses_utils.ReductionV2.AUTO, name='kl_divergence'\n)\n"
  },
  {
    "tf.keras.losses.LogCosh": "tf.keras.losses.LogCosh(\n    reduction=losses_utils.ReductionV2.AUTO, name='log_cosh'\n)\n"
  },
  {
    "tf.keras.losses.Loss": "tf.keras.losses.Loss(\n    reduction=losses_utils.ReductionV2.AUTO, name=None\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.MeanAbsoluteError": "tf.keras.losses.MeanAbsoluteError(\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='mean_absolute_error'\n)\n"
  },
  {
    "tf.keras.losses.MeanAbsolutePercentageError": "tf.keras.losses.MeanAbsolutePercentageError(\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='mean_absolute_percentage_error'\n)\n"
  },
  {
    "tf.keras.losses.MeanSquaredError": "tf.keras.losses.MeanSquaredError(\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='mean_squared_error'\n)\n"
  },
  {
    "tf.keras.losses.MeanSquaredLogarithmicError": "tf.keras.losses.MeanSquaredLogarithmicError(\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='mean_squared_logarithmic_error'\n)\n"
  },
  {
    "tf.keras.losses.Poisson": "tf.keras.losses.Poisson(\n    reduction=losses_utils.ReductionV2.AUTO, name='poisson'\n)\n"
  },
  {
    "tf.keras.losses.SparseCategoricalCrossentropy": "tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=False,\n    ignore_class=None,\n    reduction=losses_utils.ReductionV2.AUTO,\n    name='sparse_categorical_crossentropy'\n)\n"
  },
  {
    "tf.keras.losses.SquaredHinge": "tf.keras.losses.SquaredHinge(\n    reduction=losses_utils.ReductionV2.AUTO, name='squared_hinge'\n)\n"
  },
  {
    "tf.keras.metrics.binary_crossentropy": "tf.keras.metrics.binary_crossentropy(\n    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.binary_focal_crossentropy": "tf.keras.metrics.binary_focal_crossentropy(\n    y_true,\n    y_pred,\n    apply_class_balancing=False,\n    alpha=0.25,\n    gamma=2.0,\n    from_logits=False,\n    label_smoothing=0.0,\n    axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.categorical_crossentropy": "tf.keras.metrics.categorical_crossentropy(\n    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n)\n"
  },
  {
    "tf.keras.losses.categorical_hinge": "tf.keras.losses.categorical_hinge(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.cosine_similarity": "tf.keras.losses.cosine_similarity(\n    y_true, y_pred, axis=-1\n)\n"
  },
  {
    "tf.keras.losses.deserialize": "tf.keras.losses.deserialize(\n    name, custom_objects=None\n)\n"
  },
  {
    "tf.keras.losses.get": "tf.keras.losses.get(\n    identifier\n)\n"
  },
  {
    "tf.keras.metrics.hinge": "tf.keras.metrics.hinge(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.huber": "tf.keras.losses.huber(\n    y_true, y_pred, delta=1.0\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.log_cosh": "tf.keras.losses.log_cosh(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.log_cosh": "tf.keras.losses.log_cosh(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.poisson": "tf.keras.metrics.poisson(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.serialize": "tf.keras.losses.serialize(\n    loss\n)\n"
  },
  {
    "tf.keras.metrics.sparse_categorical_crossentropy": "tf.keras.metrics.sparse_categorical_crossentropy(\n    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None\n)\n"
  },
  {
    "tf.keras.metrics.squared_hinge": "tf.keras.metrics.squared_hinge(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.make_ndarray": "tf.make_ndarray(\n    tensor\n)\n"
  },
  {
    "tf.make_tensor_proto": "tf.make_tensor_proto(\n    values, dtype=None, shape=None, verify_shape=False, allow_broadcast=False\n)\n"
  },
  {
    "tf.map_fn": "tf.map_fn(\n    fn,\n    elems,\n    dtype=None,\n    parallel_iterations=None,\n    back_prop=True,\n    swap_memory=False,\n    infer_shape=True,\n    name=None,\n    fn_output_signature=None\n)\n"
  },
  {
    "tf.math.abs": "tf.math.abs(\n    x, name=None\n)\n"
  },
  {
    "tf.math.accumulate_n": "tf.math.accumulate_n(\n    inputs, shape=None, tensor_dtype=None, name=None\n)\n"
  },
  {
    "tf.math.acos": "tf.math.acos(\n    x, name=None\n)\n"
  },
  {
    "tf.math.acosh": "tf.math.acosh(\n    x, name=None\n)\n"
  },
  {
    "tf.math.add": "tf.math.add(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.add_n": "tf.math.add_n(\n    inputs, name=None\n)\n"
  },
  {
    "tf.math.angle": "tf.math.angle(\n    input, name=None\n)\n"
  },
  {
    "tf.math.approx_max_k": "tf.math.approx_max_k(\n    operand,\n    k,\n    reduction_dimension=-1,\n    recall_target=0.95,\n    reduction_input_size_override=-1,\n    aggregate_to_topk=True,\n    name=None\n)\n"
  },
  {
    "tf.math.approx_min_k": "tf.math.approx_min_k(\n    operand,\n    k,\n    reduction_dimension=-1,\n    recall_target=0.95,\n    reduction_input_size_override=-1,\n    aggregate_to_topk=True,\n    name=None\n)\n"
  },
  {
    "tf.math.argmax": "tf.math.argmax(\n    input,\n    axis=None,\n    output_type=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.math.argmin": "tf.math.argmin(\n    input,\n    axis=None,\n    output_type=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.math.asin": "tf.math.asin(\n    x, name=None\n)\n"
  },
  {
    "tf.math.asinh": "tf.math.asinh(\n    x, name=None\n)\n"
  },
  {
    "tf.math.atan": "tf.math.atan(\n    x, name=None\n)\n"
  },
  {
    "tf.math.atan2": "tf.math.atan2(\n    y, x, name=None\n)\n"
  },
  {
    "tf.math.atanh": "tf.math.atanh(\n    x, name=None\n)\n"
  },
  {
    "tf.math.bessel_i0": "tf.math.bessel_i0(\n    x, name=None\n)\n"
  },
  {
    "tf.math.bessel_i0e": "tf.math.bessel_i0e(\n    x, name=None\n)\n"
  },
  {
    "tf.math.bessel_i1": "tf.math.bessel_i1(\n    x, name=None\n)\n"
  },
  {
    "tf.math.bessel_i1e": "tf.math.bessel_i1e(\n    x, name=None\n)\n"
  },
  {
    "tf.math.betainc": "tf.math.betainc(\n    a, b, x, name=None\n)\n"
  },
  {
    "tf.math.bincount": "tf.math.bincount(\n    arr,\n    weights=None,\n    minlength=None,\n    maxlength=None,\n    dtype=tf.dtypes.int32,\n    name=None,\n    axis=None,\n    binary_output=False\n)\n"
  },
  {
    "tf.math.ceil": "tf.math.ceil(\n    x, name=None\n)\n"
  },
  {
    "tf.math.confusion_matrix": "tf.math.confusion_matrix(\n    labels,\n    predictions,\n    num_classes=None,\n    weights=None,\n    dtype=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.math.conj": "tf.math.conj(\n    x, name=None\n)\n"
  },
  {
    "tf.math.cos": "tf.math.cos(\n    x, name=None\n)\n"
  },
  {
    "tf.math.cosh": "tf.math.cosh(\n    x, name=None\n)\n"
  },
  {
    "tf.math.count_nonzero": "tf.math.count_nonzero(\n    input,\n    axis=None,\n    keepdims=None,\n    dtype=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.math.cumprod": "tf.math.cumprod(\n    x, axis=0, exclusive=False, reverse=False, name=None\n)\n"
  },
  {
    "tf.math.cumsum": "tf.math.cumsum(\n    x, axis=0, exclusive=False, reverse=False, name=None\n)\n"
  },
  {
    "tf.math.cumulative_logsumexp": "tf.math.cumulative_logsumexp(\n    x, axis=0, exclusive=False, reverse=False, name=None\n)\n"
  },
  {
    "tf.math.digamma": "tf.math.digamma(\n    x, name=None\n)\n"
  },
  {
    "tf.math.divide": "tf.math.divide(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.divide_no_nan": "tf.math.divide_no_nan(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.equal": "tf.math.equal(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.erf": "tf.math.erf(\n    x, name=None\n)\n"
  },
  {
    "tf.math.erfc": "tf.math.erfc(\n    x, name=None\n)\n"
  },
  {
    "tf.math.erfcinv": "tf.math.erfcinv(\n    x, name=None\n)\n"
  },
  {
    "tf.math.erfinv": "tf.math.erfinv(\n    x, name=None\n)\n"
  },
  {
    "tf.math.exp": "tf.math.exp(\n    x, name=None\n)\n"
  },
  {
    "tf.math.expm1": "tf.math.expm1(\n    x, name=None\n)\n"
  },
  {
    "tf.math.floor": "tf.math.floor(\n    x, name=None\n)\n"
  },
  {
    "tf.math.floordiv": "tf.math.floordiv(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.floormod": "tf.math.floormod(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.greater": "tf.math.greater(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.greater_equal": "tf.math.greater_equal(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.igamma": "tf.math.igamma(\n    a, x, name=None\n)\n"
  },
  {
    "tf.math.igammac": "tf.math.igammac(\n    a, x, name=None\n)\n"
  },
  {
    "tf.math.imag": "tf.math.imag(\n    input, name=None\n)\n"
  },
  {
    "tf.math.in_top_k": "tf.math.in_top_k(\n    targets, predictions, k, name=None\n)\n"
  },
  {
    "tf.math.invert_permutation": "tf.math.invert_permutation(\n    x, name=None\n)\n"
  },
  {
    "tf.math.is_finite": "tf.math.is_finite(\n    x, name=None\n)\n"
  },
  {
    "tf.math.is_inf": "tf.math.is_inf(\n    x, name=None\n)\n"
  },
  {
    "tf.math.is_nan": "tf.math.is_nan(\n    x, name=None\n)\n"
  },
  {
    "tf.math.is_non_decreasing": "tf.math.is_non_decreasing(\n    x, name=None\n)\n"
  },
  {
    "tf.math.is_strictly_increasing": "tf.math.is_strictly_increasing(\n    x, name=None\n)\n"
  },
  {
    "tf.math.l2_normalize": "tf.math.l2_normalize(\n    x, axis=None, epsilon=1e-12, name=None, dim=None\n)\n"
  },
  {
    "tf.math.lbeta": "tf.math.lbeta(\n    x, name=None\n)\n"
  },
  {
    "tf.math.less": "tf.math.less(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.less_equal": "tf.math.less_equal(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.lgamma": "tf.math.lgamma(\n    x, name=None\n)\n"
  },
  {
    "tf.math.log": "tf.math.log(\n    x, name=None\n)\n"
  },
  {
    "tf.math.log1p": "tf.math.log1p(\n    x, name=None\n)\n"
  },
  {
    "tf.math.log_sigmoid": "tf.math.log_sigmoid(\n    x, name=None\n)\n"
  },
  {
    "tf.nn.log_softmax": "tf.nn.log_softmax(\n    logits, axis=None, name=None\n)\n"
  },
  {
    "tf.math.logical_and": "tf.math.logical_and(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.logical_not": "tf.math.logical_not(\n    x, name=None\n)\n"
  },
  {
    "tf.math.logical_or": "tf.math.logical_or(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.logical_xor": "tf.math.logical_xor(\n    x, y, name='LogicalXor'\n)\n"
  },
  {
    "tf.math.maximum": "tf.math.maximum(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.minimum": "tf.math.minimum(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.floormod": "tf.math.floormod(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.multiply": "tf.math.multiply(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.multiply_no_nan": "tf.math.multiply_no_nan(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.ndtri": "tf.math.ndtri(\n    x, name=None\n)\n"
  },
  {
    "tf.math.negative": "tf.math.negative(\n    x, name=None\n)\n"
  },
  {
    "tf.math.nextafter": "tf.math.nextafter(\n    x1, x2, name=None\n)\n"
  },
  {
    "tf.math.not_equal": "tf.math.not_equal(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.polygamma": "tf.math.polygamma(\n    a, x, name=None\n)\n"
  },
  {
    "tf.math.polyval": "tf.math.polyval(\n    coeffs, x, name=None\n)\n"
  },
  {
    "tf.math.pow": "tf.math.pow(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.real": "tf.math.real(\n    input, name=None\n)\n"
  },
  {
    "tf.math.reciprocal": "tf.math.reciprocal(\n    x, name=None\n)\n"
  },
  {
    "tf.math.reciprocal_no_nan": "tf.math.reciprocal_no_nan(\n    x, name=None\n)\n"
  },
  {
    "tf.math.reduce_all": "tf.math.reduce_all(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_any": "tf.math.reduce_any(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_euclidean_norm": "tf.math.reduce_euclidean_norm(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_logsumexp": "tf.math.reduce_logsumexp(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_max": "tf.math.reduce_max(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_mean": "tf.math.reduce_mean(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_min": "tf.math.reduce_min(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_prod": "tf.math.reduce_prod(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_std": "tf.math.reduce_std(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_sum": "tf.math.reduce_sum(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_variance": "tf.math.reduce_variance(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.rint": "tf.math.rint(\n    x, name=None\n)\n"
  },
  {
    "tf.math.round": "tf.math.round(\n    x, name=None\n)\n"
  },
  {
    "tf.math.rsqrt": "tf.math.rsqrt(\n    x, name=None\n)\n"
  },
  {
    "tf.math.scalar_mul": "tf.math.scalar_mul(\n    scalar, x, name=None\n)\n"
  },
  {
    "tf.math.segment_max": "tf.math.segment_max(\n    data, segment_ids, name=None\n)\n"
  },
  {
    "tf.math.segment_mean": "tf.math.segment_mean(\n    data, segment_ids, name=None\n)\n"
  },
  {
    "tf.math.segment_min": "tf.math.segment_min(\n    data, segment_ids, name=None\n)\n"
  },
  {
    "tf.math.segment_prod": "tf.math.segment_prod(\n    data, segment_ids, name=None\n)\n"
  },
  {
    "tf.math.segment_sum": "tf.math.segment_sum(\n    data, segment_ids, name=None\n)\n"
  },
  {
    "tf.math.sigmoid": "tf.math.sigmoid(\n    x, name=None\n)\n"
  },
  {
    "tf.math.sign": "tf.math.sign(\n    x, name=None\n)\n"
  },
  {
    "tf.math.sin": "tf.math.sin(\n    x, name=None\n)\n"
  },
  {
    "tf.math.sinh": "tf.math.sinh(\n    x, name=None\n)\n"
  },
  {
    "tf.math.sobol_sample": "tf.math.sobol_sample(\n    dim,\n    num_results,\n    skip=0,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.nn.softmax": "tf.nn.softmax(\n    logits, axis=None, name=None\n)\n"
  },
  {
    "tf.math.softplus": "tf.math.softplus(\n    features, name=None\n)\n"
  },
  {
    "tf.nn.softsign": "tf.nn.softsign(\n    features, name=None\n)\n"
  },
  {
    "tf.math.bessel_i0": "tf.math.bessel_i0(\n    x, name=None\n)\n"
  },
  {
    "tf.math.bessel_i0e": "tf.math.bessel_i0e(\n    x, name=None\n)\n"
  },
  {
    "tf.math.bessel_i1": "tf.math.bessel_i1(\n    x, name=None\n)\n"
  },
  {
    "tf.math.bessel_i1e": "tf.math.bessel_i1e(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.bessel_j0": "tf.math.special.bessel_j0(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.bessel_j1": "tf.math.special.bessel_j1(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.bessel_k0": "tf.math.special.bessel_k0(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.bessel_k0e": "tf.math.special.bessel_k0e(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.bessel_k1": "tf.math.special.bessel_k1(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.bessel_k1e": "tf.math.special.bessel_k1e(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.bessel_y0": "tf.math.special.bessel_y0(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.bessel_y1": "tf.math.special.bessel_y1(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.dawsn": "tf.math.special.dawsn(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.expint": "tf.math.special.expint(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.fresnel_cos": "tf.math.special.fresnel_cos(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.fresnel_sin": "tf.math.special.fresnel_sin(\n    x, name=None\n)\n"
  },
  {
    "tf.math.special.spence": "tf.math.special.spence(\n    x, name=None\n)\n"
  },
  {
    "tf.math.sqrt": "tf.math.sqrt(\n    x, name=None\n)\n"
  },
  {
    "tf.math.square": "tf.math.square(\n    x, name=None\n)\n"
  },
  {
    "tf.math.squared_difference": "tf.math.squared_difference(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.subtract": "tf.math.subtract(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.tan": "tf.math.tan(\n    x, name=None\n)\n"
  },
  {
    "tf.math.tanh": "tf.math.tanh(\n    x, name=None\n)\n"
  },
  {
    "tf.math.top_k": "tf.math.top_k(\n    input, k=1, sorted=True, name=None\n)\n"
  },
  {
    "tf.math.truediv": "tf.math.truediv(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.unsorted_segment_max": "tf.math.unsorted_segment_max(\n    data, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.math.unsorted_segment_mean": "tf.math.unsorted_segment_mean(\n    data, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.math.unsorted_segment_min": "tf.math.unsorted_segment_min(\n    data, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.math.unsorted_segment_prod": "tf.math.unsorted_segment_prod(\n    data, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.math.unsorted_segment_sqrt_n": "tf.math.unsorted_segment_sqrt_n(\n    data, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.math.unsorted_segment_sum": "tf.math.unsorted_segment_sum(\n    data, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.math.xdivy": "tf.math.xdivy(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.xlog1py": "tf.math.xlog1py(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.xlogy": "tf.math.xlogy(\n    x, y, name=None\n)\n"
  },
  {
    "tf.math.zero_fraction": "tf.math.zero_fraction(\n    value, name=None\n)\n"
  },
  {
    "tf.math.zeta": "tf.math.zeta(\n    x, q, name=None\n)\n"
  },
  {
    "tf.linalg.matmul": "tf.linalg.matmul(\n    a,\n    b,\n    transpose_a=False,\n    transpose_b=False,\n    adjoint_a=False,\n    adjoint_b=False,\n    a_is_sparse=False,\n    b_is_sparse=False,\n    output_type=None,\n    name=None\n)\n"
  },
  {
    "tf.linalg.sqrtm": "tf.linalg.sqrtm(\n    input, name=None\n)\n"
  },
  {
    "tf.math.maximum": "tf.math.maximum(\n    x, y, name=None\n)\n"
  },
  {
    "tf.meshgrid": "tf.meshgrid(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.keras.metrics.AUC": "tf.keras.metrics.AUC(\n    num_thresholds=200,\n    curve='ROC',\n    summation_method='interpolation',\n    name=None,\n    dtype=None,\n    thresholds=None,\n    multi_label=False,\n    num_labels=None,\n    label_weights=None,\n    from_logits=False\n)\n"
  },
  {
    "tf.keras.metrics.Accuracy": "tf.keras.metrics.Accuracy(\n    name='accuracy', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.BinaryAccuracy": "tf.keras.metrics.BinaryAccuracy(\n    name='binary_accuracy', dtype=None, threshold=0.5\n)\n"
  },
  {
    "tf.keras.metrics.BinaryCrossentropy": "tf.keras.metrics.BinaryCrossentropy(\n    name='binary_crossentropy',\n    dtype=None,\n    from_logits=False,\n    label_smoothing=0\n)\n"
  },
  {
    "tf.keras.metrics.BinaryIoU": "tf.keras.metrics.BinaryIoU(\n    target_class_ids: Union[List[int], Tuple[int, ...]] = (0, 1),\n    threshold=0.5,\n    name=None,\n    dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.CategoricalAccuracy": "tf.keras.metrics.CategoricalAccuracy(\n    name='categorical_accuracy', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.CategoricalCrossentropy": "tf.keras.metrics.CategoricalCrossentropy(\n    name='categorical_crossentropy',\n    dtype=None,\n    from_logits=False,\n    label_smoothing=0,\n    axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.CategoricalHinge": "tf.keras.metrics.CategoricalHinge(\n    name='categorical_hinge', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.CosineSimilarity": "tf.keras.metrics.CosineSimilarity(\n    name='cosine_similarity', dtype=None, axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.FalseNegatives": "tf.keras.metrics.FalseNegatives(\n    thresholds=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.FalsePositives": "tf.keras.metrics.FalsePositives(\n    thresholds=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.Hinge": "tf.keras.metrics.Hinge(\n    name='hinge', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.IoU": "tf.keras.metrics.IoU(\n    num_classes: int,\n    target_class_ids: Union[List[int], Tuple[int, ...]],\n    name: Optional[str] = None,\n    dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n    ignore_class: Optional[int] = None,\n    sparse_y_true: bool = True,\n    sparse_y_pred: bool = True,\n    axis: int = -1\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.KLDivergence": "tf.keras.metrics.KLDivergence(\n    name='kullback_leibler_divergence', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.LogCoshError": "tf.keras.metrics.LogCoshError(\n    name='logcosh', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.Mean": "tf.keras.metrics.Mean(\n    name='mean', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanAbsoluteError": "tf.keras.metrics.MeanAbsoluteError(\n    name='mean_absolute_error', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanAbsolutePercentageError": "tf.keras.metrics.MeanAbsolutePercentageError(\n    name='mean_absolute_percentage_error', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanIoU": "tf.keras.metrics.MeanIoU(\n    num_classes: int,\n    name: Optional[str] = None,\n    dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n    ignore_class: Optional[int] = None,\n    sparse_y_true: bool = True,\n    sparse_y_pred: bool = True,\n    axis: int = -1\n)\n"
  },
  {
    "tf.keras.metrics.MeanMetricWrapper": "tf.keras.metrics.MeanMetricWrapper(\n    fn, name=None, dtype=None, **kwargs\n)\n"
  },
  {
    "tf.keras.metrics.MeanRelativeError": "tf.keras.metrics.MeanRelativeError(\n    normalizer, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanSquaredError": "tf.keras.metrics.MeanSquaredError(\n    name='mean_squared_error', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanSquaredLogarithmicError": "tf.keras.metrics.MeanSquaredLogarithmicError(\n    name='mean_squared_logarithmic_error', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.MeanTensor": "tf.keras.metrics.MeanTensor(\n    name='mean_tensor', dtype=None, shape=None\n)\n"
  },
  {
    "tf.keras.metrics.Metric": "tf.keras.metrics.Metric(\n    name=None, dtype=None, **kwargs\n)\n"
  },
  {
    "tf.keras.metrics.OneHotIoU": "tf.keras.metrics.OneHotIoU(\n    num_classes: int,\n    target_class_ids: Union[List[int], Tuple[int, ...]],\n    name=None,\n    dtype=None,\n    ignore_class: Optional[int] = None,\n    sparse_y_pred: bool = False,\n    axis: int = -1\n)\n"
  },
  {
    "tf.keras.metrics.OneHotMeanIoU": "tf.keras.metrics.OneHotMeanIoU(\n    num_classes: int,\n    name: str = None,\n    dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n    ignore_class: Optional[int] = None,\n    sparse_y_pred: bool = False,\n    axis: int = -1\n)\n"
  },
  {
    "tf.keras.metrics.Poisson": "tf.keras.metrics.Poisson(\n    name='poisson', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.Precision": "tf.keras.metrics.Precision(\n    thresholds=None, top_k=None, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.PrecisionAtRecall": "tf.keras.metrics.PrecisionAtRecall(\n    recall, num_thresholds=200, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.Recall": "tf.keras.metrics.Recall(\n    thresholds=None, top_k=None, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.RecallAtPrecision": "tf.keras.metrics.RecallAtPrecision(\n    precision, num_thresholds=200, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.RootMeanSquaredError": "tf.keras.metrics.RootMeanSquaredError(\n    name='root_mean_squared_error', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.SensitivityAtSpecificity": "tf.keras.metrics.SensitivityAtSpecificity(\n    specificity, num_thresholds=200, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.SparseCategoricalAccuracy": "tf.keras.metrics.SparseCategoricalAccuracy(\n    name='sparse_categorical_accuracy', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.SparseCategoricalCrossentropy": "tf.keras.metrics.SparseCategoricalCrossentropy(\n    name: str = 'sparse_categorical_crossentropy',\n    dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n    from_logits: bool = False,\n    ignore_class: Optional[int] = None,\n    axis: int = -1\n)\n"
  },
  {
    "tf.keras.metrics.SparseTopKCategoricalAccuracy": "tf.keras.metrics.SparseTopKCategoricalAccuracy(\n    k=5, name='sparse_top_k_categorical_accuracy', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.SpecificityAtSensitivity": "tf.keras.metrics.SpecificityAtSensitivity(\n    sensitivity, num_thresholds=200, class_id=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.SquaredHinge": "tf.keras.metrics.SquaredHinge(\n    name='squared_hinge', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.Sum": "tf.keras.metrics.Sum(\n    name='sum', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.TopKCategoricalAccuracy": "tf.keras.metrics.TopKCategoricalAccuracy(\n    k=5, name='top_k_categorical_accuracy', dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.TrueNegatives": "tf.keras.metrics.TrueNegatives(\n    thresholds=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.TruePositives": "tf.keras.metrics.TruePositives(\n    thresholds=None, name=None, dtype=None\n)\n"
  },
  {
    "tf.keras.metrics.binary_accuracy": "tf.keras.metrics.binary_accuracy(\n    y_true, y_pred, threshold=0.5\n)\n"
  },
  {
    "tf.keras.metrics.binary_crossentropy": "tf.keras.metrics.binary_crossentropy(\n    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.binary_focal_crossentropy": "tf.keras.metrics.binary_focal_crossentropy(\n    y_true,\n    y_pred,\n    apply_class_balancing=False,\n    alpha=0.25,\n    gamma=2.0,\n    from_logits=False,\n    label_smoothing=0.0,\n    axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.categorical_accuracy": "tf.keras.metrics.categorical_accuracy(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.categorical_crossentropy": "tf.keras.metrics.categorical_crossentropy(\n    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n)\n"
  },
  {
    "tf.keras.metrics.deserialize": "tf.keras.metrics.deserialize(\n    config, custom_objects=None\n)\n"
  },
  {
    "tf.keras.metrics.get": "tf.keras.metrics.get(\n    identifier\n)\n"
  },
  {
    "tf.keras.metrics.hinge": "tf.keras.metrics.hinge(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.kl_divergence": "tf.keras.metrics.kl_divergence(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.log_cosh": "tf.keras.losses.log_cosh(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.losses.log_cosh": "tf.keras.losses.log_cosh(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_error": "tf.keras.metrics.mean_absolute_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_absolute_percentage_error": "tf.keras.metrics.mean_absolute_percentage_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_error": "tf.keras.metrics.mean_squared_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.mean_squared_logarithmic_error": "tf.keras.metrics.mean_squared_logarithmic_error(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.poisson": "tf.keras.metrics.poisson(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.serialize": "tf.keras.metrics.serialize(\n    metric\n)\n"
  },
  {
    "tf.keras.metrics.sparse_categorical_accuracy": "tf.keras.metrics.sparse_categorical_accuracy(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.sparse_categorical_crossentropy": "tf.keras.metrics.sparse_categorical_crossentropy(\n    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None\n)\n"
  },
  {
    "tf.keras.metrics.sparse_top_k_categorical_accuracy": "tf.keras.metrics.sparse_top_k_categorical_accuracy(\n    y_true, y_pred, k=5\n)\n"
  },
  {
    "tf.keras.metrics.squared_hinge": "tf.keras.metrics.squared_hinge(\n    y_true, y_pred\n)\n"
  },
  {
    "tf.keras.metrics.top_k_categorical_accuracy": "tf.keras.metrics.top_k_categorical_accuracy(\n    y_true, y_pred, k=5\n)\n"
  },
  {
    "tf.math.minimum": "tf.math.minimum(\n    x, y, name=None\n)\n"
  },
  {
    "tf.mlir.experimental.convert_function": "tf.mlir.experimental.convert_function(\n    concrete_function,\n    pass_pipeline='tf-standard-pipeline',\n    show_debug_info=False\n)\n"
  },
  {
    "tf.mlir.experimental.convert_graph_def": "tf.mlir.experimental.convert_graph_def(\n    graph_def,\n    pass_pipeline='tf-standard-pipeline',\n    show_debug_info=False\n)\n"
  },
  {
    "tf.math.multiply": "tf.math.multiply(\n    x, y, name=None\n)\n"
  },
  {
    "tf.name_scope": "tf.name_scope(\n    name\n)\n"
  },
  {
    "tf.math.negative": "tf.math.negative(\n    x, name=None\n)\n"
  },
  {
    "tf.nest.assert_same_structure": "tf.nest.assert_same_structure(\n    nest1, nest2, check_types=True, expand_composites=False\n)\n"
  },
  {
    "tf.nest.flatten": "tf.nest.flatten(\n    structure, expand_composites=False\n)\n"
  },
  {
    "tf.nest.is_nested": "tf.nest.is_nested(\n    seq\n)\n"
  },
  {
    "tf.nest.map_structure": "tf.nest.map_structure(\n    func, *structure, **kwargs\n)\n"
  },
  {
    "tf.nest.pack_sequence_as": "tf.nest.pack_sequence_as(\n    structure, flat_sequence, expand_composites=False\n)\n"
  },
  {
    "tf.nn.RNNCellDeviceWrapper": "tf.nn.RNNCellDeviceWrapper(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.nn.RNNCellDropoutWrapper": "tf.nn.RNNCellDropoutWrapper(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.nn.RNNCellResidualWrapper": "tf.nn.RNNCellResidualWrapper(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.random.all_candidate_sampler": "tf.random.all_candidate_sampler(\n    true_classes, num_true, num_sampled, unique, seed=None, name=None\n)\n"
  },
  {
    "tf.math.approx_max_k": "tf.math.approx_max_k(\n    operand,\n    k,\n    reduction_dimension=-1,\n    recall_target=0.95,\n    reduction_input_size_override=-1,\n    aggregate_to_topk=True,\n    name=None\n)\n"
  },
  {
    "tf.math.approx_min_k": "tf.math.approx_min_k(\n    operand,\n    k,\n    reduction_dimension=-1,\n    recall_target=0.95,\n    reduction_input_size_override=-1,\n    aggregate_to_topk=True,\n    name=None\n)\n"
  },
  {
    "tf.nn.atrous_conv2d": "tf.nn.atrous_conv2d(\n    value, filters, rate, padding, name=None\n)\n"
  },
  {
    "tf.nn.atrous_conv2d_transpose": "tf.nn.atrous_conv2d_transpose(\n    value, filters, output_shape, rate, padding, name=None\n)\n"
  },
  {
    "tf.nn.avg_pool": "tf.nn.avg_pool(\n    input, ksize, strides, padding, data_format=None, name=None\n)\n"
  },
  {
    "tf.nn.avg_pool1d": "tf.nn.avg_pool1d(\n    input, ksize, strides, padding, data_format='NWC', name=None\n)\n"
  },
  {
    "tf.nn.avg_pool2d": "tf.nn.avg_pool2d(\n    input, ksize, strides, padding, data_format='NHWC', name=None\n)\n"
  },
  {
    "tf.nn.avg_pool3d": "tf.nn.avg_pool3d(\n    input, ksize, strides, padding, data_format='NDHWC', name=None\n)\n"
  },
  {
    "tf.nn.batch_norm_with_global_normalization": "tf.nn.batch_norm_with_global_normalization(\n    input,\n    mean,\n    variance,\n    beta,\n    gamma,\n    variance_epsilon,\n    scale_after_normalization,\n    name=None\n)\n"
  },
  {
    "tf.nn.batch_normalization": "tf.nn.batch_normalization(\n    x, mean, variance, offset, scale, variance_epsilon, name=None\n)\n"
  },
  {
    "tf.nn.bias_add": "tf.nn.bias_add(\n    value, bias, data_format=None, name=None\n)\n"
  },
  {
    "tf.nn.collapse_repeated": "tf.nn.collapse_repeated(\n    labels, seq_length, name=None\n)\n"
  },
  {
    "tf.nn.compute_accidental_hits": "tf.nn.compute_accidental_hits(\n    true_classes, sampled_candidates, num_true, seed=None, name=None\n)\n"
  },
  {
    "tf.nn.compute_average_loss": "tf.nn.compute_average_loss(\n    per_example_loss, sample_weight=None, global_batch_size=None\n)\n"
  },
  {
    "tf.nn.conv1d": "tf.nn.conv1d(\n    input,\n    filters,\n    stride,\n    padding,\n    data_format='NWC',\n    dilations=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.conv1d_transpose": "tf.nn.conv1d_transpose(\n    input,\n    filters,\n    output_shape,\n    strides,\n    padding='SAME',\n    data_format='NWC',\n    dilations=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.conv2d": "tf.nn.conv2d(\n    input,\n    filters,\n    strides,\n    padding,\n    data_format='NHWC',\n    dilations=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.conv2d_transpose": "tf.nn.conv2d_transpose(\n    input,\n    filters,\n    output_shape,\n    strides,\n    padding='SAME',\n    data_format='NHWC',\n    dilations=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.conv3d": "tf.nn.conv3d(\n    input,\n    filters,\n    strides,\n    padding,\n    data_format='NDHWC',\n    dilations=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.conv3d_transpose": "tf.nn.conv3d_transpose(\n    input,\n    filters,\n    output_shape,\n    strides,\n    padding='SAME',\n    data_format='NDHWC',\n    dilations=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.conv_transpose": "tf.nn.conv_transpose(\n    input,\n    filters,\n    output_shape,\n    strides,\n    padding='SAME',\n    data_format=None,\n    dilations=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.convolution": "tf.nn.convolution(\n    input,\n    filters,\n    strides=None,\n    padding='VALID',\n    data_format=None,\n    dilations=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.crelu": "tf.nn.crelu(\n    features, axis=-1, name=None\n)\n"
  },
  {
    "tf.nn.ctc_beam_search_decoder": "tf.nn.ctc_beam_search_decoder(\n    inputs, sequence_length, beam_width=100, top_paths=1\n)\n"
  },
  {
    "tf.nn.ctc_greedy_decoder": "tf.nn.ctc_greedy_decoder(\n    inputs, sequence_length, merge_repeated=True, blank_index=None\n)\n"
  },
  {
    "tf.nn.ctc_loss": "tf.nn.ctc_loss(\n    labels,\n    logits,\n    label_length,\n    logit_length,\n    logits_time_major=True,\n    unique=None,\n    blank_index=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.ctc_unique_labels": "tf.nn.ctc_unique_labels(\n    labels, name=None\n)\n"
  },
  {
    "tf.nn.depth_to_space": "tf.nn.depth_to_space(\n    input, block_size, data_format='NHWC', name=None\n)\n"
  },
  {
    "tf.nn.depthwise_conv2d": "tf.nn.depthwise_conv2d(\n    input,\n    filter,\n    strides,\n    padding,\n    data_format=None,\n    dilations=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.depthwise_conv2d_backprop_filter": "tf.nn.depthwise_conv2d_backprop_filter(\n    input,\n    filter_sizes,\n    out_backprop,\n    strides,\n    padding,\n    data_format='NHWC',\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.nn.depthwise_conv2d_backprop_input": "tf.nn.depthwise_conv2d_backprop_input(\n    input_sizes,\n    filter,\n    out_backprop,\n    strides,\n    padding,\n    data_format='NHWC',\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.nn.dilation2d": "tf.nn.dilation2d(\n    input, filters, strides, padding, data_format, dilations, name=None\n)\n"
  },
  {
    "tf.nn.dropout": "tf.nn.dropout(\n    x, rate, noise_shape=None, seed=None, name=None\n)\n"
  },
  {
    "tf.nn.elu": "tf.nn.elu(\n    features, name=None\n)\n"
  },
  {
    "tf.nn.embedding_lookup": "tf.nn.embedding_lookup(\n    params, ids, max_norm=None, name=None\n)\n"
  },
  {
    "tf.nn.embedding_lookup_sparse": "tf.nn.embedding_lookup_sparse(\n    params, sp_ids, sp_weights, combiner=None, max_norm=None, name=None\n)\n"
  },
  {
    "tf.nn.erosion2d": "tf.nn.erosion2d(\n    value, filters, strides, padding, data_format, dilations, name=None\n)\n"
  },
  {
    "tf.nn.experimental.stateless_dropout": "tf.nn.experimental.stateless_dropout(\n    x, rate, seed, rng_alg=None, noise_shape=None, name=None\n)\n"
  },
  {
    "tf.random.fixed_unigram_candidate_sampler": "tf.random.fixed_unigram_candidate_sampler(\n    true_classes,\n    num_true,\n    num_sampled,\n    unique,\n    range_max,\n    vocab_file='',\n    distortion=1.0,\n    num_reserved_ids=0,\n    num_shards=1,\n    shard=0,\n    unigrams=(),\n    seed=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.fractional_avg_pool": "tf.nn.fractional_avg_pool(\n    value,\n    pooling_ratio,\n    pseudo_random=False,\n    overlapping=False,\n    seed=0,\n    name=None\n)\n"
  },
  {
    "tf.nn.fractional_max_pool": "tf.nn.fractional_max_pool(\n    value,\n    pooling_ratio,\n    pseudo_random=False,\n    overlapping=False,\n    seed=0,\n    name=None\n)\n"
  },
  {
    "tf.nn.gelu": "tf.nn.gelu(\n    features, approximate=False, name=None\n)\n"
  },
  {
    "tf.math.in_top_k": "tf.math.in_top_k(\n    targets, predictions, k, name=None\n)\n"
  },
  {
    "tf.nn.isotonic_regression": "tf.nn.isotonic_regression(\n    inputs, decreasing=True, axis=-1\n)\n"
  },
  {
    "tf.nn.l2_loss": "tf.nn.l2_loss(\n    t, name=None\n)\n"
  },
  {
    "tf.math.l2_normalize": "tf.math.l2_normalize(\n    x, axis=None, epsilon=1e-12, name=None, dim=None\n)\n"
  },
  {
    "tf.nn.leaky_relu": "tf.nn.leaky_relu(\n    features, alpha=0.2, name=None\n)\n"
  },
  {
    "tf.random.learned_unigram_candidate_sampler": "tf.random.learned_unigram_candidate_sampler(\n    true_classes,\n    num_true,\n    num_sampled,\n    unique,\n    range_max,\n    seed=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.local_response_normalization": "tf.nn.local_response_normalization(\n    input, depth_radius=5, bias=1, alpha=1, beta=0.5, name=None\n)\n"
  },
  {
    "tf.nn.log_poisson_loss": "tf.nn.log_poisson_loss(\n    targets, log_input, compute_full_loss=False, name=None\n)\n"
  },
  {
    "tf.nn.log_softmax": "tf.nn.log_softmax(\n    logits, axis=None, name=None\n)\n"
  },
  {
    "tf.nn.local_response_normalization": "tf.nn.local_response_normalization(\n    input, depth_radius=5, bias=1, alpha=1, beta=0.5, name=None\n)\n"
  },
  {
    "tf.nn.max_pool": "tf.nn.max_pool(\n    input, ksize, strides, padding, data_format=None, name=None\n)\n"
  },
  {
    "tf.nn.max_pool1d": "tf.nn.max_pool1d(\n    input, ksize, strides, padding, data_format='NWC', name=None\n)\n"
  },
  {
    "tf.nn.max_pool2d": "tf.nn.max_pool2d(\n    input, ksize, strides, padding, data_format='NHWC', name=None\n)\n"
  },
  {
    "tf.nn.max_pool3d": "tf.nn.max_pool3d(\n    input, ksize, strides, padding, data_format='NDHWC', name=None\n)\n"
  },
  {
    "tf.nn.max_pool_with_argmax": "tf.nn.max_pool_with_argmax(\n    input,\n    ksize,\n    strides,\n    padding,\n    data_format='NHWC',\n    output_dtype=tf.dtypes.int64,\n    include_batch_in_index=False,\n    name=None\n)\n"
  },
  {
    "tf.nn.moments": "tf.nn.moments(\n    x, axes, shift=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.nn.nce_loss": "tf.nn.nce_loss(\n    weights,\n    biases,\n    labels,\n    inputs,\n    num_sampled,\n    num_classes,\n    num_true=1,\n    sampled_values=None,\n    remove_accidental_hits=False,\n    name='nce_loss'\n)\n"
  },
  {
    "tf.nn.normalize_moments": "tf.nn.normalize_moments(\n    counts, mean_ss, variance_ss, shift, name=None\n)\n"
  },
  {
    "tf.nn.pool": "tf.nn.pool(\n    input,\n    window_shape,\n    pooling_type,\n    strides=None,\n    padding='VALID',\n    data_format=None,\n    dilations=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.relu": "tf.nn.relu(\n    features, name=None\n)\n"
  },
  {
    "tf.nn.relu6": "tf.nn.relu6(\n    features, name=None\n)\n"
  },
  {
    "tf.nn.safe_embedding_lookup_sparse": "tf.nn.safe_embedding_lookup_sparse(\n    embedding_weights,\n    sparse_ids,\n    sparse_weights=None,\n    combiner='mean',\n    default_id=None,\n    max_norm=None,\n    name=None\n)\n"
  },
  {
    "tf.nn.sampled_softmax_loss": "tf.nn.sampled_softmax_loss(\n    weights,\n    biases,\n    labels,\n    inputs,\n    num_sampled,\n    num_classes,\n    num_true=1,\n    sampled_values=None,\n    remove_accidental_hits=True,\n    seed=None,\n    name='sampled_softmax_loss'\n)\n"
  },
  {
    "tf.nn.scale_regularization_loss": "tf.nn.scale_regularization_loss(\n    regularization_loss\n)\n"
  },
  {
    "tf.nn.selu": "tf.nn.selu(\n    features, name=None\n)\n"
  },
  {
    "tf.nn.separable_conv2d": "tf.nn.separable_conv2d(\n    input,\n    depthwise_filter,\n    pointwise_filter,\n    strides,\n    padding,\n    data_format=None,\n    dilations=None,\n    name=None\n)\n"
  },
  {
    "tf.math.sigmoid": "tf.math.sigmoid(\n    x, name=None\n)\n"
  },
  {
    "tf.nn.sigmoid_cross_entropy_with_logits": "tf.nn.sigmoid_cross_entropy_with_logits(\n    labels=None, logits=None, name=None\n)\n"
  },
  {
    "tf.nn.silu": "tf.nn.silu(\n    features, beta=1.0\n)\n"
  },
  {
    "tf.nn.softmax": "tf.nn.softmax(\n    logits, axis=None, name=None\n)\n"
  },
  {
    "tf.nn.softmax_cross_entropy_with_logits": "tf.nn.softmax_cross_entropy_with_logits(\n    labels, logits, axis=-1, name=None\n)\n"
  },
  {
    "tf.math.softplus": "tf.math.softplus(\n    features, name=None\n)\n"
  },
  {
    "tf.nn.softsign": "tf.nn.softsign(\n    features, name=None\n)\n"
  },
  {
    "tf.space_to_batch": "tf.space_to_batch(\n    input, block_shape, paddings, name=None\n)\n"
  },
  {
    "tf.nn.space_to_depth": "tf.nn.space_to_depth(\n    input, block_size, data_format='NHWC', name=None\n)\n"
  },
  {
    "tf.nn.sparse_softmax_cross_entropy_with_logits": "tf.nn.sparse_softmax_cross_entropy_with_logits(\n    labels, logits, name=None\n)\n"
  },
  {
    "tf.nn.sufficient_statistics": "tf.nn.sufficient_statistics(\n    x, axes, shift=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.nn.silu": "tf.nn.silu(\n    features, beta=1.0\n)\n"
  },
  {
    "tf.math.tanh": "tf.math.tanh(\n    x, name=None\n)\n"
  },
  {
    "tf.math.top_k": "tf.math.top_k(\n    input, k=1, sorted=True, name=None\n)\n"
  },
  {
    "tf.nn.weighted_cross_entropy_with_logits": "tf.nn.weighted_cross_entropy_with_logits(\n    labels, logits, pos_weight, name=None\n)\n"
  },
  {
    "tf.nn.weighted_moments": "tf.nn.weighted_moments(\n    x, axes, frequency_weights, keepdims=False, name=None\n)\n"
  },
  {
    "tf.nn.with_space_to_batch": "tf.nn.with_space_to_batch(\n    input,\n    dilation_rate,\n    padding,\n    op,\n    filter_shape=None,\n    spatial_dims=None,\n    data_format=None\n)\n"
  },
  {
    "tf.math.zero_fraction": "tf.math.zero_fraction(\n    value, name=None\n)\n"
  },
  {
    "tf.no_gradient": "tf.no_gradient(\n    op_type\n)\n"
  },
  {
    "tf.no_op": "tf.no_op(\n    name=None\n)\n"
  },
  {
    "tf.nondifferentiable_batch_function": "tf.nondifferentiable_batch_function(\n    num_batch_threads,\n    max_batch_size,\n    batch_timeout_micros,\n    allowed_batch_sizes=None,\n    max_enqueued_batches=10,\n    autograph=True,\n    enable_large_batch_splitting=True\n)\n"
  },
  {
    "tf.norm": "tf.norm(\n    tensor, ord='euclidean', axis=None, keepdims=None, name=None\n)\n"
  },
  {
    "tf.math.not_equal": "tf.math.not_equal(\n    x, y, name=None\n)\n"
  },
  {
    "tf.numpy_function": "tf.numpy_function(\n    func, inp, Tout, stateful=True, name=None\n)\n"
  },
  {
    "tf.one_hot": "tf.one_hot(\n    indices,\n    depth,\n    on_value=None,\n    off_value=None,\n    axis=None,\n    dtype=None,\n    name=None\n)\n"
  },
  {
    "tf.ones": "tf.ones(\n    shape,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.ones_like": "tf.ones_like(\n    input, dtype=None, name=None\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adadelta": "tf.keras.optimizers.experimental.Adadelta(\n    learning_rate=0.001,\n    rho=0.95,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adadelta',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adagrad": "tf.keras.optimizers.experimental.Adagrad(\n    learning_rate=0.001,\n    initial_accumulator_value=0.1,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adagrad',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.Adam": "tf.keras.optimizers.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adamax": "tf.keras.optimizers.experimental.Adamax(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adamax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Ftrl": "tf.keras.optimizers.experimental.Ftrl(\n    learning_rate=0.001,\n    learning_rate_power=-0.5,\n    initial_accumulator_value=0.1,\n    l1_regularization_strength=0.0,\n    l2_regularization_strength=0.0,\n    l2_shrinkage_regularization_strength=0.0,\n    beta=0.0,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Ftrl',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Nadam": "tf.keras.optimizers.experimental.Nadam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Nadam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.Optimizer": "tf.keras.optimizers.Optimizer(\n    name,\n    weight_decay=0,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.RMSprop": "tf.keras.optimizers.experimental.RMSprop(\n    learning_rate=0.001,\n    rho=0.9,\n    momentum=0.0,\n    epsilon=1e-07,\n    centered=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=100,\n    jit_compile=True,\n    name='RMSprop',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.SGD": "tf.keras.optimizers.experimental.SGD(\n    learning_rate=0.01,\n    momentum=0.0,\n    nesterov=False,\n    amsgrad=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='SGD',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.deserialize": "tf.keras.optimizers.deserialize(\n    config, custom_objects=None, **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adadelta": "tf.keras.optimizers.experimental.Adadelta(\n    learning_rate=0.001,\n    rho=0.95,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adadelta',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adafactor": "tf.keras.optimizers.experimental.Adafactor(\n    learning_rate=0.001,\n    beta_2_decay=-0.8,\n    epsilon_1=1e-30,\n    epsilon_2=0.001,\n    clip_threshold=1.0,\n    relative_step=True,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adafactor',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adagrad": "tf.keras.optimizers.experimental.Adagrad(\n    learning_rate=0.001,\n    initial_accumulator_value=0.1,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adagrad',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.Adam": "tf.keras.optimizers.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.AdamW": "tf.keras.optimizers.experimental.AdamW(\n    learning_rate=0.001,\n    weight_decay=0.004,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='AdamW',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Adamax": "tf.keras.optimizers.experimental.Adamax(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Adamax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Ftrl": "tf.keras.optimizers.experimental.Ftrl(\n    learning_rate=0.001,\n    learning_rate_power=-0.5,\n    initial_accumulator_value=0.1,\n    l1_regularization_strength=0.0,\n    l2_regularization_strength=0.0,\n    l2_shrinkage_regularization_strength=0.0,\n    beta=0.0,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Ftrl',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.Nadam": "tf.keras.optimizers.experimental.Nadam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='Nadam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.Optimizer": "tf.keras.optimizers.Optimizer(\n    name,\n    weight_decay=0,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.RMSprop": "tf.keras.optimizers.experimental.RMSprop(\n    learning_rate=0.001,\n    rho=0.9,\n    momentum=0.0,\n    epsilon=1e-07,\n    centered=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=100,\n    jit_compile=True,\n    name='RMSprop',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.experimental.SGD": "tf.keras.optimizers.experimental.SGD(\n    learning_rate=0.01,\n    momentum=0.0,\n    nesterov=False,\n    amsgrad=False,\n    weight_decay=None,\n    clipnorm=None,\n    clipvalue=None,\n    global_clipnorm=None,\n    use_ema=False,\n    ema_momentum=0.99,\n    ema_overwrite_frequency=None,\n    jit_compile=True,\n    name='SGD',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.get": "tf.keras.optimizers.get(\n    identifier, **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Adadelta": "tf.keras.optimizers.legacy.Adadelta(\n    learning_rate=0.001,\n    rho=0.95,\n    epsilon=1e-07,\n    name='Adadelta',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Adagrad": "tf.keras.optimizers.legacy.Adagrad(\n    learning_rate=0.001,\n    initial_accumulator_value=0.1,\n    epsilon=1e-07,\n    name='Adagrad',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Adam": "tf.keras.optimizers.legacy.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    name='Adam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Adamax": "tf.keras.optimizers.legacy.Adamax(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    name='Adamax',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Ftrl": "tf.keras.optimizers.legacy.Ftrl(\n    learning_rate=0.001,\n    learning_rate_power=-0.5,\n    initial_accumulator_value=0.1,\n    l1_regularization_strength=0.0,\n    l2_regularization_strength=0.0,\n    name='Ftrl',\n    l2_shrinkage_regularization_strength=0.0,\n    beta=0.0,\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Nadam": "tf.keras.optimizers.legacy.Nadam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    name='Nadam',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.Optimizer": "tf.keras.optimizers.legacy.Optimizer(\n    name, gradient_aggregator=None, gradient_transformers=None, **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.RMSprop": "tf.keras.optimizers.legacy.RMSprop(\n    learning_rate=0.001,\n    rho=0.9,\n    momentum=0.0,\n    epsilon=1e-07,\n    centered=False,\n    name='RMSprop',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.legacy.SGD": "tf.keras.optimizers.legacy.SGD(\n    learning_rate=0.01,\n    momentum=0.0,\n    nesterov=False,\n    name='SGD',\n    **kwargs\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.CosineDecay": "tf.keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate, decay_steps, alpha=0.0, name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.CosineDecayRestarts": "tf.keras.optimizers.schedules.CosineDecayRestarts(\n    initial_learning_rate,\n    first_decay_steps,\n    t_mul=2.0,\n    m_mul=1.0,\n    alpha=0.0,\n    name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.ExponentialDecay": "tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate, decay_steps, decay_rate, staircase=False, name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.InverseTimeDecay": "tf.keras.optimizers.schedules.InverseTimeDecay(\n    initial_learning_rate, decay_steps, decay_rate, staircase=False, name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.PiecewiseConstantDecay": "tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n    boundaries, values, name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.PolynomialDecay": "tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate,\n    decay_steps,\n    end_learning_rate=0.0001,\n    power=1.0,\n    cycle=False,\n    name=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.deserialize": "tf.keras.optimizers.schedules.deserialize(\n    config, custom_objects=None\n)\n"
  },
  {
    "tf.keras.optimizers.schedules.serialize": "tf.keras.optimizers.schedules.serialize(\n    learning_rate_schedule\n)\n"
  },
  {
    "tf.keras.optimizers.serialize": "tf.keras.optimizers.serialize(\n    optimizer\n)\n"
  },
  {
    "tf.pad": "tf.pad(\n    tensor, paddings, mode='CONSTANT', constant_values=0, name=None\n)\n"
  },
  {
    "tf.parallel_stack": "tf.parallel_stack(\n    values, name='parallel_stack'\n)\n"
  },
  {
    "tf.math.pow": "tf.math.pow(\n    x, y, name=None\n)\n"
  },
  {
    "tf.print": "tf.print(\n    *inputs, **kwargs\n)\n"
  },
  {
    "tf.profiler.experimental.Profile": "tf.profiler.experimental.Profile(\n    logdir, options=None\n)\n"
  },
  {
    "tf.profiler.experimental.ProfilerOptions": "tf.profiler.experimental.ProfilerOptions(\n    host_tracer_level=2,\n    python_tracer_level=0,\n    device_tracer_level=1,\n    delay_ms=None\n)\n"
  },
  {
    "tf.profiler.experimental.Trace": "tf.profiler.experimental.Trace(\n    name, **kwargs\n)\n"
  },
  {
    "tf.profiler.experimental.client.monitor": "tf.profiler.experimental.client.monitor(\n    service_addr, duration_ms, level=1\n)\n"
  },
  {
    "tf.profiler.experimental.client.trace": "tf.profiler.experimental.client.trace(\n    service_addr,\n    logdir,\n    duration_ms,\n    worker_list='',\n    num_tracing_attempts=3,\n    options=None\n)\n"
  },
  {
    "tf.profiler.experimental.server.start": "tf.profiler.experimental.server.start(\n    port\n)\n"
  },
  {
    "tf.profiler.experimental.start": "tf.profiler.experimental.start(\n    logdir, options=None\n)\n"
  },
  {
    "tf.profiler.experimental.stop": "tf.profiler.experimental.stop(\n    save=True\n)\n"
  },
  {
    "tf.py_function": "tf.py_function(\n    func, inp, Tout, name=None\n)\n"
  },
  {
    "tf.quantization.dequantize": "tf.quantization.dequantize(\n    input,\n    min_range,\n    max_range,\n    mode='MIN_COMBINED',\n    name=None,\n    axis=None,\n    narrow_range=False,\n    dtype=tf.dtypes.float32\n)\n"
  },
  {
    "tf.quantization.fake_quant_with_min_max_args": "tf.quantization.fake_quant_with_min_max_args(\n    inputs, min=-6, max=6, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.quantization.fake_quant_with_min_max_args_gradient": "tf.quantization.fake_quant_with_min_max_args_gradient(\n    gradients, inputs, min=-6, max=6, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.quantization.fake_quant_with_min_max_vars": "tf.quantization.fake_quant_with_min_max_vars(\n    inputs, min, max, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.quantization.fake_quant_with_min_max_vars_gradient": "tf.quantization.fake_quant_with_min_max_vars_gradient(\n    gradients, inputs, min, max, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.quantization.fake_quant_with_min_max_vars_per_channel": "tf.quantization.fake_quant_with_min_max_vars_per_channel(\n    inputs, min, max, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient": "tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient(\n    gradients, inputs, min, max, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.quantization.quantize": "tf.quantization.quantize(\n    input,\n    min_range,\n    max_range,\n    T,\n    mode='MIN_COMBINED',\n    round_mode='HALF_AWAY_FROM_ZERO',\n    name=None,\n    narrow_range=False,\n    axis=None,\n    ensure_minimum_range=0.01\n)\n"
  },
  {
    "tf.quantization.quantize_and_dequantize": "tf.quantization.quantize_and_dequantize(\n    input,\n    input_min,\n    input_max,\n    signed_input=True,\n    num_bits=8,\n    range_given=False,\n    round_mode='HALF_TO_EVEN',\n    name=None,\n    narrow_range=False,\n    axis=None\n)\n"
  },
  {
    "tf.quantization.quantize_and_dequantize_v2": "tf.quantization.quantize_and_dequantize_v2(\n    input,\n    input_min,\n    input_max,\n    signed_input=True,\n    num_bits=8,\n    range_given=False,\n    round_mode='HALF_TO_EVEN',\n    name=None,\n    narrow_range=False,\n    axis=None\n)\n"
  },
  {
    "tf.quantization.quantized_concat": "tf.quantization.quantized_concat(\n    concat_dim, values, input_mins, input_maxes, name=None\n)\n"
  },
  {
    "tf.queue.FIFOQueue": "tf.queue.FIFOQueue(\n    capacity,\n    dtypes,\n    shapes=None,\n    names=None,\n    shared_name=None,\n    name='fifo_queue'\n)\n"
  },
  {
    "tf.queue.PaddingFIFOQueue": "tf.queue.PaddingFIFOQueue(\n    capacity,\n    dtypes,\n    shapes,\n    names=None,\n    shared_name=None,\n    name='padding_fifo_queue'\n)\n"
  },
  {
    "tf.queue.PriorityQueue": "tf.queue.PriorityQueue(\n    capacity,\n    types,\n    shapes=None,\n    names=None,\n    shared_name=None,\n    name='priority_queue'\n)\n"
  },
  {
    "tf.queue.QueueBase": "tf.queue.QueueBase(\n    dtypes, shapes, names, queue_ref\n)\n"
  },
  {
    "tf.queue.RandomShuffleQueue": "tf.queue.RandomShuffleQueue(\n    capacity,\n    min_after_dequeue,\n    dtypes,\n    shapes=None,\n    names=None,\n    seed=None,\n    shared_name=None,\n    name='random_shuffle_queue'\n)\n"
  },
  {
    "tf.ragged.boolean_mask": "tf.ragged.boolean_mask(\n    data, mask, name=None\n)\n"
  },
  {
    "tf.ragged.constant": "tf.ragged.constant(\n    pylist,\n    dtype=None,\n    ragged_rank=None,\n    inner_shape=None,\n    name=None,\n    row_splits_dtype=tf.dtypes.int64\n)\n"
  },
  {
    "tf.ragged.cross": "tf.ragged.cross(\n    inputs, name=None\n)\n"
  },
  {
    "tf.ragged.cross_hashed": "tf.ragged.cross_hashed(\n    inputs, num_buckets=0, hash_key=None, name=None\n)\n"
  },
  {
    "tf.ragged.map_flat_values": "tf.ragged.map_flat_values(\n    op, *args, **kwargs\n)\n"
  },
  {
    "tf.ragged.range": "tf.ragged.range(\n    starts,\n    limits=None,\n    deltas=1,\n    dtype=None,\n    name=None,\n    row_splits_dtype=tf.dtypes.int64\n)\n"
  },
  {
    "tf.ragged.row_splits_to_segment_ids": "tf.ragged.row_splits_to_segment_ids(\n    splits, name=None, out_type=None\n)\n"
  },
  {
    "tf.ragged.segment_ids_to_row_splits": "tf.ragged.segment_ids_to_row_splits(\n    segment_ids, num_segments=None, out_type=None, name=None\n)\n"
  },
  {
    "tf.ragged.stack": "tf.ragged.stack(\n    values: typing.List[ragged_tensor.RaggedOrDense], axis=0, name=None\n)\n"
  },
  {
    "tf.ragged.stack_dynamic_partitions": "tf.ragged.stack_dynamic_partitions(\n    data, partitions, num_partitions, name=None\n)\n"
  },
  {
    "tf.random.Generator": "tf.random.Generator(\n    copy_from=None, state=None, alg=None\n)\n"
  },
  {
    "tf.random.all_candidate_sampler": "tf.random.all_candidate_sampler(\n    true_classes, num_true, num_sampled, unique, seed=None, name=None\n)\n"
  },
  {
    "tf.random.categorical": "tf.random.categorical(\n    logits, num_samples, dtype=None, seed=None, name=None\n)\n"
  },
  {
    "tf.random.create_rng_state": "tf.random.create_rng_state(\n    seed, alg\n)\n"
  },
  {
    "tf.random.Generator": "tf.random.Generator(\n    copy_from=None, state=None, alg=None\n)\n"
  },
  {
    "tf.random.create_rng_state": "tf.random.create_rng_state(\n    seed, alg\n)\n"
  },
  {
    "tf.random.experimental.index_shuffle": "tf.random.experimental.index_shuffle(\n    index, seed, max_index\n)\n"
  },
  {
    "tf.random.set_global_generator": "tf.random.set_global_generator(\n    generator\n)\n"
  },
  {
    "tf.random.experimental.stateless_fold_in": "tf.random.experimental.stateless_fold_in(\n    seed, data, alg='auto_select'\n)\n"
  },
  {
    "tf.random.experimental.stateless_shuffle": "tf.random.experimental.stateless_shuffle(\n    value, seed, alg='auto_select', name=None\n)\n"
  },
  {
    "tf.random.experimental.stateless_split": "tf.random.experimental.stateless_split(\n    seed, num=2, alg='auto_select'\n)\n"
  },
  {
    "tf.random.fixed_unigram_candidate_sampler": "tf.random.fixed_unigram_candidate_sampler(\n    true_classes,\n    num_true,\n    num_sampled,\n    unique,\n    range_max,\n    vocab_file='',\n    distortion=1.0,\n    num_reserved_ids=0,\n    num_shards=1,\n    shard=0,\n    unigrams=(),\n    seed=None,\n    name=None\n)\n"
  },
  {
    "tf.random.gamma": "tf.random.gamma(\n    shape,\n    alpha,\n    beta=None,\n    dtype=tf.dtypes.float32,\n    seed=None,\n    name=None\n)\n"
  },
  {
    "tf.random.learned_unigram_candidate_sampler": "tf.random.learned_unigram_candidate_sampler(\n    true_classes,\n    num_true,\n    num_sampled,\n    unique,\n    range_max,\n    seed=None,\n    name=None\n)\n"
  },
  {
    "tf.random.log_uniform_candidate_sampler": "tf.random.log_uniform_candidate_sampler(\n    true_classes,\n    num_true,\n    num_sampled,\n    unique,\n    range_max,\n    seed=None,\n    name=None\n)\n"
  },
  {
    "tf.random.normal": "tf.random.normal(\n    shape,\n    mean=0.0,\n    stddev=1.0,\n    dtype=tf.dtypes.float32,\n    seed=None,\n    name=None\n)\n"
  },
  {
    "tf.random.poisson": "tf.random.poisson(\n    shape,\n    lam,\n    dtype=tf.dtypes.float32,\n    seed=None,\n    name=None\n)\n"
  },
  {
    "tf.random.set_global_generator": "tf.random.set_global_generator(\n    generator\n)\n"
  },
  {
    "tf.random.set_seed": "tf.random.set_seed(\n    seed\n)\n"
  },
  {
    "tf.random.shuffle": "tf.random.shuffle(\n    value, seed=None, name=None\n)\n"
  },
  {
    "tf.random.stateless_binomial": "tf.random.stateless_binomial(\n    shape,\n    seed,\n    counts,\n    probs,\n    output_dtype=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.random.stateless_categorical": "tf.random.stateless_categorical(\n    logits,\n    num_samples,\n    seed,\n    dtype=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.random.stateless_gamma": "tf.random.stateless_gamma(\n    shape,\n    seed,\n    alpha,\n    beta=None,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.random.stateless_normal": "tf.random.stateless_normal(\n    shape,\n    seed,\n    mean=0.0,\n    stddev=1.0,\n    dtype=tf.dtypes.float32,\n    name=None,\n    alg='auto_select'\n)\n"
  },
  {
    "tf.random.stateless_parameterized_truncated_normal": "tf.random.stateless_parameterized_truncated_normal(\n    shape, seed, means=0.0, stddevs=1.0, minvals=-2.0, maxvals=2.0, name=None\n)\n"
  },
  {
    "tf.random.stateless_poisson": "tf.random.stateless_poisson(\n    shape,\n    seed,\n    lam,\n    dtype=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.random.stateless_truncated_normal": "tf.random.stateless_truncated_normal(\n    shape,\n    seed,\n    mean=0.0,\n    stddev=1.0,\n    dtype=tf.dtypes.float32,\n    name=None,\n    alg='auto_select'\n)\n"
  },
  {
    "tf.random.stateless_uniform": "tf.random.stateless_uniform(\n    shape,\n    seed,\n    minval=0,\n    maxval=None,\n    dtype=tf.dtypes.float32,\n    name=None,\n    alg='auto_select'\n)\n"
  },
  {
    "tf.random.truncated_normal": "tf.random.truncated_normal(\n    shape,\n    mean=0.0,\n    stddev=1.0,\n    dtype=tf.dtypes.float32,\n    seed=None,\n    name=None\n)\n"
  },
  {
    "tf.random.uniform": "tf.random.uniform(\n    shape,\n    minval=0,\n    maxval=None,\n    dtype=tf.dtypes.float32,\n    seed=None,\n    name=None\n)\n"
  },
  {
    "tf.random.uniform_candidate_sampler": "tf.random.uniform_candidate_sampler(\n    true_classes,\n    num_true,\n    num_sampled,\n    unique,\n    range_max,\n    seed=None,\n    name=None\n)\n"
  },
  {
    "tf.random_index_shuffle": "tf.random_index_shuffle(\n    index, seed, max_index, rounds=4, name=None\n)\n"
  },
  {
    "tf.random_normal_initializer": "tf.random_normal_initializer(\n    mean=0.0, stddev=0.05, seed=None\n)\n"
  },
  {
    "tf.random_uniform_initializer": "tf.random_uniform_initializer(\n    minval=-0.05, maxval=0.05, seed=None\n)\n"
  },
  {
    "tf.rank": "tf.rank(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.Abort": "tf.raw_ops.Abort(\n    error_msg='', exit_without_error=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.Abs": "tf.raw_ops.Abs(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.AccumulateNV2": "tf.raw_ops.AccumulateNV2(\n    inputs, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.AccumulatorApplyGradient": "tf.raw_ops.AccumulatorApplyGradient(\n    handle, local_step, gradient, name=None\n)\n"
  },
  {
    "tf.raw_ops.AccumulatorNumAccumulated": "tf.raw_ops.AccumulatorNumAccumulated(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.AccumulatorSetGlobalStep": "tf.raw_ops.AccumulatorSetGlobalStep(\n    handle, new_global_step, name=None\n)\n"
  },
  {
    "tf.raw_ops.AccumulatorTakeGradient": "tf.raw_ops.AccumulatorTakeGradient(\n    handle, num_required, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.Acos": "tf.raw_ops.Acos(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Acosh": "tf.raw_ops.Acosh(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Add": "tf.raw_ops.Add(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.AddManySparseToTensorsMap": "tf.raw_ops.AddManySparseToTensorsMap(\n    sparse_indices,\n    sparse_values,\n    sparse_shape,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.AddN": "tf.raw_ops.AddN(\n    inputs, name=None\n)\n"
  },
  {
    "tf.raw_ops.AddSparseToTensorsMap": "tf.raw_ops.AddSparseToTensorsMap(\n    sparse_indices,\n    sparse_values,\n    sparse_shape,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.AddV2": "tf.raw_ops.AddV2(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.AdjustContrast": "tf.raw_ops.AdjustContrast(\n    images, contrast_factor, min_value, max_value, name=None\n)\n"
  },
  {
    "tf.raw_ops.AdjustContrastv2": "tf.raw_ops.AdjustContrastv2(\n    images, contrast_factor, name=None\n)\n"
  },
  {
    "tf.raw_ops.AdjustHue": "tf.raw_ops.AdjustHue(\n    images, delta, name=None\n)\n"
  },
  {
    "tf.raw_ops.AdjustSaturation": "tf.raw_ops.AdjustSaturation(\n    images, scale, name=None\n)\n"
  },
  {
    "tf.raw_ops.All": "tf.raw_ops.All(\n    input, axis, keep_dims=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.AllCandidateSampler": "tf.raw_ops.AllCandidateSampler(\n    true_classes, num_true, num_sampled, unique, seed=0, seed2=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.AllToAll": "tf.raw_ops.AllToAll(\n    input,\n    group_assignment,\n    concat_dimension,\n    split_dimension,\n    split_count,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Angle": "tf.raw_ops.Angle(\n    input,\n    Tout=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousHashTable": "tf.raw_ops.AnonymousHashTable(\n    key_dtype, value_dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousIterator": "tf.raw_ops.AnonymousIterator(\n    output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousIteratorV2": "tf.raw_ops.AnonymousIteratorV2(\n    output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousIteratorV3": "tf.raw_ops.AnonymousIteratorV3(\n    output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousMemoryCache": "tf.raw_ops.AnonymousMemoryCache(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousMultiDeviceIterator": "tf.raw_ops.AnonymousMultiDeviceIterator(\n    devices, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousMultiDeviceIteratorV3": "tf.raw_ops.AnonymousMultiDeviceIteratorV3(\n    devices, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousMutableDenseHashTable": "tf.raw_ops.AnonymousMutableDenseHashTable(\n    empty_key,\n    deleted_key,\n    value_dtype,\n    value_shape=[],\n    initial_num_buckets=131072,\n    max_load_factor=0.8,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousMutableHashTable": "tf.raw_ops.AnonymousMutableHashTable(\n    key_dtype, value_dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousMutableHashTableOfTensors": "tf.raw_ops.AnonymousMutableHashTableOfTensors(\n    key_dtype, value_dtype, value_shape=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousRandomSeedGenerator": "tf.raw_ops.AnonymousRandomSeedGenerator(\n    seed, seed2, name=None\n)\n"
  },
  {
    "tf.raw_ops.AnonymousSeedGenerator": "tf.raw_ops.AnonymousSeedGenerator(\n    seed, seed2, reshuffle, name=None\n)\n"
  },
  {
    "tf.raw_ops.Any": "tf.raw_ops.Any(\n    input, axis, keep_dims=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyAdaMax": "tf.raw_ops.ApplyAdaMax(\n    var,\n    m,\n    v,\n    beta1_power,\n    lr,\n    beta1,\n    beta2,\n    epsilon,\n    grad,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyAdadelta": "tf.raw_ops.ApplyAdadelta(\n    var,\n    accum,\n    accum_update,\n    lr,\n    rho,\n    epsilon,\n    grad,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyAdagrad": "tf.raw_ops.ApplyAdagrad(\n    var, accum, lr, grad, use_locking=False, update_slots=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyAdagradDA": "tf.raw_ops.ApplyAdagradDA(\n    var,\n    gradient_accumulator,\n    gradient_squared_accumulator,\n    grad,\n    lr,\n    l1,\n    l2,\n    global_step,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyAdagradV2": "tf.raw_ops.ApplyAdagradV2(\n    var,\n    accum,\n    lr,\n    epsilon,\n    grad,\n    use_locking=False,\n    update_slots=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyAdam": "tf.raw_ops.ApplyAdam(\n    var,\n    m,\n    v,\n    beta1_power,\n    beta2_power,\n    lr,\n    beta1,\n    beta2,\n    epsilon,\n    grad,\n    use_locking=False,\n    use_nesterov=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyAddSign": "tf.raw_ops.ApplyAddSign(\n    var, m, lr, alpha, sign_decay, beta, grad, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyCenteredRMSProp": "tf.raw_ops.ApplyCenteredRMSProp(\n    var,\n    mg,\n    ms,\n    mom,\n    lr,\n    rho,\n    momentum,\n    epsilon,\n    grad,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyFtrl": "tf.raw_ops.ApplyFtrl(\n    var,\n    accum,\n    linear,\n    grad,\n    lr,\n    l1,\n    l2,\n    lr_power,\n    use_locking=False,\n    multiply_linear_by_lr=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyFtrlV2": "tf.raw_ops.ApplyFtrlV2(\n    var,\n    accum,\n    linear,\n    grad,\n    lr,\n    l1,\n    l2,\n    l2_shrinkage,\n    lr_power,\n    use_locking=False,\n    multiply_linear_by_lr=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyGradientDescent": "tf.raw_ops.ApplyGradientDescent(\n    var, alpha, delta, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyMomentum": "tf.raw_ops.ApplyMomentum(\n    var,\n    accum,\n    lr,\n    grad,\n    momentum,\n    use_locking=False,\n    use_nesterov=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyPowerSign": "tf.raw_ops.ApplyPowerSign(\n    var, m, lr, logbase, sign_decay, beta, grad, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyProximalAdagrad": "tf.raw_ops.ApplyProximalAdagrad(\n    var, accum, lr, l1, l2, grad, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyProximalGradientDescent": "tf.raw_ops.ApplyProximalGradientDescent(\n    var, alpha, l1, l2, delta, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ApplyRMSProp": "tf.raw_ops.ApplyRMSProp(\n    var,\n    ms,\n    mom,\n    lr,\n    rho,\n    momentum,\n    epsilon,\n    grad,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ApproxTopK": "tf.raw_ops.ApproxTopK(\n    input,\n    k,\n    reduction_dimension=-1,\n    recall_target=0.95,\n    is_max_k=True,\n    reduction_input_size_override=-1,\n    aggregate_to_topk=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ApproximateEqual": "tf.raw_ops.ApproximateEqual(\n    x, y, tolerance=1e-05, name=None\n)\n"
  },
  {
    "tf.raw_ops.ArgMax": "tf.raw_ops.ArgMax(\n    input,\n    dimension,\n    output_type=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ArgMin": "tf.raw_ops.ArgMin(\n    input,\n    dimension,\n    output_type=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.AsString": "tf.raw_ops.AsString(\n    input,\n    precision=-1,\n    scientific=False,\n    shortest=False,\n    width=-1,\n    fill='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Asin": "tf.raw_ops.Asin(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Asinh": "tf.raw_ops.Asinh(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Assert": "tf.raw_ops.Assert(\n    condition, data, summarize=3, name=None\n)\n"
  },
  {
    "tf.raw_ops.AssertCardinalityDataset": "tf.raw_ops.AssertCardinalityDataset(\n    input_dataset, cardinality, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.AssertNextDataset": "tf.raw_ops.AssertNextDataset(\n    input_dataset, transformations, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.AssertPrevDataset": "tf.raw_ops.AssertPrevDataset(\n    input_dataset, transformations, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.Assign": "tf.raw_ops.Assign(\n    ref, value, validate_shape=True, use_locking=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.AssignAdd": "tf.raw_ops.AssignAdd(\n    ref, value, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.AssignAddVariableOp": "tf.raw_ops.AssignAddVariableOp(\n    resource, value, name=None\n)\n"
  },
  {
    "tf.raw_ops.AssignSub": "tf.raw_ops.AssignSub(\n    ref, value, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.AssignSubVariableOp": "tf.raw_ops.AssignSubVariableOp(\n    resource, value, name=None\n)\n"
  },
  {
    "tf.raw_ops.AssignVariableOp": "tf.raw_ops.AssignVariableOp(\n    resource, value, validate_shape=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.AssignVariableXlaConcatND": "tf.raw_ops.AssignVariableXlaConcatND(\n    resource, inputs, num_concats, paddings=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.Atan": "tf.raw_ops.Atan(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Atan2": "tf.raw_ops.Atan2(\n    y, x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Atanh": "tf.raw_ops.Atanh(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.AudioSpectrogram": "tf.raw_ops.AudioSpectrogram(\n    input, window_size, stride, magnitude_squared=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.AudioSummary": "tf.raw_ops.AudioSummary(\n    tag, tensor, sample_rate, max_outputs=3, name=None\n)\n"
  },
  {
    "tf.raw_ops.AudioSummaryV2": "tf.raw_ops.AudioSummaryV2(\n    tag, tensor, sample_rate, max_outputs=3, name=None\n)\n"
  },
  {
    "tf.raw_ops.AutoShardDataset": "tf.raw_ops.AutoShardDataset(\n    input_dataset,\n    num_workers,\n    index,\n    output_types,\n    output_shapes,\n    auto_shard_policy=0,\n    num_replicas=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.AvgPool": "tf.raw_ops.AvgPool(\n    value, ksize, strides, padding, data_format='NHWC', name=None\n)\n"
  },
  {
    "tf.raw_ops.AvgPool3D": "tf.raw_ops.AvgPool3D(\n    input, ksize, strides, padding, data_format='NDHWC', name=None\n)\n"
  },
  {
    "tf.raw_ops.AvgPool3DGrad": "tf.raw_ops.AvgPool3DGrad(\n    orig_input_shape,\n    grad,\n    ksize,\n    strides,\n    padding,\n    data_format='NDHWC',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.AvgPoolGrad": "tf.raw_ops.AvgPoolGrad(\n    orig_input_shape,\n    grad,\n    ksize,\n    strides,\n    padding,\n    data_format='NHWC',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BandedTriangularSolve": "tf.raw_ops.BandedTriangularSolve(\n    matrix, rhs, lower=True, adjoint=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.Barrier": "tf.raw_ops.Barrier(\n    component_types,\n    shapes=[],\n    capacity=-1,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BarrierClose": "tf.raw_ops.BarrierClose(\n    handle, cancel_pending_enqueues=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.BarrierIncompleteSize": "tf.raw_ops.BarrierIncompleteSize(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.BarrierInsertMany": "tf.raw_ops.BarrierInsertMany(\n    handle, keys, values, component_index, name=None\n)\n"
  },
  {
    "tf.raw_ops.BarrierReadySize": "tf.raw_ops.BarrierReadySize(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.BarrierTakeMany": "tf.raw_ops.BarrierTakeMany(\n    handle,\n    num_elements,\n    component_types,\n    allow_small_batch=False,\n    wait_for_incomplete=False,\n    timeout_ms=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Batch": "tf.raw_ops.Batch(\n    in_tensors,\n    num_batch_threads,\n    max_batch_size,\n    batch_timeout_micros,\n    grad_timeout_micros,\n    max_enqueued_batches=10,\n    allowed_batch_sizes=[],\n    container='',\n    shared_name='',\n    batching_queue='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchCholesky": "tf.raw_ops.BatchCholesky(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchCholeskyGrad": "tf.raw_ops.BatchCholeskyGrad(\n    l, grad, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchDataset": "tf.raw_ops.BatchDataset(\n    input_dataset,\n    batch_size,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchDatasetV2": "tf.raw_ops.BatchDatasetV2(\n    input_dataset,\n    batch_size,\n    drop_remainder,\n    output_types,\n    output_shapes,\n    parallel_copy=False,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchFFT": "tf.raw_ops.BatchFFT(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchFFT2D": "tf.raw_ops.BatchFFT2D(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchFFT3D": "tf.raw_ops.BatchFFT3D(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchFunction": "tf.raw_ops.BatchFunction(\n    in_tensors,\n    captured_tensors,\n    f,\n    num_batch_threads,\n    max_batch_size,\n    batch_timeout_micros,\n    Tout,\n    max_enqueued_batches=10,\n    allowed_batch_sizes=[],\n    container='',\n    shared_name='',\n    batching_queue='',\n    enable_large_batch_splitting=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchIFFT": "tf.raw_ops.BatchIFFT(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchIFFT2D": "tf.raw_ops.BatchIFFT2D(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchIFFT3D": "tf.raw_ops.BatchIFFT3D(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatMul": "tf.raw_ops.BatchMatMul(\n    x, y, adj_x=False, adj_y=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatMulV2": "tf.raw_ops.BatchMatMulV2(\n    x, y, adj_x=False, adj_y=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatMulV3": "tf.raw_ops.BatchMatMulV3(\n    x, y, Tout, adj_x=False, adj_y=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatrixBandPart": "tf.raw_ops.BatchMatrixBandPart(\n    input, num_lower, num_upper, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatrixDeterminant": "tf.raw_ops.BatchMatrixDeterminant(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatrixDiag": "tf.raw_ops.BatchMatrixDiag(\n    diagonal, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatrixDiagPart": "tf.raw_ops.BatchMatrixDiagPart(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatrixInverse": "tf.raw_ops.BatchMatrixInverse(\n    input, adjoint=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatrixSetDiag": "tf.raw_ops.BatchMatrixSetDiag(\n    input, diagonal, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatrixSolve": "tf.raw_ops.BatchMatrixSolve(\n    matrix, rhs, adjoint=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatrixSolveLs": "tf.raw_ops.BatchMatrixSolveLs(\n    matrix, rhs, l2_regularizer, fast=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchMatrixTriangularSolve": "tf.raw_ops.BatchMatrixTriangularSolve(\n    matrix, rhs, lower=True, adjoint=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchNormWithGlobalNormalization": "tf.raw_ops.BatchNormWithGlobalNormalization(\n    t,\n    m,\n    v,\n    beta,\n    gamma,\n    variance_epsilon,\n    scale_after_normalization,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchNormWithGlobalNormalizationGrad": "tf.raw_ops.BatchNormWithGlobalNormalizationGrad(\n    t,\n    m,\n    v,\n    gamma,\n    backprop,\n    variance_epsilon,\n    scale_after_normalization,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchSelfAdjointEig": "tf.raw_ops.BatchSelfAdjointEig(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchSelfAdjointEigV2": "tf.raw_ops.BatchSelfAdjointEigV2(\n    input, compute_v=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchSvd": "tf.raw_ops.BatchSvd(\n    input, compute_uv=True, full_matrices=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchToSpace": "tf.raw_ops.BatchToSpace(\n    input, crops, block_size, name=None\n)\n"
  },
  {
    "tf.raw_ops.BatchToSpaceND": "tf.raw_ops.BatchToSpaceND(\n    input, block_shape, crops, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselI0": "tf.raw_ops.BesselI0(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselI0e": "tf.raw_ops.BesselI0e(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselI1": "tf.raw_ops.BesselI1(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselI1e": "tf.raw_ops.BesselI1e(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselJ0": "tf.raw_ops.BesselJ0(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselJ1": "tf.raw_ops.BesselJ1(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselK0": "tf.raw_ops.BesselK0(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselK0e": "tf.raw_ops.BesselK0e(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselK1": "tf.raw_ops.BesselK1(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselK1e": "tf.raw_ops.BesselK1e(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselY0": "tf.raw_ops.BesselY0(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BesselY1": "tf.raw_ops.BesselY1(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Betainc": "tf.raw_ops.Betainc(\n    a, b, x, name=None\n)\n"
  },
  {
    "tf.raw_ops.BiasAdd": "tf.raw_ops.BiasAdd(\n    value, bias, data_format='NHWC', name=None\n)\n"
  },
  {
    "tf.raw_ops.BiasAddGrad": "tf.raw_ops.BiasAddGrad(\n    out_backprop, data_format='NHWC', name=None\n)\n"
  },
  {
    "tf.raw_ops.BiasAddV1": "tf.raw_ops.BiasAddV1(\n    value, bias, name=None\n)\n"
  },
  {
    "tf.raw_ops.Bincount": "tf.raw_ops.Bincount(\n    arr, size, weights, name=None\n)\n"
  },
  {
    "tf.raw_ops.Bitcast": "tf.raw_ops.Bitcast(\n    input, type, name=None\n)\n"
  },
  {
    "tf.raw_ops.BitwiseAnd": "tf.raw_ops.BitwiseAnd(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.BitwiseOr": "tf.raw_ops.BitwiseOr(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.BitwiseXor": "tf.raw_ops.BitwiseXor(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.BlockLSTM": "tf.raw_ops.BlockLSTM(\n    seq_len_max,\n    x,\n    cs_prev,\n    h_prev,\n    w,\n    wci,\n    wcf,\n    wco,\n    b,\n    forget_bias=1,\n    cell_clip=3,\n    use_peephole=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BlockLSTMGrad": "tf.raw_ops.BlockLSTMGrad(\n    seq_len_max,\n    x,\n    cs_prev,\n    h_prev,\n    w,\n    wci,\n    wcf,\n    wco,\n    b,\n    i,\n    cs,\n    f,\n    o,\n    ci,\n    co,\n    h,\n    cs_grad,\n    h_grad,\n    use_peephole,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BlockLSTMGradV2": "tf.raw_ops.BlockLSTMGradV2(\n    seq_len_max,\n    x,\n    cs_prev,\n    h_prev,\n    w,\n    wci,\n    wcf,\n    wco,\n    b,\n    i,\n    cs,\n    f,\n    o,\n    ci,\n    co,\n    h,\n    cs_grad,\n    h_grad,\n    use_peephole,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BlockLSTMV2": "tf.raw_ops.BlockLSTMV2(\n    seq_len_max,\n    x,\n    cs_prev,\n    h_prev,\n    w,\n    wci,\n    wcf,\n    wco,\n    b,\n    cell_clip=0,\n    use_peephole=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesAggregateStats": "tf.raw_ops.BoostedTreesAggregateStats(\n    node_ids, gradients, hessians, feature, max_splits, num_buckets, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesBucketize": "tf.raw_ops.BoostedTreesBucketize(\n    float_values, bucket_boundaries, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesCalculateBestFeatureSplit": "tf.raw_ops.BoostedTreesCalculateBestFeatureSplit(\n    node_id_range,\n    stats_summary,\n    l1,\n    l2,\n    tree_complexity,\n    min_node_weight,\n    logits_dimension,\n    split_type='inequality',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesCalculateBestFeatureSplitV2": "tf.raw_ops.BoostedTreesCalculateBestFeatureSplitV2(\n    node_id_range,\n    stats_summaries_list,\n    split_types,\n    candidate_feature_ids,\n    l1,\n    l2,\n    tree_complexity,\n    min_node_weight,\n    logits_dimension,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesCalculateBestGainsPerFeature": "tf.raw_ops.BoostedTreesCalculateBestGainsPerFeature(\n    node_id_range,\n    stats_summary_list,\n    l1,\n    l2,\n    tree_complexity,\n    min_node_weight,\n    max_splits,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesCenterBias": "tf.raw_ops.BoostedTreesCenterBias(\n    tree_ensemble_handle, mean_gradients, mean_hessians, l1, l2, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesCreateEnsemble": "tf.raw_ops.BoostedTreesCreateEnsemble(\n    tree_ensemble_handle, stamp_token, tree_ensemble_serialized, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesCreateQuantileStreamResource": "tf.raw_ops.BoostedTreesCreateQuantileStreamResource(\n    quantile_stream_resource_handle,\n    epsilon,\n    num_streams,\n    max_elements=1099511627776,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesDeserializeEnsemble": "tf.raw_ops.BoostedTreesDeserializeEnsemble(\n    tree_ensemble_handle, stamp_token, tree_ensemble_serialized, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesEnsembleResourceHandleOp": "tf.raw_ops.BoostedTreesEnsembleResourceHandleOp(\n    container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesExampleDebugOutputs": "tf.raw_ops.BoostedTreesExampleDebugOutputs(\n    tree_ensemble_handle, bucketized_features, logits_dimension, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesFlushQuantileSummaries": "tf.raw_ops.BoostedTreesFlushQuantileSummaries(\n    quantile_stream_resource_handle, num_features, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesGetEnsembleStates": "tf.raw_ops.BoostedTreesGetEnsembleStates(\n    tree_ensemble_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesMakeQuantileSummaries": "tf.raw_ops.BoostedTreesMakeQuantileSummaries(\n    float_values, example_weights, epsilon, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesMakeStatsSummary": "tf.raw_ops.BoostedTreesMakeStatsSummary(\n    node_ids,\n    gradients,\n    hessians,\n    bucketized_features_list,\n    max_splits,\n    num_buckets,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesPredict": "tf.raw_ops.BoostedTreesPredict(\n    tree_ensemble_handle, bucketized_features, logits_dimension, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesQuantileStreamResourceAddSummaries": "tf.raw_ops.BoostedTreesQuantileStreamResourceAddSummaries(\n    quantile_stream_resource_handle, summaries, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesQuantileStreamResourceDeserialize": "tf.raw_ops.BoostedTreesQuantileStreamResourceDeserialize(\n    quantile_stream_resource_handle, bucket_boundaries, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesQuantileStreamResourceFlush": "tf.raw_ops.BoostedTreesQuantileStreamResourceFlush(\n    quantile_stream_resource_handle,\n    num_buckets,\n    generate_quantiles=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesQuantileStreamResourceGetBucketBoundaries": "tf.raw_ops.BoostedTreesQuantileStreamResourceGetBucketBoundaries(\n    quantile_stream_resource_handle, num_features, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesQuantileStreamResourceHandleOp": "tf.raw_ops.BoostedTreesQuantileStreamResourceHandleOp(\n    container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesSerializeEnsemble": "tf.raw_ops.BoostedTreesSerializeEnsemble(\n    tree_ensemble_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesSparseAggregateStats": "tf.raw_ops.BoostedTreesSparseAggregateStats(\n    node_ids,\n    gradients,\n    hessians,\n    feature_indices,\n    feature_values,\n    feature_shape,\n    max_splits,\n    num_buckets,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesSparseCalculateBestFeatureSplit": "tf.raw_ops.BoostedTreesSparseCalculateBestFeatureSplit(\n    node_id_range,\n    stats_summary_indices,\n    stats_summary_values,\n    stats_summary_shape,\n    l1,\n    l2,\n    tree_complexity,\n    min_node_weight,\n    logits_dimension,\n    split_type='inequality',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesTrainingPredict": "tf.raw_ops.BoostedTreesTrainingPredict(\n    tree_ensemble_handle,\n    cached_tree_ids,\n    cached_node_ids,\n    bucketized_features,\n    logits_dimension,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesUpdateEnsemble": "tf.raw_ops.BoostedTreesUpdateEnsemble(\n    tree_ensemble_handle,\n    feature_ids,\n    node_ids,\n    gains,\n    thresholds,\n    left_node_contribs,\n    right_node_contribs,\n    max_depth,\n    learning_rate,\n    pruning_mode,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BoostedTreesUpdateEnsembleV2": "tf.raw_ops.BoostedTreesUpdateEnsembleV2(\n    tree_ensemble_handle,\n    feature_ids,\n    dimension_ids,\n    node_ids,\n    gains,\n    thresholds,\n    left_node_contribs,\n    right_node_contribs,\n    split_types,\n    max_depth,\n    learning_rate,\n    pruning_mode,\n    logits_dimension=1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.BroadcastArgs": "tf.raw_ops.BroadcastArgs(\n    s0, s1, name=None\n)\n"
  },
  {
    "tf.raw_ops.BroadcastGradientArgs": "tf.raw_ops.BroadcastGradientArgs(\n    s0, s1, name=None\n)\n"
  },
  {
    "tf.raw_ops.BroadcastTo": "tf.raw_ops.BroadcastTo(\n    input, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.Bucketize": "tf.raw_ops.Bucketize(\n    input, boundaries, name=None\n)\n"
  },
  {
    "tf.raw_ops.BytesProducedStatsDataset": "tf.raw_ops.BytesProducedStatsDataset(\n    input_dataset, tag, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.CSRSparseMatrixComponents": "tf.raw_ops.CSRSparseMatrixComponents(\n    csr_sparse_matrix, index, type, name=None\n)\n"
  },
  {
    "tf.raw_ops.CSRSparseMatrixToDense": "tf.raw_ops.CSRSparseMatrixToDense(\n    sparse_input, type, name=None\n)\n"
  },
  {
    "tf.raw_ops.CSRSparseMatrixToSparseTensor": "tf.raw_ops.CSRSparseMatrixToSparseTensor(\n    sparse_matrix, type, name=None\n)\n"
  },
  {
    "tf.raw_ops.CSVDataset": "tf.raw_ops.CSVDataset(\n    filenames,\n    compression_type,\n    buffer_size,\n    header,\n    field_delim,\n    use_quote_delim,\n    na_value,\n    select_cols,\n    record_defaults,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CSVDatasetV2": "tf.raw_ops.CSVDatasetV2(\n    filenames,\n    compression_type,\n    buffer_size,\n    header,\n    field_delim,\n    use_quote_delim,\n    na_value,\n    select_cols,\n    record_defaults,\n    exclude_cols,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CTCBeamSearchDecoder": "tf.raw_ops.CTCBeamSearchDecoder(\n    inputs,\n    sequence_length,\n    beam_width,\n    top_paths,\n    merge_repeated=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CTCGreedyDecoder": "tf.raw_ops.CTCGreedyDecoder(\n    inputs, sequence_length, merge_repeated=False, blank_index=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.CTCLoss": "tf.raw_ops.CTCLoss(\n    inputs,\n    labels_indices,\n    labels_values,\n    sequence_length,\n    preprocess_collapse_repeated=False,\n    ctc_merge_repeated=True,\n    ignore_longer_outputs_than_inputs=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CTCLossV2": "tf.raw_ops.CTCLossV2(\n    inputs,\n    labels_indices,\n    labels_values,\n    sequence_length,\n    preprocess_collapse_repeated=False,\n    ctc_merge_repeated=True,\n    ignore_longer_outputs_than_inputs=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CacheDataset": "tf.raw_ops.CacheDataset(\n    input_dataset,\n    filename,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CacheDatasetV2": "tf.raw_ops.CacheDatasetV2(\n    input_dataset,\n    filename,\n    cache,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Case": "tf.raw_ops.Case(\n    branch_index, input, Tout, branches, output_shapes=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.Cast": "tf.raw_ops.Cast(\n    x, DstT, Truncate=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.Ceil": "tf.raw_ops.Ceil(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.CheckNumerics": "tf.raw_ops.CheckNumerics(\n    tensor, message, name=None\n)\n"
  },
  {
    "tf.raw_ops.CheckNumericsV2": "tf.raw_ops.CheckNumericsV2(\n    tensor, message, name=None\n)\n"
  },
  {
    "tf.raw_ops.Cholesky": "tf.raw_ops.Cholesky(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.CholeskyGrad": "tf.raw_ops.CholeskyGrad(\n    l, grad, name=None\n)\n"
  },
  {
    "tf.raw_ops.ChooseFastestBranchDataset": "tf.raw_ops.ChooseFastestBranchDataset(\n    input_dataset,\n    ratio_numerator,\n    ratio_denominator,\n    other_arguments,\n    num_elements_per_branch,\n    branches,\n    other_arguments_lengths,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ChooseFastestDataset": "tf.raw_ops.ChooseFastestDataset(\n    input_datasets, num_experiments, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ClipByValue": "tf.raw_ops.ClipByValue(\n    t, clip_value_min, clip_value_max, name=None\n)\n"
  },
  {
    "tf.raw_ops.CloseSummaryWriter": "tf.raw_ops.CloseSummaryWriter(\n    writer, name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveAllToAllV3": "tf.raw_ops.CollectiveAllToAllV3(\n    input, communicator, group_assignment, timeout_seconds=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveAssignGroupV2": "tf.raw_ops.CollectiveAssignGroupV2(\n    group_assignment, device_index, base_key, name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveBcastRecv": "tf.raw_ops.CollectiveBcastRecv(\n    T,\n    group_size,\n    group_key,\n    instance_key,\n    shape,\n    communication_hint='auto',\n    timeout_seconds=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveBcastRecvV2": "tf.raw_ops.CollectiveBcastRecvV2(\n    group_size,\n    group_key,\n    instance_key,\n    shape,\n    T,\n    communication_hint='auto',\n    timeout_seconds=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveBcastSend": "tf.raw_ops.CollectiveBcastSend(\n    input,\n    group_size,\n    group_key,\n    instance_key,\n    shape,\n    communication_hint='auto',\n    timeout_seconds=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveBcastSendV2": "tf.raw_ops.CollectiveBcastSendV2(\n    input,\n    group_size,\n    group_key,\n    instance_key,\n    communication_hint='auto',\n    timeout_seconds=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveGather": "tf.raw_ops.CollectiveGather(\n    input,\n    group_size,\n    group_key,\n    instance_key,\n    shape,\n    communication_hint='auto',\n    timeout_seconds=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveGatherV2": "tf.raw_ops.CollectiveGatherV2(\n    input,\n    group_size,\n    group_key,\n    instance_key,\n    ordering_token,\n    communication_hint='auto',\n    timeout_seconds=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveInitializeCommunicator": "tf.raw_ops.CollectiveInitializeCommunicator(\n    group_key,\n    rank,\n    group_size,\n    communication_hint='auto',\n    timeout_seconds=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectivePermute": "tf.raw_ops.CollectivePermute(\n    input, source_target_pairs, name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveReduce": "tf.raw_ops.CollectiveReduce(\n    input,\n    group_size,\n    group_key,\n    instance_key,\n    merge_op,\n    final_op,\n    subdiv_offsets,\n    wait_for=[],\n    communication_hint='auto',\n    timeout_seconds=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveReduceV2": "tf.raw_ops.CollectiveReduceV2(\n    input,\n    group_size,\n    group_key,\n    instance_key,\n    ordering_token,\n    merge_op,\n    final_op,\n    communication_hint='auto',\n    timeout_seconds=0,\n    max_subdivs_per_device=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CollectiveReduceV3": "tf.raw_ops.CollectiveReduceV3(\n    input,\n    communicator,\n    group_assignment,\n    reduction,\n    timeout_seconds=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CombinedNonMaxSuppression": "tf.raw_ops.CombinedNonMaxSuppression(\n    boxes,\n    scores,\n    max_output_size_per_class,\n    max_total_size,\n    iou_threshold,\n    score_threshold,\n    pad_per_class=False,\n    clip_boxes=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Complex": "tf.raw_ops.Complex(\n    real,\n    imag,\n    Tout=tf.dtypes.complex64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ComplexAbs": "tf.raw_ops.ComplexAbs(\n    x,\n    Tout=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CompositeTensorVariantFromComponents": "tf.raw_ops.CompositeTensorVariantFromComponents(\n    components, metadata, name=None\n)\n"
  },
  {
    "tf.raw_ops.CompositeTensorVariantToComponents": "tf.raw_ops.CompositeTensorVariantToComponents(\n    encoded, metadata, Tcomponents, name=None\n)\n"
  },
  {
    "tf.raw_ops.CompressElement": "tf.raw_ops.CompressElement(\n    components, name=None\n)\n"
  },
  {
    "tf.raw_ops.ComputeAccidentalHits": "tf.raw_ops.ComputeAccidentalHits(\n    true_classes, sampled_candidates, num_true, seed=0, seed2=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.ComputeBatchSize": "tf.raw_ops.ComputeBatchSize(\n    input_dataset, name=None\n)\n"
  },
  {
    "tf.raw_ops.Concat": "tf.raw_ops.Concat(\n    concat_dim, values, name=None\n)\n"
  },
  {
    "tf.raw_ops.ConcatOffset": "tf.raw_ops.ConcatOffset(\n    concat_dim, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.ConcatV2": "tf.raw_ops.ConcatV2(\n    values, axis, name=None\n)\n"
  },
  {
    "tf.raw_ops.ConcatenateDataset": "tf.raw_ops.ConcatenateDataset(\n    input_dataset,\n    another_dataset,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ConditionalAccumulator": "tf.raw_ops.ConditionalAccumulator(\n    dtype,\n    shape,\n    container='',\n    shared_name='',\n    reduction_type='MEAN',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ConfigureDistributedTPU": "tf.raw_ops.ConfigureDistributedTPU(\n    embedding_config='',\n    tpu_embedding_config='',\n    is_global_init=False,\n    enable_whole_mesh_compilations=False,\n    compilation_failure_closes_chips=True,\n    tpu_cancellation_closes_chips=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ConfigureTPUEmbedding": "tf.raw_ops.ConfigureTPUEmbedding(\n    config, name=None\n)\n"
  },
  {
    "tf.raw_ops.Conj": "tf.raw_ops.Conj(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.ConjugateTranspose": "tf.raw_ops.ConjugateTranspose(\n    x, perm, name=None\n)\n"
  },
  {
    "tf.raw_ops.Const": "tf.raw_ops.Const(\n    value, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.ConsumeMutexLock": "tf.raw_ops.ConsumeMutexLock(\n    mutex_lock, name=None\n)\n"
  },
  {
    "tf.raw_ops.ControlTrigger": "tf.raw_ops.ControlTrigger(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Conv2D": "tf.raw_ops.Conv2D(\n    input,\n    filter,\n    strides,\n    padding,\n    use_cudnn_on_gpu=True,\n    explicit_paddings=[],\n    data_format='NHWC',\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Conv2DBackpropFilter": "tf.raw_ops.Conv2DBackpropFilter(\n    input,\n    filter_sizes,\n    out_backprop,\n    strides,\n    padding,\n    use_cudnn_on_gpu=True,\n    explicit_paddings=[],\n    data_format='NHWC',\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Conv2DBackpropInput": "tf.raw_ops.Conv2DBackpropInput(\n    input_sizes,\n    filter,\n    out_backprop,\n    strides,\n    padding,\n    use_cudnn_on_gpu=True,\n    explicit_paddings=[],\n    data_format='NHWC',\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Conv3D": "tf.raw_ops.Conv3D(\n    input,\n    filter,\n    strides,\n    padding,\n    data_format='NDHWC',\n    dilations=[1, 1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Conv3DBackpropFilter": "tf.raw_ops.Conv3DBackpropFilter(\n    input,\n    filter,\n    out_backprop,\n    strides,\n    padding,\n    dilations=[1, 1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Conv3DBackpropFilterV2": "tf.raw_ops.Conv3DBackpropFilterV2(\n    input,\n    filter_sizes,\n    out_backprop,\n    strides,\n    padding,\n    data_format='NDHWC',\n    dilations=[1, 1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Conv3DBackpropInput": "tf.raw_ops.Conv3DBackpropInput(\n    input,\n    filter,\n    out_backprop,\n    strides,\n    padding,\n    dilations=[1, 1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Conv3DBackpropInputV2": "tf.raw_ops.Conv3DBackpropInputV2(\n    input_sizes,\n    filter,\n    out_backprop,\n    strides,\n    padding,\n    data_format='NDHWC',\n    dilations=[1, 1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Copy": "tf.raw_ops.Copy(\n    input, tensor_name='', debug_ops_spec=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.CopyHost": "tf.raw_ops.CopyHost(\n    input, tensor_name='', debug_ops_spec=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.Cos": "tf.raw_ops.Cos(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Cosh": "tf.raw_ops.Cosh(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.CountUpTo": "tf.raw_ops.CountUpTo(\n    ref, limit, name=None\n)\n"
  },
  {
    "tf.raw_ops.CreateSummaryDbWriter": "tf.raw_ops.CreateSummaryDbWriter(\n    writer, db_uri, experiment_name, run_name, user_name, name=None\n)\n"
  },
  {
    "tf.raw_ops.CreateSummaryFileWriter": "tf.raw_ops.CreateSummaryFileWriter(\n    writer, logdir, max_queue, flush_millis, filename_suffix, name=None\n)\n"
  },
  {
    "tf.raw_ops.CropAndResize": "tf.raw_ops.CropAndResize(\n    image,\n    boxes,\n    box_ind,\n    crop_size,\n    method='bilinear',\n    extrapolation_value=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CropAndResizeGradBoxes": "tf.raw_ops.CropAndResizeGradBoxes(\n    grads, image, boxes, box_ind, method='bilinear', name=None\n)\n"
  },
  {
    "tf.raw_ops.CropAndResizeGradImage": "tf.raw_ops.CropAndResizeGradImage(\n    grads,\n    boxes,\n    box_ind,\n    image_size,\n    T,\n    method='bilinear',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Cross": "tf.raw_ops.Cross(\n    a, b, name=None\n)\n"
  },
  {
    "tf.raw_ops.CrossReplicaSum": "tf.raw_ops.CrossReplicaSum(\n    input, group_assignment, name=None\n)\n"
  },
  {
    "tf.raw_ops.CudnnRNN": "tf.raw_ops.CudnnRNN(\n    input,\n    input_h,\n    input_c,\n    params,\n    rnn_mode='lstm',\n    input_mode='linear_input',\n    direction='unidirectional',\n    dropout=0,\n    seed=0,\n    seed2=0,\n    is_training=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CudnnRNNBackprop": "tf.raw_ops.CudnnRNNBackprop(\n    input,\n    input_h,\n    input_c,\n    params,\n    output,\n    output_h,\n    output_c,\n    output_backprop,\n    output_h_backprop,\n    output_c_backprop,\n    reserve_space,\n    rnn_mode='lstm',\n    input_mode='linear_input',\n    direction='unidirectional',\n    dropout=0,\n    seed=0,\n    seed2=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CudnnRNNBackpropV2": "tf.raw_ops.CudnnRNNBackpropV2(\n    input,\n    input_h,\n    input_c,\n    params,\n    output,\n    output_h,\n    output_c,\n    output_backprop,\n    output_h_backprop,\n    output_c_backprop,\n    reserve_space,\n    host_reserved,\n    rnn_mode='lstm',\n    input_mode='linear_input',\n    direction='unidirectional',\n    dropout=0,\n    seed=0,\n    seed2=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CudnnRNNBackpropV3": "tf.raw_ops.CudnnRNNBackpropV3(\n    input,\n    input_h,\n    input_c,\n    params,\n    sequence_lengths,\n    output,\n    output_h,\n    output_c,\n    output_backprop,\n    output_h_backprop,\n    output_c_backprop,\n    reserve_space,\n    host_reserved,\n    rnn_mode='lstm',\n    input_mode='linear_input',\n    direction='unidirectional',\n    dropout=0,\n    seed=0,\n    seed2=0,\n    num_proj=0,\n    time_major=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CudnnRNNCanonicalToParams": "tf.raw_ops.CudnnRNNCanonicalToParams(\n    num_layers,\n    num_units,\n    input_size,\n    weights,\n    biases,\n    rnn_mode='lstm',\n    input_mode='linear_input',\n    direction='unidirectional',\n    dropout=0,\n    seed=0,\n    seed2=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CudnnRNNCanonicalToParamsV2": "tf.raw_ops.CudnnRNNCanonicalToParamsV2(\n    num_layers,\n    num_units,\n    input_size,\n    weights,\n    biases,\n    rnn_mode='lstm',\n    input_mode='linear_input',\n    direction='unidirectional',\n    dropout=0,\n    seed=0,\n    seed2=0,\n    num_proj=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CudnnRNNParamsSize": "tf.raw_ops.CudnnRNNParamsSize(\n    num_layers,\n    num_units,\n    input_size,\n    T,\n    S,\n    rnn_mode='lstm',\n    input_mode='linear_input',\n    direction='unidirectional',\n    dropout=0,\n    seed=0,\n    seed2=0,\n    num_proj=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CudnnRNNParamsToCanonical": "tf.raw_ops.CudnnRNNParamsToCanonical(\n    num_layers,\n    num_units,\n    input_size,\n    params,\n    num_params,\n    rnn_mode='lstm',\n    input_mode='linear_input',\n    direction='unidirectional',\n    dropout=0,\n    seed=0,\n    seed2=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CudnnRNNParamsToCanonicalV2": "tf.raw_ops.CudnnRNNParamsToCanonicalV2(\n    num_layers,\n    num_units,\n    input_size,\n    params,\n    num_params_weights,\n    num_params_biases,\n    rnn_mode='lstm',\n    input_mode='linear_input',\n    direction='unidirectional',\n    dropout=0,\n    seed=0,\n    seed2=0,\n    num_proj=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CudnnRNNV2": "tf.raw_ops.CudnnRNNV2(\n    input,\n    input_h,\n    input_c,\n    params,\n    rnn_mode='lstm',\n    input_mode='linear_input',\n    direction='unidirectional',\n    dropout=0,\n    seed=0,\n    seed2=0,\n    is_training=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.CudnnRNNV3": "tf.raw_ops.CudnnRNNV3(\n    input,\n    input_h,\n    input_c,\n    params,\n    sequence_lengths,\n    rnn_mode='lstm',\n    input_mode='linear_input',\n    direction='unidirectional',\n    dropout=0,\n    seed=0,\n    seed2=0,\n    num_proj=0,\n    is_training=True,\n    time_major=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Cumprod": "tf.raw_ops.Cumprod(\n    x, axis, exclusive=False, reverse=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.Cumsum": "tf.raw_ops.Cumsum(\n    x, axis, exclusive=False, reverse=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.CumulativeLogsumexp": "tf.raw_ops.CumulativeLogsumexp(\n    x, axis, exclusive=False, reverse=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.DataFormatDimMap": "tf.raw_ops.DataFormatDimMap(\n    x, src_format='NHWC', dst_format='NCHW', name=None\n)\n"
  },
  {
    "tf.raw_ops.DataFormatVecPermute": "tf.raw_ops.DataFormatVecPermute(\n    x, src_format='NHWC', dst_format='NCHW', name=None\n)\n"
  },
  {
    "tf.raw_ops.DataServiceDataset": "tf.raw_ops.DataServiceDataset(\n    dataset_id,\n    processing_mode,\n    address,\n    protocol,\n    job_name,\n    max_outstanding_requests,\n    iteration_counter,\n    output_types,\n    output_shapes,\n    task_refresh_interval_hint_ms=-1,\n    data_transfer_protocol='',\n    target_workers='AUTO',\n    cross_trainer_cache_options='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DataServiceDatasetV2": "tf.raw_ops.DataServiceDatasetV2(\n    dataset_id,\n    processing_mode,\n    address,\n    protocol,\n    job_name,\n    consumer_index,\n    num_consumers,\n    max_outstanding_requests,\n    iteration_counter,\n    output_types,\n    output_shapes,\n    task_refresh_interval_hint_ms=-1,\n    data_transfer_protocol='',\n    target_workers='AUTO',\n    cross_trainer_cache_options='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DataServiceDatasetV3": "tf.raw_ops.DataServiceDatasetV3(\n    dataset_id,\n    processing_mode,\n    address,\n    protocol,\n    job_name,\n    consumer_index,\n    num_consumers,\n    max_outstanding_requests,\n    iteration_counter,\n    output_types,\n    output_shapes,\n    uncompress_fn,\n    task_refresh_interval_hint_ms=-1,\n    data_transfer_protocol='',\n    target_workers='AUTO',\n    uncompress=False,\n    cross_trainer_cache_options='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DataServiceDatasetV4": "tf.raw_ops.DataServiceDatasetV4(\n    dataset_id,\n    processing_mode,\n    address,\n    protocol,\n    job_name,\n    consumer_index,\n    num_consumers,\n    max_outstanding_requests,\n    iteration_counter,\n    output_types,\n    output_shapes,\n    uncompress_fn,\n    task_refresh_interval_hint_ms=-1,\n    data_transfer_protocol='',\n    target_workers='AUTO',\n    uncompress=False,\n    cross_trainer_cache_options='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DatasetCardinality": "tf.raw_ops.DatasetCardinality(\n    input_dataset, name=None\n)\n"
  },
  {
    "tf.raw_ops.DatasetFromGraph": "tf.raw_ops.DatasetFromGraph(\n    graph_def, name=None\n)\n"
  },
  {
    "tf.raw_ops.DatasetToGraph": "tf.raw_ops.DatasetToGraph(\n    input_dataset,\n    stateful_whitelist=[],\n    allow_stateful=False,\n    strip_device_assignment=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DatasetToGraphV2": "tf.raw_ops.DatasetToGraphV2(\n    input_dataset,\n    external_state_policy=0,\n    strip_device_assignment=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DatasetToSingleElement": "tf.raw_ops.DatasetToSingleElement(\n    dataset, output_types, output_shapes, metadata='', name=None\n)\n"
  },
  {
    "tf.raw_ops.DatasetToTFRecord": "tf.raw_ops.DatasetToTFRecord(\n    input_dataset, filename, compression_type, name=None\n)\n"
  },
  {
    "tf.raw_ops.Dawsn": "tf.raw_ops.Dawsn(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.DebugGradientIdentity": "tf.raw_ops.DebugGradientIdentity(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.DebugGradientRefIdentity": "tf.raw_ops.DebugGradientRefIdentity(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.DebugIdentity": "tf.raw_ops.DebugIdentity(\n    input,\n    device_name='',\n    tensor_name='',\n    debug_urls=[],\n    gated_grpc=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DebugIdentityV2": "tf.raw_ops.DebugIdentityV2(\n    input,\n    tfdbg_context_id='',\n    op_name='',\n    output_slot=-1,\n    tensor_debug_mode=-1,\n    debug_urls=[],\n    circular_buffer_size=1000,\n    tfdbg_run_id='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DebugNanCount": "tf.raw_ops.DebugNanCount(\n    input,\n    device_name='',\n    tensor_name='',\n    debug_urls=[],\n    gated_grpc=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DebugNumericSummary": "tf.raw_ops.DebugNumericSummary(\n    input,\n    device_name='',\n    tensor_name='',\n    debug_urls=[],\n    lower_bound=float('-inf'),\n    upper_bound=float('inf'),\n    mute_if_healthy=False,\n    gated_grpc=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DebugNumericSummaryV2": "tf.raw_ops.DebugNumericSummaryV2(\n    input,\n    output_dtype=tf.dtypes.float32,\n    tensor_debug_mode=-1,\n    tensor_id=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeAndCropJpeg": "tf.raw_ops.DecodeAndCropJpeg(\n    contents,\n    crop_window,\n    channels=0,\n    ratio=1,\n    fancy_upscaling=True,\n    try_recover_truncated=False,\n    acceptable_fraction=1,\n    dct_method='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeBase64": "tf.raw_ops.DecodeBase64(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeBmp": "tf.raw_ops.DecodeBmp(\n    contents, channels=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeCSV": "tf.raw_ops.DecodeCSV(\n    records,\n    record_defaults,\n    field_delim=',',\n    use_quote_delim=True,\n    na_value='',\n    select_cols=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeCompressed": "tf.raw_ops.DecodeCompressed(\n    bytes, compression_type='', name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeGif": "tf.raw_ops.DecodeGif(\n    contents, name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeImage": "tf.raw_ops.DecodeImage(\n    contents,\n    channels=0,\n    dtype=tf.dtypes.uint8,\n    expand_animations=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeJSONExample": "tf.raw_ops.DecodeJSONExample(\n    json_examples, name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeJpeg": "tf.raw_ops.DecodeJpeg(\n    contents,\n    channels=0,\n    ratio=1,\n    fancy_upscaling=True,\n    try_recover_truncated=False,\n    acceptable_fraction=1,\n    dct_method='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodePaddedRaw": "tf.raw_ops.DecodePaddedRaw(\n    input_bytes, fixed_length, out_type, little_endian=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodePng": "tf.raw_ops.DecodePng(\n    contents,\n    channels=0,\n    dtype=tf.dtypes.uint8,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeProtoV2": "tf.raw_ops.DecodeProtoV2(\n    bytes,\n    message_type,\n    field_names,\n    output_types,\n    descriptor_source='local://',\n    message_format='binary',\n    sanitize=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeRaw": "tf.raw_ops.DecodeRaw(\n    bytes, out_type, little_endian=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.DecodeWav": "tf.raw_ops.DecodeWav(\n    contents, desired_channels=-1, desired_samples=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.DeepCopy": "tf.raw_ops.DeepCopy(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.DeleteIterator": "tf.raw_ops.DeleteIterator(\n    handle, deleter, name=None\n)\n"
  },
  {
    "tf.raw_ops.DeleteMemoryCache": "tf.raw_ops.DeleteMemoryCache(\n    handle, deleter, name=None\n)\n"
  },
  {
    "tf.raw_ops.DeleteMultiDeviceIterator": "tf.raw_ops.DeleteMultiDeviceIterator(\n    multi_device_iterator, iterators, deleter, name=None\n)\n"
  },
  {
    "tf.raw_ops.DeleteRandomSeedGenerator": "tf.raw_ops.DeleteRandomSeedGenerator(\n    handle, deleter, name=None\n)\n"
  },
  {
    "tf.raw_ops.DeleteSeedGenerator": "tf.raw_ops.DeleteSeedGenerator(\n    handle, deleter, name=None\n)\n"
  },
  {
    "tf.raw_ops.DeleteSessionTensor": "tf.raw_ops.DeleteSessionTensor(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.DenseBincount": "tf.raw_ops.DenseBincount(\n    input, size, weights, binary_output=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.DenseCountSparseOutput": "tf.raw_ops.DenseCountSparseOutput(\n    values, weights, binary_output, minlength=-1, maxlength=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.DenseToCSRSparseMatrix": "tf.raw_ops.DenseToCSRSparseMatrix(\n    dense_input, indices, name=None\n)\n"
  },
  {
    "tf.raw_ops.DenseToDenseSetOperation": "tf.raw_ops.DenseToDenseSetOperation(\n    set1, set2, set_operation, validate_indices=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.DenseToSparseBatchDataset": "tf.raw_ops.DenseToSparseBatchDataset(\n    input_dataset,\n    batch_size,\n    row_shape,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DenseToSparseSetOperation": "tf.raw_ops.DenseToSparseSetOperation(\n    set1,\n    set2_indices,\n    set2_values,\n    set2_shape,\n    set_operation,\n    validate_indices=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DepthToSpace": "tf.raw_ops.DepthToSpace(\n    input, block_size, data_format='NHWC', name=None\n)\n"
  },
  {
    "tf.raw_ops.DepthwiseConv2dNative": "tf.raw_ops.DepthwiseConv2dNative(\n    input,\n    filter,\n    strides,\n    padding,\n    explicit_paddings=[],\n    data_format='NHWC',\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DepthwiseConv2dNativeBackpropFilter": "tf.raw_ops.DepthwiseConv2dNativeBackpropFilter(\n    input,\n    filter_sizes,\n    out_backprop,\n    strides,\n    padding,\n    explicit_paddings=[],\n    data_format='NHWC',\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DepthwiseConv2dNativeBackpropInput": "tf.raw_ops.DepthwiseConv2dNativeBackpropInput(\n    input_sizes,\n    filter,\n    out_backprop,\n    strides,\n    padding,\n    explicit_paddings=[],\n    data_format='NHWC',\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Dequantize": "tf.raw_ops.Dequantize(\n    input,\n    min_range,\n    max_range,\n    mode='MIN_COMBINED',\n    narrow_range=False,\n    axis=-1,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DeserializeIterator": "tf.raw_ops.DeserializeIterator(\n    resource_handle, serialized, name=None\n)\n"
  },
  {
    "tf.raw_ops.DeserializeManySparse": "tf.raw_ops.DeserializeManySparse(\n    serialized_sparse, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.DeserializeSparse": "tf.raw_ops.DeserializeSparse(\n    serialized_sparse, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.DestroyResourceOp": "tf.raw_ops.DestroyResourceOp(\n    resource, ignore_lookup_error=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.DestroyTemporaryVariable": "tf.raw_ops.DestroyTemporaryVariable(\n    ref, var_name, name=None\n)\n"
  },
  {
    "tf.raw_ops.DeviceIndex": "tf.raw_ops.DeviceIndex(\n    device_names, name=None\n)\n"
  },
  {
    "tf.raw_ops.Diag": "tf.raw_ops.Diag(\n    diagonal, name=None\n)\n"
  },
  {
    "tf.raw_ops.DiagPart": "tf.raw_ops.DiagPart(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.Digamma": "tf.raw_ops.Digamma(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Dilation2D": "tf.raw_ops.Dilation2D(\n    input, filter, strides, rates, padding, name=None\n)\n"
  },
  {
    "tf.raw_ops.Dilation2DBackpropFilter": "tf.raw_ops.Dilation2DBackpropFilter(\n    input, filter, out_backprop, strides, rates, padding, name=None\n)\n"
  },
  {
    "tf.raw_ops.Dilation2DBackpropInput": "tf.raw_ops.Dilation2DBackpropInput(\n    input, filter, out_backprop, strides, rates, padding, name=None\n)\n"
  },
  {
    "tf.raw_ops.DirectedInterleaveDataset": "tf.raw_ops.DirectedInterleaveDataset(\n    selector_input_dataset,\n    data_input_datasets,\n    output_types,\n    output_shapes,\n    stop_on_empty_dataset=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DisableCopyOnRead": "tf.raw_ops.DisableCopyOnRead(\n    resource, name=None\n)\n"
  },
  {
    "tf.raw_ops.Div": "tf.raw_ops.Div(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.DivNoNan": "tf.raw_ops.DivNoNan(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.DrawBoundingBoxes": "tf.raw_ops.DrawBoundingBoxes(\n    images, boxes, name=None\n)\n"
  },
  {
    "tf.raw_ops.DrawBoundingBoxesV2": "tf.raw_ops.DrawBoundingBoxesV2(\n    images, boxes, colors, name=None\n)\n"
  },
  {
    "tf.raw_ops.DummyIterationCounter": "tf.raw_ops.DummyIterationCounter(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DummyMemoryCache": "tf.raw_ops.DummyMemoryCache(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DummySeedGenerator": "tf.raw_ops.DummySeedGenerator(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DynamicEnqueueTPUEmbeddingArbitraryTensorBatch": "tf.raw_ops.DynamicEnqueueTPUEmbeddingArbitraryTensorBatch(\n    sample_indices_or_row_splits,\n    embedding_indices,\n    aggregation_weights,\n    mode_override,\n    device_ordinal,\n    combiners=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.DynamicPartition": "tf.raw_ops.DynamicPartition(\n    data, partitions, num_partitions, name=None\n)\n"
  },
  {
    "tf.raw_ops.DynamicStitch": "tf.raw_ops.DynamicStitch(\n    indices, data, name=None\n)\n"
  },
  {
    "tf.raw_ops.EagerPyFunc": "tf.raw_ops.EagerPyFunc(\n    input, token, Tout, is_async=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.EditDistance": "tf.raw_ops.EditDistance(\n    hypothesis_indices,\n    hypothesis_values,\n    hypothesis_shape,\n    truth_indices,\n    truth_values,\n    truth_shape,\n    normalize=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Eig": "tf.raw_ops.Eig(\n    input, Tout, compute_v=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.Einsum": "tf.raw_ops.Einsum(\n    inputs, equation, name=None\n)\n"
  },
  {
    "tf.raw_ops.Elu": "tf.raw_ops.Elu(\n    features, name=None\n)\n"
  },
  {
    "tf.raw_ops.EluGrad": "tf.raw_ops.EluGrad(\n    gradients, outputs, name=None\n)\n"
  },
  {
    "tf.raw_ops.Empty": "tf.raw_ops.Empty(\n    shape, dtype, init=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.EmptyTensorList": "tf.raw_ops.EmptyTensorList(\n    element_shape, max_num_elements, element_dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.EncodeBase64": "tf.raw_ops.EncodeBase64(\n    input, pad=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.EncodeJpeg": "tf.raw_ops.EncodeJpeg(\n    image,\n    format='',\n    quality=95,\n    progressive=False,\n    optimize_size=False,\n    chroma_downsampling=True,\n    density_unit='in',\n    x_density=300,\n    y_density=300,\n    xmp_metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.EncodeJpegVariableQuality": "tf.raw_ops.EncodeJpegVariableQuality(\n    images, quality, name=None\n)\n"
  },
  {
    "tf.raw_ops.EncodePng": "tf.raw_ops.EncodePng(\n    image, compression=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.EncodeProto": "tf.raw_ops.EncodeProto(\n    sizes,\n    values,\n    field_names,\n    message_type,\n    descriptor_source='local://',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.EncodeWav": "tf.raw_ops.EncodeWav(\n    audio, sample_rate, name=None\n)\n"
  },
  {
    "tf.raw_ops.EnqueueTPUEmbeddingArbitraryTensorBatch": "tf.raw_ops.EnqueueTPUEmbeddingArbitraryTensorBatch(\n    sample_indices_or_row_splits,\n    embedding_indices,\n    aggregation_weights,\n    mode_override,\n    device_ordinal=-1,\n    combiners=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.EnqueueTPUEmbeddingIntegerBatch": "tf.raw_ops.EnqueueTPUEmbeddingIntegerBatch(\n    batch, mode_override, device_ordinal=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.EnqueueTPUEmbeddingRaggedTensorBatch": "tf.raw_ops.EnqueueTPUEmbeddingRaggedTensorBatch(\n    sample_splits,\n    embedding_indices,\n    aggregation_weights,\n    mode_override,\n    table_ids,\n    device_ordinal=-1,\n    combiners=[],\n    max_sequence_lengths=[],\n    num_features=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.EnqueueTPUEmbeddingSparseBatch": "tf.raw_ops.EnqueueTPUEmbeddingSparseBatch(\n    sample_indices,\n    embedding_indices,\n    aggregation_weights,\n    mode_override,\n    device_ordinal=-1,\n    combiners=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.EnqueueTPUEmbeddingSparseTensorBatch": "tf.raw_ops.EnqueueTPUEmbeddingSparseTensorBatch(\n    sample_indices,\n    embedding_indices,\n    aggregation_weights,\n    mode_override,\n    table_ids,\n    device_ordinal=-1,\n    combiners=[],\n    max_sequence_lengths=[],\n    num_features=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.EnsureShape": "tf.raw_ops.EnsureShape(\n    input, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.Enter": "tf.raw_ops.Enter(\n    data, frame_name, is_constant=False, parallel_iterations=10, name=None\n)\n"
  },
  {
    "tf.raw_ops.Equal": "tf.raw_ops.Equal(\n    x, y, incompatible_shape_error=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.Erf": "tf.raw_ops.Erf(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Erfc": "tf.raw_ops.Erfc(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Erfinv": "tf.raw_ops.Erfinv(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.EuclideanNorm": "tf.raw_ops.EuclideanNorm(\n    input, axis, keep_dims=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.Exit": "tf.raw_ops.Exit(\n    data, name=None\n)\n"
  },
  {
    "tf.raw_ops.Exp": "tf.raw_ops.Exp(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExpandDims": "tf.raw_ops.ExpandDims(\n    input, axis, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalAssertNextDataset": "tf.raw_ops.ExperimentalAssertNextDataset(\n    input_dataset, transformations, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalAutoShardDataset": "tf.raw_ops.ExperimentalAutoShardDataset(\n    input_dataset,\n    num_workers,\n    index,\n    output_types,\n    output_shapes,\n    auto_shard_policy=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalBytesProducedStatsDataset": "tf.raw_ops.ExperimentalBytesProducedStatsDataset(\n    input_dataset, tag, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalCSVDataset": "tf.raw_ops.ExperimentalCSVDataset(\n    filenames,\n    compression_type,\n    buffer_size,\n    header,\n    field_delim,\n    use_quote_delim,\n    na_value,\n    select_cols,\n    record_defaults,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalChooseFastestDataset": "tf.raw_ops.ExperimentalChooseFastestDataset(\n    input_datasets, num_experiments, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalDatasetCardinality": "tf.raw_ops.ExperimentalDatasetCardinality(\n    input_dataset, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalDatasetToTFRecord": "tf.raw_ops.ExperimentalDatasetToTFRecord(\n    input_dataset, filename, compression_type, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalDenseToSparseBatchDataset": "tf.raw_ops.ExperimentalDenseToSparseBatchDataset(\n    input_dataset,\n    batch_size,\n    row_shape,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalDirectedInterleaveDataset": "tf.raw_ops.ExperimentalDirectedInterleaveDataset(\n    selector_input_dataset,\n    data_input_datasets,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalGroupByReducerDataset": "tf.raw_ops.ExperimentalGroupByReducerDataset(\n    input_dataset,\n    key_func_other_arguments,\n    init_func_other_arguments,\n    reduce_func_other_arguments,\n    finalize_func_other_arguments,\n    key_func,\n    init_func,\n    reduce_func,\n    finalize_func,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalGroupByWindowDataset": "tf.raw_ops.ExperimentalGroupByWindowDataset(\n    input_dataset,\n    key_func_other_arguments,\n    reduce_func_other_arguments,\n    window_size_func_other_arguments,\n    key_func,\n    reduce_func,\n    window_size_func,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalIgnoreErrorsDataset": "tf.raw_ops.ExperimentalIgnoreErrorsDataset(\n    input_dataset, output_types, output_shapes, log_warning=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalIteratorGetDevice": "tf.raw_ops.ExperimentalIteratorGetDevice(\n    resource, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalLMDBDataset": "tf.raw_ops.ExperimentalLMDBDataset(\n    filenames, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalLatencyStatsDataset": "tf.raw_ops.ExperimentalLatencyStatsDataset(\n    input_dataset, tag, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalMapAndBatchDataset": "tf.raw_ops.ExperimentalMapAndBatchDataset(\n    input_dataset,\n    other_arguments,\n    batch_size,\n    num_parallel_calls,\n    drop_remainder,\n    f,\n    output_types,\n    output_shapes,\n    preserve_cardinality=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalMapDataset": "tf.raw_ops.ExperimentalMapDataset(\n    input_dataset,\n    other_arguments,\n    f,\n    output_types,\n    output_shapes,\n    use_inter_op_parallelism=True,\n    preserve_cardinality=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalMatchingFilesDataset": "tf.raw_ops.ExperimentalMatchingFilesDataset(\n    patterns, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalMaxIntraOpParallelismDataset": "tf.raw_ops.ExperimentalMaxIntraOpParallelismDataset(\n    input_dataset,\n    max_intra_op_parallelism,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalNonSerializableDataset": "tf.raw_ops.ExperimentalNonSerializableDataset(\n    input_dataset, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalParallelInterleaveDataset": "tf.raw_ops.ExperimentalParallelInterleaveDataset(\n    input_dataset,\n    other_arguments,\n    cycle_length,\n    block_length,\n    sloppy,\n    buffer_output_elements,\n    prefetch_input_elements,\n    f,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalParseExampleDataset": "tf.raw_ops.ExperimentalParseExampleDataset(\n    input_dataset,\n    num_parallel_calls,\n    dense_defaults,\n    sparse_keys,\n    dense_keys,\n    sparse_types,\n    dense_shapes,\n    output_types,\n    output_shapes,\n    sloppy=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalPrivateThreadPoolDataset": "tf.raw_ops.ExperimentalPrivateThreadPoolDataset(\n    input_dataset, num_threads, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalRandomDataset": "tf.raw_ops.ExperimentalRandomDataset(\n    seed, seed2, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalRebatchDataset": "tf.raw_ops.ExperimentalRebatchDataset(\n    input_dataset,\n    num_replicas,\n    output_types,\n    output_shapes,\n    use_fallback=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalScanDataset": "tf.raw_ops.ExperimentalScanDataset(\n    input_dataset,\n    initial_state,\n    other_arguments,\n    f,\n    output_types,\n    output_shapes,\n    preserve_cardinality=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalSetStatsAggregatorDataset": "tf.raw_ops.ExperimentalSetStatsAggregatorDataset(\n    input_dataset,\n    stats_aggregator,\n    tag,\n    counter_prefix,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalSleepDataset": "tf.raw_ops.ExperimentalSleepDataset(\n    input_dataset, sleep_microseconds, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalSlidingWindowDataset": "tf.raw_ops.ExperimentalSlidingWindowDataset(\n    input_dataset,\n    window_size,\n    window_shift,\n    window_stride,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalSqlDataset": "tf.raw_ops.ExperimentalSqlDataset(\n    driver_name,\n    data_source_name,\n    query,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalStatsAggregatorHandle": "tf.raw_ops.ExperimentalStatsAggregatorHandle(\n    container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalStatsAggregatorSummary": "tf.raw_ops.ExperimentalStatsAggregatorSummary(\n    iterator, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalTakeWhileDataset": "tf.raw_ops.ExperimentalTakeWhileDataset(\n    input_dataset,\n    other_arguments,\n    predicate,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalThreadPoolDataset": "tf.raw_ops.ExperimentalThreadPoolDataset(\n    input_dataset, thread_pool, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalThreadPoolHandle": "tf.raw_ops.ExperimentalThreadPoolHandle(\n    num_threads,\n    display_name,\n    max_intra_op_parallelism=1,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalUnbatchDataset": "tf.raw_ops.ExperimentalUnbatchDataset(\n    input_dataset, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExperimentalUniqueDataset": "tf.raw_ops.ExperimentalUniqueDataset(\n    input_dataset, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.Expint": "tf.raw_ops.Expint(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Expm1": "tf.raw_ops.Expm1(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExtractGlimpse": "tf.raw_ops.ExtractGlimpse(\n    input,\n    size,\n    offsets,\n    centered=True,\n    normalized=True,\n    uniform_noise=True,\n    noise='uniform',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExtractGlimpseV2": "tf.raw_ops.ExtractGlimpseV2(\n    input,\n    size,\n    offsets,\n    centered=True,\n    normalized=True,\n    uniform_noise=True,\n    noise='uniform',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExtractImagePatches": "tf.raw_ops.ExtractImagePatches(\n    images, ksizes, strides, rates, padding, name=None\n)\n"
  },
  {
    "tf.raw_ops.ExtractJpegShape": "tf.raw_ops.ExtractJpegShape(\n    contents,\n    output_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ExtractVolumePatches": "tf.raw_ops.ExtractVolumePatches(\n    input, ksizes, strides, padding, name=None\n)\n"
  },
  {
    "tf.raw_ops.FFT": "tf.raw_ops.FFT(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.FFT2D": "tf.raw_ops.FFT2D(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.FFT3D": "tf.raw_ops.FFT3D(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.FIFOQueue": "tf.raw_ops.FIFOQueue(\n    component_types,\n    shapes=[],\n    capacity=-1,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FIFOQueueV2": "tf.raw_ops.FIFOQueueV2(\n    component_types,\n    shapes=[],\n    capacity=-1,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Fact": "tf.raw_ops.Fact(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FakeParam": "tf.raw_ops.FakeParam(\n    dtype, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.FakeQuantWithMinMaxArgs": "tf.raw_ops.FakeQuantWithMinMaxArgs(\n    inputs, min=-6, max=6, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.FakeQuantWithMinMaxArgsGradient": "tf.raw_ops.FakeQuantWithMinMaxArgsGradient(\n    gradients, inputs, min=-6, max=6, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.FakeQuantWithMinMaxVars": "tf.raw_ops.FakeQuantWithMinMaxVars(\n    inputs, min, max, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.FakeQuantWithMinMaxVarsGradient": "tf.raw_ops.FakeQuantWithMinMaxVarsGradient(\n    gradients, inputs, min, max, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel": "tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel(\n    inputs, min, max, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.FakeQuantWithMinMaxVarsPerChannelGradient": "tf.raw_ops.FakeQuantWithMinMaxVarsPerChannelGradient(\n    gradients, inputs, min, max, num_bits=8, narrow_range=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.FakeQueue": "tf.raw_ops.FakeQueue(\n    resource, name=None\n)\n"
  },
  {
    "tf.raw_ops.Fill": "tf.raw_ops.Fill(\n    dims, value, name=None\n)\n"
  },
  {
    "tf.raw_ops.FilterByLastComponentDataset": "tf.raw_ops.FilterByLastComponentDataset(\n    input_dataset, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.FilterDataset": "tf.raw_ops.FilterDataset(\n    input_dataset,\n    other_arguments,\n    predicate,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FinalizeDataset": "tf.raw_ops.FinalizeDataset(\n    input_dataset,\n    output_types,\n    output_shapes,\n    has_captured_ref=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Fingerprint": "tf.raw_ops.Fingerprint(\n    data, method, name=None\n)\n"
  },
  {
    "tf.raw_ops.FixedLengthRecordDataset": "tf.raw_ops.FixedLengthRecordDataset(\n    filenames,\n    header_bytes,\n    record_bytes,\n    footer_bytes,\n    buffer_size,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FixedLengthRecordDatasetV2": "tf.raw_ops.FixedLengthRecordDatasetV2(\n    filenames,\n    header_bytes,\n    record_bytes,\n    footer_bytes,\n    buffer_size,\n    compression_type,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FixedLengthRecordReader": "tf.raw_ops.FixedLengthRecordReader(\n    record_bytes,\n    header_bytes=0,\n    footer_bytes=0,\n    hop_bytes=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FixedLengthRecordReaderV2": "tf.raw_ops.FixedLengthRecordReaderV2(\n    record_bytes,\n    header_bytes=0,\n    footer_bytes=0,\n    hop_bytes=0,\n    container='',\n    shared_name='',\n    encoding='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FixedUnigramCandidateSampler": "tf.raw_ops.FixedUnigramCandidateSampler(\n    true_classes,\n    num_true,\n    num_sampled,\n    unique,\n    range_max,\n    vocab_file='',\n    distortion=1,\n    num_reserved_ids=0,\n    num_shards=1,\n    shard=0,\n    unigrams=[],\n    seed=0,\n    seed2=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FlatMapDataset": "tf.raw_ops.FlatMapDataset(\n    input_dataset,\n    other_arguments,\n    f,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Floor": "tf.raw_ops.Floor(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.FloorDiv": "tf.raw_ops.FloorDiv(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.FloorMod": "tf.raw_ops.FloorMod(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.FlushSummaryWriter": "tf.raw_ops.FlushSummaryWriter(\n    writer, name=None\n)\n"
  },
  {
    "tf.raw_ops.For": "tf.raw_ops.For(\n    start, limit, delta, input, body, name=None\n)\n"
  },
  {
    "tf.raw_ops.FractionalAvgPool": "tf.raw_ops.FractionalAvgPool(\n    value,\n    pooling_ratio,\n    pseudo_random=False,\n    overlapping=False,\n    deterministic=False,\n    seed=0,\n    seed2=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FractionalAvgPoolGrad": "tf.raw_ops.FractionalAvgPoolGrad(\n    orig_input_tensor_shape,\n    out_backprop,\n    row_pooling_sequence,\n    col_pooling_sequence,\n    overlapping=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FractionalMaxPool": "tf.raw_ops.FractionalMaxPool(\n    value,\n    pooling_ratio,\n    pseudo_random=False,\n    overlapping=False,\n    deterministic=False,\n    seed=0,\n    seed2=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FractionalMaxPoolGrad": "tf.raw_ops.FractionalMaxPoolGrad(\n    orig_input,\n    orig_output,\n    out_backprop,\n    row_pooling_sequence,\n    col_pooling_sequence,\n    overlapping=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FresnelCos": "tf.raw_ops.FresnelCos(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.FresnelSin": "tf.raw_ops.FresnelSin(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.FusedBatchNorm": "tf.raw_ops.FusedBatchNorm(\n    x,\n    scale,\n    offset,\n    mean,\n    variance,\n    epsilon=0.0001,\n    exponential_avg_factor=1,\n    data_format='NHWC',\n    is_training=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FusedBatchNormGrad": "tf.raw_ops.FusedBatchNormGrad(\n    y_backprop,\n    x,\n    scale,\n    reserve_space_1,\n    reserve_space_2,\n    epsilon=0.0001,\n    data_format='NHWC',\n    is_training=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FusedBatchNormGradV2": "tf.raw_ops.FusedBatchNormGradV2(\n    y_backprop,\n    x,\n    scale,\n    reserve_space_1,\n    reserve_space_2,\n    epsilon=0.0001,\n    data_format='NHWC',\n    is_training=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FusedBatchNormGradV3": "tf.raw_ops.FusedBatchNormGradV3(\n    y_backprop,\n    x,\n    scale,\n    reserve_space_1,\n    reserve_space_2,\n    reserve_space_3,\n    epsilon=0.0001,\n    data_format='NHWC',\n    is_training=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FusedBatchNormV2": "tf.raw_ops.FusedBatchNormV2(\n    x,\n    scale,\n    offset,\n    mean,\n    variance,\n    epsilon=0.0001,\n    exponential_avg_factor=1,\n    data_format='NHWC',\n    is_training=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FusedBatchNormV3": "tf.raw_ops.FusedBatchNormV3(\n    x,\n    scale,\n    offset,\n    mean,\n    variance,\n    epsilon=0.0001,\n    exponential_avg_factor=1,\n    data_format='NHWC',\n    is_training=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.FusedPadConv2D": "tf.raw_ops.FusedPadConv2D(\n    input, paddings, filter, mode, strides, padding, name=None\n)\n"
  },
  {
    "tf.raw_ops.FusedResizeAndPadConv2D": "tf.raw_ops.FusedResizeAndPadConv2D(\n    input,\n    size,\n    paddings,\n    filter,\n    mode,\n    strides,\n    padding,\n    resize_align_corners=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.GRUBlockCell": "tf.raw_ops.GRUBlockCell(\n    x, h_prev, w_ru, w_c, b_ru, b_c, name=None\n)\n"
  },
  {
    "tf.raw_ops.GRUBlockCellGrad": "tf.raw_ops.GRUBlockCellGrad(\n    x, h_prev, w_ru, w_c, b_ru, b_c, r, u, c, d_h, name=None\n)\n"
  },
  {
    "tf.raw_ops.Gather": "tf.raw_ops.Gather(\n    params, indices, validate_indices=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.GatherNd": "tf.raw_ops.GatherNd(\n    params, indices, name=None\n)\n"
  },
  {
    "tf.raw_ops.GatherV2": "tf.raw_ops.GatherV2(\n    params, indices, axis, batch_dims=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.GenerateBoundingBoxProposals": "tf.raw_ops.GenerateBoundingBoxProposals(\n    scores,\n    bbox_deltas,\n    image_info,\n    anchors,\n    nms_threshold,\n    pre_nms_topn,\n    min_size,\n    post_nms_topn=300,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.GenerateVocabRemapping": "tf.raw_ops.GenerateVocabRemapping(\n    new_vocab_file,\n    old_vocab_file,\n    new_vocab_offset,\n    num_new_vocab,\n    old_vocab_size=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.GeneratorDataset": "tf.raw_ops.GeneratorDataset(\n    init_func_other_args,\n    next_func_other_args,\n    finalize_func_other_args,\n    init_func,\n    next_func,\n    finalize_func,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.GetElementAtIndex": "tf.raw_ops.GetElementAtIndex(\n    dataset, index, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.GetOptions": "tf.raw_ops.GetOptions(\n    input_dataset, name=None\n)\n"
  },
  {
    "tf.raw_ops.GetSessionHandle": "tf.raw_ops.GetSessionHandle(\n    value, name=None\n)\n"
  },
  {
    "tf.raw_ops.GetSessionHandleV2": "tf.raw_ops.GetSessionHandleV2(\n    value, name=None\n)\n"
  },
  {
    "tf.raw_ops.GetSessionTensor": "tf.raw_ops.GetSessionTensor(\n    handle, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.Greater": "tf.raw_ops.Greater(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.GreaterEqual": "tf.raw_ops.GreaterEqual(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.GroupByReducerDataset": "tf.raw_ops.GroupByReducerDataset(\n    input_dataset,\n    key_func_other_arguments,\n    init_func_other_arguments,\n    reduce_func_other_arguments,\n    finalize_func_other_arguments,\n    key_func,\n    init_func,\n    reduce_func,\n    finalize_func,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.GroupByWindowDataset": "tf.raw_ops.GroupByWindowDataset(\n    input_dataset,\n    key_func_other_arguments,\n    reduce_func_other_arguments,\n    window_size_func_other_arguments,\n    key_func,\n    reduce_func,\n    window_size_func,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.GuaranteeConst": "tf.raw_ops.GuaranteeConst(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.HSVToRGB": "tf.raw_ops.HSVToRGB(\n    images, name=None\n)\n"
  },
  {
    "tf.raw_ops.HashTable": "tf.raw_ops.HashTable(\n    key_dtype,\n    value_dtype,\n    container='',\n    shared_name='',\n    use_node_name_sharing=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.HashTableV2": "tf.raw_ops.HashTableV2(\n    key_dtype,\n    value_dtype,\n    container='',\n    shared_name='',\n    use_node_name_sharing=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.HistogramFixedWidth": "tf.raw_ops.HistogramFixedWidth(\n    values,\n    value_range,\n    nbins,\n    dtype=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.HistogramSummary": "tf.raw_ops.HistogramSummary(\n    tag, values, name=None\n)\n"
  },
  {
    "tf.raw_ops.IFFT": "tf.raw_ops.IFFT(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.IFFT2D": "tf.raw_ops.IFFT2D(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.IFFT3D": "tf.raw_ops.IFFT3D(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.IRFFT": "tf.raw_ops.IRFFT(\n    input,\n    fft_length,\n    Treal=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.IRFFT2D": "tf.raw_ops.IRFFT2D(\n    input,\n    fft_length,\n    Treal=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.IRFFT3D": "tf.raw_ops.IRFFT3D(\n    input,\n    fft_length,\n    Treal=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Identity": "tf.raw_ops.Identity(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.IdentityN": "tf.raw_ops.IdentityN(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.IdentityReader": "tf.raw_ops.IdentityReader(\n    container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.IdentityReaderV2": "tf.raw_ops.IdentityReaderV2(\n    container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.If": "tf.raw_ops.If(\n    cond, input, Tout, then_branch, else_branch, output_shapes=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.Igamma": "tf.raw_ops.Igamma(\n    a, x, name=None\n)\n"
  },
  {
    "tf.raw_ops.IgammaGradA": "tf.raw_ops.IgammaGradA(\n    a, x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Igammac": "tf.raw_ops.Igammac(\n    a, x, name=None\n)\n"
  },
  {
    "tf.raw_ops.IgnoreErrorsDataset": "tf.raw_ops.IgnoreErrorsDataset(\n    input_dataset, output_types, output_shapes, log_warning=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.Imag": "tf.raw_ops.Imag(\n    input,\n    Tout=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ImageProjectiveTransformV2": "tf.raw_ops.ImageProjectiveTransformV2(\n    images,\n    transforms,\n    output_shape,\n    interpolation,\n    fill_mode='CONSTANT',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ImageProjectiveTransformV3": "tf.raw_ops.ImageProjectiveTransformV3(\n    images,\n    transforms,\n    output_shape,\n    fill_value,\n    interpolation,\n    fill_mode='CONSTANT',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ImageSummary": "tf.raw_ops.ImageSummary(\n    tag,\n    tensor,\n    max_images=3,\n    bad_color=_execute.make_tensor(\\n    'dtype: DT_UINT8 tensor_shape { dim { size: 4 } } int_val: 255 int_val: 0 int_val: 0 int_val: 255 '\\n    , 'bad_color'),\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ImmutableConst": "tf.raw_ops.ImmutableConst(\n    dtype, shape, memory_region_name, name=None\n)\n"
  },
  {
    "tf.raw_ops.ImportEvent": "tf.raw_ops.ImportEvent(\n    writer, event, name=None\n)\n"
  },
  {
    "tf.raw_ops.InTopK": "tf.raw_ops.InTopK(\n    predictions, targets, k, name=None\n)\n"
  },
  {
    "tf.raw_ops.InTopKV2": "tf.raw_ops.InTopKV2(\n    predictions, targets, k, name=None\n)\n"
  },
  {
    "tf.raw_ops.InfeedDequeue": "tf.raw_ops.InfeedDequeue(\n    dtype, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.InfeedDequeueTuple": "tf.raw_ops.InfeedDequeueTuple(\n    dtypes, shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.InfeedEnqueue": "tf.raw_ops.InfeedEnqueue(\n    input, shape=[], layout=[], device_ordinal=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.InfeedEnqueuePrelinearizedBuffer": "tf.raw_ops.InfeedEnqueuePrelinearizedBuffer(\n    input, device_ordinal=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.InfeedEnqueueTuple": "tf.raw_ops.InfeedEnqueueTuple(\n    inputs, shapes, layouts=[], device_ordinal=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.InitializeTable": "tf.raw_ops.InitializeTable(\n    table_handle, keys, values, name=None\n)\n"
  },
  {
    "tf.raw_ops.InitializeTableFromDataset": "tf.raw_ops.InitializeTableFromDataset(\n    table_handle, dataset, name=None\n)\n"
  },
  {
    "tf.raw_ops.InitializeTableFromTextFile": "tf.raw_ops.InitializeTableFromTextFile(\n    table_handle,\n    filename,\n    key_index,\n    value_index,\n    vocab_size=-1,\n    delimiter='\\t',\n    offset=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.InitializeTableFromTextFileV2": "tf.raw_ops.InitializeTableFromTextFileV2(\n    table_handle,\n    filename,\n    key_index,\n    value_index,\n    vocab_size=-1,\n    delimiter='\\t',\n    offset=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.InitializeTableV2": "tf.raw_ops.InitializeTableV2(\n    table_handle, keys, values, name=None\n)\n"
  },
  {
    "tf.raw_ops.InplaceAdd": "tf.raw_ops.InplaceAdd(\n    x, i, v, name=None\n)\n"
  },
  {
    "tf.raw_ops.InplaceSub": "tf.raw_ops.InplaceSub(\n    x, i, v, name=None\n)\n"
  },
  {
    "tf.raw_ops.InplaceUpdate": "tf.raw_ops.InplaceUpdate(\n    x, i, v, name=None\n)\n"
  },
  {
    "tf.raw_ops.InterleaveDataset": "tf.raw_ops.InterleaveDataset(\n    input_dataset,\n    other_arguments,\n    cycle_length,\n    block_length,\n    f,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Inv": "tf.raw_ops.Inv(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.InvGrad": "tf.raw_ops.InvGrad(\n    y, dy, name=None\n)\n"
  },
  {
    "tf.raw_ops.Invert": "tf.raw_ops.Invert(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.InvertPermutation": "tf.raw_ops.InvertPermutation(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.IsBoostedTreesEnsembleInitialized": "tf.raw_ops.IsBoostedTreesEnsembleInitialized(\n    tree_ensemble_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.IsBoostedTreesQuantileStreamResourceInitialized": "tf.raw_ops.IsBoostedTreesQuantileStreamResourceInitialized(\n    quantile_stream_resource_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.IsFinite": "tf.raw_ops.IsFinite(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.IsInf": "tf.raw_ops.IsInf(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.IsNan": "tf.raw_ops.IsNan(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.IsTPUEmbeddingInitialized": "tf.raw_ops.IsTPUEmbeddingInitialized(\n    config='', name=None\n)\n"
  },
  {
    "tf.raw_ops.IsVariableInitialized": "tf.raw_ops.IsVariableInitialized(\n    ref, name=None\n)\n"
  },
  {
    "tf.raw_ops.IsotonicRegression": "tf.raw_ops.IsotonicRegression(\n    input,\n    output_dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Iterator": "tf.raw_ops.Iterator(\n    shared_name, container, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.IteratorFromStringHandle": "tf.raw_ops.IteratorFromStringHandle(\n    string_handle, output_types=[], output_shapes=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.IteratorFromStringHandleV2": "tf.raw_ops.IteratorFromStringHandleV2(\n    string_handle, output_types=[], output_shapes=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.IteratorGetDevice": "tf.raw_ops.IteratorGetDevice(\n    resource, name=None\n)\n"
  },
  {
    "tf.raw_ops.IteratorGetNext": "tf.raw_ops.IteratorGetNext(\n    iterator, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.IteratorGetNextAsOptional": "tf.raw_ops.IteratorGetNextAsOptional(\n    iterator, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.IteratorGetNextSync": "tf.raw_ops.IteratorGetNextSync(\n    iterator, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.IteratorToStringHandle": "tf.raw_ops.IteratorToStringHandle(\n    resource_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.IteratorV2": "tf.raw_ops.IteratorV2(\n    shared_name, container, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.L2Loss": "tf.raw_ops.L2Loss(\n    t, name=None\n)\n"
  },
  {
    "tf.raw_ops.LMDBDataset": "tf.raw_ops.LMDBDataset(\n    filenames, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.LMDBReader": "tf.raw_ops.LMDBReader(\n    container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.LRN": "tf.raw_ops.LRN(\n    input, depth_radius=5, bias=1, alpha=1, beta=0.5, name=None\n)\n"
  },
  {
    "tf.raw_ops.LRNGrad": "tf.raw_ops.LRNGrad(\n    input_grads,\n    input_image,\n    output_image,\n    depth_radius=5,\n    bias=1,\n    alpha=1,\n    beta=0.5,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LSTMBlockCell": "tf.raw_ops.LSTMBlockCell(\n    x,\n    cs_prev,\n    h_prev,\n    w,\n    wci,\n    wcf,\n    wco,\n    b,\n    forget_bias=1,\n    cell_clip=3,\n    use_peephole=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LSTMBlockCellGrad": "tf.raw_ops.LSTMBlockCellGrad(\n    x,\n    cs_prev,\n    h_prev,\n    w,\n    wci,\n    wcf,\n    wco,\n    b,\n    i,\n    cs,\n    f,\n    o,\n    ci,\n    co,\n    cs_grad,\n    h_grad,\n    use_peephole,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LatencyStatsDataset": "tf.raw_ops.LatencyStatsDataset(\n    input_dataset, tag, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.LeakyRelu": "tf.raw_ops.LeakyRelu(\n    features, alpha=0.2, name=None\n)\n"
  },
  {
    "tf.raw_ops.LeakyReluGrad": "tf.raw_ops.LeakyReluGrad(\n    gradients, features, alpha=0.2, name=None\n)\n"
  },
  {
    "tf.raw_ops.LearnedUnigramCandidateSampler": "tf.raw_ops.LearnedUnigramCandidateSampler(\n    true_classes,\n    num_true,\n    num_sampled,\n    unique,\n    range_max,\n    seed=0,\n    seed2=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LeftShift": "tf.raw_ops.LeftShift(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.LegacyParallelInterleaveDatasetV2": "tf.raw_ops.LegacyParallelInterleaveDatasetV2(\n    input_dataset,\n    other_arguments,\n    cycle_length,\n    block_length,\n    buffer_output_elements,\n    prefetch_input_elements,\n    f,\n    output_types,\n    output_shapes,\n    deterministic='default',\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Less": "tf.raw_ops.Less(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.LessEqual": "tf.raw_ops.LessEqual(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.Lgamma": "tf.raw_ops.Lgamma(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.LinSpace": "tf.raw_ops.LinSpace(\n    start, stop, num, name=None\n)\n"
  },
  {
    "tf.raw_ops.ListDataset": "tf.raw_ops.ListDataset(\n    tensors, output_types, output_shapes, metadata='', name=None\n)\n"
  },
  {
    "tf.raw_ops.ListDiff": "tf.raw_ops.ListDiff(\n    x,\n    y,\n    out_idx=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadAndRemapMatrix": "tf.raw_ops.LoadAndRemapMatrix(\n    ckpt_path,\n    old_tensor_name,\n    row_remapping,\n    col_remapping,\n    initializing_values,\n    num_rows,\n    num_cols,\n    max_rows_in_memory=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadDataset": "tf.raw_ops.LoadDataset(\n    path,\n    reader_func_other_args,\n    output_types,\n    output_shapes,\n    reader_func,\n    compression='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingADAMParameters": "tf.raw_ops.LoadTPUEmbeddingADAMParameters(\n    parameters,\n    momenta,\n    velocities,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingAdadeltaParameters": "tf.raw_ops.LoadTPUEmbeddingAdadeltaParameters(\n    parameters,\n    accumulators,\n    updates,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingAdagradMomentumParameters": "tf.raw_ops.LoadTPUEmbeddingAdagradMomentumParameters(\n    parameters,\n    accumulators,\n    momenta,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingAdagradParameters": "tf.raw_ops.LoadTPUEmbeddingAdagradParameters(\n    parameters,\n    accumulators,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingCenteredRMSPropParameters": "tf.raw_ops.LoadTPUEmbeddingCenteredRMSPropParameters(\n    parameters,\n    ms,\n    mom,\n    mg,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingFTRLParameters": "tf.raw_ops.LoadTPUEmbeddingFTRLParameters(\n    parameters,\n    accumulators,\n    linears,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingFrequencyEstimatorParameters": "tf.raw_ops.LoadTPUEmbeddingFrequencyEstimatorParameters(\n    parameters,\n    last_hit_step,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingMDLAdagradLightParameters": "tf.raw_ops.LoadTPUEmbeddingMDLAdagradLightParameters(\n    parameters,\n    accumulators,\n    weights,\n    benefits,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingMomentumParameters": "tf.raw_ops.LoadTPUEmbeddingMomentumParameters(\n    parameters,\n    momenta,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingProximalAdagradParameters": "tf.raw_ops.LoadTPUEmbeddingProximalAdagradParameters(\n    parameters,\n    accumulators,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingProximalYogiParameters": "tf.raw_ops.LoadTPUEmbeddingProximalYogiParameters(\n    parameters,\n    v,\n    m,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingRMSPropParameters": "tf.raw_ops.LoadTPUEmbeddingRMSPropParameters(\n    parameters,\n    ms,\n    mom,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LoadTPUEmbeddingStochasticGradientDescentParameters": "tf.raw_ops.LoadTPUEmbeddingStochasticGradientDescentParameters(\n    parameters,\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Log": "tf.raw_ops.Log(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Log1p": "tf.raw_ops.Log1p(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.LogMatrixDeterminant": "tf.raw_ops.LogMatrixDeterminant(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.LogSoftmax": "tf.raw_ops.LogSoftmax(\n    logits, name=None\n)\n"
  },
  {
    "tf.raw_ops.LogUniformCandidateSampler": "tf.raw_ops.LogUniformCandidateSampler(\n    true_classes,\n    num_true,\n    num_sampled,\n    unique,\n    range_max,\n    seed=0,\n    seed2=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.LogicalAnd": "tf.raw_ops.LogicalAnd(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.LogicalNot": "tf.raw_ops.LogicalNot(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.LogicalOr": "tf.raw_ops.LogicalOr(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.LookupTableExport": "tf.raw_ops.LookupTableExport(\n    table_handle, Tkeys, Tvalues, name=None\n)\n"
  },
  {
    "tf.raw_ops.LookupTableExportV2": "tf.raw_ops.LookupTableExportV2(\n    table_handle, Tkeys, Tvalues, name=None\n)\n"
  },
  {
    "tf.raw_ops.LookupTableFind": "tf.raw_ops.LookupTableFind(\n    table_handle, keys, default_value, name=None\n)\n"
  },
  {
    "tf.raw_ops.LookupTableFindV2": "tf.raw_ops.LookupTableFindV2(\n    table_handle, keys, default_value, name=None\n)\n"
  },
  {
    "tf.raw_ops.LookupTableImport": "tf.raw_ops.LookupTableImport(\n    table_handle, keys, values, name=None\n)\n"
  },
  {
    "tf.raw_ops.LookupTableImportV2": "tf.raw_ops.LookupTableImportV2(\n    table_handle, keys, values, name=None\n)\n"
  },
  {
    "tf.raw_ops.LookupTableInsert": "tf.raw_ops.LookupTableInsert(\n    table_handle, keys, values, name=None\n)\n"
  },
  {
    "tf.raw_ops.LookupTableInsertV2": "tf.raw_ops.LookupTableInsertV2(\n    table_handle, keys, values, name=None\n)\n"
  },
  {
    "tf.raw_ops.LookupTableRemoveV2": "tf.raw_ops.LookupTableRemoveV2(\n    table_handle, keys, name=None\n)\n"
  },
  {
    "tf.raw_ops.LookupTableSize": "tf.raw_ops.LookupTableSize(\n    table_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.LookupTableSizeV2": "tf.raw_ops.LookupTableSizeV2(\n    table_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.LoopCond": "tf.raw_ops.LoopCond(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.LowerBound": "tf.raw_ops.LowerBound(\n    sorted_inputs,\n    values,\n    out_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Lu": "tf.raw_ops.Lu(\n    input,\n    output_idx_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MakeIterator": "tf.raw_ops.MakeIterator(\n    dataset, iterator, name=None\n)\n"
  },
  {
    "tf.raw_ops.MapAndBatchDataset": "tf.raw_ops.MapAndBatchDataset(\n    input_dataset,\n    other_arguments,\n    batch_size,\n    num_parallel_calls,\n    drop_remainder,\n    f,\n    output_types,\n    output_shapes,\n    preserve_cardinality=False,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MapClear": "tf.raw_ops.MapClear(\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MapDataset": "tf.raw_ops.MapDataset(\n    input_dataset,\n    other_arguments,\n    f,\n    output_types,\n    output_shapes,\n    use_inter_op_parallelism=True,\n    preserve_cardinality=False,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MapDefun": "tf.raw_ops.MapDefun(\n    arguments,\n    captured_inputs,\n    output_types,\n    output_shapes,\n    f,\n    max_intra_op_parallelism=1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MapIncompleteSize": "tf.raw_ops.MapIncompleteSize(\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MapPeek": "tf.raw_ops.MapPeek(\n    key,\n    indices,\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MapSize": "tf.raw_ops.MapSize(\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MapStage": "tf.raw_ops.MapStage(\n    key,\n    indices,\n    values,\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MapUnstage": "tf.raw_ops.MapUnstage(\n    key,\n    indices,\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MapUnstageNoKey": "tf.raw_ops.MapUnstageNoKey(\n    indices,\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MatMul": "tf.raw_ops.MatMul(\n    a, b, transpose_a=False, transpose_b=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatchingFiles": "tf.raw_ops.MatchingFiles(\n    pattern, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatchingFilesDataset": "tf.raw_ops.MatchingFilesDataset(\n    patterns, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixBandPart": "tf.raw_ops.MatrixBandPart(\n    input, num_lower, num_upper, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixDeterminant": "tf.raw_ops.MatrixDeterminant(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixDiag": "tf.raw_ops.MatrixDiag(\n    diagonal, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixDiagPart": "tf.raw_ops.MatrixDiagPart(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixDiagPartV2": "tf.raw_ops.MatrixDiagPartV2(\n    input, k, padding_value, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixDiagPartV3": "tf.raw_ops.MatrixDiagPartV3(\n    input, k, padding_value, align='RIGHT_LEFT', name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixDiagV2": "tf.raw_ops.MatrixDiagV2(\n    diagonal, k, num_rows, num_cols, padding_value, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixDiagV3": "tf.raw_ops.MatrixDiagV3(\n    diagonal,\n    k,\n    num_rows,\n    num_cols,\n    padding_value,\n    align='RIGHT_LEFT',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixExponential": "tf.raw_ops.MatrixExponential(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixInverse": "tf.raw_ops.MatrixInverse(\n    input, adjoint=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixLogarithm": "tf.raw_ops.MatrixLogarithm(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixSetDiag": "tf.raw_ops.MatrixSetDiag(\n    input, diagonal, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixSetDiagV2": "tf.raw_ops.MatrixSetDiagV2(\n    input, diagonal, k, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixSetDiagV3": "tf.raw_ops.MatrixSetDiagV3(\n    input, diagonal, k, align='RIGHT_LEFT', name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixSolve": "tf.raw_ops.MatrixSolve(\n    matrix, rhs, adjoint=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixSolveLs": "tf.raw_ops.MatrixSolveLs(\n    matrix, rhs, l2_regularizer, fast=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixSquareRoot": "tf.raw_ops.MatrixSquareRoot(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.MatrixTriangularSolve": "tf.raw_ops.MatrixTriangularSolve(\n    matrix, rhs, lower=True, adjoint=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.Max": "tf.raw_ops.Max(\n    input, axis, keep_dims=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxIntraOpParallelismDataset": "tf.raw_ops.MaxIntraOpParallelismDataset(\n    input_dataset,\n    max_intra_op_parallelism,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPool": "tf.raw_ops.MaxPool(\n    input,\n    ksize,\n    strides,\n    padding,\n    explicit_paddings=[],\n    data_format='NHWC',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPool3D": "tf.raw_ops.MaxPool3D(\n    input, ksize, strides, padding, data_format='NDHWC', name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPool3DGrad": "tf.raw_ops.MaxPool3DGrad(\n    orig_input,\n    orig_output,\n    grad,\n    ksize,\n    strides,\n    padding,\n    data_format='NDHWC',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPool3DGradGrad": "tf.raw_ops.MaxPool3DGradGrad(\n    orig_input,\n    orig_output,\n    grad,\n    ksize,\n    strides,\n    padding,\n    data_format='NDHWC',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPoolGrad": "tf.raw_ops.MaxPoolGrad(\n    orig_input,\n    orig_output,\n    grad,\n    ksize,\n    strides,\n    padding,\n    explicit_paddings=[],\n    data_format='NHWC',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPoolGradGrad": "tf.raw_ops.MaxPoolGradGrad(\n    orig_input,\n    orig_output,\n    grad,\n    ksize,\n    strides,\n    padding,\n    data_format='NHWC',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPoolGradGradV2": "tf.raw_ops.MaxPoolGradGradV2(\n    orig_input,\n    orig_output,\n    grad,\n    ksize,\n    strides,\n    padding,\n    data_format='NHWC',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPoolGradGradWithArgmax": "tf.raw_ops.MaxPoolGradGradWithArgmax(\n    input,\n    grad,\n    argmax,\n    ksize,\n    strides,\n    padding,\n    include_batch_in_index=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPoolGradV2": "tf.raw_ops.MaxPoolGradV2(\n    orig_input,\n    orig_output,\n    grad,\n    ksize,\n    strides,\n    padding,\n    data_format='NHWC',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPoolGradWithArgmax": "tf.raw_ops.MaxPoolGradWithArgmax(\n    input,\n    grad,\n    argmax,\n    ksize,\n    strides,\n    padding,\n    include_batch_in_index=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPoolV2": "tf.raw_ops.MaxPoolV2(\n    input, ksize, strides, padding, data_format='NHWC', name=None\n)\n"
  },
  {
    "tf.raw_ops.MaxPoolWithArgmax": "tf.raw_ops.MaxPoolWithArgmax(\n    input,\n    ksize,\n    strides,\n    padding,\n    Targmax=tf.dtypes.int64,\n    include_batch_in_index=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Maximum": "tf.raw_ops.Maximum(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.Mean": "tf.raw_ops.Mean(\n    input, axis, keep_dims=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.Merge": "tf.raw_ops.Merge(\n    inputs, name=None\n)\n"
  },
  {
    "tf.raw_ops.MergeSummary": "tf.raw_ops.MergeSummary(\n    inputs, name=None\n)\n"
  },
  {
    "tf.raw_ops.MergeV2Checkpoints": "tf.raw_ops.MergeV2Checkpoints(\n    checkpoint_prefixes,\n    destination_prefix,\n    delete_old_dirs=True,\n    allow_missing_files=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Mfcc": "tf.raw_ops.Mfcc(\n    spectrogram,\n    sample_rate,\n    upper_frequency_limit=4000,\n    lower_frequency_limit=20,\n    filterbank_channel_count=40,\n    dct_coefficient_count=13,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Min": "tf.raw_ops.Min(\n    input, axis, keep_dims=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.Minimum": "tf.raw_ops.Minimum(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.MirrorPad": "tf.raw_ops.MirrorPad(\n    input, paddings, mode, name=None\n)\n"
  },
  {
    "tf.raw_ops.MirrorPadGrad": "tf.raw_ops.MirrorPadGrad(\n    input, paddings, mode, name=None\n)\n"
  },
  {
    "tf.raw_ops.Mod": "tf.raw_ops.Mod(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.ModelDataset": "tf.raw_ops.ModelDataset(\n    input_dataset,\n    output_types,\n    output_shapes,\n    algorithm=0,\n    cpu_budget=0,\n    ram_budget=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Mul": "tf.raw_ops.Mul(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.MulNoNan": "tf.raw_ops.MulNoNan(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.MultiDeviceIterator": "tf.raw_ops.MultiDeviceIterator(\n    devices, shared_name, container, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.MultiDeviceIteratorFromStringHandle": "tf.raw_ops.MultiDeviceIteratorFromStringHandle(\n    string_handle, output_types=[], output_shapes=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.MultiDeviceIteratorGetNextFromShard": "tf.raw_ops.MultiDeviceIteratorGetNextFromShard(\n    multi_device_iterator,\n    shard_num,\n    incarnation_id,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MultiDeviceIteratorInit": "tf.raw_ops.MultiDeviceIteratorInit(\n    dataset, multi_device_iterator, max_buffer_size, name=None\n)\n"
  },
  {
    "tf.raw_ops.MultiDeviceIteratorToStringHandle": "tf.raw_ops.MultiDeviceIteratorToStringHandle(\n    multi_device_iterator, name=None\n)\n"
  },
  {
    "tf.raw_ops.Multinomial": "tf.raw_ops.Multinomial(\n    logits,\n    num_samples,\n    seed=0,\n    seed2=0,\n    output_dtype=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MutableDenseHashTable": "tf.raw_ops.MutableDenseHashTable(\n    empty_key,\n    value_dtype,\n    container='',\n    shared_name='',\n    use_node_name_sharing=False,\n    value_shape=[],\n    initial_num_buckets=131072,\n    max_load_factor=0.8,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MutableDenseHashTableV2": "tf.raw_ops.MutableDenseHashTableV2(\n    empty_key,\n    deleted_key,\n    value_dtype,\n    container='',\n    shared_name='',\n    use_node_name_sharing=False,\n    value_shape=[],\n    initial_num_buckets=131072,\n    max_load_factor=0.8,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MutableHashTable": "tf.raw_ops.MutableHashTable(\n    key_dtype,\n    value_dtype,\n    container='',\n    shared_name='',\n    use_node_name_sharing=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MutableHashTableOfTensors": "tf.raw_ops.MutableHashTableOfTensors(\n    key_dtype,\n    value_dtype,\n    container='',\n    shared_name='',\n    use_node_name_sharing=False,\n    value_shape=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MutableHashTableOfTensorsV2": "tf.raw_ops.MutableHashTableOfTensorsV2(\n    key_dtype,\n    value_dtype,\n    container='',\n    shared_name='',\n    use_node_name_sharing=False,\n    value_shape=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MutableHashTableV2": "tf.raw_ops.MutableHashTableV2(\n    key_dtype,\n    value_dtype,\n    container='',\n    shared_name='',\n    use_node_name_sharing=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.MutexLock": "tf.raw_ops.MutexLock(\n    mutex, name=None\n)\n"
  },
  {
    "tf.raw_ops.MutexV2": "tf.raw_ops.MutexV2(\n    container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.NcclAllReduce": "tf.raw_ops.NcclAllReduce(\n    input, reduction, num_devices, shared_name, name=None\n)\n"
  },
  {
    "tf.raw_ops.NcclBroadcast": "tf.raw_ops.NcclBroadcast(\n    input, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.NcclReduce": "tf.raw_ops.NcclReduce(\n    input, reduction, name=None\n)\n"
  },
  {
    "tf.raw_ops.Ndtri": "tf.raw_ops.Ndtri(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Neg": "tf.raw_ops.Neg(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.NextAfter": "tf.raw_ops.NextAfter(\n    x1, x2, name=None\n)\n"
  },
  {
    "tf.raw_ops.NextIteration": "tf.raw_ops.NextIteration(\n    data, name=None\n)\n"
  },
  {
    "tf.raw_ops.NoOp": "tf.raw_ops.NoOp(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.NonDeterministicInts": "tf.raw_ops.NonDeterministicInts(\n    shape,\n    dtype=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.NonMaxSuppression": "tf.raw_ops.NonMaxSuppression(\n    boxes, scores, max_output_size, iou_threshold=0.5, name=None\n)\n"
  },
  {
    "tf.raw_ops.NonMaxSuppressionV2": "tf.raw_ops.NonMaxSuppressionV2(\n    boxes, scores, max_output_size, iou_threshold, name=None\n)\n"
  },
  {
    "tf.raw_ops.NonMaxSuppressionV3": "tf.raw_ops.NonMaxSuppressionV3(\n    boxes, scores, max_output_size, iou_threshold, score_threshold, name=None\n)\n"
  },
  {
    "tf.raw_ops.NonMaxSuppressionV4": "tf.raw_ops.NonMaxSuppressionV4(\n    boxes,\n    scores,\n    max_output_size,\n    iou_threshold,\n    score_threshold,\n    pad_to_max_output_size=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.NonMaxSuppressionV5": "tf.raw_ops.NonMaxSuppressionV5(\n    boxes,\n    scores,\n    max_output_size,\n    iou_threshold,\n    score_threshold,\n    soft_nms_sigma,\n    pad_to_max_output_size=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.NonMaxSuppressionWithOverlaps": "tf.raw_ops.NonMaxSuppressionWithOverlaps(\n    overlaps,\n    scores,\n    max_output_size,\n    overlap_threshold,\n    score_threshold,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.NonSerializableDataset": "tf.raw_ops.NonSerializableDataset(\n    input_dataset, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.NotEqual": "tf.raw_ops.NotEqual(\n    x, y, incompatible_shape_error=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.NthElement": "tf.raw_ops.NthElement(\n    input, n, reverse=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.OneHot": "tf.raw_ops.OneHot(\n    indices, depth, on_value, off_value, axis=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.OneShotIterator": "tf.raw_ops.OneShotIterator(\n    dataset_factory,\n    output_types,\n    output_shapes,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OnesLike": "tf.raw_ops.OnesLike(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.OptimizeDataset": "tf.raw_ops.OptimizeDataset(\n    input_dataset,\n    optimizations,\n    output_types,\n    output_shapes,\n    optimization_configs=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OptimizeDatasetV2": "tf.raw_ops.OptimizeDatasetV2(\n    input_dataset,\n    optimizations_enabled,\n    optimizations_disabled,\n    optimizations_default,\n    output_types,\n    output_shapes,\n    optimization_configs=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OptionalFromValue": "tf.raw_ops.OptionalFromValue(\n    components, name=None\n)\n"
  },
  {
    "tf.raw_ops.OptionalGetValue": "tf.raw_ops.OptionalGetValue(\n    optional, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.OptionalHasValue": "tf.raw_ops.OptionalHasValue(\n    optional, name=None\n)\n"
  },
  {
    "tf.raw_ops.OptionalNone": "tf.raw_ops.OptionalNone(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OptionsDataset": "tf.raw_ops.OptionsDataset(\n    input_dataset,\n    serialized_options,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OrderedMapClear": "tf.raw_ops.OrderedMapClear(\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OrderedMapIncompleteSize": "tf.raw_ops.OrderedMapIncompleteSize(\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OrderedMapPeek": "tf.raw_ops.OrderedMapPeek(\n    key,\n    indices,\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OrderedMapSize": "tf.raw_ops.OrderedMapSize(\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OrderedMapStage": "tf.raw_ops.OrderedMapStage(\n    key,\n    indices,\n    values,\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OrderedMapUnstage": "tf.raw_ops.OrderedMapUnstage(\n    key,\n    indices,\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OrderedMapUnstageNoKey": "tf.raw_ops.OrderedMapUnstageNoKey(\n    indices,\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.OutfeedDequeue": "tf.raw_ops.OutfeedDequeue(\n    dtype, shape, device_ordinal=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.OutfeedDequeueTuple": "tf.raw_ops.OutfeedDequeueTuple(\n    dtypes, shapes, device_ordinal=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.OutfeedDequeueTupleV2": "tf.raw_ops.OutfeedDequeueTupleV2(\n    device_ordinal, dtypes, shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.OutfeedDequeueV2": "tf.raw_ops.OutfeedDequeueV2(\n    device_ordinal, dtype, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.OutfeedEnqueue": "tf.raw_ops.OutfeedEnqueue(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.OutfeedEnqueueTuple": "tf.raw_ops.OutfeedEnqueueTuple(\n    inputs, name=None\n)\n"
  },
  {
    "tf.raw_ops.Pack": "tf.raw_ops.Pack(\n    values, axis=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.Pad": "tf.raw_ops.Pad(\n    input, paddings, name=None\n)\n"
  },
  {
    "tf.raw_ops.PadV2": "tf.raw_ops.PadV2(\n    input, paddings, constant_values, name=None\n)\n"
  },
  {
    "tf.raw_ops.PaddedBatchDataset": "tf.raw_ops.PaddedBatchDataset(\n    input_dataset,\n    batch_size,\n    padded_shapes,\n    padding_values,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.PaddedBatchDatasetV2": "tf.raw_ops.PaddedBatchDatasetV2(\n    input_dataset,\n    batch_size,\n    padded_shapes,\n    padding_values,\n    drop_remainder,\n    output_shapes,\n    parallel_copy=False,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.PaddingFIFOQueue": "tf.raw_ops.PaddingFIFOQueue(\n    component_types,\n    shapes=[],\n    capacity=-1,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.PaddingFIFOQueueV2": "tf.raw_ops.PaddingFIFOQueueV2(\n    component_types,\n    shapes=[],\n    capacity=-1,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParallelBatchDataset": "tf.raw_ops.ParallelBatchDataset(\n    input_dataset,\n    batch_size,\n    num_parallel_calls,\n    drop_remainder,\n    output_types,\n    output_shapes,\n    parallel_copy=False,\n    deterministic='default',\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParallelConcat": "tf.raw_ops.ParallelConcat(\n    values, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.ParallelDynamicStitch": "tf.raw_ops.ParallelDynamicStitch(\n    indices, data, name=None\n)\n"
  },
  {
    "tf.raw_ops.ParallelFilterDataset": "tf.raw_ops.ParallelFilterDataset(\n    input_dataset,\n    other_arguments,\n    num_parallel_calls,\n    predicate,\n    output_types,\n    output_shapes,\n    deterministic='default',\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParallelInterleaveDataset": "tf.raw_ops.ParallelInterleaveDataset(\n    input_dataset,\n    other_arguments,\n    cycle_length,\n    block_length,\n    sloppy,\n    buffer_output_elements,\n    prefetch_input_elements,\n    f,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParallelInterleaveDatasetV2": "tf.raw_ops.ParallelInterleaveDatasetV2(\n    input_dataset,\n    other_arguments,\n    cycle_length,\n    block_length,\n    num_parallel_calls,\n    f,\n    output_types,\n    output_shapes,\n    sloppy=False,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParallelInterleaveDatasetV3": "tf.raw_ops.ParallelInterleaveDatasetV3(\n    input_dataset,\n    other_arguments,\n    cycle_length,\n    block_length,\n    num_parallel_calls,\n    f,\n    output_types,\n    output_shapes,\n    deterministic='default',\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParallelInterleaveDatasetV4": "tf.raw_ops.ParallelInterleaveDatasetV4(\n    input_dataset,\n    other_arguments,\n    cycle_length,\n    block_length,\n    buffer_output_elements,\n    prefetch_input_elements,\n    num_parallel_calls,\n    f,\n    output_types,\n    output_shapes,\n    deterministic='default',\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParallelMapDataset": "tf.raw_ops.ParallelMapDataset(\n    input_dataset,\n    other_arguments,\n    num_parallel_calls,\n    f,\n    output_types,\n    output_shapes,\n    use_inter_op_parallelism=True,\n    sloppy=False,\n    preserve_cardinality=False,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParallelMapDatasetV2": "tf.raw_ops.ParallelMapDatasetV2(\n    input_dataset,\n    other_arguments,\n    num_parallel_calls,\n    f,\n    output_types,\n    output_shapes,\n    use_inter_op_parallelism=True,\n    deterministic='default',\n    preserve_cardinality=False,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParameterizedTruncatedNormal": "tf.raw_ops.ParameterizedTruncatedNormal(\n    shape, means, stdevs, minvals, maxvals, seed=0, seed2=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.ParseExample": "tf.raw_ops.ParseExample(\n    serialized,\n    names,\n    sparse_keys,\n    dense_keys,\n    dense_defaults,\n    sparse_types,\n    dense_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParseExampleDataset": "tf.raw_ops.ParseExampleDataset(\n    input_dataset,\n    num_parallel_calls,\n    dense_defaults,\n    sparse_keys,\n    dense_keys,\n    sparse_types,\n    dense_shapes,\n    output_types,\n    output_shapes,\n    sloppy=False,\n    ragged_keys=[],\n    ragged_value_types=[],\n    ragged_split_types=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParseExampleDatasetV2": "tf.raw_ops.ParseExampleDatasetV2(\n    input_dataset,\n    num_parallel_calls,\n    dense_defaults,\n    sparse_keys,\n    dense_keys,\n    sparse_types,\n    dense_shapes,\n    output_types,\n    output_shapes,\n    deterministic='default',\n    ragged_keys=[],\n    ragged_value_types=[],\n    ragged_split_types=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParseExampleV2": "tf.raw_ops.ParseExampleV2(\n    serialized,\n    names,\n    sparse_keys,\n    dense_keys,\n    ragged_keys,\n    dense_defaults,\n    num_sparse,\n    sparse_types,\n    ragged_value_types,\n    ragged_split_types,\n    dense_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParseSequenceExample": "tf.raw_ops.ParseSequenceExample(\n    serialized,\n    debug_name,\n    context_dense_defaults,\n    feature_list_dense_missing_assumed_empty,\n    context_sparse_keys,\n    context_dense_keys,\n    feature_list_sparse_keys,\n    feature_list_dense_keys,\n    Ncontext_sparse=0,\n    Ncontext_dense=0,\n    Nfeature_list_sparse=0,\n    Nfeature_list_dense=0,\n    context_sparse_types=[],\n    feature_list_dense_types=[],\n    context_dense_shapes=[],\n    feature_list_sparse_types=[],\n    feature_list_dense_shapes=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParseSequenceExampleV2": "tf.raw_ops.ParseSequenceExampleV2(\n    serialized,\n    debug_name,\n    context_sparse_keys,\n    context_dense_keys,\n    context_ragged_keys,\n    feature_list_sparse_keys,\n    feature_list_dense_keys,\n    feature_list_ragged_keys,\n    feature_list_dense_missing_assumed_empty,\n    context_dense_defaults,\n    Ncontext_sparse=0,\n    context_sparse_types=[],\n    context_ragged_value_types=[],\n    context_ragged_split_types=[],\n    context_dense_shapes=[],\n    Nfeature_list_sparse=0,\n    Nfeature_list_dense=0,\n    feature_list_dense_types=[],\n    feature_list_sparse_types=[],\n    feature_list_ragged_value_types=[],\n    feature_list_ragged_split_types=[],\n    feature_list_dense_shapes=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParseSingleExample": "tf.raw_ops.ParseSingleExample(\n    serialized,\n    dense_defaults,\n    num_sparse,\n    sparse_keys,\n    dense_keys,\n    sparse_types,\n    dense_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParseSingleSequenceExample": "tf.raw_ops.ParseSingleSequenceExample(\n    serialized,\n    feature_list_dense_missing_assumed_empty,\n    context_sparse_keys,\n    context_dense_keys,\n    feature_list_sparse_keys,\n    feature_list_dense_keys,\n    context_dense_defaults,\n    debug_name,\n    context_sparse_types=[],\n    feature_list_dense_types=[],\n    context_dense_shapes=[],\n    feature_list_sparse_types=[],\n    feature_list_dense_shapes=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ParseTensor": "tf.raw_ops.ParseTensor(\n    serialized, out_type, name=None\n)\n"
  },
  {
    "tf.raw_ops.PartitionedCall": "tf.raw_ops.PartitionedCall(\n    args,\n    Tout,\n    f,\n    config='',\n    config_proto='',\n    executor_type='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Placeholder": "tf.raw_ops.Placeholder(\n    dtype, shape=None, name=None\n)\n"
  },
  {
    "tf.raw_ops.PlaceholderV2": "tf.raw_ops.PlaceholderV2(\n    dtype, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.PlaceholderWithDefault": "tf.raw_ops.PlaceholderWithDefault(\n    input, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.Polygamma": "tf.raw_ops.Polygamma(\n    a, x, name=None\n)\n"
  },
  {
    "tf.raw_ops.PopulationCount": "tf.raw_ops.PopulationCount(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Pow": "tf.raw_ops.Pow(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.PrefetchDataset": "tf.raw_ops.PrefetchDataset(\n    input_dataset,\n    buffer_size,\n    output_types,\n    output_shapes,\n    slack_period=0,\n    legacy_autotune=True,\n    buffer_size_min=0,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Prelinearize": "tf.raw_ops.Prelinearize(\n    input, shape=[], layout=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.PrelinearizeTuple": "tf.raw_ops.PrelinearizeTuple(\n    inputs, shapes, layouts=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.PreventGradient": "tf.raw_ops.PreventGradient(\n    input, message='', name=None\n)\n"
  },
  {
    "tf.raw_ops.Print": "tf.raw_ops.Print(\n    input, data, message='', first_n=-1, summarize=3, name=None\n)\n"
  },
  {
    "tf.raw_ops.PrintV2": "tf.raw_ops.PrintV2(\n    input, output_stream='stderr', end='\\n', name=None\n)\n"
  },
  {
    "tf.raw_ops.PriorityQueue": "tf.raw_ops.PriorityQueue(\n    shapes,\n    component_types=[],\n    capacity=-1,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.PriorityQueueV2": "tf.raw_ops.PriorityQueueV2(\n    shapes,\n    component_types=[],\n    capacity=-1,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.PrivateThreadPoolDataset": "tf.raw_ops.PrivateThreadPoolDataset(\n    input_dataset, num_threads, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.Prod": "tf.raw_ops.Prod(\n    input, axis, keep_dims=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.PyFunc": "tf.raw_ops.PyFunc(\n    input, token, Tout, name=None\n)\n"
  },
  {
    "tf.raw_ops.PyFuncStateless": "tf.raw_ops.PyFuncStateless(\n    input, token, Tout, name=None\n)\n"
  },
  {
    "tf.raw_ops.Qr": "tf.raw_ops.Qr(\n    input, full_matrices=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizeAndDequantize": "tf.raw_ops.QuantizeAndDequantize(\n    input,\n    signed_input=True,\n    num_bits=8,\n    range_given=False,\n    input_min=0,\n    input_max=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizeAndDequantizeV2": "tf.raw_ops.QuantizeAndDequantizeV2(\n    input,\n    input_min,\n    input_max,\n    signed_input=True,\n    num_bits=8,\n    range_given=False,\n    round_mode='HALF_TO_EVEN',\n    narrow_range=False,\n    axis=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizeAndDequantizeV3": "tf.raw_ops.QuantizeAndDequantizeV3(\n    input,\n    input_min,\n    input_max,\n    num_bits,\n    signed_input=True,\n    range_given=True,\n    narrow_range=False,\n    axis=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizeAndDequantizeV4": "tf.raw_ops.QuantizeAndDequantizeV4(\n    input,\n    input_min,\n    input_max,\n    signed_input=True,\n    num_bits=8,\n    range_given=False,\n    round_mode='HALF_TO_EVEN',\n    narrow_range=False,\n    axis=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizeAndDequantizeV4Grad": "tf.raw_ops.QuantizeAndDequantizeV4Grad(\n    gradients, input, input_min, input_max, axis=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizeDownAndShrinkRange": "tf.raw_ops.QuantizeDownAndShrinkRange(\n    input, input_min, input_max, out_type, name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizeV2": "tf.raw_ops.QuantizeV2(\n    input,\n    min_range,\n    max_range,\n    T,\n    mode='MIN_COMBINED',\n    round_mode='HALF_AWAY_FROM_ZERO',\n    narrow_range=False,\n    axis=-1,\n    ensure_minimum_range=0.01,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedAdd": "tf.raw_ops.QuantizedAdd(\n    x,\n    y,\n    min_x,\n    max_x,\n    min_y,\n    max_y,\n    Toutput=tf.dtypes.qint32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedAvgPool": "tf.raw_ops.QuantizedAvgPool(\n    input, min_input, max_input, ksize, strides, padding, name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedBatchNormWithGlobalNormalization": "tf.raw_ops.QuantizedBatchNormWithGlobalNormalization(\n    t,\n    t_min,\n    t_max,\n    m,\n    m_min,\n    m_max,\n    v,\n    v_min,\n    v_max,\n    beta,\n    beta_min,\n    beta_max,\n    gamma,\n    gamma_min,\n    gamma_max,\n    out_type,\n    variance_epsilon,\n    scale_after_normalization,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedBiasAdd": "tf.raw_ops.QuantizedBiasAdd(\n    input, bias, min_input, max_input, min_bias, max_bias, out_type, name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConcat": "tf.raw_ops.QuantizedConcat(\n    concat_dim, values, input_mins, input_maxes, name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2D": "tf.raw_ops.QuantizedConv2D(\n    input,\n    filter,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    strides,\n    padding,\n    out_type=tf.dtypes.qint32,\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2DAndRelu": "tf.raw_ops.QuantizedConv2DAndRelu(\n    input,\n    filter,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    strides,\n    padding,\n    out_type=tf.dtypes.qint32,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2DAndReluAndRequantize": "tf.raw_ops.QuantizedConv2DAndReluAndRequantize(\n    input,\n    filter,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    min_freezed_output,\n    max_freezed_output,\n    strides,\n    padding,\n    out_type=tf.dtypes.quint8,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2DAndRequantize": "tf.raw_ops.QuantizedConv2DAndRequantize(\n    input,\n    filter,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    min_freezed_output,\n    max_freezed_output,\n    strides,\n    padding,\n    out_type=tf.dtypes.qint8,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2DPerChannel": "tf.raw_ops.QuantizedConv2DPerChannel(\n    input,\n    filter,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    strides,\n    padding,\n    out_type=tf.dtypes.qint32,\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2DWithBias": "tf.raw_ops.QuantizedConv2DWithBias(\n    input,\n    filter,\n    bias,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    strides,\n    padding,\n    out_type=tf.dtypes.qint32,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2DWithBiasAndRelu": "tf.raw_ops.QuantizedConv2DWithBiasAndRelu(\n    input,\n    filter,\n    bias,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    strides,\n    padding,\n    out_type=tf.dtypes.qint32,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2DWithBiasAndReluAndRequantize": "tf.raw_ops.QuantizedConv2DWithBiasAndReluAndRequantize(\n    input,\n    filter,\n    bias,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    min_freezed_output,\n    max_freezed_output,\n    strides,\n    padding,\n    out_type=tf.dtypes.quint8,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2DWithBiasAndRequantize": "tf.raw_ops.QuantizedConv2DWithBiasAndRequantize(\n    input,\n    filter,\n    bias,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    min_freezed_output,\n    max_freezed_output,\n    strides,\n    padding,\n    out_type=tf.dtypes.qint8,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2DWithBiasSignedSumAndReluAndRequantize": "tf.raw_ops.QuantizedConv2DWithBiasSignedSumAndReluAndRequantize(\n    input,\n    filter,\n    bias,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    min_freezed_output,\n    max_freezed_output,\n    summand,\n    min_summand,\n    max_summand,\n    strides,\n    padding,\n    out_type=tf.dtypes.quint8,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2DWithBiasSumAndRelu": "tf.raw_ops.QuantizedConv2DWithBiasSumAndRelu(\n    input,\n    filter,\n    bias,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    summand,\n    strides,\n    padding,\n    out_type=tf.dtypes.qint32,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedConv2DWithBiasSumAndReluAndRequantize": "tf.raw_ops.QuantizedConv2DWithBiasSumAndReluAndRequantize(\n    input,\n    filter,\n    bias,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    min_freezed_output,\n    max_freezed_output,\n    summand,\n    min_summand,\n    max_summand,\n    strides,\n    padding,\n    out_type=tf.dtypes.quint8,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedDepthwiseConv2D": "tf.raw_ops.QuantizedDepthwiseConv2D(\n    input,\n    filter,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    strides,\n    padding,\n    out_type=tf.dtypes.qint32,\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedDepthwiseConv2DWithBias": "tf.raw_ops.QuantizedDepthwiseConv2DWithBias(\n    input,\n    filter,\n    bias,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    strides,\n    padding,\n    out_type=tf.dtypes.qint32,\n    dilations=[1, 1, 1, 1],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedDepthwiseConv2DWithBiasAndRelu": "tf.raw_ops.QuantizedDepthwiseConv2DWithBiasAndRelu(\n    input,\n    filter,\n    bias,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    strides,\n    padding,\n    out_type=tf.dtypes.qint32,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedDepthwiseConv2DWithBiasAndReluAndRequantize": "tf.raw_ops.QuantizedDepthwiseConv2DWithBiasAndReluAndRequantize(\n    input,\n    filter,\n    bias,\n    min_input,\n    max_input,\n    min_filter,\n    max_filter,\n    min_freezed_output,\n    max_freezed_output,\n    strides,\n    padding,\n    out_type=tf.dtypes.quint8,\n    dilations=[1, 1, 1, 1],\n    padding_list=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedInstanceNorm": "tf.raw_ops.QuantizedInstanceNorm(\n    x,\n    x_min,\n    x_max,\n    output_range_given=False,\n    given_y_min=0,\n    given_y_max=0,\n    variance_epsilon=1e-05,\n    min_separation=0.001,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedMatMul": "tf.raw_ops.QuantizedMatMul(\n    a,\n    b,\n    min_a,\n    max_a,\n    min_b,\n    max_b,\n    Toutput=tf.dtypes.qint32,\n    transpose_a=False,\n    transpose_b=False,\n    Tactivation=tf.dtypes.quint8,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedMatMulWithBias": "tf.raw_ops.QuantizedMatMulWithBias(\n    a,\n    b,\n    bias,\n    min_a,\n    max_a,\n    min_b,\n    max_b,\n    Toutput=tf.dtypes.qint32,\n    transpose_a=False,\n    transpose_b=False,\n    input_quant_mode='MIN_FIRST',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedMatMulWithBiasAndDequantize": "tf.raw_ops.QuantizedMatMulWithBiasAndDequantize(\n    a,\n    b,\n    bias,\n    min_a,\n    max_a,\n    min_b,\n    max_b,\n    min_freezed_output,\n    max_freezed_output,\n    Toutput,\n    transpose_a=False,\n    transpose_b=False,\n    input_quant_mode='MIN_FIRST',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedMatMulWithBiasAndRelu": "tf.raw_ops.QuantizedMatMulWithBiasAndRelu(\n    a,\n    b,\n    bias,\n    min_a,\n    max_a,\n    min_b,\n    max_b,\n    Toutput=tf.dtypes.qint32,\n    transpose_a=False,\n    transpose_b=False,\n    input_quant_mode='MIN_FIRST',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedMatMulWithBiasAndReluAndRequantize": "tf.raw_ops.QuantizedMatMulWithBiasAndReluAndRequantize(\n    a,\n    b,\n    bias,\n    min_a,\n    max_a,\n    min_b,\n    max_b,\n    min_freezed_output,\n    max_freezed_output,\n    Toutput=tf.dtypes.quint8,\n    transpose_a=False,\n    transpose_b=False,\n    input_quant_mode='MIN_FIRST',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedMatMulWithBiasAndRequantize": "tf.raw_ops.QuantizedMatMulWithBiasAndRequantize(\n    a,\n    b,\n    bias,\n    min_a,\n    max_a,\n    min_b,\n    max_b,\n    min_freezed_output,\n    max_freezed_output,\n    Toutput=tf.dtypes.quint8,\n    transpose_a=False,\n    transpose_b=False,\n    input_quant_mode='MIN_FIRST',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedMaxPool": "tf.raw_ops.QuantizedMaxPool(\n    input, min_input, max_input, ksize, strides, padding, name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedMul": "tf.raw_ops.QuantizedMul(\n    x,\n    y,\n    min_x,\n    max_x,\n    min_y,\n    max_y,\n    Toutput=tf.dtypes.qint32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedRelu": "tf.raw_ops.QuantizedRelu(\n    features,\n    min_features,\n    max_features,\n    out_type=tf.dtypes.quint8,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedRelu6": "tf.raw_ops.QuantizedRelu6(\n    features,\n    min_features,\n    max_features,\n    out_type=tf.dtypes.quint8,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedReluX": "tf.raw_ops.QuantizedReluX(\n    features,\n    max_value,\n    min_features,\n    max_features,\n    out_type=tf.dtypes.quint8,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedReshape": "tf.raw_ops.QuantizedReshape(\n    tensor, shape, input_min, input_max, name=None\n)\n"
  },
  {
    "tf.raw_ops.QuantizedResizeBilinear": "tf.raw_ops.QuantizedResizeBilinear(\n    images,\n    size,\n    min,\n    max,\n    align_corners=False,\n    half_pixel_centers=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueClose": "tf.raw_ops.QueueClose(\n    handle, cancel_pending_enqueues=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueCloseV2": "tf.raw_ops.QueueCloseV2(\n    handle, cancel_pending_enqueues=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueDequeue": "tf.raw_ops.QueueDequeue(\n    handle, component_types, timeout_ms=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueDequeueMany": "tf.raw_ops.QueueDequeueMany(\n    handle, n, component_types, timeout_ms=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueDequeueManyV2": "tf.raw_ops.QueueDequeueManyV2(\n    handle, n, component_types, timeout_ms=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueDequeueUpTo": "tf.raw_ops.QueueDequeueUpTo(\n    handle, n, component_types, timeout_ms=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueDequeueUpToV2": "tf.raw_ops.QueueDequeueUpToV2(\n    handle, n, component_types, timeout_ms=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueDequeueV2": "tf.raw_ops.QueueDequeueV2(\n    handle, component_types, timeout_ms=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueEnqueue": "tf.raw_ops.QueueEnqueue(\n    handle, components, timeout_ms=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueEnqueueMany": "tf.raw_ops.QueueEnqueueMany(\n    handle, components, timeout_ms=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueEnqueueManyV2": "tf.raw_ops.QueueEnqueueManyV2(\n    handle, components, timeout_ms=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueEnqueueV2": "tf.raw_ops.QueueEnqueueV2(\n    handle, components, timeout_ms=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueIsClosed": "tf.raw_ops.QueueIsClosed(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueIsClosedV2": "tf.raw_ops.QueueIsClosedV2(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueSize": "tf.raw_ops.QueueSize(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.QueueSizeV2": "tf.raw_ops.QueueSizeV2(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.RFFT": "tf.raw_ops.RFFT(\n    input,\n    fft_length,\n    Tcomplex=tf.dtypes.complex64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RFFT2D": "tf.raw_ops.RFFT2D(\n    input,\n    fft_length,\n    Tcomplex=tf.dtypes.complex64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RFFT3D": "tf.raw_ops.RFFT3D(\n    input,\n    fft_length,\n    Tcomplex=tf.dtypes.complex64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RGBToHSV": "tf.raw_ops.RGBToHSV(\n    images, name=None\n)\n"
  },
  {
    "tf.raw_ops.RaggedBincount": "tf.raw_ops.RaggedBincount(\n    splits, values, size, weights, binary_output=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.RaggedCountSparseOutput": "tf.raw_ops.RaggedCountSparseOutput(\n    splits,\n    values,\n    weights,\n    binary_output,\n    minlength=-1,\n    maxlength=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RaggedCross": "tf.raw_ops.RaggedCross(\n    ragged_values,\n    ragged_row_splits,\n    sparse_indices,\n    sparse_values,\n    sparse_shape,\n    dense_inputs,\n    input_order,\n    hashed_output,\n    num_buckets,\n    hash_key,\n    out_values_type,\n    out_row_splits_type,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RaggedGather": "tf.raw_ops.RaggedGather(\n    params_nested_splits,\n    params_dense_values,\n    indices,\n    OUTPUT_RAGGED_RANK,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RaggedRange": "tf.raw_ops.RaggedRange(\n    starts,\n    limits,\n    deltas,\n    Tsplits=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RaggedTensorFromVariant": "tf.raw_ops.RaggedTensorFromVariant(\n    encoded_ragged,\n    input_ragged_rank,\n    output_ragged_rank,\n    Tvalues,\n    Tsplits=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RaggedTensorToSparse": "tf.raw_ops.RaggedTensorToSparse(\n    rt_nested_splits, rt_dense_values, name=None\n)\n"
  },
  {
    "tf.raw_ops.RaggedTensorToTensor": "tf.raw_ops.RaggedTensorToTensor(\n    shape,\n    values,\n    default_value,\n    row_partition_tensors,\n    row_partition_types,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RaggedTensorToVariant": "tf.raw_ops.RaggedTensorToVariant(\n    rt_nested_splits, rt_dense_values, batched_input, name=None\n)\n"
  },
  {
    "tf.raw_ops.RaggedTensorToVariantGradient": "tf.raw_ops.RaggedTensorToVariantGradient(\n    encoded_ragged_grad, row_splits, dense_values_shape, Tvalues, name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomCrop": "tf.raw_ops.RandomCrop(\n    image, size, seed=0, seed2=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomDataset": "tf.raw_ops.RandomDataset(\n    seed, seed2, output_types, output_shapes, metadata='', name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomGamma": "tf.raw_ops.RandomGamma(\n    shape, alpha, seed=0, seed2=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomGammaGrad": "tf.raw_ops.RandomGammaGrad(\n    alpha, sample, name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomIndexShuffle": "tf.raw_ops.RandomIndexShuffle(\n    index, seed, max_index, rounds=4, name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomPoisson": "tf.raw_ops.RandomPoisson(\n    shape, rate, seed=0, seed2=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomPoissonV2": "tf.raw_ops.RandomPoissonV2(\n    shape,\n    rate,\n    seed=0,\n    seed2=0,\n    dtype=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomShuffle": "tf.raw_ops.RandomShuffle(\n    value, seed=0, seed2=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomShuffleQueue": "tf.raw_ops.RandomShuffleQueue(\n    component_types,\n    shapes=[],\n    capacity=-1,\n    min_after_dequeue=0,\n    seed=0,\n    seed2=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomShuffleQueueV2": "tf.raw_ops.RandomShuffleQueueV2(\n    component_types,\n    shapes=[],\n    capacity=-1,\n    min_after_dequeue=0,\n    seed=0,\n    seed2=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomStandardNormal": "tf.raw_ops.RandomStandardNormal(\n    shape, dtype, seed=0, seed2=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomUniform": "tf.raw_ops.RandomUniform(\n    shape, dtype, seed=0, seed2=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.RandomUniformInt": "tf.raw_ops.RandomUniformInt(\n    shape, minval, maxval, seed=0, seed2=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.Range": "tf.raw_ops.Range(\n    start, limit, delta, name=None\n)\n"
  },
  {
    "tf.raw_ops.RangeDataset": "tf.raw_ops.RangeDataset(\n    start,\n    stop,\n    step,\n    output_types,\n    output_shapes,\n    metadata='',\n    replicate_on_split=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Rank": "tf.raw_ops.Rank(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReadFile": "tf.raw_ops.ReadFile(\n    filename, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReadVariableOp": "tf.raw_ops.ReadVariableOp(\n    resource, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReadVariableXlaSplitND": "tf.raw_ops.ReadVariableXlaSplitND(\n    resource, T, N, num_splits, paddings=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderNumRecordsProduced": "tf.raw_ops.ReaderNumRecordsProduced(\n    reader_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderNumRecordsProducedV2": "tf.raw_ops.ReaderNumRecordsProducedV2(\n    reader_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderNumWorkUnitsCompleted": "tf.raw_ops.ReaderNumWorkUnitsCompleted(\n    reader_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderNumWorkUnitsCompletedV2": "tf.raw_ops.ReaderNumWorkUnitsCompletedV2(\n    reader_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderRead": "tf.raw_ops.ReaderRead(\n    reader_handle, queue_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderReadUpTo": "tf.raw_ops.ReaderReadUpTo(\n    reader_handle, queue_handle, num_records, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderReadUpToV2": "tf.raw_ops.ReaderReadUpToV2(\n    reader_handle, queue_handle, num_records, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderReadV2": "tf.raw_ops.ReaderReadV2(\n    reader_handle, queue_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderReset": "tf.raw_ops.ReaderReset(\n    reader_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderResetV2": "tf.raw_ops.ReaderResetV2(\n    reader_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderRestoreState": "tf.raw_ops.ReaderRestoreState(\n    reader_handle, state, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderRestoreStateV2": "tf.raw_ops.ReaderRestoreStateV2(\n    reader_handle, state, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderSerializeState": "tf.raw_ops.ReaderSerializeState(\n    reader_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReaderSerializeStateV2": "tf.raw_ops.ReaderSerializeStateV2(\n    reader_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.Real": "tf.raw_ops.Real(\n    input,\n    Tout=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RealDiv": "tf.raw_ops.RealDiv(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.RebatchDataset": "tf.raw_ops.RebatchDataset(\n    input_dataset,\n    num_replicas,\n    output_types,\n    output_shapes,\n    use_fallback=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RebatchDatasetV2": "tf.raw_ops.RebatchDatasetV2(\n    input_dataset,\n    batch_sizes,\n    drop_remainder,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Reciprocal": "tf.raw_ops.Reciprocal(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReciprocalGrad": "tf.raw_ops.ReciprocalGrad(\n    y, dy, name=None\n)\n"
  },
  {
    "tf.raw_ops.RecordInput": "tf.raw_ops.RecordInput(\n    file_pattern,\n    file_random_seed=301,\n    file_shuffle_shift_ratio=0,\n    file_buffer_size=10000,\n    file_parallelism=16,\n    batch_size=32,\n    compression_type='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Recv": "tf.raw_ops.Recv(\n    tensor_type,\n    tensor_name,\n    send_device,\n    send_device_incarnation,\n    recv_device,\n    client_terminated=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RecvTPUEmbeddingActivations": "tf.raw_ops.RecvTPUEmbeddingActivations(\n    num_outputs, config, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReduceDataset": "tf.raw_ops.ReduceDataset(\n    input_dataset,\n    initial_state,\n    other_arguments,\n    f,\n    output_types,\n    output_shapes,\n    use_inter_op_parallelism=True,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ReduceJoin": "tf.raw_ops.ReduceJoin(\n    inputs,\n    reduction_indices,\n    keep_dims=False,\n    separator='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RefEnter": "tf.raw_ops.RefEnter(\n    data, frame_name, is_constant=False, parallel_iterations=10, name=None\n)\n"
  },
  {
    "tf.raw_ops.RefExit": "tf.raw_ops.RefExit(\n    data, name=None\n)\n"
  },
  {
    "tf.raw_ops.RefIdentity": "tf.raw_ops.RefIdentity(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.RefMerge": "tf.raw_ops.RefMerge(\n    inputs, name=None\n)\n"
  },
  {
    "tf.raw_ops.RefNextIteration": "tf.raw_ops.RefNextIteration(\n    data, name=None\n)\n"
  },
  {
    "tf.raw_ops.RefSelect": "tf.raw_ops.RefSelect(\n    index, inputs, name=None\n)\n"
  },
  {
    "tf.raw_ops.RefSwitch": "tf.raw_ops.RefSwitch(\n    data, pred, name=None\n)\n"
  },
  {
    "tf.raw_ops.RegexFullMatch": "tf.raw_ops.RegexFullMatch(\n    input, pattern, name=None\n)\n"
  },
  {
    "tf.raw_ops.RegexReplace": "tf.raw_ops.RegexReplace(\n    input, pattern, rewrite, replace_global=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.RegisterDataset": "tf.raw_ops.RegisterDataset(\n    dataset,\n    address,\n    protocol,\n    external_state_policy,\n    element_spec='',\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RegisterDatasetV2": "tf.raw_ops.RegisterDatasetV2(\n    dataset,\n    address,\n    protocol,\n    external_state_policy,\n    element_spec='',\n    requested_dataset_id='',\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Relu": "tf.raw_ops.Relu(\n    features, name=None\n)\n"
  },
  {
    "tf.raw_ops.Relu6": "tf.raw_ops.Relu6(\n    features, name=None\n)\n"
  },
  {
    "tf.raw_ops.Relu6Grad": "tf.raw_ops.Relu6Grad(\n    gradients, features, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReluGrad": "tf.raw_ops.ReluGrad(\n    gradients, features, name=None\n)\n"
  },
  {
    "tf.raw_ops.RemoteCall": "tf.raw_ops.RemoteCall(\n    target, args, Tout, f, name=None\n)\n"
  },
  {
    "tf.raw_ops.RepeatDataset": "tf.raw_ops.RepeatDataset(\n    input_dataset,\n    count,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RequantizationRange": "tf.raw_ops.RequantizationRange(\n    input, input_min, input_max, name=None\n)\n"
  },
  {
    "tf.raw_ops.RequantizationRangePerChannel": "tf.raw_ops.RequantizationRangePerChannel(\n    input, input_min, input_max, clip_value_max, name=None\n)\n"
  },
  {
    "tf.raw_ops.Requantize": "tf.raw_ops.Requantize(\n    input,\n    input_min,\n    input_max,\n    requested_output_min,\n    requested_output_max,\n    out_type,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RequantizePerChannel": "tf.raw_ops.RequantizePerChannel(\n    input,\n    input_min,\n    input_max,\n    requested_output_min,\n    requested_output_max,\n    out_type=tf.dtypes.quint8,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Reshape": "tf.raw_ops.Reshape(\n    tensor, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResizeArea": "tf.raw_ops.ResizeArea(\n    images, size, align_corners=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResizeBicubic": "tf.raw_ops.ResizeBicubic(\n    images, size, align_corners=False, half_pixel_centers=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResizeBicubicGrad": "tf.raw_ops.ResizeBicubicGrad(\n    grads,\n    original_image,\n    align_corners=False,\n    half_pixel_centers=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResizeBilinear": "tf.raw_ops.ResizeBilinear(\n    images, size, align_corners=False, half_pixel_centers=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResizeBilinearGrad": "tf.raw_ops.ResizeBilinearGrad(\n    grads,\n    original_image,\n    align_corners=False,\n    half_pixel_centers=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResizeNearestNeighbor": "tf.raw_ops.ResizeNearestNeighbor(\n    images, size, align_corners=False, half_pixel_centers=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResizeNearestNeighborGrad": "tf.raw_ops.ResizeNearestNeighborGrad(\n    grads, size, align_corners=False, half_pixel_centers=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceAccumulatorApplyGradient": "tf.raw_ops.ResourceAccumulatorApplyGradient(\n    handle, local_step, gradient, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceAccumulatorNumAccumulated": "tf.raw_ops.ResourceAccumulatorNumAccumulated(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceAccumulatorSetGlobalStep": "tf.raw_ops.ResourceAccumulatorSetGlobalStep(\n    handle, new_global_step, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceAccumulatorTakeGradient": "tf.raw_ops.ResourceAccumulatorTakeGradient(\n    handle, num_required, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyAdaMax": "tf.raw_ops.ResourceApplyAdaMax(\n    var,\n    m,\n    v,\n    beta1_power,\n    lr,\n    beta1,\n    beta2,\n    epsilon,\n    grad,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyAdadelta": "tf.raw_ops.ResourceApplyAdadelta(\n    var,\n    accum,\n    accum_update,\n    lr,\n    rho,\n    epsilon,\n    grad,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyAdagrad": "tf.raw_ops.ResourceApplyAdagrad(\n    var, accum, lr, grad, use_locking=False, update_slots=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyAdagradDA": "tf.raw_ops.ResourceApplyAdagradDA(\n    var,\n    gradient_accumulator,\n    gradient_squared_accumulator,\n    grad,\n    lr,\n    l1,\n    l2,\n    global_step,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyAdagradV2": "tf.raw_ops.ResourceApplyAdagradV2(\n    var,\n    accum,\n    lr,\n    epsilon,\n    grad,\n    use_locking=False,\n    update_slots=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyAdam": "tf.raw_ops.ResourceApplyAdam(\n    var,\n    m,\n    v,\n    beta1_power,\n    beta2_power,\n    lr,\n    beta1,\n    beta2,\n    epsilon,\n    grad,\n    use_locking=False,\n    use_nesterov=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyAdamWithAmsgrad": "tf.raw_ops.ResourceApplyAdamWithAmsgrad(\n    var,\n    m,\n    v,\n    vhat,\n    beta1_power,\n    beta2_power,\n    lr,\n    beta1,\n    beta2,\n    epsilon,\n    grad,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyAddSign": "tf.raw_ops.ResourceApplyAddSign(\n    var, m, lr, alpha, sign_decay, beta, grad, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyCenteredRMSProp": "tf.raw_ops.ResourceApplyCenteredRMSProp(\n    var,\n    mg,\n    ms,\n    mom,\n    lr,\n    rho,\n    momentum,\n    epsilon,\n    grad,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyFtrl": "tf.raw_ops.ResourceApplyFtrl(\n    var,\n    accum,\n    linear,\n    grad,\n    lr,\n    l1,\n    l2,\n    lr_power,\n    use_locking=False,\n    multiply_linear_by_lr=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyFtrlV2": "tf.raw_ops.ResourceApplyFtrlV2(\n    var,\n    accum,\n    linear,\n    grad,\n    lr,\n    l1,\n    l2,\n    l2_shrinkage,\n    lr_power,\n    use_locking=False,\n    multiply_linear_by_lr=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyGradientDescent": "tf.raw_ops.ResourceApplyGradientDescent(\n    var, alpha, delta, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyKerasMomentum": "tf.raw_ops.ResourceApplyKerasMomentum(\n    var,\n    accum,\n    lr,\n    grad,\n    momentum,\n    use_locking=False,\n    use_nesterov=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyMomentum": "tf.raw_ops.ResourceApplyMomentum(\n    var,\n    accum,\n    lr,\n    grad,\n    momentum,\n    use_locking=False,\n    use_nesterov=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyPowerSign": "tf.raw_ops.ResourceApplyPowerSign(\n    var, m, lr, logbase, sign_decay, beta, grad, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyProximalAdagrad": "tf.raw_ops.ResourceApplyProximalAdagrad(\n    var, accum, lr, l1, l2, grad, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyProximalGradientDescent": "tf.raw_ops.ResourceApplyProximalGradientDescent(\n    var, alpha, l1, l2, delta, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceApplyRMSProp": "tf.raw_ops.ResourceApplyRMSProp(\n    var,\n    ms,\n    mom,\n    lr,\n    rho,\n    momentum,\n    epsilon,\n    grad,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceConditionalAccumulator": "tf.raw_ops.ResourceConditionalAccumulator(\n    dtype,\n    shape,\n    container='',\n    shared_name='',\n    reduction_type='MEAN',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceCountUpTo": "tf.raw_ops.ResourceCountUpTo(\n    resource, limit, T, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceGather": "tf.raw_ops.ResourceGather(\n    resource, indices, dtype, batch_dims=0, validate_indices=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceGatherNd": "tf.raw_ops.ResourceGatherNd(\n    resource, indices, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterAdd": "tf.raw_ops.ResourceScatterAdd(\n    resource, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterDiv": "tf.raw_ops.ResourceScatterDiv(\n    resource, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterMax": "tf.raw_ops.ResourceScatterMax(\n    resource, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterMin": "tf.raw_ops.ResourceScatterMin(\n    resource, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterMul": "tf.raw_ops.ResourceScatterMul(\n    resource, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterNdAdd": "tf.raw_ops.ResourceScatterNdAdd(\n    ref, indices, updates, use_locking=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterNdMax": "tf.raw_ops.ResourceScatterNdMax(\n    ref, indices, updates, use_locking=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterNdMin": "tf.raw_ops.ResourceScatterNdMin(\n    ref, indices, updates, use_locking=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterNdSub": "tf.raw_ops.ResourceScatterNdSub(\n    ref, indices, updates, use_locking=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterNdUpdate": "tf.raw_ops.ResourceScatterNdUpdate(\n    ref, indices, updates, use_locking=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterSub": "tf.raw_ops.ResourceScatterSub(\n    resource, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceScatterUpdate": "tf.raw_ops.ResourceScatterUpdate(\n    resource, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyAdadelta": "tf.raw_ops.ResourceSparseApplyAdadelta(\n    var,\n    accum,\n    accum_update,\n    lr,\n    rho,\n    epsilon,\n    grad,\n    indices,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyAdagrad": "tf.raw_ops.ResourceSparseApplyAdagrad(\n    var,\n    accum,\n    lr,\n    grad,\n    indices,\n    use_locking=False,\n    update_slots=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyAdagradDA": "tf.raw_ops.ResourceSparseApplyAdagradDA(\n    var,\n    gradient_accumulator,\n    gradient_squared_accumulator,\n    grad,\n    indices,\n    lr,\n    l1,\n    l2,\n    global_step,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyAdagradV2": "tf.raw_ops.ResourceSparseApplyAdagradV2(\n    var,\n    accum,\n    lr,\n    epsilon,\n    grad,\n    indices,\n    use_locking=False,\n    update_slots=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyCenteredRMSProp": "tf.raw_ops.ResourceSparseApplyCenteredRMSProp(\n    var,\n    mg,\n    ms,\n    mom,\n    lr,\n    rho,\n    momentum,\n    epsilon,\n    grad,\n    indices,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyFtrl": "tf.raw_ops.ResourceSparseApplyFtrl(\n    var,\n    accum,\n    linear,\n    grad,\n    indices,\n    lr,\n    l1,\n    l2,\n    lr_power,\n    use_locking=False,\n    multiply_linear_by_lr=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyFtrlV2": "tf.raw_ops.ResourceSparseApplyFtrlV2(\n    var,\n    accum,\n    linear,\n    grad,\n    indices,\n    lr,\n    l1,\n    l2,\n    l2_shrinkage,\n    lr_power,\n    use_locking=False,\n    multiply_linear_by_lr=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyKerasMomentum": "tf.raw_ops.ResourceSparseApplyKerasMomentum(\n    var,\n    accum,\n    lr,\n    grad,\n    indices,\n    momentum,\n    use_locking=False,\n    use_nesterov=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyMomentum": "tf.raw_ops.ResourceSparseApplyMomentum(\n    var,\n    accum,\n    lr,\n    grad,\n    indices,\n    momentum,\n    use_locking=False,\n    use_nesterov=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyProximalAdagrad": "tf.raw_ops.ResourceSparseApplyProximalAdagrad(\n    var, accum, lr, l1, l2, grad, indices, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyProximalGradientDescent": "tf.raw_ops.ResourceSparseApplyProximalGradientDescent(\n    var, alpha, l1, l2, grad, indices, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceSparseApplyRMSProp": "tf.raw_ops.ResourceSparseApplyRMSProp(\n    var,\n    ms,\n    mom,\n    lr,\n    rho,\n    momentum,\n    epsilon,\n    grad,\n    indices,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ResourceStridedSliceAssign": "tf.raw_ops.ResourceStridedSliceAssign(\n    ref,\n    begin,\n    end,\n    strides,\n    value,\n    begin_mask=0,\n    end_mask=0,\n    ellipsis_mask=0,\n    new_axis_mask=0,\n    shrink_axis_mask=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Restore": "tf.raw_ops.Restore(\n    file_pattern, tensor_name, dt, preferred_shard=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.RestoreSlice": "tf.raw_ops.RestoreSlice(\n    file_pattern,\n    tensor_name,\n    shape_and_slice,\n    dt,\n    preferred_shard=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RestoreV2": "tf.raw_ops.RestoreV2(\n    prefix, tensor_names, shape_and_slices, dtypes, name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingADAMParameters": "tf.raw_ops.RetrieveTPUEmbeddingADAMParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingAdadeltaParameters": "tf.raw_ops.RetrieveTPUEmbeddingAdadeltaParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingAdagradMomentumParameters": "tf.raw_ops.RetrieveTPUEmbeddingAdagradMomentumParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingAdagradParameters": "tf.raw_ops.RetrieveTPUEmbeddingAdagradParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingCenteredRMSPropParameters": "tf.raw_ops.RetrieveTPUEmbeddingCenteredRMSPropParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingFTRLParameters": "tf.raw_ops.RetrieveTPUEmbeddingFTRLParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingFrequencyEstimatorParameters": "tf.raw_ops.RetrieveTPUEmbeddingFrequencyEstimatorParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingMDLAdagradLightParameters": "tf.raw_ops.RetrieveTPUEmbeddingMDLAdagradLightParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingMomentumParameters": "tf.raw_ops.RetrieveTPUEmbeddingMomentumParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingProximalAdagradParameters": "tf.raw_ops.RetrieveTPUEmbeddingProximalAdagradParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingProximalYogiParameters": "tf.raw_ops.RetrieveTPUEmbeddingProximalYogiParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingRMSPropParameters": "tf.raw_ops.RetrieveTPUEmbeddingRMSPropParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.RetrieveTPUEmbeddingStochasticGradientDescentParameters": "tf.raw_ops.RetrieveTPUEmbeddingStochasticGradientDescentParameters(\n    num_shards,\n    shard_id,\n    table_id=-1,\n    table_name='',\n    config='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Reverse": "tf.raw_ops.Reverse(\n    tensor, dims, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReverseSequence": "tf.raw_ops.ReverseSequence(\n    input, seq_lengths, seq_dim, batch_dim=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.ReverseV2": "tf.raw_ops.ReverseV2(\n    tensor, axis, name=None\n)\n"
  },
  {
    "tf.raw_ops.RewriteDataset": "tf.raw_ops.RewriteDataset(\n    input_dataset, rewrite_name, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.RightShift": "tf.raw_ops.RightShift(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.Rint": "tf.raw_ops.Rint(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.RngReadAndSkip": "tf.raw_ops.RngReadAndSkip(\n    resource, alg, delta, name=None\n)\n"
  },
  {
    "tf.raw_ops.RngSkip": "tf.raw_ops.RngSkip(\n    resource, algorithm, delta, name=None\n)\n"
  },
  {
    "tf.raw_ops.Roll": "tf.raw_ops.Roll(\n    input, shift, axis, name=None\n)\n"
  },
  {
    "tf.raw_ops.Round": "tf.raw_ops.Round(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Rsqrt": "tf.raw_ops.Rsqrt(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.RsqrtGrad": "tf.raw_ops.RsqrtGrad(\n    y, dy, name=None\n)\n"
  },
  {
    "tf.raw_ops.SampleDistortedBoundingBox": "tf.raw_ops.SampleDistortedBoundingBox(\n    image_size,\n    bounding_boxes,\n    seed=0,\n    seed2=0,\n    min_object_covered=0.1,\n    aspect_ratio_range=[0.75, 1.33],\n    area_range=[0.05, 1],\n    max_attempts=100,\n    use_image_if_no_bounding_boxes=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SampleDistortedBoundingBoxV2": "tf.raw_ops.SampleDistortedBoundingBoxV2(\n    image_size,\n    bounding_boxes,\n    min_object_covered,\n    seed=0,\n    seed2=0,\n    aspect_ratio_range=[0.75, 1.33],\n    area_range=[0.05, 1],\n    max_attempts=100,\n    use_image_if_no_bounding_boxes=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SamplingDataset": "tf.raw_ops.SamplingDataset(\n    input_dataset, rate, seed, seed2, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.Save": "tf.raw_ops.Save(\n    filename, tensor_names, data, name=None\n)\n"
  },
  {
    "tf.raw_ops.SaveDataset": "tf.raw_ops.SaveDataset(\n    input_dataset,\n    path,\n    shard_func_other_args,\n    shard_func,\n    compression='',\n    use_shard_func=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SaveDatasetV2": "tf.raw_ops.SaveDatasetV2(\n    input_dataset,\n    path,\n    shard_func_other_args,\n    shard_func,\n    output_types,\n    output_shapes,\n    compression='',\n    use_shard_func=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SaveSlices": "tf.raw_ops.SaveSlices(\n    filename, tensor_names, shapes_and_slices, data, name=None\n)\n"
  },
  {
    "tf.raw_ops.SaveV2": "tf.raw_ops.SaveV2(\n    prefix, tensor_names, shape_and_slices, tensors, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScalarSummary": "tf.raw_ops.ScalarSummary(\n    tags, values, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScaleAndTranslate": "tf.raw_ops.ScaleAndTranslate(\n    images,\n    size,\n    scale,\n    translation,\n    kernel_type='lanczos3',\n    antialias=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ScaleAndTranslateGrad": "tf.raw_ops.ScaleAndTranslateGrad(\n    grads,\n    original_image,\n    scale,\n    translation,\n    kernel_type='lanczos3',\n    antialias=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ScanDataset": "tf.raw_ops.ScanDataset(\n    input_dataset,\n    initial_state,\n    other_arguments,\n    f,\n    output_types,\n    output_shapes,\n    preserve_cardinality=False,\n    use_default_device=True,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterAdd": "tf.raw_ops.ScatterAdd(\n    ref, indices, updates, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterDiv": "tf.raw_ops.ScatterDiv(\n    ref, indices, updates, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterMax": "tf.raw_ops.ScatterMax(\n    ref, indices, updates, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterMin": "tf.raw_ops.ScatterMin(\n    ref, indices, updates, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterMul": "tf.raw_ops.ScatterMul(\n    ref, indices, updates, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterNd": "tf.raw_ops.ScatterNd(\n    indices, updates, shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterNdAdd": "tf.raw_ops.ScatterNdAdd(\n    ref, indices, updates, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterNdMax": "tf.raw_ops.ScatterNdMax(\n    ref, indices, updates, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterNdMin": "tf.raw_ops.ScatterNdMin(\n    ref, indices, updates, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterNdNonAliasingAdd": "tf.raw_ops.ScatterNdNonAliasingAdd(\n    input, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterNdSub": "tf.raw_ops.ScatterNdSub(\n    ref, indices, updates, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterNdUpdate": "tf.raw_ops.ScatterNdUpdate(\n    ref, indices, updates, use_locking=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterSub": "tf.raw_ops.ScatterSub(\n    ref, indices, updates, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.ScatterUpdate": "tf.raw_ops.ScatterUpdate(\n    ref, indices, updates, use_locking=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.SdcaFprint": "tf.raw_ops.SdcaFprint(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.SdcaOptimizer": "tf.raw_ops.SdcaOptimizer(\n    sparse_example_indices,\n    sparse_feature_indices,\n    sparse_feature_values,\n    dense_features,\n    example_weights,\n    example_labels,\n    sparse_indices,\n    sparse_weights,\n    dense_weights,\n    example_state_data,\n    loss_type,\n    l1,\n    l2,\n    num_loss_partitions,\n    num_inner_iterations,\n    adaptative=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SdcaOptimizerV2": "tf.raw_ops.SdcaOptimizerV2(\n    sparse_example_indices,\n    sparse_feature_indices,\n    sparse_feature_values,\n    dense_features,\n    example_weights,\n    example_labels,\n    sparse_indices,\n    sparse_weights,\n    dense_weights,\n    example_state_data,\n    loss_type,\n    l1,\n    l2,\n    num_loss_partitions,\n    num_inner_iterations,\n    adaptive=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SdcaShrinkL1": "tf.raw_ops.SdcaShrinkL1(\n    weights, l1, l2, name=None\n)\n"
  },
  {
    "tf.raw_ops.SegmentMax": "tf.raw_ops.SegmentMax(\n    data, segment_ids, name=None\n)\n"
  },
  {
    "tf.raw_ops.SegmentMean": "tf.raw_ops.SegmentMean(\n    data, segment_ids, name=None\n)\n"
  },
  {
    "tf.raw_ops.SegmentMin": "tf.raw_ops.SegmentMin(\n    data, segment_ids, name=None\n)\n"
  },
  {
    "tf.raw_ops.SegmentProd": "tf.raw_ops.SegmentProd(\n    data, segment_ids, name=None\n)\n"
  },
  {
    "tf.raw_ops.SegmentSum": "tf.raw_ops.SegmentSum(\n    data, segment_ids, name=None\n)\n"
  },
  {
    "tf.raw_ops.Select": "tf.raw_ops.Select(\n    condition, x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.SelectV2": "tf.raw_ops.SelectV2(\n    condition, t, e, name=None\n)\n"
  },
  {
    "tf.raw_ops.SelfAdjointEig": "tf.raw_ops.SelfAdjointEig(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.SelfAdjointEigV2": "tf.raw_ops.SelfAdjointEigV2(\n    input, compute_v=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.Selu": "tf.raw_ops.Selu(\n    features, name=None\n)\n"
  },
  {
    "tf.raw_ops.SeluGrad": "tf.raw_ops.SeluGrad(\n    gradients, outputs, name=None\n)\n"
  },
  {
    "tf.raw_ops.Send": "tf.raw_ops.Send(\n    tensor,\n    tensor_name,\n    send_device,\n    send_device_incarnation,\n    recv_device,\n    client_terminated=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SendTPUEmbeddingGradients": "tf.raw_ops.SendTPUEmbeddingGradients(\n    inputs, learning_rates, config, name=None\n)\n"
  },
  {
    "tf.raw_ops.SerializeIterator": "tf.raw_ops.SerializeIterator(\n    resource_handle, external_state_policy=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.SerializeManySparse": "tf.raw_ops.SerializeManySparse(\n    sparse_indices,\n    sparse_values,\n    sparse_shape,\n    out_type=tf.dtypes.string,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SerializeSparse": "tf.raw_ops.SerializeSparse(\n    sparse_indices,\n    sparse_values,\n    sparse_shape,\n    out_type=tf.dtypes.string,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SerializeTensor": "tf.raw_ops.SerializeTensor(\n    tensor, name=None\n)\n"
  },
  {
    "tf.raw_ops.SetSize": "tf.raw_ops.SetSize(\n    set_indices, set_values, set_shape, validate_indices=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.SetStatsAggregatorDataset": "tf.raw_ops.SetStatsAggregatorDataset(\n    input_dataset,\n    stats_aggregator,\n    tag,\n    counter_prefix,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Shape": "tf.raw_ops.Shape(\n    input,\n    out_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ShapeN": "tf.raw_ops.ShapeN(\n    input,\n    out_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ShardDataset": "tf.raw_ops.ShardDataset(\n    input_dataset,\n    num_shards,\n    index,\n    output_types,\n    output_shapes,\n    require_non_empty=False,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ShardedFilename": "tf.raw_ops.ShardedFilename(\n    basename, shard, num_shards, name=None\n)\n"
  },
  {
    "tf.raw_ops.ShardedFilespec": "tf.raw_ops.ShardedFilespec(\n    basename, num_shards, name=None\n)\n"
  },
  {
    "tf.raw_ops.ShuffleAndRepeatDataset": "tf.raw_ops.ShuffleAndRepeatDataset(\n    input_dataset,\n    buffer_size,\n    seed,\n    seed2,\n    count,\n    output_types,\n    output_shapes,\n    reshuffle_each_iteration=True,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ShuffleAndRepeatDatasetV2": "tf.raw_ops.ShuffleAndRepeatDatasetV2(\n    input_dataset,\n    buffer_size,\n    seed,\n    seed2,\n    count,\n    seed_generator,\n    output_types,\n    output_shapes,\n    reshuffle_each_iteration=True,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ShuffleDataset": "tf.raw_ops.ShuffleDataset(\n    input_dataset,\n    buffer_size,\n    seed,\n    seed2,\n    output_types,\n    output_shapes,\n    reshuffle_each_iteration=True,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ShuffleDatasetV2": "tf.raw_ops.ShuffleDatasetV2(\n    input_dataset,\n    buffer_size,\n    seed_generator,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ShuffleDatasetV3": "tf.raw_ops.ShuffleDatasetV3(\n    input_dataset,\n    buffer_size,\n    seed,\n    seed2,\n    seed_generator,\n    output_types,\n    output_shapes,\n    reshuffle_each_iteration=True,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ShutdownDistributedTPU": "tf.raw_ops.ShutdownDistributedTPU(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Sigmoid": "tf.raw_ops.Sigmoid(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.SigmoidGrad": "tf.raw_ops.SigmoidGrad(\n    y, dy, name=None\n)\n"
  },
  {
    "tf.raw_ops.Sign": "tf.raw_ops.Sign(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Sin": "tf.raw_ops.Sin(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Sinh": "tf.raw_ops.Sinh(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Size": "tf.raw_ops.Size(\n    input,\n    out_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SkipDataset": "tf.raw_ops.SkipDataset(\n    input_dataset,\n    count,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SleepDataset": "tf.raw_ops.SleepDataset(\n    input_dataset, sleep_microseconds, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.Slice": "tf.raw_ops.Slice(\n    input, begin, size, name=None\n)\n"
  },
  {
    "tf.raw_ops.SlidingWindowDataset": "tf.raw_ops.SlidingWindowDataset(\n    input_dataset,\n    window_size,\n    window_shift,\n    window_stride,\n    output_types,\n    output_shapes,\n    drop_remainder=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Snapshot": "tf.raw_ops.Snapshot(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.SnapshotDataset": "tf.raw_ops.SnapshotDataset(\n    input_dataset,\n    path,\n    output_types,\n    output_shapes,\n    compression='',\n    reader_path_prefix='',\n    writer_path_prefix='',\n    shard_size_bytes=10737418240,\n    pending_snapshot_expiry_seconds=86400,\n    num_reader_threads=1,\n    reader_buffer_size=1,\n    num_writer_threads=1,\n    writer_buffer_size=1,\n    shuffle_on_read=False,\n    seed=0,\n    seed2=0,\n    mode='auto',\n    snapshot_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SnapshotDatasetReader": "tf.raw_ops.SnapshotDatasetReader(\n    shard_dir,\n    start_index,\n    output_types,\n    output_shapes,\n    version,\n    compression='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SnapshotDatasetV2": "tf.raw_ops.SnapshotDatasetV2(\n    input_dataset,\n    path,\n    reader_func_other_args,\n    shard_func_other_args,\n    output_types,\n    output_shapes,\n    reader_func,\n    shard_func,\n    compression='',\n    reader_prefix='',\n    writer_prefix='',\n    hash_valid=False,\n    hash=0,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SnapshotNestedDatasetReader": "tf.raw_ops.SnapshotNestedDatasetReader(\n    inputs, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.SobolSample": "tf.raw_ops.SobolSample(\n    dim,\n    num_results,\n    skip,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Softmax": "tf.raw_ops.Softmax(\n    logits, name=None\n)\n"
  },
  {
    "tf.raw_ops.SoftmaxCrossEntropyWithLogits": "tf.raw_ops.SoftmaxCrossEntropyWithLogits(\n    features, labels, name=None\n)\n"
  },
  {
    "tf.raw_ops.Softplus": "tf.raw_ops.Softplus(\n    features, name=None\n)\n"
  },
  {
    "tf.raw_ops.SoftplusGrad": "tf.raw_ops.SoftplusGrad(\n    gradients, features, name=None\n)\n"
  },
  {
    "tf.raw_ops.Softsign": "tf.raw_ops.Softsign(\n    features, name=None\n)\n"
  },
  {
    "tf.raw_ops.SoftsignGrad": "tf.raw_ops.SoftsignGrad(\n    gradients, features, name=None\n)\n"
  },
  {
    "tf.raw_ops.SpaceToBatch": "tf.raw_ops.SpaceToBatch(\n    input, paddings, block_size, name=None\n)\n"
  },
  {
    "tf.raw_ops.SpaceToBatchND": "tf.raw_ops.SpaceToBatchND(\n    input, block_shape, paddings, name=None\n)\n"
  },
  {
    "tf.raw_ops.SpaceToDepth": "tf.raw_ops.SpaceToDepth(\n    input, block_size, data_format='NHWC', name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseAccumulatorApplyGradient": "tf.raw_ops.SparseAccumulatorApplyGradient(\n    handle,\n    local_step,\n    gradient_indices,\n    gradient_values,\n    gradient_shape,\n    has_known_shape,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseAccumulatorTakeGradient": "tf.raw_ops.SparseAccumulatorTakeGradient(\n    handle, num_required, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseAdd": "tf.raw_ops.SparseAdd(\n    a_indices,\n    a_values,\n    a_shape,\n    b_indices,\n    b_values,\n    b_shape,\n    thresh,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseAddGrad": "tf.raw_ops.SparseAddGrad(\n    backprop_val_grad, a_indices, b_indices, sum_indices, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseApplyAdadelta": "tf.raw_ops.SparseApplyAdadelta(\n    var,\n    accum,\n    accum_update,\n    lr,\n    rho,\n    epsilon,\n    grad,\n    indices,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseApplyAdagrad": "tf.raw_ops.SparseApplyAdagrad(\n    var,\n    accum,\n    lr,\n    grad,\n    indices,\n    use_locking=False,\n    update_slots=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseApplyAdagradDA": "tf.raw_ops.SparseApplyAdagradDA(\n    var,\n    gradient_accumulator,\n    gradient_squared_accumulator,\n    grad,\n    indices,\n    lr,\n    l1,\n    l2,\n    global_step,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseApplyAdagradV2": "tf.raw_ops.SparseApplyAdagradV2(\n    var,\n    accum,\n    lr,\n    epsilon,\n    grad,\n    indices,\n    use_locking=False,\n    update_slots=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseApplyCenteredRMSProp": "tf.raw_ops.SparseApplyCenteredRMSProp(\n    var,\n    mg,\n    ms,\n    mom,\n    lr,\n    rho,\n    momentum,\n    epsilon,\n    grad,\n    indices,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseApplyFtrl": "tf.raw_ops.SparseApplyFtrl(\n    var,\n    accum,\n    linear,\n    grad,\n    indices,\n    lr,\n    l1,\n    l2,\n    lr_power,\n    use_locking=False,\n    multiply_linear_by_lr=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseApplyFtrlV2": "tf.raw_ops.SparseApplyFtrlV2(\n    var,\n    accum,\n    linear,\n    grad,\n    indices,\n    lr,\n    l1,\n    l2,\n    l2_shrinkage,\n    lr_power,\n    use_locking=False,\n    multiply_linear_by_lr=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseApplyMomentum": "tf.raw_ops.SparseApplyMomentum(\n    var,\n    accum,\n    lr,\n    grad,\n    indices,\n    momentum,\n    use_locking=False,\n    use_nesterov=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseApplyProximalAdagrad": "tf.raw_ops.SparseApplyProximalAdagrad(\n    var, accum, lr, l1, l2, grad, indices, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseApplyProximalGradientDescent": "tf.raw_ops.SparseApplyProximalGradientDescent(\n    var, alpha, l1, l2, grad, indices, use_locking=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseApplyRMSProp": "tf.raw_ops.SparseApplyRMSProp(\n    var,\n    ms,\n    mom,\n    lr,\n    rho,\n    momentum,\n    epsilon,\n    grad,\n    indices,\n    use_locking=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseBincount": "tf.raw_ops.SparseBincount(\n    indices, values, dense_shape, size, weights, binary_output=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseConcat": "tf.raw_ops.SparseConcat(\n    indices, values, shapes, concat_dim, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseConditionalAccumulator": "tf.raw_ops.SparseConditionalAccumulator(\n    dtype,\n    shape,\n    container='',\n    shared_name='',\n    reduction_type='MEAN',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseCountSparseOutput": "tf.raw_ops.SparseCountSparseOutput(\n    indices,\n    values,\n    dense_shape,\n    weights,\n    binary_output,\n    minlength=-1,\n    maxlength=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseCross": "tf.raw_ops.SparseCross(\n    indices,\n    values,\n    shapes,\n    dense_inputs,\n    hashed_output,\n    num_buckets,\n    hash_key,\n    out_type,\n    internal_type,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseCrossHashed": "tf.raw_ops.SparseCrossHashed(\n    indices,\n    values,\n    shapes,\n    dense_inputs,\n    num_buckets,\n    strong_hash,\n    salt,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseCrossV2": "tf.raw_ops.SparseCrossV2(\n    indices, values, shapes, dense_inputs, sep, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseDenseCwiseAdd": "tf.raw_ops.SparseDenseCwiseAdd(\n    sp_indices, sp_values, sp_shape, dense, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseDenseCwiseDiv": "tf.raw_ops.SparseDenseCwiseDiv(\n    sp_indices, sp_values, sp_shape, dense, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseDenseCwiseMul": "tf.raw_ops.SparseDenseCwiseMul(\n    sp_indices, sp_values, sp_shape, dense, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseFillEmptyRows": "tf.raw_ops.SparseFillEmptyRows(\n    indices, values, dense_shape, default_value, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseFillEmptyRowsGrad": "tf.raw_ops.SparseFillEmptyRowsGrad(\n    reverse_index_map, grad_values, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatMul": "tf.raw_ops.SparseMatMul(\n    a,\n    b,\n    transpose_a=False,\n    transpose_b=False,\n    a_is_sparse=False,\n    b_is_sparse=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatrixAdd": "tf.raw_ops.SparseMatrixAdd(\n    a, b, alpha, beta, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatrixMatMul": "tf.raw_ops.SparseMatrixMatMul(\n    a,\n    b,\n    transpose_a=False,\n    transpose_b=False,\n    adjoint_a=False,\n    adjoint_b=False,\n    transpose_output=False,\n    conjugate_output=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatrixMul": "tf.raw_ops.SparseMatrixMul(\n    a, b, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatrixNNZ": "tf.raw_ops.SparseMatrixNNZ(\n    sparse_matrix, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatrixOrderingAMD": "tf.raw_ops.SparseMatrixOrderingAMD(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatrixSoftmax": "tf.raw_ops.SparseMatrixSoftmax(\n    logits, type, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatrixSoftmaxGrad": "tf.raw_ops.SparseMatrixSoftmaxGrad(\n    softmax, grad_softmax, type, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatrixSparseCholesky": "tf.raw_ops.SparseMatrixSparseCholesky(\n    input, permutation, type, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatrixSparseMatMul": "tf.raw_ops.SparseMatrixSparseMatMul(\n    a,\n    b,\n    type,\n    transpose_a=False,\n    transpose_b=False,\n    adjoint_a=False,\n    adjoint_b=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatrixTranspose": "tf.raw_ops.SparseMatrixTranspose(\n    input, type, conjugate=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseMatrixZeros": "tf.raw_ops.SparseMatrixZeros(\n    dense_shape, type, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseReduceMax": "tf.raw_ops.SparseReduceMax(\n    input_indices,\n    input_values,\n    input_shape,\n    reduction_axes,\n    keep_dims=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseReduceMaxSparse": "tf.raw_ops.SparseReduceMaxSparse(\n    input_indices,\n    input_values,\n    input_shape,\n    reduction_axes,\n    keep_dims=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseReduceSum": "tf.raw_ops.SparseReduceSum(\n    input_indices,\n    input_values,\n    input_shape,\n    reduction_axes,\n    keep_dims=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseReduceSumSparse": "tf.raw_ops.SparseReduceSumSparse(\n    input_indices,\n    input_values,\n    input_shape,\n    reduction_axes,\n    keep_dims=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseReorder": "tf.raw_ops.SparseReorder(\n    input_indices, input_values, input_shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseReshape": "tf.raw_ops.SparseReshape(\n    input_indices, input_shape, new_shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSegmentMean": "tf.raw_ops.SparseSegmentMean(\n    data, indices, segment_ids, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSegmentMeanGrad": "tf.raw_ops.SparseSegmentMeanGrad(\n    grad, indices, segment_ids, output_dim0, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSegmentMeanWithNumSegments": "tf.raw_ops.SparseSegmentMeanWithNumSegments(\n    data, indices, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSegmentSqrtN": "tf.raw_ops.SparseSegmentSqrtN(\n    data, indices, segment_ids, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSegmentSqrtNGrad": "tf.raw_ops.SparseSegmentSqrtNGrad(\n    grad, indices, segment_ids, output_dim0, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSegmentSqrtNWithNumSegments": "tf.raw_ops.SparseSegmentSqrtNWithNumSegments(\n    data, indices, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSegmentSum": "tf.raw_ops.SparseSegmentSum(\n    data, indices, segment_ids, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSegmentSumGrad": "tf.raw_ops.SparseSegmentSumGrad(\n    grad, indices, segment_ids, output_dim0, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSegmentSumWithNumSegments": "tf.raw_ops.SparseSegmentSumWithNumSegments(\n    data, indices, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSlice": "tf.raw_ops.SparseSlice(\n    indices, values, shape, start, size, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSliceGrad": "tf.raw_ops.SparseSliceGrad(\n    backprop_val_grad, input_indices, input_start, output_indices, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSoftmax": "tf.raw_ops.SparseSoftmax(\n    sp_indices, sp_values, sp_shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSoftmaxCrossEntropyWithLogits": "tf.raw_ops.SparseSoftmaxCrossEntropyWithLogits(\n    features, labels, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSparseMaximum": "tf.raw_ops.SparseSparseMaximum(\n    a_indices, a_values, a_shape, b_indices, b_values, b_shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSparseMinimum": "tf.raw_ops.SparseSparseMinimum(\n    a_indices, a_values, a_shape, b_indices, b_values, b_shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseSplit": "tf.raw_ops.SparseSplit(\n    split_dim, indices, values, shape, num_split, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseTensorDenseAdd": "tf.raw_ops.SparseTensorDenseAdd(\n    a_indices, a_values, a_shape, b, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseTensorDenseMatMul": "tf.raw_ops.SparseTensorDenseMatMul(\n    a_indices,\n    a_values,\n    a_shape,\n    b,\n    adjoint_a=False,\n    adjoint_b=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseTensorSliceDataset": "tf.raw_ops.SparseTensorSliceDataset(\n    indices, values, dense_shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseTensorToCSRSparseMatrix": "tf.raw_ops.SparseTensorToCSRSparseMatrix(\n    indices, values, dense_shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseToDense": "tf.raw_ops.SparseToDense(\n    sparse_indices,\n    output_shape,\n    sparse_values,\n    default_value,\n    validate_indices=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.SparseToSparseSetOperation": "tf.raw_ops.SparseToSparseSetOperation(\n    set1_indices,\n    set1_values,\n    set1_shape,\n    set2_indices,\n    set2_values,\n    set2_shape,\n    set_operation,\n    validate_indices=True,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Spence": "tf.raw_ops.Spence(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Split": "tf.raw_ops.Split(\n    axis, value, num_split, name=None\n)\n"
  },
  {
    "tf.raw_ops.SplitV": "tf.raw_ops.SplitV(\n    value, size_splits, axis, num_split, name=None\n)\n"
  },
  {
    "tf.raw_ops.SqlDataset": "tf.raw_ops.SqlDataset(\n    driver_name,\n    data_source_name,\n    query,\n    output_types,\n    output_shapes,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Sqrt": "tf.raw_ops.Sqrt(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.SqrtGrad": "tf.raw_ops.SqrtGrad(\n    y, dy, name=None\n)\n"
  },
  {
    "tf.raw_ops.Square": "tf.raw_ops.Square(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.SquaredDifference": "tf.raw_ops.SquaredDifference(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.Squeeze": "tf.raw_ops.Squeeze(\n    input, axis=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.Stack": "tf.raw_ops.Stack(\n    elem_type, stack_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.StackClose": "tf.raw_ops.StackClose(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.StackCloseV2": "tf.raw_ops.StackCloseV2(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.StackPop": "tf.raw_ops.StackPop(\n    handle, elem_type, name=None\n)\n"
  },
  {
    "tf.raw_ops.StackPopV2": "tf.raw_ops.StackPopV2(\n    handle, elem_type, name=None\n)\n"
  },
  {
    "tf.raw_ops.StackPush": "tf.raw_ops.StackPush(\n    handle, elem, swap_memory=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.StackPushV2": "tf.raw_ops.StackPushV2(\n    handle, elem, swap_memory=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.StackV2": "tf.raw_ops.StackV2(\n    max_size, elem_type, stack_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.Stage": "tf.raw_ops.Stage(\n    values,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StageClear": "tf.raw_ops.StageClear(\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StagePeek": "tf.raw_ops.StagePeek(\n    index,\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StageSize": "tf.raw_ops.StageSize(\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatefulPartitionedCall": "tf.raw_ops.StatefulPartitionedCall(\n    args,\n    Tout,\n    f,\n    config='',\n    config_proto='',\n    executor_type='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatefulRandomBinomial": "tf.raw_ops.StatefulRandomBinomial(\n    resource,\n    algorithm,\n    shape,\n    counts,\n    probs,\n    dtype=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatefulStandardNormal": "tf.raw_ops.StatefulStandardNormal(\n    resource,\n    shape,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatefulStandardNormalV2": "tf.raw_ops.StatefulStandardNormalV2(\n    resource,\n    algorithm,\n    shape,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatefulTruncatedNormal": "tf.raw_ops.StatefulTruncatedNormal(\n    resource,\n    algorithm,\n    shape,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatefulUniform": "tf.raw_ops.StatefulUniform(\n    resource,\n    algorithm,\n    shape,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatefulUniformFullInt": "tf.raw_ops.StatefulUniformFullInt(\n    resource,\n    algorithm,\n    shape,\n    dtype=tf.dtypes.uint64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatefulUniformInt": "tf.raw_ops.StatefulUniformInt(\n    resource, algorithm, shape, minval, maxval, name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessCase": "tf.raw_ops.StatelessCase(\n    branch_index, input, Tout, branches, output_shapes=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessIf": "tf.raw_ops.StatelessIf(\n    cond, input, Tout, then_branch, else_branch, output_shapes=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessMultinomial": "tf.raw_ops.StatelessMultinomial(\n    logits,\n    num_samples,\n    seed,\n    output_dtype=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessParameterizedTruncatedNormal": "tf.raw_ops.StatelessParameterizedTruncatedNormal(\n    shape, seed, means, stddevs, minvals, maxvals, name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomBinomial": "tf.raw_ops.StatelessRandomBinomial(\n    shape,\n    seed,\n    counts,\n    probs,\n    dtype=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomGammaV2": "tf.raw_ops.StatelessRandomGammaV2(\n    shape, seed, alpha, name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomGetAlg": "tf.raw_ops.StatelessRandomGetAlg(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomGetKeyCounter": "tf.raw_ops.StatelessRandomGetKeyCounter(\n    seed, name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomGetKeyCounterAlg": "tf.raw_ops.StatelessRandomGetKeyCounterAlg(\n    seed, name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomNormal": "tf.raw_ops.StatelessRandomNormal(\n    shape,\n    seed,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomNormalV2": "tf.raw_ops.StatelessRandomNormalV2(\n    shape,\n    key,\n    counter,\n    alg,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomPoisson": "tf.raw_ops.StatelessRandomPoisson(\n    shape, seed, lam, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomUniform": "tf.raw_ops.StatelessRandomUniform(\n    shape,\n    seed,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomUniformFullInt": "tf.raw_ops.StatelessRandomUniformFullInt(\n    shape,\n    seed,\n    dtype=tf.dtypes.uint64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomUniformFullIntV2": "tf.raw_ops.StatelessRandomUniformFullIntV2(\n    shape,\n    key,\n    counter,\n    alg,\n    dtype=tf.dtypes.uint64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomUniformInt": "tf.raw_ops.StatelessRandomUniformInt(\n    shape, seed, minval, maxval, name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomUniformIntV2": "tf.raw_ops.StatelessRandomUniformIntV2(\n    shape, key, counter, alg, minval, maxval, name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessRandomUniformV2": "tf.raw_ops.StatelessRandomUniformV2(\n    shape,\n    key,\n    counter,\n    alg,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessSampleDistortedBoundingBox": "tf.raw_ops.StatelessSampleDistortedBoundingBox(\n    image_size,\n    bounding_boxes,\n    min_object_covered,\n    seed,\n    aspect_ratio_range=[0.75, 1.33],\n    area_range=[0.05, 1],\n    max_attempts=100,\n    use_image_if_no_bounding_boxes=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessShuffle": "tf.raw_ops.StatelessShuffle(\n    value, key, counter, alg, name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessTruncatedNormal": "tf.raw_ops.StatelessTruncatedNormal(\n    shape,\n    seed,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessTruncatedNormalV2": "tf.raw_ops.StatelessTruncatedNormalV2(\n    shape,\n    key,\n    counter,\n    alg,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StatelessWhile": "tf.raw_ops.StatelessWhile(\n    input, cond, body, output_shapes=[], parallel_iterations=10, name=None\n)\n"
  },
  {
    "tf.raw_ops.StaticRegexFullMatch": "tf.raw_ops.StaticRegexFullMatch(\n    input, pattern, name=None\n)\n"
  },
  {
    "tf.raw_ops.StaticRegexReplace": "tf.raw_ops.StaticRegexReplace(\n    input, pattern, rewrite, replace_global=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.StatsAggregatorHandle": "tf.raw_ops.StatsAggregatorHandle(\n    container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.StatsAggregatorHandleV2": "tf.raw_ops.StatsAggregatorHandleV2(\n    container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.StatsAggregatorSetSummaryWriter": "tf.raw_ops.StatsAggregatorSetSummaryWriter(\n    stats_aggregator, summary, name=None\n)\n"
  },
  {
    "tf.raw_ops.StatsAggregatorSummary": "tf.raw_ops.StatsAggregatorSummary(\n    iterator, name=None\n)\n"
  },
  {
    "tf.raw_ops.StopGradient": "tf.raw_ops.StopGradient(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.StridedSlice": "tf.raw_ops.StridedSlice(\n    input,\n    begin,\n    end,\n    strides,\n    begin_mask=0,\n    end_mask=0,\n    ellipsis_mask=0,\n    new_axis_mask=0,\n    shrink_axis_mask=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StridedSliceAssign": "tf.raw_ops.StridedSliceAssign(\n    ref,\n    begin,\n    end,\n    strides,\n    value,\n    begin_mask=0,\n    end_mask=0,\n    ellipsis_mask=0,\n    new_axis_mask=0,\n    shrink_axis_mask=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StridedSliceGrad": "tf.raw_ops.StridedSliceGrad(\n    shape,\n    begin,\n    end,\n    strides,\n    dy,\n    begin_mask=0,\n    end_mask=0,\n    ellipsis_mask=0,\n    new_axis_mask=0,\n    shrink_axis_mask=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StringFormat": "tf.raw_ops.StringFormat(\n    inputs,\n    template='%s',\n    placeholder='%s',\n    summarize=3,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StringJoin": "tf.raw_ops.StringJoin(\n    inputs, separator='', name=None\n)\n"
  },
  {
    "tf.raw_ops.StringLength": "tf.raw_ops.StringLength(\n    input, unit='BYTE', name=None\n)\n"
  },
  {
    "tf.raw_ops.StringLower": "tf.raw_ops.StringLower(\n    input, encoding='', name=None\n)\n"
  },
  {
    "tf.raw_ops.StringNGrams": "tf.raw_ops.StringNGrams(\n    data,\n    data_splits,\n    separator,\n    ngram_widths,\n    left_pad,\n    right_pad,\n    pad_width,\n    preserve_short_sequences,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StringSplit": "tf.raw_ops.StringSplit(\n    input, delimiter, skip_empty=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.StringSplitV2": "tf.raw_ops.StringSplitV2(\n    input, sep, maxsplit=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.StringStrip": "tf.raw_ops.StringStrip(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.StringToHashBucket": "tf.raw_ops.StringToHashBucket(\n    string_tensor, num_buckets, name=None\n)\n"
  },
  {
    "tf.raw_ops.StringToHashBucketFast": "tf.raw_ops.StringToHashBucketFast(\n    input, num_buckets, name=None\n)\n"
  },
  {
    "tf.raw_ops.StringToHashBucketStrong": "tf.raw_ops.StringToHashBucketStrong(\n    input, num_buckets, key, name=None\n)\n"
  },
  {
    "tf.raw_ops.StringToNumber": "tf.raw_ops.StringToNumber(\n    string_tensor,\n    out_type=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.StringUpper": "tf.raw_ops.StringUpper(\n    input, encoding='', name=None\n)\n"
  },
  {
    "tf.raw_ops.Sub": "tf.raw_ops.Sub(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.Substr": "tf.raw_ops.Substr(\n    input, pos, len, unit='BYTE', name=None\n)\n"
  },
  {
    "tf.raw_ops.Sum": "tf.raw_ops.Sum(\n    input, axis, keep_dims=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.SummaryWriter": "tf.raw_ops.SummaryWriter(\n    shared_name='', container='', name=None\n)\n"
  },
  {
    "tf.raw_ops.Svd": "tf.raw_ops.Svd(\n    input, compute_uv=True, full_matrices=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.Switch": "tf.raw_ops.Switch(\n    data, pred, name=None\n)\n"
  },
  {
    "tf.raw_ops.SymbolicGradient": "tf.raw_ops.SymbolicGradient(\n    input, Tout, f, name=None\n)\n"
  },
  {
    "tf.raw_ops.TFRecordDataset": "tf.raw_ops.TFRecordDataset(\n    filenames, compression_type, buffer_size, metadata='', name=None\n)\n"
  },
  {
    "tf.raw_ops.TFRecordReader": "tf.raw_ops.TFRecordReader(\n    container='',\n    shared_name='',\n    compression_type='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TFRecordReaderV2": "tf.raw_ops.TFRecordReaderV2(\n    container='',\n    shared_name='',\n    compression_type='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TPUCompilationResult": "tf.raw_ops.TPUCompilationResult(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TPUEmbeddingActivations": "tf.raw_ops.TPUEmbeddingActivations(\n    embedding_variable, sliced_activations, table_id, lookup_id, name=None\n)\n"
  },
  {
    "tf.raw_ops.TPUOrdinalSelector": "tf.raw_ops.TPUOrdinalSelector(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TPUPartitionedCall": "tf.raw_ops.TPUPartitionedCall(\n    args, device_ordinal, Tout, f, autotuner_thresh=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.TPUPartitionedInput": "tf.raw_ops.TPUPartitionedInput(\n    inputs, partition_dim=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.TPUPartitionedOutput": "tf.raw_ops.TPUPartitionedOutput(\n    inputs, num_splits, partition_dim=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.TPUReplicateMetadata": "tf.raw_ops.TPUReplicateMetadata(\n    num_replicas,\n    num_cores_per_replica=1,\n    topology='',\n    use_tpu=True,\n    device_assignment=[],\n    computation_shape=[],\n    host_compute_core=[],\n    padding_map=[],\n    step_marker_location='STEP_MARK_AT_ENTRY',\n    allow_soft_placement=False,\n    use_spmd_for_xla_partitioning=False,\n    tpu_compile_options_proto='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TPUReplicatedInput": "tf.raw_ops.TPUReplicatedInput(\n    inputs, is_mirrored_variable=False, index=-1, is_packed=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.TPUReplicatedOutput": "tf.raw_ops.TPUReplicatedOutput(\n    input, num_replicas, name=None\n)\n"
  },
  {
    "tf.raw_ops.TakeDataset": "tf.raw_ops.TakeDataset(\n    input_dataset,\n    count,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TakeManySparseFromTensorsMap": "tf.raw_ops.TakeManySparseFromTensorsMap(\n    sparse_handles,\n    dtype,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TakeWhileDataset": "tf.raw_ops.TakeWhileDataset(\n    input_dataset,\n    other_arguments,\n    predicate,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Tan": "tf.raw_ops.Tan(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Tanh": "tf.raw_ops.Tanh(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.TanhGrad": "tf.raw_ops.TanhGrad(\n    y, dy, name=None\n)\n"
  },
  {
    "tf.raw_ops.TemporaryVariable": "tf.raw_ops.TemporaryVariable(\n    shape, dtype, var_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArray": "tf.raw_ops.TensorArray(\n    size,\n    dtype,\n    dynamic_size=False,\n    clear_after_read=True,\n    tensor_array_name='',\n    element_shape=None,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayClose": "tf.raw_ops.TensorArrayClose(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayCloseV2": "tf.raw_ops.TensorArrayCloseV2(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayCloseV3": "tf.raw_ops.TensorArrayCloseV3(\n    handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayConcat": "tf.raw_ops.TensorArrayConcat(\n    handle, flow_in, dtype, element_shape_except0=None, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayConcatV2": "tf.raw_ops.TensorArrayConcatV2(\n    handle, flow_in, dtype, element_shape_except0=None, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayConcatV3": "tf.raw_ops.TensorArrayConcatV3(\n    handle, flow_in, dtype, element_shape_except0=None, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayGather": "tf.raw_ops.TensorArrayGather(\n    handle, indices, flow_in, dtype, element_shape=None, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayGatherV2": "tf.raw_ops.TensorArrayGatherV2(\n    handle, indices, flow_in, dtype, element_shape=None, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayGatherV3": "tf.raw_ops.TensorArrayGatherV3(\n    handle, indices, flow_in, dtype, element_shape=None, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayGrad": "tf.raw_ops.TensorArrayGrad(\n    handle, flow_in, source, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayGradV2": "tf.raw_ops.TensorArrayGradV2(\n    handle, flow_in, source, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayGradV3": "tf.raw_ops.TensorArrayGradV3(\n    handle, flow_in, source, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayGradWithShape": "tf.raw_ops.TensorArrayGradWithShape(\n    handle, flow_in, shape_to_prepend, source, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayPack": "tf.raw_ops.TensorArrayPack(\n    handle, flow_in, dtype, element_shape=None, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayRead": "tf.raw_ops.TensorArrayRead(\n    handle, index, flow_in, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayReadV2": "tf.raw_ops.TensorArrayReadV2(\n    handle, index, flow_in, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayReadV3": "tf.raw_ops.TensorArrayReadV3(\n    handle, index, flow_in, dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayScatter": "tf.raw_ops.TensorArrayScatter(\n    handle, indices, value, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayScatterV2": "tf.raw_ops.TensorArrayScatterV2(\n    handle, indices, value, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayScatterV3": "tf.raw_ops.TensorArrayScatterV3(\n    handle, indices, value, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArraySize": "tf.raw_ops.TensorArraySize(\n    handle, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArraySizeV2": "tf.raw_ops.TensorArraySizeV2(\n    handle, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArraySizeV3": "tf.raw_ops.TensorArraySizeV3(\n    handle, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArraySplit": "tf.raw_ops.TensorArraySplit(\n    handle, value, lengths, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArraySplitV2": "tf.raw_ops.TensorArraySplitV2(\n    handle, value, lengths, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArraySplitV3": "tf.raw_ops.TensorArraySplitV3(\n    handle, value, lengths, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayUnpack": "tf.raw_ops.TensorArrayUnpack(\n    handle, value, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayV2": "tf.raw_ops.TensorArrayV2(\n    size,\n    dtype,\n    element_shape=None,\n    dynamic_size=False,\n    clear_after_read=True,\n    tensor_array_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayV3": "tf.raw_ops.TensorArrayV3(\n    size,\n    dtype,\n    element_shape=None,\n    dynamic_size=False,\n    clear_after_read=True,\n    identical_element_shapes=False,\n    tensor_array_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayWrite": "tf.raw_ops.TensorArrayWrite(\n    handle, index, value, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayWriteV2": "tf.raw_ops.TensorArrayWriteV2(\n    handle, index, value, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorArrayWriteV3": "tf.raw_ops.TensorArrayWriteV3(\n    handle, index, value, flow_in, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorDataset": "tf.raw_ops.TensorDataset(\n    components, output_shapes, metadata='', name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListConcat": "tf.raw_ops.TensorListConcat(\n    input_handle, element_dtype, element_shape=None, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListConcatLists": "tf.raw_ops.TensorListConcatLists(\n    input_a, input_b, element_dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListConcatV2": "tf.raw_ops.TensorListConcatV2(\n    input_handle, element_shape, leading_dims, element_dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListElementShape": "tf.raw_ops.TensorListElementShape(\n    input_handle, shape_type, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListFromTensor": "tf.raw_ops.TensorListFromTensor(\n    tensor, element_shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListGather": "tf.raw_ops.TensorListGather(\n    input_handle, indices, element_shape, element_dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListGetItem": "tf.raw_ops.TensorListGetItem(\n    input_handle, index, element_shape, element_dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListLength": "tf.raw_ops.TensorListLength(\n    input_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListPopBack": "tf.raw_ops.TensorListPopBack(\n    input_handle, element_shape, element_dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListPushBack": "tf.raw_ops.TensorListPushBack(\n    input_handle, tensor, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListPushBackBatch": "tf.raw_ops.TensorListPushBackBatch(\n    input_handles, tensor, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListReserve": "tf.raw_ops.TensorListReserve(\n    element_shape, num_elements, element_dtype, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListResize": "tf.raw_ops.TensorListResize(\n    input_handle, size, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListScatter": "tf.raw_ops.TensorListScatter(\n    tensor, indices, element_shape, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListScatterIntoExistingList": "tf.raw_ops.TensorListScatterIntoExistingList(\n    input_handle, tensor, indices, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListScatterV2": "tf.raw_ops.TensorListScatterV2(\n    tensor, indices, element_shape, num_elements, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListSetItem": "tf.raw_ops.TensorListSetItem(\n    input_handle, index, item, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListSplit": "tf.raw_ops.TensorListSplit(\n    tensor, element_shape, lengths, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorListStack": "tf.raw_ops.TensorListStack(\n    input_handle, element_shape, element_dtype, num_elements=-1, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorScatterAdd": "tf.raw_ops.TensorScatterAdd(\n    tensor, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorScatterMax": "tf.raw_ops.TensorScatterMax(\n    tensor, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorScatterMin": "tf.raw_ops.TensorScatterMin(\n    tensor, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorScatterSub": "tf.raw_ops.TensorScatterSub(\n    tensor, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorScatterUpdate": "tf.raw_ops.TensorScatterUpdate(\n    tensor, indices, updates, name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorSliceDataset": "tf.raw_ops.TensorSliceDataset(\n    components,\n    output_shapes,\n    is_files=False,\n    metadata='',\n    replicate_on_split=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorStridedSliceUpdate": "tf.raw_ops.TensorStridedSliceUpdate(\n    input,\n    begin,\n    end,\n    strides,\n    value,\n    begin_mask=0,\n    end_mask=0,\n    ellipsis_mask=0,\n    new_axis_mask=0,\n    shrink_axis_mask=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorSummary": "tf.raw_ops.TensorSummary(\n    tensor,\n    description='',\n    labels=[],\n    display_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TensorSummaryV2": "tf.raw_ops.TensorSummaryV2(\n    tag, tensor, serialized_summary_metadata, name=None\n)\n"
  },
  {
    "tf.raw_ops.TextLineDataset": "tf.raw_ops.TextLineDataset(\n    filenames, compression_type, buffer_size, metadata='', name=None\n)\n"
  },
  {
    "tf.raw_ops.TextLineReader": "tf.raw_ops.TextLineReader(\n    skip_header_lines=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.TextLineReaderV2": "tf.raw_ops.TextLineReaderV2(\n    skip_header_lines=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ThreadPoolDataset": "tf.raw_ops.ThreadPoolDataset(\n    input_dataset, thread_pool, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.ThreadPoolHandle": "tf.raw_ops.ThreadPoolHandle(\n    num_threads,\n    display_name,\n    max_intra_op_parallelism=1,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ThreadUnsafeUnigramCandidateSampler": "tf.raw_ops.ThreadUnsafeUnigramCandidateSampler(\n    true_classes,\n    num_true,\n    num_sampled,\n    unique,\n    range_max,\n    seed=0,\n    seed2=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Tile": "tf.raw_ops.Tile(\n    input, multiples, name=None\n)\n"
  },
  {
    "tf.raw_ops.TileGrad": "tf.raw_ops.TileGrad(\n    input, multiples, name=None\n)\n"
  },
  {
    "tf.raw_ops.Timestamp": "tf.raw_ops.Timestamp(\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.ToBool": "tf.raw_ops.ToBool(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.TopK": "tf.raw_ops.TopK(\n    input, k, sorted=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.TopKV2": "tf.raw_ops.TopKV2(\n    input, k, sorted=True, name=None\n)\n"
  },
  {
    "tf.raw_ops.Transpose": "tf.raw_ops.Transpose(\n    x, perm, name=None\n)\n"
  },
  {
    "tf.raw_ops.TridiagonalMatMul": "tf.raw_ops.TridiagonalMatMul(\n    superdiag, maindiag, subdiag, rhs, name=None\n)\n"
  },
  {
    "tf.raw_ops.TridiagonalSolve": "tf.raw_ops.TridiagonalSolve(\n    diagonals, rhs, partial_pivoting=True, perturb_singular=False, name=None\n)\n"
  },
  {
    "tf.raw_ops.TruncateDiv": "tf.raw_ops.TruncateDiv(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.TruncateMod": "tf.raw_ops.TruncateMod(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.TruncatedNormal": "tf.raw_ops.TruncatedNormal(\n    shape, dtype, seed=0, seed2=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.Unbatch": "tf.raw_ops.Unbatch(\n    batched_tensor,\n    batch_index,\n    id,\n    timeout_micros,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UnbatchDataset": "tf.raw_ops.UnbatchDataset(\n    input_dataset,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UnbatchGrad": "tf.raw_ops.UnbatchGrad(\n    original_input,\n    batch_index,\n    grad,\n    id,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UncompressElement": "tf.raw_ops.UncompressElement(\n    compressed, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.UnicodeDecode": "tf.raw_ops.UnicodeDecode(\n    input,\n    input_encoding,\n    errors='replace',\n    replacement_char=65533,\n    replace_control_characters=False,\n    Tsplits=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UnicodeDecodeWithOffsets": "tf.raw_ops.UnicodeDecodeWithOffsets(\n    input,\n    input_encoding,\n    errors='replace',\n    replacement_char=65533,\n    replace_control_characters=False,\n    Tsplits=tf.dtypes.int64,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UnicodeEncode": "tf.raw_ops.UnicodeEncode(\n    input_values,\n    input_splits,\n    output_encoding,\n    errors='replace',\n    replacement_char=65533,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UnicodeScript": "tf.raw_ops.UnicodeScript(\n    input, name=None\n)\n"
  },
  {
    "tf.raw_ops.UnicodeTranscode": "tf.raw_ops.UnicodeTranscode(\n    input,\n    input_encoding,\n    output_encoding,\n    errors='replace',\n    replacement_char=65533,\n    replace_control_characters=False,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UniformCandidateSampler": "tf.raw_ops.UniformCandidateSampler(\n    true_classes,\n    num_true,\n    num_sampled,\n    unique,\n    range_max,\n    seed=0,\n    seed2=0,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UniformDequantize": "tf.raw_ops.UniformDequantize(\n    input,\n    scales,\n    zero_points,\n    Tout,\n    quantization_min_val,\n    quantization_max_val,\n    quantization_axis=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UniformQuantize": "tf.raw_ops.UniformQuantize(\n    input,\n    scales,\n    zero_points,\n    Tout,\n    quantization_min_val,\n    quantization_max_val,\n    quantization_axis=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UniformQuantizedClipByValue": "tf.raw_ops.UniformQuantizedClipByValue(\n    operand,\n    min,\n    max,\n    scales,\n    zero_points,\n    quantization_min_val,\n    quantization_max_val,\n    quantization_axis=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UniformQuantizedDot": "tf.raw_ops.UniformQuantizedDot(\n    lhs,\n    rhs,\n    lhs_scales,\n    lhs_zero_points,\n    rhs_scales,\n    rhs_zero_points,\n    output_scales,\n    output_zero_points,\n    Tout,\n    lhs_quantization_min_val,\n    lhs_quantization_max_val,\n    rhs_quantization_min_val,\n    rhs_quantization_max_val,\n    output_quantization_min_val,\n    output_quantization_max_val,\n    lhs_quantization_axis=-1,\n    rhs_quantization_axis=-1,\n    output_quantization_axis=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UniformQuantizedDotHybrid": "tf.raw_ops.UniformQuantizedDotHybrid(\n    lhs,\n    rhs,\n    rhs_scales,\n    rhs_zero_points,\n    Tout,\n    rhs_quantization_min_val,\n    rhs_quantization_max_val,\n    rhs_quantization_axis=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UniformRequantize": "tf.raw_ops.UniformRequantize(\n    input,\n    input_scales,\n    input_zero_points,\n    output_scales,\n    output_zero_points,\n    Tout,\n    input_quantization_min_val,\n    input_quantization_max_val,\n    output_quantization_min_val,\n    output_quantization_max_val,\n    input_quantization_axis=-1,\n    output_quantization_axis=-1,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Unique": "tf.raw_ops.Unique(\n    x,\n    out_idx=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UniqueDataset": "tf.raw_ops.UniqueDataset(\n    input_dataset,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UniqueV2": "tf.raw_ops.UniqueV2(\n    x,\n    axis,\n    out_idx=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UniqueWithCounts": "tf.raw_ops.UniqueWithCounts(\n    x,\n    out_idx=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UniqueWithCountsV2": "tf.raw_ops.UniqueWithCountsV2(\n    x,\n    axis,\n    out_idx=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.Unpack": "tf.raw_ops.Unpack(\n    value, num, axis=0, name=None\n)\n"
  },
  {
    "tf.raw_ops.UnravelIndex": "tf.raw_ops.UnravelIndex(\n    indices, dims, name=None\n)\n"
  },
  {
    "tf.raw_ops.UnsortedSegmentJoin": "tf.raw_ops.UnsortedSegmentJoin(\n    inputs, segment_ids, num_segments, separator='', name=None\n)\n"
  },
  {
    "tf.raw_ops.UnsortedSegmentMax": "tf.raw_ops.UnsortedSegmentMax(\n    data, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.raw_ops.UnsortedSegmentMin": "tf.raw_ops.UnsortedSegmentMin(\n    data, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.raw_ops.UnsortedSegmentProd": "tf.raw_ops.UnsortedSegmentProd(\n    data, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.raw_ops.UnsortedSegmentSum": "tf.raw_ops.UnsortedSegmentSum(\n    data, segment_ids, num_segments, name=None\n)\n"
  },
  {
    "tf.raw_ops.Unstage": "tf.raw_ops.Unstage(\n    dtypes,\n    capacity=0,\n    memory_limit=0,\n    container='',\n    shared_name='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.UnwrapDatasetVariant": "tf.raw_ops.UnwrapDatasetVariant(\n    input_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.UpperBound": "tf.raw_ops.UpperBound(\n    sorted_inputs,\n    values,\n    out_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.VarHandleOp": "tf.raw_ops.VarHandleOp(\n    dtype,\n    shape,\n    container='',\n    shared_name='',\n    allowed_devices=[],\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.VarIsInitializedOp": "tf.raw_ops.VarIsInitializedOp(\n    resource, name=None\n)\n"
  },
  {
    "tf.raw_ops.Variable": "tf.raw_ops.Variable(\n    shape, dtype, container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.VariableShape": "tf.raw_ops.VariableShape(\n    input,\n    out_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.VariableV2": "tf.raw_ops.VariableV2(\n    shape, dtype, container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.Where": "tf.raw_ops.Where(\n    condition, name=None\n)\n"
  },
  {
    "tf.raw_ops.While": "tf.raw_ops.While(\n    input, cond, body, output_shapes=[], parallel_iterations=10, name=None\n)\n"
  },
  {
    "tf.raw_ops.WholeFileReader": "tf.raw_ops.WholeFileReader(\n    container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.WholeFileReaderV2": "tf.raw_ops.WholeFileReaderV2(\n    container='', shared_name='', name=None\n)\n"
  },
  {
    "tf.raw_ops.WindowDataset": "tf.raw_ops.WindowDataset(\n    input_dataset,\n    size,\n    shift,\n    stride,\n    drop_remainder,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.raw_ops.WindowOp": "tf.raw_ops.WindowOp(\n    inputs, output_types, output_shapes, name=None\n)\n"
  },
  {
    "tf.raw_ops.WorkerHeartbeat": "tf.raw_ops.WorkerHeartbeat(\n    request, name=None\n)\n"
  },
  {
    "tf.raw_ops.WrapDatasetVariant": "tf.raw_ops.WrapDatasetVariant(\n    input_handle, name=None\n)\n"
  },
  {
    "tf.raw_ops.WriteAudioSummary": "tf.raw_ops.WriteAudioSummary(\n    writer, step, tag, tensor, sample_rate, max_outputs=3, name=None\n)\n"
  },
  {
    "tf.raw_ops.WriteFile": "tf.raw_ops.WriteFile(\n    filename, contents, name=None\n)\n"
  },
  {
    "tf.raw_ops.WriteGraphSummary": "tf.raw_ops.WriteGraphSummary(\n    writer, step, tensor, name=None\n)\n"
  },
  {
    "tf.raw_ops.WriteHistogramSummary": "tf.raw_ops.WriteHistogramSummary(\n    writer, step, tag, values, name=None\n)\n"
  },
  {
    "tf.raw_ops.WriteImageSummary": "tf.raw_ops.WriteImageSummary(\n    writer, step, tag, tensor, bad_color, max_images=3, name=None\n)\n"
  },
  {
    "tf.raw_ops.WriteRawProtoSummary": "tf.raw_ops.WriteRawProtoSummary(\n    writer, step, tensor, name=None\n)\n"
  },
  {
    "tf.raw_ops.WriteScalarSummary": "tf.raw_ops.WriteScalarSummary(\n    writer, step, tag, value, name=None\n)\n"
  },
  {
    "tf.raw_ops.WriteSummary": "tf.raw_ops.WriteSummary(\n    writer, step, tensor, tag, summary_metadata, name=None\n)\n"
  },
  {
    "tf.raw_ops.Xdivy": "tf.raw_ops.Xdivy(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.XlaConcatND": "tf.raw_ops.XlaConcatND(\n    inputs, num_concats, paddings=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.XlaSplitND": "tf.raw_ops.XlaSplitND(\n    input, N, num_splits, paddings=[], name=None\n)\n"
  },
  {
    "tf.raw_ops.Xlog1py": "tf.raw_ops.Xlog1py(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.Xlogy": "tf.raw_ops.Xlogy(\n    x, y, name=None\n)\n"
  },
  {
    "tf.raw_ops.ZerosLike": "tf.raw_ops.ZerosLike(\n    x, name=None\n)\n"
  },
  {
    "tf.raw_ops.Zeta": "tf.raw_ops.Zeta(\n    x, q, name=None\n)\n"
  },
  {
    "tf.raw_ops.ZipDataset": "tf.raw_ops.ZipDataset(\n    input_datasets,\n    output_types,\n    output_shapes,\n    metadata='',\n    name=None\n)\n"
  },
  {
    "tf.realdiv": "tf.realdiv(\n    x, y, name=None\n)\n"
  },
  {
    "tf.recompute_grad": "tf.recompute_grad(\n    f\n)\n"
  },
  {
    "tf.math.reduce_all": "tf.math.reduce_all(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_any": "tf.math.reduce_any(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_logsumexp": "tf.math.reduce_logsumexp(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_max": "tf.math.reduce_max(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_mean": "tf.math.reduce_mean(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_min": "tf.math.reduce_min(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_prod": "tf.math.reduce_prod(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.math.reduce_sum": "tf.math.reduce_sum(\n    input_tensor, axis=None, keepdims=False, name=None\n)\n"
  },
  {
    "tf.register_tensor_conversion_function": "tf.register_tensor_conversion_function(\n    base_type, conversion_func, priority=100\n)\n"
  },
  {
    "tf.repeat": "tf.repeat(\n    input, repeats, axis=None, name=None\n)\n"
  },
  {
    "tf.required_space_to_batch_paddings": "tf.required_space_to_batch_paddings(\n    input_shape, block_shape, base_paddings=None, name=None\n)\n"
  },
  {
    "tf.reshape": "tf.reshape(\n    tensor, shape, name=None\n)\n"
  },
  {
    "tf.reverse": "tf.reverse(\n    tensor, axis, name=None\n)\n"
  },
  {
    "tf.reverse_sequence": "tf.reverse_sequence(\n    input, seq_lengths, seq_axis=None, batch_axis=None, name=None\n)\n"
  },
  {
    "tf.roll": "tf.roll(\n    input, shift, axis, name=None\n)\n"
  },
  {
    "tf.math.round": "tf.math.round(\n    x, name=None\n)\n"
  },
  {
    "tf.dtypes.saturate_cast": "tf.dtypes.saturate_cast(\n    value, dtype, name=None\n)\n"
  },
  {
    "tf.saved_model.Asset": "tf.saved_model.Asset(\n    path\n)\n"
  },
  {
    "tf.saved_model.LoadOptions": "tf.saved_model.LoadOptions(\n    allow_partial_checkpoint=False,\n    experimental_io_device=None,\n    experimental_skip_checkpoint=False,\n    experimental_variable_policy=None\n)\n"
  },
  {
    "tf.saved_model.SaveOptions": "tf.saved_model.SaveOptions(\n    namespace_whitelist=None,\n    save_debug_info=False,\n    function_aliases=None,\n    experimental_io_device=None,\n    experimental_variable_policy=None,\n    experimental_custom_gradients=True\n)\n"
  },
  {
    "tf.saved_model.contains_saved_model": "tf.saved_model.contains_saved_model(\n    export_dir\n)\n"
  },
  {
    "tf.saved_model.experimental.TrackableResource": "tf.saved_model.experimental.TrackableResource(\n    device=''\n)\n"
  },
  {
    "tf.saved_model.load": "tf.saved_model.load(\n    export_dir, tags=None, options=None\n)\n"
  },
  {
    "tf.saved_model.save": "tf.saved_model.save(\n    obj, export_dir, signatures=None, options=None\n)\n"
  },
  {
    "tf.math.scalar_mul": "tf.math.scalar_mul(\n    scalar, x, name=None\n)\n"
  },
  {
    "tf.scan": "tf.scan(\n    fn,\n    elems,\n    initializer=None,\n    parallel_iterations=10,\n    back_prop=True,\n    swap_memory=False,\n    infer_shape=True,\n    reverse=False,\n    name=None\n)\n"
  },
  {
    "tf.scatter_nd": "tf.scatter_nd(\n    indices, updates, shape, name=None\n)\n"
  },
  {
    "tf.searchsorted": "tf.searchsorted(\n    sorted_sequence,\n    values,\n    side='left',\n    out_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.sequence_mask": "tf.sequence_mask(\n    lengths,\n    maxlen=None,\n    dtype=tf.dtypes.bool,\n    name=None\n)\n"
  },
  {
    "tf.sets.difference": "tf.sets.difference(\n    a, b, aminusb=True, validate_indices=True\n)\n"
  },
  {
    "tf.sets.intersection": "tf.sets.intersection(\n    a, b, validate_indices=True\n)\n"
  },
  {
    "tf.sets.size": "tf.sets.size(\n    a, validate_indices=True\n)\n"
  },
  {
    "tf.sets.union": "tf.sets.union(\n    a, b, validate_indices=True\n)\n"
  },
  {
    "tf.shape": "tf.shape(\n    input,\n    out_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.shape_n": "tf.shape_n(\n    input,\n    out_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.math.sigmoid": "tf.math.sigmoid(\n    x, name=None\n)\n"
  },
  {
    "tf.math.sign": "tf.math.sign(\n    x, name=None\n)\n"
  },
  {
    "tf.signal.dct": "tf.signal.dct(\n    input, type=2, n=None, axis=-1, norm=None, name=None\n)\n"
  },
  {
    "tf.signal.fft": "tf.signal.fft(\n    input, name=None\n)\n"
  },
  {
    "tf.signal.fft2d": "tf.signal.fft2d(\n    input, name=None\n)\n"
  },
  {
    "tf.signal.fft3d": "tf.signal.fft3d(\n    input, name=None\n)\n"
  },
  {
    "tf.signal.fftshift": "tf.signal.fftshift(\n    x, axes=None, name=None\n)\n"
  },
  {
    "tf.signal.frame": "tf.signal.frame(\n    signal,\n    frame_length,\n    frame_step,\n    pad_end=False,\n    pad_value=0,\n    axis=-1,\n    name=None\n)\n"
  },
  {
    "tf.signal.hamming_window": "tf.signal.hamming_window(\n    window_length,\n    periodic=True,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.signal.hann_window": "tf.signal.hann_window(\n    window_length,\n    periodic=True,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.signal.idct": "tf.signal.idct(\n    input, type=2, n=None, axis=-1, norm=None, name=None\n)\n"
  },
  {
    "tf.signal.ifft": "tf.signal.ifft(\n    input, name=None\n)\n"
  },
  {
    "tf.signal.ifft2d": "tf.signal.ifft2d(\n    input, name=None\n)\n"
  },
  {
    "tf.signal.ifft3d": "tf.signal.ifft3d(\n    input, name=None\n)\n"
  },
  {
    "tf.signal.ifftshift": "tf.signal.ifftshift(\n    x, axes=None, name=None\n)\n"
  },
  {
    "tf.signal.inverse_mdct": "tf.signal.inverse_mdct(\n    mdcts,\n    window_fn=tf.signal.vorbis_window,\n    norm=None,\n    name=None\n)\n"
  },
  {
    "tf.signal.inverse_stft": "tf.signal.inverse_stft(\n    stfts,\n    frame_length,\n    frame_step,\n    fft_length=None,\n    window_fn=tf.signal.hann_window,\n    name=None\n)\n"
  },
  {
    "tf.signal.inverse_stft_window_fn": "tf.signal.inverse_stft_window_fn(\n    frame_step,\n    forward_window_fn=tf.signal.hann_window,\n    name=None\n)\n"
  },
  {
    "tf.signal.irfft": "tf.signal.irfft(\n    input_tensor, fft_length=None, name=None\n)\n"
  },
  {
    "tf.signal.irfft2d": "tf.signal.irfft2d(\n    input_tensor, fft_length=None, name=None\n)\n"
  },
  {
    "tf.signal.irfft3d": "tf.signal.irfft3d(\n    input_tensor, fft_length=None, name=None\n)\n"
  },
  {
    "tf.signal.kaiser_bessel_derived_window": "tf.signal.kaiser_bessel_derived_window(\n    window_length,\n    beta=12.0,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.signal.kaiser_window": "tf.signal.kaiser_window(\n    window_length,\n    beta=12.0,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.signal.linear_to_mel_weight_matrix": "tf.signal.linear_to_mel_weight_matrix(\n    num_mel_bins=20,\n    num_spectrogram_bins=129,\n    sample_rate=8000,\n    lower_edge_hertz=125.0,\n    upper_edge_hertz=3800.0,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.signal.mdct": "tf.signal.mdct(\n    signals,\n    frame_length,\n    window_fn=tf.signal.vorbis_window,\n    pad_end=False,\n    norm=None,\n    name=None\n)\n"
  },
  {
    "tf.signal.mfccs_from_log_mel_spectrograms": "tf.signal.mfccs_from_log_mel_spectrograms(\n    log_mel_spectrograms, name=None\n)\n"
  },
  {
    "tf.signal.overlap_and_add": "tf.signal.overlap_and_add(\n    signal, frame_step, name=None\n)\n"
  },
  {
    "tf.signal.rfft": "tf.signal.rfft(\n    input_tensor, fft_length=None, name=None\n)\n"
  },
  {
    "tf.signal.rfft2d": "tf.signal.rfft2d(\n    input_tensor, fft_length=None, name=None\n)\n"
  },
  {
    "tf.signal.rfft3d": "tf.signal.rfft3d(\n    input_tensor, fft_length=None, name=None\n)\n"
  },
  {
    "tf.signal.stft": "tf.signal.stft(\n    signals,\n    frame_length,\n    frame_step,\n    fft_length=None,\n    window_fn=tf.signal.hann_window,\n    pad_end=False,\n    name=None\n)\n"
  },
  {
    "tf.signal.vorbis_window": "tf.signal.vorbis_window(\n    window_length,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.math.sin": "tf.math.sin(\n    x, name=None\n)\n"
  },
  {
    "tf.math.sinh": "tf.math.sinh(\n    x, name=None\n)\n"
  },
  {
    "tf.size": "tf.size(\n    input,\n    out_type=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.slice": "tf.slice(\n    input_, begin, size, name=None\n)\n"
  },
  {
    "tf.sort": "tf.sort(\n    values, axis=-1, direction='ASCENDING', name=None\n)\n"
  },
  {
    "tf.space_to_batch": "tf.space_to_batch(\n    input, block_shape, paddings, name=None\n)\n"
  },
  {
    "tf.space_to_batch_nd": "tf.space_to_batch_nd(\n    input, block_shape, paddings, name=None\n)\n"
  },
  {
    "tf.sparse.SparseTensor": "tf.sparse.SparseTensor(\n    indices, values, dense_shape\n)\n"
  },
  {
    "tf.sparse.add": "tf.sparse.add(\n    a, b, threshold=0\n)\n"
  },
  {
    "tf.sparse.bincount": "tf.sparse.bincount(\n    values,\n    weights=None,\n    axis=0,\n    minlength=None,\n    maxlength=None,\n    binary_output=False,\n    name=None\n)\n"
  },
  {
    "tf.sparse.concat": "tf.sparse.concat(\n    axis, sp_inputs, expand_nonconcat_dims=False, name=None\n)\n"
  },
  {
    "tf.sparse.cross": "tf.sparse.cross(\n    inputs, name=None, separator=None\n)\n"
  },
  {
    "tf.sparse.cross_hashed": "tf.sparse.cross_hashed(\n    inputs, num_buckets=0, hash_key=None, name=None\n)\n"
  },
  {
    "tf.sparse.expand_dims": "tf.sparse.expand_dims(\n    sp_input, axis=None, name=None\n)\n"
  },
  {
    "tf.sparse.eye": "tf.sparse.eye(\n    num_rows,\n    num_columns=None,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.sparse.fill_empty_rows": "tf.sparse.fill_empty_rows(\n    sp_input, default_value, name=None\n)\n"
  },
  {
    "tf.sparse.from_dense": "tf.sparse.from_dense(\n    tensor, name=None\n)\n"
  },
  {
    "tf.sparse.map_values": "tf.sparse.map_values(\n    op, *args, **kwargs\n)\n"
  },
  {
    "tf.sparse.mask": "tf.sparse.mask(\n    a, mask_indices, name=None\n)\n"
  },
  {
    "tf.sparse.maximum": "tf.sparse.maximum(\n    sp_a, sp_b, name=None\n)\n"
  },
  {
    "tf.sparse.minimum": "tf.sparse.minimum(\n    sp_a, sp_b, name=None\n)\n"
  },
  {
    "tf.sparse.reduce_max": "tf.sparse.reduce_max(\n    sp_input, axis=None, keepdims=None, output_is_sparse=False, name=None\n)\n"
  },
  {
    "tf.sparse.reduce_sum": "tf.sparse.reduce_sum(\n    sp_input, axis=None, keepdims=None, output_is_sparse=False, name=None\n)\n"
  },
  {
    "tf.sparse.reorder": "tf.sparse.reorder(\n    sp_input, name=None\n)\n"
  },
  {
    "tf.sparse.reset_shape": "tf.sparse.reset_shape(\n    sp_input, new_shape=None\n)\n"
  },
  {
    "tf.sparse.reshape": "tf.sparse.reshape(\n    sp_input, shape, name=None\n)\n"
  },
  {
    "tf.sparse.retain": "tf.sparse.retain(\n    sp_input, to_retain\n)\n"
  },
  {
    "tf.sparse.segment_mean": "tf.sparse.segment_mean(\n    data, indices, segment_ids, num_segments=None, name=None\n)\n"
  },
  {
    "tf.sparse.segment_sqrt_n": "tf.sparse.segment_sqrt_n(\n    data, indices, segment_ids, num_segments=None, name=None\n)\n"
  },
  {
    "tf.sparse.segment_sum": "tf.sparse.segment_sum(\n    data, indices, segment_ids, num_segments=None, name=None\n)\n"
  },
  {
    "tf.sparse.slice": "tf.sparse.slice(\n    sp_input, start, size, name=None\n)\n"
  },
  {
    "tf.sparse.softmax": "tf.sparse.softmax(\n    sp_input, name=None\n)\n"
  },
  {
    "tf.sparse.sparse_dense_matmul": "tf.sparse.sparse_dense_matmul(\n    sp_a, b, adjoint_a=False, adjoint_b=False, name=None\n)\n"
  },
  {
    "tf.sparse.split": "tf.sparse.split(\n    sp_input=None, num_split=None, axis=None, name=None\n)\n"
  },
  {
    "tf.sparse.to_dense": "tf.sparse.to_dense(\n    sp_input, default_value=None, validate_indices=True, name=None\n)\n"
  },
  {
    "tf.sparse.to_indicator": "tf.sparse.to_indicator(\n    sp_input, vocab_size, name=None\n)\n"
  },
  {
    "tf.sparse.transpose": "tf.sparse.transpose(\n    sp_input, perm=None, name=None\n)\n"
  },
  {
    "tf.split": "tf.split(\n    value, num_or_size_splits, axis=0, num=None, name='split'\n)\n"
  },
  {
    "tf.math.sqrt": "tf.math.sqrt(\n    x, name=None\n)\n"
  },
  {
    "tf.math.square": "tf.math.square(\n    x, name=None\n)\n"
  },
  {
    "tf.squeeze": "tf.squeeze(\n    input, axis=None, name=None\n)\n"
  },
  {
    "tf.stack": "tf.stack(\n    values, axis=0, name='stack'\n)\n"
  },
  {
    "tf.stop_gradient": "tf.stop_gradient(\n    input, name=None\n)\n"
  },
  {
    "tf.strided_slice": "tf.strided_slice(\n    input_,\n    begin,\n    end,\n    strides=None,\n    begin_mask=0,\n    end_mask=0,\n    ellipsis_mask=0,\n    new_axis_mask=0,\n    shrink_axis_mask=0,\n    var=None,\n    name=None\n)\n"
  },
  {
    "tf.strings.as_string": "tf.strings.as_string(\n    input,\n    precision=-1,\n    scientific=False,\n    shortest=False,\n    width=-1,\n    fill='',\n    name=None\n)\n"
  },
  {
    "tf.strings.bytes_split": "tf.strings.bytes_split(\n    input, name=None\n)\n"
  },
  {
    "tf.strings.format": "tf.strings.format(\n    template, inputs, placeholder='{}', summarize=3, name=None\n)\n"
  },
  {
    "tf.strings.join": "tf.strings.join(\n    inputs, separator='', name=None\n)\n"
  },
  {
    "tf.strings.length": "tf.strings.length(\n    input, unit='BYTE', name=None\n)\n"
  },
  {
    "tf.strings.lower": "tf.strings.lower(\n    input, encoding='', name=None\n)\n"
  },
  {
    "tf.strings.ngrams": "tf.strings.ngrams(\n    data,\n    ngram_width,\n    separator=' ',\n    pad_values=None,\n    padding_width=None,\n    preserve_short_sequences=False,\n    name=None\n)\n"
  },
  {
    "tf.strings.reduce_join": "tf.strings.reduce_join(\n    inputs, axis=None, keepdims=False, separator='', name=None\n)\n"
  },
  {
    "tf.strings.regex_full_match": "tf.strings.regex_full_match(\n    input, pattern, name=None\n)\n"
  },
  {
    "tf.strings.regex_replace": "tf.strings.regex_replace(\n    input, pattern, rewrite, replace_global=True, name=None\n)\n"
  },
  {
    "tf.strings.split": "tf.strings.split(\n    input, sep=None, maxsplit=-1, name=None\n)\n"
  },
  {
    "tf.strings.strip": "tf.strings.strip(\n    input, name=None\n)\n"
  },
  {
    "tf.strings.substr": "tf.strings.substr(\n    input, pos, len, unit='BYTE', name=None\n)\n"
  },
  {
    "tf.strings.to_hash_bucket": "tf.strings.to_hash_bucket(\n    input, num_buckets, name=None\n)\n"
  },
  {
    "tf.strings.to_hash_bucket_fast": "tf.strings.to_hash_bucket_fast(\n    input, num_buckets, name=None\n)\n"
  },
  {
    "tf.strings.to_hash_bucket_strong": "tf.strings.to_hash_bucket_strong(\n    input, num_buckets, key, name=None\n)\n"
  },
  {
    "tf.strings.to_number": "tf.strings.to_number(\n    input,\n    out_type=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.strings.unicode_decode": "tf.strings.unicode_decode(\n    input,\n    input_encoding,\n    errors='replace',\n    replacement_char=65533,\n    replace_control_characters=False,\n    name=None\n)\n"
  },
  {
    "tf.strings.unicode_decode_with_offsets": "tf.strings.unicode_decode_with_offsets(\n    input,\n    input_encoding,\n    errors='replace',\n    replacement_char=65533,\n    replace_control_characters=False,\n    name=None\n)\n"
  },
  {
    "tf.strings.unicode_encode": "tf.strings.unicode_encode(\n    input,\n    output_encoding,\n    errors='replace',\n    replacement_char=65533,\n    name=None\n)\n"
  },
  {
    "tf.strings.unicode_script": "tf.strings.unicode_script(\n    input, name=None\n)\n"
  },
  {
    "tf.strings.unicode_split": "tf.strings.unicode_split(\n    input,\n    input_encoding,\n    errors='replace',\n    replacement_char=65533,\n    name=None\n)\n"
  },
  {
    "tf.strings.unicode_split_with_offsets": "tf.strings.unicode_split_with_offsets(\n    input,\n    input_encoding,\n    errors='replace',\n    replacement_char=65533,\n    name=None\n)\n"
  },
  {
    "tf.strings.unicode_transcode": "tf.strings.unicode_transcode(\n    input,\n    input_encoding,\n    output_encoding,\n    errors='replace',\n    replacement_char=65533,\n    replace_control_characters=False,\n    name=None\n)\n"
  },
  {
    "tf.strings.unsorted_segment_join": "tf.strings.unsorted_segment_join(\n    inputs, segment_ids, num_segments, separator='', name=None\n)\n"
  },
  {
    "tf.strings.upper": "tf.strings.upper(\n    input, encoding='', name=None\n)\n"
  },
  {
    "tf.math.subtract": "tf.math.subtract(\n    x, y, name=None\n)\n"
  },
  {
    "tf.summary.audio": "tf.summary.audio(\n    name,\n    data,\n    sample_rate,\n    step=None,\n    max_outputs=3,\n    encoding=None,\n    description=None\n)\n"
  },
  {
    "tf.summary.create_file_writer": "tf.summary.create_file_writer(\n    logdir,\n    max_queue=None,\n    flush_millis=None,\n    filename_suffix=None,\n    name=None,\n    experimental_trackable=False\n)\n"
  },
  {
    "tf.summary.flush": "tf.summary.flush(\n    writer=None, name=None\n)\n"
  },
  {
    "tf.summary.graph": "tf.summary.graph(\n    graph_data\n)\n"
  },
  {
    "tf.summary.histogram": "tf.summary.histogram(\n    name, data, step=None, buckets=None, description=None\n)\n"
  },
  {
    "tf.summary.image": "tf.summary.image(\n    name, data, step=None, max_outputs=3, description=None\n)\n"
  },
  {
    "tf.summary.scalar": "tf.summary.scalar(\n    name, data, step=None, description=None\n)\n"
  },
  {
    "tf.summary.text": "tf.summary.text(\n    name, data, step=None, description=None\n)\n"
  },
  {
    "tf.summary.trace_export": "tf.summary.trace_export(\n    name, step=None, profiler_outdir=None\n)\n"
  },
  {
    "tf.summary.trace_on": "tf.summary.trace_on(\n    graph=True, profiler=False\n)\n"
  },
  {
    "tf.summary.write": "tf.summary.write(\n    tag, tensor, step=None, metadata=None, name=None\n)\n"
  },
  {
    "tf.switch_case": "tf.switch_case(\n    branch_index, branch_fns, default=None, name='switch_case'\n)\n"
  },
  {
    "tf.math.tan": "tf.math.tan(\n    x, name=None\n)\n"
  },
  {
    "tf.math.tanh": "tf.math.tanh(\n    x, name=None\n)\n"
  },
  {
    "tf.tensor_scatter_nd_add": "tf.tensor_scatter_nd_add(\n    tensor, indices, updates, name=None\n)\n"
  },
  {
    "tf.tensor_scatter_nd_max": "tf.tensor_scatter_nd_max(\n    tensor, indices, updates, name=None\n)\n"
  },
  {
    "tf.tensor_scatter_nd_min": "tf.tensor_scatter_nd_min(\n    tensor, indices, updates, name=None\n)\n"
  },
  {
    "tf.tensor_scatter_nd_sub": "tf.tensor_scatter_nd_sub(\n    tensor, indices, updates, name=None\n)\n"
  },
  {
    "tf.tensor_scatter_nd_update": "tf.tensor_scatter_nd_update(\n    tensor, indices, updates, name=None\n)\n"
  },
  {
    "tf.tensordot": "tf.tensordot(\n    a, b, axes, name=None\n)\n"
  },
  {
    "tf.test.TestCase": "tf.test.TestCase(\n    methodName='runTest'\n)\n"
  },
  {
    "tf.test.TestCase.failureException": "tf.test.TestCase.failureException(\n    *args, **kwargs\n)\n"
  },
  {
    "tf.test.assert_equal_graph_def": "tf.test.assert_equal_graph_def(\n    expected, actual\n)\n"
  },
  {
    "tf.test.compute_gradient": "tf.test.compute_gradient(\n    f, x, delta=None\n)\n"
  },
  {
    "tf.test.create_local_cluster": "tf.test.create_local_cluster(\n    num_workers,\n    num_ps,\n    protocol='grpc',\n    worker_config=None,\n    ps_config=None\n)\n"
  },
  {
    "tf.test.disable_with_predicate": "tf.test.disable_with_predicate(\n    pred, skip_message\n)\n"
  },
  {
    "tf.test.is_gpu_available": "tf.test.is_gpu_available(\n    cuda_only=False, min_cuda_compute_capability=None\n)\n"
  },
  {
    "tf.test.main": "tf.test.main(\n    argv=None\n)\n"
  },
  {
    "tf.test.with_eager_op_as_function": "tf.test.with_eager_op_as_function(\n    cls=None, only_as_function=False\n)\n"
  },
  {
    "tf.tile": "tf.tile(\n    input, multiples, name=None\n)\n"
  },
  {
    "tf.timestamp": "tf.timestamp(\n    name=None\n)\n"
  },
  {
    "tf.tpu.XLAOptions": "tf.tpu.XLAOptions(\n    use_spmd_for_xla_partitioning=True, enable_xla_dynamic_padder=True\n)\n"
  },
  {
    "tf.tpu.experimental.DeviceAssignment": "tf.tpu.experimental.DeviceAssignment(\n    topology: tf.tpu.experimental.Topology,\n    core_assignment: np.ndarray\n)\n"
  },
  {
    "tf.tpu.experimental.HardwareFeature": "tf.tpu.experimental.HardwareFeature(\n    tpu_hardware_feature_proto\n)\n"
  },
  {
    "tf.tpu.experimental.TPUSystemMetadata": "tf.tpu.experimental.TPUSystemMetadata(\n    num_cores, num_hosts, num_of_cores_per_host, topology, devices\n)\n"
  },
  {
    "tf.tpu.experimental.Topology": "tf.tpu.experimental.Topology(\n    serialized=None, mesh_shape=None, device_coordinates=None\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.Adagrad": "tf.tpu.experimental.embedding.Adagrad(\n    learning_rate: Union[float, Callable[[], float]] = 0.001,\n    initial_accumulator_value: float = 0.1,\n    use_gradient_accumulation: bool = True,\n    clip_weight_min: Optional[float] = None,\n    clip_weight_max: Optional[float] = None,\n    weight_decay_factor: Optional[float] = None,\n    multiply_weight_decay_factor_by_learning_rate: bool = None,\n    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,\n    clipvalue: Optional[ClipValueType] = None\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.AdagradMomentum": "tf.tpu.experimental.embedding.AdagradMomentum(\n    learning_rate: Union[float, Callable[[], float]] = 0.001,\n    momentum: float = 0.0,\n    use_nesterov: bool = False,\n    exponent: float = 2,\n    beta2: float = 1,\n    epsilon: float = 1e-10,\n    use_gradient_accumulation: bool = True,\n    clip_weight_min: Optional[float] = None,\n    clip_weight_max: Optional[float] = None,\n    weight_decay_factor: Optional[float] = None,\n    multiply_weight_decay_factor_by_learning_rate: bool = None,\n    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,\n    clipvalue: Optional[ClipValueType] = None\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.Adam": "tf.tpu.experimental.embedding.Adam(\n    learning_rate: Union[float, Callable[[], float]] = 0.001,\n    beta_1: float = 0.9,\n    beta_2: float = 0.999,\n    epsilon: float = 1e-07,\n    lazy_adam: bool = True,\n    sum_inside_sqrt: bool = True,\n    use_gradient_accumulation: bool = True,\n    clip_weight_min: Optional[float] = None,\n    clip_weight_max: Optional[float] = None,\n    weight_decay_factor: Optional[float] = None,\n    multiply_weight_decay_factor_by_learning_rate: bool = None,\n    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,\n    clipvalue: Optional[ClipValueType] = None\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.FTRL": "tf.tpu.experimental.embedding.FTRL(\n    learning_rate: Union[float, Callable[[], float]] = 0.001,\n    learning_rate_power: float = -0.5,\n    l1_regularization_strength: float = 0.0,\n    l2_regularization_strength: float = 0.0,\n    beta: float = 0.0,\n    initial_accumulator_value: float = 0.1,\n    use_gradient_accumulation: bool = True,\n    clip_weight_min: Optional[float] = None,\n    clip_weight_max: Optional[float] = None,\n    weight_decay_factor: Optional[float] = None,\n    multiply_weight_decay_factor_by_learning_rate: bool = None,\n    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,\n    clipvalue: Optional[ClipValueType] = None,\n    multiply_linear_by_learning_rate: bool = False,\n    allow_zero_accumulator: bool = False\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.FeatureConfig": "tf.tpu.experimental.embedding.FeatureConfig(\n    table: tf.tpu.experimental.embedding.TableConfig,\n    max_sequence_length: int = 0,\n    validate_weights_and_indices: bool = True,\n    output_shape: Optional[Union[List[int], tf.TensorShape]] = None,\n    name: Optional[Text] = None\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.QuantizationConfig": "tf.tpu.experimental.embedding.QuantizationConfig(\n    num_buckets: int, lower: float, upper: float\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.SGD": "tf.tpu.experimental.embedding.SGD(\n    learning_rate: Union[float, Callable[[], float]] = 0.01,\n    use_gradient_accumulation: bool = True,\n    clip_weight_min: Optional[float] = None,\n    clip_weight_max: Optional[float] = None,\n    weight_decay_factor: Optional[float] = None,\n    multiply_weight_decay_factor_by_learning_rate: bool = None,\n    clipvalue: Optional[ClipValueType] = None\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.TPUEmbedding": "tf.tpu.experimental.embedding.TPUEmbedding(\n    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable],\n    optimizer: Optional[tpu_embedding_v2_utils._Optimizer],\n    pipeline_execution_with_tensor_core: bool = False\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.TPUEmbeddingForServing": "tf.tpu.experimental.embedding.TPUEmbeddingForServing(\n    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable],\n    optimizer: Optional[tpu_embedding_v2_utils._Optimizer]\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.TPUEmbeddingV0": "tf.tpu.experimental.embedding.TPUEmbeddingV0(\n    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable],\n    optimizer: Optional[tpu_embedding_v2_utils._Optimizer]\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.TableConfig": "tf.tpu.experimental.embedding.TableConfig(\n    vocabulary_size: int,\n    dim: int,\n    initializer: Optional[Callable[[Any], None]] = None,\n    optimizer: Optional[_Optimizer] = None,\n    combiner: Text = 'mean',\n    name: Optional[Text] = None,\n    quantization_config: tf.tpu.experimental.embedding.QuantizationConfig = None\n)\n"
  },
  {
    "tf.tpu.experimental.embedding.serving_embedding_lookup": "tf.tpu.experimental.embedding.serving_embedding_lookup(\n    inputs: Any,\n    weights: Optional[Any],\n    tables: Dict[tf.tpu.experimental.embedding.TableConfig, tf.Variable],\n    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable]\n) -> Any\n"
  },
  {
    "tf.tpu.experimental.initialize_tpu_system": "tf.tpu.experimental.initialize_tpu_system(\n    cluster_resolver=None\n)\n"
  },
  {
    "tf.tpu.experimental.shutdown_tpu_system": "tf.tpu.experimental.shutdown_tpu_system(\n    cluster_resolver=None\n)\n"
  },
  {
    "tf.io.parse_example": "tf.io.parse_example("
  },
  {
    "tf.train.Checkpoint": "tf.train.Checkpoint(\n    root=None, **kwargs\n)\n"
  },
  {
    "tf.train.CheckpointManager": "tf.train.CheckpointManager(\n    checkpoint,\n    directory,\n    max_to_keep,\n    keep_checkpoint_every_n_hours=None,\n    checkpoint_name='ckpt',\n    step_counter=None,\n    checkpoint_interval=None,\n    init_fn=None\n)\n"
  },
  {
    "tf.train.CheckpointOptions": "tf.train.CheckpointOptions(\n    experimental_io_device=None, experimental_enable_async_checkpoint=False\n)\n"
  },
  {
    "tf.train.CheckpointView": "tf.train.CheckpointView(\n    save_path\n)\n"
  },
  {
    "tf.train.ClusterSpec": "tf.train.ClusterSpec(\n    cluster\n)\n"
  },
  {
    "tf.train.Coordinator": "tf.train.Coordinator(\n    clean_stop_exception_types=None\n)\n"
  },
  {
    "tf.io.parse_example": "tf.io.parse_example("
  },
  {
    "tf.train.ExponentialMovingAverage": "tf.train.ExponentialMovingAverage(\n    decay,\n    num_updates=None,\n    zero_debias=False,\n    name='ExponentialMovingAverage'\n)\n"
  },
  {
    "tf.io.parse_example": "tf.io.parse_example("
  },
  {
    "tf.io.parse_example": "tf.io.parse_example("
  },
  {
    "tf.io.parse_example": "tf.io.parse_example("
  },
  {
    "tf.io.parse_example": "tf.io.parse_example("
  },
  {
    "tf.train.TrackableView": "tf.train.TrackableView(\n    root\n)\n"
  },
  {
    "tf.train.checkpoints_iterator": "tf.train.checkpoints_iterator(\n    checkpoint_dir, min_interval_secs=0, timeout=None, timeout_fn=None\n)\n"
  },
  {
    "tf.train.get_checkpoint_state": "tf.train.get_checkpoint_state(\n    checkpoint_dir, latest_filename=None\n)\n"
  },
  {
    "tf.train.latest_checkpoint": "tf.train.latest_checkpoint(\n    checkpoint_dir, latest_filename=None\n)\n"
  },
  {
    "tf.train.list_variables": "tf.train.list_variables(\n    ckpt_dir_or_file\n)\n"
  },
  {
    "tf.train.load_checkpoint": "tf.train.load_checkpoint(\n    ckpt_dir_or_file\n)\n"
  },
  {
    "tf.train.load_variable": "tf.train.load_variable(\n    ckpt_dir_or_file, name\n)\n"
  },
  {
    "tf.transpose": "tf.transpose(\n    a, perm=None, conjugate=False, name='transpose'\n)\n"
  },
  {
    "tf.math.truediv": "tf.math.truediv(\n    x, y, name=None\n)\n"
  },
  {
    "tf.truncatediv": "tf.truncatediv(\n    x, y, name=None\n)\n"
  },
  {
    "tf.truncatemod": "tf.truncatemod(\n    x, y, name=None\n)\n"
  },
  {
    "tf.tuple": "tf.tuple(\n    tensors, control_inputs=None, name=None\n)\n"
  },
  {
    "tf.type_spec_from_value": "tf.type_spec_from_value(\n    value\n) -> tf.TypeSpec\n"
  },
  {
    "tf.unique": "tf.unique(\n    x,\n    out_idx=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.unique_with_counts": "tf.unique_with_counts(\n    x,\n    out_idx=tf.dtypes.int32,\n    name=None\n)\n"
  },
  {
    "tf.unravel_index": "tf.unravel_index(\n    indices, dims, name=None\n)\n"
  },
  {
    "tf.unstack": "tf.unstack(\n    value, num=None, axis=0, name='unstack'\n)\n"
  },
  {
    "tf.vectorized_map": "tf.vectorized_map(\n    fn, elems, fallback_to_while_loop=True, warn=True\n)\n"
  },
  {
    "tf.where": "tf.where(\n    condition, x=None, y=None, name=None\n)\n"
  },
  {
    "tf.while_loop": "tf.while_loop(\n    cond,\n    body,\n    loop_vars,\n    shape_invariants=None,\n    parallel_iterations=10,\n    back_prop=True,\n    swap_memory=False,\n    maximum_iterations=None,\n    name=None\n)\n"
  },
  {
    "tf.xla.experimental.compile": "tf.xla.experimental.compile(\n    computation, inputs=None\n)\n"
  },
  {
    "tf.zeros": "tf.zeros(\n    shape,\n    dtype=tf.dtypes.float32,\n    name=None\n)\n"
  },
  {
    "tf.zeros_like": "tf.zeros_like(\n    input, dtype=None, name=None\n)\n"
  }
]
