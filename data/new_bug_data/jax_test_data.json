[{
    "Id": 0,
    "commit_link": "https://github.com/google/jax/commit/5e418f5ab2692d4791816e85ed82eb0834a579cb",
    "date": "2024-07-25T13:16:34-07:00",
    "message": "Added argument validation to mosaic_gpu_init_tma_desc\n\nThis should help with understanding cuTensorMapEncodeTiled failures, since\nCUDA doesn't provide any details beyond the error return code.\n\nNote that this change also ensures that TMA descriptors are 64-byte aligned.\n\nPiperOrigin-RevId: 656062820",
    "changes": [
        {
            "name": "__init__.py",
            "path": "jax/experimental/mosaic/gpu/__init__.py",
            "patches": [
                {
                    "old_start": 733,
                    "old_length": 7,
                    "new_start": 733,
                    "new_length": 9,
                    "hunk": "@@ -733,7 +733,9 @@ def _lower_as_gpu_kernel(\n       out_refs = arg_refs[len(in_ref_tys):]\n       prof_buffer = out_refs.pop() if prof_spec is not None else None\n       empty_arr_ty = ir.Type.parse(\"!llvm.array<0 x i8>\")\n-      scratch_alloc = llvm.AllocaOp(ptr_ty, c(1, i64), empty_arr_ty)\n+      scratch_alloc = llvm.AllocaOp(\n+          ptr_ty, c(1, i64), empty_arr_ty, alignment=TMA_DESCRIPTOR_ALIGNMENT\n+      )\n       scratch_arr = llvm.load(empty_arr_ty, scratch_alloc.result)\n       with _launch(\n           token, grid, cluster, block, scratch_arr, smem_scratch_shape,\n"
                }
            ],
            "whole_deleted": "-      scratch_alloc = llvm.AllocaOp(ptr_ty, c(1, i64), empty_arr_ty)\n",
            "whole_added": "+      scratch_alloc = llvm.AllocaOp(\n+          ptr_ty, c(1, i64), empty_arr_ty, alignment=TMA_DESCRIPTOR_ALIGNMENT\n+      )\n",
            "whole_hunk": "@@ -733,7 +733,9 @@ def _lower_as_gpu_kernel(\n       out_refs = arg_refs[len(in_ref_tys):]\n       prof_buffer = out_refs.pop() if prof_spec is not None else None\n       empty_arr_ty = ir.Type.parse(\"!llvm.array<0 x i8>\")\n-      scratch_alloc = llvm.AllocaOp(ptr_ty, c(1, i64), empty_arr_ty)\n+      scratch_alloc = llvm.AllocaOp(\n+          ptr_ty, c(1, i64), empty_arr_ty, alignment=TMA_DESCRIPTOR_ALIGNMENT\n+      )\n       scratch_arr = llvm.load(empty_arr_ty, scratch_alloc.result)\n       with _launch(\n           token, grid, cluster, block, scratch_arr, smem_scratch_shape,\n"
        },
        {
            "name": "runtime.cc",
            "path": "jaxlib/mosaic/gpu/runtime.cc",
            "patches": [
                {
                    "old_start": 15,
                    "old_length": 7,
                    "new_start": 15,
                    "new_length": 7,
                    "hunk": "@@ -15,7 +15,7 @@ limitations under the License.\n \n #include <cstdint>\n #include <cstdio>\n-#include <cstdlib>\n+\n #include \"third_party/gpus/cuda/include/cuda.h\"\n \n extern \"C\" {\n"
                },
                {
                    "old_start": 24,
                    "old_length": 6,
                    "new_start": 24,
                    "new_length": 13,
                    "hunk": "@@ -24,6 +24,13 @@ void mosaic_gpu_init_tma_desc(CUtensorMap *tma_desc, void *base_addr,\n                               int64_t elem_bytewidth, int64_t rank,\n                               int64_t *sizes, int64_t *strides,\n                               int64_t swizzle_bytes, int64_t *window_shape) {\n+  if (((uintptr_t)tma_desc) % 64 != 0) {\n+    fprintf(stderr,\n+            \"TMA descriptor address must be 64 byte aligned, but got: %p\\n\",\n+            tma_desc);\n+    abort();\n+  }\n+\n   CUtensorMapDataType data_type;\n   if (elem_bytewidth == 1) {\n     data_type = CU_TENSOR_MAP_DATA_TYPE_UINT8;\n"
                },
                {
                    "old_start": 37,
                    "old_length": 9,
                    "new_start": 44,
                    "new_length": 20,
                    "hunk": "@@ -37,9 +44,20 @@ void mosaic_gpu_init_tma_desc(CUtensorMap *tma_desc, void *base_addr,\n     fprintf(stderr, \"Unsupported element size: %ld\\n\", elem_bytewidth);\n     abort();\n   }\n+  if (rank < 1 || rank > 5) {\n+    fprintf(stderr, \"Rank must be in [1, 5], but got %ld\\n\", rank);\n+    abort();\n+  }\n   cuuint64_t tma_sizes[5] = {1, 1, 1, 1, 1};\n   for (int i = 0; i < rank; ++i) {\n-    tma_sizes[i] = static_cast<cuuint64_t>(sizes[rank - i - 1]);\n+    cuuint64_t tma_size_i = static_cast<cuuint64_t>(sizes[rank - i - 1]);\n+    if (tma_size_i > static_cast<cuuint64_t>(1) << 32) {\n+      fprintf(stderr,\n+              \"TMA size must be less than 2**32, but got %ld at index %ld\\n\",\n+              tma_size_i, rank - i - 1);\n+      abort();\n+    }\n+    tma_sizes[i] = tma_size_i;\n   }\n   cuuint64_t tma_strides[5] = {1, 1, 1, 1, 1};\n   if (strides[rank - 1] != 1) {\n"
                },
                {
                    "old_start": 48,
                    "old_length": 12,
                    "new_start": 66,
                    "new_length": 29,
                    "hunk": "@@ -48,12 +66,29 @@ void mosaic_gpu_init_tma_desc(CUtensorMap *tma_desc, void *base_addr,\n     abort();\n   }\n   for (int i = 0; i < rank - 1; ++i) {  // We skip the implicit minor stride.\n-    tma_strides[i] =\n+    cuuint64_t tma_stride_i =\n         static_cast<cuuint64_t>(strides[rank - i - 2] * elem_bytewidth);\n+    if (tma_stride_i % 16 != 0 || tma_stride_i >= static_cast<cuuint64_t>(1)\n+                                                      << 40) {\n+      fprintf(stderr,\n+              \"Byte strides must be divisble by 16 and less than 2**40, but \"\n+              \"got %ld (item stride = %ld, item size = %ld) at index %ld\\n\",\n+              tma_stride_i, strides[rank - 1], elem_bytewidth, rank - i - 2);\n+      abort();\n+    }\n+    tma_strides[i] = tma_stride_i;\n   }\n   cuuint32_t tma_window_shape[5] = {1, 1, 1, 1, 1};\n   for (int64_t i = 0; i < rank; ++i) {\n-    tma_window_shape[i] = static_cast<cuuint32_t>(window_shape[rank - i - 1]);\n+    cuuint32_t tma_window_shape_i =\n+        static_cast<cuuint32_t>(window_shape[rank - i - 1]);\n+    if (tma_window_shape_i > 256) {\n+      fprintf(stderr,\n+              \"Window shape must be in [0, 256], but got %d at index %ld\\n\",\n+              tma_window_shape_i, rank - i - 1);\n+      abort();\n+    }\n+    tma_window_shape[i] = tma_window_shape_i;\n   }\n   cuuint32_t element_strides[5] = {1, 1, 1, 1, 1};\n   CUtensorMapSwizzle swizzle;"
                }
            ],
            "whole_deleted": "-#include <cstdlib>\n-    tma_sizes[i] = static_cast<cuuint64_t>(sizes[rank - i - 1]);\n-    tma_strides[i] =\n-    tma_window_shape[i] = static_cast<cuuint32_t>(window_shape[rank - i - 1]);\n",
            "whole_added": "+\n+  if (((uintptr_t)tma_desc) % 64 != 0) {\n+    fprintf(stderr,\n+            \"TMA descriptor address must be 64 byte aligned, but got: %p\\n\",\n+            tma_desc);\n+    abort();\n+  }\n+\n+  if (rank < 1 || rank > 5) {\n+    fprintf(stderr, \"Rank must be in [1, 5], but got %ld\\n\", rank);\n+    abort();\n+  }\n+    cuuint64_t tma_size_i = static_cast<cuuint64_t>(sizes[rank - i - 1]);\n+    if (tma_size_i > static_cast<cuuint64_t>(1) << 32) {\n+      fprintf(stderr,\n+              \"TMA size must be less than 2**32, but got %ld at index %ld\\n\",\n+              tma_size_i, rank - i - 1);\n+      abort();\n+    }\n+    tma_sizes[i] = tma_size_i;\n+    cuuint64_t tma_stride_i =\n+    if (tma_stride_i % 16 != 0 || tma_stride_i >= static_cast<cuuint64_t>(1)\n+                                                      << 40) {\n+      fprintf(stderr,\n+              \"Byte strides must be divisble by 16 and less than 2**40, but \"\n+              \"got %ld (item stride = %ld, item size = %ld) at index %ld\\n\",\n+              tma_stride_i, strides[rank - 1], elem_bytewidth, rank - i - 2);\n+      abort();\n+    }\n+    tma_strides[i] = tma_stride_i;\n+    cuuint32_t tma_window_shape_i =\n+        static_cast<cuuint32_t>(window_shape[rank - i - 1]);\n+    if (tma_window_shape_i > 256) {\n+      fprintf(stderr,\n+              \"Window shape must be in [0, 256], but got %d at index %ld\\n\",\n+              tma_window_shape_i, rank - i - 1);\n+      abort();\n+    }\n+    tma_window_shape[i] = tma_window_shape_i;\n",
            "whole_hunk": "@@ -15,7 +15,7 @@ limitations under the License.\n \n #include <cstdint>\n #include <cstdio>\n-#include <cstdlib>\n+\n #include \"third_party/gpus/cuda/include/cuda.h\"\n \n extern \"C\" {\n@@ -24,6 +24,13 @@ void mosaic_gpu_init_tma_desc(CUtensorMap *tma_desc, void *base_addr,\n                               int64_t elem_bytewidth, int64_t rank,\n                               int64_t *sizes, int64_t *strides,\n                               int64_t swizzle_bytes, int64_t *window_shape) {\n+  if (((uintptr_t)tma_desc) % 64 != 0) {\n+    fprintf(stderr,\n+            \"TMA descriptor address must be 64 byte aligned, but got: %p\\n\",\n+            tma_desc);\n+    abort();\n+  }\n+\n   CUtensorMapDataType data_type;\n   if (elem_bytewidth == 1) {\n     data_type = CU_TENSOR_MAP_DATA_TYPE_UINT8;\n@@ -37,9 +44,20 @@ void mosaic_gpu_init_tma_desc(CUtensorMap *tma_desc, void *base_addr,\n     fprintf(stderr, \"Unsupported element size: %ld\\n\", elem_bytewidth);\n     abort();\n   }\n+  if (rank < 1 || rank > 5) {\n+    fprintf(stderr, \"Rank must be in [1, 5], but got %ld\\n\", rank);\n+    abort();\n+  }\n   cuuint64_t tma_sizes[5] = {1, 1, 1, 1, 1};\n   for (int i = 0; i < rank; ++i) {\n-    tma_sizes[i] = static_cast<cuuint64_t>(sizes[rank - i - 1]);\n+    cuuint64_t tma_size_i = static_cast<cuuint64_t>(sizes[rank - i - 1]);\n+    if (tma_size_i > static_cast<cuuint64_t>(1) << 32) {\n+      fprintf(stderr,\n+              \"TMA size must be less than 2**32, but got %ld at index %ld\\n\",\n+              tma_size_i, rank - i - 1);\n+      abort();\n+    }\n+    tma_sizes[i] = tma_size_i;\n   }\n   cuuint64_t tma_strides[5] = {1, 1, 1, 1, 1};\n   if (strides[rank - 1] != 1) {\n@@ -48,12 +66,29 @@ void mosaic_gpu_init_tma_desc(CUtensorMap *tma_desc, void *base_addr,\n     abort();\n   }\n   for (int i = 0; i < rank - 1; ++i) {  // We skip the implicit minor stride.\n-    tma_strides[i] =\n+    cuuint64_t tma_stride_i =\n         static_cast<cuuint64_t>(strides[rank - i - 2] * elem_bytewidth);\n+    if (tma_stride_i % 16 != 0 || tma_stride_i >= static_cast<cuuint64_t>(1)\n+                                                      << 40) {\n+      fprintf(stderr,\n+              \"Byte strides must be divisble by 16 and less than 2**40, but \"\n+              \"got %ld (item stride = %ld, item size = %ld) at index %ld\\n\",\n+              tma_stride_i, strides[rank - 1], elem_bytewidth, rank - i - 2);\n+      abort();\n+    }\n+    tma_strides[i] = tma_stride_i;\n   }\n   cuuint32_t tma_window_shape[5] = {1, 1, 1, 1, 1};\n   for (int64_t i = 0; i < rank; ++i) {\n-    tma_window_shape[i] = static_cast<cuuint32_t>(window_shape[rank - i - 1]);\n+    cuuint32_t tma_window_shape_i =\n+        static_cast<cuuint32_t>(window_shape[rank - i - 1]);\n+    if (tma_window_shape_i > 256) {\n+      fprintf(stderr,\n+              \"Window shape must be in [0, 256], but got %d at index %ld\\n\",\n+              tma_window_shape_i, rank - i - 1);\n+      abort();\n+    }\n+    tma_window_shape[i] = tma_window_shape_i;\n   }\n   cuuint32_t element_strides[5] = {1, 1, 1, 1, 1};\n   CUtensorMapSwizzle swizzle;"
        }
    ]
},
{
    "Id": 1,
    "commit_link": "https://github.com/google/jax/commit/220ec2aa6953423625997b516e1c1e6e96288053",
    "date": "2024-07-24T16:58:35-07:00",
    "message": "[Mosaic TPU] (8,128),-2 -> (8,128) for non-zero and replicated 2nd minor offset\n\nAlso fix bug where relayouts for fully replicated source assumed it was a no-op without checking implicit dims\n\nPiperOrigin-RevId: 655746766",
    "changes": [
        {
            "name": "layout.h",
            "path": "jaxlib/mosaic/dialect/tpu/layout.h",
            "patches": [
                {
                    "old_start": 288,
                    "old_length": 7,
                    "new_start": 288,
                    "new_length": 7,
                    "hunk": "@@ -288,7 +288,7 @@ class VectorLayout {\n   }\n \n   template <typename T>\n-  void insertImplicit(SmallVector<T> &vec, T value) const {\n+  void insertImplicit(SmallVectorImpl<T> &vec, T value) const {\n     CHECK_GE(vec.size(), layout_rank());\n     switch (implicit_dim_) {\n       case ImplicitDim::kNone:\n"
                },
                {
                    "old_start": 302,
                    "old_length": 7,
                    "new_start": 302,
                    "new_length": 7,
                    "hunk": "@@ -302,7 +302,7 @@ class VectorLayout {\n   }\n \n   template <typename T>\n-  void eraseImplicit(SmallVector<T> &vec) const {\n+  void eraseImplicit(SmallVectorImpl<T> &vec) const {\n     CHECK_GE(vec.size(), 2);\n     switch (implicit_dim_) {\n       case ImplicitDim::kNone:\n"
                }
            ],
            "whole_deleted": "-  void insertImplicit(SmallVector<T> &vec, T value) const {\n-  void eraseImplicit(SmallVector<T> &vec) const {\n",
            "whole_added": "+  void insertImplicit(SmallVectorImpl<T> &vec, T value) const {\n+  void eraseImplicit(SmallVectorImpl<T> &vec) const {\n",
            "whole_hunk": "@@ -288,7 +288,7 @@ class VectorLayout {\n   }\n \n   template <typename T>\n-  void insertImplicit(SmallVector<T> &vec, T value) const {\n+  void insertImplicit(SmallVectorImpl<T> &vec, T value) const {\n     CHECK_GE(vec.size(), layout_rank());\n     switch (implicit_dim_) {\n       case ImplicitDim::kNone:\n@@ -302,7 +302,7 @@ class VectorLayout {\n   }\n \n   template <typename T>\n-  void eraseImplicit(SmallVector<T> &vec) const {\n+  void eraseImplicit(SmallVectorImpl<T> &vec) const {\n     CHECK_GE(vec.size(), 2);\n     switch (implicit_dim_) {\n       case ImplicitDim::kNone:\n"
        },
        {
            "name": "apply_vector_layout.cc",
            "path": "jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc",
            "patches": [
                {
                    "old_start": 5021,
                    "old_length": 8,
                    "new_start": 5021,
                    "new_length": 37,
                    "hunk": "@@ -5021,8 +5021,37 @@ FailureOr<TypedValue<VectorType>> relayout(RewriteContext &ctx,\n   if (bitwidth != dst.bitwidth()) {\n     return emitError(v.getLoc(), \"Can't change bitwidth during a relayout\");\n   }\n-  const int packing = src.packing();\n   VectorType vty = v.getType();\n+  {\n+    // Replication imposes a replication constraint on the *logical* value of\n+    // the vector: When moving along a replicated axis, all elements must be\n+    // equal. Note that when the axis is a singleton, there is effectively no\n+    // added *logical* constraint.\n+    // For example, a vector<2x2xf32> v with no implicit dims and layout offsets\n+    // {*, 0} is expected to satisfy v[0, 0] == v[1, 0] and v[0, 1] == v[1, 1].\n+    // Relayout does not change the logical value of the vector. Any replication\n+    // constraints in the result must be guaranteed by the source layout.\n+    SmallVector<LayoutOffset, 2> src_offsets(ArrayRef(src.offsets()));\n+    SmallVector<LayoutOffset, 2> dst_offsets(ArrayRef(dst.offsets()));\n+    // Remove implicit dims to get offsets for trailing logical dims.\n+    src.eraseImplicit(src_offsets);\n+    dst.eraseImplicit(dst_offsets);\n+    for (int i = dst_offsets.size(); i > 0; --i) {\n+      const int64_t dim_size = *(vty.getShape().end() - i);\n+      const bool dim_replicated_in_dst = !*(dst_offsets.end() - i);\n+      // If the dim is untiled in the src layout, then there is no guarantee of\n+      // replication, because we don't track replication for untiled dims.\n+      const bool dim_replicated_in_src =\n+          i <= src_offsets.size() && !*(src_offsets.end() - i);\n+      if (dim_replicated_in_dst && !dim_replicated_in_src && dim_size != 1) {\n+        return emitError(v.getLoc(),\n+                         \"Invalid relayout: Non-singleton logical dimension is \"\n+                         \"replicated in destination but not in source for \")\n+               << vty << \": \" << src << \" -> \" << dst;\n+      }\n+    }\n+  }\n+  const int packing = src.packing();\n \n   // Save the original value of dst to use it at the end. It determines the\n   // out_layout of the result of assemble.\n"
                },
                {
                    "old_start": 5054,
                    "old_length": 8,
                    "new_start": 5083,
                    "new_length": 8,
                    "hunk": "@@ -5054,8 +5083,8 @@ FailureOr<TypedValue<VectorType>> relayout(RewriteContext &ctx,\n                     /*use_implicit_shape=*/true)\n         .getResult();\n   }\n-  if (!src.offsets()[0].has_value() && !src.offsets()[1].has_value() &&\n-      src.tilesPerVreg(target_shape) == 1) {\n+  if (src.layout_rank() >= dst.layout_rank() && !src.offsets()[0].has_value() &&\n+      !src.offsets()[1].has_value() && src.tilesPerVreg(target_shape) == 1) {\n     // A fully replicated value is always easy to relayout\n     // It would be nice to be able to assert this here, but given replicated\n     // values our rules can introduce equivalent expressions.\n"
                },
                {
                    "old_start": 5258,
                    "old_length": 25,
                    "new_start": 5287,
                    "new_length": 29,
                    "hunk": "@@ -5258,25 +5287,29 @@ FailureOr<TypedValue<VectorType>> relayout(RewriteContext &ctx,\n                // This drops the implicit second minor dimension.\n       src.implicit_dim() == VectorLayout::ImplicitDim::kSecondMinor &&\n       dst.implicit_dim() == VectorLayout::ImplicitDim::kNone &&\n-      src.bitwidth() == 32 && src.offsets() == dst.offsets() &&\n-      src.offsets() == LayoutOffsets{0, 0} && src.tiling() == dst.tiling() &&\n+      src.bitwidth() == 32 && dst.offsets()[0] &&\n+      src.offsets()[1] == dst.offsets()[1] && src.tiling() == dst.tiling() &&\n       src.tiling() == std::array<int64_t, 2>{8, 128}) {\n     xla::Array<Value> src_tiles_retiled(\n         dst.tileArrayImplicitShape(vty.getShape(), target_shape));\n-    src_tiles_retiled.Each(\n-        [&](const absl::Span<const int64_t> idx, Value *tile) {\n-          for (int dst_sl_idx = 0; dst_sl_idx < 8; ++dst_sl_idx) {\n-            SmallVector<int64_t> src_idx(idx.begin(), idx.end());\n-            src.insertImplicit<int64_t>(src_idx, 0);\n-            auto second_minor_idx = idx.size() - 2;\n-            src_idx[second_minor_idx] = 8 * idx[second_minor_idx] + dst_sl_idx;\n-            if (src_idx[second_minor_idx] >= src_tiles.dim(second_minor_idx)) {\n-              break;\n-            }\n-            *tile = copy_one_sublane(builder, src_tiles(src_idx), 0, *tile,\n-                                     dst_sl_idx, target_shape);\n-          }\n-        });\n+    src_tiles_retiled.Each([&](const absl::Span<const int64_t> idx,\n+                               Value *tile) {\n+      const int64_t dst_2nd_minor_idx = idx.size() - 2;\n+      SmallVector<int64_t> src_idx(idx.begin(), idx.end());\n+      src.insertImplicit<int64_t>(src_idx, 0);\n+      const int dst_sl_start =\n+          idx[dst_2nd_minor_idx] == 0 ? *dst.offsets()[0] : 0;\n+      src_idx[dst_2nd_minor_idx] = target_shape[0] * idx[dst_2nd_minor_idx] +\n+                                   dst_sl_start - *dst.offsets()[0];\n+      for (int dst_sl_idx = dst_sl_start;\n+           dst_sl_idx < target_shape[0] &&\n+           src_idx[dst_2nd_minor_idx] < src_tiles.dim(dst_2nd_minor_idx);\n+           ++dst_sl_idx, ++src_idx[dst_2nd_minor_idx]) {\n+        *tile = copy_one_sublane(builder, src_tiles(src_idx),\n+                                 src.offsets()[0].value_or(dst_sl_idx), *tile,\n+                                 dst_sl_idx, target_shape);\n+      }\n+    });\n     src = dst;\n     src_tiles = std::move(src_tiles_retiled);\n   }"
                }
            ],
            "whole_deleted": "-  const int packing = src.packing();\n-  if (!src.offsets()[0].has_value() && !src.offsets()[1].has_value() &&\n-      src.tilesPerVreg(target_shape) == 1) {\n-      src.bitwidth() == 32 && src.offsets() == dst.offsets() &&\n-      src.offsets() == LayoutOffsets{0, 0} && src.tiling() == dst.tiling() &&\n-    src_tiles_retiled.Each(\n-        [&](const absl::Span<const int64_t> idx, Value *tile) {\n-          for (int dst_sl_idx = 0; dst_sl_idx < 8; ++dst_sl_idx) {\n-            SmallVector<int64_t> src_idx(idx.begin(), idx.end());\n-            src.insertImplicit<int64_t>(src_idx, 0);\n-            auto second_minor_idx = idx.size() - 2;\n-            src_idx[second_minor_idx] = 8 * idx[second_minor_idx] + dst_sl_idx;\n-            if (src_idx[second_minor_idx] >= src_tiles.dim(second_minor_idx)) {\n-              break;\n-            }\n-            *tile = copy_one_sublane(builder, src_tiles(src_idx), 0, *tile,\n-                                     dst_sl_idx, target_shape);\n-          }\n-        });\n",
            "whole_added": "+  {\n+    // Replication imposes a replication constraint on the *logical* value of\n+    // the vector: When moving along a replicated axis, all elements must be\n+    // equal. Note that when the axis is a singleton, there is effectively no\n+    // added *logical* constraint.\n+    // For example, a vector<2x2xf32> v with no implicit dims and layout offsets\n+    // {*, 0} is expected to satisfy v[0, 0] == v[1, 0] and v[0, 1] == v[1, 1].\n+    // Relayout does not change the logical value of the vector. Any replication\n+    // constraints in the result must be guaranteed by the source layout.\n+    SmallVector<LayoutOffset, 2> src_offsets(ArrayRef(src.offsets()));\n+    SmallVector<LayoutOffset, 2> dst_offsets(ArrayRef(dst.offsets()));\n+    // Remove implicit dims to get offsets for trailing logical dims.\n+    src.eraseImplicit(src_offsets);\n+    dst.eraseImplicit(dst_offsets);\n+    for (int i = dst_offsets.size(); i > 0; --i) {\n+      const int64_t dim_size = *(vty.getShape().end() - i);\n+      const bool dim_replicated_in_dst = !*(dst_offsets.end() - i);\n+      // If the dim is untiled in the src layout, then there is no guarantee of\n+      // replication, because we don't track replication for untiled dims.\n+      const bool dim_replicated_in_src =\n+          i <= src_offsets.size() && !*(src_offsets.end() - i);\n+      if (dim_replicated_in_dst && !dim_replicated_in_src && dim_size != 1) {\n+        return emitError(v.getLoc(),\n+                         \"Invalid relayout: Non-singleton logical dimension is \"\n+                         \"replicated in destination but not in source for \")\n+               << vty << \": \" << src << \" -> \" << dst;\n+      }\n+    }\n+  }\n+  const int packing = src.packing();\n+  if (src.layout_rank() >= dst.layout_rank() && !src.offsets()[0].has_value() &&\n+      !src.offsets()[1].has_value() && src.tilesPerVreg(target_shape) == 1) {\n+      src.bitwidth() == 32 && dst.offsets()[0] &&\n+      src.offsets()[1] == dst.offsets()[1] && src.tiling() == dst.tiling() &&\n+    src_tiles_retiled.Each([&](const absl::Span<const int64_t> idx,\n+                               Value *tile) {\n+      const int64_t dst_2nd_minor_idx = idx.size() - 2;\n+      SmallVector<int64_t> src_idx(idx.begin(), idx.end());\n+      src.insertImplicit<int64_t>(src_idx, 0);\n+      const int dst_sl_start =\n+          idx[dst_2nd_minor_idx] == 0 ? *dst.offsets()[0] : 0;\n+      src_idx[dst_2nd_minor_idx] = target_shape[0] * idx[dst_2nd_minor_idx] +\n+                                   dst_sl_start - *dst.offsets()[0];\n+      for (int dst_sl_idx = dst_sl_start;\n+           dst_sl_idx < target_shape[0] &&\n+           src_idx[dst_2nd_minor_idx] < src_tiles.dim(dst_2nd_minor_idx);\n+           ++dst_sl_idx, ++src_idx[dst_2nd_minor_idx]) {\n+        *tile = copy_one_sublane(builder, src_tiles(src_idx),\n+                                 src.offsets()[0].value_or(dst_sl_idx), *tile,\n+                                 dst_sl_idx, target_shape);\n+      }\n+    });\n",
            "whole_hunk": "@@ -5021,8 +5021,37 @@ FailureOr<TypedValue<VectorType>> relayout(RewriteContext &ctx,\n   if (bitwidth != dst.bitwidth()) {\n     return emitError(v.getLoc(), \"Can't change bitwidth during a relayout\");\n   }\n-  const int packing = src.packing();\n   VectorType vty = v.getType();\n+  {\n+    // Replication imposes a replication constraint on the *logical* value of\n+    // the vector: When moving along a replicated axis, all elements must be\n+    // equal. Note that when the axis is a singleton, there is effectively no\n+    // added *logical* constraint.\n+    // For example, a vector<2x2xf32> v with no implicit dims and layout offsets\n+    // {*, 0} is expected to satisfy v[0, 0] == v[1, 0] and v[0, 1] == v[1, 1].\n+    // Relayout does not change the logical value of the vector. Any replication\n+    // constraints in the result must be guaranteed by the source layout.\n+    SmallVector<LayoutOffset, 2> src_offsets(ArrayRef(src.offsets()));\n+    SmallVector<LayoutOffset, 2> dst_offsets(ArrayRef(dst.offsets()));\n+    // Remove implicit dims to get offsets for trailing logical dims.\n+    src.eraseImplicit(src_offsets);\n+    dst.eraseImplicit(dst_offsets);\n+    for (int i = dst_offsets.size(); i > 0; --i) {\n+      const int64_t dim_size = *(vty.getShape().end() - i);\n+      const bool dim_replicated_in_dst = !*(dst_offsets.end() - i);\n+      // If the dim is untiled in the src layout, then there is no guarantee of\n+      // replication, because we don't track replication for untiled dims.\n+      const bool dim_replicated_in_src =\n+          i <= src_offsets.size() && !*(src_offsets.end() - i);\n+      if (dim_replicated_in_dst && !dim_replicated_in_src && dim_size != 1) {\n+        return emitError(v.getLoc(),\n+                         \"Invalid relayout: Non-singleton logical dimension is \"\n+                         \"replicated in destination but not in source for \")\n+               << vty << \": \" << src << \" -> \" << dst;\n+      }\n+    }\n+  }\n+  const int packing = src.packing();\n \n   // Save the original value of dst to use it at the end. It determines the\n   // out_layout of the result of assemble.\n@@ -5054,8 +5083,8 @@ FailureOr<TypedValue<VectorType>> relayout(RewriteContext &ctx,\n                     /*use_implicit_shape=*/true)\n         .getResult();\n   }\n-  if (!src.offsets()[0].has_value() && !src.offsets()[1].has_value() &&\n-      src.tilesPerVreg(target_shape) == 1) {\n+  if (src.layout_rank() >= dst.layout_rank() && !src.offsets()[0].has_value() &&\n+      !src.offsets()[1].has_value() && src.tilesPerVreg(target_shape) == 1) {\n     // A fully replicated value is always easy to relayout\n     // It would be nice to be able to assert this here, but given replicated\n     // values our rules can introduce equivalent expressions.\n@@ -5258,25 +5287,29 @@ FailureOr<TypedValue<VectorType>> relayout(RewriteContext &ctx,\n                // This drops the implicit second minor dimension.\n       src.implicit_dim() == VectorLayout::ImplicitDim::kSecondMinor &&\n       dst.implicit_dim() == VectorLayout::ImplicitDim::kNone &&\n-      src.bitwidth() == 32 && src.offsets() == dst.offsets() &&\n-      src.offsets() == LayoutOffsets{0, 0} && src.tiling() == dst.tiling() &&\n+      src.bitwidth() == 32 && dst.offsets()[0] &&\n+      src.offsets()[1] == dst.offsets()[1] && src.tiling() == dst.tiling() &&\n       src.tiling() == std::array<int64_t, 2>{8, 128}) {\n     xla::Array<Value> src_tiles_retiled(\n         dst.tileArrayImplicitShape(vty.getShape(), target_shape));\n-    src_tiles_retiled.Each(\n-        [&](const absl::Span<const int64_t> idx, Value *tile) {\n-          for (int dst_sl_idx = 0; dst_sl_idx < 8; ++dst_sl_idx) {\n-            SmallVector<int64_t> src_idx(idx.begin(), idx.end());\n-            src.insertImplicit<int64_t>(src_idx, 0);\n-            auto second_minor_idx = idx.size() - 2;\n-            src_idx[second_minor_idx] = 8 * idx[second_minor_idx] + dst_sl_idx;\n-            if (src_idx[second_minor_idx] >= src_tiles.dim(second_minor_idx)) {\n-              break;\n-            }\n-            *tile = copy_one_sublane(builder, src_tiles(src_idx), 0, *tile,\n-                                     dst_sl_idx, target_shape);\n-          }\n-        });\n+    src_tiles_retiled.Each([&](const absl::Span<const int64_t> idx,\n+                               Value *tile) {\n+      const int64_t dst_2nd_minor_idx = idx.size() - 2;\n+      SmallVector<int64_t> src_idx(idx.begin(), idx.end());\n+      src.insertImplicit<int64_t>(src_idx, 0);\n+      const int dst_sl_start =\n+          idx[dst_2nd_minor_idx] == 0 ? *dst.offsets()[0] : 0;\n+      src_idx[dst_2nd_minor_idx] = target_shape[0] * idx[dst_2nd_minor_idx] +\n+                                   dst_sl_start - *dst.offsets()[0];\n+      for (int dst_sl_idx = dst_sl_start;\n+           dst_sl_idx < target_shape[0] &&\n+           src_idx[dst_2nd_minor_idx] < src_tiles.dim(dst_2nd_minor_idx);\n+           ++dst_sl_idx, ++src_idx[dst_2nd_minor_idx]) {\n+        *tile = copy_one_sublane(builder, src_tiles(src_idx),\n+                                 src.offsets()[0].value_or(dst_sl_idx), *tile,\n+                                 dst_sl_idx, target_shape);\n+      }\n+    });\n     src = dst;\n     src_tiles = std::move(src_tiles_retiled);\n   }"
        }
    ]
},
{
    "Id": 2,
    "commit_link": "https://github.com/google/jax/commit/0d7531b4f12b26368387210ca504a8e37d23baa0",
    "date": "2024-07-22T11:15:43-07:00",
    "message": "Merge pull request #22567 from jakevdp:fft-norm-validation\n\nPiperOrigin-RevId: 654828825",
    "changes": [
        {
            "name": "fft.py",
            "path": "jax/_src/scipy/fft.py",
            "patches": [
                {
                    "old_start": 98,
                    "old_length": 6,
                    "new_start": 98,
                    "new_length": 8,
                    "hunk": "@@ -98,6 +98,8 @@ def dct(x: Array, type: int = 2, n: int | None = None,\n   \"\"\"\n   if type != 2:\n     raise NotImplementedError('Only DCT type 2 is implemented.')\n+  if norm is not None and norm not in ['ortho']:\n+    raise ValueError(f\"jax.scipy.fft.dct: {norm=!r} is not implemented\")\n \n   axis = canonicalize_axis(axis, x.ndim)\n   if n is not None:\n"
                },
                {
                    "old_start": 196,
                    "old_length": 6,
                    "new_start": 198,
                    "new_length": 8,
                    "hunk": "@@ -196,6 +198,8 @@ def dctn(x: Array, type: int = 2,\n   \"\"\"\n   if type != 2:\n     raise NotImplementedError('Only DCT type 2 is implemented.')\n+  if norm is not None and norm not in ['ortho']:\n+    raise ValueError(f\"jax.scipy.fft.dctn: {norm=!r} is not implemented\")\n \n   if axes is None:\n     axes = range(x.ndim)\n"
                },
                {
                    "old_start": 282,
                    "old_length": 6,
                    "new_start": 286,
                    "new_length": 8,
                    "hunk": "@@ -282,6 +286,8 @@ def idct(x: Array, type: int = 2, n: int | None = None,\n   \"\"\"\n   if type != 2:\n     raise NotImplementedError('Only DCT type 2 is implemented.')\n+  if norm is not None and norm not in ['ortho']:\n+    raise ValueError(f\"jax.scipy.fft.idct: {norm=!r} is not implemented\")\n \n   axis = canonicalize_axis(axis, x.ndim)\n   if n is not None:\n"
                },
                {
                    "old_start": 378,
                    "old_length": 6,
                    "new_start": 384,
                    "new_length": 8,
                    "hunk": "@@ -378,6 +384,8 @@ def idctn(x: Array, type: int = 2,\n   \"\"\"\n   if type != 2:\n     raise NotImplementedError('Only DCT type 2 is implemented.')\n+  if norm is not None and norm not in ['ortho']:\n+    raise ValueError(f\"jax.scipy.fft.idctn: {norm=!r} is not implemented\")\n \n   if axes is None:\n     axes = range(x.ndim)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  if norm is not None and norm not in ['ortho']:\n+    raise ValueError(f\"jax.scipy.fft.dct: {norm=!r} is not implemented\")\n+  if norm is not None and norm not in ['ortho']:\n+    raise ValueError(f\"jax.scipy.fft.dctn: {norm=!r} is not implemented\")\n+  if norm is not None and norm not in ['ortho']:\n+    raise ValueError(f\"jax.scipy.fft.idct: {norm=!r} is not implemented\")\n+  if norm is not None and norm not in ['ortho']:\n+    raise ValueError(f\"jax.scipy.fft.idctn: {norm=!r} is not implemented\")\n",
            "whole_hunk": "@@ -98,6 +98,8 @@ def dct(x: Array, type: int = 2, n: int | None = None,\n   \"\"\"\n   if type != 2:\n     raise NotImplementedError('Only DCT type 2 is implemented.')\n+  if norm is not None and norm not in ['ortho']:\n+    raise ValueError(f\"jax.scipy.fft.dct: {norm=!r} is not implemented\")\n \n   axis = canonicalize_axis(axis, x.ndim)\n   if n is not None:\n@@ -196,6 +198,8 @@ def dctn(x: Array, type: int = 2,\n   \"\"\"\n   if type != 2:\n     raise NotImplementedError('Only DCT type 2 is implemented.')\n+  if norm is not None and norm not in ['ortho']:\n+    raise ValueError(f\"jax.scipy.fft.dctn: {norm=!r} is not implemented\")\n \n   if axes is None:\n     axes = range(x.ndim)\n@@ -282,6 +286,8 @@ def idct(x: Array, type: int = 2, n: int | None = None,\n   \"\"\"\n   if type != 2:\n     raise NotImplementedError('Only DCT type 2 is implemented.')\n+  if norm is not None and norm not in ['ortho']:\n+    raise ValueError(f\"jax.scipy.fft.idct: {norm=!r} is not implemented\")\n \n   axis = canonicalize_axis(axis, x.ndim)\n   if n is not None:\n@@ -378,6 +384,8 @@ def idctn(x: Array, type: int = 2,\n   \"\"\"\n   if type != 2:\n     raise NotImplementedError('Only DCT type 2 is implemented.')\n+  if norm is not None and norm not in ['ortho']:\n+    raise ValueError(f\"jax.scipy.fft.idctn: {norm=!r} is not implemented\")\n \n   if axes is None:\n     axes = range(x.ndim)"
        }
    ]
},
{
    "Id": 3,
    "commit_link": "https://github.com/google/jax/commit/ab9fc2d8393f258d8b97dcbcde3be379ac8386c1",
    "date": "2024-07-17T13:07:15-07:00",
    "message": "PR #22404: [cuDNN SDPA] fix bias sharding check and dbias calculation\n\nImported from GitHub PR https://github.com/google/jax/pull/22404\n\n* only check bias batch/num_head sharding spec if present. Both dims could be broadcast.\n* dbias calculation is incorrect in spmd and all_reduce is required.\nCopybara import of the project:\n\n--\ncb81b80626bcf17db875bad5526cd2f24c989049 by cjkkkk <ske@nvidia.com>:\n\nfix sharding\n\nMerging this change closes #22404\n\nCOPYBARA_INTEGRATE_REVIEW=https://github.com/google/jax/pull/22404 from Cjkkkk:fix_bias_sharding_dbias_all_reduce cb81b80626bcf17db875bad5526cd2f24c989049\nPiperOrigin-RevId: 653335832",
    "changes": [
        {
            "name": "fused_attention_stablehlo.py",
            "path": "jax/_src/cudnn/fused_attention_stablehlo.py",
            "patches": [
                {
                    "old_start": 713,
                    "old_length": 7,
                    "new_start": 713,
                    "new_length": 8,
                    "hunk": "@@ -713,7 +713,8 @@ def _check_qkv_bias_mask_spec(\n   # check bias spec\n   if bias_spec:\n     *bias_batch_spec, bias_num_head_spec, bias_q_seq_spec, bias_kv_seq_spec = bias_spec\n-    if bias_batch_spec != batch_spec or bias_num_head_spec != num_head_spec:\n+    if any(bias_batch_spec) and bias_batch_spec != batch_spec or \\\n+      bias_num_head_spec is not None and bias_num_head_spec != num_head_spec:\n       raise ValueError(\n         \"Query and bias should have same sharding on batch and num_head dim.\")\n     if bias_q_seq_spec is not None or bias_kv_seq_spec is not None:\n"
                },
                {
                    "old_start": 805,
                    "old_length": 7,
                    "new_start": 806,
                    "new_length": 8,
                    "hunk": "@@ -805,7 +806,8 @@ def _dot_product_attention_bwd_partition(\n   out_shardings = _infer_bwd_output_sharding(mesh, arg_shapes, variadic_args)\n   # args sharding\n   arg_shardings = tuple([arg_i.sharding for arg_i in arg_shapes])\n-  impl = functools.partial(\n+  def sharded_impl(*args):\n+    impl = functools.partial(\n       _dot_product_attention_bwd_impl,\n       scale=scale,\n       seed=seed,\n"
                },
                {
                    "old_start": 813,
                    "old_length": 8,
                    "new_start": 815,
                    "new_length": 17,
                    "hunk": "@@ -813,8 +815,17 @@ def _dot_product_attention_bwd_partition(\n       variadic_args=variadic_args,\n       mask_type=mask_type,\n       layout=layout,\n-  )\n-  return mesh, impl, out_shardings, arg_shardings\n+    )\n+    grads = impl(*args)\n+    _, has_dbias = variadic_args\n+    if has_dbias:\n+      query_spec = arg_shardings[0].spec\n+      batch_spec = query_spec[0]\n+      local_dbias = grads[3]\n+      global_dbias = jax.lax.psum(local_dbias, batch_spec)\n+      grads = grads[:3] + [global_dbias]\n+    return grads\n+  return mesh, sharded_impl, out_shardings, arg_shardings\n \n # Create dot_product_attention_fwd_p for forward operation.\n _dot_product_attention_fwd_p = core.Primitive(\"dot_product_attention_fwd\")\n"
                }
            ],
            "whole_deleted": "-    if bias_batch_spec != batch_spec or bias_num_head_spec != num_head_spec:\n-  impl = functools.partial(\n-  )\n-  return mesh, impl, out_shardings, arg_shardings\n",
            "whole_added": "+    if any(bias_batch_spec) and bias_batch_spec != batch_spec or \\\n+      bias_num_head_spec is not None and bias_num_head_spec != num_head_spec:\n+  def sharded_impl(*args):\n+    impl = functools.partial(\n+    )\n+    grads = impl(*args)\n+    _, has_dbias = variadic_args\n+    if has_dbias:\n+      query_spec = arg_shardings[0].spec\n+      batch_spec = query_spec[0]\n+      local_dbias = grads[3]\n+      global_dbias = jax.lax.psum(local_dbias, batch_spec)\n+      grads = grads[:3] + [global_dbias]\n+    return grads\n+  return mesh, sharded_impl, out_shardings, arg_shardings\n",
            "whole_hunk": "@@ -713,7 +713,8 @@ def _check_qkv_bias_mask_spec(\n   # check bias spec\n   if bias_spec:\n     *bias_batch_spec, bias_num_head_spec, bias_q_seq_spec, bias_kv_seq_spec = bias_spec\n-    if bias_batch_spec != batch_spec or bias_num_head_spec != num_head_spec:\n+    if any(bias_batch_spec) and bias_batch_spec != batch_spec or \\\n+      bias_num_head_spec is not None and bias_num_head_spec != num_head_spec:\n       raise ValueError(\n         \"Query and bias should have same sharding on batch and num_head dim.\")\n     if bias_q_seq_spec is not None or bias_kv_seq_spec is not None:\n@@ -805,7 +806,8 @@ def _dot_product_attention_bwd_partition(\n   out_shardings = _infer_bwd_output_sharding(mesh, arg_shapes, variadic_args)\n   # args sharding\n   arg_shardings = tuple([arg_i.sharding for arg_i in arg_shapes])\n-  impl = functools.partial(\n+  def sharded_impl(*args):\n+    impl = functools.partial(\n       _dot_product_attention_bwd_impl,\n       scale=scale,\n       seed=seed,\n@@ -813,8 +815,17 @@ def _dot_product_attention_bwd_partition(\n       variadic_args=variadic_args,\n       mask_type=mask_type,\n       layout=layout,\n-  )\n-  return mesh, impl, out_shardings, arg_shardings\n+    )\n+    grads = impl(*args)\n+    _, has_dbias = variadic_args\n+    if has_dbias:\n+      query_spec = arg_shardings[0].spec\n+      batch_spec = query_spec[0]\n+      local_dbias = grads[3]\n+      global_dbias = jax.lax.psum(local_dbias, batch_spec)\n+      grads = grads[:3] + [global_dbias]\n+    return grads\n+  return mesh, sharded_impl, out_shardings, arg_shardings\n \n # Create dot_product_attention_fwd_p for forward operation.\n _dot_product_attention_fwd_p = core.Primitive(\"dot_product_attention_fwd\")\n"
        },
        {
            "name": "fused_attention_stablehlo_test.py",
            "path": "tests/fused_attention_stablehlo_test.py",
            "patches": [
                {
                    "old_start": 348,
                    "old_length": 34,
                    "new_start": 348,
                    "new_length": 54,
                    "hunk": "@@ -348,34 +348,54 @@ class DotProductAttentionTest(jtu.JaxTestCase):\n       self.skipTest(\"Requires >= cuDNN 8.9.6\")\n     k1, k2, k3, k4, k5 = jax.random.split(jax.random.key(0), 5)\n     query = jax.random.normal(\n-        k1, (2, 1024, 4, 64), dtype=jnp.bfloat16)\n+        k1, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n     key = jax.random.normal(\n-        k2, (2, 1024, 4, 64), dtype=jnp.bfloat16)\n+        k2, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n     value = jax.random.normal(\n-        k3, (2, 1024, 4, 64), dtype=jnp.bfloat16)\n+        k3, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n     grad = jax.random.normal(\n-        k4, (2, 1024, 4, 64), dtype=jnp.bfloat16)\n+        k4, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n     bias = jax.random.normal(\n         k5, (4, 1024, 1024), dtype=jnp.bfloat16)\n-    jitted_sdpa_train = jax.jit(\n-      partial(\n-        sdpa_train, scale=1.0, mask_type=MaskType.NO_MASK, dropout_rate=0),\n-    )\n+    devices = np.array(jax.local_devices()[:4])\n+    devices = devices.reshape((2, 2))\n+    with Mesh(devices, (\"dp\", \"tp\")) as mesh:\n+      qkv_spec = PartitionSpec(\"dp\", None, \"tp\", None)\n+      qkv_sharding = NamedSharding(mesh, qkv_spec)\n+      bias_spec = PartitionSpec(\"tp\", None, None)\n+      bias_sharding = NamedSharding(mesh, bias_spec)\n+      replicated = NamedSharding(mesh, PartitionSpec())\n+      in_shardings = (qkv_sharding, qkv_sharding, qkv_sharding,\n+                      qkv_sharding, bias_sharding, replicated)\n+      out_shardings = (qkv_sharding, (qkv_sharding, qkv_sharding, qkv_sharding, bias_sharding))\n+      query = jax.device_put(query, qkv_sharding)\n+      key = jax.device_put(key, qkv_sharding)\n+      value = jax.device_put(value, qkv_sharding)\n+      grad = jax.device_put(grad, qkv_sharding)\n+      bias = jax.device_put(bias, bias_sharding)\n+      jitted_sdpa_train = jax.jit(\n+        partial(\n+          sdpa_train, scale=1.0, mask_type=MaskType.NO_MASK, dropout_rate=0),\n+        in_shardings=in_shardings,\n+        out_shardings=out_shardings\n+      )\n \n-    jitted_sdpa_train_ref = jax.jit(\n-      partial(\n-        sdpa_train_ref, scale=1.0, mask_type=MaskType.NO_MASK, dropout_rate=0),\n-    )\n+      jitted_sdpa_train_ref = jax.jit(\n+        partial(\n+          sdpa_train_ref, scale=1.0, mask_type=MaskType.NO_MASK, dropout_rate=0),\n+        in_shardings=in_shardings,\n+        out_shardings=out_shardings\n+      )\n \n-    out, (query_grad, key_grad, value_grad, bias_grad) = \\\n-      jitted_sdpa_train(query, key, value, grad, bias, None)\n-    out_ref, (query_grad_ref, key_grad_ref, value_grad_ref, bias_grad_ref) = \\\n-      jitted_sdpa_train_ref(query, key, value, grad, bias, None)\n-    self.assertArraysAllClose(out_ref, out, rtol=1e-5, atol=1e-5)\n-    self.assertArraysAllClose(query_grad_ref, query_grad, rtol=1e-2, atol=1e-2)\n-    self.assertArraysAllClose(key_grad_ref, key_grad, rtol=1e-5, atol=1e-5)\n-    self.assertArraysAllClose(value_grad_ref, value_grad, rtol=1e-5, atol=1e-5)\n-    self.assertArraysAllClose(bias_grad_ref, bias_grad, rtol=1e-5, atol=1e-5)\n+      out, (query_grad, key_grad, value_grad, bias_grad) = \\\n+        jitted_sdpa_train(query, key, value, grad, bias, None)\n+      out_ref, (query_grad_ref, key_grad_ref, value_grad_ref, bias_grad_ref) = \\\n+        jitted_sdpa_train_ref(query, key, value, grad, bias, None)\n+      self.assertArraysAllClose(out_ref, out, rtol=1e-5, atol=1e-5)\n+      self.assertArraysAllClose(query_grad_ref, query_grad, rtol=1e-2, atol=1e-2)\n+      self.assertArraysAllClose(key_grad_ref, key_grad, rtol=1e-5, atol=1e-5)\n+      self.assertArraysAllClose(value_grad_ref, value_grad, rtol=1e-5, atol=1e-5)\n+      self.assertArraysAllClose(bias_grad_ref, bias_grad, rtol=1e-5, atol=1e-5)\n \n   @jtu.run_on_devices(\"cuda\")\n   def test_layouts(self):"
                }
            ],
            "whole_deleted": "-        k1, (2, 1024, 4, 64), dtype=jnp.bfloat16)\n-        k2, (2, 1024, 4, 64), dtype=jnp.bfloat16)\n-        k3, (2, 1024, 4, 64), dtype=jnp.bfloat16)\n-        k4, (2, 1024, 4, 64), dtype=jnp.bfloat16)\n-    jitted_sdpa_train = jax.jit(\n-      partial(\n-        sdpa_train, scale=1.0, mask_type=MaskType.NO_MASK, dropout_rate=0),\n-    )\n-    jitted_sdpa_train_ref = jax.jit(\n-      partial(\n-        sdpa_train_ref, scale=1.0, mask_type=MaskType.NO_MASK, dropout_rate=0),\n-    )\n-    out, (query_grad, key_grad, value_grad, bias_grad) = \\\n-      jitted_sdpa_train(query, key, value, grad, bias, None)\n-    out_ref, (query_grad_ref, key_grad_ref, value_grad_ref, bias_grad_ref) = \\\n-      jitted_sdpa_train_ref(query, key, value, grad, bias, None)\n-    self.assertArraysAllClose(out_ref, out, rtol=1e-5, atol=1e-5)\n-    self.assertArraysAllClose(query_grad_ref, query_grad, rtol=1e-2, atol=1e-2)\n-    self.assertArraysAllClose(key_grad_ref, key_grad, rtol=1e-5, atol=1e-5)\n-    self.assertArraysAllClose(value_grad_ref, value_grad, rtol=1e-5, atol=1e-5)\n-    self.assertArraysAllClose(bias_grad_ref, bias_grad, rtol=1e-5, atol=1e-5)\n",
            "whole_added": "+        k1, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n+        k2, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n+        k3, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n+        k4, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n+    devices = np.array(jax.local_devices()[:4])\n+    devices = devices.reshape((2, 2))\n+    with Mesh(devices, (\"dp\", \"tp\")) as mesh:\n+      qkv_spec = PartitionSpec(\"dp\", None, \"tp\", None)\n+      qkv_sharding = NamedSharding(mesh, qkv_spec)\n+      bias_spec = PartitionSpec(\"tp\", None, None)\n+      bias_sharding = NamedSharding(mesh, bias_spec)\n+      replicated = NamedSharding(mesh, PartitionSpec())\n+      in_shardings = (qkv_sharding, qkv_sharding, qkv_sharding,\n+                      qkv_sharding, bias_sharding, replicated)\n+      out_shardings = (qkv_sharding, (qkv_sharding, qkv_sharding, qkv_sharding, bias_sharding))\n+      query = jax.device_put(query, qkv_sharding)\n+      key = jax.device_put(key, qkv_sharding)\n+      value = jax.device_put(value, qkv_sharding)\n+      grad = jax.device_put(grad, qkv_sharding)\n+      bias = jax.device_put(bias, bias_sharding)\n+      jitted_sdpa_train = jax.jit(\n+        partial(\n+          sdpa_train, scale=1.0, mask_type=MaskType.NO_MASK, dropout_rate=0),\n+        in_shardings=in_shardings,\n+        out_shardings=out_shardings\n+      )\n+      jitted_sdpa_train_ref = jax.jit(\n+        partial(\n+          sdpa_train_ref, scale=1.0, mask_type=MaskType.NO_MASK, dropout_rate=0),\n+        in_shardings=in_shardings,\n+        out_shardings=out_shardings\n+      )\n+      out, (query_grad, key_grad, value_grad, bias_grad) = \\\n+        jitted_sdpa_train(query, key, value, grad, bias, None)\n+      out_ref, (query_grad_ref, key_grad_ref, value_grad_ref, bias_grad_ref) = \\\n+        jitted_sdpa_train_ref(query, key, value, grad, bias, None)\n+      self.assertArraysAllClose(out_ref, out, rtol=1e-5, atol=1e-5)\n+      self.assertArraysAllClose(query_grad_ref, query_grad, rtol=1e-2, atol=1e-2)\n+      self.assertArraysAllClose(key_grad_ref, key_grad, rtol=1e-5, atol=1e-5)\n+      self.assertArraysAllClose(value_grad_ref, value_grad, rtol=1e-5, atol=1e-5)\n+      self.assertArraysAllClose(bias_grad_ref, bias_grad, rtol=1e-5, atol=1e-5)\n",
            "whole_hunk": "@@ -348,34 +348,54 @@ class DotProductAttentionTest(jtu.JaxTestCase):\n       self.skipTest(\"Requires >= cuDNN 8.9.6\")\n     k1, k2, k3, k4, k5 = jax.random.split(jax.random.key(0), 5)\n     query = jax.random.normal(\n-        k1, (2, 1024, 4, 64), dtype=jnp.bfloat16)\n+        k1, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n     key = jax.random.normal(\n-        k2, (2, 1024, 4, 64), dtype=jnp.bfloat16)\n+        k2, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n     value = jax.random.normal(\n-        k3, (2, 1024, 4, 64), dtype=jnp.bfloat16)\n+        k3, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n     grad = jax.random.normal(\n-        k4, (2, 1024, 4, 64), dtype=jnp.bfloat16)\n+        k4, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n     bias = jax.random.normal(\n         k5, (4, 1024, 1024), dtype=jnp.bfloat16)\n-    jitted_sdpa_train = jax.jit(\n-      partial(\n-        sdpa_train, scale=1.0, mask_type=MaskType.NO_MASK, dropout_rate=0),\n-    )\n+    devices = np.array(jax.local_devices()[:4])\n+    devices = devices.reshape((2, 2))\n+    with Mesh(devices, (\"dp\", \"tp\")) as mesh:\n+      qkv_spec = PartitionSpec(\"dp\", None, \"tp\", None)\n+      qkv_sharding = NamedSharding(mesh, qkv_spec)\n+      bias_spec = PartitionSpec(\"tp\", None, None)\n+      bias_sharding = NamedSharding(mesh, bias_spec)\n+      replicated = NamedSharding(mesh, PartitionSpec())\n+      in_shardings = (qkv_sharding, qkv_sharding, qkv_sharding,\n+                      qkv_sharding, bias_sharding, replicated)\n+      out_shardings = (qkv_sharding, (qkv_sharding, qkv_sharding, qkv_sharding, bias_sharding))\n+      query = jax.device_put(query, qkv_sharding)\n+      key = jax.device_put(key, qkv_sharding)\n+      value = jax.device_put(value, qkv_sharding)\n+      grad = jax.device_put(grad, qkv_sharding)\n+      bias = jax.device_put(bias, bias_sharding)\n+      jitted_sdpa_train = jax.jit(\n+        partial(\n+          sdpa_train, scale=1.0, mask_type=MaskType.NO_MASK, dropout_rate=0),\n+        in_shardings=in_shardings,\n+        out_shardings=out_shardings\n+      )\n \n-    jitted_sdpa_train_ref = jax.jit(\n-      partial(\n-        sdpa_train_ref, scale=1.0, mask_type=MaskType.NO_MASK, dropout_rate=0),\n-    )\n+      jitted_sdpa_train_ref = jax.jit(\n+        partial(\n+          sdpa_train_ref, scale=1.0, mask_type=MaskType.NO_MASK, dropout_rate=0),\n+        in_shardings=in_shardings,\n+        out_shardings=out_shardings\n+      )\n \n-    out, (query_grad, key_grad, value_grad, bias_grad) = \\\n-      jitted_sdpa_train(query, key, value, grad, bias, None)\n-    out_ref, (query_grad_ref, key_grad_ref, value_grad_ref, bias_grad_ref) = \\\n-      jitted_sdpa_train_ref(query, key, value, grad, bias, None)\n-    self.assertArraysAllClose(out_ref, out, rtol=1e-5, atol=1e-5)\n-    self.assertArraysAllClose(query_grad_ref, query_grad, rtol=1e-2, atol=1e-2)\n-    self.assertArraysAllClose(key_grad_ref, key_grad, rtol=1e-5, atol=1e-5)\n-    self.assertArraysAllClose(value_grad_ref, value_grad, rtol=1e-5, atol=1e-5)\n-    self.assertArraysAllClose(bias_grad_ref, bias_grad, rtol=1e-5, atol=1e-5)\n+      out, (query_grad, key_grad, value_grad, bias_grad) = \\\n+        jitted_sdpa_train(query, key, value, grad, bias, None)\n+      out_ref, (query_grad_ref, key_grad_ref, value_grad_ref, bias_grad_ref) = \\\n+        jitted_sdpa_train_ref(query, key, value, grad, bias, None)\n+      self.assertArraysAllClose(out_ref, out, rtol=1e-5, atol=1e-5)\n+      self.assertArraysAllClose(query_grad_ref, query_grad, rtol=1e-2, atol=1e-2)\n+      self.assertArraysAllClose(key_grad_ref, key_grad, rtol=1e-5, atol=1e-5)\n+      self.assertArraysAllClose(value_grad_ref, value_grad, rtol=1e-5, atol=1e-5)\n+      self.assertArraysAllClose(bias_grad_ref, bias_grad, rtol=1e-5, atol=1e-5)\n \n   @jtu.run_on_devices(\"cuda\")\n   def test_layouts(self):"
        }
    ]
},
{
    "Id": 4,
    "commit_link": "https://github.com/google/jax/commit/bb7a6995f936bec352d07f8cf2780ed2c2ab77a1",
    "date": "2024-07-15T13:58:23-07:00",
    "message": "Remove the `spmd_mode` check. It's disabled in OSS since a long time.\n\nPiperOrigin-RevId: 652591122",
    "changes": [
        {
            "name": "pxla.py",
            "path": "jax/_src/interpreters/pxla.py",
            "patches": [
                {
                    "old_start": 2153,
                    "old_length": 12,
                    "new_start": 2153,
                    "new_length": 6,
                    "hunk": "@@ -2153,12 +2153,6 @@ def to_gspmd_sharding(s: JSharding, ndim: int) -> GSPMDSharding:\n                         _device_list=getattr(s, '_internal_device_list', None))\n \n \n-# Dummy function which is a no-op in OSS since enhanced barrier is switched on\n-# in OSS.\n-def spmd_mode_check(da_object, inline):\n-  return\n-\n-\n def _discharge_refs_jaxpr(closed_jaxpr, in_shardings, in_layouts,\n                           donated_invars, out_shardings, out_layouts):\n   if any(isinstance(e, RefEffect) for e in closed_jaxpr.effects):\n"
                },
                {
                    "old_start": 2192,
                    "old_length": 7,
                    "new_start": 2186,
                    "new_length": 6,
                    "hunk": "@@ -2192,7 +2186,6 @@ def lower_sharding_computation(\n     donated_invars: Sequence[bool],\n     *,\n     keep_unused: bool,\n-    inline: bool,\n     devices_from_context: Sequence[xc.Device] | None,\n     lowering_platforms: tuple[str, ...] | None,\n     lowering_parameters: mlir.LoweringParameters,\n"
                },
                {
                    "old_start": 2274,
                    "old_length": 8,
                    "new_start": 2267,
                    "new_length": 6,
                    "hunk": "@@ -2274,8 +2267,6 @@ def lower_sharding_computation(\n   else:\n     propagated_out_mem_kinds = (None,) * len(global_out_avals)\n \n-  spmd_mode_check(da_object, inline)\n-\n   # 2. Build up the HLO\n   semantic_in_shardings = SemanticallyEqualShardings(\n       in_shardings, global_in_avals)  # type: ignore\n"
                }
            ],
            "whole_deleted": "-# Dummy function which is a no-op in OSS since enhanced barrier is switched on\n-# in OSS.\n-def spmd_mode_check(da_object, inline):\n-  return\n-\n-\n-    inline: bool,\n-  spmd_mode_check(da_object, inline)\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -2153,12 +2153,6 @@ def to_gspmd_sharding(s: JSharding, ndim: int) -> GSPMDSharding:\n                         _device_list=getattr(s, '_internal_device_list', None))\n \n \n-# Dummy function which is a no-op in OSS since enhanced barrier is switched on\n-# in OSS.\n-def spmd_mode_check(da_object, inline):\n-  return\n-\n-\n def _discharge_refs_jaxpr(closed_jaxpr, in_shardings, in_layouts,\n                           donated_invars, out_shardings, out_layouts):\n   if any(isinstance(e, RefEffect) for e in closed_jaxpr.effects):\n@@ -2192,7 +2186,6 @@ def lower_sharding_computation(\n     donated_invars: Sequence[bool],\n     *,\n     keep_unused: bool,\n-    inline: bool,\n     devices_from_context: Sequence[xc.Device] | None,\n     lowering_platforms: tuple[str, ...] | None,\n     lowering_parameters: mlir.LoweringParameters,\n@@ -2274,8 +2267,6 @@ def lower_sharding_computation(\n   else:\n     propagated_out_mem_kinds = (None,) * len(global_out_avals)\n \n-  spmd_mode_check(da_object, inline)\n-\n   # 2. Build up the HLO\n   semantic_in_shardings = SemanticallyEqualShardings(\n       in_shardings, global_in_avals)  # type: ignore\n"
        },
        {
            "name": "maps.py",
            "path": "jax/_src/maps.py",
            "patches": [
                {
                    "old_start": 715,
                    "old_length": 7,
                    "new_start": 715,
                    "new_length": 7,
                    "hunk": "@@ -715,7 +715,7 @@ def make_xmap_callable(fun: lu.WrappedFun,\n         core.ClosedJaxpr(jaxpr, consts), 'jit', name,\n         (UNSPECIFIED,) * len(in_avals), (UNSPECIFIED,) * len(out_avals),\n         (None,) * len(in_avals), (None,) * len(out_avals),\n-        donated_invars, keep_unused=True, inline=False,\n+        donated_invars, keep_unused=True,\n         devices_from_context=None, lowering_platforms=None,\n         lowering_parameters=lowering_parameters, pgle_profiler=None)\n \n"
                }
            ],
            "whole_deleted": "-        donated_invars, keep_unused=True, inline=False,\n",
            "whole_added": "+        donated_invars, keep_unused=True,\n",
            "whole_hunk": "@@ -715,7 +715,7 @@ def make_xmap_callable(fun: lu.WrappedFun,\n         core.ClosedJaxpr(jaxpr, consts), 'jit', name,\n         (UNSPECIFIED,) * len(in_avals), (UNSPECIFIED,) * len(out_avals),\n         (None,) * len(in_avals), (None,) * len(out_avals),\n-        donated_invars, keep_unused=True, inline=False,\n+        donated_invars, keep_unused=True,\n         devices_from_context=None, lowering_platforms=None,\n         lowering_parameters=lowering_parameters, pgle_profiler=None)\n \n"
        },
        {
            "name": "pjit.py",
            "path": "jax/_src/pjit.py",
            "patches": [
                {
                    "old_start": 1781,
                    "old_length": 7,
                    "new_start": 1781,
                    "new_length": 7,
                    "hunk": "@@ -1781,7 +1781,7 @@ def _pjit_lower_cached(\n     return pxla.lower_sharding_computation(\n         jaxpr, api_name, name, in_shardings, out_shardings,\n         in_layouts, out_layouts, tuple(donated_invars),\n-        keep_unused=keep_unused, inline=inline,\n+        keep_unused=keep_unused,\n         devices_from_context=(\n             None if mesh is None or mesh.empty else list(mesh.devices.flat)),\n         lowering_platforms=lowering_platforms,"
                }
            ],
            "whole_deleted": "-        keep_unused=keep_unused, inline=inline,\n",
            "whole_added": "+        keep_unused=keep_unused,\n",
            "whole_hunk": "@@ -1781,7 +1781,7 @@ def _pjit_lower_cached(\n     return pxla.lower_sharding_computation(\n         jaxpr, api_name, name, in_shardings, out_shardings,\n         in_layouts, out_layouts, tuple(donated_invars),\n-        keep_unused=keep_unused, inline=inline,\n+        keep_unused=keep_unused,\n         devices_from_context=(\n             None if mesh is None or mesh.empty else list(mesh.devices.flat)),\n         lowering_platforms=lowering_platforms,"
        }
    ]
},
{
    "Id": 5,
    "commit_link": "https://github.com/google/jax/commit/7016ca48297dbd98b0d9481517c8566885120c04",
    "date": "2024-07-12T13:59:50-07:00",
    "message": "[Mosaic] Strengthen check on return types from RegionOp\n\nPiperOrigin-RevId: 651879359",
    "changes": [
        {
            "name": "tpu_ops.cc",
            "path": "jaxlib/mosaic/dialect/tpu/tpu_ops.cc",
            "patches": [
                {
                    "old_start": 360,
                    "old_length": 8,
                    "new_start": 360,
                    "new_length": 9,
                    "hunk": "@@ -360,8 +360,9 @@ LogicalResult WaitDMAOp::verify() {\n \n LogicalResult RegionOp::verify() {\n   for (auto result_type : getResultTypes()) {\n-    if (isa<MemRefType>(result_type)) {\n-      return emitOpError(\"Region result cannot be a memref.\");\n+    if (!isa<FloatType, IntegerType, VectorType, IndexType>(result_type)) {\n+      return emitOpError(\n+          \"Region result must be float, int, index or a vector type.\");\n     }\n   }\n   return success();"
                }
            ],
            "whole_deleted": "-    if (isa<MemRefType>(result_type)) {\n-      return emitOpError(\"Region result cannot be a memref.\");\n",
            "whole_added": "+    if (!isa<FloatType, IntegerType, VectorType, IndexType>(result_type)) {\n+      return emitOpError(\n+          \"Region result must be float, int, index or a vector type.\");\n",
            "whole_hunk": "@@ -360,8 +360,9 @@ LogicalResult WaitDMAOp::verify() {\n \n LogicalResult RegionOp::verify() {\n   for (auto result_type : getResultTypes()) {\n-    if (isa<MemRefType>(result_type)) {\n-      return emitOpError(\"Region result cannot be a memref.\");\n+    if (!isa<FloatType, IntegerType, VectorType, IndexType>(result_type)) {\n+      return emitOpError(\n+          \"Region result must be float, int, index or a vector type.\");\n     }\n   }\n   return success();"
        }
    ]
},
{
    "Id": 6,
    "commit_link": "https://github.com/google/jax/commit/1e1bca0706c92907657d35ddb8f269ce507f61c6",
    "date": "2024-07-12T09:23:37-07:00",
    "message": "Check for layout mismatch between array's layout and layout specified via in_shardings to jit by only checking `major_to_minor` if `_tiling` is None. Otherwise, check the entire layout.\n\nPiperOrigin-RevId: 651796471",
    "changes": [
        {
            "name": "pxla.py",
            "path": "jax/_src/interpreters/pxla.py",
            "patches": [
                {
                    "old_start": 2726,
                    "old_length": 24,
                    "new_start": 2726,
                    "new_length": 21,
                    "hunk": "@@ -2726,24 +2726,21 @@ def maybe_recover_user_shardings(\n \n   return new_shardings\n \n-\n-def _check_xla_user_layout(ul, xl, what: str):\n+def is_user_xla_layout_equal(ul: DeviceLocalLayout | AutoLayout,\n+                             xl: DeviceLocalLayout) -> bool:\n   if xla_extension_version >= 274:\n-    if ul._tiling is None:\n-      if ul.major_to_minor != xl.major_to_minor:\n-        raise AssertionError(\n-            f\"Unexpected XLA layout override: (XLA) {xl} != {ul} \"\n-            f\"(User {what} layout)\")\n+    if isinstance(ul, DeviceLocalLayout) and ul._tiling is None:\n+      return ul.major_to_minor == xl.major_to_minor\n     else:\n-      if ul != xl:\n-        raise AssertionError(\n-            f\"Unexpected XLA layout override: (XLA) {xl} != {ul} \"\n-            f\"(User {what} layout)\")\n+      return ul == xl\n   else:\n-    if ul != xl:\n-      raise AssertionError(\n-          f\"Unexpected XLA layout override: (XLA) {xl} != {ul} \"\n-          f\"(User {what} layout)\")\n+    return ul == xl\n+\n+def _check_user_xla_layout(ul, xl, what: str):\n+  if not is_user_xla_layout_equal(ul, xl):\n+    raise AssertionError(\n+        f\"Unexpected XLA layout override: (XLA) {xl} != {ul} \"\n+        f\"(User {what} layout)\")\n \n \n def _get_layouts_from_executable(\n"
                },
                {
                    "old_start": 2763,
                    "old_length": 7,
                    "new_start": 2760,
                    "new_length": 7,
                    "hunk": "@@ -2763,7 +2760,7 @@ def _get_layouts_from_executable(\n   for x, i in safe_zip(in_layouts_xla, in_layouts):\n     x = DeviceLocalLayout.from_pjrt_layout(x)\n     if isinstance(i, DeviceLocalLayout):\n-      _check_xla_user_layout(i, x, \"input\")\n+      _check_user_xla_layout(i, x, \"input\")\n     # Always append the XLA layout because it has the full information\n     # (tiling, etc) even if the user layout does not specify tiling.\n     new_in_layouts.append(x)\n"
                },
                {
                    "old_start": 2772,
                    "old_length": 7,
                    "new_start": 2769,
                    "new_length": 7,
                    "hunk": "@@ -2772,7 +2769,7 @@ def _get_layouts_from_executable(\n   for x, o in safe_zip(out_layouts_xla, out_layouts):\n     x = DeviceLocalLayout.from_pjrt_layout(x)\n     if isinstance(o, DeviceLocalLayout):\n-      _check_xla_user_layout(o, x, \"output\")\n+      _check_user_xla_layout(o, x, \"output\")\n     # Always append the XLA layout because it has the full information\n     # (tiling, etc) even if the user layout does not specify tiling.\n     new_out_layouts.append(x)\n"
                }
            ],
            "whole_deleted": "-\n-def _check_xla_user_layout(ul, xl, what: str):\n-    if ul._tiling is None:\n-      if ul.major_to_minor != xl.major_to_minor:\n-        raise AssertionError(\n-            f\"Unexpected XLA layout override: (XLA) {xl} != {ul} \"\n-            f\"(User {what} layout)\")\n-      if ul != xl:\n-        raise AssertionError(\n-            f\"Unexpected XLA layout override: (XLA) {xl} != {ul} \"\n-            f\"(User {what} layout)\")\n-    if ul != xl:\n-      raise AssertionError(\n-          f\"Unexpected XLA layout override: (XLA) {xl} != {ul} \"\n-          f\"(User {what} layout)\")\n-      _check_xla_user_layout(i, x, \"input\")\n-      _check_xla_user_layout(o, x, \"output\")\n",
            "whole_added": "+def is_user_xla_layout_equal(ul: DeviceLocalLayout | AutoLayout,\n+                             xl: DeviceLocalLayout) -> bool:\n+    if isinstance(ul, DeviceLocalLayout) and ul._tiling is None:\n+      return ul.major_to_minor == xl.major_to_minor\n+      return ul == xl\n+    return ul == xl\n+\n+def _check_user_xla_layout(ul, xl, what: str):\n+  if not is_user_xla_layout_equal(ul, xl):\n+    raise AssertionError(\n+        f\"Unexpected XLA layout override: (XLA) {xl} != {ul} \"\n+        f\"(User {what} layout)\")\n+      _check_user_xla_layout(i, x, \"input\")\n+      _check_user_xla_layout(o, x, \"output\")\n",
            "whole_hunk": "@@ -2726,24 +2726,21 @@ def maybe_recover_user_shardings(\n \n   return new_shardings\n \n-\n-def _check_xla_user_layout(ul, xl, what: str):\n+def is_user_xla_layout_equal(ul: DeviceLocalLayout | AutoLayout,\n+                             xl: DeviceLocalLayout) -> bool:\n   if xla_extension_version >= 274:\n-    if ul._tiling is None:\n-      if ul.major_to_minor != xl.major_to_minor:\n-        raise AssertionError(\n-            f\"Unexpected XLA layout override: (XLA) {xl} != {ul} \"\n-            f\"(User {what} layout)\")\n+    if isinstance(ul, DeviceLocalLayout) and ul._tiling is None:\n+      return ul.major_to_minor == xl.major_to_minor\n     else:\n-      if ul != xl:\n-        raise AssertionError(\n-            f\"Unexpected XLA layout override: (XLA) {xl} != {ul} \"\n-            f\"(User {what} layout)\")\n+      return ul == xl\n   else:\n-    if ul != xl:\n-      raise AssertionError(\n-          f\"Unexpected XLA layout override: (XLA) {xl} != {ul} \"\n-          f\"(User {what} layout)\")\n+    return ul == xl\n+\n+def _check_user_xla_layout(ul, xl, what: str):\n+  if not is_user_xla_layout_equal(ul, xl):\n+    raise AssertionError(\n+        f\"Unexpected XLA layout override: (XLA) {xl} != {ul} \"\n+        f\"(User {what} layout)\")\n \n \n def _get_layouts_from_executable(\n@@ -2763,7 +2760,7 @@ def _get_layouts_from_executable(\n   for x, i in safe_zip(in_layouts_xla, in_layouts):\n     x = DeviceLocalLayout.from_pjrt_layout(x)\n     if isinstance(i, DeviceLocalLayout):\n-      _check_xla_user_layout(i, x, \"input\")\n+      _check_user_xla_layout(i, x, \"input\")\n     # Always append the XLA layout because it has the full information\n     # (tiling, etc) even if the user layout does not specify tiling.\n     new_in_layouts.append(x)\n@@ -2772,7 +2769,7 @@ def _get_layouts_from_executable(\n   for x, o in safe_zip(out_layouts_xla, out_layouts):\n     x = DeviceLocalLayout.from_pjrt_layout(x)\n     if isinstance(o, DeviceLocalLayout):\n-      _check_xla_user_layout(o, x, \"output\")\n+      _check_user_xla_layout(o, x, \"output\")\n     # Always append the XLA layout because it has the full information\n     # (tiling, etc) even if the user layout does not specify tiling.\n     new_out_layouts.append(x)\n"
        },
        {
            "name": "pjit.py",
            "path": "jax/_src/pjit.py",
            "patches": [
                {
                    "old_start": 1464,
                    "old_length": 8,
                    "new_start": 1463,
                    "new_length": 10,
                    "hunk": "@@ -1464,8 +1463,10 @@ def _resolve_in_layouts(args, jit_in_layouts, resolved_in_shardings, in_avals):\n       # arg_layout can be None because some backends don't implement the\n       # required layout methods. Hence `arr.layout` can return\n       # `Layout(None, sharding)`\n-      if (committed and not is_pmap_sharding and\n-          arg_layout is not None and arg_layout != jit_in_l):\n+      if (committed\n+          and not is_pmap_sharding\n+          and arg_layout is not None\n+          and not pxla.is_user_xla_layout_equal(jit_in_l, arg_layout)):\n         extra_msg = ''\n         if isinstance(jit_in_l, AutoLayout):\n           extra_msg = (\n"
                }
            ],
            "whole_deleted": "-      if (committed and not is_pmap_sharding and\n-          arg_layout is not None and arg_layout != jit_in_l):\n",
            "whole_added": "+      if (committed\n+          and not is_pmap_sharding\n+          and arg_layout is not None\n+          and not pxla.is_user_xla_layout_equal(jit_in_l, arg_layout)):\n",
            "whole_hunk": "@@ -1464,8 +1463,10 @@ def _resolve_in_layouts(args, jit_in_layouts, resolved_in_shardings, in_avals):\n       # arg_layout can be None because some backends don't implement the\n       # required layout methods. Hence `arr.layout` can return\n       # `Layout(None, sharding)`\n-      if (committed and not is_pmap_sharding and\n-          arg_layout is not None and arg_layout != jit_in_l):\n+      if (committed\n+          and not is_pmap_sharding\n+          and arg_layout is not None\n+          and not pxla.is_user_xla_layout_equal(jit_in_l, arg_layout)):\n         extra_msg = ''\n         if isinstance(jit_in_l, AutoLayout):\n           extra_msg = (\n"
        },
        {
            "name": "layout_test.py",
            "path": "tests/layout_test.py",
            "patches": [
                {
                    "old_start": 456,
                    "old_length": 6,
                    "new_start": 456,
                    "new_length": 35,
                    "hunk": "@@ -456,6 +456,35 @@ class LayoutTest(jtu.JaxTestCase):\n         '.*Length of major_to_minor and the rank of the value should match.*'):\n       jax.device_put(inp, l)\n \n+  def test_concrete_layout_in_shardings(self):\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+    s = NamedSharding(mesh, P('x', 'y'))\n+    shape = (16, 128)\n+    np_inp = np.arange(math.prod(shape)).reshape(shape)\n+    arr = jax.device_put(np_inp, s)\n+\n+    custom_dll = DLL(major_to_minor=(0, 1))\n+\n+    @partial(jax.jit, in_shardings=Layout(custom_dll, s))\n+    def f(x):\n+      return x.T\n+\n+    out = f(arr)\n+    self.assertArraysEqual(out, np_inp.T)\n+    self.assertEqual(out.layout.device_local_layout.major_to_minor,\n+                     custom_dll.major_to_minor[::-1])\n+\n+    custom_dll2 = DLL(major_to_minor=(1, 0))\n+\n+    @partial(jax.jit, in_shardings=Layout(custom_dll2, s))\n+    def g(x):\n+      return x.T\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        'Layout passed to jit does not match the layout on the respective arg'):\n+      g(arr)\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  def test_concrete_layout_in_shardings(self):\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+    s = NamedSharding(mesh, P('x', 'y'))\n+    shape = (16, 128)\n+    np_inp = np.arange(math.prod(shape)).reshape(shape)\n+    arr = jax.device_put(np_inp, s)\n+\n+    custom_dll = DLL(major_to_minor=(0, 1))\n+\n+    @partial(jax.jit, in_shardings=Layout(custom_dll, s))\n+    def f(x):\n+      return x.T\n+\n+    out = f(arr)\n+    self.assertArraysEqual(out, np_inp.T)\n+    self.assertEqual(out.layout.device_local_layout.major_to_minor,\n+                     custom_dll.major_to_minor[::-1])\n+\n+    custom_dll2 = DLL(major_to_minor=(1, 0))\n+\n+    @partial(jax.jit, in_shardings=Layout(custom_dll2, s))\n+    def g(x):\n+      return x.T\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        'Layout passed to jit does not match the layout on the respective arg'):\n+      g(arr)\n+\n",
            "whole_hunk": "@@ -456,6 +456,35 @@ class LayoutTest(jtu.JaxTestCase):\n         '.*Length of major_to_minor and the rank of the value should match.*'):\n       jax.device_put(inp, l)\n \n+  def test_concrete_layout_in_shardings(self):\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+    s = NamedSharding(mesh, P('x', 'y'))\n+    shape = (16, 128)\n+    np_inp = np.arange(math.prod(shape)).reshape(shape)\n+    arr = jax.device_put(np_inp, s)\n+\n+    custom_dll = DLL(major_to_minor=(0, 1))\n+\n+    @partial(jax.jit, in_shardings=Layout(custom_dll, s))\n+    def f(x):\n+      return x.T\n+\n+    out = f(arr)\n+    self.assertArraysEqual(out, np_inp.T)\n+    self.assertEqual(out.layout.device_local_layout.major_to_minor,\n+                     custom_dll.major_to_minor[::-1])\n+\n+    custom_dll2 = DLL(major_to_minor=(1, 0))\n+\n+    @partial(jax.jit, in_shardings=Layout(custom_dll2, s))\n+    def g(x):\n+      return x.T\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        'Layout passed to jit does not match the layout on the respective arg'):\n+      g(arr)\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())"
        }
    ]
},
{
    "Id": 7,
    "commit_link": "https://github.com/google/jax/commit/ff3dc0f5fb2187b69cf3e191bd1c0f66a2d2b71b",
    "date": "2024-07-12T08:10:43-07:00",
    "message": "Add check_compatible_aval checks to Layout. It checks if `len(major_to_minor) == len(aval.shape)`.\n\nPiperOrigin-RevId: 651777179",
    "changes": [
        {
            "name": "api.py",
            "path": "jax/_src/api.py",
            "patches": [
                {
                    "old_start": 2463,
                    "old_length": 13,
                    "new_start": 2463,
                    "new_length": 13,
                    "hunk": "@@ -2463,13 +2463,13 @@ def device_put(\n   with config.explicit_device_put_scope():\n     x_flat, treedef = tree_flatten(x)\n     if (device is None or\n-         isinstance(device, (xc.Device, Sharding, TransferToMemoryKind))):\n+        isinstance(device, (xc.Device, Sharding, TransferToMemoryKind))):\n       device_flat = [device] * len(x_flat)\n     else:\n       device_flat = flatten_axes(\"device_put device\", treedef, device)\n \n     if (src is None or\n-         isinstance(src, (xc.Device, Sharding, TransferToMemoryKind))):\n+        isinstance(src, (xc.Device, Sharding, TransferToMemoryKind))):\n       src_flat = [_infer_src_sharding(src, xf) for xf in x_flat]\n     else:\n       src_flat = flatten_axes(\"device_put source\", treedef, src)\n"
                }
            ],
            "whole_deleted": "-         isinstance(device, (xc.Device, Sharding, TransferToMemoryKind))):\n-         isinstance(src, (xc.Device, Sharding, TransferToMemoryKind))):\n",
            "whole_added": "+        isinstance(device, (xc.Device, Sharding, TransferToMemoryKind))):\n+        isinstance(src, (xc.Device, Sharding, TransferToMemoryKind))):\n",
            "whole_hunk": "@@ -2463,13 +2463,13 @@ def device_put(\n   with config.explicit_device_put_scope():\n     x_flat, treedef = tree_flatten(x)\n     if (device is None or\n-         isinstance(device, (xc.Device, Sharding, TransferToMemoryKind))):\n+        isinstance(device, (xc.Device, Sharding, TransferToMemoryKind))):\n       device_flat = [device] * len(x_flat)\n     else:\n       device_flat = flatten_axes(\"device_put device\", treedef, device)\n \n     if (src is None or\n-         isinstance(src, (xc.Device, Sharding, TransferToMemoryKind))):\n+        isinstance(src, (xc.Device, Sharding, TransferToMemoryKind))):\n       src_flat = [_infer_src_sharding(src, xf) for xf in x_flat]\n     else:\n       src_flat = flatten_axes(\"device_put source\", treedef, src)\n"
        },
        {
            "name": "layout.py",
            "path": "jax/_src/layout.py",
            "patches": [
                {
                    "old_start": 23,
                    "old_length": 6,
                    "new_start": 23,
                    "new_length": 7,
                    "hunk": "@@ -23,6 +23,7 @@ from jax._src.sharding_impls import AUTO as AutoSharding, is_auto\n from jax._src.lib import xla_client as xc\n from jax._src.lib import xla_extension_version\n \n+Shape = tuple[int, ...]\n \n class AutoLayout:\n \n"
                },
                {
                    "old_start": 83,
                    "old_length": 6,
                    "new_start": 84,
                    "new_length": 13,
                    "hunk": "@@ -83,6 +84,13 @@ if xla_extension_version >= 274:\n         xla_layout = xc.Layout(self.major_to_minor[::-1], self._tiling,  # type: ignore\n                                sub_byte_size)\n       return str(xla_layout)\n+\n+    def check_compatible_aval(self, aval_shape: Shape):\n+      if len(self.major_to_minor) != len(aval_shape):\n+        raise ValueError(\n+            f'Length of major_to_minor and the rank of the value should match.'\n+            f' Got major_to_minor={self.major_to_minor} and shape={aval_shape}')\n+\n else:\n   class DeviceLocalLayout:  # type: ignore\n     layout: xc.PjRtLayout\n"
                },
                {
                    "old_start": 111,
                    "old_length": 6,
                    "new_start": 119,
                    "new_length": 9,
                    "hunk": "@@ -111,6 +119,9 @@ else:\n     def _to_xla_layout(self, dtype) -> str:\n       return self._layout_str\n \n+    def check_compatible_aval(self, aval_shape: Shape):\n+      pass\n+\n \n LayoutOptions = Union[DeviceLocalLayout, None, AutoLayout]  # pytype: disable=invalid-annotation\n ShardingOptions = Union[Sharding, None, AutoSharding]\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+Shape = tuple[int, ...]\n+\n+    def check_compatible_aval(self, aval_shape: Shape):\n+      if len(self.major_to_minor) != len(aval_shape):\n+        raise ValueError(\n+            f'Length of major_to_minor and the rank of the value should match.'\n+            f' Got major_to_minor={self.major_to_minor} and shape={aval_shape}')\n+\n+    def check_compatible_aval(self, aval_shape: Shape):\n+      pass\n+\n",
            "whole_hunk": "@@ -23,6 +23,7 @@ from jax._src.sharding_impls import AUTO as AutoSharding, is_auto\n from jax._src.lib import xla_client as xc\n from jax._src.lib import xla_extension_version\n \n+Shape = tuple[int, ...]\n \n class AutoLayout:\n \n@@ -83,6 +84,13 @@ if xla_extension_version >= 274:\n         xla_layout = xc.Layout(self.major_to_minor[::-1], self._tiling,  # type: ignore\n                                sub_byte_size)\n       return str(xla_layout)\n+\n+    def check_compatible_aval(self, aval_shape: Shape):\n+      if len(self.major_to_minor) != len(aval_shape):\n+        raise ValueError(\n+            f'Length of major_to_minor and the rank of the value should match.'\n+            f' Got major_to_minor={self.major_to_minor} and shape={aval_shape}')\n+\n else:\n   class DeviceLocalLayout:  # type: ignore\n     layout: xc.PjRtLayout\n@@ -111,6 +119,9 @@ else:\n     def _to_xla_layout(self, dtype) -> str:\n       return self._layout_str\n \n+    def check_compatible_aval(self, aval_shape: Shape):\n+      pass\n+\n \n LayoutOptions = Union[DeviceLocalLayout, None, AutoLayout]  # pytype: disable=invalid-annotation\n ShardingOptions = Union[Sharding, None, AutoSharding]\n"
        },
        {
            "name": "pjit.py",
            "path": "jax/_src/pjit.py",
            "patches": [
                {
                    "old_start": 631,
                    "old_length": 7,
                    "new_start": 631,
                    "new_length": 7,
                    "hunk": "@@ -631,7 +631,7 @@ def _infer_params_impl(\n       in_avals, in_tree, dbg, device_or_backend_set, have_kwargs)\n \n   attr_token = _attr_token(flat_fun, in_type)\n-  jaxpr, consts, out_type, attrs_tracked = _create_pjit_jaxpr(\n+  jaxpr, consts, out_avals, attrs_tracked = _create_pjit_jaxpr(\n       flat_fun, in_type, attr_token, dbg,\n       HashableFunction(res_paths, closure=()),\n       IgnoreKey(ji.inline))\n"
                },
                {
                    "old_start": 640,
                    "old_length": 7,
                    "new_start": 640,
                    "new_length": 7,
                    "hunk": "@@ -640,7 +640,7 @@ def _infer_params_impl(\n   out_shardings_flat, out_layouts_flat = _check_and_canonicalize_out_shardings(\n       out_shardings_treedef, out_shardings_leaves, ji.out_layouts_treedef,\n       ji.out_layouts_leaves, HashableFunction(out_tree, closure=()),\n-      tuple(out_type), jaxpr.jaxpr.debug_info, device_or_backend_set)\n+      tuple(out_avals), jaxpr.jaxpr.debug_info, device_or_backend_set)\n \n   assert len(explicit_args) == len(in_shardings_flat) == len(in_layouts_flat)\n \n"
                },
                {
                    "old_start": 1123,
                    "old_length": 6,
                    "new_start": 1123,
                    "new_length": 9,
                    "hunk": "@@ -1123,6 +1123,9 @@ def _process_in_axis_resources(in_shardings_treedef, in_shardings_leaves,\n     pjit_check_aval_sharding(in_shardings_flat, in_avals,\n                              None if debug_info is None else debug_info.arg_names,\n                              \"pjit arguments\", allow_uneven_sharding=False)\n+    check_aval_layout_compatibility(\n+        in_layouts_flat, in_avals,\n+        None if debug_info is None else debug_info.arg_names, \"jit arguments\")\n   return in_shardings_flat, in_layouts_flat\n \n callsites: set[str] = set()\n"
                },
                {
                    "old_start": 1298,
                    "old_length": 11,
                    "new_start": 1301,
                    "new_length": 11,
                    "hunk": "@@ -1298,11 +1301,11 @@ def _create_pjit_jaxpr(\n @util.cache(max_size=4096, trace_context_in_key=False)\n def _check_and_canonicalize_out_shardings(\n     out_shardings_treedef, out_shardings_leaves, out_layouts_treedef,\n-    out_layouts_leaves, out_tree, out_type, debug_info, device_or_backend_set):\n+    out_layouts_leaves, out_tree, out_avals, debug_info, device_or_backend_set):\n   orig_out_shardings = tree_unflatten(out_shardings_treedef, out_shardings_leaves)\n   if (is_unspecified(orig_out_shardings) or\n       isinstance(orig_out_shardings, sharding.Sharding)):\n-    out_shardings_flat = (orig_out_shardings,) * len(out_type)\n+    out_shardings_flat = (orig_out_shardings,) * len(out_avals)\n   else:\n     out_shardings_flat = flatten_axis_resources(\n         \"pjit out_shardings\", out_tree(), orig_out_shardings,\n"
                },
                {
                    "old_start": 1310,
                    "old_length": 16,
                    "new_start": 1313,
                    "new_length": 19,
                    "hunk": "@@ -1310,16 +1313,19 @@ def _check_and_canonicalize_out_shardings(\n \n   out_layouts = tree_unflatten(out_layouts_treedef, out_layouts_leaves)\n   if out_layouts is None:\n-    out_layouts_flat = (out_layouts,) * len(out_type)\n+    out_layouts_flat = (out_layouts,) * len(out_avals)\n   else:\n     out_layouts_flat = flatten_axis_resources(\n         \"pjit out_layouts\", out_tree(), out_layouts, tupled_args=False)\n \n   if not config.dynamic_shapes.value:\n     pjit_check_aval_sharding(\n-        out_shardings_flat, out_type,\n+        out_shardings_flat, out_avals,\n         None if debug_info is None else debug_info.result_paths,\n         \"pjit outputs\", allow_uneven_sharding=False)\n+    check_aval_layout_compatibility(\n+        out_layouts_flat, out_avals,\n+        None if debug_info is None else debug_info.result_paths, \"jit outputs\")\n   return out_shardings_flat, out_layouts_flat\n \n \n"
                },
                {
                    "old_start": 1406,
                    "old_length": 6,
                    "new_start": 1412,
                    "new_length": 22,
                    "hunk": "@@ -1406,6 +1412,22 @@ def pjit_check_aval_sharding(\n                          f\"(full shape: {shape})\")\n \n \n+def check_aval_layout_compatibility(\n+    layouts, flat_avals, names: tuple[str, ...] | None, what_aval: str):\n+  new_names = [''] * len(layouts) if names is None else names\n+  for aval, l, name in zip(flat_avals, layouts, new_names):\n+    if l is None or isinstance(l, AutoLayout):\n+      continue\n+    name_str = f' with pytree key path {name}' if name else ''\n+    shape = aval.shape\n+    try:\n+      l.check_compatible_aval(shape)\n+    except ValueError as e:\n+      raise ValueError(\n+          f'One of {what_aval}{name_str} is incompatible with its layout '\n+          f'annotation {l}: {e}')\n+\n+\n # -------------------- pjit rules --------------------\n \n pjit_p = core.AxisPrimitive(\"pjit\")\n"
                },
                {
                    "old_start": 2513,
                    "old_length": 6,
                    "new_start": 2535,
                    "new_length": 9,
                    "hunk": "@@ -2513,6 +2535,9 @@ def with_sharding_constraint(x, shardings):\n       shardings_flat, x_flat, None, \"with_sharding_constraint arguments\",\n       allow_uneven_sharding=True)\n \n+  check_aval_layout_compatibility(user_layouts_flat, x_flat, None,\n+                                  \"with_sharding_constraint arguments\")\n+\n   outs = [sharding_constraint_p.bind(xf, sharding=s, layout=l,\n                                      resource_env=resource_env,\n                                      unconstrained_dims=ud)\n"
                }
            ],
            "whole_deleted": "-  jaxpr, consts, out_type, attrs_tracked = _create_pjit_jaxpr(\n-      tuple(out_type), jaxpr.jaxpr.debug_info, device_or_backend_set)\n-    out_layouts_leaves, out_tree, out_type, debug_info, device_or_backend_set):\n-    out_shardings_flat = (orig_out_shardings,) * len(out_type)\n-    out_layouts_flat = (out_layouts,) * len(out_type)\n-        out_shardings_flat, out_type,\n",
            "whole_added": "+  jaxpr, consts, out_avals, attrs_tracked = _create_pjit_jaxpr(\n+      tuple(out_avals), jaxpr.jaxpr.debug_info, device_or_backend_set)\n+    check_aval_layout_compatibility(\n+        in_layouts_flat, in_avals,\n+        None if debug_info is None else debug_info.arg_names, \"jit arguments\")\n+    out_layouts_leaves, out_tree, out_avals, debug_info, device_or_backend_set):\n+    out_shardings_flat = (orig_out_shardings,) * len(out_avals)\n+    out_layouts_flat = (out_layouts,) * len(out_avals)\n+        out_shardings_flat, out_avals,\n+    check_aval_layout_compatibility(\n+        out_layouts_flat, out_avals,\n+        None if debug_info is None else debug_info.result_paths, \"jit outputs\")\n+def check_aval_layout_compatibility(\n+    layouts, flat_avals, names: tuple[str, ...] | None, what_aval: str):\n+  new_names = [''] * len(layouts) if names is None else names\n+  for aval, l, name in zip(flat_avals, layouts, new_names):\n+    if l is None or isinstance(l, AutoLayout):\n+      continue\n+    name_str = f' with pytree key path {name}' if name else ''\n+    shape = aval.shape\n+    try:\n+      l.check_compatible_aval(shape)\n+    except ValueError as e:\n+      raise ValueError(\n+          f'One of {what_aval}{name_str} is incompatible with its layout '\n+          f'annotation {l}: {e}')\n+\n+\n+  check_aval_layout_compatibility(user_layouts_flat, x_flat, None,\n+                                  \"with_sharding_constraint arguments\")\n+\n",
            "whole_hunk": "@@ -631,7 +631,7 @@ def _infer_params_impl(\n       in_avals, in_tree, dbg, device_or_backend_set, have_kwargs)\n \n   attr_token = _attr_token(flat_fun, in_type)\n-  jaxpr, consts, out_type, attrs_tracked = _create_pjit_jaxpr(\n+  jaxpr, consts, out_avals, attrs_tracked = _create_pjit_jaxpr(\n       flat_fun, in_type, attr_token, dbg,\n       HashableFunction(res_paths, closure=()),\n       IgnoreKey(ji.inline))\n@@ -640,7 +640,7 @@ def _infer_params_impl(\n   out_shardings_flat, out_layouts_flat = _check_and_canonicalize_out_shardings(\n       out_shardings_treedef, out_shardings_leaves, ji.out_layouts_treedef,\n       ji.out_layouts_leaves, HashableFunction(out_tree, closure=()),\n-      tuple(out_type), jaxpr.jaxpr.debug_info, device_or_backend_set)\n+      tuple(out_avals), jaxpr.jaxpr.debug_info, device_or_backend_set)\n \n   assert len(explicit_args) == len(in_shardings_flat) == len(in_layouts_flat)\n \n@@ -1123,6 +1123,9 @@ def _process_in_axis_resources(in_shardings_treedef, in_shardings_leaves,\n     pjit_check_aval_sharding(in_shardings_flat, in_avals,\n                              None if debug_info is None else debug_info.arg_names,\n                              \"pjit arguments\", allow_uneven_sharding=False)\n+    check_aval_layout_compatibility(\n+        in_layouts_flat, in_avals,\n+        None if debug_info is None else debug_info.arg_names, \"jit arguments\")\n   return in_shardings_flat, in_layouts_flat\n \n callsites: set[str] = set()\n@@ -1298,11 +1301,11 @@ def _create_pjit_jaxpr(\n @util.cache(max_size=4096, trace_context_in_key=False)\n def _check_and_canonicalize_out_shardings(\n     out_shardings_treedef, out_shardings_leaves, out_layouts_treedef,\n-    out_layouts_leaves, out_tree, out_type, debug_info, device_or_backend_set):\n+    out_layouts_leaves, out_tree, out_avals, debug_info, device_or_backend_set):\n   orig_out_shardings = tree_unflatten(out_shardings_treedef, out_shardings_leaves)\n   if (is_unspecified(orig_out_shardings) or\n       isinstance(orig_out_shardings, sharding.Sharding)):\n-    out_shardings_flat = (orig_out_shardings,) * len(out_type)\n+    out_shardings_flat = (orig_out_shardings,) * len(out_avals)\n   else:\n     out_shardings_flat = flatten_axis_resources(\n         \"pjit out_shardings\", out_tree(), orig_out_shardings,\n@@ -1310,16 +1313,19 @@ def _check_and_canonicalize_out_shardings(\n \n   out_layouts = tree_unflatten(out_layouts_treedef, out_layouts_leaves)\n   if out_layouts is None:\n-    out_layouts_flat = (out_layouts,) * len(out_type)\n+    out_layouts_flat = (out_layouts,) * len(out_avals)\n   else:\n     out_layouts_flat = flatten_axis_resources(\n         \"pjit out_layouts\", out_tree(), out_layouts, tupled_args=False)\n \n   if not config.dynamic_shapes.value:\n     pjit_check_aval_sharding(\n-        out_shardings_flat, out_type,\n+        out_shardings_flat, out_avals,\n         None if debug_info is None else debug_info.result_paths,\n         \"pjit outputs\", allow_uneven_sharding=False)\n+    check_aval_layout_compatibility(\n+        out_layouts_flat, out_avals,\n+        None if debug_info is None else debug_info.result_paths, \"jit outputs\")\n   return out_shardings_flat, out_layouts_flat\n \n \n@@ -1406,6 +1412,22 @@ def pjit_check_aval_sharding(\n                          f\"(full shape: {shape})\")\n \n \n+def check_aval_layout_compatibility(\n+    layouts, flat_avals, names: tuple[str, ...] | None, what_aval: str):\n+  new_names = [''] * len(layouts) if names is None else names\n+  for aval, l, name in zip(flat_avals, layouts, new_names):\n+    if l is None or isinstance(l, AutoLayout):\n+      continue\n+    name_str = f' with pytree key path {name}' if name else ''\n+    shape = aval.shape\n+    try:\n+      l.check_compatible_aval(shape)\n+    except ValueError as e:\n+      raise ValueError(\n+          f'One of {what_aval}{name_str} is incompatible with its layout '\n+          f'annotation {l}: {e}')\n+\n+\n # -------------------- pjit rules --------------------\n \n pjit_p = core.AxisPrimitive(\"pjit\")\n@@ -2513,6 +2535,9 @@ def with_sharding_constraint(x, shardings):\n       shardings_flat, x_flat, None, \"with_sharding_constraint arguments\",\n       allow_uneven_sharding=True)\n \n+  check_aval_layout_compatibility(user_layouts_flat, x_flat, None,\n+                                  \"with_sharding_constraint arguments\")\n+\n   outs = [sharding_constraint_p.bind(xf, sharding=s, layout=l,\n                                      resource_env=resource_env,\n                                      unconstrained_dims=ud)\n"
        },
        {
            "name": "layout_test.py",
            "path": "tests/layout_test.py",
            "patches": [
                {
                    "old_start": 14,
                    "old_length": 6,
                    "new_start": 14,
                    "new_length": 7,
                    "hunk": "@@ -14,6 +14,7 @@\n \n import contextlib\n import math\n+from functools import partial\n from absl.testing import absltest\n import numpy as np\n \n"
                },
                {
                    "old_start": 431,
                    "old_length": 6,
                    "new_start": 432,
                    "new_length": 30,
                    "hunk": "@@ -431,6 +432,30 @@ class LayoutTest(jtu.JaxTestCase):\n     self.assertEqual(out.layout.device_local_layout.major_to_minor,\n                      custom_dll.major_to_minor)\n \n+  def test_compatible_aval_error(self):\n+    custom_dll = DLL(major_to_minor=(0, 1, 2))\n+    l = Layout(custom_dll, SingleDeviceSharding(jax.devices()[0]))\n+    inp = np.arange(8)\n+\n+    @partial(jax.jit, in_shardings=l)\n+    def f(x):\n+      return x * 2\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        '.*Length of major_to_minor and the rank of the value should match.*'):\n+      f(inp)\n+\n+  def test_incompatible_aval_error_device_put(self):\n+    custom_dll = DLL(major_to_minor=(0, 1, 2))\n+    l = Layout(custom_dll, SingleDeviceSharding(jax.devices()[0]))\n+    inp = np.arange(8)\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        '.*Length of major_to_minor and the rank of the value should match.*'):\n+      jax.device_put(inp, l)\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+from functools import partial\n+  def test_compatible_aval_error(self):\n+    custom_dll = DLL(major_to_minor=(0, 1, 2))\n+    l = Layout(custom_dll, SingleDeviceSharding(jax.devices()[0]))\n+    inp = np.arange(8)\n+\n+    @partial(jax.jit, in_shardings=l)\n+    def f(x):\n+      return x * 2\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        '.*Length of major_to_minor and the rank of the value should match.*'):\n+      f(inp)\n+\n+  def test_incompatible_aval_error_device_put(self):\n+    custom_dll = DLL(major_to_minor=(0, 1, 2))\n+    l = Layout(custom_dll, SingleDeviceSharding(jax.devices()[0]))\n+    inp = np.arange(8)\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        '.*Length of major_to_minor and the rank of the value should match.*'):\n+      jax.device_put(inp, l)\n+\n",
            "whole_hunk": "@@ -14,6 +14,7 @@\n \n import contextlib\n import math\n+from functools import partial\n from absl.testing import absltest\n import numpy as np\n \n@@ -431,6 +432,30 @@ class LayoutTest(jtu.JaxTestCase):\n     self.assertEqual(out.layout.device_local_layout.major_to_minor,\n                      custom_dll.major_to_minor)\n \n+  def test_compatible_aval_error(self):\n+    custom_dll = DLL(major_to_minor=(0, 1, 2))\n+    l = Layout(custom_dll, SingleDeviceSharding(jax.devices()[0]))\n+    inp = np.arange(8)\n+\n+    @partial(jax.jit, in_shardings=l)\n+    def f(x):\n+      return x * 2\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        '.*Length of major_to_minor and the rank of the value should match.*'):\n+      f(inp)\n+\n+  def test_incompatible_aval_error_device_put(self):\n+    custom_dll = DLL(major_to_minor=(0, 1, 2))\n+    l = Layout(custom_dll, SingleDeviceSharding(jax.devices()[0]))\n+    inp = np.arange(8)\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        '.*Length of major_to_minor and the rank of the value should match.*'):\n+      jax.device_put(inp, l)\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())"
        }
    ]
},
{
    "Id": 8,
    "commit_link": "https://github.com/google/jax/commit/637370baa0dcde453f8e96a7f54a23800023d39e",
    "date": "2024-07-12T08:07:52-05:00",
    "message": "[ROCM] Fix version checks after rocm pjrt integration",
    "changes": [
        {
            "name": "linalg_test.py",
            "path": "tests/linalg_test.py",
            "patches": [
                {
                    "old_start": 47,
                    "old_length": 7,
                    "new_start": 47,
                    "new_length": 7,
                    "hunk": "@@ -47,7 +47,7 @@ int_types = jtu.dtypes.all_integer\n \n def _is_required_cuda_version_satisfied(cuda_version):\n   version = xla_bridge.get_backend().platform_version\n-  if version == \"<unknown>\" or version.split()[0] == \"rocm\":\n+  if version == \"<unknown>\" or \"rocm\" in version.split():\n     return False\n   else:\n     return int(version.split()[-1]) >= cuda_version\n"
                }
            ],
            "whole_deleted": "-  if version == \"<unknown>\" or version.split()[0] == \"rocm\":\n",
            "whole_added": "+  if version == \"<unknown>\" or \"rocm\" in version.split():\n",
            "whole_hunk": "@@ -47,7 +47,7 @@ int_types = jtu.dtypes.all_integer\n \n def _is_required_cuda_version_satisfied(cuda_version):\n   version = xla_bridge.get_backend().platform_version\n-  if version == \"<unknown>\" or version.split()[0] == \"rocm\":\n+  if version == \"<unknown>\" or \"rocm\" in version.split():\n     return False\n   else:\n     return int(version.split()[-1]) >= cuda_version\n"
        },
        {
            "name": "sparse_bcoo_bcsr_test.py",
            "path": "tests/sparse_bcoo_bcsr_test.py",
            "patches": [
                {
                    "old_start": 142,
                    "old_length": 7,
                    "new_start": 142,
                    "new_length": 7,
                    "hunk": "@@ -142,7 +142,7 @@ all_dtypes = jtu.dtypes.integer + jtu.dtypes.floating + jtu.dtypes.complex\n \n def _is_required_cuda_version_satisfied(cuda_version):\n   version = xla_bridge.get_backend().platform_version\n-  if version == \"<unknown>\" or version.split()[0] == \"rocm\":\n+  if version == \"<unknown>\" or \"rocm\" in version.split():\n     return False\n   else:\n     return int(version.split()[-1]) >= cuda_version\n"
                }
            ],
            "whole_deleted": "-  if version == \"<unknown>\" or version.split()[0] == \"rocm\":\n",
            "whole_added": "+  if version == \"<unknown>\" or \"rocm\" in version.split():\n",
            "whole_hunk": "@@ -142,7 +142,7 @@ all_dtypes = jtu.dtypes.integer + jtu.dtypes.floating + jtu.dtypes.complex\n \n def _is_required_cuda_version_satisfied(cuda_version):\n   version = xla_bridge.get_backend().platform_version\n-  if version == \"<unknown>\" or version.split()[0] == \"rocm\":\n+  if version == \"<unknown>\" or \"rocm\" in version.split():\n     return False\n   else:\n     return int(version.split()[-1]) >= cuda_version\n"
        },
        {
            "name": "sparse_test.py",
            "path": "tests/sparse_test.py",
            "patches": [
                {
                    "old_start": 414,
                    "old_length": 7,
                    "new_start": 414,
                    "new_length": 7,
                    "hunk": "@@ -414,7 +414,7 @@ class cuSparseTest(sptu.SparseTestCase):\n   @jtu.run_on_devices(\"gpu\")\n   def test_gpu_translation_rule(self):\n     version = xla_bridge.get_backend().platform_version\n-    if version.split()[0] != \"rocm\":\n+    if \"rocm\" not in version.split():\n       cuda_version = None if version == \"<unknown>\" else int(\n           version.split()[-1])\n       if cuda_version is None or cuda_version < 11000:"
                }
            ],
            "whole_deleted": "-    if version.split()[0] != \"rocm\":\n",
            "whole_added": "+    if \"rocm\" not in version.split():\n",
            "whole_hunk": "@@ -414,7 +414,7 @@ class cuSparseTest(sptu.SparseTestCase):\n   @jtu.run_on_devices(\"gpu\")\n   def test_gpu_translation_rule(self):\n     version = xla_bridge.get_backend().platform_version\n-    if version.split()[0] != \"rocm\":\n+    if \"rocm\" not in version.split():\n       cuda_version = None if version == \"<unknown>\" else int(\n           version.split()[-1])\n       if cuda_version is None or cuda_version < 11000:"
        }
    ]
},
{
    "Id": 9,
    "commit_link": "https://github.com/google/jax/commit/ad8e6713ba692f9e51c2376360b5832aeaa60458",
    "date": "2024-07-10T12:57:06-07:00",
    "message": "[JAX] Check if an array is deleted when resharding it with a different device order\n\n`jax.device_put()` that changes only the device order did not have a check on\nwhether the input array has been deleted or donated. When this is the case, it\nwould generate an error \"TypeError: 'NoneType' object is not iterable\" when\nattempting to access `array._arrays`\". Calling an explicit check makes the\nerror message more understandable.\n\nPiperOrigin-RevId: 651122614",
    "changes": [
        {
            "name": "dispatch.py",
            "path": "jax/_src/dispatch.py",
            "patches": [
                {
                    "old_start": 333,
                    "old_length": 6,
                    "new_start": 333,
                    "new_length": 7,
                    "hunk": "@@ -333,6 +333,7 @@ def _identity_fn(x):\n def _different_device_order_reshard(x, target_sharding):\n   from jax._src import api, array\n \n+  x._check_if_deleted()\n   inp_sharding = x.sharding\n \n   if inp_sharding._device_assignment == target_sharding._device_assignment:\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  x._check_if_deleted()\n",
            "whole_hunk": "@@ -333,6 +333,7 @@ def _identity_fn(x):\n def _different_device_order_reshard(x, target_sharding):\n   from jax._src import api, array\n \n+  x._check_if_deleted()\n   inp_sharding = x.sharding\n \n   if inp_sharding._device_assignment == target_sharding._device_assignment:\n"
        },
        {
            "name": "api_test.py",
            "path": "tests/api_test.py",
            "patches": [
                {
                    "old_start": 1932,
                    "old_length": 6,
                    "new_start": 1932,
                    "new_length": 45,
                    "hunk": "@@ -1932,6 +1932,45 @@ class APITest(jtu.JaxTestCase):\n       x = api.device_put(val, device=cpu_device)\n       self.assertEqual(x.devices(), {cpu_device})\n \n+  def test_device_put_on_single_device_donated_buffer_fails(self):\n+    @partial(jax.jit, donate_argnums=0)\n+    def f(inp1):\n+      return inp1 * 2\n+\n+    x = jnp.zeros((10,), jnp.float32)\n+    f(x)\n+\n+    with self.assertRaises(RuntimeError):\n+      result = jax.device_put(x, jax.devices()[0])\n+      result.block_until_ready()\n+\n+    with self.assertRaises(RuntimeError):\n+      result = jax.device_put(x, jax.devices()[-1])\n+      result.block_until_ready()\n+\n+  def test_device_put_on_multi_device_donated_buffer_fails(self):\n+    @partial(jax.jit, donate_argnums=0)\n+    def f(inp1):\n+      return inp1 * 2\n+\n+    mesh1 = jax.sharding.Mesh(jax.devices(), (\"x\",))\n+    s1 = jax.NamedSharding(mesh1, P(\"x\"))\n+\n+    mesh2 = jax.sharding.Mesh(tuple(reversed(jax.devices())), (\"x\",))\n+    s2 = jax.NamedSharding(mesh2, P(\"x\"))\n+\n+    x = jax.device_put(np.arange(len(jax.devices()), dtype=jnp.float32), s1)\n+    f(x)\n+\n+    with self.assertRaises(RuntimeError):\n+      result = jax.device_put(x, s1)\n+      result.block_until_ready()\n+\n+    with self.assertRaises(RuntimeError):\n+      result = jax.device_put(x, s2)\n+      result.block_until_ready()\n+\n+\n   @jax.default_matmul_precision(\"float32\")\n   def test_jacobian(self):\n     R = self.rng().randn"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  def test_device_put_on_single_device_donated_buffer_fails(self):\n+    @partial(jax.jit, donate_argnums=0)\n+    def f(inp1):\n+      return inp1 * 2\n+\n+    x = jnp.zeros((10,), jnp.float32)\n+    f(x)\n+\n+    with self.assertRaises(RuntimeError):\n+      result = jax.device_put(x, jax.devices()[0])\n+      result.block_until_ready()\n+\n+    with self.assertRaises(RuntimeError):\n+      result = jax.device_put(x, jax.devices()[-1])\n+      result.block_until_ready()\n+\n+  def test_device_put_on_multi_device_donated_buffer_fails(self):\n+    @partial(jax.jit, donate_argnums=0)\n+    def f(inp1):\n+      return inp1 * 2\n+\n+    mesh1 = jax.sharding.Mesh(jax.devices(), (\"x\",))\n+    s1 = jax.NamedSharding(mesh1, P(\"x\"))\n+\n+    mesh2 = jax.sharding.Mesh(tuple(reversed(jax.devices())), (\"x\",))\n+    s2 = jax.NamedSharding(mesh2, P(\"x\"))\n+\n+    x = jax.device_put(np.arange(len(jax.devices()), dtype=jnp.float32), s1)\n+    f(x)\n+\n+    with self.assertRaises(RuntimeError):\n+      result = jax.device_put(x, s1)\n+      result.block_until_ready()\n+\n+    with self.assertRaises(RuntimeError):\n+      result = jax.device_put(x, s2)\n+      result.block_until_ready()\n+\n+\n",
            "whole_hunk": "@@ -1932,6 +1932,45 @@ class APITest(jtu.JaxTestCase):\n       x = api.device_put(val, device=cpu_device)\n       self.assertEqual(x.devices(), {cpu_device})\n \n+  def test_device_put_on_single_device_donated_buffer_fails(self):\n+    @partial(jax.jit, donate_argnums=0)\n+    def f(inp1):\n+      return inp1 * 2\n+\n+    x = jnp.zeros((10,), jnp.float32)\n+    f(x)\n+\n+    with self.assertRaises(RuntimeError):\n+      result = jax.device_put(x, jax.devices()[0])\n+      result.block_until_ready()\n+\n+    with self.assertRaises(RuntimeError):\n+      result = jax.device_put(x, jax.devices()[-1])\n+      result.block_until_ready()\n+\n+  def test_device_put_on_multi_device_donated_buffer_fails(self):\n+    @partial(jax.jit, donate_argnums=0)\n+    def f(inp1):\n+      return inp1 * 2\n+\n+    mesh1 = jax.sharding.Mesh(jax.devices(), (\"x\",))\n+    s1 = jax.NamedSharding(mesh1, P(\"x\"))\n+\n+    mesh2 = jax.sharding.Mesh(tuple(reversed(jax.devices())), (\"x\",))\n+    s2 = jax.NamedSharding(mesh2, P(\"x\"))\n+\n+    x = jax.device_put(np.arange(len(jax.devices()), dtype=jnp.float32), s1)\n+    f(x)\n+\n+    with self.assertRaises(RuntimeError):\n+      result = jax.device_put(x, s1)\n+      result.block_until_ready()\n+\n+    with self.assertRaises(RuntimeError):\n+      result = jax.device_put(x, s2)\n+      result.block_until_ready()\n+\n+\n   @jax.default_matmul_precision(\"float32\")\n   def test_jacobian(self):\n     R = self.rng().randn"
        }
    ]
},
{
    "Id": 11,
    "commit_link": "https://github.com/google/jax/commit/33bd2925f088f974a85b2520f70119b4e16d4144",
    "date": "2024-07-10T06:12:19-07:00",
    "message": "[export] Fix poly shape check for vjp function with integer valued, polymorphic output.\n\nPiperOrigin-RevId: 650990009",
    "changes": [
        {
            "name": "_export.py",
            "path": "jax/_src/export/_export.py",
            "patches": [
                {
                    "old_start": 1142,
                    "old_length": 7,
                    "new_start": 1142,
                    "new_length": 7,
                    "hunk": "@@ -1142,7 +1142,7 @@ def call(exported: Exported) -> Callable[..., jax.Array]:\n     def fix_float0_ct(ct_res, expected_aval):\n       if expected_aval.dtype != dtypes.float0:\n         return ct_res\n-      return ad_util.zeros_like_aval(expected_aval)\n+      return ad_util.zeros_like_jaxval(ct_res)\n \n     ct_res_fixed = map(fix_float0_ct,\n                        ct_res_flat, exp_vjp.in_avals[len(args_flat):])\n"
                }
            ],
            "whole_deleted": "-      return ad_util.zeros_like_aval(expected_aval)\n",
            "whole_added": "+      return ad_util.zeros_like_jaxval(ct_res)\n",
            "whole_hunk": "@@ -1142,7 +1142,7 @@ def call(exported: Exported) -> Callable[..., jax.Array]:\n     def fix_float0_ct(ct_res, expected_aval):\n       if expected_aval.dtype != dtypes.float0:\n         return ct_res\n-      return ad_util.zeros_like_aval(expected_aval)\n+      return ad_util.zeros_like_jaxval(ct_res)\n \n     ct_res_fixed = map(fix_float0_ct,\n                        ct_res_flat, exp_vjp.in_avals[len(args_flat):])\n"
        },
        {
            "name": "export_test.py",
            "path": "tests/export_test.py",
            "patches": [
                {
                    "old_start": 462,
                    "old_length": 7,
                    "new_start": 462,
                    "new_length": 9,
                    "hunk": "@@ -462,7 +462,9 @@ class JaxExportTest(jtu.JaxTestCase):\n     self.assertAllClose(jax.grad(jax.grad(jax.grad(f)))(x),\n                         jax.grad(jax.grad(jax.grad(f1)))(x))\n \n-  def test_grad_int(self):\n+  @jtu.parameterized_filterable(\n+    kwargs=[dict(poly_shape=True), dict(poly_shape=False)])\n+  def test_grad_int(self, poly_shape):\n     def f(xi, xf):\n       return (2 * xi.T, xf.T * xf.T)\n \n"
                },
                {
                    "old_start": 480,
                    "old_length": 7,
                    "new_start": 482,
                    "new_length": 11,
                    "hunk": "@@ -480,7 +482,11 @@ class JaxExportTest(jtu.JaxTestCase):\n     self.assertAllClose(res, (xi_ct, xf_ct))\n     (f_outi_ct2, f_outf_ct2), = f_vjp2((xi_ct, xf_ct))\n \n-    exp = get_exported(jax.jit(f), vjp_order=2)(xi, xf)\n+    if poly_shape:\n+      args = export.symbolic_args_specs([xi, xf], shapes_specs=[\"2, a\", \"a, 4\"])\n+    else:\n+      args = (xi, xf)\n+    exp = get_exported(jax.jit(f), vjp_order=2)(*args)\n     fr = exp.call\n \n     res = fr(xi, xf)"
                }
            ],
            "whole_deleted": "-  def test_grad_int(self):\n-    exp = get_exported(jax.jit(f), vjp_order=2)(xi, xf)\n",
            "whole_added": "+  @jtu.parameterized_filterable(\n+    kwargs=[dict(poly_shape=True), dict(poly_shape=False)])\n+  def test_grad_int(self, poly_shape):\n+    if poly_shape:\n+      args = export.symbolic_args_specs([xi, xf], shapes_specs=[\"2, a\", \"a, 4\"])\n+    else:\n+      args = (xi, xf)\n+    exp = get_exported(jax.jit(f), vjp_order=2)(*args)\n",
            "whole_hunk": "@@ -462,7 +462,9 @@ class JaxExportTest(jtu.JaxTestCase):\n     self.assertAllClose(jax.grad(jax.grad(jax.grad(f)))(x),\n                         jax.grad(jax.grad(jax.grad(f1)))(x))\n \n-  def test_grad_int(self):\n+  @jtu.parameterized_filterable(\n+    kwargs=[dict(poly_shape=True), dict(poly_shape=False)])\n+  def test_grad_int(self, poly_shape):\n     def f(xi, xf):\n       return (2 * xi.T, xf.T * xf.T)\n \n@@ -480,7 +482,11 @@ class JaxExportTest(jtu.JaxTestCase):\n     self.assertAllClose(res, (xi_ct, xf_ct))\n     (f_outi_ct2, f_outf_ct2), = f_vjp2((xi_ct, xf_ct))\n \n-    exp = get_exported(jax.jit(f), vjp_order=2)(xi, xf)\n+    if poly_shape:\n+      args = export.symbolic_args_specs([xi, xf], shapes_specs=[\"2, a\", \"a, 4\"])\n+    else:\n+      args = (xi, xf)\n+    exp = get_exported(jax.jit(f), vjp_order=2)(*args)\n     fr = exp.call\n \n     res = fr(xi, xf)"
        }
    ]
},
{
    "Id": 13,
    "commit_link": "https://github.com/google/jax/commit/9e9acc9eccc1de7945c82cd978d99ace04d167a6",
    "date": "2024-07-03T11:07:58-04:00",
    "message": "Fix compatibility with nightly numpy\n\nNumpy recently merged support for the 2023.12 revision of the Array API:\nhttps://github.com/numpy/numpy/pull/26724\n\nThis breaks two of our tests:\n\n1. The first breakage was caused by differences in how numpy and JAX\n   cast negative floats to `uint8`. Specifically\n   `np.float32(-1).astype(np.uint8)` returns `np.uint8(255)` whereas\n   `jnp.float32(-1).astype(jnp.uint8)` produces `Array(0, dtype=uint8)`.\n   We don't make any promises about consistency with casting floats to\n   ints, noting that this can even be backend dependent. To fix our\n   test, we now only generate positive inputs when the output dtype is\n   unsigned.\n\n2. The second failure was caused by the fact that the approach we took\n   in #20550 to support backwards compatibility and the Array API for\n   `clip` differs from the one used in numpy/numpy#26724. Again, the\n   behavior is consistent, but it produces a different signature. I've\n   skipped checking `clip`'s signature, but we should revisit it once\n   the `a_min` and `a_max` parameters have been removed from JAX.\n\nFixes #22251",
    "changes": [
        {
            "name": "lax_numpy_reducers_test.py",
            "path": "tests/lax_numpy_reducers_test.py",
            "patches": [
                {
                    "old_start": 815,
                    "old_length": 10,
                    "new_start": 814,
                    "new_length": 14,
                    "hunk": "@@ -815,10 +814,14 @@ class JaxNumpyReducerTests(jtu.JaxTestCase):\n         out = jnp.concat([jnp.zeros(zeros_shape, dtype=out.dtype), out], axis=axis)\n       return out\n \n-\n     # We currently \"cheat\" to ensure we have JAX arrays, not NumPy arrays as\n     # input because we rely on JAX-specific casting behavior\n-    args_maker = lambda: [jnp.array(rng(shape, dtype))]\n+    def args_maker():\n+      x = jnp.array(rng(shape, dtype))\n+      if out_dtype in unsigned_dtypes:\n+        x = 10 * jnp.abs(x)\n+      return [x]\n+\n     np_op = getattr(np, \"cumulative_sum\", np_mock_op)\n     kwargs = dict(axis=axis, dtype=out_dtype, include_initial=include_initial)\n \n"
                }
            ],
            "whole_deleted": "-\n-    args_maker = lambda: [jnp.array(rng(shape, dtype))]\n",
            "whole_added": "+    def args_maker():\n+      x = jnp.array(rng(shape, dtype))\n+      if out_dtype in unsigned_dtypes:\n+        x = 10 * jnp.abs(x)\n+      return [x]\n+\n",
            "whole_hunk": "@@ -815,10 +814,14 @@ class JaxNumpyReducerTests(jtu.JaxTestCase):\n         out = jnp.concat([jnp.zeros(zeros_shape, dtype=out.dtype), out], axis=axis)\n       return out\n \n-\n     # We currently \"cheat\" to ensure we have JAX arrays, not NumPy arrays as\n     # input because we rely on JAX-specific casting behavior\n-    args_maker = lambda: [jnp.array(rng(shape, dtype))]\n+    def args_maker():\n+      x = jnp.array(rng(shape, dtype))\n+      if out_dtype in unsigned_dtypes:\n+        x = 10 * jnp.abs(x)\n+      return [x]\n+\n     np_op = getattr(np, \"cumulative_sum\", np_mock_op)\n     kwargs = dict(axis=axis, dtype=out_dtype, include_initial=include_initial)\n \n"
        },
        {
            "name": "lax_numpy_test.py",
            "path": "tests/lax_numpy_test.py",
            "patches": [
                {
                    "old_start": 5971,
                    "old_length": 6,
                    "new_start": 5971,
                    "new_length": 7,
                    "hunk": "@@ -5971,6 +5971,7 @@ class NumpySignaturesTest(jtu.JaxTestCase):\n       'copy': ['subok'],\n       'corrcoef': ['ddof', 'bias', 'dtype'],\n       'cov': ['dtype'],\n+      'cumulative_sum': ['out'],\n       'empty_like': ['subok', 'order'],\n       'einsum': ['kwargs'],\n       'einsum_path': ['einsum_call'],\n"
                },
                {
                    "old_start": 6021,
                    "old_length": 6,
                    "new_start": 6022,
                    "new_length": 15,
                    "hunk": "@@ -6021,6 +6022,15 @@ class NumpySignaturesTest(jtu.JaxTestCase):\n         # numpy 1.24 re-orders the density and weights arguments.\n         # TODO(jakevdp): migrate histogram APIs to match newer numpy versions.\n         continue\n+      if name == \"clip\":\n+        # JAX's support of the Array API spec for clip, and the way it handles\n+        # backwards compatibility was introduced in\n+        # https://github.com/google/jax/pull/20550 with a different signature\n+        # from the one in numpy, introduced in\n+        # https://github.com/numpy/numpy/pull/26724\n+        # TODO(dfm): After our deprecation period for the clip arguments ends\n+        # it should be possible to reintroduce the check.\n+        continue\n       # Note: can't use inspect.getfullargspec due to numpy issue\n       # https://github.com/numpy/numpy/issues/12225\n       try:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+      'cumulative_sum': ['out'],\n+      if name == \"clip\":\n+        # JAX's support of the Array API spec for clip, and the way it handles\n+        # backwards compatibility was introduced in\n+        # https://github.com/google/jax/pull/20550 with a different signature\n+        # from the one in numpy, introduced in\n+        # https://github.com/numpy/numpy/pull/26724\n+        # TODO(dfm): After our deprecation period for the clip arguments ends\n+        # it should be possible to reintroduce the check.\n+        continue\n",
            "whole_hunk": "@@ -5971,6 +5971,7 @@ class NumpySignaturesTest(jtu.JaxTestCase):\n       'copy': ['subok'],\n       'corrcoef': ['ddof', 'bias', 'dtype'],\n       'cov': ['dtype'],\n+      'cumulative_sum': ['out'],\n       'empty_like': ['subok', 'order'],\n       'einsum': ['kwargs'],\n       'einsum_path': ['einsum_call'],\n@@ -6021,6 +6022,15 @@ class NumpySignaturesTest(jtu.JaxTestCase):\n         # numpy 1.24 re-orders the density and weights arguments.\n         # TODO(jakevdp): migrate histogram APIs to match newer numpy versions.\n         continue\n+      if name == \"clip\":\n+        # JAX's support of the Array API spec for clip, and the way it handles\n+        # backwards compatibility was introduced in\n+        # https://github.com/google/jax/pull/20550 with a different signature\n+        # from the one in numpy, introduced in\n+        # https://github.com/numpy/numpy/pull/26724\n+        # TODO(dfm): After our deprecation period for the clip arguments ends\n+        # it should be possible to reintroduce the check.\n+        continue\n       # Note: can't use inspect.getfullargspec due to numpy issue\n       # https://github.com/numpy/numpy/issues/12225\n       try:"
        }
    ]
},
{
    "Id": 14,
    "commit_link": "https://github.com/google/jax/commit/484d09f4afd3784fb13f27ba716fb5663ee8cd05",
    "date": "2024-07-02T10:52:11-07:00",
    "message": "[Pallas][Mosaic] Relax dynamic index on 2nd minor dim in load/store.\n\nWe support any dynamic index on 2nd minor dim in either of the cases:\n1. The minormost dim size of a unsliced memref matches VREG lane count.\n2. Load/store one row on the second minormost dim, which triggers implicit strided load/store.\n\nNote: For the default cases which can not skip the alignment check, we still use dynamic slice + static load/store solution to reduce scalar core work. We should figure out a way to optimize this in all cases.\nPiperOrigin-RevId: 648771794",
    "changes": [
        {
            "name": "apply_vector_layout.cc",
            "path": "jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc",
            "patches": [
                {
                    "old_start": 2741,
                    "old_length": 7,
                    "new_start": 2741,
                    "new_length": 7,
                    "hunk": "@@ -2741,7 +2741,7 @@ LogicalResult vector_load_rule(RewriteContext &ctx, Operation &op,\n   const VectorLayout &layout_out = *layouts_out.front();\n   ImplicitLocOpBuilder builder(op.getLoc(), &op);\n   auto load_op = cast<vector::LoadOp>(op);\n-  const auto memref_ty = cast<MemRefType>(load_op.getBase().getType());\n+  const auto memref_ty = getMemRefType(load_op.getBase());\n   const auto vty = cast<VectorType>(load_op.getResult().getType());\n   FAILUREOR_ASSIGN_OR_RETURN(\n       VectorType target_ty,\n"
                },
                {
                    "old_start": 2772,
                    "old_length": 36,
                    "new_start": 2772,
                    "new_length": 80,
                    "hunk": "@@ -2772,36 +2772,80 @@ LogicalResult vector_load_rule(RewriteContext &ctx, Operation &op,\n   }\n   // TODO(apaszke): Check that loads are from vmem!\n \n-  int tiled_dims = is_1d ? 1 : 2;\n-  Value base_addr;\n-  SmallVector<int64_t> base_indices;\n-  if (auto const_indices =\n-          getIntConstsFromOperandRange(load_op.getIndices(), /*silent=*/true);\n-      succeeded(const_indices)) {\n-    base_addr = load_op.getBase();\n-    base_indices = std::move(*const_indices);\n-  } else {\n-    auto slice_result =\n-        sliceRef(builder, load_op.getBase(), load_op.getVectorType().getShape(),\n-                 load_op.getIndices(),\n-                 ArrayRef<int64_t>(memref_tiling).take_back(tiled_dims));\n-    if (failed(slice_result)) {\n-      return failure();\n+  bool can_support_unaligned_dynamic_index = false;\n+  bool must_support_unaligned_dynamic_index = false;\n+  if (load_op.getIndices().size() > 1) {\n+    auto second_minor_idx = load_op.getIndices().take_back(2)[0];\n+    if (failed(getIntConst(second_minor_idx, /*silent=*/true)) &&\n+        !isGuaranteedDivisible(second_minor_idx, memref_tiling[0])) {\n+      must_support_unaligned_dynamic_index = true;\n     }\n-    std::tie(base_addr, base_indices) = *slice_result;\n   }\n-  auto tile_base_idxs = ArrayRef<int64_t>(base_indices).take_back(tiled_dims);\n-  auto batch_base_idxs = ArrayRef<int64_t>(base_indices).drop_back(tiled_dims);\n-\n   const SmallVector<int64_t> implicit_shape =\n       layout_out.implicitShape(vty.getShape());\n   const int64_t ss = implicit_shape[implicit_shape.size() - 2];\n   int64_t sublane_stride = 1;\n+  // Handle special patterns that allow us to support more flexible loads.\n   if (layout_out.bitwidth() == 32 &&\n       layout_out.tiling() == std::array<int64_t, 2>{1, ctx.target_shape[1]} &&\n       ss == 1) {\n+    // Loading a single row on the 2nd minor dim into the (1, 128) layout. We\n+    // can use sublane striding to perform the relayout as part of the load.\n     sublane_stride = memref_tiling[0];\n+    can_support_unaligned_dynamic_index = true;\n+  } else {\n+    // Otherwise, if the memref has a short last dimension and is contiguous\n+    // all the tiled layouts become equivalent, so we can handle unaligned\n+    // dynamic indices without any special case.\n+    auto mem_layout = dyn_cast<TiledLayoutAttr>(memref_ty.getLayout());\n+    if (!mem_layout) {\n+      return op.emitOpError(\"Expected a tiled memref\");\n+    }\n+    auto tile_strides = mem_layout.getTileStrides();\n+    if (memref_ty.getShape().back() == ctx.target_shape[1] &&\n+        tile_strides.take_back(2) == ArrayRef<int64_t>{1, 1}) {\n+      can_support_unaligned_dynamic_index = true;\n+    }\n   }\n+\n+  auto add_idx = [&](const Value &v, int64_t d) -> Value {\n+    if (auto cst = getIntConst(v, /*silent=*/true); succeeded(cst)) {\n+      return IdxConst(cst.value() + d, builder, op.getLoc());\n+    }\n+    return builder.create<arith::AddIOp>(v, IdxConst(d, builder, op.getLoc()));\n+  };\n+\n+  int tiled_dims = is_1d ? 1 : 2;\n+  Value base_addr = load_op.getBase();\n+  SmallVector<Value, 4> base_indices = load_op.getIndices();\n+\n+  if (must_support_unaligned_dynamic_index) {\n+    if (!can_support_unaligned_dynamic_index) {\n+      return op.emitOpError(\n+          \"Not implemented: dynamic load with unaligned indices\");\n+    }\n+  } else {\n+    // Convert dynamic load to dynamic slice + static load. This saves us a\n+    // bunch of scalar core work.\n+    auto slice_result =\n+        sliceRef(builder, load_op.getBase(), load_op.getVectorType().getShape(),\n+                 load_op.getIndices(),\n+                 ArrayRef<int64_t>(memref_tiling).take_back(tiled_dims));\n+    if (failed(slice_result)) {\n+      return failure();\n+    }\n+    base_addr = slice_result->first;\n+    CHECK_EQ(slice_result->second.size(), base_indices.size());\n+    for (int i = 0; i < base_indices.size(); ++i) {\n+      base_indices[i] = IdxConst(slice_result->second[i], builder, op.getLoc());\n+    }\n+  }\n+\n+  // TODO(jevinjiang): ideally we should update the base addr and use static\n+  // indices even for the cases that can skip alignment check. This can save us\n+  // a bunch of scalar core work.\n+  auto tile_base_idxs = ArrayRef<Value>(base_indices).take_back(tiled_dims);\n+  auto batch_base_idxs = ArrayRef<Value>(base_indices).drop_back(tiled_dims);\n   const LayoutOffsets offsets = layout_out.offsets();\n   AffineMap load_map;\n   arith::ConstantOp padding;\n"
                },
                {
                    "old_start": 2841,
                    "old_length": 21,
                    "new_start": 2885,
                    "new_length": 18,
                    "hunk": "@@ -2841,21 +2885,18 @@ LogicalResult vector_load_rule(RewriteContext &ctx, Operation &op,\n         CHECK_EQ(num_dims, tile_idxs.size());\n         SmallVector<Value> idxs(tile_idxs.size());\n         for (int64_t i = 0; i < num_batch_dims; ++i) {\n-          idxs[i] = IdxConst(batch_base_idxs[i] + tile_idxs[i], builder,\n-                             load_op->getLoc());\n+          idxs[i] = add_idx(batch_base_idxs[i], tile_idxs[i]);\n         }\n-        const int64_t base_l = tile_base_idxs.back();\n+        const auto base_l = tile_base_idxs.back();\n         const int64_t lidx = tile_idxs[num_dims - 1];\n         idxs[num_dims - 1] =\n-            IdxConst(base_l + lidx * vreg_slice[1] - *offsets[1], builder,\n-                     load_op->getLoc());\n+            add_idx(base_l, lidx * vreg_slice[1] - offsets[1].value_or(0));\n         if (!is_1d) {\n           CHECK_EQ(tile_base_idxs.size(), 2);\n-          const int64_t base_s = tile_base_idxs.front();\n+          const auto base_s = tile_base_idxs.front();\n           const int64_t sidx = tile_idxs[num_dims - 2];\n           idxs[num_dims - 2] =\n-              IdxConst(base_s + sidx * vreg_slice[0] - offsets[0].value_or(0),\n-                       builder, load_op->getLoc());\n+              add_idx(base_s, sidx * vreg_slice[0] - offsets[0].value_or(0));\n         }\n         TPU_ASSERT_OP(tile_idxs[num_dims - 1] + ctx.target_shape[1] <=\n                       memref_ty.getShape()[num_dims - 1]);\n"
                },
                {
                    "old_start": 3919,
                    "old_length": 6,
                    "new_start": 3960,
                    "new_length": 7,
                    "hunk": "@@ -3919,6 +3960,7 @@ LogicalResult vector_store_rule(RewriteContext &ctx, Operation &op,\n   vector::StoreOp store_op = cast<vector::StoreOp>(op);\n   const VectorType ty = store_op.getValueToStore().getType();\n   const VectorLayout &to_store_layout = *layouts_in.front();\n+  const auto memref_ty = getMemRefType(store_op.getBase());\n   if (!ty.getRank()) {\n     return op.emitOpError(\"Not implemented: scalar stores to vmem\");\n   }\n"
                },
                {
                    "old_start": 3944,
                    "old_length": 15,
                    "new_start": 3986,
                    "new_length": 59,
                    "hunk": "@@ -3944,15 +3986,59 @@ LogicalResult vector_store_rule(RewriteContext &ctx, Operation &op,\n     }\n   }\n \n+  bool can_support_unaligned_dynamic_index = false;\n+  bool must_support_unaligned_dynamic_index = false;\n+  if (store_op.getIndices().size() > 1) {\n+    auto second_minor_idx = store_op.getIndices().take_back(2)[0];\n+    if (failed(getIntConst(second_minor_idx, /*silent=*/true)) &&\n+        !isGuaranteedDivisible(second_minor_idx, memref_tiling[0])) {\n+      must_support_unaligned_dynamic_index = true;\n+    }\n+  }\n+  int64_t sublane_stride = 1;\n+  // Handle special patterns that allow us to support more flexible loads.\n+  if (to_store_layout.bitwidth() == 32 &&\n+      to_store_layout.tiling() == Tiling{1, ctx.target_shape[1]}) {\n+    // Storing a single row on the 2nd minor dim from the (1, 128) layout. We\n+    // can use sublane striding to perform the relayout as part of the store.\n+    // The stride of store should be the number of sublanes in memref tile when\n+    // store a single sublane.\n+    sublane_stride = memref_tiling[0];\n+    can_support_unaligned_dynamic_index = true;\n+  } else {\n+    // Otherwise, if the memref has a short last dimension and is contiguous\n+    // all the tiled layouts become equivalent, so we can handle unaligned\n+    // dynamic indices without any special case.\n+    auto mem_layout = dyn_cast<TiledLayoutAttr>(memref_ty.getLayout());\n+    if (!mem_layout) {\n+      return op.emitOpError(\"Expected a tiled memref\");\n+    }\n+    auto tile_strides = mem_layout.getTileStrides();\n+    if (memref_ty.getShape().back() == ctx.target_shape[1] &&\n+        tile_strides.take_back(2) == ArrayRef<int64_t>{1, 1}) {\n+      can_support_unaligned_dynamic_index = true;\n+    }\n+  }\n+\n+  auto add_idx = [&](const Value &v, int64_t d) -> Value {\n+    if (auto cst = getIntConst(v, /*silent=*/true); succeeded(cst)) {\n+      return IdxConst(cst.value() + d, builder, op.getLoc());\n+    }\n+    return builder.create<arith::AddIOp>(v, IdxConst(d, builder, op.getLoc()));\n+  };\n+\n   int tiled_dims = is_1d ? 1 : 2;\n-  Value base_addr;\n-  SmallVector<int64_t> base_indices;\n-  if (auto const_indices =\n-          getIntConstsFromOperandRange(store_op.getIndices(), /*silent=*/true);\n-      succeeded(const_indices)) {\n-    base_addr = store_op.getBase();\n-    base_indices = std::move(*const_indices);\n+  Value base_addr = store_op.getBase();\n+  SmallVector<Value, 4> base_indices = store_op.getIndices();\n+\n+  if (must_support_unaligned_dynamic_index) {\n+    if (!can_support_unaligned_dynamic_index) {\n+      return op.emitOpError(\n+          \"Not implemented: dynamic store with unaligned indices\");\n+    }\n   } else {\n+    // Convert dynamic store to dynamic slice + static store. This saves us a\n+    // bunch of scalar core work.\n     auto slice_result =\n         sliceRef(builder, store_op.getBase(),\n                  store_op.getVectorType().getShape(), store_op.getIndices(),\n"
                },
                {
                    "old_start": 3960,
                    "old_length": 18,
                    "new_start": 4046,
                    "new_length": 27,
                    "hunk": "@@ -3960,18 +4046,27 @@ LogicalResult vector_store_rule(RewriteContext &ctx, Operation &op,\n     if (failed(slice_result)) {\n       return failure();\n     }\n-    std::tie(base_addr, base_indices) = *slice_result;\n+    base_addr = slice_result->first;\n+    CHECK_EQ(slice_result->second.size(), base_indices.size());\n+    for (int i = 0; i < base_indices.size(); ++i) {\n+      base_indices[i] = IdxConst(slice_result->second[i], builder, op.getLoc());\n+    }\n   }\n-  auto tile_base_idxs = ArrayRef<int64_t>(base_indices).take_back(tiled_dims);\n-  auto batch_base_idxs = ArrayRef<int64_t>(base_indices).drop_back(tiled_dims);\n+\n+  // TODO(jevinjiang): ideally we should update the base addr and use static\n+  // indices even for the cases that can skip alignment check. This can save\n+  // us a bunch of scalar core work.\n+  auto tile_base_idxs = ArrayRef<Value>(base_indices).take_back(tiled_dims);\n+  auto batch_base_idxs = ArrayRef<Value>(base_indices).drop_back(tiled_dims);\n \n   FAILUREOR_ASSIGN_OR_RETURN(\n       xla::Array<Value> tiles,\n       disassemble(builder, to_store_layout, store_op.getValueToStore(),\n                   ctx.target_shape));\n   const int64_t ndims = ty.getRank();\n-  const int64_t base_s = is_1d ? 0 : tile_base_idxs.front();\n-  const int64_t base_l = tile_base_idxs.back();\n+  const auto base_s =\n+      is_1d ? IdxConst(0, builder, op.getLoc()) : tile_base_idxs.front();\n+  const auto base_l = tile_base_idxs.back();\n   if (is_1d) {\n     tiles.Reshape(\n         to_store_layout.implicitShape(toArrayRef(tiles.dimensions())));\n"
                },
                {
                    "old_start": 3984,
                    "old_length": 13,
                    "new_start": 4079,
                    "new_length": 6,
                    "hunk": "@@ -3984,13 +4079,6 @@ LogicalResult vector_store_rule(RewriteContext &ctx, Operation &op,\n   }\n   const SmallVector<int64_t> stored_shape =\n       to_store_layout.implicitShape(ty.getShape());\n-  int64_t sublane_stride = 1;\n-  // The stride of store should be the number of sublanes in memref tile when\n-  // store a single sublane.\n-  if (to_store_layout.bitwidth() == 32 &&\n-      to_store_layout.tiling() == Tiling{1, ctx.target_shape[1]}) {\n-    sublane_stride = memref_tiling[0];\n-  }\n   const std::array<int64_t, 2> vreg_slice =\n       to_store_layout.vregSlice(ctx.target_shape);\n   const absl::Status status =\n"
                },
                {
                    "old_start": 4002,
                    "old_length": 17,
                    "new_start": 4090,
                    "new_length": 15,
                    "hunk": "@@ -4002,17 +4090,15 @@ LogicalResult vector_store_rule(RewriteContext &ctx, Operation &op,\n         const int64_t sidx = *(idx.end() - 2);\n         const int64_t lidx = *(idx.end() - 1);\n         SmallVector<Value> indices(ndims);\n-        auto boundIdxConst = std::bind(IdxConst, std::placeholders::_1, builder,\n-                                       store_op->getLoc());\n         for (int64_t i = 0; i < batch_base_idxs.size(); ++i) {\n-          indices[i] = boundIdxConst(batch_base_idxs[i] + idx[i]);\n+          indices[i] = add_idx(batch_base_idxs[i], idx[i]);\n         }\n         if (!is_1d) {\n           *(indices.end() - 2) =\n-              boundIdxConst(base_s + sidx * vreg_slice[0] - *sublane_offset);\n+              add_idx(base_s, sidx * vreg_slice[0] - *sublane_offset);\n         }\n         *(indices.end() - 1) =\n-            boundIdxConst(base_l + lidx * vreg_slice[1] - *lane_offset);\n+            add_idx(base_l, lidx * vreg_slice[1] - *lane_offset);\n         const DenseBoolArrayAttr sublane_mask =\n             bounds->getSublaneMask(store_op->getContext(), ctx.target_shape);\n         const bool masks_subelements =\n"
                },
                {
                    "old_start": 4079,
                    "old_length": 7,
                    "new_start": 4165,
                    "new_length": 7,
                    "hunk": "@@ -4079,7 +4165,7 @@ LogicalResult vector_store_rule(RewriteContext &ctx, Operation &op,\n   }\n   store_op->erase();\n   return success();\n-}\n+  }\n \n LogicalResult vector_transpose_rule(RewriteContext &ctx, Operation &op,\n                                     const ArrayRef<Layout> layouts_in,\n"
                }
            ],
            "whole_deleted": "-  const auto memref_ty = cast<MemRefType>(load_op.getBase().getType());\n-  int tiled_dims = is_1d ? 1 : 2;\n-  Value base_addr;\n-  SmallVector<int64_t> base_indices;\n-  if (auto const_indices =\n-          getIntConstsFromOperandRange(load_op.getIndices(), /*silent=*/true);\n-      succeeded(const_indices)) {\n-    base_addr = load_op.getBase();\n-    base_indices = std::move(*const_indices);\n-  } else {\n-    auto slice_result =\n-        sliceRef(builder, load_op.getBase(), load_op.getVectorType().getShape(),\n-                 load_op.getIndices(),\n-                 ArrayRef<int64_t>(memref_tiling).take_back(tiled_dims));\n-    if (failed(slice_result)) {\n-      return failure();\n-    std::tie(base_addr, base_indices) = *slice_result;\n-  auto tile_base_idxs = ArrayRef<int64_t>(base_indices).take_back(tiled_dims);\n-  auto batch_base_idxs = ArrayRef<int64_t>(base_indices).drop_back(tiled_dims);\n-\n-          idxs[i] = IdxConst(batch_base_idxs[i] + tile_idxs[i], builder,\n-                             load_op->getLoc());\n-        const int64_t base_l = tile_base_idxs.back();\n-            IdxConst(base_l + lidx * vreg_slice[1] - *offsets[1], builder,\n-                     load_op->getLoc());\n-          const int64_t base_s = tile_base_idxs.front();\n-              IdxConst(base_s + sidx * vreg_slice[0] - offsets[0].value_or(0),\n-                       builder, load_op->getLoc());\n-  Value base_addr;\n-  SmallVector<int64_t> base_indices;\n-  if (auto const_indices =\n-          getIntConstsFromOperandRange(store_op.getIndices(), /*silent=*/true);\n-      succeeded(const_indices)) {\n-    base_addr = store_op.getBase();\n-    base_indices = std::move(*const_indices);\n-    std::tie(base_addr, base_indices) = *slice_result;\n-  auto tile_base_idxs = ArrayRef<int64_t>(base_indices).take_back(tiled_dims);\n-  auto batch_base_idxs = ArrayRef<int64_t>(base_indices).drop_back(tiled_dims);\n-  const int64_t base_s = is_1d ? 0 : tile_base_idxs.front();\n-  const int64_t base_l = tile_base_idxs.back();\n-  int64_t sublane_stride = 1;\n-  // The stride of store should be the number of sublanes in memref tile when\n-  // store a single sublane.\n-  if (to_store_layout.bitwidth() == 32 &&\n-      to_store_layout.tiling() == Tiling{1, ctx.target_shape[1]}) {\n-    sublane_stride = memref_tiling[0];\n-  }\n-        auto boundIdxConst = std::bind(IdxConst, std::placeholders::_1, builder,\n-                                       store_op->getLoc());\n-          indices[i] = boundIdxConst(batch_base_idxs[i] + idx[i]);\n-              boundIdxConst(base_s + sidx * vreg_slice[0] - *sublane_offset);\n-            boundIdxConst(base_l + lidx * vreg_slice[1] - *lane_offset);\n-}\n",
            "whole_added": "+  const auto memref_ty = getMemRefType(load_op.getBase());\n+  bool can_support_unaligned_dynamic_index = false;\n+  bool must_support_unaligned_dynamic_index = false;\n+  if (load_op.getIndices().size() > 1) {\n+    auto second_minor_idx = load_op.getIndices().take_back(2)[0];\n+    if (failed(getIntConst(second_minor_idx, /*silent=*/true)) &&\n+        !isGuaranteedDivisible(second_minor_idx, memref_tiling[0])) {\n+      must_support_unaligned_dynamic_index = true;\n+  // Handle special patterns that allow us to support more flexible loads.\n+    // Loading a single row on the 2nd minor dim into the (1, 128) layout. We\n+    // can use sublane striding to perform the relayout as part of the load.\n+    can_support_unaligned_dynamic_index = true;\n+  } else {\n+    // Otherwise, if the memref has a short last dimension and is contiguous\n+    // all the tiled layouts become equivalent, so we can handle unaligned\n+    // dynamic indices without any special case.\n+    auto mem_layout = dyn_cast<TiledLayoutAttr>(memref_ty.getLayout());\n+    if (!mem_layout) {\n+      return op.emitOpError(\"Expected a tiled memref\");\n+    }\n+    auto tile_strides = mem_layout.getTileStrides();\n+    if (memref_ty.getShape().back() == ctx.target_shape[1] &&\n+        tile_strides.take_back(2) == ArrayRef<int64_t>{1, 1}) {\n+      can_support_unaligned_dynamic_index = true;\n+    }\n+\n+  auto add_idx = [&](const Value &v, int64_t d) -> Value {\n+    if (auto cst = getIntConst(v, /*silent=*/true); succeeded(cst)) {\n+      return IdxConst(cst.value() + d, builder, op.getLoc());\n+    }\n+    return builder.create<arith::AddIOp>(v, IdxConst(d, builder, op.getLoc()));\n+  };\n+\n+  int tiled_dims = is_1d ? 1 : 2;\n+  Value base_addr = load_op.getBase();\n+  SmallVector<Value, 4> base_indices = load_op.getIndices();\n+\n+  if (must_support_unaligned_dynamic_index) {\n+    if (!can_support_unaligned_dynamic_index) {\n+      return op.emitOpError(\n+          \"Not implemented: dynamic load with unaligned indices\");\n+    }\n+  } else {\n+    // Convert dynamic load to dynamic slice + static load. This saves us a\n+    // bunch of scalar core work.\n+    auto slice_result =\n+        sliceRef(builder, load_op.getBase(), load_op.getVectorType().getShape(),\n+                 load_op.getIndices(),\n+                 ArrayRef<int64_t>(memref_tiling).take_back(tiled_dims));\n+    if (failed(slice_result)) {\n+      return failure();\n+    }\n+    base_addr = slice_result->first;\n+    CHECK_EQ(slice_result->second.size(), base_indices.size());\n+    for (int i = 0; i < base_indices.size(); ++i) {\n+      base_indices[i] = IdxConst(slice_result->second[i], builder, op.getLoc());\n+    }\n+  }\n+\n+  // TODO(jevinjiang): ideally we should update the base addr and use static\n+  // indices even for the cases that can skip alignment check. This can save us\n+  // a bunch of scalar core work.\n+  auto tile_base_idxs = ArrayRef<Value>(base_indices).take_back(tiled_dims);\n+  auto batch_base_idxs = ArrayRef<Value>(base_indices).drop_back(tiled_dims);\n+          idxs[i] = add_idx(batch_base_idxs[i], tile_idxs[i]);\n+        const auto base_l = tile_base_idxs.back();\n+            add_idx(base_l, lidx * vreg_slice[1] - offsets[1].value_or(0));\n+          const auto base_s = tile_base_idxs.front();\n+              add_idx(base_s, sidx * vreg_slice[0] - offsets[0].value_or(0));\n+  const auto memref_ty = getMemRefType(store_op.getBase());\n+  bool can_support_unaligned_dynamic_index = false;\n+  bool must_support_unaligned_dynamic_index = false;\n+  if (store_op.getIndices().size() > 1) {\n+    auto second_minor_idx = store_op.getIndices().take_back(2)[0];\n+    if (failed(getIntConst(second_minor_idx, /*silent=*/true)) &&\n+        !isGuaranteedDivisible(second_minor_idx, memref_tiling[0])) {\n+      must_support_unaligned_dynamic_index = true;\n+    }\n+  }\n+  int64_t sublane_stride = 1;\n+  // Handle special patterns that allow us to support more flexible loads.\n+  if (to_store_layout.bitwidth() == 32 &&\n+      to_store_layout.tiling() == Tiling{1, ctx.target_shape[1]}) {\n+    // Storing a single row on the 2nd minor dim from the (1, 128) layout. We\n+    // can use sublane striding to perform the relayout as part of the store.\n+    // The stride of store should be the number of sublanes in memref tile when\n+    // store a single sublane.\n+    sublane_stride = memref_tiling[0];\n+    can_support_unaligned_dynamic_index = true;\n+  } else {\n+    // Otherwise, if the memref has a short last dimension and is contiguous\n+    // all the tiled layouts become equivalent, so we can handle unaligned\n+    // dynamic indices without any special case.\n+    auto mem_layout = dyn_cast<TiledLayoutAttr>(memref_ty.getLayout());\n+    if (!mem_layout) {\n+      return op.emitOpError(\"Expected a tiled memref\");\n+    }\n+    auto tile_strides = mem_layout.getTileStrides();\n+    if (memref_ty.getShape().back() == ctx.target_shape[1] &&\n+        tile_strides.take_back(2) == ArrayRef<int64_t>{1, 1}) {\n+      can_support_unaligned_dynamic_index = true;\n+    }\n+  }\n+\n+  auto add_idx = [&](const Value &v, int64_t d) -> Value {\n+    if (auto cst = getIntConst(v, /*silent=*/true); succeeded(cst)) {\n+      return IdxConst(cst.value() + d, builder, op.getLoc());\n+    }\n+    return builder.create<arith::AddIOp>(v, IdxConst(d, builder, op.getLoc()));\n+  };\n+\n+  Value base_addr = store_op.getBase();\n+  SmallVector<Value, 4> base_indices = store_op.getIndices();\n+\n+  if (must_support_unaligned_dynamic_index) {\n+    if (!can_support_unaligned_dynamic_index) {\n+      return op.emitOpError(\n+          \"Not implemented: dynamic store with unaligned indices\");\n+    }\n+    // Convert dynamic store to dynamic slice + static store. This saves us a\n+    // bunch of scalar core work.\n+    base_addr = slice_result->first;\n+    CHECK_EQ(slice_result->second.size(), base_indices.size());\n+    for (int i = 0; i < base_indices.size(); ++i) {\n+      base_indices[i] = IdxConst(slice_result->second[i], builder, op.getLoc());\n+    }\n+\n+  // TODO(jevinjiang): ideally we should update the base addr and use static\n+  // indices even for the cases that can skip alignment check. This can save\n+  // us a bunch of scalar core work.\n+  auto tile_base_idxs = ArrayRef<Value>(base_indices).take_back(tiled_dims);\n+  auto batch_base_idxs = ArrayRef<Value>(base_indices).drop_back(tiled_dims);\n+  const auto base_s =\n+      is_1d ? IdxConst(0, builder, op.getLoc()) : tile_base_idxs.front();\n+  const auto base_l = tile_base_idxs.back();\n+          indices[i] = add_idx(batch_base_idxs[i], idx[i]);\n+              add_idx(base_s, sidx * vreg_slice[0] - *sublane_offset);\n+            add_idx(base_l, lidx * vreg_slice[1] - *lane_offset);\n+  }\n",
            "whole_hunk": "@@ -2741,7 +2741,7 @@ LogicalResult vector_load_rule(RewriteContext &ctx, Operation &op,\n   const VectorLayout &layout_out = *layouts_out.front();\n   ImplicitLocOpBuilder builder(op.getLoc(), &op);\n   auto load_op = cast<vector::LoadOp>(op);\n-  const auto memref_ty = cast<MemRefType>(load_op.getBase().getType());\n+  const auto memref_ty = getMemRefType(load_op.getBase());\n   const auto vty = cast<VectorType>(load_op.getResult().getType());\n   FAILUREOR_ASSIGN_OR_RETURN(\n       VectorType target_ty,\n@@ -2772,36 +2772,80 @@ LogicalResult vector_load_rule(RewriteContext &ctx, Operation &op,\n   }\n   // TODO(apaszke): Check that loads are from vmem!\n \n-  int tiled_dims = is_1d ? 1 : 2;\n-  Value base_addr;\n-  SmallVector<int64_t> base_indices;\n-  if (auto const_indices =\n-          getIntConstsFromOperandRange(load_op.getIndices(), /*silent=*/true);\n-      succeeded(const_indices)) {\n-    base_addr = load_op.getBase();\n-    base_indices = std::move(*const_indices);\n-  } else {\n-    auto slice_result =\n-        sliceRef(builder, load_op.getBase(), load_op.getVectorType().getShape(),\n-                 load_op.getIndices(),\n-                 ArrayRef<int64_t>(memref_tiling).take_back(tiled_dims));\n-    if (failed(slice_result)) {\n-      return failure();\n+  bool can_support_unaligned_dynamic_index = false;\n+  bool must_support_unaligned_dynamic_index = false;\n+  if (load_op.getIndices().size() > 1) {\n+    auto second_minor_idx = load_op.getIndices().take_back(2)[0];\n+    if (failed(getIntConst(second_minor_idx, /*silent=*/true)) &&\n+        !isGuaranteedDivisible(second_minor_idx, memref_tiling[0])) {\n+      must_support_unaligned_dynamic_index = true;\n     }\n-    std::tie(base_addr, base_indices) = *slice_result;\n   }\n-  auto tile_base_idxs = ArrayRef<int64_t>(base_indices).take_back(tiled_dims);\n-  auto batch_base_idxs = ArrayRef<int64_t>(base_indices).drop_back(tiled_dims);\n-\n   const SmallVector<int64_t> implicit_shape =\n       layout_out.implicitShape(vty.getShape());\n   const int64_t ss = implicit_shape[implicit_shape.size() - 2];\n   int64_t sublane_stride = 1;\n+  // Handle special patterns that allow us to support more flexible loads.\n   if (layout_out.bitwidth() == 32 &&\n       layout_out.tiling() == std::array<int64_t, 2>{1, ctx.target_shape[1]} &&\n       ss == 1) {\n+    // Loading a single row on the 2nd minor dim into the (1, 128) layout. We\n+    // can use sublane striding to perform the relayout as part of the load.\n     sublane_stride = memref_tiling[0];\n+    can_support_unaligned_dynamic_index = true;\n+  } else {\n+    // Otherwise, if the memref has a short last dimension and is contiguous\n+    // all the tiled layouts become equivalent, so we can handle unaligned\n+    // dynamic indices without any special case.\n+    auto mem_layout = dyn_cast<TiledLayoutAttr>(memref_ty.getLayout());\n+    if (!mem_layout) {\n+      return op.emitOpError(\"Expected a tiled memref\");\n+    }\n+    auto tile_strides = mem_layout.getTileStrides();\n+    if (memref_ty.getShape().back() == ctx.target_shape[1] &&\n+        tile_strides.take_back(2) == ArrayRef<int64_t>{1, 1}) {\n+      can_support_unaligned_dynamic_index = true;\n+    }\n   }\n+\n+  auto add_idx = [&](const Value &v, int64_t d) -> Value {\n+    if (auto cst = getIntConst(v, /*silent=*/true); succeeded(cst)) {\n+      return IdxConst(cst.value() + d, builder, op.getLoc());\n+    }\n+    return builder.create<arith::AddIOp>(v, IdxConst(d, builder, op.getLoc()));\n+  };\n+\n+  int tiled_dims = is_1d ? 1 : 2;\n+  Value base_addr = load_op.getBase();\n+  SmallVector<Value, 4> base_indices = load_op.getIndices();\n+\n+  if (must_support_unaligned_dynamic_index) {\n+    if (!can_support_unaligned_dynamic_index) {\n+      return op.emitOpError(\n+          \"Not implemented: dynamic load with unaligned indices\");\n+    }\n+  } else {\n+    // Convert dynamic load to dynamic slice + static load. This saves us a\n+    // bunch of scalar core work.\n+    auto slice_result =\n+        sliceRef(builder, load_op.getBase(), load_op.getVectorType().getShape(),\n+                 load_op.getIndices(),\n+                 ArrayRef<int64_t>(memref_tiling).take_back(tiled_dims));\n+    if (failed(slice_result)) {\n+      return failure();\n+    }\n+    base_addr = slice_result->first;\n+    CHECK_EQ(slice_result->second.size(), base_indices.size());\n+    for (int i = 0; i < base_indices.size(); ++i) {\n+      base_indices[i] = IdxConst(slice_result->second[i], builder, op.getLoc());\n+    }\n+  }\n+\n+  // TODO(jevinjiang): ideally we should update the base addr and use static\n+  // indices even for the cases that can skip alignment check. This can save us\n+  // a bunch of scalar core work.\n+  auto tile_base_idxs = ArrayRef<Value>(base_indices).take_back(tiled_dims);\n+  auto batch_base_idxs = ArrayRef<Value>(base_indices).drop_back(tiled_dims);\n   const LayoutOffsets offsets = layout_out.offsets();\n   AffineMap load_map;\n   arith::ConstantOp padding;\n@@ -2841,21 +2885,18 @@ LogicalResult vector_load_rule(RewriteContext &ctx, Operation &op,\n         CHECK_EQ(num_dims, tile_idxs.size());\n         SmallVector<Value> idxs(tile_idxs.size());\n         for (int64_t i = 0; i < num_batch_dims; ++i) {\n-          idxs[i] = IdxConst(batch_base_idxs[i] + tile_idxs[i], builder,\n-                             load_op->getLoc());\n+          idxs[i] = add_idx(batch_base_idxs[i], tile_idxs[i]);\n         }\n-        const int64_t base_l = tile_base_idxs.back();\n+        const auto base_l = tile_base_idxs.back();\n         const int64_t lidx = tile_idxs[num_dims - 1];\n         idxs[num_dims - 1] =\n-            IdxConst(base_l + lidx * vreg_slice[1] - *offsets[1], builder,\n-                     load_op->getLoc());\n+            add_idx(base_l, lidx * vreg_slice[1] - offsets[1].value_or(0));\n         if (!is_1d) {\n           CHECK_EQ(tile_base_idxs.size(), 2);\n-          const int64_t base_s = tile_base_idxs.front();\n+          const auto base_s = tile_base_idxs.front();\n           const int64_t sidx = tile_idxs[num_dims - 2];\n           idxs[num_dims - 2] =\n-              IdxConst(base_s + sidx * vreg_slice[0] - offsets[0].value_or(0),\n-                       builder, load_op->getLoc());\n+              add_idx(base_s, sidx * vreg_slice[0] - offsets[0].value_or(0));\n         }\n         TPU_ASSERT_OP(tile_idxs[num_dims - 1] + ctx.target_shape[1] <=\n                       memref_ty.getShape()[num_dims - 1]);\n@@ -3919,6 +3960,7 @@ LogicalResult vector_store_rule(RewriteContext &ctx, Operation &op,\n   vector::StoreOp store_op = cast<vector::StoreOp>(op);\n   const VectorType ty = store_op.getValueToStore().getType();\n   const VectorLayout &to_store_layout = *layouts_in.front();\n+  const auto memref_ty = getMemRefType(store_op.getBase());\n   if (!ty.getRank()) {\n     return op.emitOpError(\"Not implemented: scalar stores to vmem\");\n   }\n@@ -3944,15 +3986,59 @@ LogicalResult vector_store_rule(RewriteContext &ctx, Operation &op,\n     }\n   }\n \n+  bool can_support_unaligned_dynamic_index = false;\n+  bool must_support_unaligned_dynamic_index = false;\n+  if (store_op.getIndices().size() > 1) {\n+    auto second_minor_idx = store_op.getIndices().take_back(2)[0];\n+    if (failed(getIntConst(second_minor_idx, /*silent=*/true)) &&\n+        !isGuaranteedDivisible(second_minor_idx, memref_tiling[0])) {\n+      must_support_unaligned_dynamic_index = true;\n+    }\n+  }\n+  int64_t sublane_stride = 1;\n+  // Handle special patterns that allow us to support more flexible loads.\n+  if (to_store_layout.bitwidth() == 32 &&\n+      to_store_layout.tiling() == Tiling{1, ctx.target_shape[1]}) {\n+    // Storing a single row on the 2nd minor dim from the (1, 128) layout. We\n+    // can use sublane striding to perform the relayout as part of the store.\n+    // The stride of store should be the number of sublanes in memref tile when\n+    // store a single sublane.\n+    sublane_stride = memref_tiling[0];\n+    can_support_unaligned_dynamic_index = true;\n+  } else {\n+    // Otherwise, if the memref has a short last dimension and is contiguous\n+    // all the tiled layouts become equivalent, so we can handle unaligned\n+    // dynamic indices without any special case.\n+    auto mem_layout = dyn_cast<TiledLayoutAttr>(memref_ty.getLayout());\n+    if (!mem_layout) {\n+      return op.emitOpError(\"Expected a tiled memref\");\n+    }\n+    auto tile_strides = mem_layout.getTileStrides();\n+    if (memref_ty.getShape().back() == ctx.target_shape[1] &&\n+        tile_strides.take_back(2) == ArrayRef<int64_t>{1, 1}) {\n+      can_support_unaligned_dynamic_index = true;\n+    }\n+  }\n+\n+  auto add_idx = [&](const Value &v, int64_t d) -> Value {\n+    if (auto cst = getIntConst(v, /*silent=*/true); succeeded(cst)) {\n+      return IdxConst(cst.value() + d, builder, op.getLoc());\n+    }\n+    return builder.create<arith::AddIOp>(v, IdxConst(d, builder, op.getLoc()));\n+  };\n+\n   int tiled_dims = is_1d ? 1 : 2;\n-  Value base_addr;\n-  SmallVector<int64_t> base_indices;\n-  if (auto const_indices =\n-          getIntConstsFromOperandRange(store_op.getIndices(), /*silent=*/true);\n-      succeeded(const_indices)) {\n-    base_addr = store_op.getBase();\n-    base_indices = std::move(*const_indices);\n+  Value base_addr = store_op.getBase();\n+  SmallVector<Value, 4> base_indices = store_op.getIndices();\n+\n+  if (must_support_unaligned_dynamic_index) {\n+    if (!can_support_unaligned_dynamic_index) {\n+      return op.emitOpError(\n+          \"Not implemented: dynamic store with unaligned indices\");\n+    }\n   } else {\n+    // Convert dynamic store to dynamic slice + static store. This saves us a\n+    // bunch of scalar core work.\n     auto slice_result =\n         sliceRef(builder, store_op.getBase(),\n                  store_op.getVectorType().getShape(), store_op.getIndices(),\n@@ -3960,18 +4046,27 @@ LogicalResult vector_store_rule(RewriteContext &ctx, Operation &op,\n     if (failed(slice_result)) {\n       return failure();\n     }\n-    std::tie(base_addr, base_indices) = *slice_result;\n+    base_addr = slice_result->first;\n+    CHECK_EQ(slice_result->second.size(), base_indices.size());\n+    for (int i = 0; i < base_indices.size(); ++i) {\n+      base_indices[i] = IdxConst(slice_result->second[i], builder, op.getLoc());\n+    }\n   }\n-  auto tile_base_idxs = ArrayRef<int64_t>(base_indices).take_back(tiled_dims);\n-  auto batch_base_idxs = ArrayRef<int64_t>(base_indices).drop_back(tiled_dims);\n+\n+  // TODO(jevinjiang): ideally we should update the base addr and use static\n+  // indices even for the cases that can skip alignment check. This can save\n+  // us a bunch of scalar core work.\n+  auto tile_base_idxs = ArrayRef<Value>(base_indices).take_back(tiled_dims);\n+  auto batch_base_idxs = ArrayRef<Value>(base_indices).drop_back(tiled_dims);\n \n   FAILUREOR_ASSIGN_OR_RETURN(\n       xla::Array<Value> tiles,\n       disassemble(builder, to_store_layout, store_op.getValueToStore(),\n                   ctx.target_shape));\n   const int64_t ndims = ty.getRank();\n-  const int64_t base_s = is_1d ? 0 : tile_base_idxs.front();\n-  const int64_t base_l = tile_base_idxs.back();\n+  const auto base_s =\n+      is_1d ? IdxConst(0, builder, op.getLoc()) : tile_base_idxs.front();\n+  const auto base_l = tile_base_idxs.back();\n   if (is_1d) {\n     tiles.Reshape(\n         to_store_layout.implicitShape(toArrayRef(tiles.dimensions())));\n@@ -3984,13 +4079,6 @@ LogicalResult vector_store_rule(RewriteContext &ctx, Operation &op,\n   }\n   const SmallVector<int64_t> stored_shape =\n       to_store_layout.implicitShape(ty.getShape());\n-  int64_t sublane_stride = 1;\n-  // The stride of store should be the number of sublanes in memref tile when\n-  // store a single sublane.\n-  if (to_store_layout.bitwidth() == 32 &&\n-      to_store_layout.tiling() == Tiling{1, ctx.target_shape[1]}) {\n-    sublane_stride = memref_tiling[0];\n-  }\n   const std::array<int64_t, 2> vreg_slice =\n       to_store_layout.vregSlice(ctx.target_shape);\n   const absl::Status status =\n@@ -4002,17 +4090,15 @@ LogicalResult vector_store_rule(RewriteContext &ctx, Operation &op,\n         const int64_t sidx = *(idx.end() - 2);\n         const int64_t lidx = *(idx.end() - 1);\n         SmallVector<Value> indices(ndims);\n-        auto boundIdxConst = std::bind(IdxConst, std::placeholders::_1, builder,\n-                                       store_op->getLoc());\n         for (int64_t i = 0; i < batch_base_idxs.size(); ++i) {\n-          indices[i] = boundIdxConst(batch_base_idxs[i] + idx[i]);\n+          indices[i] = add_idx(batch_base_idxs[i], idx[i]);\n         }\n         if (!is_1d) {\n           *(indices.end() - 2) =\n-              boundIdxConst(base_s + sidx * vreg_slice[0] - *sublane_offset);\n+              add_idx(base_s, sidx * vreg_slice[0] - *sublane_offset);\n         }\n         *(indices.end() - 1) =\n-            boundIdxConst(base_l + lidx * vreg_slice[1] - *lane_offset);\n+            add_idx(base_l, lidx * vreg_slice[1] - *lane_offset);\n         const DenseBoolArrayAttr sublane_mask =\n             bounds->getSublaneMask(store_op->getContext(), ctx.target_shape);\n         const bool masks_subelements =\n@@ -4079,7 +4165,7 @@ LogicalResult vector_store_rule(RewriteContext &ctx, Operation &op,\n   }\n   store_op->erase();\n   return success();\n-}\n+  }\n \n LogicalResult vector_transpose_rule(RewriteContext &ctx, Operation &op,\n                                     const ArrayRef<Layout> layouts_in,\n"
        },
        {
            "name": "infer_vector_layout.cc",
            "path": "jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc",
            "patches": [
                {
                    "old_start": 1109,
                    "old_length": 20,
                    "new_start": 1109,
                    "new_length": 23,
                    "hunk": "@@ -1109,20 +1109,23 @@ class VectorLayoutInferer {\n \n     SmallVector<Layout, 4> in_layout(op->getNumOperands(), kNoLayout);\n     CHECK_EQ(op->getNumOperands(), op.getIndices().size() + 1);\n-    SmallVector<int64_t, 2> tile_offsets;  // indices % tiling\n-    for (int i = 0; i < tiling.size(); ++i) {\n-      int dim = rank - tiling.size() + i;\n+    // Infer the static offset on a given tiling dimension.\n+    auto infer_offset = [&](int64_t &offset,\n+                            int64_t tiling_dim) -> LogicalResult {\n+      int dim = rank - tiling.size() + tiling_dim;\n       Value tiled_index = op.getIndices()[dim];\n       if (auto cst_op = tiled_index.getDefiningOp<arith::ConstantOp>()) {\n-        tile_offsets.push_back(cast<IntegerAttr>(cst_op.getValue()).getInt() %\n-                               tiling[i]);\n-      } else {\n-        if (failed(verifyDivisibleIndex(tiled_index, tiling[i], dim, op))) {\n-          return failure();\n-        }\n-        tile_offsets.push_back(0);\n+        offset =\n+            cast<IntegerAttr>(cst_op.getValue()).getInt() % tiling[tiling_dim];\n+        return success();\n       }\n-    }\n+      if (failed(\n+              verifyDivisibleIndex(tiled_index, tiling[tiling_dim], dim, op))) {\n+        return failure();\n+      }\n+      offset = 0;\n+      return success();\n+    };\n \n     if (rank == 0) {\n       op.emitOpError(\"rank 0 vectors unsupported\");\n"
                },
                {
                    "old_start": 1133,
                    "old_length": 15,
                    "new_start": 1136,
                    "new_length": 17,
                    "hunk": "@@ -1133,15 +1136,17 @@ class VectorLayoutInferer {\n       const int64_t lane_tiling = packing * target_shape_[1];\n       auto tile = tiling.front();\n       TPU_CHECK_OP(tile % lane_tiling == 0, \"Unsupported tiling for 1D load\");\n-      CHECK_EQ(tile_offsets.size(), 1);\n+      int64_t offset;\n+      if (failed(infer_offset(offset, 0))) {\n+        return failure();\n+      }\n       // TODO(apaszke): We could generate replicated loads for short values.\n       setLayout(op, in_layout,\n-                VectorLayout(bitwidth, {0, tile_offsets[0] % lane_tiling},\n+                VectorLayout(bitwidth, {0, offset % lane_tiling},\n                              {1, lane_tiling}, ImplicitDim::kSecondMinor));\n     } else {  // rank >= 2\n       TPU_CHECK_OP(tiling.size() == 2, \"Expected 2D tiling in 2D+ loads\");\n-      CHECK_EQ(tile_offsets.size(), 2);\n-      std::array<std::optional<int64_t>, 2> offsets;\n+      LayoutOffsets offsets = {0, 0};\n       const auto tile_src_shape = src_ty.getShape().take_back(2);\n       const auto tile_res_shape = res_ty.getShape().take_back(2);\n       const int64_t num_sublanes = tile_res_shape[0];\n"
                },
                {
                    "old_start": 1155,
                    "old_length": 10,
                    "new_start": 1160,
                    "new_length": 12,
                    "hunk": "@@ -1155,10 +1160,12 @@ class VectorLayoutInferer {\n       if (bitwidth == 32 &&\n           (tile_src_shape[1] <= target_shape_[1] || num_sublanes == 1)) {\n         offsets[0] = 0;\n-      } else {\n-        offsets[0] = tile_offsets[0];\n+      } else if (failed(infer_offset(*offsets[0], 0))) {\n+        return failure();\n+      }\n+      if (failed(infer_offset(*offsets[1], 1))) {\n+        return failure();\n       }\n-      offsets[1] = tile_offsets[1];\n       std::array<int64_t, 2> layout_tiling{tiling[0], tiling[1]};\n       if (num_sublanes == 1 && bitwidth == 32 &&\n           tiling[1] == target_shape_[1] &&\n"
                },
                {
                    "old_start": 1462,
                    "old_length": 20,
                    "new_start": 1469,
                    "new_length": 23,
                    "hunk": "@@ -1462,20 +1469,23 @@ class VectorLayoutInferer {\n     }\n     auto tiling = *maybe_tiling;\n \n-    SmallVector<int64_t, 2> tile_offsets;  // indices % tiling\n-    for (int i = 0; i < tiling.size(); ++i) {\n-      int dim = rank - tiling.size() + i;\n+    // Infer the static offset on a given tiling dimension.\n+    auto infer_offset = [&](int64_t &offset,\n+                            int64_t tiling_dim) -> LogicalResult {\n+      int dim = rank - tiling.size() + tiling_dim;\n       Value tiled_index = op.getIndices()[dim];\n       if (auto cst_op = tiled_index.getDefiningOp<arith::ConstantOp>()) {\n-        tile_offsets.push_back(cast<IntegerAttr>(cst_op.getValue()).getInt() %\n-                               tiling[i]);\n-      } else {\n-        if (failed(verifyDivisibleIndex(tiled_index, tiling[i], dim, op))) {\n-          return failure();\n-        }\n-        tile_offsets.push_back(0);\n+        offset =\n+            cast<IntegerAttr>(cst_op.getValue()).getInt() % tiling[tiling_dim];\n+        return success();\n       }\n-    }\n+      if (failed(\n+              verifyDivisibleIndex(tiled_index, tiling[tiling_dim], dim, op))) {\n+        return failure();\n+      }\n+      offset = 0;\n+      return success();\n+    };\n \n     Layout store_layout;\n     if (rank == 0) {\n"
                },
                {
                    "old_start": 1488,
                    "old_length": 14,
                    "new_start": 1498,
                    "new_length": 15,
                    "hunk": "@@ -1488,14 +1498,15 @@ class VectorLayoutInferer {\n       auto tile = tiling.front();\n       TPU_CHECK_OP(tile % lane_tiling == 0,\n                    \"Unsupported 1D tiling for 1D store\");\n-      CHECK_EQ(tile_offsets.size(), 1);\n-      store_layout =\n-          VectorLayout(bitwidth, {0, tile_offsets[0] % lane_tiling},\n-                       {1, lane_tiling}, ImplicitDim::kSecondMinor);\n+      int64_t offset;\n+      if (failed(infer_offset(offset, 0))) {\n+        return failure();\n+      }\n+      store_layout = VectorLayout(bitwidth, {0, offset % lane_tiling},\n+                                  {1, lane_tiling}, ImplicitDim::kSecondMinor);\n     } else {  // rank >= 2  // NOLINT(readability-else-after-return)\n       TPU_CHECK_OP(tiling.size() == 2, \"Expected 2D tiling in 2D+ store\");\n-      CHECK_EQ(tile_offsets.size(), 2);\n-      std::array<std::optional<int64_t>, 2> offsets;\n+      LayoutOffsets offsets = {0, 0};\n       const auto tile_ref_shape = ref_ty.getShape().take_back(2);\n       const auto tile_store_shape = store_ty.getShape().take_back(2);\n       const int64_t num_sublanes = tile_store_shape[0];\n"
                },
                {
                    "old_start": 1509,
                    "old_length": 10,
                    "new_start": 1520,
                    "new_length": 12,
                    "hunk": "@@ -1509,10 +1520,12 @@ class VectorLayoutInferer {\n       if (bitwidth == 32 &&\n           (tile_ref_shape[1] <= target_shape_[1] || num_sublanes == 1)) {\n         offsets[0] = 0;\n-      } else {\n-        offsets[0] = tile_offsets[0];\n+      } else if (failed(infer_offset(*offsets[0], 0))) {\n+        return failure();\n+      }\n+      if (failed(infer_offset(*offsets[1], 1))) {\n+        return failure();\n       }\n-      offsets[1] = tile_offsets[1];\n       if (num_sublanes == 1 && bitwidth == 32 &&\n           tiling[1] == target_shape_[1] &&\n           tile_store_shape[1] > target_shape_[1]) {"
                }
            ],
            "whole_deleted": "-    SmallVector<int64_t, 2> tile_offsets;  // indices % tiling\n-    for (int i = 0; i < tiling.size(); ++i) {\n-      int dim = rank - tiling.size() + i;\n-        tile_offsets.push_back(cast<IntegerAttr>(cst_op.getValue()).getInt() %\n-                               tiling[i]);\n-      } else {\n-        if (failed(verifyDivisibleIndex(tiled_index, tiling[i], dim, op))) {\n-          return failure();\n-        }\n-        tile_offsets.push_back(0);\n-    }\n-      CHECK_EQ(tile_offsets.size(), 1);\n-                VectorLayout(bitwidth, {0, tile_offsets[0] % lane_tiling},\n-      CHECK_EQ(tile_offsets.size(), 2);\n-      std::array<std::optional<int64_t>, 2> offsets;\n-      } else {\n-        offsets[0] = tile_offsets[0];\n-      offsets[1] = tile_offsets[1];\n-    SmallVector<int64_t, 2> tile_offsets;  // indices % tiling\n-    for (int i = 0; i < tiling.size(); ++i) {\n-      int dim = rank - tiling.size() + i;\n-        tile_offsets.push_back(cast<IntegerAttr>(cst_op.getValue()).getInt() %\n-                               tiling[i]);\n-      } else {\n-        if (failed(verifyDivisibleIndex(tiled_index, tiling[i], dim, op))) {\n-          return failure();\n-        }\n-        tile_offsets.push_back(0);\n-    }\n-      CHECK_EQ(tile_offsets.size(), 1);\n-      store_layout =\n-          VectorLayout(bitwidth, {0, tile_offsets[0] % lane_tiling},\n-                       {1, lane_tiling}, ImplicitDim::kSecondMinor);\n-      CHECK_EQ(tile_offsets.size(), 2);\n-      std::array<std::optional<int64_t>, 2> offsets;\n-      } else {\n-        offsets[0] = tile_offsets[0];\n-      offsets[1] = tile_offsets[1];\n",
            "whole_added": "+    // Infer the static offset on a given tiling dimension.\n+    auto infer_offset = [&](int64_t &offset,\n+                            int64_t tiling_dim) -> LogicalResult {\n+      int dim = rank - tiling.size() + tiling_dim;\n+        offset =\n+            cast<IntegerAttr>(cst_op.getValue()).getInt() % tiling[tiling_dim];\n+        return success();\n+      if (failed(\n+              verifyDivisibleIndex(tiled_index, tiling[tiling_dim], dim, op))) {\n+        return failure();\n+      }\n+      offset = 0;\n+      return success();\n+    };\n+      int64_t offset;\n+      if (failed(infer_offset(offset, 0))) {\n+        return failure();\n+      }\n+                VectorLayout(bitwidth, {0, offset % lane_tiling},\n+      LayoutOffsets offsets = {0, 0};\n+      } else if (failed(infer_offset(*offsets[0], 0))) {\n+        return failure();\n+      }\n+      if (failed(infer_offset(*offsets[1], 1))) {\n+        return failure();\n+    // Infer the static offset on a given tiling dimension.\n+    auto infer_offset = [&](int64_t &offset,\n+                            int64_t tiling_dim) -> LogicalResult {\n+      int dim = rank - tiling.size() + tiling_dim;\n+        offset =\n+            cast<IntegerAttr>(cst_op.getValue()).getInt() % tiling[tiling_dim];\n+        return success();\n+      if (failed(\n+              verifyDivisibleIndex(tiled_index, tiling[tiling_dim], dim, op))) {\n+        return failure();\n+      }\n+      offset = 0;\n+      return success();\n+    };\n+      int64_t offset;\n+      if (failed(infer_offset(offset, 0))) {\n+        return failure();\n+      }\n+      store_layout = VectorLayout(bitwidth, {0, offset % lane_tiling},\n+                                  {1, lane_tiling}, ImplicitDim::kSecondMinor);\n+      LayoutOffsets offsets = {0, 0};\n+      } else if (failed(infer_offset(*offsets[0], 0))) {\n+        return failure();\n+      }\n+      if (failed(infer_offset(*offsets[1], 1))) {\n+        return failure();\n",
            "whole_hunk": "@@ -1109,20 +1109,23 @@ class VectorLayoutInferer {\n \n     SmallVector<Layout, 4> in_layout(op->getNumOperands(), kNoLayout);\n     CHECK_EQ(op->getNumOperands(), op.getIndices().size() + 1);\n-    SmallVector<int64_t, 2> tile_offsets;  // indices % tiling\n-    for (int i = 0; i < tiling.size(); ++i) {\n-      int dim = rank - tiling.size() + i;\n+    // Infer the static offset on a given tiling dimension.\n+    auto infer_offset = [&](int64_t &offset,\n+                            int64_t tiling_dim) -> LogicalResult {\n+      int dim = rank - tiling.size() + tiling_dim;\n       Value tiled_index = op.getIndices()[dim];\n       if (auto cst_op = tiled_index.getDefiningOp<arith::ConstantOp>()) {\n-        tile_offsets.push_back(cast<IntegerAttr>(cst_op.getValue()).getInt() %\n-                               tiling[i]);\n-      } else {\n-        if (failed(verifyDivisibleIndex(tiled_index, tiling[i], dim, op))) {\n-          return failure();\n-        }\n-        tile_offsets.push_back(0);\n+        offset =\n+            cast<IntegerAttr>(cst_op.getValue()).getInt() % tiling[tiling_dim];\n+        return success();\n       }\n-    }\n+      if (failed(\n+              verifyDivisibleIndex(tiled_index, tiling[tiling_dim], dim, op))) {\n+        return failure();\n+      }\n+      offset = 0;\n+      return success();\n+    };\n \n     if (rank == 0) {\n       op.emitOpError(\"rank 0 vectors unsupported\");\n@@ -1133,15 +1136,17 @@ class VectorLayoutInferer {\n       const int64_t lane_tiling = packing * target_shape_[1];\n       auto tile = tiling.front();\n       TPU_CHECK_OP(tile % lane_tiling == 0, \"Unsupported tiling for 1D load\");\n-      CHECK_EQ(tile_offsets.size(), 1);\n+      int64_t offset;\n+      if (failed(infer_offset(offset, 0))) {\n+        return failure();\n+      }\n       // TODO(apaszke): We could generate replicated loads for short values.\n       setLayout(op, in_layout,\n-                VectorLayout(bitwidth, {0, tile_offsets[0] % lane_tiling},\n+                VectorLayout(bitwidth, {0, offset % lane_tiling},\n                              {1, lane_tiling}, ImplicitDim::kSecondMinor));\n     } else {  // rank >= 2\n       TPU_CHECK_OP(tiling.size() == 2, \"Expected 2D tiling in 2D+ loads\");\n-      CHECK_EQ(tile_offsets.size(), 2);\n-      std::array<std::optional<int64_t>, 2> offsets;\n+      LayoutOffsets offsets = {0, 0};\n       const auto tile_src_shape = src_ty.getShape().take_back(2);\n       const auto tile_res_shape = res_ty.getShape().take_back(2);\n       const int64_t num_sublanes = tile_res_shape[0];\n@@ -1155,10 +1160,12 @@ class VectorLayoutInferer {\n       if (bitwidth == 32 &&\n           (tile_src_shape[1] <= target_shape_[1] || num_sublanes == 1)) {\n         offsets[0] = 0;\n-      } else {\n-        offsets[0] = tile_offsets[0];\n+      } else if (failed(infer_offset(*offsets[0], 0))) {\n+        return failure();\n+      }\n+      if (failed(infer_offset(*offsets[1], 1))) {\n+        return failure();\n       }\n-      offsets[1] = tile_offsets[1];\n       std::array<int64_t, 2> layout_tiling{tiling[0], tiling[1]};\n       if (num_sublanes == 1 && bitwidth == 32 &&\n           tiling[1] == target_shape_[1] &&\n@@ -1462,20 +1469,23 @@ class VectorLayoutInferer {\n     }\n     auto tiling = *maybe_tiling;\n \n-    SmallVector<int64_t, 2> tile_offsets;  // indices % tiling\n-    for (int i = 0; i < tiling.size(); ++i) {\n-      int dim = rank - tiling.size() + i;\n+    // Infer the static offset on a given tiling dimension.\n+    auto infer_offset = [&](int64_t &offset,\n+                            int64_t tiling_dim) -> LogicalResult {\n+      int dim = rank - tiling.size() + tiling_dim;\n       Value tiled_index = op.getIndices()[dim];\n       if (auto cst_op = tiled_index.getDefiningOp<arith::ConstantOp>()) {\n-        tile_offsets.push_back(cast<IntegerAttr>(cst_op.getValue()).getInt() %\n-                               tiling[i]);\n-      } else {\n-        if (failed(verifyDivisibleIndex(tiled_index, tiling[i], dim, op))) {\n-          return failure();\n-        }\n-        tile_offsets.push_back(0);\n+        offset =\n+            cast<IntegerAttr>(cst_op.getValue()).getInt() % tiling[tiling_dim];\n+        return success();\n       }\n-    }\n+      if (failed(\n+              verifyDivisibleIndex(tiled_index, tiling[tiling_dim], dim, op))) {\n+        return failure();\n+      }\n+      offset = 0;\n+      return success();\n+    };\n \n     Layout store_layout;\n     if (rank == 0) {\n@@ -1488,14 +1498,15 @@ class VectorLayoutInferer {\n       auto tile = tiling.front();\n       TPU_CHECK_OP(tile % lane_tiling == 0,\n                    \"Unsupported 1D tiling for 1D store\");\n-      CHECK_EQ(tile_offsets.size(), 1);\n-      store_layout =\n-          VectorLayout(bitwidth, {0, tile_offsets[0] % lane_tiling},\n-                       {1, lane_tiling}, ImplicitDim::kSecondMinor);\n+      int64_t offset;\n+      if (failed(infer_offset(offset, 0))) {\n+        return failure();\n+      }\n+      store_layout = VectorLayout(bitwidth, {0, offset % lane_tiling},\n+                                  {1, lane_tiling}, ImplicitDim::kSecondMinor);\n     } else {  // rank >= 2  // NOLINT(readability-else-after-return)\n       TPU_CHECK_OP(tiling.size() == 2, \"Expected 2D tiling in 2D+ store\");\n-      CHECK_EQ(tile_offsets.size(), 2);\n-      std::array<std::optional<int64_t>, 2> offsets;\n+      LayoutOffsets offsets = {0, 0};\n       const auto tile_ref_shape = ref_ty.getShape().take_back(2);\n       const auto tile_store_shape = store_ty.getShape().take_back(2);\n       const int64_t num_sublanes = tile_store_shape[0];\n@@ -1509,10 +1520,12 @@ class VectorLayoutInferer {\n       if (bitwidth == 32 &&\n           (tile_ref_shape[1] <= target_shape_[1] || num_sublanes == 1)) {\n         offsets[0] = 0;\n-      } else {\n-        offsets[0] = tile_offsets[0];\n+      } else if (failed(infer_offset(*offsets[0], 0))) {\n+        return failure();\n+      }\n+      if (failed(infer_offset(*offsets[1], 1))) {\n+        return failure();\n       }\n-      offsets[1] = tile_offsets[1];\n       if (num_sublanes == 1 && bitwidth == 32 &&\n           tiling[1] == target_shape_[1] &&\n           tile_store_shape[1] > target_shape_[1]) {"
        }
    ]
},
{
    "Id": 16,
    "commit_link": "https://github.com/google/jax/commit/1a91fe76779ad3c97abb4e35a9a6070c026c2513",
    "date": "2024-06-28T12:56:43-07:00",
    "message": "Explicitly disallow duplicated devices during array construction\n\n`jax.make_array_from_single_device_arrays` should not allow passing more than one array on the same device as that would lead to an invalid array. While some of this case is already detected by later checks (e.g., `ArrayImpl._check_and_rearrange`), this CL explicitly checks the device list before calling IFRT so that we don't create an invalid IFRT array to begin with.\n\nPiperOrigin-RevId: 647772472",
    "changes": [
        {
            "name": "array_test.py",
            "path": "tests/array_test.py",
            "patches": [
                {
                    "old_start": 314,
                    "old_length": 24,
                    "new_start": 314,
                    "new_length": 28,
                    "hunk": "@@ -314,24 +314,28 @@ class JaxArrayTest(jtu.JaxTestCase):\n     self.assertTrue(dispatch.is_single_device_sharding(out.sharding))\n \n   def test_wrong_num_arrays(self):\n+    if jax.device_count() < 4:\n+      self.skipTest('Requires more than 4 devices')\n     shape = (8, 2)\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n-    devices = jax.local_devices()[:8] # Taking up to 8 devices\n+    mesh = jtu.create_global_mesh((1, 2), ('x', 'y'))\n+    devices = jax.local_devices()[:2]  # Taking up to 2 devices\n     s = jax.sharding.NamedSharding(mesh, P('x', 'y'))\n     inp_data = np.arange(math.prod(shape), dtype=np.float32).reshape(shape)\n     di_map = s.devices_indices_map(shape)\n     bufs = [jax.device_put(inp_data[di_map[d]], d) for d in devices]\n     with self.assertRaisesRegex(\n         ValueError,\n-        r'Expected 8 per-device arrays \\(this is how many devices are addressable '\n-        r'by the sharding\\), but got 4'):\n-      array.ArrayImpl(core.ShapedArray(shape, np.float32), s, bufs[:4], committed=True)\n+        r'Expected 2 per-device arrays \\(this is how many devices are addressable '\n+        r'by the sharding\\), but got 1'):\n+      array.ArrayImpl(core.ShapedArray(shape, np.float32), s, bufs[:1], committed=True)\n \n+    for buf, d in zip(list(bufs), jax.local_devices()[2:4]):\n+      bufs.append(jax.device_put(buf, d))\n     with self.assertRaisesRegex(\n         ValueError,\n-        r'Expected 8 per-device arrays \\(this is how many devices are addressable '\n-        r'by the sharding\\), but got 16'):\n-      array.ArrayImpl(core.ShapedArray(shape, np.float32), s, bufs + bufs, committed=True)\n+        r'Expected 2 per-device arrays \\(this is how many devices are addressable '\n+        r'by the sharding\\), but got 4'):\n+      array.ArrayImpl(core.ShapedArray(shape, np.float32), s, bufs, committed=True)\n \n   def test_arrays_not_in_device_assignment(self):\n     if jax.device_count() < 4:\n"
                },
                {
                    "old_start": 351,
                    "old_length": 21,
                    "new_start": 355,
                    "new_length": 6,
                    "hunk": "@@ -351,21 +355,6 @@ class JaxArrayTest(jtu.JaxTestCase):\n         \"in the sharding.\"):\n       array.ArrayImpl(core.ShapedArray(shape, np.float32), s, bufs, committed=True)\n \n-  def test_more_devices_in_sharding_than_arrays(self):\n-    shape = (8, 2)\n-    mesh = jtu.create_global_mesh((1, 2), ('x', 'y'))\n-    # Sharding device ids = {0, 1}\n-    s = jax.sharding.NamedSharding(mesh, P('x'))\n-    inp_data = np.arange(math.prod(shape), dtype=np.float32).reshape(shape)\n-    # _arrays device ids = {0, 0}\n-    bufs = [jax.device_put(inp_data, jax.devices()[0]) for _ in range(2)]\n-    with self.assertRaisesRegex(\n-        ValueError,\n-        \"Addressable devices and per-device arrays devices do not match. \"\n-        r\"Sharding contains devices \\{1\\} that are not present in per-device \"\n-        \"arrays.\"):\n-      array.ArrayImpl(core.ShapedArray(shape, np.float32), s, bufs, committed=True)\n-\n   def test_different_devices_in_arrays_than_sharding(self):\n     if jax.device_count() < 3:\n       self.skipTest('Requires more than 3 devices')\n"
                },
                {
                    "old_start": 384,
                    "old_length": 6,
                    "new_start": 373,
                    "new_length": 22,
                    "hunk": "@@ -384,6 +373,22 @@ class JaxArrayTest(jtu.JaxTestCase):\n         \"in the sharding.\"):\n       array.ArrayImpl(core.ShapedArray(shape, np.float32), s, bufs, committed=True)\n \n+  def test_duplicated_devices_in_arrays(self):\n+    if xc._version <= 274:\n+      self.skipTest('Test requires jaxlib version 275')\n+    shape = (8, 2)\n+    mesh = jtu.create_global_mesh((1, 2), ('x', 'y'))\n+    # Sharding device ids = {0, 1}\n+    s = jax.sharding.NamedSharding(mesh, P('x'))\n+    inp_data = np.arange(math.prod(shape), dtype=np.float32).reshape(shape)\n+    # _arrays device ids = {0, 2}\n+    bufs = [jax.device_put(inp_data, jax.devices()[0]) for _ in range(2)]\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        'When making an array from single-device arrays, the input arrays must'\n+        ' be from distinct devices'):\n+      array.ArrayImpl(core.ShapedArray(shape, np.float32), s, bufs, committed=True)\n+\n   @parameterized.named_parameters(\n       (\"mesh_x_y\", P(\"x\", \"y\"), (2, 2)),\n       (\"mesh_x\", P(\"x\"), (2, 4)),\n"
                },
                {
                    "old_start": 1324,
                    "old_length": 7,
                    "new_start": 1329,
                    "new_length": 7,
                    "hunk": "@@ -1324,7 +1329,7 @@ class RngShardingTest(jtu.JaxTestCase):\n     s = jax.sharding.NamedSharding(mesh, pspec)\n \n     n = math.prod(global_shape)\n-    global_x = jnp.arange(n).astype('uint32').reshape(global_shape)\n+    global_x = np.arange(n).astype('uint32').reshape(global_shape)\n     x = array.make_array_from_callback(global_x.shape, s, lambda i: global_x[i])\n \n     # check computation is fully partitioned and without any communication"
                }
            ],
            "whole_deleted": "-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n-    devices = jax.local_devices()[:8] # Taking up to 8 devices\n-        r'Expected 8 per-device arrays \\(this is how many devices are addressable '\n-        r'by the sharding\\), but got 4'):\n-      array.ArrayImpl(core.ShapedArray(shape, np.float32), s, bufs[:4], committed=True)\n-        r'Expected 8 per-device arrays \\(this is how many devices are addressable '\n-        r'by the sharding\\), but got 16'):\n-      array.ArrayImpl(core.ShapedArray(shape, np.float32), s, bufs + bufs, committed=True)\n-  def test_more_devices_in_sharding_than_arrays(self):\n-    shape = (8, 2)\n-    mesh = jtu.create_global_mesh((1, 2), ('x', 'y'))\n-    # Sharding device ids = {0, 1}\n-    s = jax.sharding.NamedSharding(mesh, P('x'))\n-    inp_data = np.arange(math.prod(shape), dtype=np.float32).reshape(shape)\n-    # _arrays device ids = {0, 0}\n-    bufs = [jax.device_put(inp_data, jax.devices()[0]) for _ in range(2)]\n-    with self.assertRaisesRegex(\n-        ValueError,\n-        \"Addressable devices and per-device arrays devices do not match. \"\n-        r\"Sharding contains devices \\{1\\} that are not present in per-device \"\n-        \"arrays.\"):\n-      array.ArrayImpl(core.ShapedArray(shape, np.float32), s, bufs, committed=True)\n-\n-    global_x = jnp.arange(n).astype('uint32').reshape(global_shape)\n",
            "whole_added": "+    if jax.device_count() < 4:\n+      self.skipTest('Requires more than 4 devices')\n+    mesh = jtu.create_global_mesh((1, 2), ('x', 'y'))\n+    devices = jax.local_devices()[:2]  # Taking up to 2 devices\n+        r'Expected 2 per-device arrays \\(this is how many devices are addressable '\n+        r'by the sharding\\), but got 1'):\n+      array.ArrayImpl(core.ShapedArray(shape, np.float32), s, bufs[:1], committed=True)\n+    for buf, d in zip(list(bufs), jax.local_devices()[2:4]):\n+      bufs.append(jax.device_put(buf, d))\n+        r'Expected 2 per-device arrays \\(this is how many devices are addressable '\n+        r'by the sharding\\), but got 4'):\n+      array.ArrayImpl(core.ShapedArray(shape, np.float32), s, bufs, committed=True)\n+  def test_duplicated_devices_in_arrays(self):\n+    if xc._version <= 274:\n+      self.skipTest('Test requires jaxlib version 275')\n+    shape = (8, 2)\n+    mesh = jtu.create_global_mesh((1, 2), ('x', 'y'))\n+    # Sharding device ids = {0, 1}\n+    s = jax.sharding.NamedSharding(mesh, P('x'))\n+    inp_data = np.arange(math.prod(shape), dtype=np.float32).reshape(shape)\n+    # _arrays device ids = {0, 2}\n+    bufs = [jax.device_put(inp_data, jax.devices()[0]) for _ in range(2)]\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        'When making an array from single-device arrays, the input arrays must'\n+        ' be from distinct devices'):\n+      array.ArrayImpl(core.ShapedArray(shape, np.float32), s, bufs, committed=True)\n+\n+    global_x = np.arange(n).astype('uint32').reshape(global_shape)\n",
            "whole_hunk": "@@ -314,24 +314,28 @@ class JaxArrayTest(jtu.JaxTestCase):\n     self.assertTrue(dispatch.is_single_device_sharding(out.sharding))\n \n   def test_wrong_num_arrays(self):\n+    if jax.device_count() < 4:\n+      self.skipTest('Requires more than 4 devices')\n     shape = (8, 2)\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n-    devices = jax.local_devices()[:8] # Taking up to 8 devices\n+    mesh = jtu.create_global_mesh((1, 2), ('x', 'y'))\n+    devices = jax.local_devices()[:2]  # Taking up to 2 devices\n     s = jax.sharding.NamedSharding(mesh, P('x', 'y'))\n     inp_data = np.arange(math.prod(shape), dtype=np.float32).reshape(shape)\n     di_map = s.devices_indices_map(shape)\n     bufs = [jax.device_put(inp_data[di_map[d]], d) for d in devices]\n     with self.assertRaisesRegex(\n         ValueError,\n-        r'Expected 8 per-device arrays \\(this is how many devices are addressable '\n-        r'by the sharding\\), but got 4'):\n-      array.ArrayImpl(core.ShapedArray(shape, np.float32), s, bufs[:4], committed=True)\n+        r'Expected 2 per-device arrays \\(this is how many devices are addressable '\n+        r'by the sharding\\), but got 1'):\n+      array.ArrayImpl(core.ShapedArray(shape, np.float32), s, bufs[:1], committed=True)\n \n+    for buf, d in zip(list(bufs), jax.local_devices()[2:4]):\n+      bufs.append(jax.device_put(buf, d))\n     with self.assertRaisesRegex(\n         ValueError,\n-        r'Expected 8 per-device arrays \\(this is how many devices are addressable '\n-        r'by the sharding\\), but got 16'):\n-      array.ArrayImpl(core.ShapedArray(shape, np.float32), s, bufs + bufs, committed=True)\n+        r'Expected 2 per-device arrays \\(this is how many devices are addressable '\n+        r'by the sharding\\), but got 4'):\n+      array.ArrayImpl(core.ShapedArray(shape, np.float32), s, bufs, committed=True)\n \n   def test_arrays_not_in_device_assignment(self):\n     if jax.device_count() < 4:\n@@ -351,21 +355,6 @@ class JaxArrayTest(jtu.JaxTestCase):\n         \"in the sharding.\"):\n       array.ArrayImpl(core.ShapedArray(shape, np.float32), s, bufs, committed=True)\n \n-  def test_more_devices_in_sharding_than_arrays(self):\n-    shape = (8, 2)\n-    mesh = jtu.create_global_mesh((1, 2), ('x', 'y'))\n-    # Sharding device ids = {0, 1}\n-    s = jax.sharding.NamedSharding(mesh, P('x'))\n-    inp_data = np.arange(math.prod(shape), dtype=np.float32).reshape(shape)\n-    # _arrays device ids = {0, 0}\n-    bufs = [jax.device_put(inp_data, jax.devices()[0]) for _ in range(2)]\n-    with self.assertRaisesRegex(\n-        ValueError,\n-        \"Addressable devices and per-device arrays devices do not match. \"\n-        r\"Sharding contains devices \\{1\\} that are not present in per-device \"\n-        \"arrays.\"):\n-      array.ArrayImpl(core.ShapedArray(shape, np.float32), s, bufs, committed=True)\n-\n   def test_different_devices_in_arrays_than_sharding(self):\n     if jax.device_count() < 3:\n       self.skipTest('Requires more than 3 devices')\n@@ -384,6 +373,22 @@ class JaxArrayTest(jtu.JaxTestCase):\n         \"in the sharding.\"):\n       array.ArrayImpl(core.ShapedArray(shape, np.float32), s, bufs, committed=True)\n \n+  def test_duplicated_devices_in_arrays(self):\n+    if xc._version <= 274:\n+      self.skipTest('Test requires jaxlib version 275')\n+    shape = (8, 2)\n+    mesh = jtu.create_global_mesh((1, 2), ('x', 'y'))\n+    # Sharding device ids = {0, 1}\n+    s = jax.sharding.NamedSharding(mesh, P('x'))\n+    inp_data = np.arange(math.prod(shape), dtype=np.float32).reshape(shape)\n+    # _arrays device ids = {0, 2}\n+    bufs = [jax.device_put(inp_data, jax.devices()[0]) for _ in range(2)]\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        'When making an array from single-device arrays, the input arrays must'\n+        ' be from distinct devices'):\n+      array.ArrayImpl(core.ShapedArray(shape, np.float32), s, bufs, committed=True)\n+\n   @parameterized.named_parameters(\n       (\"mesh_x_y\", P(\"x\", \"y\"), (2, 2)),\n       (\"mesh_x\", P(\"x\"), (2, 4)),\n@@ -1324,7 +1329,7 @@ class RngShardingTest(jtu.JaxTestCase):\n     s = jax.sharding.NamedSharding(mesh, pspec)\n \n     n = math.prod(global_shape)\n-    global_x = jnp.arange(n).astype('uint32').reshape(global_shape)\n+    global_x = np.arange(n).astype('uint32').reshape(global_shape)\n     x = array.make_array_from_callback(global_x.shape, s, lambda i: global_x[i])\n \n     # check computation is fully partitioned and without any communication"
        }
    ]
},
{
    "Id": 17,
    "commit_link": "https://github.com/google/jax/commit/061f4df82a07cb0a347be14d9f2960b1ac2c224f",
    "date": "2024-06-28T09:46:49-07:00",
    "message": "Make `device_put` work with inputs which are host local and the sharding is global sharding i.e. sharding spanning across multiple hosts.\n\nUse `multihost_utils.assert_equal` to check if the input is the same across all hosts.\n\nDo some formatting fixes too ;)\n\nPiperOrigin-RevId: 647711853",
    "changes": [
        {
            "name": "dispatch.py",
            "path": "jax/_src/dispatch.py",
            "patches": [
                {
                    "old_start": 40,
                    "old_length": 6,
                    "new_start": 40,
                    "new_length": 7,
                    "hunk": "@@ -40,6 +40,7 @@ from jax._src import util\n from jax._src import xla_bridge as xb\n from jax._src.interpreters import ad\n from jax._src.interpreters import batching\n+from jax._src.abstract_arrays import array_types\n from jax._src.interpreters import mlir\n from jax._src.interpreters import xla\n from jax._src.interpreters import pxla\n"
                },
                {
                    "old_start": 364,
                    "old_length": 7,
                    "new_start": 365,
                    "new_length": 8,
                    "hunk": "@@ -364,7 +365,8 @@ def _mcjax_reshard(x, target_sharding):\n \n   new_x = array.make_array_from_single_device_arrays(\n       x.shape,\n-      GSPMDSharding(target_sharding._device_assignment, new_hlo_sharding),\n+      GSPMDSharding(target_sharding._device_assignment, new_hlo_sharding,\n+                    memory_kind=target_sharding.memory_kind),\n       x._arrays,\n   )\n \n"
                },
                {
                    "old_start": 398,
                    "old_length": 25,
                    "new_start": 400,
                    "new_length": 33,
                    "hunk": "@@ -398,25 +400,33 @@ class _DeferredShardArg:\n \n def _device_put_sharding_impl(x, aval, device):\n   from jax._src import array\n+  from jax.experimental import multihost_utils\n \n   if isinstance(device, Sharding):\n     s = device\n     if getattr(x, 'sharding', None) == s and getattr(x, '_committed', False):\n       return x\n+\n     if (not s.is_fully_addressable and\n         isinstance(x, array.ArrayImpl) and not x.is_fully_addressable):\n-      # This has to be XLACompatible because _mcjax_reshard will run a\n-      # XLA computation.\n       assert isinstance(s, Sharding)\n       return _mcjax_reshard(x, s)\n+\n     if not s.is_fully_addressable:\n+      if ((isinstance(x, array.ArrayImpl) and not x._committed) or\n+          type(x) in array_types):\n+        # TODO(yashkatariya): Move this check to `jit`.\n+        multihost_utils.assert_equal(\n+            x, fail_message=(\n+                f\"{type(x)} passed to device_put is not the same on each\"\n+                \" process. Make sure you are passing the same value of\"\n+                f\" {type(x)} on each process.\"))\n+        return api.jit(_identity_fn, out_shardings=s)(x)\n       # TODO(yashkatariya,mattjj): Link to a doc about McJAX and jax.Array.\n       raise ValueError(\n           \"device_put's second argument must be a Device or a Sharding which\"\n-          f\" represents addressable devices, but got {s}. You are probably\"\n-          \" trying to use device_put in multi-controller JAX which is not\"\n-          \" supported. Please use jax.make_array_from_single_device_arrays API\"\n-          \" or pass device or Sharding which represents addressable devices.\")\n+          f\" represents addressable devices, but got {s}. Please pass device or\"\n+          \" Sharding which represents addressable devices.\")\n     return _DeferredShardArg(x, s, aval, True)\n \n   # Only `Device` exists below. `Sharding` instance is handled above.\n"
                }
            ],
            "whole_deleted": "-      GSPMDSharding(target_sharding._device_assignment, new_hlo_sharding),\n-      # This has to be XLACompatible because _mcjax_reshard will run a\n-      # XLA computation.\n-          f\" represents addressable devices, but got {s}. You are probably\"\n-          \" trying to use device_put in multi-controller JAX which is not\"\n-          \" supported. Please use jax.make_array_from_single_device_arrays API\"\n-          \" or pass device or Sharding which represents addressable devices.\")\n",
            "whole_added": "+from jax._src.abstract_arrays import array_types\n+      GSPMDSharding(target_sharding._device_assignment, new_hlo_sharding,\n+                    memory_kind=target_sharding.memory_kind),\n+  from jax.experimental import multihost_utils\n+\n+\n+      if ((isinstance(x, array.ArrayImpl) and not x._committed) or\n+          type(x) in array_types):\n+        # TODO(yashkatariya): Move this check to `jit`.\n+        multihost_utils.assert_equal(\n+            x, fail_message=(\n+                f\"{type(x)} passed to device_put is not the same on each\"\n+                \" process. Make sure you are passing the same value of\"\n+                f\" {type(x)} on each process.\"))\n+        return api.jit(_identity_fn, out_shardings=s)(x)\n+          f\" represents addressable devices, but got {s}. Please pass device or\"\n+          \" Sharding which represents addressable devices.\")\n",
            "whole_hunk": "@@ -40,6 +40,7 @@ from jax._src import util\n from jax._src import xla_bridge as xb\n from jax._src.interpreters import ad\n from jax._src.interpreters import batching\n+from jax._src.abstract_arrays import array_types\n from jax._src.interpreters import mlir\n from jax._src.interpreters import xla\n from jax._src.interpreters import pxla\n@@ -364,7 +365,8 @@ def _mcjax_reshard(x, target_sharding):\n \n   new_x = array.make_array_from_single_device_arrays(\n       x.shape,\n-      GSPMDSharding(target_sharding._device_assignment, new_hlo_sharding),\n+      GSPMDSharding(target_sharding._device_assignment, new_hlo_sharding,\n+                    memory_kind=target_sharding.memory_kind),\n       x._arrays,\n   )\n \n@@ -398,25 +400,33 @@ class _DeferredShardArg:\n \n def _device_put_sharding_impl(x, aval, device):\n   from jax._src import array\n+  from jax.experimental import multihost_utils\n \n   if isinstance(device, Sharding):\n     s = device\n     if getattr(x, 'sharding', None) == s and getattr(x, '_committed', False):\n       return x\n+\n     if (not s.is_fully_addressable and\n         isinstance(x, array.ArrayImpl) and not x.is_fully_addressable):\n-      # This has to be XLACompatible because _mcjax_reshard will run a\n-      # XLA computation.\n       assert isinstance(s, Sharding)\n       return _mcjax_reshard(x, s)\n+\n     if not s.is_fully_addressable:\n+      if ((isinstance(x, array.ArrayImpl) and not x._committed) or\n+          type(x) in array_types):\n+        # TODO(yashkatariya): Move this check to `jit`.\n+        multihost_utils.assert_equal(\n+            x, fail_message=(\n+                f\"{type(x)} passed to device_put is not the same on each\"\n+                \" process. Make sure you are passing the same value of\"\n+                f\" {type(x)} on each process.\"))\n+        return api.jit(_identity_fn, out_shardings=s)(x)\n       # TODO(yashkatariya,mattjj): Link to a doc about McJAX and jax.Array.\n       raise ValueError(\n           \"device_put's second argument must be a Device or a Sharding which\"\n-          f\" represents addressable devices, but got {s}. You are probably\"\n-          \" trying to use device_put in multi-controller JAX which is not\"\n-          \" supported. Please use jax.make_array_from_single_device_arrays API\"\n-          \" or pass device or Sharding which represents addressable devices.\")\n+          f\" represents addressable devices, but got {s}. Please pass device or\"\n+          \" Sharding which represents addressable devices.\")\n     return _DeferredShardArg(x, s, aval, True)\n \n   # Only `Device` exists below. `Sharding` instance is handled above.\n"
        },
        {
            "name": "sharding_impls.py",
            "path": "jax/_src/sharding_impls.py",
            "patches": [
                {
                    "old_start": 1296,
                    "old_length": 10,
                    "new_start": 1296,
                    "new_length": 7,
                    "hunk": "@@ -1296,10 +1296,7 @@ class NonUniformShardingError(ValueError):\n \n \n def get_process_index_and_count(\n-    tensor_sharding: sharding.Sharding,\n-    dim: int,\n-    ndims: int,\n-) -> tuple[int, int]:\n+    tensor_sharding: sharding.Sharding, dim: int, ndims: int) -> tuple[int, int]:\n   \"\"\"Get current process index and number of unique processes for given dimension.\n \n   This function facilitates mapping of process-level data to individual\n"
                },
                {
                    "old_start": 1365,
                    "old_length": 10,
                    "new_start": 1362,
                    "new_length": 8,
                    "hunk": "@@ -1365,10 +1362,8 @@ def get_process_index_and_count(\n   \"\"\"\n   # TODO(sandler, yashkatariya): Consider making this function public.\n \n-  if (\n-      tensor_sharding.is_fully_addressable\n-      or tensor_sharding.is_fully_replicated\n-  ):\n+  if (tensor_sharding.is_fully_addressable or\n+      tensor_sharding.is_fully_replicated):\n     return (0, 1)\n   num_devices = len(tensor_sharding.device_set)\n   # Get device to indices map, we don't care about the concrete\n"
                },
                {
                    "old_start": 1416,
                    "old_length": 9,
                    "new_start": 1411,
                    "new_length": 7,
                    "hunk": "@@ -1416,9 +1411,7 @@ def get_process_index_and_count(\n \n \n def local_to_global_shape(\n-    sharding: sharding.Sharding,\n-    local_shape: Shape,\n-) -> tuple[int | None, ...]:\n+    sharding: sharding.Sharding, local_shape: Shape) -> tuple[int | None, ...]:\n   \"\"\"Computes the global shape given the per process if possible.\n \n   The returned shape will have the size of the global tensor in that dimension\n"
                },
                {
                    "old_start": 1467,
                    "old_length": 8,
                    "new_start": 1460,
                    "new_length": 7,
                    "hunk": "@@ -1467,8 +1460,7 @@ def local_to_global_shape(\n   for i, local_dim in enumerate(local_shape):\n     try:\n       _, shard_count = get_process_index_and_count(\n-          sharding, i, ndims=len(local_shape)\n-      )\n+          sharding, i, ndims=len(local_shape))\n       global_shape[i] = local_dim * shard_count\n     except NonUniformShardingError:\n       global_shape[i] = None\n"
                },
                {
                    "old_start": 1478,
                    "old_length": 10,
                    "new_start": 1470,
                    "new_length": 7,
                    "hunk": "@@ -1478,10 +1470,7 @@ def local_to_global_shape(\n \n \n def num_addressable_indices(\n-    tensor_sharding: sharding.Sharding,\n-    dim: int,\n-    global_shape: Shape,\n-) -> int:\n+    tensor_sharding: sharding.Sharding, dim: int, global_shape: Shape) -> int:\n   \"\"\"Returns the number of indices for given dimension this host has access to.\n \n   Each host can have multiple number of devices that are spanning"
                }
            ],
            "whole_deleted": "-    tensor_sharding: sharding.Sharding,\n-    dim: int,\n-    ndims: int,\n-) -> tuple[int, int]:\n-  if (\n-      tensor_sharding.is_fully_addressable\n-      or tensor_sharding.is_fully_replicated\n-  ):\n-    sharding: sharding.Sharding,\n-    local_shape: Shape,\n-) -> tuple[int | None, ...]:\n-          sharding, i, ndims=len(local_shape)\n-      )\n-    tensor_sharding: sharding.Sharding,\n-    dim: int,\n-    global_shape: Shape,\n-) -> int:\n",
            "whole_added": "+    tensor_sharding: sharding.Sharding, dim: int, ndims: int) -> tuple[int, int]:\n+  if (tensor_sharding.is_fully_addressable or\n+      tensor_sharding.is_fully_replicated):\n+    sharding: sharding.Sharding, local_shape: Shape) -> tuple[int | None, ...]:\n+          sharding, i, ndims=len(local_shape))\n+    tensor_sharding: sharding.Sharding, dim: int, global_shape: Shape) -> int:\n",
            "whole_hunk": "@@ -1296,10 +1296,7 @@ class NonUniformShardingError(ValueError):\n \n \n def get_process_index_and_count(\n-    tensor_sharding: sharding.Sharding,\n-    dim: int,\n-    ndims: int,\n-) -> tuple[int, int]:\n+    tensor_sharding: sharding.Sharding, dim: int, ndims: int) -> tuple[int, int]:\n   \"\"\"Get current process index and number of unique processes for given dimension.\n \n   This function facilitates mapping of process-level data to individual\n@@ -1365,10 +1362,8 @@ def get_process_index_and_count(\n   \"\"\"\n   # TODO(sandler, yashkatariya): Consider making this function public.\n \n-  if (\n-      tensor_sharding.is_fully_addressable\n-      or tensor_sharding.is_fully_replicated\n-  ):\n+  if (tensor_sharding.is_fully_addressable or\n+      tensor_sharding.is_fully_replicated):\n     return (0, 1)\n   num_devices = len(tensor_sharding.device_set)\n   # Get device to indices map, we don't care about the concrete\n@@ -1416,9 +1411,7 @@ def get_process_index_and_count(\n \n \n def local_to_global_shape(\n-    sharding: sharding.Sharding,\n-    local_shape: Shape,\n-) -> tuple[int | None, ...]:\n+    sharding: sharding.Sharding, local_shape: Shape) -> tuple[int | None, ...]:\n   \"\"\"Computes the global shape given the per process if possible.\n \n   The returned shape will have the size of the global tensor in that dimension\n@@ -1467,8 +1460,7 @@ def local_to_global_shape(\n   for i, local_dim in enumerate(local_shape):\n     try:\n       _, shard_count = get_process_index_and_count(\n-          sharding, i, ndims=len(local_shape)\n-      )\n+          sharding, i, ndims=len(local_shape))\n       global_shape[i] = local_dim * shard_count\n     except NonUniformShardingError:\n       global_shape[i] = None\n@@ -1478,10 +1470,7 @@ def local_to_global_shape(\n \n \n def num_addressable_indices(\n-    tensor_sharding: sharding.Sharding,\n-    dim: int,\n-    global_shape: Shape,\n-) -> int:\n+    tensor_sharding: sharding.Sharding, dim: int, global_shape: Shape) -> int:\n   \"\"\"Returns the number of indices for given dimension this host has access to.\n \n   Each host can have multiple number of devices that are spanning"
        }
    ]
},
{
    "Id": 18,
    "commit_link": "https://github.com/google/jax/commit/3ebebdfb76a9d9a03540b99a224a02ed547c3969",
    "date": "2024-06-28T06:08:36-07:00",
    "message": "[Mosaic GPU] Stop using nvgpu for TMA\n\nIt seems like nvgpu dialect bakes in a bunch of overly restrictive checks in its verifiers\nand doesn't really buy us much in this case. nvvm works just fine.\n\nPiperOrigin-RevId: 647653684",
    "changes": [
        {
            "name": "__init__.py",
            "path": "jax/experimental/mosaic/gpu/__init__.py",
            "patches": [
                {
                    "old_start": 38,
                    "old_length": 7,
                    "new_start": 38,
                    "new_length": 6,
                    "hunk": "@@ -38,7 +38,6 @@ from jaxlib.mlir.dialects import func\n from jaxlib.mlir.dialects import gpu\n from jaxlib.mlir.dialects import llvm\n from jaxlib.mlir.dialects import memref\n-from jaxlib.mlir.dialects import nvgpu\n from jaxlib.mlir.dialects import nvvm\n from jaxlib.mlir.passmanager import PassManager\n import numpy as np\n"
                },
                {
                    "old_start": 272,
                    "old_length": 20,
                    "new_start": 271,
                    "new_length": 8,
                    "hunk": "@@ -272,20 +271,8 @@ class LaunchContext:\n       transformed_slice_shape: tuple[int, ...],\n       swizzle: int | None,\n   ):\n-    index = ir.IndexType.get()\n-    ref_ty = ir.MemRefType(ref.type)\n     tma_desc_key = (ref, transformed_slice_shape, swizzle, gmem_transform)\n     if (tma_desc := self.tma_descriptors.get(tma_desc_key, None)) is None:\n-      swizzle_str = f\"swizzle_{swizzle}b\" if swizzle is not None else \"none\"\n-      default_tensor_map_attrs = dict(\n-          swizzle=swizzle_str, l2promo=\"none\", oob=\"zero\", interleave=\"none\"\n-      )\n-      tensor_map_ty = utils.get_tensormap_descriptor(\n-          tensor=(\n-              f\"memref<{'x'.join(map(str, transformed_slice_shape))}x{ref_ty.element_type}, 3>\"\n-          ),\n-          **default_tensor_map_attrs,\n-      )\n       with ir.InsertionPoint(self.launch_op):\n         for t in gmem_transform:\n           ref = t.apply(ref)\n"
                },
                {
                    "old_start": 318,
                    "old_length": 9,
                    "new_start": 305,
                    "new_length": 7,
                    "hunk": "@@ -318,9 +305,7 @@ class LaunchContext:\n       def cast_tma_desc(device_ptr):\n         # TODO(apaszke): Investigate why prefetching can cause launch failures\n         # nvvm.prefetch_tensormap(device_ptr)\n-        return builtin.unrealized_conversion_cast(\n-            [tensor_map_ty], [device_ptr]\n-        )\n+        return device_ptr\n       tma_desc = self._alloc_scratch(\n           TMA_DESCRIPTOR_BYTES,\n           alignment=TMA_DESCRIPTOR_ALIGNMENT,\n"
                },
                {
                    "old_start": 343,
                    "old_length": 6,
                    "new_start": 328,
                    "new_length": 7,
                    "hunk": "@@ -343,6 +328,7 @@ class LaunchContext:\n       uniform: bool = True,\n   ):\n     index = ir.IndexType.get()\n+    i32 = ir.IntegerType.get_signless(32)\n     smem = ir.Attribute.parse(\"#gpu.address_space<workgroup>\")\n     src_ref_ty = ir.MemRefType(src_ref.type)\n     dst_ref_ty = ir.MemRefType(dst_ref.type)\n"
                },
                {
                    "old_start": 401,
                    "old_length": 8,
                    "new_start": 387,
                    "new_length": 10,
                    "hunk": "@@ -401,8 +387,10 @@ class LaunchContext:\n         gmem_ref, gmem_transform, slice_shape, swizzle,\n     )\n \n-    # nvgpu TMA instructions expect reversed indices...\n-    rev_dyn_based_indices = reversed(dyn_base_indices)\n+    # We constuct TMA descriptors in column-major order.\n+    rev_dyn_base_indices = [\n+        arith.index_cast(i32, idx) for idx in reversed(dyn_base_indices)\n+    ]\n \n     uniform_ctx = (\n         functools.partial(mgpu.single_thread, per_block=False)\n"
                },
                {
                    "old_start": 410,
                    "old_length": 30,
                    "new_start": 398,
                    "new_length": 32,
                    "hunk": "@@ -410,30 +398,32 @@ class LaunchContext:\n         else contextlib.nullcontext\n     )\n \n+    rank = len(slice_shape)\n+    if rank > 5:  # TODO: apaszke - Implement stride compression\n+      raise ValueError(\"Async copies only support striding up to 5 dimensions\")\n+    smem_ptr = utils.memref_ptr(smem_ref, memory_space=3)\n     if gmem_ref is src_ref:\n+      assert barrier is not None  # for pytype\n+      slice_bytes = c(np.prod(slice_shape) * mgpu.bytewidth(element_type), i32)\n+      barrier_ptr = barrier.get_ptr()\n       with uniform_ctx():\n-        assert barrier is not None  # for pytype\n-        barrier_group = barrier.barrier_array.value\n-        barrier_idx = barrier.offset\n         if arrive:\n-          slice_bytes = c(\n-              np.prod(slice_shape) * mgpu.bytewidth(element_type), index\n-          )\n-          nvgpu.mbarrier_arrive_expect_tx(\n-              barrier_group, slice_bytes, barrier_idx\n-          )\n-        nvgpu.tma_async_load(\n-            smem_ref, barrier_group, tma_desc, rev_dyn_based_indices, barrier_idx\n+          nvvm.mbarrier_arrive_expect_tx_shared(barrier_ptr, slice_bytes)\n+        nvvm.cp_async_bulk_tensor_shared_cluster_global(\n+            smem_ptr, tma_desc, rev_dyn_base_indices, barrier_ptr, []\n         )\n     else:\n       with uniform_ctx():\n-        nvgpu.tma_async_store(smem_ref, tma_desc, rev_dyn_based_indices)\n+        nvvm.cp_async_bulk_tensor_global_shared_cta(\n+            tma_desc, smem_ptr, rev_dyn_base_indices\n+        )\n         nvvm.cp_async_bulk_commit_group()\n \n   def await_async_copy(\n       self, allow_groups: int, await_read_only: bool = False\n   ):\n     nvvm.cp_async_bulk_wait_group(allow_groups, read=await_read_only)\n+    # TODO(apaszke): Use a warpgroup barrier!!!\n     gpu.barrier()  # Groups are supposedly tracked per-thread\n \n \n"
                }
            ],
            "whole_deleted": "-from jaxlib.mlir.dialects import nvgpu\n-    index = ir.IndexType.get()\n-    ref_ty = ir.MemRefType(ref.type)\n-      swizzle_str = f\"swizzle_{swizzle}b\" if swizzle is not None else \"none\"\n-      default_tensor_map_attrs = dict(\n-          swizzle=swizzle_str, l2promo=\"none\", oob=\"zero\", interleave=\"none\"\n-      )\n-      tensor_map_ty = utils.get_tensormap_descriptor(\n-          tensor=(\n-              f\"memref<{'x'.join(map(str, transformed_slice_shape))}x{ref_ty.element_type}, 3>\"\n-          ),\n-          **default_tensor_map_attrs,\n-      )\n-        return builtin.unrealized_conversion_cast(\n-            [tensor_map_ty], [device_ptr]\n-        )\n-    # nvgpu TMA instructions expect reversed indices...\n-    rev_dyn_based_indices = reversed(dyn_base_indices)\n-        assert barrier is not None  # for pytype\n-        barrier_group = barrier.barrier_array.value\n-        barrier_idx = barrier.offset\n-          slice_bytes = c(\n-              np.prod(slice_shape) * mgpu.bytewidth(element_type), index\n-          )\n-          nvgpu.mbarrier_arrive_expect_tx(\n-              barrier_group, slice_bytes, barrier_idx\n-          )\n-        nvgpu.tma_async_load(\n-            smem_ref, barrier_group, tma_desc, rev_dyn_based_indices, barrier_idx\n-        nvgpu.tma_async_store(smem_ref, tma_desc, rev_dyn_based_indices)\n",
            "whole_added": "+        return device_ptr\n+    i32 = ir.IntegerType.get_signless(32)\n+    # We constuct TMA descriptors in column-major order.\n+    rev_dyn_base_indices = [\n+        arith.index_cast(i32, idx) for idx in reversed(dyn_base_indices)\n+    ]\n+    rank = len(slice_shape)\n+    if rank > 5:  # TODO: apaszke - Implement stride compression\n+      raise ValueError(\"Async copies only support striding up to 5 dimensions\")\n+    smem_ptr = utils.memref_ptr(smem_ref, memory_space=3)\n+      assert barrier is not None  # for pytype\n+      slice_bytes = c(np.prod(slice_shape) * mgpu.bytewidth(element_type), i32)\n+      barrier_ptr = barrier.get_ptr()\n+          nvvm.mbarrier_arrive_expect_tx_shared(barrier_ptr, slice_bytes)\n+        nvvm.cp_async_bulk_tensor_shared_cluster_global(\n+            smem_ptr, tma_desc, rev_dyn_base_indices, barrier_ptr, []\n+        nvvm.cp_async_bulk_tensor_global_shared_cta(\n+            tma_desc, smem_ptr, rev_dyn_base_indices\n+        )\n+    # TODO(apaszke): Use a warpgroup barrier!!!\n",
            "whole_hunk": "@@ -38,7 +38,6 @@ from jaxlib.mlir.dialects import func\n from jaxlib.mlir.dialects import gpu\n from jaxlib.mlir.dialects import llvm\n from jaxlib.mlir.dialects import memref\n-from jaxlib.mlir.dialects import nvgpu\n from jaxlib.mlir.dialects import nvvm\n from jaxlib.mlir.passmanager import PassManager\n import numpy as np\n@@ -272,20 +271,8 @@ class LaunchContext:\n       transformed_slice_shape: tuple[int, ...],\n       swizzle: int | None,\n   ):\n-    index = ir.IndexType.get()\n-    ref_ty = ir.MemRefType(ref.type)\n     tma_desc_key = (ref, transformed_slice_shape, swizzle, gmem_transform)\n     if (tma_desc := self.tma_descriptors.get(tma_desc_key, None)) is None:\n-      swizzle_str = f\"swizzle_{swizzle}b\" if swizzle is not None else \"none\"\n-      default_tensor_map_attrs = dict(\n-          swizzle=swizzle_str, l2promo=\"none\", oob=\"zero\", interleave=\"none\"\n-      )\n-      tensor_map_ty = utils.get_tensormap_descriptor(\n-          tensor=(\n-              f\"memref<{'x'.join(map(str, transformed_slice_shape))}x{ref_ty.element_type}, 3>\"\n-          ),\n-          **default_tensor_map_attrs,\n-      )\n       with ir.InsertionPoint(self.launch_op):\n         for t in gmem_transform:\n           ref = t.apply(ref)\n@@ -318,9 +305,7 @@ class LaunchContext:\n       def cast_tma_desc(device_ptr):\n         # TODO(apaszke): Investigate why prefetching can cause launch failures\n         # nvvm.prefetch_tensormap(device_ptr)\n-        return builtin.unrealized_conversion_cast(\n-            [tensor_map_ty], [device_ptr]\n-        )\n+        return device_ptr\n       tma_desc = self._alloc_scratch(\n           TMA_DESCRIPTOR_BYTES,\n           alignment=TMA_DESCRIPTOR_ALIGNMENT,\n@@ -343,6 +328,7 @@ class LaunchContext:\n       uniform: bool = True,\n   ):\n     index = ir.IndexType.get()\n+    i32 = ir.IntegerType.get_signless(32)\n     smem = ir.Attribute.parse(\"#gpu.address_space<workgroup>\")\n     src_ref_ty = ir.MemRefType(src_ref.type)\n     dst_ref_ty = ir.MemRefType(dst_ref.type)\n@@ -401,8 +387,10 @@ class LaunchContext:\n         gmem_ref, gmem_transform, slice_shape, swizzle,\n     )\n \n-    # nvgpu TMA instructions expect reversed indices...\n-    rev_dyn_based_indices = reversed(dyn_base_indices)\n+    # We constuct TMA descriptors in column-major order.\n+    rev_dyn_base_indices = [\n+        arith.index_cast(i32, idx) for idx in reversed(dyn_base_indices)\n+    ]\n \n     uniform_ctx = (\n         functools.partial(mgpu.single_thread, per_block=False)\n@@ -410,30 +398,32 @@ class LaunchContext:\n         else contextlib.nullcontext\n     )\n \n+    rank = len(slice_shape)\n+    if rank > 5:  # TODO: apaszke - Implement stride compression\n+      raise ValueError(\"Async copies only support striding up to 5 dimensions\")\n+    smem_ptr = utils.memref_ptr(smem_ref, memory_space=3)\n     if gmem_ref is src_ref:\n+      assert barrier is not None  # for pytype\n+      slice_bytes = c(np.prod(slice_shape) * mgpu.bytewidth(element_type), i32)\n+      barrier_ptr = barrier.get_ptr()\n       with uniform_ctx():\n-        assert barrier is not None  # for pytype\n-        barrier_group = barrier.barrier_array.value\n-        barrier_idx = barrier.offset\n         if arrive:\n-          slice_bytes = c(\n-              np.prod(slice_shape) * mgpu.bytewidth(element_type), index\n-          )\n-          nvgpu.mbarrier_arrive_expect_tx(\n-              barrier_group, slice_bytes, barrier_idx\n-          )\n-        nvgpu.tma_async_load(\n-            smem_ref, barrier_group, tma_desc, rev_dyn_based_indices, barrier_idx\n+          nvvm.mbarrier_arrive_expect_tx_shared(barrier_ptr, slice_bytes)\n+        nvvm.cp_async_bulk_tensor_shared_cluster_global(\n+            smem_ptr, tma_desc, rev_dyn_base_indices, barrier_ptr, []\n         )\n     else:\n       with uniform_ctx():\n-        nvgpu.tma_async_store(smem_ref, tma_desc, rev_dyn_based_indices)\n+        nvvm.cp_async_bulk_tensor_global_shared_cta(\n+            tma_desc, smem_ptr, rev_dyn_base_indices\n+        )\n         nvvm.cp_async_bulk_commit_group()\n \n   def await_async_copy(\n       self, allow_groups: int, await_read_only: bool = False\n   ):\n     nvvm.cp_async_bulk_wait_group(allow_groups, read=await_read_only)\n+    # TODO(apaszke): Use a warpgroup barrier!!!\n     gpu.barrier()  # Groups are supposedly tracked per-thread\n \n \n"
        },
        {
            "name": "utils.py",
            "path": "jax/experimental/mosaic/gpu/utils.py",
            "patches": [
                {
                    "old_start": 103,
                    "old_length": 12,
                    "new_start": 103,
                    "new_length": 6,
                    "hunk": "@@ -103,12 +103,6 @@ def c(val: int | float, ty):\n   return arith.constant(ty, attr)\n \n \n-def get_tensormap_descriptor(**attrs):\n-  return ir.Type.parse(\n-      f\"!nvgpu.tensormap.descriptor<{', '.join(k + '=' + v for k, v in attrs.items())}>\"\n-  )\n-\n-\n def debug_print(fmt, *args, uniform=True):\n   type_formats = []\n   new_args = []\n"
                },
                {
                    "old_start": 756,
                    "old_length": 3,
                    "new_start": 750,
                    "new_length": 35,
                    "hunk": "@@ -756,3 +750,35 @@ def warp_tree_reduce(value, op, group_size):\n     result = op(result, other_result)\n \n   return result\n+\n+\n+def memref_ptr(memref_arg, memory_space=None):\n+  i64 = ir.IntegerType.get_signless(64)\n+  memref_ty = ir.MemRefType(memref_arg.type)\n+  if len(memref_ty.shape) == 0:\n+    raise NotImplementedError\n+  elem_bytewidth = bytewidth(memref_ty.element_type)\n+  rank = len(memref_ty.shape)\n+  # TODO: Read out memory space from memref\n+  space = \"\" if memory_space is None else \"<\" + str(memory_space) + \">\"\n+  ptr_ty = ir.Type.parse(\"!llvm.ptr\" + space)\n+  desc_ty = ir.Type.parse(\n+      f\"!llvm.struct<({ptr_ty}, {ptr_ty}, i64, array<{rank} x i64>,\"\n+      f\" array<{rank} x i64>)>\"\n+  )\n+  desc = builtin.UnrealizedConversionCastOp([desc_ty], [memref_arg])\n+  aligned_ptr = llvm.extractvalue(ptr_ty, desc, [1])\n+  offset_elems = llvm.extractvalue(i64, desc, [2])\n+  offset_bytes = llvm.mul(\n+      offset_elems,\n+      c(elem_bytewidth, i64),\n+      overflow_flags=llvm.IntegerOverflowFlags.none,\n+  )\n+  return llvm.inttoptr(\n+      ptr_ty,\n+      llvm.add(\n+          llvm.ptrtoint(i64, aligned_ptr),\n+          offset_bytes,\n+          overflow_flags=llvm.IntegerOverflowFlags.none,\n+      ),\n+  )\n"
                }
            ],
            "whole_deleted": "-def get_tensormap_descriptor(**attrs):\n-  return ir.Type.parse(\n-      f\"!nvgpu.tensormap.descriptor<{', '.join(k + '=' + v for k, v in attrs.items())}>\"\n-  )\n-\n-\n",
            "whole_added": "+\n+\n+def memref_ptr(memref_arg, memory_space=None):\n+  i64 = ir.IntegerType.get_signless(64)\n+  memref_ty = ir.MemRefType(memref_arg.type)\n+  if len(memref_ty.shape) == 0:\n+    raise NotImplementedError\n+  elem_bytewidth = bytewidth(memref_ty.element_type)\n+  rank = len(memref_ty.shape)\n+  # TODO: Read out memory space from memref\n+  space = \"\" if memory_space is None else \"<\" + str(memory_space) + \">\"\n+  ptr_ty = ir.Type.parse(\"!llvm.ptr\" + space)\n+  desc_ty = ir.Type.parse(\n+      f\"!llvm.struct<({ptr_ty}, {ptr_ty}, i64, array<{rank} x i64>,\"\n+      f\" array<{rank} x i64>)>\"\n+  )\n+  desc = builtin.UnrealizedConversionCastOp([desc_ty], [memref_arg])\n+  aligned_ptr = llvm.extractvalue(ptr_ty, desc, [1])\n+  offset_elems = llvm.extractvalue(i64, desc, [2])\n+  offset_bytes = llvm.mul(\n+      offset_elems,\n+      c(elem_bytewidth, i64),\n+      overflow_flags=llvm.IntegerOverflowFlags.none,\n+  )\n+  return llvm.inttoptr(\n+      ptr_ty,\n+      llvm.add(\n+          llvm.ptrtoint(i64, aligned_ptr),\n+          offset_bytes,\n+          overflow_flags=llvm.IntegerOverflowFlags.none,\n+      ),\n+  )\n",
            "whole_hunk": "@@ -103,12 +103,6 @@ def c(val: int | float, ty):\n   return arith.constant(ty, attr)\n \n \n-def get_tensormap_descriptor(**attrs):\n-  return ir.Type.parse(\n-      f\"!nvgpu.tensormap.descriptor<{', '.join(k + '=' + v for k, v in attrs.items())}>\"\n-  )\n-\n-\n def debug_print(fmt, *args, uniform=True):\n   type_formats = []\n   new_args = []\n@@ -756,3 +750,35 @@ def warp_tree_reduce(value, op, group_size):\n     result = op(result, other_result)\n \n   return result\n+\n+\n+def memref_ptr(memref_arg, memory_space=None):\n+  i64 = ir.IntegerType.get_signless(64)\n+  memref_ty = ir.MemRefType(memref_arg.type)\n+  if len(memref_ty.shape) == 0:\n+    raise NotImplementedError\n+  elem_bytewidth = bytewidth(memref_ty.element_type)\n+  rank = len(memref_ty.shape)\n+  # TODO: Read out memory space from memref\n+  space = \"\" if memory_space is None else \"<\" + str(memory_space) + \">\"\n+  ptr_ty = ir.Type.parse(\"!llvm.ptr\" + space)\n+  desc_ty = ir.Type.parse(\n+      f\"!llvm.struct<({ptr_ty}, {ptr_ty}, i64, array<{rank} x i64>,\"\n+      f\" array<{rank} x i64>)>\"\n+  )\n+  desc = builtin.UnrealizedConversionCastOp([desc_ty], [memref_arg])\n+  aligned_ptr = llvm.extractvalue(ptr_ty, desc, [1])\n+  offset_elems = llvm.extractvalue(i64, desc, [2])\n+  offset_bytes = llvm.mul(\n+      offset_elems,\n+      c(elem_bytewidth, i64),\n+      overflow_flags=llvm.IntegerOverflowFlags.none,\n+  )\n+  return llvm.inttoptr(\n+      ptr_ty,\n+      llvm.add(\n+          llvm.ptrtoint(i64, aligned_ptr),\n+          offset_bytes,\n+          overflow_flags=llvm.IntegerOverflowFlags.none,\n+      ),\n+  )\n"
        },
        {
            "name": "wgmma.py",
            "path": "jax/experimental/mosaic/gpu/wgmma.py",
            "patches": [
                {
                    "old_start": 28,
                    "old_length": 6,
                    "new_start": 28,
                    "new_length": 7,
                    "hunk": "@@ -28,6 +28,7 @@ from jaxlib.mlir.dialects import vector\n import numpy as np\n \n from . import dsl as mgpu\n+from . import utils\n \n # mypy: ignore-errors\n \n"
                },
                {
                    "old_start": 85,
                    "old_length": 37,
                    "new_start": 86,
                    "new_length": 10,
                    "hunk": "@@ -85,37 +86,10 @@ def wgmma_encode(x: int):\n   return result\n \n \n-def llvm_mul(x, y):\n-  return llvm.mul(x, y, overflow_flags=llvm.IntegerOverflowFlags.none)\n-\n-\n def llvm_add(x, y):\n   return llvm.add(x, y, overflow_flags=llvm.IntegerOverflowFlags.none)\n \n \n-def get_memref_base(memref_arg, memory_space=None):\n-  i64 = ir.IntegerType.get_signless(64)\n-  memref_ty = ir.MemRefType(memref_arg.type)\n-  if len(memref_ty.shape) == 0:\n-    raise NotImplementedError\n-  elem_bytewidth = bytewidth(memref_ty.element_type)\n-  rank = len(memref_ty.shape)\n-  # TODO: Read out memory space from memref\n-  space = \"\" if memory_space is None else \"<\" + str(memory_space) + \">\"\n-  ptr_ty = ir.Type.parse(\"!llvm.ptr\" + space)\n-  desc_ty = ir.Type.parse(\n-      f\"!llvm.struct<({ptr_ty}, {ptr_ty}, i64, array<{rank} x i64>,\"\n-      f\" array<{rank} x i64>)>\"\n-  )\n-  desc = builtin.UnrealizedConversionCastOp([desc_ty], [memref_arg])\n-  aligned_ptr = llvm.extractvalue(ptr_ty, desc, [1])\n-  offset_elems = llvm.extractvalue(i64, desc, [2])\n-  offset_bytes = llvm_mul(offset_elems, c(elem_bytewidth, i64))\n-  return llvm.inttoptr(\n-      ptr_ty, llvm_add(llvm.ptrtoint(i64, aligned_ptr), offset_bytes)\n-  )\n-\n-\n def create_descriptor(\n     memref_arg,\n     leading_byte_offset: int,\n"
                },
                {
                    "old_start": 125,
                    "old_length": 7,
                    "new_start": 99,
                    "new_length": 7,
                    "hunk": "@@ -125,7 +99,7 @@ def create_descriptor(\n     nvgpu_type=None,\n ):\n   i64 = ir.IntegerType.get_signless(64)\n-  ptr_val = llvm.ptrtoint(i64, get_memref_base(memref_arg, memory_space))\n+  ptr_val = llvm.ptrtoint(i64, utils.memref_ptr(memref_arg, memory_space))\n   if swizzle is None:\n     swizzle_encoding = 0\n   elif swizzle == 128:\n"
                }
            ],
            "whole_deleted": "-def llvm_mul(x, y):\n-  return llvm.mul(x, y, overflow_flags=llvm.IntegerOverflowFlags.none)\n-\n-\n-def get_memref_base(memref_arg, memory_space=None):\n-  i64 = ir.IntegerType.get_signless(64)\n-  memref_ty = ir.MemRefType(memref_arg.type)\n-  if len(memref_ty.shape) == 0:\n-    raise NotImplementedError\n-  elem_bytewidth = bytewidth(memref_ty.element_type)\n-  rank = len(memref_ty.shape)\n-  # TODO: Read out memory space from memref\n-  space = \"\" if memory_space is None else \"<\" + str(memory_space) + \">\"\n-  ptr_ty = ir.Type.parse(\"!llvm.ptr\" + space)\n-  desc_ty = ir.Type.parse(\n-      f\"!llvm.struct<({ptr_ty}, {ptr_ty}, i64, array<{rank} x i64>,\"\n-      f\" array<{rank} x i64>)>\"\n-  )\n-  desc = builtin.UnrealizedConversionCastOp([desc_ty], [memref_arg])\n-  aligned_ptr = llvm.extractvalue(ptr_ty, desc, [1])\n-  offset_elems = llvm.extractvalue(i64, desc, [2])\n-  offset_bytes = llvm_mul(offset_elems, c(elem_bytewidth, i64))\n-  return llvm.inttoptr(\n-      ptr_ty, llvm_add(llvm.ptrtoint(i64, aligned_ptr), offset_bytes)\n-  )\n-\n-\n-  ptr_val = llvm.ptrtoint(i64, get_memref_base(memref_arg, memory_space))\n",
            "whole_added": "+from . import utils\n+  ptr_val = llvm.ptrtoint(i64, utils.memref_ptr(memref_arg, memory_space))\n",
            "whole_hunk": "@@ -28,6 +28,7 @@ from jaxlib.mlir.dialects import vector\n import numpy as np\n \n from . import dsl as mgpu\n+from . import utils\n \n # mypy: ignore-errors\n \n@@ -85,37 +86,10 @@ def wgmma_encode(x: int):\n   return result\n \n \n-def llvm_mul(x, y):\n-  return llvm.mul(x, y, overflow_flags=llvm.IntegerOverflowFlags.none)\n-\n-\n def llvm_add(x, y):\n   return llvm.add(x, y, overflow_flags=llvm.IntegerOverflowFlags.none)\n \n \n-def get_memref_base(memref_arg, memory_space=None):\n-  i64 = ir.IntegerType.get_signless(64)\n-  memref_ty = ir.MemRefType(memref_arg.type)\n-  if len(memref_ty.shape) == 0:\n-    raise NotImplementedError\n-  elem_bytewidth = bytewidth(memref_ty.element_type)\n-  rank = len(memref_ty.shape)\n-  # TODO: Read out memory space from memref\n-  space = \"\" if memory_space is None else \"<\" + str(memory_space) + \">\"\n-  ptr_ty = ir.Type.parse(\"!llvm.ptr\" + space)\n-  desc_ty = ir.Type.parse(\n-      f\"!llvm.struct<({ptr_ty}, {ptr_ty}, i64, array<{rank} x i64>,\"\n-      f\" array<{rank} x i64>)>\"\n-  )\n-  desc = builtin.UnrealizedConversionCastOp([desc_ty], [memref_arg])\n-  aligned_ptr = llvm.extractvalue(ptr_ty, desc, [1])\n-  offset_elems = llvm.extractvalue(i64, desc, [2])\n-  offset_bytes = llvm_mul(offset_elems, c(elem_bytewidth, i64))\n-  return llvm.inttoptr(\n-      ptr_ty, llvm_add(llvm.ptrtoint(i64, aligned_ptr), offset_bytes)\n-  )\n-\n-\n def create_descriptor(\n     memref_arg,\n     leading_byte_offset: int,\n@@ -125,7 +99,7 @@ def create_descriptor(\n     nvgpu_type=None,\n ):\n   i64 = ir.IntegerType.get_signless(64)\n-  ptr_val = llvm.ptrtoint(i64, get_memref_base(memref_arg, memory_space))\n+  ptr_val = llvm.ptrtoint(i64, utils.memref_ptr(memref_arg, memory_space))\n   if swizzle is None:\n     swizzle_encoding = 0\n   elif swizzle == 128:\n"
        },
        {
            "name": "gpu_test.py",
            "path": "tests/mosaic/gpu_test.py",
            "patches": [
                {
                    "old_start": 715,
                    "old_length": 7,
                    "new_start": 715,
                    "new_length": 7,
                    "hunk": "@@ -715,7 +715,7 @@ class TMATest(TestCase):\n       shape=((64, 64), (5, 64), (2, 3, 5, 64)),\n       dtype=(jnp.float16, jnp.float32),\n   )\n-  def test_tma_load(self, swizzle, shape, dtype):\n+  def test_tma_load_basic(self, swizzle, shape, dtype):\n     if dtype == jnp.float32:\n       shape = (*shape[:-1], shape[-1] // 2)\n     i1 = ir.IntegerType.get_signless(1)"
                }
            ],
            "whole_deleted": "-  def test_tma_load(self, swizzle, shape, dtype):\n",
            "whole_added": "+  def test_tma_load_basic(self, swizzle, shape, dtype):\n",
            "whole_hunk": "@@ -715,7 +715,7 @@ class TMATest(TestCase):\n       shape=((64, 64), (5, 64), (2, 3, 5, 64)),\n       dtype=(jnp.float16, jnp.float32),\n   )\n-  def test_tma_load(self, swizzle, shape, dtype):\n+  def test_tma_load_basic(self, swizzle, shape, dtype):\n     if dtype == jnp.float32:\n       shape = (*shape[:-1], shape[-1] // 2)\n     i1 = ir.IntegerType.get_signless(1)"
        }
    ]
},
{
    "Id": 19,
    "commit_link": "https://github.com/google/jax/commit/d737abda4874c0571b4e38b03cd98444d9e558e5",
    "date": "2024-06-23T07:26:12+03:00",
    "message": "[export] Fix multi-platform lowering for unknown platform, with donated_argnums\n\nI had to ensure that the check for platforms supporting donation\nonly kicks in when we actually have donation.",
    "changes": [
        {
            "name": "_export.py",
            "path": "jax/_src/export/_export.py",
            "patches": [
                {
                    "old_start": 161,
                    "old_length": 10,
                    "new_start": 161,
                    "new_length": 10,
                    "hunk": "@@ -161,10 +161,10 @@ class Exported:\n         the mesh. See `out_shardings_jax` for a way to turn these\n         into sharding specification that can be used with JAX APIs.\n     nr_devices: the number of devices that the module has been lowered for.\n-    platforms: a tuple containing at least one of 'tpu', 'cpu',\n-        'cuda', 'rocm'. See https://jax.readthedocs.io/en/latest/export.html#module-calling-convention\n-        for the calling convention for when\n-        there are multiple export platforms.\n+    platforms: a tuple containing the platforms for which the function should\n+        be exported. The set of platforms in JAX is open-ended; users can\n+        add platforms. JAX built-in platforms are: 'tpu', 'cpu', 'cuda', 'rocm'.\n+        See https://jax.readthedocs.io/en/latest/export/export.html#cross-platform-and-multi-platform-export.\n     ordered_effects: the ordered effects present in the serialized module.\n         This is present from serialization version 9. See https://jax.readthedocs.io/en/latest/export.html#module-calling-convention\n         for the calling convention in presence of ordered effects.\n"
                }
            ],
            "whole_deleted": "-    platforms: a tuple containing at least one of 'tpu', 'cpu',\n-        'cuda', 'rocm'. See https://jax.readthedocs.io/en/latest/export.html#module-calling-convention\n-        for the calling convention for when\n-        there are multiple export platforms.\n",
            "whole_added": "+    platforms: a tuple containing the platforms for which the function should\n+        be exported. The set of platforms in JAX is open-ended; users can\n+        add platforms. JAX built-in platforms are: 'tpu', 'cpu', 'cuda', 'rocm'.\n+        See https://jax.readthedocs.io/en/latest/export/export.html#cross-platform-and-multi-platform-export.\n",
            "whole_hunk": "@@ -161,10 +161,10 @@ class Exported:\n         the mesh. See `out_shardings_jax` for a way to turn these\n         into sharding specification that can be used with JAX APIs.\n     nr_devices: the number of devices that the module has been lowered for.\n-    platforms: a tuple containing at least one of 'tpu', 'cpu',\n-        'cuda', 'rocm'. See https://jax.readthedocs.io/en/latest/export.html#module-calling-convention\n-        for the calling convention for when\n-        there are multiple export platforms.\n+    platforms: a tuple containing the platforms for which the function should\n+        be exported. The set of platforms in JAX is open-ended; users can\n+        add platforms. JAX built-in platforms are: 'tpu', 'cpu', 'cuda', 'rocm'.\n+        See https://jax.readthedocs.io/en/latest/export/export.html#cross-platform-and-multi-platform-export.\n     ordered_effects: the ordered effects present in the serialized module.\n         This is present from serialization version 9. See https://jax.readthedocs.io/en/latest/export.html#module-calling-convention\n         for the calling convention in presence of ordered effects.\n"
        },
        {
            "name": "mlir.py",
            "path": "jax/_src/interpreters/mlir.py",
            "patches": [
                {
                    "old_start": 886,
                    "old_length": 7,
                    "new_start": 886,
                    "new_length": 8,
                    "hunk": "@@ -886,7 +886,8 @@ def lower_jaxpr_to_module(\n   platforms_with_donation = [p for p in platforms\n                              if p in _platforms_with_donation]\n   if platforms_with_donation:\n-    if len(platforms_with_donation) != len(platforms):\n+    if len(platforms_with_donation) != len(platforms) and (\n+        xla_donated_args or any(donated_args)):\n       raise NotImplementedError(\n         \"In multi-platform lowering either all or no lowering platforms \"\n         f\"should support donation. Lowering for {platforms} of which \"\n"
                }
            ],
            "whole_deleted": "-    if len(platforms_with_donation) != len(platforms):\n",
            "whole_added": "+    if len(platforms_with_donation) != len(platforms) and (\n+        xla_donated_args or any(donated_args)):\n",
            "whole_hunk": "@@ -886,7 +886,8 @@ def lower_jaxpr_to_module(\n   platforms_with_donation = [p for p in platforms\n                              if p in _platforms_with_donation]\n   if platforms_with_donation:\n-    if len(platforms_with_donation) != len(platforms):\n+    if len(platforms_with_donation) != len(platforms) and (\n+        xla_donated_args or any(donated_args)):\n       raise NotImplementedError(\n         \"In multi-platform lowering either all or no lowering platforms \"\n         f\"should support donation. Lowering for {platforms} of which \"\n"
        },
        {
            "name": "export_test.py",
            "path": "tests/export_test.py",
            "patches": [
                {
                    "old_start": 911,
                    "old_length": 6,
                    "new_start": 911,
                    "new_length": 17,
                    "hunk": "@@ -911,6 +911,17 @@ class JaxExportTest(jtu.JaxTestCase):\n     a = exp2.in_avals[0].shape[0]\n     self.assertEqual(exp2.out_avals[0].shape, output_shape(a))\n \n+  def test_with_donation(self):\n+    f = jax.jit(jnp.sin, donate_argnums=(0,))\n+    x = np.arange(3, dtype=np.float32)\n+    exp = export.export(f)(x)\n+\n+    def caller(x):\n+      y = exp.call(x)\n+      return x + y\n+    res = jax.jit(caller)(x)\n+    self.assertAllClose(res, x + np.sin(x))\n+\n   def test_poly_call_pmap(self):\n     if len(jax.devices()) < 2:\n       self.skipTest(\"Need at least 2 devices\")\n"
                },
                {
                    "old_start": 1312,
                    "old_length": 7,
                    "new_start": 1323,
                    "new_length": 7,
                    "hunk": "@@ -1312,7 +1323,7 @@ class JaxExportTest(jtu.JaxTestCase):\n   def test_multi_platform(self):\n     x = np.arange(8, dtype=np.float32)\n     exp = get_exported(jax.jit(_testing_multi_platform_func),\n-                       lowering_platforms=(\"tpu\", \"cpu\", \"cuda\",\"rocm\"))(x)\n+                       lowering_platforms=(\"tpu\", \"cpu\", \"cuda\", \"rocm\"))(x)\n     self.assertEqual(exp.platforms, (\"tpu\", \"cpu\", \"cuda\", \"rocm\"))\n     module_str = str(exp.mlir_module())\n     expected_main_re = (\n"
                },
                {
                    "old_start": 1335,
                    "old_length": 14,
                    "new_start": 1346,
                    "new_length": 14,
                    "hunk": "@@ -1335,14 +1346,14 @@ class JaxExportTest(jtu.JaxTestCase):\n   def test_multi_platform_nested(self):\n     x = np.arange(5, dtype=np.float32)\n     exp = get_exported(jax.jit(lambda x: _testing_multi_platform_func(jnp.sin(x))),\n-                       lowering_platforms=(\"cpu\", \"tpu\", \"cuda\",\"rocm\"))(x)\n-    self.assertEqual(exp.platforms, (\"cpu\", \"tpu\", \"cuda\",\"rocm\"))\n+                       lowering_platforms=(\"cpu\", \"tpu\", \"cuda\", \"rocm\"))(x)\n+    self.assertEqual(exp.platforms, (\"cpu\", \"tpu\", \"cuda\", \"rocm\"))\n \n     # Now serialize the call to the exported using a different sequence of\n     # lowering platforms, but included in the lowering platforms for the\n     # nested exported.\n     exp2 = get_exported(jax.jit(exp.call),\n-                        lowering_platforms=(\"cpu\", \"cuda\",\"rocm\"))(x)\n+                        lowering_platforms=(\"cpu\", \"cuda\", \"rocm\"))(x)\n \n     # Ensure that we do not have multiple lowerings of the exported function\n     exp2_module_str = str(exp2.mlir_module())\n"
                },
                {
                    "old_start": 1361,
                    "old_length": 7,
                    "new_start": 1372,
                    "new_length": 7,
                    "hunk": "@@ -1361,7 +1372,7 @@ class JaxExportTest(jtu.JaxTestCase):\n   def test_multi_platform_nested_inside_single_platform_export(self):\n     x = np.arange(5, dtype=np.float32)\n     exp = get_exported(jax.jit(_testing_multi_platform_func),\n-                       lowering_platforms=(\"cpu\", \"tpu\", \"cuda\",\"rocm\"))(x)\n+                       lowering_platforms=(\"cpu\", \"tpu\", \"cuda\", \"rocm\"))(x)\n     self.assertEqual(exp.platforms, (\"cpu\", \"tpu\", \"cuda\", \"rocm\"))\n \n     # Now serialize the call for the current platform.\n"
                },
                {
                    "old_start": 1430,
                    "old_length": 6,
                    "new_start": 1441,
                    "new_length": 31,
                    "hunk": "@@ -1430,6 +1441,31 @@ class JaxExportTest(jtu.JaxTestCase):\n     expected = x * np.float32(dict(cpu=2, gpu=3, tpu=4)[jtu.device_under_test()])\n     self.assertAllClose(exp.call(x), expected)\n \n+  def test_multi_platform_unknown_platform(self):\n+    x = np.arange(8, dtype=np.float32)\n+    exp = get_exported(jax.jit(jnp.sin),\n+                       lowering_platforms=(\"tpu\", \"cpu\", \"cuda\", \"other\"))(x)\n+    self.assertEqual(exp.platforms, (\"tpu\", \"cpu\", \"cuda\", \"other\"))\n+\n+\n+  def test_multi_platform_with_donation(self):\n+    f = jax.jit(jnp.sin, donate_argnums=(0,))\n+    x = np.arange(3, dtype=np.float32)\n+    exp = export.export(f, platforms=[\"cpu\", \"tpu\"])(x)\n+    if jtu.device_under_test() not in [\"cpu\", \"tpu\"]:\n+      self.skipTest(\"other platform\")\n+\n+    def caller(x):\n+      y = exp.call(x)\n+      return x + y\n+    res = jax.jit(caller)(x)\n+    self.assertAllClose(res, x + np.sin(x))\n+\n+    with self.assertRaisesRegex(\n+        NotImplementedError,\n+        \"In multi-platform lowering either all or no lowering platforms should support donation\"):\n+      export.export(f, platforms=[\"cpu\", \"tpu\", \"other\"])(x)\n+\n   def test_multi_platform_and_poly(self):\n     if jtu.test_device_matches([\"gpu\"]):\n       # The export is not applicable to GPU"
                }
            ],
            "whole_deleted": "-                       lowering_platforms=(\"tpu\", \"cpu\", \"cuda\",\"rocm\"))(x)\n-                       lowering_platforms=(\"cpu\", \"tpu\", \"cuda\",\"rocm\"))(x)\n-    self.assertEqual(exp.platforms, (\"cpu\", \"tpu\", \"cuda\",\"rocm\"))\n-                        lowering_platforms=(\"cpu\", \"cuda\",\"rocm\"))(x)\n-                       lowering_platforms=(\"cpu\", \"tpu\", \"cuda\",\"rocm\"))(x)\n",
            "whole_added": "+  def test_with_donation(self):\n+    f = jax.jit(jnp.sin, donate_argnums=(0,))\n+    x = np.arange(3, dtype=np.float32)\n+    exp = export.export(f)(x)\n+\n+    def caller(x):\n+      y = exp.call(x)\n+      return x + y\n+    res = jax.jit(caller)(x)\n+    self.assertAllClose(res, x + np.sin(x))\n+\n+                       lowering_platforms=(\"tpu\", \"cpu\", \"cuda\", \"rocm\"))(x)\n+                       lowering_platforms=(\"cpu\", \"tpu\", \"cuda\", \"rocm\"))(x)\n+    self.assertEqual(exp.platforms, (\"cpu\", \"tpu\", \"cuda\", \"rocm\"))\n+                        lowering_platforms=(\"cpu\", \"cuda\", \"rocm\"))(x)\n+                       lowering_platforms=(\"cpu\", \"tpu\", \"cuda\", \"rocm\"))(x)\n+  def test_multi_platform_unknown_platform(self):\n+    x = np.arange(8, dtype=np.float32)\n+    exp = get_exported(jax.jit(jnp.sin),\n+                       lowering_platforms=(\"tpu\", \"cpu\", \"cuda\", \"other\"))(x)\n+    self.assertEqual(exp.platforms, (\"tpu\", \"cpu\", \"cuda\", \"other\"))\n+\n+\n+  def test_multi_platform_with_donation(self):\n+    f = jax.jit(jnp.sin, donate_argnums=(0,))\n+    x = np.arange(3, dtype=np.float32)\n+    exp = export.export(f, platforms=[\"cpu\", \"tpu\"])(x)\n+    if jtu.device_under_test() not in [\"cpu\", \"tpu\"]:\n+      self.skipTest(\"other platform\")\n+\n+    def caller(x):\n+      y = exp.call(x)\n+      return x + y\n+    res = jax.jit(caller)(x)\n+    self.assertAllClose(res, x + np.sin(x))\n+\n+    with self.assertRaisesRegex(\n+        NotImplementedError,\n+        \"In multi-platform lowering either all or no lowering platforms should support donation\"):\n+      export.export(f, platforms=[\"cpu\", \"tpu\", \"other\"])(x)\n+\n",
            "whole_hunk": "@@ -911,6 +911,17 @@ class JaxExportTest(jtu.JaxTestCase):\n     a = exp2.in_avals[0].shape[0]\n     self.assertEqual(exp2.out_avals[0].shape, output_shape(a))\n \n+  def test_with_donation(self):\n+    f = jax.jit(jnp.sin, donate_argnums=(0,))\n+    x = np.arange(3, dtype=np.float32)\n+    exp = export.export(f)(x)\n+\n+    def caller(x):\n+      y = exp.call(x)\n+      return x + y\n+    res = jax.jit(caller)(x)\n+    self.assertAllClose(res, x + np.sin(x))\n+\n   def test_poly_call_pmap(self):\n     if len(jax.devices()) < 2:\n       self.skipTest(\"Need at least 2 devices\")\n@@ -1312,7 +1323,7 @@ class JaxExportTest(jtu.JaxTestCase):\n   def test_multi_platform(self):\n     x = np.arange(8, dtype=np.float32)\n     exp = get_exported(jax.jit(_testing_multi_platform_func),\n-                       lowering_platforms=(\"tpu\", \"cpu\", \"cuda\",\"rocm\"))(x)\n+                       lowering_platforms=(\"tpu\", \"cpu\", \"cuda\", \"rocm\"))(x)\n     self.assertEqual(exp.platforms, (\"tpu\", \"cpu\", \"cuda\", \"rocm\"))\n     module_str = str(exp.mlir_module())\n     expected_main_re = (\n@@ -1335,14 +1346,14 @@ class JaxExportTest(jtu.JaxTestCase):\n   def test_multi_platform_nested(self):\n     x = np.arange(5, dtype=np.float32)\n     exp = get_exported(jax.jit(lambda x: _testing_multi_platform_func(jnp.sin(x))),\n-                       lowering_platforms=(\"cpu\", \"tpu\", \"cuda\",\"rocm\"))(x)\n-    self.assertEqual(exp.platforms, (\"cpu\", \"tpu\", \"cuda\",\"rocm\"))\n+                       lowering_platforms=(\"cpu\", \"tpu\", \"cuda\", \"rocm\"))(x)\n+    self.assertEqual(exp.platforms, (\"cpu\", \"tpu\", \"cuda\", \"rocm\"))\n \n     # Now serialize the call to the exported using a different sequence of\n     # lowering platforms, but included in the lowering platforms for the\n     # nested exported.\n     exp2 = get_exported(jax.jit(exp.call),\n-                        lowering_platforms=(\"cpu\", \"cuda\",\"rocm\"))(x)\n+                        lowering_platforms=(\"cpu\", \"cuda\", \"rocm\"))(x)\n \n     # Ensure that we do not have multiple lowerings of the exported function\n     exp2_module_str = str(exp2.mlir_module())\n@@ -1361,7 +1372,7 @@ class JaxExportTest(jtu.JaxTestCase):\n   def test_multi_platform_nested_inside_single_platform_export(self):\n     x = np.arange(5, dtype=np.float32)\n     exp = get_exported(jax.jit(_testing_multi_platform_func),\n-                       lowering_platforms=(\"cpu\", \"tpu\", \"cuda\",\"rocm\"))(x)\n+                       lowering_platforms=(\"cpu\", \"tpu\", \"cuda\", \"rocm\"))(x)\n     self.assertEqual(exp.platforms, (\"cpu\", \"tpu\", \"cuda\", \"rocm\"))\n \n     # Now serialize the call for the current platform.\n@@ -1430,6 +1441,31 @@ class JaxExportTest(jtu.JaxTestCase):\n     expected = x * np.float32(dict(cpu=2, gpu=3, tpu=4)[jtu.device_under_test()])\n     self.assertAllClose(exp.call(x), expected)\n \n+  def test_multi_platform_unknown_platform(self):\n+    x = np.arange(8, dtype=np.float32)\n+    exp = get_exported(jax.jit(jnp.sin),\n+                       lowering_platforms=(\"tpu\", \"cpu\", \"cuda\", \"other\"))(x)\n+    self.assertEqual(exp.platforms, (\"tpu\", \"cpu\", \"cuda\", \"other\"))\n+\n+\n+  def test_multi_platform_with_donation(self):\n+    f = jax.jit(jnp.sin, donate_argnums=(0,))\n+    x = np.arange(3, dtype=np.float32)\n+    exp = export.export(f, platforms=[\"cpu\", \"tpu\"])(x)\n+    if jtu.device_under_test() not in [\"cpu\", \"tpu\"]:\n+      self.skipTest(\"other platform\")\n+\n+    def caller(x):\n+      y = exp.call(x)\n+      return x + y\n+    res = jax.jit(caller)(x)\n+    self.assertAllClose(res, x + np.sin(x))\n+\n+    with self.assertRaisesRegex(\n+        NotImplementedError,\n+        \"In multi-platform lowering either all or no lowering platforms should support donation\"):\n+      export.export(f, platforms=[\"cpu\", \"tpu\", \"other\"])(x)\n+\n   def test_multi_platform_and_poly(self):\n     if jtu.test_device_matches([\"gpu\"]):\n       # The export is not applicable to GPU"
        }
    ]
},
{
    "Id": 20,
    "commit_link": "https://github.com/google/jax/commit/afa6e6751a98b081803342ed5053a177b32f54e5",
    "date": "2024-06-13T11:29:25-07:00",
    "message": "Merge pull request #21853 from jakevdp:check-hashable-dtype\n\nPiperOrigin-RevId: 643066121",
    "changes": [
        {
            "name": "dtypes.py",
            "path": "jax/_src/dtypes.py",
            "patches": [
                {
                    "old_start": 752,
                    "old_length": 14,
                    "new_start": 752,
                    "new_length": 14,
                    "hunk": "@@ -752,14 +752,14 @@ def check_user_dtype_supported(dtype, fun_name=None):\n     msg = f\"JAX only supports number and bool dtypes, got dtype {dtype}\"\n     msg += f\" in {fun_name}\" if fun_name else \"\"\n     raise TypeError(msg)\n-  if dtype is not None and np_dtype != canonicalize_dtype(dtype):\n+  if dtype is not None and np_dtype != canonicalize_dtype(np_dtype):\n     msg = (\"Explicitly requested dtype {} {} is not available, \"\n            \"and will be truncated to dtype {}. To enable more dtypes, set the \"\n            \"jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell \"\n            \"environment variable. \"\n            \"See https://github.com/google/jax#current-gotchas for more.\")\n     fun_name = f\"requested in {fun_name}\" if fun_name else \"\"\n-    truncated_dtype = canonicalize_dtype(dtype).name\n+    truncated_dtype = canonicalize_dtype(np_dtype).name\n     warnings.warn(msg.format(dtype, fun_name, truncated_dtype), stacklevel=3)\n \n def safe_to_cast(input_dtype_or_value: Any,\n"
                }
            ],
            "whole_deleted": "-  if dtype is not None and np_dtype != canonicalize_dtype(dtype):\n-    truncated_dtype = canonicalize_dtype(dtype).name\n",
            "whole_added": "+  if dtype is not None and np_dtype != canonicalize_dtype(np_dtype):\n+    truncated_dtype = canonicalize_dtype(np_dtype).name\n",
            "whole_hunk": "@@ -752,14 +752,14 @@ def check_user_dtype_supported(dtype, fun_name=None):\n     msg = f\"JAX only supports number and bool dtypes, got dtype {dtype}\"\n     msg += f\" in {fun_name}\" if fun_name else \"\"\n     raise TypeError(msg)\n-  if dtype is not None and np_dtype != canonicalize_dtype(dtype):\n+  if dtype is not None and np_dtype != canonicalize_dtype(np_dtype):\n     msg = (\"Explicitly requested dtype {} {} is not available, \"\n            \"and will be truncated to dtype {}. To enable more dtypes, set the \"\n            \"jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell \"\n            \"environment variable. \"\n            \"See https://github.com/google/jax#current-gotchas for more.\")\n     fun_name = f\"requested in {fun_name}\" if fun_name else \"\"\n-    truncated_dtype = canonicalize_dtype(dtype).name\n+    truncated_dtype = canonicalize_dtype(np_dtype).name\n     warnings.warn(msg.format(dtype, fun_name, truncated_dtype), stacklevel=3)\n \n def safe_to_cast(input_dtype_or_value: Any,\n"
        },
        {
            "name": "dtypes_test.py",
            "path": "tests/dtypes_test.py",
            "patches": [
                {
                    "old_start": 558,
                    "old_length": 6,
                    "new_start": 558,
                    "new_length": 13,
                    "hunk": "@@ -558,6 +558,13 @@ class DtypesTest(jtu.JaxTestCase):\n     _, new_scale = jax.jit(jax.grad(outer, (0, 1)))(jnp.float32(3.14), scale)\n     self.assertAllClose(new_scale, jnp.float32(1.0))\n \n+  def test_check_dtype_non_hashable(self):\n+    # regression test for issue with checking non-hashable custom dtype\n+    class MyDtype:\n+      __hash__ = None\n+      dtype = np.dtype('float32')\n+    dtypes.check_user_dtype_supported(MyDtype())\n+\n class EArrayTest(jtu.JaxTestCase):\n \n   @parameterized.parameters([True, False])"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  def test_check_dtype_non_hashable(self):\n+    # regression test for issue with checking non-hashable custom dtype\n+    class MyDtype:\n+      __hash__ = None\n+      dtype = np.dtype('float32')\n+    dtypes.check_user_dtype_supported(MyDtype())\n+\n",
            "whole_hunk": "@@ -558,6 +558,13 @@ class DtypesTest(jtu.JaxTestCase):\n     _, new_scale = jax.jit(jax.grad(outer, (0, 1)))(jnp.float32(3.14), scale)\n     self.assertAllClose(new_scale, jnp.float32(1.0))\n \n+  def test_check_dtype_non_hashable(self):\n+    # regression test for issue with checking non-hashable custom dtype\n+    class MyDtype:\n+      __hash__ = None\n+      dtype = np.dtype('float32')\n+    dtypes.check_user_dtype_supported(MyDtype())\n+\n class EArrayTest(jtu.JaxTestCase):\n \n   @parameterized.parameters([True, False])"
        }
    ]
},
{
    "Id": 21,
    "commit_link": "https://github.com/google/jax/commit/97db0e758d4280204ba605226795bc8643e40517",
    "date": "2024-06-12T08:48:58+02:00",
    "message": "[pallas] Add support for cross-platform lowering\n\nWhen implementing this I have discovered that the\nmulti-platform lowering support does not handle the case when\nthe lowering rule for a platform invoke tracing (via `mlir.lower_fun`)\nand that tracing encounters a primitive that has lowering rules\nonly for a particular platform. To support this, I have added\nthe `LoweringRuleContext.platforms` to override\n`ModuleContext.platforms` with a potentially narrower set\nof lowering platforms. Added a test for this scenario.",
    "changes": [
        {
            "name": "mlir.py",
            "path": "jax/_src/interpreters/mlir.py",
            "patches": [
                {
                    "old_start": 585,
                    "old_length": 6,
                    "new_start": 585,
                    "new_length": 8,
                    "hunk": "@@ -585,6 +585,8 @@ class ModuleContext:\n   ip: ir.InsertionPoint\n   symbol_table: ir.SymbolTable\n   backend_or_name: str | xb.XlaBackend | None\n+  # The lowering platforms for the module. Can be more than one only when\n+  # exporting.\n   platforms: Sequence[str]\n   axis_context: AxisContext\n   keepalives: list[Any]\n"
                },
                {
                    "old_start": 689,
                    "old_length": 6,
                    "new_start": 691,
                    "new_length": 9,
                    "hunk": "@@ -689,6 +691,9 @@ class LoweringRuleContext:\n   # module_context.shape_poly_state.dim_vars\n   dim_var_values: Sequence[ir.Value] = ()\n   compute_type: str | None = None\n+  # Override module_context.platforms if not None. Used during multi-platform\n+  # lowering, when in a scope with a subset of the module_context.platforms.\n+  platforms: Sequence[str] | None = None\n \n   def set_tokens_out(self, tokens_out: TokenSet):\n     assert self.tokens_out is None, 'Should only set `tokens_out` once.'\n"
                },
                {
                    "old_start": 1662,
                    "old_length": 7,
                    "new_start": 1667,
                    "new_length": 7,
                    "hunk": "@@ -1662,7 +1667,7 @@ def lower_per_platform(ctx: LoweringRuleContext,\n    rule_args: the args of the lowering rules.\n    rule_kwargs: the kwargs of the lowering rules.\n   \"\"\"\n-  platforms: Sequence[str] = ctx.module_context.platforms\n+  platforms: Sequence[str] = ctx.platforms or ctx.module_context.platforms\n   # Special case the common case (single-platform lowering)\n   if len(platforms) == 1:\n     rule = platform_rules.get(platforms[0], default_rule)\n"
                },
                {
                    "old_start": 1723,
                    "old_length": 7,
                    "new_start": 1728,
                    "new_length": 10,
                    "hunk": "@@ -1723,7 +1728,10 @@ def lower_per_platform(ctx: LoweringRuleContext,\n                       index=rule_idx_op,\n                       num_branches=len(kept_rules))\n   for i, rule in enumerate(kept_rules):\n-    inner_ctx = ctx.replace()\n+    platforms_for_this_rule = [p\n+                               for p, rule_idx in platform_to_kept_rules_idx.items()\n+                               if rule_idx == i]\n+    inner_ctx = ctx.replace(platforms=platforms_for_this_rule)\n     branch = case_op.regions[i].blocks.append()\n     with ir.InsertionPoint(branch):\n       output = rule(inner_ctx, *rule_args, **rule_kwargs)\n"
                },
                {
                    "old_start": 1764,
                    "old_length": 7,
                    "new_start": 1772,
                    "new_length": 7,
                    "hunk": "@@ -1764,7 +1772,7 @@ def lower_fun(fun: Callable, multiple_results: bool = True) -> Callable:\n \n   The returned function does not use `avals_out`, so callers may pass any value\n   as `avals_out`.\"\"\"\n-  def f_lowered(ctx, *args, **params):\n+  def f_lowered(ctx: LoweringRuleContext, *args, **params):\n     f = fun if multiple_results else lambda *args, **kw: (fun(*args, **kw),)\n     wrapped_fun = lu.wrap_init(f, params)\n \n"
                },
                {
                    "old_start": 1774,
                    "old_length": 11,
                    "new_start": 1782,
                    "new_length": 12,
                    "hunk": "@@ -1774,11 +1782,12 @@ def lower_fun(fun: Callable, multiple_results: bool = True) -> Callable:\n       # case, we need to form a jaxpr with leading binders for those axis size\n       # arguments (by computing an InputType and using trace_to_jaxpr_dynamic2),\n       # and we need to call jaxpr_subcomp with these arguments made explicit.\n+      assert ctx.axis_size_env is not None\n       args = (*ctx.axis_size_env.values(), *args)\n       idx = {d: core.DBIdx(i) for i, d in enumerate(ctx.axis_size_env)}\n       i32_aval = core.ShapedArray((), np.dtype('int32'))\n       implicit_args = [(i32_aval, False)] * len(ctx.axis_size_env)\n-      explicit_args = [(a.update(shape=tuple(idx.get(d, d) for d in a.shape))\n+      explicit_args = [(a.update(shape=tuple(idx.get(d, d) for d in a.shape))  # type: ignore\n                         if type(a) is core.DShapedArray else a, True)\n                        for a in ctx.avals_in]\n       wrapped_fun = lu.annotate(wrapped_fun, (*implicit_args, *explicit_args))\n"
                },
                {
                    "old_start": 1787,
                    "old_length": 8,
                    "new_start": 1796,
                    "new_length": 12,
                    "hunk": "@@ -1787,8 +1796,12 @@ def lower_fun(fun: Callable, multiple_results: bool = True) -> Callable:\n       jaxpr, _, consts, () = pe.trace_to_jaxpr_dynamic(wrapped_fun, ctx.avals_in)\n       # TODO(frostig,mattjj): check ctx.avals_out against jaxpr avals out?\n \n+    if ctx.platforms is not None:\n+      sub_context = ctx.module_context.replace(platforms=ctx.platforms)\n+    else:\n+      sub_context = ctx.module_context\n     out, tokens = jaxpr_subcomp(\n-        ctx.module_context, jaxpr, ctx.name_stack, ctx.tokens_in,\n+        sub_context, jaxpr, ctx.name_stack, ctx.tokens_in,\n         _ir_consts(consts), *map(wrap_singleton_ir_values, args),\n         dim_var_values=ctx.dim_var_values)\n     ctx.set_tokens_out(tokens)\n"
                }
            ],
            "whole_deleted": "-  platforms: Sequence[str] = ctx.module_context.platforms\n-    inner_ctx = ctx.replace()\n-  def f_lowered(ctx, *args, **params):\n-      explicit_args = [(a.update(shape=tuple(idx.get(d, d) for d in a.shape))\n-        ctx.module_context, jaxpr, ctx.name_stack, ctx.tokens_in,\n",
            "whole_added": "+  # The lowering platforms for the module. Can be more than one only when\n+  # exporting.\n+  # Override module_context.platforms if not None. Used during multi-platform\n+  # lowering, when in a scope with a subset of the module_context.platforms.\n+  platforms: Sequence[str] | None = None\n+  platforms: Sequence[str] = ctx.platforms or ctx.module_context.platforms\n+    platforms_for_this_rule = [p\n+                               for p, rule_idx in platform_to_kept_rules_idx.items()\n+                               if rule_idx == i]\n+    inner_ctx = ctx.replace(platforms=platforms_for_this_rule)\n+  def f_lowered(ctx: LoweringRuleContext, *args, **params):\n+      assert ctx.axis_size_env is not None\n+      explicit_args = [(a.update(shape=tuple(idx.get(d, d) for d in a.shape))  # type: ignore\n+    if ctx.platforms is not None:\n+      sub_context = ctx.module_context.replace(platforms=ctx.platforms)\n+    else:\n+      sub_context = ctx.module_context\n+        sub_context, jaxpr, ctx.name_stack, ctx.tokens_in,\n",
            "whole_hunk": "@@ -585,6 +585,8 @@ class ModuleContext:\n   ip: ir.InsertionPoint\n   symbol_table: ir.SymbolTable\n   backend_or_name: str | xb.XlaBackend | None\n+  # The lowering platforms for the module. Can be more than one only when\n+  # exporting.\n   platforms: Sequence[str]\n   axis_context: AxisContext\n   keepalives: list[Any]\n@@ -689,6 +691,9 @@ class LoweringRuleContext:\n   # module_context.shape_poly_state.dim_vars\n   dim_var_values: Sequence[ir.Value] = ()\n   compute_type: str | None = None\n+  # Override module_context.platforms if not None. Used during multi-platform\n+  # lowering, when in a scope with a subset of the module_context.platforms.\n+  platforms: Sequence[str] | None = None\n \n   def set_tokens_out(self, tokens_out: TokenSet):\n     assert self.tokens_out is None, 'Should only set `tokens_out` once.'\n@@ -1662,7 +1667,7 @@ def lower_per_platform(ctx: LoweringRuleContext,\n    rule_args: the args of the lowering rules.\n    rule_kwargs: the kwargs of the lowering rules.\n   \"\"\"\n-  platforms: Sequence[str] = ctx.module_context.platforms\n+  platforms: Sequence[str] = ctx.platforms or ctx.module_context.platforms\n   # Special case the common case (single-platform lowering)\n   if len(platforms) == 1:\n     rule = platform_rules.get(platforms[0], default_rule)\n@@ -1723,7 +1728,10 @@ def lower_per_platform(ctx: LoweringRuleContext,\n                       index=rule_idx_op,\n                       num_branches=len(kept_rules))\n   for i, rule in enumerate(kept_rules):\n-    inner_ctx = ctx.replace()\n+    platforms_for_this_rule = [p\n+                               for p, rule_idx in platform_to_kept_rules_idx.items()\n+                               if rule_idx == i]\n+    inner_ctx = ctx.replace(platforms=platforms_for_this_rule)\n     branch = case_op.regions[i].blocks.append()\n     with ir.InsertionPoint(branch):\n       output = rule(inner_ctx, *rule_args, **rule_kwargs)\n@@ -1764,7 +1772,7 @@ def lower_fun(fun: Callable, multiple_results: bool = True) -> Callable:\n \n   The returned function does not use `avals_out`, so callers may pass any value\n   as `avals_out`.\"\"\"\n-  def f_lowered(ctx, *args, **params):\n+  def f_lowered(ctx: LoweringRuleContext, *args, **params):\n     f = fun if multiple_results else lambda *args, **kw: (fun(*args, **kw),)\n     wrapped_fun = lu.wrap_init(f, params)\n \n@@ -1774,11 +1782,12 @@ def lower_fun(fun: Callable, multiple_results: bool = True) -> Callable:\n       # case, we need to form a jaxpr with leading binders for those axis size\n       # arguments (by computing an InputType and using trace_to_jaxpr_dynamic2),\n       # and we need to call jaxpr_subcomp with these arguments made explicit.\n+      assert ctx.axis_size_env is not None\n       args = (*ctx.axis_size_env.values(), *args)\n       idx = {d: core.DBIdx(i) for i, d in enumerate(ctx.axis_size_env)}\n       i32_aval = core.ShapedArray((), np.dtype('int32'))\n       implicit_args = [(i32_aval, False)] * len(ctx.axis_size_env)\n-      explicit_args = [(a.update(shape=tuple(idx.get(d, d) for d in a.shape))\n+      explicit_args = [(a.update(shape=tuple(idx.get(d, d) for d in a.shape))  # type: ignore\n                         if type(a) is core.DShapedArray else a, True)\n                        for a in ctx.avals_in]\n       wrapped_fun = lu.annotate(wrapped_fun, (*implicit_args, *explicit_args))\n@@ -1787,8 +1796,12 @@ def lower_fun(fun: Callable, multiple_results: bool = True) -> Callable:\n       jaxpr, _, consts, () = pe.trace_to_jaxpr_dynamic(wrapped_fun, ctx.avals_in)\n       # TODO(frostig,mattjj): check ctx.avals_out against jaxpr avals out?\n \n+    if ctx.platforms is not None:\n+      sub_context = ctx.module_context.replace(platforms=ctx.platforms)\n+    else:\n+      sub_context = ctx.module_context\n     out, tokens = jaxpr_subcomp(\n-        ctx.module_context, jaxpr, ctx.name_stack, ctx.tokens_in,\n+        sub_context, jaxpr, ctx.name_stack, ctx.tokens_in,\n         _ir_consts(consts), *map(wrap_singleton_ir_values, args),\n         dim_var_values=ctx.dim_var_values)\n     ctx.set_tokens_out(tokens)\n"
        },
        {
            "name": "pallas_call_registration.py",
            "path": "jax/_src/pallas/mosaic/pallas_call_registration.py",
            "patches": [
                {
                    "old_start": 96,
                    "old_length": 7,
                    "new_start": 96,
                    "new_length": 7,
                    "hunk": "@@ -96,7 +96,7 @@ def pallas_call_tpu_lowering_rule(\n     return mosaic.as_tpu_kernel(\n         mosaic_module,\n         out_avals,\n-        backend=ctx.module_context.backend,\n+        backend=\"tpu\",\n         kernel_name=name,\n         cost_estimate=mosaic_params.get(\"cost_estimate\"),\n         vmem_limit_bytes=mosaic_params.get(\"vmem_limit_bytes\"),\n"
                }
            ],
            "whole_deleted": "-        backend=ctx.module_context.backend,\n",
            "whole_added": "+        backend=\"tpu\",\n",
            "whole_hunk": "@@ -96,7 +96,7 @@ def pallas_call_tpu_lowering_rule(\n     return mosaic.as_tpu_kernel(\n         mosaic_module,\n         out_avals,\n-        backend=ctx.module_context.backend,\n+        backend=\"tpu\",\n         kernel_name=name,\n         cost_estimate=mosaic_params.get(\"cost_estimate\"),\n         vmem_limit_bytes=mosaic_params.get(\"vmem_limit_bytes\"),\n"
        },
        {
            "name": "pallas_call.py",
            "path": "jax/_src/pallas/pallas_call.py",
            "patches": [
                {
                    "old_start": 720,
                    "old_length": 38,
                    "new_start": 720,
                    "new_length": 48,
                    "hunk": "@@ -720,38 +720,48 @@ def _pallas_call_lowering(\n     impl = partial(_pallas_call_impl, **params, interpret=True)\n     return mlir.lower_fun(impl, multiple_results=True)(ctx, *in_nodes)\n \n-  try:\n-    [platform] = ctx.module_context.platforms\n-  except ValueError:\n-    raise ValueError(\n-        \"Can only lower pallas_call on a single platform.\"\n-    ) from None\n-\n-  if platform == \"cpu\":\n+  def cpu_lowering(ctx: mlir.LoweringRuleContext,\n+                   *in_nodes: mlir.ir.Value | Sequence[mlir.ir.Value],\n+                   **params):\n     raise ValueError(\"Only interpret mode is supported on CPU backend.\")\n-  elif platform == \"cuda\" or platform == \"rocm\":\n+\n+  def tpu_lowering(ctx: mlir.LoweringRuleContext,\n+                   *in_nodes: mlir.ir.Value | Sequence[mlir.ir.Value],\n+                   **params):\n+    try:\n+      from jax._src.pallas.mosaic import pallas_call_registration\n+    except ImportError:\n+      raise _unsupported_lowering_error(\"tpu\")\n+    else:\n+      return pallas_call_registration.pallas_call_tpu_lowering_rule(\n+          ctx, *in_nodes, **params\n+      )\n+\n+  def gpu_lowering(ctx: mlir.LoweringRuleContext,\n+                   *in_nodes: mlir.ir.Value | Sequence[mlir.ir.Value],\n+                   **params):\n     try:\n       if _PALLAS_USE_MOSAIC_GPU.value:\n         from jax._src.pallas.mosaic_gpu import pallas_call_registration\n       else:\n         from jax._src.pallas.triton import pallas_call_registration  # type: ignore\n     except ImportError:\n-      pass\n+      raise _unsupported_lowering_error(\"gpu\")\n     else:\n       return pallas_call_registration.pallas_call_lowering(\n-          ctx, *in_nodes, interpret=interpret, **params\n-      )\n-  elif platform == \"tpu\":\n-    try:\n-      from jax._src.pallas.mosaic import pallas_call_registration  # type: ignore\n-    except ImportError:\n-      pass\n-    else:\n-      return pallas_call_registration.pallas_call_tpu_lowering_rule(\n-          ctx, *in_nodes, interpret=interpret, **params\n+          ctx, *in_nodes, **params\n       )\n \n-  raise _unsupported_lowering_error(platform)\n+  return mlir.lower_per_platform(ctx, \"pallas_call\",\n+                                 dict(cpu=cpu_lowering,\n+                                      tpu=tpu_lowering,\n+                                      cuda=gpu_lowering,\n+                                      rocm=gpu_lowering),\n+                                 None,  # default_rule\n+                                 effects.no_effects,\n+                                 *in_nodes,\n+                                 interpret=interpret,\n+                                 **params)\n \n \n mlir.register_lowering(pallas_call_p, _pallas_call_lowering)\n"
                }
            ],
            "whole_deleted": "-  try:\n-    [platform] = ctx.module_context.platforms\n-  except ValueError:\n-    raise ValueError(\n-        \"Can only lower pallas_call on a single platform.\"\n-    ) from None\n-\n-  if platform == \"cpu\":\n-  elif platform == \"cuda\" or platform == \"rocm\":\n-      pass\n-          ctx, *in_nodes, interpret=interpret, **params\n-      )\n-  elif platform == \"tpu\":\n-    try:\n-      from jax._src.pallas.mosaic import pallas_call_registration  # type: ignore\n-    except ImportError:\n-      pass\n-    else:\n-      return pallas_call_registration.pallas_call_tpu_lowering_rule(\n-          ctx, *in_nodes, interpret=interpret, **params\n-  raise _unsupported_lowering_error(platform)\n",
            "whole_added": "+  def cpu_lowering(ctx: mlir.LoweringRuleContext,\n+                   *in_nodes: mlir.ir.Value | Sequence[mlir.ir.Value],\n+                   **params):\n+\n+  def tpu_lowering(ctx: mlir.LoweringRuleContext,\n+                   *in_nodes: mlir.ir.Value | Sequence[mlir.ir.Value],\n+                   **params):\n+    try:\n+      from jax._src.pallas.mosaic import pallas_call_registration\n+    except ImportError:\n+      raise _unsupported_lowering_error(\"tpu\")\n+    else:\n+      return pallas_call_registration.pallas_call_tpu_lowering_rule(\n+          ctx, *in_nodes, **params\n+      )\n+\n+  def gpu_lowering(ctx: mlir.LoweringRuleContext,\n+                   *in_nodes: mlir.ir.Value | Sequence[mlir.ir.Value],\n+                   **params):\n+      raise _unsupported_lowering_error(\"gpu\")\n+          ctx, *in_nodes, **params\n+  return mlir.lower_per_platform(ctx, \"pallas_call\",\n+                                 dict(cpu=cpu_lowering,\n+                                      tpu=tpu_lowering,\n+                                      cuda=gpu_lowering,\n+                                      rocm=gpu_lowering),\n+                                 None,  # default_rule\n+                                 effects.no_effects,\n+                                 *in_nodes,\n+                                 interpret=interpret,\n+                                 **params)\n",
            "whole_hunk": "@@ -720,38 +720,48 @@ def _pallas_call_lowering(\n     impl = partial(_pallas_call_impl, **params, interpret=True)\n     return mlir.lower_fun(impl, multiple_results=True)(ctx, *in_nodes)\n \n-  try:\n-    [platform] = ctx.module_context.platforms\n-  except ValueError:\n-    raise ValueError(\n-        \"Can only lower pallas_call on a single platform.\"\n-    ) from None\n-\n-  if platform == \"cpu\":\n+  def cpu_lowering(ctx: mlir.LoweringRuleContext,\n+                   *in_nodes: mlir.ir.Value | Sequence[mlir.ir.Value],\n+                   **params):\n     raise ValueError(\"Only interpret mode is supported on CPU backend.\")\n-  elif platform == \"cuda\" or platform == \"rocm\":\n+\n+  def tpu_lowering(ctx: mlir.LoweringRuleContext,\n+                   *in_nodes: mlir.ir.Value | Sequence[mlir.ir.Value],\n+                   **params):\n+    try:\n+      from jax._src.pallas.mosaic import pallas_call_registration\n+    except ImportError:\n+      raise _unsupported_lowering_error(\"tpu\")\n+    else:\n+      return pallas_call_registration.pallas_call_tpu_lowering_rule(\n+          ctx, *in_nodes, **params\n+      )\n+\n+  def gpu_lowering(ctx: mlir.LoweringRuleContext,\n+                   *in_nodes: mlir.ir.Value | Sequence[mlir.ir.Value],\n+                   **params):\n     try:\n       if _PALLAS_USE_MOSAIC_GPU.value:\n         from jax._src.pallas.mosaic_gpu import pallas_call_registration\n       else:\n         from jax._src.pallas.triton import pallas_call_registration  # type: ignore\n     except ImportError:\n-      pass\n+      raise _unsupported_lowering_error(\"gpu\")\n     else:\n       return pallas_call_registration.pallas_call_lowering(\n-          ctx, *in_nodes, interpret=interpret, **params\n-      )\n-  elif platform == \"tpu\":\n-    try:\n-      from jax._src.pallas.mosaic import pallas_call_registration  # type: ignore\n-    except ImportError:\n-      pass\n-    else:\n-      return pallas_call_registration.pallas_call_tpu_lowering_rule(\n-          ctx, *in_nodes, interpret=interpret, **params\n+          ctx, *in_nodes, **params\n       )\n \n-  raise _unsupported_lowering_error(platform)\n+  return mlir.lower_per_platform(ctx, \"pallas_call\",\n+                                 dict(cpu=cpu_lowering,\n+                                      tpu=tpu_lowering,\n+                                      cuda=gpu_lowering,\n+                                      rocm=gpu_lowering),\n+                                 None,  # default_rule\n+                                 effects.no_effects,\n+                                 *in_nodes,\n+                                 interpret=interpret,\n+                                 **params)\n \n \n mlir.register_lowering(pallas_call_p, _pallas_call_lowering)\n"
        },
        {
            "name": "pallas_call_registration.py",
            "path": "jax/_src/pallas/triton/pallas_call_registration.py",
            "patches": [
                {
                    "old_start": 76,
                    "old_length": 9,
                    "new_start": 76,
                    "new_length": 8,
                    "hunk": "@@ -76,9 +76,8 @@ def pallas_call_lowering(\n     )\n   triton_params = compiler_params.get(\"triton\", compiler_params)\n   num_warps = triton_params.pop(\"num_warps\", 4)\n-  if len(ctx.module_context.platforms) > 1:\n-    raise NotImplementedError(\"multi-platform lowering for Pallas kernels\")\n-  if ctx.module_context.platforms[0] == \"rocm\":\n+  [lowering_platform] = ctx.platforms or ctx.module_context.platforms\n+  if lowering_platform == \"rocm\":\n     num_stages = triton_params.pop(\"num_stages\", 1)\n   else:\n     num_stages = triton_params.pop(\"num_stages\", 3)\n"
                }
            ],
            "whole_deleted": "-  if len(ctx.module_context.platforms) > 1:\n-    raise NotImplementedError(\"multi-platform lowering for Pallas kernels\")\n-  if ctx.module_context.platforms[0] == \"rocm\":\n",
            "whole_added": "+  [lowering_platform] = ctx.platforms or ctx.module_context.platforms\n+  if lowering_platform == \"rocm\":\n",
            "whole_hunk": "@@ -76,9 +76,8 @@ def pallas_call_lowering(\n     )\n   triton_params = compiler_params.get(\"triton\", compiler_params)\n   num_warps = triton_params.pop(\"num_warps\", 4)\n-  if len(ctx.module_context.platforms) > 1:\n-    raise NotImplementedError(\"multi-platform lowering for Pallas kernels\")\n-  if ctx.module_context.platforms[0] == \"rocm\":\n+  [lowering_platform] = ctx.platforms or ctx.module_context.platforms\n+  if lowering_platform == \"rocm\":\n     num_stages = triton_params.pop(\"num_stages\", 1)\n   else:\n     num_stages = triton_params.pop(\"num_stages\", 3)\n"
        },
        {
            "name": "export_test.py",
            "path": "tests/export_test.py",
            "patches": [
                {
                    "old_start": 13,
                    "old_length": 6,
                    "new_start": 13,
                    "new_length": 7,
                    "hunk": "@@ -13,6 +13,7 @@\n # limitations under the License.\n from __future__ import annotations\n \n+from collections.abc import Sequence\n import contextlib\n import dataclasses\n import functools\n"
                },
                {
                    "old_start": 1369,
                    "old_length": 6,
                    "new_start": 1370,
                    "new_length": 64,
                    "hunk": "@@ -1369,6 +1370,64 @@ class JaxExportTest(jtu.JaxTestCase):\n     res2 = exp2.call(x)\n     self.assertAllClose(res2, _testing_multi_platform_fun_expected(x))\n \n+  def test_multi_platform_mlir_lower_fun_with_platform_specific_primitives(self):\n+    # A primitive with multiple lowering rules, which themselves involve\n+    # tracing primitives with per-platform rules, using mlir.lower_fun.\n+    # This situation arises for Pallas lowering.\n+    def times_n_lowering(n: int, ctx: mlir.LoweringRuleContext,\n+                         x: mlir.ir.Value) -> Sequence[mlir.ir.Value]:\n+      # Lowering n * x\n+      res = x\n+      for i in range(n - 1):\n+        res = mlir.hlo.AddOp(res, x)\n+      return res.results\n+\n+    times_2 = core.Primitive(\"__testing_times_2\")  # x2 for cpu\n+    times_2.def_abstract_eval(lambda x: x)\n+    # Define lowering rules only for the relevant platforms, ensure there\n+    # is no error about missing lowering rules\n+    mlir.register_lowering(times_2, functools.partial(times_n_lowering, 2),\n+                           \"cpu\")\n+\n+    times_3 = core.Primitive(\"__testing_times_3\")  # x3 for cuda\n+    times_3.def_abstract_eval(lambda x: x)\n+    mlir.register_lowering(times_3, functools.partial(times_n_lowering, 3),\n+                           \"cuda\")\n+\n+    times_4 = core.Primitive(\"__testing_times_4\")  # x4 for tpu\n+    times_4.def_abstract_eval(lambda x: x)\n+    mlir.register_lowering(times_4, functools.partial(times_n_lowering, 4),\n+                           \"tpu\")\n+\n+    times_2_or_3 = core.Primitive(\"__testing_times_2_or_3\")  # x2 for cpu, x3 for cuda\n+    times_2_or_3.def_abstract_eval(lambda x: x)\n+    mlir.register_lowering(times_2_or_3,\n+                           mlir.lower_fun(times_2.bind,\n+                                          multiple_results=False), \"cpu\")\n+    mlir.register_lowering(times_2_or_3,\n+                           mlir.lower_fun(times_3.bind,\n+                                          multiple_results=False), \"cuda\")\n+\n+    times_2_or_3_or_4 = core.Primitive(\"__testing_times_2_or_3_or_4\")  # x2 for cpu, x3 for cuda, x4 for tpu\n+    times_2_or_3_or_4.def_abstract_eval(lambda x: x)\n+    times_2_or_3_or_4_lowering_cpu_cuda = mlir.lower_fun(times_2_or_3.bind,\n+                                                         multiple_results=False)\n+    for platform in [\"cpu\", \"cuda\"]:\n+      mlir.register_lowering(times_2_or_3_or_4,\n+                             times_2_or_3_or_4_lowering_cpu_cuda,\n+                             platform)\n+    mlir.register_lowering(times_2_or_3_or_4, mlir.lower_fun(times_4.bind,\n+                                                             multiple_results=False),\n+                           \"tpu\")\n+\n+    @jax.jit\n+    def f(x):\n+      return times_2_or_3_or_4.bind(x)\n+    x = np.float32(42.)\n+    exp = export.export(f, lowering_platforms=[\"cpu\", \"cuda\", \"tpu\"])(x)\n+    expected = x * np.float32(dict(cpu=2, gpu=3, tpu=4)[jtu.device_under_test()])\n+    self.assertAllClose(exp.call(x), expected)\n+\n   def test_multi_platform_and_poly(self):\n     if jtu.test_device_matches([\"gpu\"]):\n       # The export is not applicable to GPU\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+from collections.abc import Sequence\n+  def test_multi_platform_mlir_lower_fun_with_platform_specific_primitives(self):\n+    # A primitive with multiple lowering rules, which themselves involve\n+    # tracing primitives with per-platform rules, using mlir.lower_fun.\n+    # This situation arises for Pallas lowering.\n+    def times_n_lowering(n: int, ctx: mlir.LoweringRuleContext,\n+                         x: mlir.ir.Value) -> Sequence[mlir.ir.Value]:\n+      # Lowering n * x\n+      res = x\n+      for i in range(n - 1):\n+        res = mlir.hlo.AddOp(res, x)\n+      return res.results\n+\n+    times_2 = core.Primitive(\"__testing_times_2\")  # x2 for cpu\n+    times_2.def_abstract_eval(lambda x: x)\n+    # Define lowering rules only for the relevant platforms, ensure there\n+    # is no error about missing lowering rules\n+    mlir.register_lowering(times_2, functools.partial(times_n_lowering, 2),\n+                           \"cpu\")\n+\n+    times_3 = core.Primitive(\"__testing_times_3\")  # x3 for cuda\n+    times_3.def_abstract_eval(lambda x: x)\n+    mlir.register_lowering(times_3, functools.partial(times_n_lowering, 3),\n+                           \"cuda\")\n+\n+    times_4 = core.Primitive(\"__testing_times_4\")  # x4 for tpu\n+    times_4.def_abstract_eval(lambda x: x)\n+    mlir.register_lowering(times_4, functools.partial(times_n_lowering, 4),\n+                           \"tpu\")\n+\n+    times_2_or_3 = core.Primitive(\"__testing_times_2_or_3\")  # x2 for cpu, x3 for cuda\n+    times_2_or_3.def_abstract_eval(lambda x: x)\n+    mlir.register_lowering(times_2_or_3,\n+                           mlir.lower_fun(times_2.bind,\n+                                          multiple_results=False), \"cpu\")\n+    mlir.register_lowering(times_2_or_3,\n+                           mlir.lower_fun(times_3.bind,\n+                                          multiple_results=False), \"cuda\")\n+\n+    times_2_or_3_or_4 = core.Primitive(\"__testing_times_2_or_3_or_4\")  # x2 for cpu, x3 for cuda, x4 for tpu\n+    times_2_or_3_or_4.def_abstract_eval(lambda x: x)\n+    times_2_or_3_or_4_lowering_cpu_cuda = mlir.lower_fun(times_2_or_3.bind,\n+                                                         multiple_results=False)\n+    for platform in [\"cpu\", \"cuda\"]:\n+      mlir.register_lowering(times_2_or_3_or_4,\n+                             times_2_or_3_or_4_lowering_cpu_cuda,\n+                             platform)\n+    mlir.register_lowering(times_2_or_3_or_4, mlir.lower_fun(times_4.bind,\n+                                                             multiple_results=False),\n+                           \"tpu\")\n+\n+    @jax.jit\n+    def f(x):\n+      return times_2_or_3_or_4.bind(x)\n+    x = np.float32(42.)\n+    exp = export.export(f, lowering_platforms=[\"cpu\", \"cuda\", \"tpu\"])(x)\n+    expected = x * np.float32(dict(cpu=2, gpu=3, tpu=4)[jtu.device_under_test()])\n+    self.assertAllClose(exp.call(x), expected)\n+\n",
            "whole_hunk": "@@ -13,6 +13,7 @@\n # limitations under the License.\n from __future__ import annotations\n \n+from collections.abc import Sequence\n import contextlib\n import dataclasses\n import functools\n@@ -1369,6 +1370,64 @@ class JaxExportTest(jtu.JaxTestCase):\n     res2 = exp2.call(x)\n     self.assertAllClose(res2, _testing_multi_platform_fun_expected(x))\n \n+  def test_multi_platform_mlir_lower_fun_with_platform_specific_primitives(self):\n+    # A primitive with multiple lowering rules, which themselves involve\n+    # tracing primitives with per-platform rules, using mlir.lower_fun.\n+    # This situation arises for Pallas lowering.\n+    def times_n_lowering(n: int, ctx: mlir.LoweringRuleContext,\n+                         x: mlir.ir.Value) -> Sequence[mlir.ir.Value]:\n+      # Lowering n * x\n+      res = x\n+      for i in range(n - 1):\n+        res = mlir.hlo.AddOp(res, x)\n+      return res.results\n+\n+    times_2 = core.Primitive(\"__testing_times_2\")  # x2 for cpu\n+    times_2.def_abstract_eval(lambda x: x)\n+    # Define lowering rules only for the relevant platforms, ensure there\n+    # is no error about missing lowering rules\n+    mlir.register_lowering(times_2, functools.partial(times_n_lowering, 2),\n+                           \"cpu\")\n+\n+    times_3 = core.Primitive(\"__testing_times_3\")  # x3 for cuda\n+    times_3.def_abstract_eval(lambda x: x)\n+    mlir.register_lowering(times_3, functools.partial(times_n_lowering, 3),\n+                           \"cuda\")\n+\n+    times_4 = core.Primitive(\"__testing_times_4\")  # x4 for tpu\n+    times_4.def_abstract_eval(lambda x: x)\n+    mlir.register_lowering(times_4, functools.partial(times_n_lowering, 4),\n+                           \"tpu\")\n+\n+    times_2_or_3 = core.Primitive(\"__testing_times_2_or_3\")  # x2 for cpu, x3 for cuda\n+    times_2_or_3.def_abstract_eval(lambda x: x)\n+    mlir.register_lowering(times_2_or_3,\n+                           mlir.lower_fun(times_2.bind,\n+                                          multiple_results=False), \"cpu\")\n+    mlir.register_lowering(times_2_or_3,\n+                           mlir.lower_fun(times_3.bind,\n+                                          multiple_results=False), \"cuda\")\n+\n+    times_2_or_3_or_4 = core.Primitive(\"__testing_times_2_or_3_or_4\")  # x2 for cpu, x3 for cuda, x4 for tpu\n+    times_2_or_3_or_4.def_abstract_eval(lambda x: x)\n+    times_2_or_3_or_4_lowering_cpu_cuda = mlir.lower_fun(times_2_or_3.bind,\n+                                                         multiple_results=False)\n+    for platform in [\"cpu\", \"cuda\"]:\n+      mlir.register_lowering(times_2_or_3_or_4,\n+                             times_2_or_3_or_4_lowering_cpu_cuda,\n+                             platform)\n+    mlir.register_lowering(times_2_or_3_or_4, mlir.lower_fun(times_4.bind,\n+                                                             multiple_results=False),\n+                           \"tpu\")\n+\n+    @jax.jit\n+    def f(x):\n+      return times_2_or_3_or_4.bind(x)\n+    x = np.float32(42.)\n+    exp = export.export(f, lowering_platforms=[\"cpu\", \"cuda\", \"tpu\"])(x)\n+    expected = x * np.float32(dict(cpu=2, gpu=3, tpu=4)[jtu.device_under_test()])\n+    self.assertAllClose(exp.call(x), expected)\n+\n   def test_multi_platform_and_poly(self):\n     if jtu.test_device_matches([\"gpu\"]):\n       # The export is not applicable to GPU\n"
        },
        {
            "name": "export_pallas_test.py",
            "path": "tests/pallas/export_pallas_test.py",
            "patches": [
                {
                    "old_start": 43,
                    "old_length": 11,
                    "new_start": 43,
                    "new_length": 12,
                    "hunk": "@@ -43,11 +43,12 @@ class ExportTest(jtu.JaxTestCase):\n     a = np.arange(8)\n     exp = export.export(\n         add_vectors,\n-        # TODO(necula): Make this test work on GPU also\n-        lowering_platforms=[\"tpu\"],\n+        lowering_platforms=[\"tpu\", \"cuda\"],\n     )(a, a)\n \n-    if jtu.device_under_test() == \"tpu\":\n+    if (jtu.device_under_test() == \"tpu\" or\n+        (jtu.device_under_test() == \"gpu\" and\n+         jtu.is_cuda_compute_capability_at_least(\"8.0\"))):\n       res = export.call(exp)(a, a)\n       self.assertAllClose(res, a + a)\n "
                }
            ],
            "whole_deleted": "-        # TODO(necula): Make this test work on GPU also\n-        lowering_platforms=[\"tpu\"],\n-    if jtu.device_under_test() == \"tpu\":\n",
            "whole_added": "+        lowering_platforms=[\"tpu\", \"cuda\"],\n+    if (jtu.device_under_test() == \"tpu\" or\n+        (jtu.device_under_test() == \"gpu\" and\n+         jtu.is_cuda_compute_capability_at_least(\"8.0\"))):\n",
            "whole_hunk": "@@ -43,11 +43,12 @@ class ExportTest(jtu.JaxTestCase):\n     a = np.arange(8)\n     exp = export.export(\n         add_vectors,\n-        # TODO(necula): Make this test work on GPU also\n-        lowering_platforms=[\"tpu\"],\n+        lowering_platforms=[\"tpu\", \"cuda\"],\n     )(a, a)\n \n-    if jtu.device_under_test() == \"tpu\":\n+    if (jtu.device_under_test() == \"tpu\" or\n+        (jtu.device_under_test() == \"gpu\" and\n+         jtu.is_cuda_compute_capability_at_least(\"8.0\"))):\n       res = export.call(exp)(a, a)\n       self.assertAllClose(res, a + a)\n "
        }
    ]
},
{
    "Id": 22,
    "commit_link": "https://github.com/google/jax/commit/70f6ab3128a5d7ef26c924e4c2c51d0301bd58ea",
    "date": "2024-06-11T12:22:00+01:00",
    "message": "Updated the type annotations of *_spec= parameters of pl.pallas_call\n\nThe previous type did not work for nested pytrees and for some reason neither\npytype nor mypy flagged that.\n\nI also re-enabled type checking for most pallas/*.py files.",
    "changes": [
        {
            "name": "core.py",
            "path": "jax/_src/pallas/core.py",
            "patches": [
                {
                    "old_start": 15,
                    "old_length": 13,
                    "new_start": 15,
                    "new_length": 12,
                    "hunk": "@@ -15,13 +15,12 @@\n \"\"\"Module for pallas-core functionality.\"\"\"\n from __future__ import annotations\n \n+from collections.abc import Iterator, Sequence\n import copy\n-from collections.abc import Sequence\n import contextlib\n import dataclasses\n import functools\n from typing import Any, Callable, Union\n-from collections.abc import Iterator\n \n from jax._src import api_util\n from jax._src import core as jax_core\n"
                },
                {
                    "old_start": 33,
                    "old_length": 9,
                    "new_start": 32,
                    "new_length": 6,
                    "hunk": "@@ -33,9 +32,6 @@ from jax._src.interpreters import partial_eval as pe\n from jax._src.state import discharge as state_discharge\n import jax.numpy as jnp\n \n-# TODO(sharadmv): enable type checking\n-# mypy: ignore-errors\n-\n partial = functools.partial\n Grid = tuple[Union[int, jax_core.Array, None], ...]  # None indicates that the bound is dynamic.\n DynamicGrid = tuple[Union[int, jax_core.Array], ...]\n"
                },
                {
                    "old_start": 156,
                    "old_length": 6,
                    "new_start": 152,
                    "new_length": 10,
                    "hunk": "@@ -156,6 +152,10 @@ class BlockSpec:\n     return out\n \n \n+# A PyTree of BlockSpec | NoBlockSpec.\n+BlockSpecTree = Any\n+\n+\n @dataclasses.dataclass(frozen=True)\n class BlockMapping:\n   block_shape: tuple[Mapped | int, ...]\n"
                },
                {
                    "old_start": 201,
                    "old_length": 7,
                    "new_start": 201,
                    "new_length": 7,
                    "hunk": "@@ -201,7 +201,7 @@ class GridMapping:\n   def static_grid(self) -> StaticGrid:\n     if self.num_dynamic_grid_bounds:\n       raise ValueError(\"Expected a grid with fully static bounds\")\n-    return self.grid  # typing: ignore\n+    return self.grid  # type: ignore\n \n \n def _preprocess_grid(grid: Grid | int | None) -> Grid:\n"
                },
                {
                    "old_start": 213,
                    "old_length": 9,
                    "new_start": 213,
                    "new_length": 9,
                    "hunk": "@@ -213,9 +213,9 @@ def _preprocess_grid(grid: Grid | int | None) -> Grid:\n \n \n def _convert_block_spec_to_block_mapping(\n-    in_avals: list[jax_core.ShapedArray], block_spec: BlockSpec | None,\n+    in_avals: Sequence[jax_core.ShapedArray], block_spec: BlockSpec,\n     aval: jax_core.ShapedArray, in_tree: Any,\n-    ) -> BlockSpec | None:\n+) -> BlockMapping | None:\n   if block_spec is no_block_spec:\n     return None\n   if block_spec.index_map is None:\n"
                },
                {
                    "old_start": 283,
                    "old_length": 12,
                    "new_start": 283,
                    "new_length": 8,
                    "hunk": "@@ -283,12 +283,8 @@ class GridSpec:\n   def __init__(\n       self,\n       grid: Grid | None = None,\n-      in_specs: BlockSpec\n-      | Sequence[BlockSpec | NoBlockSpec]\n-      | NoBlockSpec = no_block_spec,\n-      out_specs: BlockSpec\n-      | Sequence[BlockSpec | NoBlockSpec]\n-      | NoBlockSpec = no_block_spec,\n+      in_specs: BlockSpecTree = no_block_spec,\n+      out_specs: BlockSpecTree = no_block_spec,\n   ):\n     # Be more lenient for in/out_specs\n     if isinstance(in_specs, list):\n"
                }
            ],
            "whole_deleted": "-from collections.abc import Sequence\n-from collections.abc import Iterator\n-# TODO(sharadmv): enable type checking\n-# mypy: ignore-errors\n-\n-    return self.grid  # typing: ignore\n-    in_avals: list[jax_core.ShapedArray], block_spec: BlockSpec | None,\n-    ) -> BlockSpec | None:\n-      in_specs: BlockSpec\n-      | Sequence[BlockSpec | NoBlockSpec]\n-      | NoBlockSpec = no_block_spec,\n-      out_specs: BlockSpec\n-      | Sequence[BlockSpec | NoBlockSpec]\n-      | NoBlockSpec = no_block_spec,\n",
            "whole_added": "+from collections.abc import Iterator, Sequence\n+# A PyTree of BlockSpec | NoBlockSpec.\n+BlockSpecTree = Any\n+\n+\n+    return self.grid  # type: ignore\n+    in_avals: Sequence[jax_core.ShapedArray], block_spec: BlockSpec,\n+) -> BlockMapping | None:\n+      in_specs: BlockSpecTree = no_block_spec,\n+      out_specs: BlockSpecTree = no_block_spec,\n",
            "whole_hunk": "@@ -15,13 +15,12 @@\n \"\"\"Module for pallas-core functionality.\"\"\"\n from __future__ import annotations\n \n+from collections.abc import Iterator, Sequence\n import copy\n-from collections.abc import Sequence\n import contextlib\n import dataclasses\n import functools\n from typing import Any, Callable, Union\n-from collections.abc import Iterator\n \n from jax._src import api_util\n from jax._src import core as jax_core\n@@ -33,9 +32,6 @@ from jax._src.interpreters import partial_eval as pe\n from jax._src.state import discharge as state_discharge\n import jax.numpy as jnp\n \n-# TODO(sharadmv): enable type checking\n-# mypy: ignore-errors\n-\n partial = functools.partial\n Grid = tuple[Union[int, jax_core.Array, None], ...]  # None indicates that the bound is dynamic.\n DynamicGrid = tuple[Union[int, jax_core.Array], ...]\n@@ -156,6 +152,10 @@ class BlockSpec:\n     return out\n \n \n+# A PyTree of BlockSpec | NoBlockSpec.\n+BlockSpecTree = Any\n+\n+\n @dataclasses.dataclass(frozen=True)\n class BlockMapping:\n   block_shape: tuple[Mapped | int, ...]\n@@ -201,7 +201,7 @@ class GridMapping:\n   def static_grid(self) -> StaticGrid:\n     if self.num_dynamic_grid_bounds:\n       raise ValueError(\"Expected a grid with fully static bounds\")\n-    return self.grid  # typing: ignore\n+    return self.grid  # type: ignore\n \n \n def _preprocess_grid(grid: Grid | int | None) -> Grid:\n@@ -213,9 +213,9 @@ def _preprocess_grid(grid: Grid | int | None) -> Grid:\n \n \n def _convert_block_spec_to_block_mapping(\n-    in_avals: list[jax_core.ShapedArray], block_spec: BlockSpec | None,\n+    in_avals: Sequence[jax_core.ShapedArray], block_spec: BlockSpec,\n     aval: jax_core.ShapedArray, in_tree: Any,\n-    ) -> BlockSpec | None:\n+) -> BlockMapping | None:\n   if block_spec is no_block_spec:\n     return None\n   if block_spec.index_map is None:\n@@ -283,12 +283,8 @@ class GridSpec:\n   def __init__(\n       self,\n       grid: Grid | None = None,\n-      in_specs: BlockSpec\n-      | Sequence[BlockSpec | NoBlockSpec]\n-      | NoBlockSpec = no_block_spec,\n-      out_specs: BlockSpec\n-      | Sequence[BlockSpec | NoBlockSpec]\n-      | NoBlockSpec = no_block_spec,\n+      in_specs: BlockSpecTree = no_block_spec,\n+      out_specs: BlockSpecTree = no_block_spec,\n   ):\n     # Be more lenient for in/out_specs\n     if isinstance(in_specs, list):\n"
        },
        {
            "name": "core.py",
            "path": "jax/_src/pallas/mosaic/core.py",
            "patches": [
                {
                    "old_start": 19,
                    "old_length": 7,
                    "new_start": 19,
                    "new_length": 7,
                    "hunk": "@@ -19,7 +19,7 @@ from collections.abc import Sequence\n import dataclasses\n import enum\n import functools\n-from typing import Any, Union\n+from typing import Any\n \n from jax._src import core as jax_core\n from jax._src import dtypes\n"
                },
                {
                    "old_start": 28,
                    "old_length": 15,
                    "new_start": 28,
                    "new_length": 13,
                    "hunk": "@@ -28,15 +28,13 @@ from jax._src import util\n import jax.numpy as jnp\n from jax._src.pallas import core as pallas_core\n \n-# TODO(sharadmv): enable type checking\n-# mypy: ignore-errors\n-\n map, unsafe_map = util.safe_map, map\n zip, unsafe_zip = util.safe_zip, zip\n \n partial = functools.partial\n Grid = pallas_core.Grid\n BlockSpec = pallas_core.BlockSpec\n+BlockSpecTree = pallas_core.BlockSpecTree\n GridMapping = pallas_core.GridMapping\n NoBlockSpec = pallas_core.NoBlockSpec\n AbstractMemoryRef = pallas_core.AbstractMemoryRef\n"
                },
                {
                    "old_start": 97,
                    "old_length": 6,
                    "new_start": 95,
                    "new_length": 7,
                    "hunk": "@@ -97,6 +95,7 @@ class SemaphoreType(enum.Enum):\n   BARRIER = \"barrier\"\n \n   def __call__(self, shape: tuple[int, ...]):\n+    dtype: Any\n     if self == SemaphoreType.DMA:\n       dtype = DmaSemaphoreTy()\n     elif self == SemaphoreType.BARRIER:\n"
                },
                {
                    "old_start": 143,
                    "old_length": 9,
                    "new_start": 142,
                    "new_length": 6,
                    "hunk": "@@ -143,9 +142,6 @@ def _make_aval(obj: object) -> jax_core.AbstractValue:\n                    \"Only VMEM and SemaphoreType are supported.\")\n \n \n-BlockSpecTree = Union[BlockSpec, NoBlockSpec, Sequence[\"BlockSpecTree\"]]\n-\n-\n @dataclasses.dataclass(init=False, unsafe_hash=True)\n class PrefetchScalarGridSpec(pallas_core.GridSpec):\n   grid: Grid\n"
                }
            ],
            "whole_deleted": "-from typing import Any, Union\n-# TODO(sharadmv): enable type checking\n-# mypy: ignore-errors\n-\n-BlockSpecTree = Union[BlockSpec, NoBlockSpec, Sequence[\"BlockSpecTree\"]]\n-\n-\n",
            "whole_added": "+from typing import Any\n+BlockSpecTree = pallas_core.BlockSpecTree\n+    dtype: Any\n",
            "whole_hunk": "@@ -19,7 +19,7 @@ from collections.abc import Sequence\n import dataclasses\n import enum\n import functools\n-from typing import Any, Union\n+from typing import Any\n \n from jax._src import core as jax_core\n from jax._src import dtypes\n@@ -28,15 +28,13 @@ from jax._src import util\n import jax.numpy as jnp\n from jax._src.pallas import core as pallas_core\n \n-# TODO(sharadmv): enable type checking\n-# mypy: ignore-errors\n-\n map, unsafe_map = util.safe_map, map\n zip, unsafe_zip = util.safe_zip, zip\n \n partial = functools.partial\n Grid = pallas_core.Grid\n BlockSpec = pallas_core.BlockSpec\n+BlockSpecTree = pallas_core.BlockSpecTree\n GridMapping = pallas_core.GridMapping\n NoBlockSpec = pallas_core.NoBlockSpec\n AbstractMemoryRef = pallas_core.AbstractMemoryRef\n@@ -97,6 +95,7 @@ class SemaphoreType(enum.Enum):\n   BARRIER = \"barrier\"\n \n   def __call__(self, shape: tuple[int, ...]):\n+    dtype: Any\n     if self == SemaphoreType.DMA:\n       dtype = DmaSemaphoreTy()\n     elif self == SemaphoreType.BARRIER:\n@@ -143,9 +142,6 @@ def _make_aval(obj: object) -> jax_core.AbstractValue:\n                    \"Only VMEM and SemaphoreType are supported.\")\n \n \n-BlockSpecTree = Union[BlockSpec, NoBlockSpec, Sequence[\"BlockSpecTree\"]]\n-\n-\n @dataclasses.dataclass(init=False, unsafe_hash=True)\n class PrefetchScalarGridSpec(pallas_core.GridSpec):\n   grid: Grid\n"
        },
        {
            "name": "pallas_call.py",
            "path": "jax/_src/pallas/pallas_call.py",
            "patches": [
                {
                    "old_start": 55,
                    "old_length": 10,
                    "new_start": 55,
                    "new_length": 11,
                    "hunk": "@@ -55,10 +55,11 @@ map, unsafe_map = safe_map, map\n zip, unsafe_zip = safe_zip, zip\n \n Grid = pallas_core.Grid\n-BlockSpec = pallas_core.BlockSpec\n GridSpec = pallas_core.GridSpec\n BlockMapping = pallas_core.BlockMapping\n GridMapping = pallas_core.GridMapping\n+BlockSpec = pallas_core.BlockSpec\n+BlockSpecTree = pallas_core.BlockSpecTree\n NoBlockSpec = pallas_core.NoBlockSpec\n no_block_spec = pallas_core.no_block_spec\n \n"
                },
                {
                    "old_start": 763,
                    "old_length": 14,
                    "new_start": 764,
                    "new_length": 13,
                    "hunk": "@@ -763,14 +764,13 @@ def pallas_call(\n     grid_spec: GridSpec | None = None,\n     debug: bool = False,\n     grid: Grid | None = None,\n-    in_specs: Sequence[BlockSpec | NoBlockSpec] | NoBlockSpec = no_block_spec,\n-    out_specs: BlockSpec | NoBlockSpec\n-    | Sequence[BlockSpec | NoBlockSpec] = no_block_spec,\n+    in_specs: BlockSpecTree = no_block_spec,\n+    out_specs: BlockSpecTree = no_block_spec,\n     input_output_aliases: dict[int, int] = {},\n     interpret: bool = False,\n     name: str | None = None,\n     compiler_params: dict[str, Any] | None = None,\n-):\n+) -> Callable[..., Any]:\n   name = _extract_function_name(f, name)\n   if compiler_params is None:\n     compiler_params = {}\n"
                }
            ],
            "whole_deleted": "-BlockSpec = pallas_core.BlockSpec\n-    in_specs: Sequence[BlockSpec | NoBlockSpec] | NoBlockSpec = no_block_spec,\n-    out_specs: BlockSpec | NoBlockSpec\n-    | Sequence[BlockSpec | NoBlockSpec] = no_block_spec,\n-):\n",
            "whole_added": "+BlockSpec = pallas_core.BlockSpec\n+BlockSpecTree = pallas_core.BlockSpecTree\n+    in_specs: BlockSpecTree = no_block_spec,\n+    out_specs: BlockSpecTree = no_block_spec,\n+) -> Callable[..., Any]:\n",
            "whole_hunk": "@@ -55,10 +55,11 @@ map, unsafe_map = safe_map, map\n zip, unsafe_zip = safe_zip, zip\n \n Grid = pallas_core.Grid\n-BlockSpec = pallas_core.BlockSpec\n GridSpec = pallas_core.GridSpec\n BlockMapping = pallas_core.BlockMapping\n GridMapping = pallas_core.GridMapping\n+BlockSpec = pallas_core.BlockSpec\n+BlockSpecTree = pallas_core.BlockSpecTree\n NoBlockSpec = pallas_core.NoBlockSpec\n no_block_spec = pallas_core.no_block_spec\n \n@@ -763,14 +764,13 @@ def pallas_call(\n     grid_spec: GridSpec | None = None,\n     debug: bool = False,\n     grid: Grid | None = None,\n-    in_specs: Sequence[BlockSpec | NoBlockSpec] | NoBlockSpec = no_block_spec,\n-    out_specs: BlockSpec | NoBlockSpec\n-    | Sequence[BlockSpec | NoBlockSpec] = no_block_spec,\n+    in_specs: BlockSpecTree = no_block_spec,\n+    out_specs: BlockSpecTree = no_block_spec,\n     input_output_aliases: dict[int, int] = {},\n     interpret: bool = False,\n     name: str | None = None,\n     compiler_params: dict[str, Any] | None = None,\n-):\n+) -> Callable[..., Any]:\n   name = _extract_function_name(f, name)\n   if compiler_params is None:\n     compiler_params = {}\n"
        },
        {
            "name": "primitives.py",
            "path": "jax/_src/pallas/primitives.py",
            "patches": [
                {
                    "old_start": 39,
                    "old_length": 10,
                    "new_start": 39,
                    "new_length": 6,
                    "hunk": "@@ -39,10 +39,6 @@ from jax._src.state import primitives as sp\n from jax.interpreters import mlir\n import jax.numpy as jnp\n \n-\n-# TODO(sharadmv): enable type checking\n-# mypy: ignore-errors\n-\n partial = functools.partial\n Slice = indexing.Slice\n NDIndexer = indexing.NDIndexer\n"
                },
                {
                    "old_start": 64,
                    "old_length": 6,
                    "new_start": 60,
                    "new_length": 7,
                    "hunk": "@@ -64,6 +60,7 @@ program_id_p.def_custom_bind(program_id_bind)\n \n def _program_id_impl(*, axis: int):\n   grid_env = pallas_core.current_grid_env()\n+  assert grid_env\n   return grid_env[axis].axis_index\n program_id_p.def_impl(_program_id_impl)\n \n"
                },
                {
                    "old_start": 87,
                    "old_length": 6,
                    "new_start": 84,
                    "new_length": 7,
                    "hunk": "@@ -87,6 +84,7 @@ def _num_programs_bind(*, axis: int):\n @num_programs_p.def_impl\n def _num_programs_impl(*, axis: int):\n   grid_env = pallas_core.current_grid_env()\n+  assert grid_env\n   return jnp.asarray(grid_env[axis].axis_size, dtype=jnp.int32)\n \n @num_programs_p.def_abstract_eval\n"
                },
                {
                    "old_start": 569,
                    "old_length": 7,
                    "new_start": 567,
                    "new_length": 7,
                    "hunk": "@@ -569,7 +567,7 @@ def debug_print(fmt: str, *args: jax.ArrayLike):\n   \"\"\"  # fmt: skip\n   has_placeholders = False\n   if fmt:\n-    _, field_name, *_ = next(string.Formatter().parse(fmt))\n+    _, field_name, *_ = next(iter(string.Formatter().parse(fmt)))\n     has_placeholders = field_name is not None\n   return debug_print_p.bind(*args, fmt=fmt, has_placeholders=has_placeholders)\n \n"
                }
            ],
            "whole_deleted": "-\n-# TODO(sharadmv): enable type checking\n-# mypy: ignore-errors\n-\n-    _, field_name, *_ = next(string.Formatter().parse(fmt))\n",
            "whole_added": "+  assert grid_env\n+  assert grid_env\n+    _, field_name, *_ = next(iter(string.Formatter().parse(fmt)))\n",
            "whole_hunk": "@@ -39,10 +39,6 @@ from jax._src.state import primitives as sp\n from jax.interpreters import mlir\n import jax.numpy as jnp\n \n-\n-# TODO(sharadmv): enable type checking\n-# mypy: ignore-errors\n-\n partial = functools.partial\n Slice = indexing.Slice\n NDIndexer = indexing.NDIndexer\n@@ -64,6 +60,7 @@ program_id_p.def_custom_bind(program_id_bind)\n \n def _program_id_impl(*, axis: int):\n   grid_env = pallas_core.current_grid_env()\n+  assert grid_env\n   return grid_env[axis].axis_index\n program_id_p.def_impl(_program_id_impl)\n \n@@ -87,6 +84,7 @@ def _num_programs_bind(*, axis: int):\n @num_programs_p.def_impl\n def _num_programs_impl(*, axis: int):\n   grid_env = pallas_core.current_grid_env()\n+  assert grid_env\n   return jnp.asarray(grid_env[axis].axis_size, dtype=jnp.int32)\n \n @num_programs_p.def_abstract_eval\n@@ -569,7 +567,7 @@ def debug_print(fmt: str, *args: jax.ArrayLike):\n   \"\"\"  # fmt: skip\n   has_placeholders = False\n   if fmt:\n-    _, field_name, *_ = next(string.Formatter().parse(fmt))\n+    _, field_name, *_ = next(iter(string.Formatter().parse(fmt)))\n     has_placeholders = field_name is not None\n   return debug_print_p.bind(*args, fmt=fmt, has_placeholders=has_placeholders)\n \n"
        },
        {
            "name": "flash_attention.py",
            "path": "jax/experimental/pallas/ops/tpu/flash_attention.py",
            "patches": [
                {
                    "old_start": 1099,
                    "old_length": 7,
                    "new_start": 1099,
                    "new_length": 7,
                    "hunk": "@@ -1099,7 +1099,7 @@ def _flash_attention_bwd_dkv(\n         grid_spec=pltpu.PrefetchScalarGridSpec(\n             num_scalar_prefetch=0,\n             grid=grid,\n-            in_specs=in_specs,  # type: ignore\n+            in_specs=in_specs,\n             out_specs=out_specs,\n             scratch_shapes=scratch_shapes,\n         ),\n"
                },
                {
                    "old_start": 1444,
                    "old_length": 8,
                    "new_start": 1444,
                    "new_length": 8,
                    "hunk": "@@ -1444,8 +1444,8 @@ def _flash_attention_bwd_dq(\n         grid_spec=pltpu.PrefetchScalarGridSpec(\n             num_scalar_prefetch=0,\n             grid=grid,\n-            in_specs=in_specs,  # type: ignore\n-            out_specs=out_specs,  # type: ignore\n+            in_specs=in_specs,\n+            out_specs=out_specs,\n             scratch_shapes=scratch_shapes,\n         ),\n         out_shape=out_shapes,"
                }
            ],
            "whole_deleted": "-            in_specs=in_specs,  # type: ignore\n-            in_specs=in_specs,  # type: ignore\n-            out_specs=out_specs,  # type: ignore\n",
            "whole_added": "+            in_specs=in_specs,\n+            in_specs=in_specs,\n+            out_specs=out_specs,\n",
            "whole_hunk": "@@ -1099,7 +1099,7 @@ def _flash_attention_bwd_dkv(\n         grid_spec=pltpu.PrefetchScalarGridSpec(\n             num_scalar_prefetch=0,\n             grid=grid,\n-            in_specs=in_specs,  # type: ignore\n+            in_specs=in_specs,\n             out_specs=out_specs,\n             scratch_shapes=scratch_shapes,\n         ),\n@@ -1444,8 +1444,8 @@ def _flash_attention_bwd_dq(\n         grid_spec=pltpu.PrefetchScalarGridSpec(\n             num_scalar_prefetch=0,\n             grid=grid,\n-            in_specs=in_specs,  # type: ignore\n-            out_specs=out_specs,  # type: ignore\n+            in_specs=in_specs,\n+            out_specs=out_specs,\n             scratch_shapes=scratch_shapes,\n         ),\n         out_shape=out_shapes,"
        }
    ]
},
{
    "Id": 23,
    "commit_link": "https://github.com/google/jax/commit/0786da8fd880515ba156a345f4a703c87f7382e3",
    "date": "2024-06-07T20:07:42+01:00",
    "message": "Removed unnecessary mypy exclusions from pyproject.toml\n\n* 2/3 files type check just fine now\n* the remaining one could be handled via a file-level directive",
    "changes": [
        {
            "name": "test_harnesses.py",
            "path": "jax/_src/internal_test_util/test_harnesses.py",
            "patches": [
                {
                    "old_start": 62,
                    "old_length": 6,
                    "new_start": 62,
                    "new_length": 9,
                    "hunk": "@@ -62,6 +62,9 @@ from jax._src.lax import windowed_reductions as lax_windowed_reductions\n from jax._src.lib import xla_client\n from jax._src import random as jax_random\n \n+# mypy generates a lot of false positive due to re-assigned variables.\n+# mypy: disable-error-code=\"assignment, no-redef\"\n+\n # The code in this file relies on the values of some flags that are defined by\n # jtu. Note that the following can not always be moved to a test file since\n # then the test file has to import jtu first (to define the flags) which is not\n"
                },
                {
                    "old_start": 172,
                    "old_length": 9,
                    "new_start": 175,
                    "new_length": 9,
                    "hunk": "@@ -172,9 +175,9 @@ class Harness:\n     self.group_name = jtu.sanitize_test_name(group_name)\n     self.name = jtu.sanitize_test_name(name)\n     self.fullname = self.name if self.group_name is None else f\"{self.group_name}_{self.name}\"\n-    self.fun = fun  # type: ignore[assignment]\n+    self.fun = fun\n     self.arg_descriptors = arg_descriptors\n-    self.rng_factory = rng_factory  # type: ignore[assignment]\n+    self.rng_factory = rng_factory\n     self.jax_unimplemented = jax_unimplemented\n     self.dtype = dtype\n     self.params = params\n"
                },
                {
                    "old_start": 2060,
                    "old_length": 18,
                    "new_start": 2063,
                    "new_length": 17,
                    "hunk": "@@ -2060,18 +2063,17 @@ def _make_slice_harness(name,\n   define(\n       lax.slice_p,\n       f\"{name}_a={jtu.format_shape_dtype_string(shape, dtype)}_{start_indices=}_{limit_indices=}_{strides=}\",\n-      # type: ignore\n       lax.slice,\n       [\n-          RandArg(shape, dtype),  # type: ignore\n-          StaticArg(start_indices),  # type: ignore\n-          StaticArg(limit_indices),  # type: ignore\n+          RandArg(shape, dtype),\n+          StaticArg(start_indices),\n+          StaticArg(limit_indices),\n           StaticArg(strides)\n-      ],  # type: ignore\n+      ],\n       dtype=dtype,\n-      shape=shape,  # type: ignore\n-      start_indices=start_indices,  # type: ignore\n-      limit_indices=limit_indices)  # type: ignore\n+      shape=shape,\n+      start_indices=start_indices,\n+      limit_indices=limit_indices)\n \n \n # Test first all dtypes\n"
                },
                {
                    "old_start": 2161,
                    "old_length": 17,
                    "new_start": 2163,
                    "new_length": 16,
                    "hunk": "@@ -2161,17 +2163,16 @@ def _make_dynamic_slice_harness(name,\n     define(\n         lax.dynamic_slice_p,\n         f\"{name}_a={jtu.format_shape_dtype_string(shape, dtype)}_{start_indices=}_{limit_indices=}_enablexla={enable_xla}\",\n-        # type: ignore\n         lax.dynamic_slice,\n         [\n-            RandArg(shape, dtype),  # type: ignore\n+            RandArg(shape, dtype),\n             np.array(list(start_indices)),\n             StaticArg(tuple(map(operator.sub, limit_indices, start_indices)))\n-        ],  # type: ignore\n+        ],\n         dtype=dtype,\n-        shape=shape,  # type: ignore\n-        start_indices=start_indices,  # type: ignore\n-        limit_indices=limit_indices,  # type: ignore\n+        shape=shape,\n+        start_indices=start_indices,\n+        limit_indices=limit_indices,\n         enable_xla=enable_xla)\n \n \n"
                },
                {
                    "old_start": 2218,
                    "old_length": 19,
                    "new_start": 2219,
                    "new_length": 19,
                    "hunk": "@@ -2218,19 +2219,19 @@ def _make_dynamic_update_slice_harness(name,\n     define(\n         lax.dynamic_update_slice_p,\n         (\n-            f\"{name}_operand={jtu.format_shape_dtype_string(shape, dtype)}\"  # type: ignore\n+            f\"{name}_operand={jtu.format_shape_dtype_string(shape, dtype)}\"\n             f\"_update={jtu.format_shape_dtype_string(update_shape, dtype)}\"\n             f\"_{start_indices=}_{enable_xla=}\"),\n         lax.dynamic_update_slice,\n         [\n-            RandArg(shape, dtype),  # type: ignore\n-            RandArg(update_shape, dtype),  # type: ignore\n+            RandArg(shape, dtype),\n+            RandArg(update_shape, dtype),\n             np.array(start_indices)\n-        ],  # type: ignore\n+        ],\n         dtype=dtype,\n-        shape=shape,  # type: ignore\n-        start_indices=start_indices,  # type: ignore\n-        update_shape=update_shape,  # type: ignore\n+        shape=shape,\n+        start_indices=start_indices,\n+        update_shape=update_shape,\n         enable_xla=enable_xla)\n \n \n"
                },
                {
                    "old_start": 2261,
                    "old_length": 12,
                    "new_start": 2262,
                    "new_length": 12,
                    "hunk": "@@ -2261,12 +2262,12 @@ def _make_squeeze_harness(name,\n                           dtype=np.float32):\n   define(\n       lax.squeeze_p,\n-      f\"{name}_inshape={jtu.format_shape_dtype_string(shape, dtype)}_{dimensions=}\",  # type: ignore\n+      f\"{name}_inshape={jtu.format_shape_dtype_string(shape, dtype)}_{dimensions=}\",\n       lax.squeeze,\n-      [RandArg(shape, dtype), StaticArg(dimensions)],  # type: ignore[has-type]\n+      [RandArg(shape, dtype), StaticArg(dimensions)],\n       dtype=dtype,\n       arg_shape=shape,\n-      dimensions=dimensions)  # type: ignore[has-type]\n+      dimensions=dimensions)\n \n \n # Test first all dtypes\n"
                },
                {
                    "old_start": 3312,
                    "old_length": 6,
                    "new_start": 3313,
                    "new_length": 7,
                    "hunk": "@@ -3312,6 +3313,7 @@ for padding, lhs_dilation, rhs_dilation in [\n         lhs_dilation=lhs_dilation,\n         rhs_dilation=rhs_dilation)\n \n+key_types: list[tuple[tuple[int, ...], jax.typing.DTypeLike]]\n key_types = [((4,), np.uint32)]\n if config.enable_x64.value:\n   key_types.append(((2,), np.uint64))\n"
                }
            ],
            "whole_deleted": "-    self.fun = fun  # type: ignore[assignment]\n-    self.rng_factory = rng_factory  # type: ignore[assignment]\n-      # type: ignore\n-          RandArg(shape, dtype),  # type: ignore\n-          StaticArg(start_indices),  # type: ignore\n-          StaticArg(limit_indices),  # type: ignore\n-      ],  # type: ignore\n-      shape=shape,  # type: ignore\n-      start_indices=start_indices,  # type: ignore\n-      limit_indices=limit_indices)  # type: ignore\n-        # type: ignore\n-            RandArg(shape, dtype),  # type: ignore\n-        ],  # type: ignore\n-        shape=shape,  # type: ignore\n-        start_indices=start_indices,  # type: ignore\n-        limit_indices=limit_indices,  # type: ignore\n-            f\"{name}_operand={jtu.format_shape_dtype_string(shape, dtype)}\"  # type: ignore\n-            RandArg(shape, dtype),  # type: ignore\n-            RandArg(update_shape, dtype),  # type: ignore\n-        ],  # type: ignore\n-        shape=shape,  # type: ignore\n-        start_indices=start_indices,  # type: ignore\n-        update_shape=update_shape,  # type: ignore\n-      f\"{name}_inshape={jtu.format_shape_dtype_string(shape, dtype)}_{dimensions=}\",  # type: ignore\n-      [RandArg(shape, dtype), StaticArg(dimensions)],  # type: ignore[has-type]\n-      dimensions=dimensions)  # type: ignore[has-type]\n",
            "whole_added": "+# mypy generates a lot of false positive due to re-assigned variables.\n+# mypy: disable-error-code=\"assignment, no-redef\"\n+\n+    self.fun = fun\n+    self.rng_factory = rng_factory\n+          RandArg(shape, dtype),\n+          StaticArg(start_indices),\n+          StaticArg(limit_indices),\n+      ],\n+      shape=shape,\n+      start_indices=start_indices,\n+      limit_indices=limit_indices)\n+            RandArg(shape, dtype),\n+        ],\n+        shape=shape,\n+        start_indices=start_indices,\n+        limit_indices=limit_indices,\n+            f\"{name}_operand={jtu.format_shape_dtype_string(shape, dtype)}\"\n+            RandArg(shape, dtype),\n+            RandArg(update_shape, dtype),\n+        ],\n+        shape=shape,\n+        start_indices=start_indices,\n+        update_shape=update_shape,\n+      f\"{name}_inshape={jtu.format_shape_dtype_string(shape, dtype)}_{dimensions=}\",\n+      [RandArg(shape, dtype), StaticArg(dimensions)],\n+      dimensions=dimensions)\n+key_types: list[tuple[tuple[int, ...], jax.typing.DTypeLike]]\n",
            "whole_hunk": "@@ -62,6 +62,9 @@ from jax._src.lax import windowed_reductions as lax_windowed_reductions\n from jax._src.lib import xla_client\n from jax._src import random as jax_random\n \n+# mypy generates a lot of false positive due to re-assigned variables.\n+# mypy: disable-error-code=\"assignment, no-redef\"\n+\n # The code in this file relies on the values of some flags that are defined by\n # jtu. Note that the following can not always be moved to a test file since\n # then the test file has to import jtu first (to define the flags) which is not\n@@ -172,9 +175,9 @@ class Harness:\n     self.group_name = jtu.sanitize_test_name(group_name)\n     self.name = jtu.sanitize_test_name(name)\n     self.fullname = self.name if self.group_name is None else f\"{self.group_name}_{self.name}\"\n-    self.fun = fun  # type: ignore[assignment]\n+    self.fun = fun\n     self.arg_descriptors = arg_descriptors\n-    self.rng_factory = rng_factory  # type: ignore[assignment]\n+    self.rng_factory = rng_factory\n     self.jax_unimplemented = jax_unimplemented\n     self.dtype = dtype\n     self.params = params\n@@ -2060,18 +2063,17 @@ def _make_slice_harness(name,\n   define(\n       lax.slice_p,\n       f\"{name}_a={jtu.format_shape_dtype_string(shape, dtype)}_{start_indices=}_{limit_indices=}_{strides=}\",\n-      # type: ignore\n       lax.slice,\n       [\n-          RandArg(shape, dtype),  # type: ignore\n-          StaticArg(start_indices),  # type: ignore\n-          StaticArg(limit_indices),  # type: ignore\n+          RandArg(shape, dtype),\n+          StaticArg(start_indices),\n+          StaticArg(limit_indices),\n           StaticArg(strides)\n-      ],  # type: ignore\n+      ],\n       dtype=dtype,\n-      shape=shape,  # type: ignore\n-      start_indices=start_indices,  # type: ignore\n-      limit_indices=limit_indices)  # type: ignore\n+      shape=shape,\n+      start_indices=start_indices,\n+      limit_indices=limit_indices)\n \n \n # Test first all dtypes\n@@ -2161,17 +2163,16 @@ def _make_dynamic_slice_harness(name,\n     define(\n         lax.dynamic_slice_p,\n         f\"{name}_a={jtu.format_shape_dtype_string(shape, dtype)}_{start_indices=}_{limit_indices=}_enablexla={enable_xla}\",\n-        # type: ignore\n         lax.dynamic_slice,\n         [\n-            RandArg(shape, dtype),  # type: ignore\n+            RandArg(shape, dtype),\n             np.array(list(start_indices)),\n             StaticArg(tuple(map(operator.sub, limit_indices, start_indices)))\n-        ],  # type: ignore\n+        ],\n         dtype=dtype,\n-        shape=shape,  # type: ignore\n-        start_indices=start_indices,  # type: ignore\n-        limit_indices=limit_indices,  # type: ignore\n+        shape=shape,\n+        start_indices=start_indices,\n+        limit_indices=limit_indices,\n         enable_xla=enable_xla)\n \n \n@@ -2218,19 +2219,19 @@ def _make_dynamic_update_slice_harness(name,\n     define(\n         lax.dynamic_update_slice_p,\n         (\n-            f\"{name}_operand={jtu.format_shape_dtype_string(shape, dtype)}\"  # type: ignore\n+            f\"{name}_operand={jtu.format_shape_dtype_string(shape, dtype)}\"\n             f\"_update={jtu.format_shape_dtype_string(update_shape, dtype)}\"\n             f\"_{start_indices=}_{enable_xla=}\"),\n         lax.dynamic_update_slice,\n         [\n-            RandArg(shape, dtype),  # type: ignore\n-            RandArg(update_shape, dtype),  # type: ignore\n+            RandArg(shape, dtype),\n+            RandArg(update_shape, dtype),\n             np.array(start_indices)\n-        ],  # type: ignore\n+        ],\n         dtype=dtype,\n-        shape=shape,  # type: ignore\n-        start_indices=start_indices,  # type: ignore\n-        update_shape=update_shape,  # type: ignore\n+        shape=shape,\n+        start_indices=start_indices,\n+        update_shape=update_shape,\n         enable_xla=enable_xla)\n \n \n@@ -2261,12 +2262,12 @@ def _make_squeeze_harness(name,\n                           dtype=np.float32):\n   define(\n       lax.squeeze_p,\n-      f\"{name}_inshape={jtu.format_shape_dtype_string(shape, dtype)}_{dimensions=}\",  # type: ignore\n+      f\"{name}_inshape={jtu.format_shape_dtype_string(shape, dtype)}_{dimensions=}\",\n       lax.squeeze,\n-      [RandArg(shape, dtype), StaticArg(dimensions)],  # type: ignore[has-type]\n+      [RandArg(shape, dtype), StaticArg(dimensions)],\n       dtype=dtype,\n       arg_shape=shape,\n-      dimensions=dimensions)  # type: ignore[has-type]\n+      dimensions=dimensions)\n \n \n # Test first all dtypes\n@@ -3312,6 +3313,7 @@ for padding, lhs_dilation, rhs_dilation in [\n         lhs_dilation=lhs_dilation,\n         rhs_dilation=rhs_dilation)\n \n+key_types: list[tuple[tuple[int, ...], jax.typing.DTypeLike]]\n key_types = [((4,), np.uint32)]\n if config.enable_x64.value:\n   key_types.append(((2,), np.uint64))\n"
        },
        {
            "name": "pyproject.toml",
            "path": "pyproject.toml",
            "patches": [
                {
                    "old_start": 44,
                    "old_length": 14,
                    "new_start": 44,
                    "new_length": 6,
                    "hunk": "@@ -44,14 +44,6 @@ module = [\n ]\n ignore_missing_imports = true\n \n-[[tool.mypy.overrides]]\n-module = [\n-    \"jax.interpreters.autospmd\",\n-    \"jax.lax.lax_parallel\",\n-    \"jax._src.internal_test_util.test_harnesses\",\n-]\n-ignore_errors = true\n-\n [tool.pytest.ini_options]\n markers = [\n     \"multiaccelerator: indicates that a test can make use of and possibly requires multiple accelerators\","
                }
            ],
            "whole_deleted": "-[[tool.mypy.overrides]]\n-module = [\n-    \"jax.interpreters.autospmd\",\n-    \"jax.lax.lax_parallel\",\n-    \"jax._src.internal_test_util.test_harnesses\",\n-]\n-ignore_errors = true\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -44,14 +44,6 @@ module = [\n ]\n ignore_missing_imports = true\n \n-[[tool.mypy.overrides]]\n-module = [\n-    \"jax.interpreters.autospmd\",\n-    \"jax.lax.lax_parallel\",\n-    \"jax._src.internal_test_util.test_harnesses\",\n-]\n-ignore_errors = true\n-\n [tool.pytest.ini_options]\n markers = [\n     \"multiaccelerator: indicates that a test can make use of and possibly requires multiple accelerators\","
        }
    ]
},
{
    "Id": 24,
    "commit_link": "https://github.com/google/jax/commit/fbf2a62aa128cbb171ace64faaa9e9b1ceca5f7b",
    "date": "2024-06-06T11:38:56-07:00",
    "message": "Remove `jaxpr` and `name` from `Lowered` because `specialize` already has those. This keeps the abstraction boundary clear. Adapt `export` to use `specialize`.\n\nPiperOrigin-RevId: 640968129",
    "changes": [
        {
            "name": "api.py",
            "path": "jax/_src/api.py",
            "patches": [
                {
                    "old_start": 1811,
                    "old_length": 9,
                    "new_start": 1811,
                    "new_length": 41,
                    "hunk": "@@ -1811,9 +1811,41 @@ def _cpp_pmap(\n \n   pmap_f = wraps(fun)(cpp_mapped_f)\n \n+  @api_boundary\n+  def specialize(*args, **kwargs):\n+    lowering_parameters = kwargs.pop(\n+        '_experimental_lowering_parameters', mlir.LoweringParameters())\n+    p = _prepare_pmap(\n+        fun, in_axes, out_axes, static_broadcasted_tuple, donate_tuple,\n+        devices, backend, axis_size, args, kwargs)\n+    abstract_args = list(map(shaped_abstractify, p.flat_args))\n+    lower_callable = partial(\n+        pxla.lower_parallel_callable, p.flat_fun, backend, axis_name,\n+        axis_size=p.local_axis_size, global_axis_size=p.global_axis_size,\n+        devices=p.devices,\n+        name=p.flat_fun.__name__,\n+        in_axes=p.in_axes_flat,\n+        out_axes_thunk=p.out_axes_thunk,\n+        donated_invars=p.donated_invars,\n+        is_explicit_global_axis_size=p.is_explicit_global_axis_size,\n+        avals=abstract_args,\n+        lowering_parameters=lowering_parameters)\n+    jaxpr, _, _, _, _ = pxla.get_pmap_jaxpr(\n+        p.flat_fun, backend, axis_name,\n+        axis_size=p.local_axis_size, global_axis_size=p.global_axis_size,\n+        devices=p.devices,\n+        name=p.flat_fun.__name__,\n+        in_axes=p.in_axes_flat,\n+        out_axes_thunk=p.out_axes_thunk,\n+        avals=abstract_args)\n+    args_info = stages.make_args_info(p.in_tree, abstract_args, donate_tuple)\n+    return stages.Specialized(jaxpr, args_info, p.flat_fun.__name__,\n+                              p.out_tree(), lower_callable)\n+\n   pmap_f.lower = _pmap_lower(\n       fun, axis_name, in_axes, out_axes, static_broadcasted_tuple, devices,\n       backend, axis_size, donate_tuple)\n+  pmap_f.specialize = specialize\n \n   return pmap_f\n \n"
                },
                {
                    "old_start": 1845,
                    "old_length": 7,
                    "new_start": 1877,
                    "new_length": 7,
                    "hunk": "@@ -1845,7 +1877,7 @@ def _pmap_lower(fun, axis_name, in_axes, out_axes, static_broadcasted_tuple,\n         fun, in_axes, out_axes, static_broadcasted_tuple, donate_tuple,\n         devices, backend, axis_size, args, kwargs)\n     abstract_args = list(map(shaped_abstractify, p.flat_args))\n-    computation, closed_jaxpr = pxla.lower_parallel_callable(\n+    computation = pxla.lower_parallel_callable(\n         p.flat_fun, backend, axis_name,\n         axis_size=p.local_axis_size, global_axis_size=p.global_axis_size,\n         devices=p.devices,\n"
                },
                {
                    "old_start": 1857,
                    "old_length": 8,
                    "new_start": 1889,
                    "new_length": 7,
                    "hunk": "@@ -1857,8 +1889,7 @@ def _pmap_lower(fun, axis_name, in_axes, out_axes, static_broadcasted_tuple,\n         avals=abstract_args,\n         lowering_parameters=lowering_parameters)\n     return stages.Lowered.from_flat_info(\n-        computation, p.in_tree, abstract_args, donate_tuple, p.out_tree(),\n-        fun_name=p.flat_fun.__name__, jaxpr=closed_jaxpr)\n+        computation, p.in_tree, abstract_args, donate_tuple, p.out_tree())\n \n   return lower\n \n"
                }
            ],
            "whole_deleted": "-    computation, closed_jaxpr = pxla.lower_parallel_callable(\n-        computation, p.in_tree, abstract_args, donate_tuple, p.out_tree(),\n-        fun_name=p.flat_fun.__name__, jaxpr=closed_jaxpr)\n",
            "whole_added": "+  @api_boundary\n+  def specialize(*args, **kwargs):\n+    lowering_parameters = kwargs.pop(\n+        '_experimental_lowering_parameters', mlir.LoweringParameters())\n+    p = _prepare_pmap(\n+        fun, in_axes, out_axes, static_broadcasted_tuple, donate_tuple,\n+        devices, backend, axis_size, args, kwargs)\n+    abstract_args = list(map(shaped_abstractify, p.flat_args))\n+    lower_callable = partial(\n+        pxla.lower_parallel_callable, p.flat_fun, backend, axis_name,\n+        axis_size=p.local_axis_size, global_axis_size=p.global_axis_size,\n+        devices=p.devices,\n+        name=p.flat_fun.__name__,\n+        in_axes=p.in_axes_flat,\n+        out_axes_thunk=p.out_axes_thunk,\n+        donated_invars=p.donated_invars,\n+        is_explicit_global_axis_size=p.is_explicit_global_axis_size,\n+        avals=abstract_args,\n+        lowering_parameters=lowering_parameters)\n+    jaxpr, _, _, _, _ = pxla.get_pmap_jaxpr(\n+        p.flat_fun, backend, axis_name,\n+        axis_size=p.local_axis_size, global_axis_size=p.global_axis_size,\n+        devices=p.devices,\n+        name=p.flat_fun.__name__,\n+        in_axes=p.in_axes_flat,\n+        out_axes_thunk=p.out_axes_thunk,\n+        avals=abstract_args)\n+    args_info = stages.make_args_info(p.in_tree, abstract_args, donate_tuple)\n+    return stages.Specialized(jaxpr, args_info, p.flat_fun.__name__,\n+                              p.out_tree(), lower_callable)\n+\n+  pmap_f.specialize = specialize\n+    computation = pxla.lower_parallel_callable(\n+        computation, p.in_tree, abstract_args, donate_tuple, p.out_tree())\n",
            "whole_hunk": "@@ -1811,9 +1811,41 @@ def _cpp_pmap(\n \n   pmap_f = wraps(fun)(cpp_mapped_f)\n \n+  @api_boundary\n+  def specialize(*args, **kwargs):\n+    lowering_parameters = kwargs.pop(\n+        '_experimental_lowering_parameters', mlir.LoweringParameters())\n+    p = _prepare_pmap(\n+        fun, in_axes, out_axes, static_broadcasted_tuple, donate_tuple,\n+        devices, backend, axis_size, args, kwargs)\n+    abstract_args = list(map(shaped_abstractify, p.flat_args))\n+    lower_callable = partial(\n+        pxla.lower_parallel_callable, p.flat_fun, backend, axis_name,\n+        axis_size=p.local_axis_size, global_axis_size=p.global_axis_size,\n+        devices=p.devices,\n+        name=p.flat_fun.__name__,\n+        in_axes=p.in_axes_flat,\n+        out_axes_thunk=p.out_axes_thunk,\n+        donated_invars=p.donated_invars,\n+        is_explicit_global_axis_size=p.is_explicit_global_axis_size,\n+        avals=abstract_args,\n+        lowering_parameters=lowering_parameters)\n+    jaxpr, _, _, _, _ = pxla.get_pmap_jaxpr(\n+        p.flat_fun, backend, axis_name,\n+        axis_size=p.local_axis_size, global_axis_size=p.global_axis_size,\n+        devices=p.devices,\n+        name=p.flat_fun.__name__,\n+        in_axes=p.in_axes_flat,\n+        out_axes_thunk=p.out_axes_thunk,\n+        avals=abstract_args)\n+    args_info = stages.make_args_info(p.in_tree, abstract_args, donate_tuple)\n+    return stages.Specialized(jaxpr, args_info, p.flat_fun.__name__,\n+                              p.out_tree(), lower_callable)\n+\n   pmap_f.lower = _pmap_lower(\n       fun, axis_name, in_axes, out_axes, static_broadcasted_tuple, devices,\n       backend, axis_size, donate_tuple)\n+  pmap_f.specialize = specialize\n \n   return pmap_f\n \n@@ -1845,7 +1877,7 @@ def _pmap_lower(fun, axis_name, in_axes, out_axes, static_broadcasted_tuple,\n         fun, in_axes, out_axes, static_broadcasted_tuple, donate_tuple,\n         devices, backend, axis_size, args, kwargs)\n     abstract_args = list(map(shaped_abstractify, p.flat_args))\n-    computation, closed_jaxpr = pxla.lower_parallel_callable(\n+    computation = pxla.lower_parallel_callable(\n         p.flat_fun, backend, axis_name,\n         axis_size=p.local_axis_size, global_axis_size=p.global_axis_size,\n         devices=p.devices,\n@@ -1857,8 +1889,7 @@ def _pmap_lower(fun, axis_name, in_axes, out_axes, static_broadcasted_tuple,\n         avals=abstract_args,\n         lowering_parameters=lowering_parameters)\n     return stages.Lowered.from_flat_info(\n-        computation, p.in_tree, abstract_args, donate_tuple, p.out_tree(),\n-        fun_name=p.flat_fun.__name__, jaxpr=closed_jaxpr)\n+        computation, p.in_tree, abstract_args, donate_tuple, p.out_tree())\n \n   return lower\n \n"
        },
        {
            "name": "pxla.py",
            "path": "jax/_src/interpreters/pxla.py",
            "patches": [
                {
                    "old_start": 557,
                    "old_length": 7,
                    "new_start": 557,
                    "new_length": 7,
                    "hunk": "@@ -557,7 +557,7 @@ def parallel_callable(fun: lu.WrappedFun,\n                       donated_invars: Sequence[bool],\n                       is_explicit_global_axis_size: bool,\n                       *avals):\n-  pmap_computation, _ = lower_parallel_callable(\n+  pmap_computation = lower_parallel_callable(\n       fun, backend_name, axis_name, axis_size, global_axis_size, devices, name,\n       in_axes, out_axes_thunk, donated_invars,\n       is_explicit_global_axis_size, avals,\n"
                },
                {
                    "old_start": 665,
                    "old_length": 6,
                    "new_start": 665,
                    "new_length": 31,
                    "hunk": "@@ -665,6 +665,31 @@ def stage_parallel_callable(\n   return jaxpr, consts, replicas, shards\n \n \n+def get_pmap_jaxpr(\n+    fun: lu.WrappedFun,\n+    backend_name: str | None,\n+    axis_name: core.AxisName,\n+    axis_size: int,\n+    global_axis_size: int,\n+    devices: Sequence[xc.Device] | None,\n+    name: str,\n+    in_axes: Iterable[int | None],\n+    out_axes_thunk: Callable[[], Sequence[int | None]],\n+    avals: Sequence[core.AbstractValue]):\n+  if devices is not None and backend_name is None:\n+    backend = xb.get_device_backend(devices[0])\n+  else:\n+    backend = xb.get_backend(backend_name)\n+\n+  pci = ParallelCallableInfo(\n+      name, backend, axis_name, axis_size, global_axis_size, devices,\n+      in_axes, out_axes_thunk, avals)\n+  jaxpr, consts, replicas, shards = stage_parallel_callable(pci, fun)\n+  jaxpr = core.remove_named_axis_effects(jaxpr, {axis_name})\n+  closed_jaxpr = core.ClosedJaxpr(jaxpr, consts)\n+  return closed_jaxpr, backend, replicas, shards, pci\n+\n+\n @profiler.annotate_function\n def lower_parallel_callable(\n     fun: lu.WrappedFun,\n"
                },
                {
                    "old_start": 680,
                    "old_length": 7,
                    "new_start": 705,
                    "new_length": 7,
                    "hunk": "@@ -680,7 +705,7 @@ def lower_parallel_callable(\n     is_explicit_global_axis_size: bool,\n     avals: Sequence[core.AbstractValue],\n     *,\n-    lowering_parameters: mlir.LoweringParameters) -> tuple[PmapComputation, core.ClosedJaxpr]:\n+    lowering_parameters: mlir.LoweringParameters) -> PmapComputation:\n   # Determine global_axis_size for use in AxisEnv.\n   # TODO(mattjj,skyewm): revive this check (inner_pmap always False now)\n   # if xb.process_count() > 1 and global_axis_size is None and inner_pmap:\n"
                },
                {
                    "old_start": 691,
                    "old_length": 10,
                    "new_start": 716,
                    "new_length": 10,
                    "hunk": "@@ -691,10 +716,10 @@ def lower_parallel_callable(\n         f\"Specified axis_size {global_axis_size} doesn't match received \"\n         f\"axis_size {axis_size}.\")\n \n-  if devices is not None and backend_name is None:\n-    backend = xb.get_device_backend(devices[0])\n-  else:\n-    backend = xb.get_backend(backend_name)\n+  closed_jaxpr, backend, replicas, shards, pci = get_pmap_jaxpr(\n+    fun, backend_name, axis_name, axis_size, global_axis_size, devices, name,\n+    in_axes, out_axes_thunk, avals)\n+  jaxpr = closed_jaxpr.jaxpr\n \n   no_nested_sharding = False\n   must_run_on_all_devices = False\n"
                },
                {
                    "old_start": 711,
                    "old_length": 10,
                    "new_start": 736,
                    "new_length": 6,
                    "hunk": "@@ -711,10 +736,6 @@ def lower_parallel_callable(\n         # devices). Nested sharding is ok in this case.\n         must_run_on_all_devices = True\n \n-  pci = ParallelCallableInfo(\n-      name, backend, axis_name, axis_size, global_axis_size, devices,\n-      in_axes, out_axes_thunk, avals)\n-  jaxpr, consts, replicas, shards = stage_parallel_callable(pci, fun)\n   if logger.isEnabledFor(logging.DEBUG):\n     logger.debug(\"sharded_avals: %s\", shards.sharded_avals)\n     logger.debug(\"global_sharded_avals: %s\", shards.global_sharded_avals)\n"
                },
                {
                    "old_start": 756,
                    "old_length": 8,
                    "new_start": 777,
                    "new_length": 6,
                    "hunk": "@@ -756,8 +777,6 @@ def lower_parallel_callable(\n   axis_env = sharding_impls.AxisEnv(\n       replicas.num_global_replicas, (axis_name,), (global_axis_size,))\n   name_stack = source_info_util.new_name_stack(wrap_name(name, 'pmap'))\n-  jaxpr = core.remove_named_axis_effects(jaxpr, {axis_name})\n-  closed_jaxpr = core.ClosedJaxpr(jaxpr, consts)\n   replicated_args = [axis is None for axis in in_axes]\n   tuple_args = dispatch.should_tuple_args(len(shards.global_sharded_avals),\n                                           backend.platform)\n"
                },
                {
                    "old_start": 798,
                    "old_length": 7,
                    "new_start": 817,
                    "new_length": 7,
                    "hunk": "@@ -798,7 +817,7 @@ def lower_parallel_callable(\n                          keepalive=lowering_result.keepalive,\n                          host_callbacks=lowering_result.host_callbacks,\n                          jaxpr_debug_info=closed_jaxpr.jaxpr.debug_info,\n-                         shape_poly_state=lowering_result.shape_poly_state), closed_jaxpr\n+                         shape_poly_state=lowering_result.shape_poly_state)\n \n \n def _pmap_unmap_shaped_array(\n"
                }
            ],
            "whole_deleted": "-  pmap_computation, _ = lower_parallel_callable(\n-    lowering_parameters: mlir.LoweringParameters) -> tuple[PmapComputation, core.ClosedJaxpr]:\n-  if devices is not None and backend_name is None:\n-    backend = xb.get_device_backend(devices[0])\n-  else:\n-    backend = xb.get_backend(backend_name)\n-  pci = ParallelCallableInfo(\n-      name, backend, axis_name, axis_size, global_axis_size, devices,\n-      in_axes, out_axes_thunk, avals)\n-  jaxpr, consts, replicas, shards = stage_parallel_callable(pci, fun)\n-  jaxpr = core.remove_named_axis_effects(jaxpr, {axis_name})\n-  closed_jaxpr = core.ClosedJaxpr(jaxpr, consts)\n-                         shape_poly_state=lowering_result.shape_poly_state), closed_jaxpr\n",
            "whole_added": "+  pmap_computation = lower_parallel_callable(\n+def get_pmap_jaxpr(\n+    fun: lu.WrappedFun,\n+    backend_name: str | None,\n+    axis_name: core.AxisName,\n+    axis_size: int,\n+    global_axis_size: int,\n+    devices: Sequence[xc.Device] | None,\n+    name: str,\n+    in_axes: Iterable[int | None],\n+    out_axes_thunk: Callable[[], Sequence[int | None]],\n+    avals: Sequence[core.AbstractValue]):\n+  if devices is not None and backend_name is None:\n+    backend = xb.get_device_backend(devices[0])\n+  else:\n+    backend = xb.get_backend(backend_name)\n+\n+  pci = ParallelCallableInfo(\n+      name, backend, axis_name, axis_size, global_axis_size, devices,\n+      in_axes, out_axes_thunk, avals)\n+  jaxpr, consts, replicas, shards = stage_parallel_callable(pci, fun)\n+  jaxpr = core.remove_named_axis_effects(jaxpr, {axis_name})\n+  closed_jaxpr = core.ClosedJaxpr(jaxpr, consts)\n+  return closed_jaxpr, backend, replicas, shards, pci\n+\n+\n+    lowering_parameters: mlir.LoweringParameters) -> PmapComputation:\n+  closed_jaxpr, backend, replicas, shards, pci = get_pmap_jaxpr(\n+    fun, backend_name, axis_name, axis_size, global_axis_size, devices, name,\n+    in_axes, out_axes_thunk, avals)\n+  jaxpr = closed_jaxpr.jaxpr\n+                         shape_poly_state=lowering_result.shape_poly_state)\n",
            "whole_hunk": "@@ -557,7 +557,7 @@ def parallel_callable(fun: lu.WrappedFun,\n                       donated_invars: Sequence[bool],\n                       is_explicit_global_axis_size: bool,\n                       *avals):\n-  pmap_computation, _ = lower_parallel_callable(\n+  pmap_computation = lower_parallel_callable(\n       fun, backend_name, axis_name, axis_size, global_axis_size, devices, name,\n       in_axes, out_axes_thunk, donated_invars,\n       is_explicit_global_axis_size, avals,\n@@ -665,6 +665,31 @@ def stage_parallel_callable(\n   return jaxpr, consts, replicas, shards\n \n \n+def get_pmap_jaxpr(\n+    fun: lu.WrappedFun,\n+    backend_name: str | None,\n+    axis_name: core.AxisName,\n+    axis_size: int,\n+    global_axis_size: int,\n+    devices: Sequence[xc.Device] | None,\n+    name: str,\n+    in_axes: Iterable[int | None],\n+    out_axes_thunk: Callable[[], Sequence[int | None]],\n+    avals: Sequence[core.AbstractValue]):\n+  if devices is not None and backend_name is None:\n+    backend = xb.get_device_backend(devices[0])\n+  else:\n+    backend = xb.get_backend(backend_name)\n+\n+  pci = ParallelCallableInfo(\n+      name, backend, axis_name, axis_size, global_axis_size, devices,\n+      in_axes, out_axes_thunk, avals)\n+  jaxpr, consts, replicas, shards = stage_parallel_callable(pci, fun)\n+  jaxpr = core.remove_named_axis_effects(jaxpr, {axis_name})\n+  closed_jaxpr = core.ClosedJaxpr(jaxpr, consts)\n+  return closed_jaxpr, backend, replicas, shards, pci\n+\n+\n @profiler.annotate_function\n def lower_parallel_callable(\n     fun: lu.WrappedFun,\n@@ -680,7 +705,7 @@ def lower_parallel_callable(\n     is_explicit_global_axis_size: bool,\n     avals: Sequence[core.AbstractValue],\n     *,\n-    lowering_parameters: mlir.LoweringParameters) -> tuple[PmapComputation, core.ClosedJaxpr]:\n+    lowering_parameters: mlir.LoweringParameters) -> PmapComputation:\n   # Determine global_axis_size for use in AxisEnv.\n   # TODO(mattjj,skyewm): revive this check (inner_pmap always False now)\n   # if xb.process_count() > 1 and global_axis_size is None and inner_pmap:\n@@ -691,10 +716,10 @@ def lower_parallel_callable(\n         f\"Specified axis_size {global_axis_size} doesn't match received \"\n         f\"axis_size {axis_size}.\")\n \n-  if devices is not None and backend_name is None:\n-    backend = xb.get_device_backend(devices[0])\n-  else:\n-    backend = xb.get_backend(backend_name)\n+  closed_jaxpr, backend, replicas, shards, pci = get_pmap_jaxpr(\n+    fun, backend_name, axis_name, axis_size, global_axis_size, devices, name,\n+    in_axes, out_axes_thunk, avals)\n+  jaxpr = closed_jaxpr.jaxpr\n \n   no_nested_sharding = False\n   must_run_on_all_devices = False\n@@ -711,10 +736,6 @@ def lower_parallel_callable(\n         # devices). Nested sharding is ok in this case.\n         must_run_on_all_devices = True\n \n-  pci = ParallelCallableInfo(\n-      name, backend, axis_name, axis_size, global_axis_size, devices,\n-      in_axes, out_axes_thunk, avals)\n-  jaxpr, consts, replicas, shards = stage_parallel_callable(pci, fun)\n   if logger.isEnabledFor(logging.DEBUG):\n     logger.debug(\"sharded_avals: %s\", shards.sharded_avals)\n     logger.debug(\"global_sharded_avals: %s\", shards.global_sharded_avals)\n@@ -756,8 +777,6 @@ def lower_parallel_callable(\n   axis_env = sharding_impls.AxisEnv(\n       replicas.num_global_replicas, (axis_name,), (global_axis_size,))\n   name_stack = source_info_util.new_name_stack(wrap_name(name, 'pmap'))\n-  jaxpr = core.remove_named_axis_effects(jaxpr, {axis_name})\n-  closed_jaxpr = core.ClosedJaxpr(jaxpr, consts)\n   replicated_args = [axis is None for axis in in_axes]\n   tuple_args = dispatch.should_tuple_args(len(shards.global_sharded_avals),\n                                           backend.platform)\n@@ -798,7 +817,7 @@ def lower_parallel_callable(\n                          keepalive=lowering_result.keepalive,\n                          host_callbacks=lowering_result.host_callbacks,\n                          jaxpr_debug_info=closed_jaxpr.jaxpr.debug_info,\n-                         shape_poly_state=lowering_result.shape_poly_state), closed_jaxpr\n+                         shape_poly_state=lowering_result.shape_poly_state)\n \n \n def _pmap_unmap_shaped_array(\n"
        },
        {
            "name": "maps.py",
            "path": "jax/_src/maps.py",
            "patches": [
                {
                    "old_start": 617,
                    "old_length": 7,
                    "new_start": 617,
                    "new_length": 7,
                    "hunk": "@@ -617,7 +617,7 @@ def xmap(fun: Callable,\n         '_experimental_lowering_platform', mlir.LoweringParameters())\n     fun_flat, args_flat, params, in_tree, out_tree = infer_params(*args)\n     avals_flat = [shaped_abstractify(arg) for arg in args_flat]\n-    computation, jaxpr = make_xmap_callable(\n+    computation = make_xmap_callable(\n         fun_flat, params['name'], params['in_axes'], params['out_axes_thunk'],\n         params['donated_invars'], params['global_axis_sizes'], params['axis_resources'],\n         params['resource_env'], params['backend'], params['spmd_in_axes'],\n"
                },
                {
                    "old_start": 627,
                    "old_length": 8,
                    "new_start": 627,
                    "new_length": 7,
                    "hunk": "@@ -627,8 +627,7 @@ def xmap(fun: Callable,\n     in_tree = treedef_tuple([in_tree, tree_flatten({})[1]])\n     in_avals = in_tree.unflatten(avals_flat)\n     return stages.Lowered.from_flat_info(\n-        computation, in_tree, in_avals, donate_argnums, out_tree(),\n-        no_kwargs=True, fun_name=params['name'], jaxpr=jaxpr)\n+        computation, in_tree, in_avals, donate_argnums, out_tree())\n \n   fun_mapped.lower = lower\n   return type_cast(stages.Wrapped, fun_mapped)\n"
                },
                {
                    "old_start": 637,
                    "old_length": 7,
                    "new_start": 636,
                    "new_length": 7,
                    "hunk": "@@ -637,7 +636,7 @@ def xmap_impl(fun: lu.WrappedFun, *args, name, in_axes, out_axes_thunk, donated_\n               global_axis_sizes, axis_resources, resource_env, backend,\n               spmd_in_axes, spmd_out_axes_thunk):\n   in_avals = [core.raise_to_shaped(core.get_aval(arg)) for arg in args]\n-  computation, _ = make_xmap_callable(\n+  computation = make_xmap_callable(\n       fun, name, in_axes, out_axes_thunk, donated_invars, global_axis_sizes,\n       axis_resources, resource_env, backend,\n       spmd_in_axes, spmd_out_axes_thunk,\n"
                },
                {
                    "old_start": 709,
                    "old_length": 7,
                    "new_start": 708,
                    "new_length": 7,
                    "hunk": "@@ -709,7 +708,7 @@ def make_xmap_callable(fun: lu.WrappedFun,\n         in_shardings, out_shardings, donated_invars,\n         use_spmd_lowering, in_avals,\n         tiling_method=tiling_method,\n-        lowering_parameters=lowering_parameters), jaxpr\n+        lowering_parameters=lowering_parameters)\n   else:\n     jaxpr, out_avals, consts = pe.trace_to_jaxpr_final(f, in_avals)\n     return pxla.lower_sharding_computation(\n"
                },
                {
                    "old_start": 717,
                    "old_length": 7,
                    "new_start": 716,
                    "new_length": 7,
                    "hunk": "@@ -717,7 +716,7 @@ def make_xmap_callable(fun: lu.WrappedFun,\n         (UNSPECIFIED,) * len(in_avals), (UNSPECIFIED,) * len(out_avals),\n         (None,) * len(in_avals), (None,) * len(out_avals),\n         donated_invars, keep_unused=True, inline=False,\n-        devices_from_context=None, lowering_parameters=lowering_parameters), jaxpr\n+        devices_from_context=None, lowering_parameters=lowering_parameters)\n \n \n class EvaluationPlan(NamedTuple):\n"
                }
            ],
            "whole_deleted": "-    computation, jaxpr = make_xmap_callable(\n-        computation, in_tree, in_avals, donate_argnums, out_tree(),\n-        no_kwargs=True, fun_name=params['name'], jaxpr=jaxpr)\n-  computation, _ = make_xmap_callable(\n-        lowering_parameters=lowering_parameters), jaxpr\n-        devices_from_context=None, lowering_parameters=lowering_parameters), jaxpr\n",
            "whole_added": "+    computation = make_xmap_callable(\n+        computation, in_tree, in_avals, donate_argnums, out_tree())\n+  computation = make_xmap_callable(\n+        lowering_parameters=lowering_parameters)\n+        devices_from_context=None, lowering_parameters=lowering_parameters)\n",
            "whole_hunk": "@@ -617,7 +617,7 @@ def xmap(fun: Callable,\n         '_experimental_lowering_platform', mlir.LoweringParameters())\n     fun_flat, args_flat, params, in_tree, out_tree = infer_params(*args)\n     avals_flat = [shaped_abstractify(arg) for arg in args_flat]\n-    computation, jaxpr = make_xmap_callable(\n+    computation = make_xmap_callable(\n         fun_flat, params['name'], params['in_axes'], params['out_axes_thunk'],\n         params['donated_invars'], params['global_axis_sizes'], params['axis_resources'],\n         params['resource_env'], params['backend'], params['spmd_in_axes'],\n@@ -627,8 +627,7 @@ def xmap(fun: Callable,\n     in_tree = treedef_tuple([in_tree, tree_flatten({})[1]])\n     in_avals = in_tree.unflatten(avals_flat)\n     return stages.Lowered.from_flat_info(\n-        computation, in_tree, in_avals, donate_argnums, out_tree(),\n-        no_kwargs=True, fun_name=params['name'], jaxpr=jaxpr)\n+        computation, in_tree, in_avals, donate_argnums, out_tree())\n \n   fun_mapped.lower = lower\n   return type_cast(stages.Wrapped, fun_mapped)\n@@ -637,7 +636,7 @@ def xmap_impl(fun: lu.WrappedFun, *args, name, in_axes, out_axes_thunk, donated_\n               global_axis_sizes, axis_resources, resource_env, backend,\n               spmd_in_axes, spmd_out_axes_thunk):\n   in_avals = [core.raise_to_shaped(core.get_aval(arg)) for arg in args]\n-  computation, _ = make_xmap_callable(\n+  computation = make_xmap_callable(\n       fun, name, in_axes, out_axes_thunk, donated_invars, global_axis_sizes,\n       axis_resources, resource_env, backend,\n       spmd_in_axes, spmd_out_axes_thunk,\n@@ -709,7 +708,7 @@ def make_xmap_callable(fun: lu.WrappedFun,\n         in_shardings, out_shardings, donated_invars,\n         use_spmd_lowering, in_avals,\n         tiling_method=tiling_method,\n-        lowering_parameters=lowering_parameters), jaxpr\n+        lowering_parameters=lowering_parameters)\n   else:\n     jaxpr, out_avals, consts = pe.trace_to_jaxpr_final(f, in_avals)\n     return pxla.lower_sharding_computation(\n@@ -717,7 +716,7 @@ def make_xmap_callable(fun: lu.WrappedFun,\n         (UNSPECIFIED,) * len(in_avals), (UNSPECIFIED,) * len(out_avals),\n         (None,) * len(in_avals), (None,) * len(out_avals),\n         donated_invars, keep_unused=True, inline=False,\n-        devices_from_context=None, lowering_parameters=lowering_parameters), jaxpr\n+        devices_from_context=None, lowering_parameters=lowering_parameters)\n \n \n class EvaluationPlan(NamedTuple):\n"
        },
        {
            "name": "pjit.py",
            "path": "jax/_src/pjit.py",
            "patches": [
                {
                    "old_start": 498,
                    "old_length": 8,
                    "new_start": 498,
                    "new_length": 7,
                    "hunk": "@@ -498,8 +498,7 @@ def _make_jit_wrapper(jit_info: PjitInfo):\n     donate_argnums = tuple(i for i, d in enumerate(donated_invars) if d)\n     jaxpr = params[\"jaxpr\"]\n     return stages.Lowered.from_flat_info(\n-        lowering, in_tree, jaxpr.in_avals, donate_argnums, out_tree,\n-        fun_name=params[\"name\"], jaxpr=jaxpr)\n+        lowering, in_tree, jaxpr.in_avals, donate_argnums, out_tree)\n \n   @api_boundary\n   def eval_shape(*args, **kwargs):\n"
                }
            ],
            "whole_deleted": "-        lowering, in_tree, jaxpr.in_avals, donate_argnums, out_tree,\n-        fun_name=params[\"name\"], jaxpr=jaxpr)\n",
            "whole_added": "+        lowering, in_tree, jaxpr.in_avals, donate_argnums, out_tree)\n",
            "whole_hunk": "@@ -498,8 +498,7 @@ def _make_jit_wrapper(jit_info: PjitInfo):\n     donate_argnums = tuple(i for i, d in enumerate(donated_invars) if d)\n     jaxpr = params[\"jaxpr\"]\n     return stages.Lowered.from_flat_info(\n-        lowering, in_tree, jaxpr.in_avals, donate_argnums, out_tree,\n-        fun_name=params[\"name\"], jaxpr=jaxpr)\n+        lowering, in_tree, jaxpr.in_avals, donate_argnums, out_tree)\n \n   @api_boundary\n   def eval_shape(*args, **kwargs):\n"
        },
        {
            "name": "stages.py",
            "path": "jax/_src/stages.py",
            "patches": [
                {
                    "old_start": 643,
                    "old_length": 30,
                    "new_start": 643,
                    "new_length": 23,
                    "hunk": "@@ -643,30 +643,23 @@ class Lowered(Stage):\n   querying properties of lowered computations across JAX's various\n   lowering paths (:func:`~jax.jit`, :func:`~jax.pmap`, etc.).\n   \"\"\"\n-  __slots__ = [\"_lowering\", \"args_info\", \"out_tree\", \"_no_kwargs\", \"_fun_name\", \"_jaxpr\"]\n+  __slots__ = [\"_lowering\", \"args_info\", \"out_tree\", \"_no_kwargs\"]\n   _lowering: XlaLowering\n   args_info: Any                # PyTree of ArgInfo\n   out_tree: tree_util.PyTreeDef\n   _no_kwargs: bool\n-  _fun_name: str\n-  _jaxpr: core.ClosedJaxpr | None  # Can be None when this class is constructed\n-                                   # outside of JAX core.\n \n   def __init__(\n       self,\n       lowering: XlaLowering,\n       args_info,  # PyTree of ArgInfo\n       out_tree: tree_util.PyTreeDef,\n-      no_kwargs: bool = False,\n-      fun_name: str = \"<unnamed function>\",\n-      jaxpr: core.ClosedJaxpr | None = None):\n+      no_kwargs: bool = False):\n \n     self._lowering = lowering\n     self.args_info = args_info\n     self.out_tree = out_tree\n     self._no_kwargs = no_kwargs\n-    self._fun_name = fun_name\n-    self._jaxpr = jaxpr\n \n   @classmethod\n   def from_flat_info(cls,\n"
                },
                {
                    "old_start": 675,
                    "old_length": 9,
                    "new_start": 668,
                    "new_length": 7,
                    "hunk": "@@ -675,9 +668,7 @@ class Lowered(Stage):\n                      in_avals,\n                      donate_argnums: tuple[int, ...],\n                      out_tree: tree_util.PyTreeDef,\n-                     no_kwargs: bool = False,\n-                     fun_name: str = \"<unnamed function>\",\n-                     jaxpr: core.ClosedJaxpr | None = None):\n+                     no_kwargs: bool = False):\n     \"\"\"Initialize from flat info (``in_avals`` etc.) and an input PyTreeDef.\n \n     Args:\n"
                },
                {
                    "old_start": 686,
                    "old_length": 15,
                    "new_start": 677,
                    "new_length": 12,
                    "hunk": "@@ -686,15 +677,12 @@ class Lowered(Stage):\n       no_kwargs: If ``True`` the transformation, and the\n         ``Compiled`` returned from this object will not support keyword\n         arguments (an error will be raised if some are provided).\n-      fun_name: the name of the lowered function.\n-      jaxpr: the Jaxpr of the lowered function. The value `None` is for\n-        backwards compatibility, and is used only outside JAX core.\n     \"\"\"\n     return cls(\n         lowering,\n         make_args_info(in_tree, in_avals, donate_argnums),\n         out_tree,\n-        no_kwargs=no_kwargs, fun_name=fun_name, jaxpr=jaxpr)\n+        no_kwargs=no_kwargs)\n \n   @property\n   def out_info(self):  # PyTree of OutInfo\n"
                }
            ],
            "whole_deleted": "-  __slots__ = [\"_lowering\", \"args_info\", \"out_tree\", \"_no_kwargs\", \"_fun_name\", \"_jaxpr\"]\n-  _fun_name: str\n-  _jaxpr: core.ClosedJaxpr | None  # Can be None when this class is constructed\n-                                   # outside of JAX core.\n-      no_kwargs: bool = False,\n-      fun_name: str = \"<unnamed function>\",\n-      jaxpr: core.ClosedJaxpr | None = None):\n-    self._fun_name = fun_name\n-    self._jaxpr = jaxpr\n-                     no_kwargs: bool = False,\n-                     fun_name: str = \"<unnamed function>\",\n-                     jaxpr: core.ClosedJaxpr | None = None):\n-      fun_name: the name of the lowered function.\n-      jaxpr: the Jaxpr of the lowered function. The value `None` is for\n-        backwards compatibility, and is used only outside JAX core.\n-        no_kwargs=no_kwargs, fun_name=fun_name, jaxpr=jaxpr)\n",
            "whole_added": "+  __slots__ = [\"_lowering\", \"args_info\", \"out_tree\", \"_no_kwargs\"]\n+      no_kwargs: bool = False):\n+                     no_kwargs: bool = False):\n+        no_kwargs=no_kwargs)\n",
            "whole_hunk": "@@ -643,30 +643,23 @@ class Lowered(Stage):\n   querying properties of lowered computations across JAX's various\n   lowering paths (:func:`~jax.jit`, :func:`~jax.pmap`, etc.).\n   \"\"\"\n-  __slots__ = [\"_lowering\", \"args_info\", \"out_tree\", \"_no_kwargs\", \"_fun_name\", \"_jaxpr\"]\n+  __slots__ = [\"_lowering\", \"args_info\", \"out_tree\", \"_no_kwargs\"]\n   _lowering: XlaLowering\n   args_info: Any                # PyTree of ArgInfo\n   out_tree: tree_util.PyTreeDef\n   _no_kwargs: bool\n-  _fun_name: str\n-  _jaxpr: core.ClosedJaxpr | None  # Can be None when this class is constructed\n-                                   # outside of JAX core.\n \n   def __init__(\n       self,\n       lowering: XlaLowering,\n       args_info,  # PyTree of ArgInfo\n       out_tree: tree_util.PyTreeDef,\n-      no_kwargs: bool = False,\n-      fun_name: str = \"<unnamed function>\",\n-      jaxpr: core.ClosedJaxpr | None = None):\n+      no_kwargs: bool = False):\n \n     self._lowering = lowering\n     self.args_info = args_info\n     self.out_tree = out_tree\n     self._no_kwargs = no_kwargs\n-    self._fun_name = fun_name\n-    self._jaxpr = jaxpr\n \n   @classmethod\n   def from_flat_info(cls,\n@@ -675,9 +668,7 @@ class Lowered(Stage):\n                      in_avals,\n                      donate_argnums: tuple[int, ...],\n                      out_tree: tree_util.PyTreeDef,\n-                     no_kwargs: bool = False,\n-                     fun_name: str = \"<unnamed function>\",\n-                     jaxpr: core.ClosedJaxpr | None = None):\n+                     no_kwargs: bool = False):\n     \"\"\"Initialize from flat info (``in_avals`` etc.) and an input PyTreeDef.\n \n     Args:\n@@ -686,15 +677,12 @@ class Lowered(Stage):\n       no_kwargs: If ``True`` the transformation, and the\n         ``Compiled`` returned from this object will not support keyword\n         arguments (an error will be raised if some are provided).\n-      fun_name: the name of the lowered function.\n-      jaxpr: the Jaxpr of the lowered function. The value `None` is for\n-        backwards compatibility, and is used only outside JAX core.\n     \"\"\"\n     return cls(\n         lowering,\n         make_args_info(in_tree, in_avals, donate_argnums),\n         out_tree,\n-        no_kwargs=no_kwargs, fun_name=fun_name, jaxpr=jaxpr)\n+        no_kwargs=no_kwargs)\n \n   @property\n   def out_info(self):  # PyTree of OutInfo\n"
        },
        {
            "name": "_export.py",
            "path": "jax/experimental/export/_export.py",
            "patches": [
                {
                    "old_start": 433,
                    "old_length": 15,
                    "new_start": 433,
                    "new_length": 17,
                    "hunk": "@@ -433,15 +433,17 @@ def export(fun_jax: Callable,\n   \"\"\"\n \n   def do_export(*args_specs, **kwargs_specs) -> Exported:\n-    if not hasattr(fun_jax, \"lower\"):\n+    if hasattr(fun_jax, \"lower\"):\n+      # If we have a pjit or pmap already we do not wrap with another, and we\n+      # allow shardings.\n+      wrapped_fun_jax = fun_jax\n+    else:\n       # We support convert(pjit(f_jax)) and convert(jit(f_jax)) but also\n       # convert(f_jax), in which case a \"jit\" is implied. In that case we raise\n       # an error if the lowered function contains non-replicated sharding annotations.\n       wrapped_fun_jax = jax.jit(fun_jax)\n-    else:\n-      # If we have a pjit or pmap already we do not wrap with another, and we\n-      # allow shardings.\n-      wrapped_fun_jax = fun_jax  # type: ignore\n+\n+    has_specialize = hasattr(wrapped_fun_jax, \"specialize\")\n \n     if lowering_platforms is not None:\n       actual_lowering_platforms = tuple(lowering_platforms)\n"
                },
                {
                    "old_start": 464,
                    "old_length": 19,
                    "new_start": 466,
                    "new_length": 32,
                    "hunk": "@@ -464,19 +466,32 @@ def export(fun_jax: Callable,\n               self_descr=f\"current (from {shape_poly.args_kwargs_path_to_str(symbolic_scope[1])}) \",\n               other_descr=shape_poly.args_kwargs_path_to_str(k_path))\n \n-    lowered = wrapped_fun_jax.lower(\n-        *args_specs, **kwargs_specs,\n-        _experimental_lowering_parameters=mlir.LoweringParameters(\n+    if has_specialize:\n+      specialized = wrapped_fun_jax.specialize(\n+          *args_specs, **kwargs_specs,\n+          _experimental_lowering_parameters=mlir.LoweringParameters(\n             platforms=actual_lowering_platforms,\n             for_export=True,\n-        ))\n+          ))\n+      jaxpr, fun_name = specialized.jaxpr, specialized.fun_name\n+      lowered = specialized.lower()\n+    else:\n+      lowered = wrapped_fun_jax.lower(\n+          *args_specs, **kwargs_specs,\n+          _experimental_lowering_parameters=mlir.LoweringParameters(\n+              platforms=actual_lowering_platforms,\n+              for_export=True,\n+          ))\n+      jaxpr, fun_name = None, util.fun_name(wrapped_fun_jax)\n     return _export_lowered(\n-        lowered, disabled_checks=disabled_checks,\n+        lowered, jaxpr, fun_name,\n+        disabled_checks=disabled_checks,\n         _device_assignment_for_internal_jax2tf_use_only=_device_assignment_for_internal_jax2tf_use_only)\n   return do_export\n \n def _export_lowered(\n     lowered: stages.Lowered,\n+    jaxpr: core.ClosedJaxpr, fun_name: str,\n     disabled_checks: Sequence[DisabledSafetyCheck] = (),\n     _device_assignment_for_internal_jax2tf_use_only = None,\n   ) -> Exported:\n"
                },
                {
                    "old_start": 563,
                    "old_length": 8,
                    "new_start": 578,
                    "new_length": 8,
                    "hunk": "@@ -563,8 +578,8 @@ def _export_lowered(\n   def _get_exported_vjp(exp_primal: Exported) -> Exported:\n     # Turn the primal jaxpr into a function, in preparation for exporting\n     # the VJP. Note that jaxpr_as_fun produces a function with flat arguments\n-    assert(lowered._jaxpr is not None)  # None only when the lowered was created outside JAX\n-    fun_jax = core.jaxpr_as_fun(lowered._jaxpr)\n+    assert(jaxpr is not None)  # None only when the lowered was created outside JAX\n+    fun_jax = core.jaxpr_as_fun(jaxpr)\n \n     fun_vjp_jax, vjp_in_avals = _get_vjp_fun(fun_jax,\n                                              in_tree=exp_primal.in_tree,\n"
                },
                {
                    "old_start": 580,
                    "old_length": 7,
                    "new_start": 595,
                    "new_length": 7,
                    "hunk": "@@ -580,7 +595,7 @@ def _export_lowered(\n                   disabled_checks=exp_primal.disabled_safety_checks)(*vjp_in_avals)\n \n   return Exported(\n-      fun_name=lowered._fun_name,\n+      fun_name=fun_name,\n       in_tree=lowered.in_tree,\n       out_tree=lowered.out_tree,\n       in_avals=tuple(args_avals_flat),\n"
                }
            ],
            "whole_deleted": "-    if not hasattr(fun_jax, \"lower\"):\n-    else:\n-      # If we have a pjit or pmap already we do not wrap with another, and we\n-      # allow shardings.\n-      wrapped_fun_jax = fun_jax  # type: ignore\n-    lowered = wrapped_fun_jax.lower(\n-        *args_specs, **kwargs_specs,\n-        _experimental_lowering_parameters=mlir.LoweringParameters(\n-        ))\n-        lowered, disabled_checks=disabled_checks,\n-    assert(lowered._jaxpr is not None)  # None only when the lowered was created outside JAX\n-    fun_jax = core.jaxpr_as_fun(lowered._jaxpr)\n-      fun_name=lowered._fun_name,\n",
            "whole_added": "+    if hasattr(fun_jax, \"lower\"):\n+      # If we have a pjit or pmap already we do not wrap with another, and we\n+      # allow shardings.\n+      wrapped_fun_jax = fun_jax\n+    else:\n+\n+    has_specialize = hasattr(wrapped_fun_jax, \"specialize\")\n+    if has_specialize:\n+      specialized = wrapped_fun_jax.specialize(\n+          *args_specs, **kwargs_specs,\n+          _experimental_lowering_parameters=mlir.LoweringParameters(\n+          ))\n+      jaxpr, fun_name = specialized.jaxpr, specialized.fun_name\n+      lowered = specialized.lower()\n+    else:\n+      lowered = wrapped_fun_jax.lower(\n+          *args_specs, **kwargs_specs,\n+          _experimental_lowering_parameters=mlir.LoweringParameters(\n+              platforms=actual_lowering_platforms,\n+              for_export=True,\n+          ))\n+      jaxpr, fun_name = None, util.fun_name(wrapped_fun_jax)\n+        lowered, jaxpr, fun_name,\n+        disabled_checks=disabled_checks,\n+    jaxpr: core.ClosedJaxpr, fun_name: str,\n+    assert(jaxpr is not None)  # None only when the lowered was created outside JAX\n+    fun_jax = core.jaxpr_as_fun(jaxpr)\n+      fun_name=fun_name,\n",
            "whole_hunk": "@@ -433,15 +433,17 @@ def export(fun_jax: Callable,\n   \"\"\"\n \n   def do_export(*args_specs, **kwargs_specs) -> Exported:\n-    if not hasattr(fun_jax, \"lower\"):\n+    if hasattr(fun_jax, \"lower\"):\n+      # If we have a pjit or pmap already we do not wrap with another, and we\n+      # allow shardings.\n+      wrapped_fun_jax = fun_jax\n+    else:\n       # We support convert(pjit(f_jax)) and convert(jit(f_jax)) but also\n       # convert(f_jax), in which case a \"jit\" is implied. In that case we raise\n       # an error if the lowered function contains non-replicated sharding annotations.\n       wrapped_fun_jax = jax.jit(fun_jax)\n-    else:\n-      # If we have a pjit or pmap already we do not wrap with another, and we\n-      # allow shardings.\n-      wrapped_fun_jax = fun_jax  # type: ignore\n+\n+    has_specialize = hasattr(wrapped_fun_jax, \"specialize\")\n \n     if lowering_platforms is not None:\n       actual_lowering_platforms = tuple(lowering_platforms)\n@@ -464,19 +466,32 @@ def export(fun_jax: Callable,\n               self_descr=f\"current (from {shape_poly.args_kwargs_path_to_str(symbolic_scope[1])}) \",\n               other_descr=shape_poly.args_kwargs_path_to_str(k_path))\n \n-    lowered = wrapped_fun_jax.lower(\n-        *args_specs, **kwargs_specs,\n-        _experimental_lowering_parameters=mlir.LoweringParameters(\n+    if has_specialize:\n+      specialized = wrapped_fun_jax.specialize(\n+          *args_specs, **kwargs_specs,\n+          _experimental_lowering_parameters=mlir.LoweringParameters(\n             platforms=actual_lowering_platforms,\n             for_export=True,\n-        ))\n+          ))\n+      jaxpr, fun_name = specialized.jaxpr, specialized.fun_name\n+      lowered = specialized.lower()\n+    else:\n+      lowered = wrapped_fun_jax.lower(\n+          *args_specs, **kwargs_specs,\n+          _experimental_lowering_parameters=mlir.LoweringParameters(\n+              platforms=actual_lowering_platforms,\n+              for_export=True,\n+          ))\n+      jaxpr, fun_name = None, util.fun_name(wrapped_fun_jax)\n     return _export_lowered(\n-        lowered, disabled_checks=disabled_checks,\n+        lowered, jaxpr, fun_name,\n+        disabled_checks=disabled_checks,\n         _device_assignment_for_internal_jax2tf_use_only=_device_assignment_for_internal_jax2tf_use_only)\n   return do_export\n \n def _export_lowered(\n     lowered: stages.Lowered,\n+    jaxpr: core.ClosedJaxpr, fun_name: str,\n     disabled_checks: Sequence[DisabledSafetyCheck] = (),\n     _device_assignment_for_internal_jax2tf_use_only = None,\n   ) -> Exported:\n@@ -563,8 +578,8 @@ def _export_lowered(\n   def _get_exported_vjp(exp_primal: Exported) -> Exported:\n     # Turn the primal jaxpr into a function, in preparation for exporting\n     # the VJP. Note that jaxpr_as_fun produces a function with flat arguments\n-    assert(lowered._jaxpr is not None)  # None only when the lowered was created outside JAX\n-    fun_jax = core.jaxpr_as_fun(lowered._jaxpr)\n+    assert(jaxpr is not None)  # None only when the lowered was created outside JAX\n+    fun_jax = core.jaxpr_as_fun(jaxpr)\n \n     fun_vjp_jax, vjp_in_avals = _get_vjp_fun(fun_jax,\n                                              in_tree=exp_primal.in_tree,\n@@ -580,7 +595,7 @@ def _export_lowered(\n                   disabled_checks=exp_primal.disabled_safety_checks)(*vjp_in_avals)\n \n   return Exported(\n-      fun_name=lowered._fun_name,\n+      fun_name=fun_name,\n       in_tree=lowered.in_tree,\n       out_tree=lowered.out_tree,\n       in_avals=tuple(args_avals_flat),\n"
        },
        {
            "name": "sharding_test.py",
            "path": "jax/experimental/jax2tf/tests/sharding_test.py",
            "patches": [
                {
                    "old_start": 34,
                    "old_length": 7,
                    "new_start": 34,
                    "new_length": 6,
                    "hunk": "@@ -34,7 +34,6 @@ import jax\n from jax._src import compiler\n from jax._src import config\n from jax._src import maps\n-from jax._src.maps import xmap\n from jax._src import test_util as jtu\n from jax._src import xla_bridge\n from jax import lax\n"
                },
                {
                    "old_start": 56,
                    "old_length": 7,
                    "new_start": 55,
                    "new_length": 6,
                    "hunk": "@@ -56,7 +55,6 @@ config.parse_flags_with_absl()\n from jax.experimental.jax2tf.tests import tf_test_util\n \n prev_xla_flags = None\n-prev_spmd_lowering_flag = None\n \n topology = None\n \n"
                },
                {
                    "old_start": 78,
                    "old_length": 9,
                    "new_start": 76,
                    "new_length": 6,
                    "hunk": "@@ -78,9 +76,6 @@ def setUpModule():\n                                \" --xla_force_host_platform_device_count=8\")\n   # Clear any cached backends so new CPU backend will pick up the env var.\n   xla_bridge.get_backend.cache_clear()\n-  global prev_spmd_lowering_flag\n-  prev_spmd_lowering_flag = maps.SPMD_LOWERING.value\n-  config.update('experimental_xmap_spmd_lowering', True)\n \n \n def tearDownModule():\n"
                },
                {
                    "old_start": 89,
                    "old_length": 7,
                    "new_start": 84,
                    "new_length": 6,
                    "hunk": "@@ -89,7 +84,6 @@ def tearDownModule():\n   else:\n     os.environ[\"XLA_FLAGS\"] = prev_xla_flags\n   xla_bridge.get_backend.cache_clear()\n-  config.update('experimental_xmap_spmd_lowering', prev_spmd_lowering_flag)\n \n \n class ShardingTest(tf_test_util.JaxToTfTestCase):\n"
                },
                {
                    "old_start": 536,
                    "old_length": 115,
                    "new_start": 530,
                    "new_length": 6,
                    "hunk": "@@ -536,115 +530,6 @@ class ShardingTest(tf_test_util.JaxToTfTestCase):\n           \"function with sharded arguments or results must be used under a `tf.function` context\"):\n         jax2tf.convert(f_jax)(a)\n \n-  def test_xmap_basic(self):\n-    devices = np.reshape(self.devices, (1, 2))\n-    ashape = (16, 8, 5)\n-    a = np.arange(np.prod(ashape), dtype=np.float32).reshape(ashape)\n-    bshape = (2, 7)\n-    b = np.arange(np.prod(bshape), dtype=np.float32).reshape(bshape)\n-\n-    # f_jax: f32[16,8,5], f32[2,7] -> f32[16,8,10], f32[2,28]\n-    # lambda ...: f32[5], f32[7] -> f32[10], f32[28]\n-    f_jax = xmap(lambda a, b: (jnp.concatenate([a, a], axis=0) * 2.,\n-                               jnp.concatenate([b, b, b, b], axis=0) * 4.),\n-                 in_axes=({0: 'a', 1: 'b'}, ['c', ...]),\n-                 out_axes=({0: 'a', 1: 'b'}, ['c', ...]),\n-                 axis_resources={'a': 'x', 'b': 'y', 'c': 'x'})\n-\n-    @tf.function(autograph=False, jit_compile=True)\n-    def f_tf(a, b):\n-      # xmap works only with native serialization\n-      f_converted = jax2tf.convert(f_jax, native_serialization=True)\n-      if jtu.test_device_matches([\"tpu\"]):\n-        res = tf.compat.v1.tpu.rewrite(\n-            f_converted, [tf.convert_to_tensor(a), tf.convert_to_tensor(b)],\n-            device_assignment=self.device_assignment(\n-                computation_shape=[1, 1, 1, 2])\n-        )\n-        return (res[0], res[1])\n-      else:\n-        return f_converted(a, b)\n-\n-    with Mesh(devices, ('x', 'y')):\n-      res_jax = f_jax(a, b)\n-      self.assertAllClose(res_jax, (jnp.concatenate([a, a], axis=2) * 2.,\n-                                    jnp.concatenate([b, b, b, b], axis=1) * 4.))\n-      res_tf = f_tf(a, b)\n-      self.assertAllClose(res_tf, res_jax)\n-\n-      self.check_sharding(\n-          jax2tf.convert(f_jax, native_serialization=True), [a, b],\n-          checks=[\n-              (r\"f32\\[16,8,5\\].*custom_call_target.*Sharding.*sharding.*devices=\\[1,2,1\\]\", 1),\n-              # The output sharding\n-              (r\"f32\\[2,7\\].*custom_call_target.*Sharding.*sharding.*replicated\", 1),\n-              (r\"f32\\[2,28\\].*custom_call_target.*Sharding.*sharding.*replicated\", 1),\n-          ])\n-\n-  def test_xmap_collective_reduce(self):\n-    devices = np.reshape(self.devices, (1, 2))\n-    ashape = (16, 8, 5)\n-    a = np.arange(np.prod(ashape), dtype=np.float32).reshape(ashape)\n-    bshape = (2, 7)\n-    b = np.arange(np.prod(bshape), dtype=np.float32).reshape(bshape)\n-    f_jax = xmap(lambda a, b: (lax.psum(a * 2., 'a'), b * 4.),\n-                 in_axes=(['a', 'b', ...], {0: 'c'}),\n-                 out_axes=(['b', ...], {0: 'c'}),\n-                 axis_resources={'a': 'x', 'b': 'y', 'c': 'x'})\n-\n-    @tf.function(autograph=False, jit_compile=True)\n-    def f_tf(a, b):\n-      f_converted = jax2tf.convert(f_jax, native_serialization=True)\n-      if jtu.test_device_matches([\"tpu\"]):\n-        res = tf.compat.v1.tpu.rewrite(\n-            f_converted, [tf.convert_to_tensor(a), tf.convert_to_tensor(b)],\n-            device_assignment=self.device_assignment(\n-                computation_shape=[1, 1, 1, 2])\n-        )\n-        return (res[0], res[1])\n-      else:\n-        return f_converted(a, b)\n-\n-    with Mesh(devices, ('x', 'y')):\n-      res_jax = f_jax(a, b)\n-      self.assertAllClose(res_jax, ((a * 2.).sum(0), b * 4.))\n-      res_tf = f_tf(a, b)\n-      self.assertAllClose(res_tf, res_jax)\n-      self.check_sharding(\n-          jax2tf.convert(f_jax, native_serialization=True), [a, b],\n-          checks=[\n-              (r\"f32\\[16,8,5\\].*custom_call_target.*Sharding.*sharding.*devices=\\[1,2,1\\]\", 1),\n-              (r\"f32\\[2,7\\].*custom_call_target.*Sharding.*sharding.*replicated\", 2),\n-              (r\"f32\\[8,5\\].*custom_call_target.*Sharding.*sharding.*devices=\\[2,1\\]\", 1),\n-          ])\n-\n-  def test_grad_xmap(self):\n-    devices = np.reshape(self.devices, (1, 2))\n-    ashape = (16, 8, 5)\n-    a = np.arange(np.prod(ashape), dtype=np.float32).reshape(ashape)\n-\n-    # f_jax: f32[16,8,5]-> f32[16,8,10]\n-    # lambda ...: f32[5]-> f32[10]\n-    f_jax = xmap(lambda a: jnp.concatenate([a, a], axis=0) * 2.,\n-                 in_axes=({0: 'a', 1: 'b'}),\n-                 out_axes={0: 'a', 1: 'b'},\n-                 axis_resources={'a': 'x', 'b': 'y'})\n-\n-    def f_grad_tf(a, res_ct):\n-      with tf.GradientTape(persistent=True) as tape:\n-        tape.watch(a)\n-        res_tf = jax2tf.convert(f_jax, native_serialization=True)(a)\n-        return tape.gradient(res_tf, a, output_gradients=res_ct)\n-\n-    with Mesh(devices, ('x', 'y')):\n-      self.check_sharding(f_grad_tf, [a, np.concatenate([a, a], axis=2)],\n-          checks=[\n-              # Primal input and grad output\n-              (r\"f32\\[16,8,5\\].*custom_call_target.*Sharding.*sharding.*devices=\\[1,2,1\\]\", self.GEQ(2)),\n-              # Input cotangent\n-              (r\"f32\\[16,8,10\\].*custom_call_target.*Sharding.*sharding.*devices=\\[1,2,1\\]\", self.GEQ(1)),\n-          ])\n-\n   @jtu.ignore_warning(category=UserWarning,\n                       message=\"all_to_all .* are only implemented properly for TPUs and GPUs .*\")\n   def test_shmap_all_to_all(self):\n"
                }
            ],
            "whole_deleted": "-from jax._src.maps import xmap\n-prev_spmd_lowering_flag = None\n-  global prev_spmd_lowering_flag\n-  prev_spmd_lowering_flag = maps.SPMD_LOWERING.value\n-  config.update('experimental_xmap_spmd_lowering', True)\n-  config.update('experimental_xmap_spmd_lowering', prev_spmd_lowering_flag)\n-  def test_xmap_basic(self):\n-    devices = np.reshape(self.devices, (1, 2))\n-    ashape = (16, 8, 5)\n-    a = np.arange(np.prod(ashape), dtype=np.float32).reshape(ashape)\n-    bshape = (2, 7)\n-    b = np.arange(np.prod(bshape), dtype=np.float32).reshape(bshape)\n-\n-    # f_jax: f32[16,8,5], f32[2,7] -> f32[16,8,10], f32[2,28]\n-    # lambda ...: f32[5], f32[7] -> f32[10], f32[28]\n-    f_jax = xmap(lambda a, b: (jnp.concatenate([a, a], axis=0) * 2.,\n-                               jnp.concatenate([b, b, b, b], axis=0) * 4.),\n-                 in_axes=({0: 'a', 1: 'b'}, ['c', ...]),\n-                 out_axes=({0: 'a', 1: 'b'}, ['c', ...]),\n-                 axis_resources={'a': 'x', 'b': 'y', 'c': 'x'})\n-\n-    @tf.function(autograph=False, jit_compile=True)\n-    def f_tf(a, b):\n-      # xmap works only with native serialization\n-      f_converted = jax2tf.convert(f_jax, native_serialization=True)\n-      if jtu.test_device_matches([\"tpu\"]):\n-        res = tf.compat.v1.tpu.rewrite(\n-            f_converted, [tf.convert_to_tensor(a), tf.convert_to_tensor(b)],\n-            device_assignment=self.device_assignment(\n-                computation_shape=[1, 1, 1, 2])\n-        )\n-        return (res[0], res[1])\n-      else:\n-        return f_converted(a, b)\n-\n-    with Mesh(devices, ('x', 'y')):\n-      res_jax = f_jax(a, b)\n-      self.assertAllClose(res_jax, (jnp.concatenate([a, a], axis=2) * 2.,\n-                                    jnp.concatenate([b, b, b, b], axis=1) * 4.))\n-      res_tf = f_tf(a, b)\n-      self.assertAllClose(res_tf, res_jax)\n-\n-      self.check_sharding(\n-          jax2tf.convert(f_jax, native_serialization=True), [a, b],\n-          checks=[\n-              (r\"f32\\[16,8,5\\].*custom_call_target.*Sharding.*sharding.*devices=\\[1,2,1\\]\", 1),\n-              # The output sharding\n-              (r\"f32\\[2,7\\].*custom_call_target.*Sharding.*sharding.*replicated\", 1),\n-              (r\"f32\\[2,28\\].*custom_call_target.*Sharding.*sharding.*replicated\", 1),\n-          ])\n-\n-  def test_xmap_collective_reduce(self):\n-    devices = np.reshape(self.devices, (1, 2))\n-    ashape = (16, 8, 5)\n-    a = np.arange(np.prod(ashape), dtype=np.float32).reshape(ashape)\n-    bshape = (2, 7)\n-    b = np.arange(np.prod(bshape), dtype=np.float32).reshape(bshape)\n-    f_jax = xmap(lambda a, b: (lax.psum(a * 2., 'a'), b * 4.),\n-                 in_axes=(['a', 'b', ...], {0: 'c'}),\n-                 out_axes=(['b', ...], {0: 'c'}),\n-                 axis_resources={'a': 'x', 'b': 'y', 'c': 'x'})\n-\n-    @tf.function(autograph=False, jit_compile=True)\n-    def f_tf(a, b):\n-      f_converted = jax2tf.convert(f_jax, native_serialization=True)\n-      if jtu.test_device_matches([\"tpu\"]):\n-        res = tf.compat.v1.tpu.rewrite(\n-            f_converted, [tf.convert_to_tensor(a), tf.convert_to_tensor(b)],\n-            device_assignment=self.device_assignment(\n-                computation_shape=[1, 1, 1, 2])\n-        )\n-        return (res[0], res[1])\n-      else:\n-        return f_converted(a, b)\n-\n-    with Mesh(devices, ('x', 'y')):\n-      res_jax = f_jax(a, b)\n-      self.assertAllClose(res_jax, ((a * 2.).sum(0), b * 4.))\n-      res_tf = f_tf(a, b)\n-      self.assertAllClose(res_tf, res_jax)\n-      self.check_sharding(\n-          jax2tf.convert(f_jax, native_serialization=True), [a, b],\n-          checks=[\n-              (r\"f32\\[16,8,5\\].*custom_call_target.*Sharding.*sharding.*devices=\\[1,2,1\\]\", 1),\n-              (r\"f32\\[2,7\\].*custom_call_target.*Sharding.*sharding.*replicated\", 2),\n-              (r\"f32\\[8,5\\].*custom_call_target.*Sharding.*sharding.*devices=\\[2,1\\]\", 1),\n-          ])\n-\n-  def test_grad_xmap(self):\n-    devices = np.reshape(self.devices, (1, 2))\n-    ashape = (16, 8, 5)\n-    a = np.arange(np.prod(ashape), dtype=np.float32).reshape(ashape)\n-\n-    # f_jax: f32[16,8,5]-> f32[16,8,10]\n-    # lambda ...: f32[5]-> f32[10]\n-    f_jax = xmap(lambda a: jnp.concatenate([a, a], axis=0) * 2.,\n-                 in_axes=({0: 'a', 1: 'b'}),\n-                 out_axes={0: 'a', 1: 'b'},\n-                 axis_resources={'a': 'x', 'b': 'y'})\n-\n-    def f_grad_tf(a, res_ct):\n-      with tf.GradientTape(persistent=True) as tape:\n-        tape.watch(a)\n-        res_tf = jax2tf.convert(f_jax, native_serialization=True)(a)\n-        return tape.gradient(res_tf, a, output_gradients=res_ct)\n-\n-    with Mesh(devices, ('x', 'y')):\n-      self.check_sharding(f_grad_tf, [a, np.concatenate([a, a], axis=2)],\n-          checks=[\n-              # Primal input and grad output\n-              (r\"f32\\[16,8,5\\].*custom_call_target.*Sharding.*sharding.*devices=\\[1,2,1\\]\", self.GEQ(2)),\n-              # Input cotangent\n-              (r\"f32\\[16,8,10\\].*custom_call_target.*Sharding.*sharding.*devices=\\[1,2,1\\]\", self.GEQ(1)),\n-          ])\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -34,7 +34,6 @@ import jax\n from jax._src import compiler\n from jax._src import config\n from jax._src import maps\n-from jax._src.maps import xmap\n from jax._src import test_util as jtu\n from jax._src import xla_bridge\n from jax import lax\n@@ -56,7 +55,6 @@ config.parse_flags_with_absl()\n from jax.experimental.jax2tf.tests import tf_test_util\n \n prev_xla_flags = None\n-prev_spmd_lowering_flag = None\n \n topology = None\n \n@@ -78,9 +76,6 @@ def setUpModule():\n                                \" --xla_force_host_platform_device_count=8\")\n   # Clear any cached backends so new CPU backend will pick up the env var.\n   xla_bridge.get_backend.cache_clear()\n-  global prev_spmd_lowering_flag\n-  prev_spmd_lowering_flag = maps.SPMD_LOWERING.value\n-  config.update('experimental_xmap_spmd_lowering', True)\n \n \n def tearDownModule():\n@@ -89,7 +84,6 @@ def tearDownModule():\n   else:\n     os.environ[\"XLA_FLAGS\"] = prev_xla_flags\n   xla_bridge.get_backend.cache_clear()\n-  config.update('experimental_xmap_spmd_lowering', prev_spmd_lowering_flag)\n \n \n class ShardingTest(tf_test_util.JaxToTfTestCase):\n@@ -536,115 +530,6 @@ class ShardingTest(tf_test_util.JaxToTfTestCase):\n           \"function with sharded arguments or results must be used under a `tf.function` context\"):\n         jax2tf.convert(f_jax)(a)\n \n-  def test_xmap_basic(self):\n-    devices = np.reshape(self.devices, (1, 2))\n-    ashape = (16, 8, 5)\n-    a = np.arange(np.prod(ashape), dtype=np.float32).reshape(ashape)\n-    bshape = (2, 7)\n-    b = np.arange(np.prod(bshape), dtype=np.float32).reshape(bshape)\n-\n-    # f_jax: f32[16,8,5], f32[2,7] -> f32[16,8,10], f32[2,28]\n-    # lambda ...: f32[5], f32[7] -> f32[10], f32[28]\n-    f_jax = xmap(lambda a, b: (jnp.concatenate([a, a], axis=0) * 2.,\n-                               jnp.concatenate([b, b, b, b], axis=0) * 4.),\n-                 in_axes=({0: 'a', 1: 'b'}, ['c', ...]),\n-                 out_axes=({0: 'a', 1: 'b'}, ['c', ...]),\n-                 axis_resources={'a': 'x', 'b': 'y', 'c': 'x'})\n-\n-    @tf.function(autograph=False, jit_compile=True)\n-    def f_tf(a, b):\n-      # xmap works only with native serialization\n-      f_converted = jax2tf.convert(f_jax, native_serialization=True)\n-      if jtu.test_device_matches([\"tpu\"]):\n-        res = tf.compat.v1.tpu.rewrite(\n-            f_converted, [tf.convert_to_tensor(a), tf.convert_to_tensor(b)],\n-            device_assignment=self.device_assignment(\n-                computation_shape=[1, 1, 1, 2])\n-        )\n-        return (res[0], res[1])\n-      else:\n-        return f_converted(a, b)\n-\n-    with Mesh(devices, ('x', 'y')):\n-      res_jax = f_jax(a, b)\n-      self.assertAllClose(res_jax, (jnp.concatenate([a, a], axis=2) * 2.,\n-                                    jnp.concatenate([b, b, b, b], axis=1) * 4.))\n-      res_tf = f_tf(a, b)\n-      self.assertAllClose(res_tf, res_jax)\n-\n-      self.check_sharding(\n-          jax2tf.convert(f_jax, native_serialization=True), [a, b],\n-          checks=[\n-              (r\"f32\\[16,8,5\\].*custom_call_target.*Sharding.*sharding.*devices=\\[1,2,1\\]\", 1),\n-              # The output sharding\n-              (r\"f32\\[2,7\\].*custom_call_target.*Sharding.*sharding.*replicated\", 1),\n-              (r\"f32\\[2,28\\].*custom_call_target.*Sharding.*sharding.*replicated\", 1),\n-          ])\n-\n-  def test_xmap_collective_reduce(self):\n-    devices = np.reshape(self.devices, (1, 2))\n-    ashape = (16, 8, 5)\n-    a = np.arange(np.prod(ashape), dtype=np.float32).reshape(ashape)\n-    bshape = (2, 7)\n-    b = np.arange(np.prod(bshape), dtype=np.float32).reshape(bshape)\n-    f_jax = xmap(lambda a, b: (lax.psum(a * 2., 'a'), b * 4.),\n-                 in_axes=(['a', 'b', ...], {0: 'c'}),\n-                 out_axes=(['b', ...], {0: 'c'}),\n-                 axis_resources={'a': 'x', 'b': 'y', 'c': 'x'})\n-\n-    @tf.function(autograph=False, jit_compile=True)\n-    def f_tf(a, b):\n-      f_converted = jax2tf.convert(f_jax, native_serialization=True)\n-      if jtu.test_device_matches([\"tpu\"]):\n-        res = tf.compat.v1.tpu.rewrite(\n-            f_converted, [tf.convert_to_tensor(a), tf.convert_to_tensor(b)],\n-            device_assignment=self.device_assignment(\n-                computation_shape=[1, 1, 1, 2])\n-        )\n-        return (res[0], res[1])\n-      else:\n-        return f_converted(a, b)\n-\n-    with Mesh(devices, ('x', 'y')):\n-      res_jax = f_jax(a, b)\n-      self.assertAllClose(res_jax, ((a * 2.).sum(0), b * 4.))\n-      res_tf = f_tf(a, b)\n-      self.assertAllClose(res_tf, res_jax)\n-      self.check_sharding(\n-          jax2tf.convert(f_jax, native_serialization=True), [a, b],\n-          checks=[\n-              (r\"f32\\[16,8,5\\].*custom_call_target.*Sharding.*sharding.*devices=\\[1,2,1\\]\", 1),\n-              (r\"f32\\[2,7\\].*custom_call_target.*Sharding.*sharding.*replicated\", 2),\n-              (r\"f32\\[8,5\\].*custom_call_target.*Sharding.*sharding.*devices=\\[2,1\\]\", 1),\n-          ])\n-\n-  def test_grad_xmap(self):\n-    devices = np.reshape(self.devices, (1, 2))\n-    ashape = (16, 8, 5)\n-    a = np.arange(np.prod(ashape), dtype=np.float32).reshape(ashape)\n-\n-    # f_jax: f32[16,8,5]-> f32[16,8,10]\n-    # lambda ...: f32[5]-> f32[10]\n-    f_jax = xmap(lambda a: jnp.concatenate([a, a], axis=0) * 2.,\n-                 in_axes=({0: 'a', 1: 'b'}),\n-                 out_axes={0: 'a', 1: 'b'},\n-                 axis_resources={'a': 'x', 'b': 'y'})\n-\n-    def f_grad_tf(a, res_ct):\n-      with tf.GradientTape(persistent=True) as tape:\n-        tape.watch(a)\n-        res_tf = jax2tf.convert(f_jax, native_serialization=True)(a)\n-        return tape.gradient(res_tf, a, output_gradients=res_ct)\n-\n-    with Mesh(devices, ('x', 'y')):\n-      self.check_sharding(f_grad_tf, [a, np.concatenate([a, a], axis=2)],\n-          checks=[\n-              # Primal input and grad output\n-              (r\"f32\\[16,8,5\\].*custom_call_target.*Sharding.*sharding.*devices=\\[1,2,1\\]\", self.GEQ(2)),\n-              # Input cotangent\n-              (r\"f32\\[16,8,10\\].*custom_call_target.*Sharding.*sharding.*devices=\\[1,2,1\\]\", self.GEQ(1)),\n-          ])\n-\n   @jtu.ignore_warning(category=UserWarning,\n                       message=\"all_to_all .* are only implemented properly for TPUs and GPUs .*\")\n   def test_shmap_all_to_all(self):\n"
        },
        {
            "name": "export_test.py",
            "path": "tests/export_test.py",
            "patches": [
                {
                    "old_start": 556,
                    "old_length": 12,
                    "new_start": 556,
                    "new_length": 16,
                    "hunk": "@@ -556,12 +556,16 @@ class JaxExportTest(jtu.JaxTestCase):\n         return jnp.sin(x)\n \n       # This makes it look like a jitted-function\n-      def lower(self, x,\n-                _experimental_lowering_parameters=None):\n+      def lower(self, x, _experimental_lowering_parameters=None):\n         return jax.jit(self.__call__).lower(\n             x,\n             _experimental_lowering_parameters=_experimental_lowering_parameters)\n \n+      def specialize(self, x, _experimental_lowering_parameters=None):\n+        return jax.jit(self.__call__).specialize(\n+            x,\n+            _experimental_lowering_parameters=_experimental_lowering_parameters)\n+\n     a, = export.symbolic_shape(\"a,\")\n     # No error\n     _ = get_exported(MyCallable())("
                }
            ],
            "whole_deleted": "-      def lower(self, x,\n-                _experimental_lowering_parameters=None):\n",
            "whole_added": "+      def lower(self, x, _experimental_lowering_parameters=None):\n+      def specialize(self, x, _experimental_lowering_parameters=None):\n+        return jax.jit(self.__call__).specialize(\n+            x,\n+            _experimental_lowering_parameters=_experimental_lowering_parameters)\n+\n",
            "whole_hunk": "@@ -556,12 +556,16 @@ class JaxExportTest(jtu.JaxTestCase):\n         return jnp.sin(x)\n \n       # This makes it look like a jitted-function\n-      def lower(self, x,\n-                _experimental_lowering_parameters=None):\n+      def lower(self, x, _experimental_lowering_parameters=None):\n         return jax.jit(self.__call__).lower(\n             x,\n             _experimental_lowering_parameters=_experimental_lowering_parameters)\n \n+      def specialize(self, x, _experimental_lowering_parameters=None):\n+        return jax.jit(self.__call__).specialize(\n+            x,\n+            _experimental_lowering_parameters=_experimental_lowering_parameters)\n+\n     a, = export.symbolic_shape(\"a,\")\n     # No error\n     _ = get_exported(MyCallable())("
        }
    ]
},
{
    "Id": 25,
    "commit_link": "https://github.com/google/jax/commit/a46d5c2a300e12e28359df101901fb7309f3b6ce",
    "date": "2024-06-04T16:49:11-04:00",
    "message": "Simplify flaky test of grad-of-pmap cache hits\n\nAs described in https://github.com/google/jax/issues/21643, we're seeing\ntest failures in one `pmap` test under very specific circumstances. I\nhaven't been able to solve the issue, or even track down the original\nsource, since the failure has only been reproduced when running the full\ntest suite with `pytest`. Instead, this PR makes this test more lenient,\ntesting that grad-of-pmap produces the appropriate cache hits when used\na second time, rather than also checking the total number of `pmap` and\n`jit` lowerings required.",
    "changes": [
        {
            "name": "pmap_test.py",
            "path": "tests/pmap_test.py",
            "patches": [
                {
                    "old_start": 2075,
                    "old_length": 13,
                    "new_start": 2075,
                    "new_length": 10,
                    "hunk": "@@ -2075,13 +2075,10 @@ class PythonPmapTest(jtu.JaxTestCase):\n     def f(x):\n       return jnp.sin(x)\n \n+    # warm-up the cache\n     x = jnp.ones(axis_size)\n-    f(x)  # warm-up any dispatching compilations\n-\n-    with jtu.count_jit_and_pmap_compiles() as count:  # noqa: F841\n-      _, f_bwd  = jax.vjp(f, x)\n-      _ = f_bwd(x)\n-    self.assertEqual(count[0], 2)  # one for fwd, one for bwd\n+    _, f_bwd  = jax.vjp(f, x)\n+    _ = f_bwd(x)\n \n     with jtu.count_jit_and_pmap_compiles() as count:  # noqa: F841\n       _, f_bwd2  = jax.vjp(f, x)"
                }
            ],
            "whole_deleted": "-    f(x)  # warm-up any dispatching compilations\n-\n-    with jtu.count_jit_and_pmap_compiles() as count:  # noqa: F841\n-      _, f_bwd  = jax.vjp(f, x)\n-      _ = f_bwd(x)\n-    self.assertEqual(count[0], 2)  # one for fwd, one for bwd\n",
            "whole_added": "+    # warm-up the cache\n+    _, f_bwd  = jax.vjp(f, x)\n+    _ = f_bwd(x)\n",
            "whole_hunk": "@@ -2075,13 +2075,10 @@ class PythonPmapTest(jtu.JaxTestCase):\n     def f(x):\n       return jnp.sin(x)\n \n+    # warm-up the cache\n     x = jnp.ones(axis_size)\n-    f(x)  # warm-up any dispatching compilations\n-\n-    with jtu.count_jit_and_pmap_compiles() as count:  # noqa: F841\n-      _, f_bwd  = jax.vjp(f, x)\n-      _ = f_bwd(x)\n-    self.assertEqual(count[0], 2)  # one for fwd, one for bwd\n+    _, f_bwd  = jax.vjp(f, x)\n+    _ = f_bwd(x)\n \n     with jtu.count_jit_and_pmap_compiles() as count:  # noqa: F841\n       _, f_bwd2  = jax.vjp(f, x)"
        }
    ]
},
{
    "Id": 26,
    "commit_link": "https://github.com/google/jax/commit/2333d5c7c3214feb9828bb8f67fa6ea50e4a10ef",
    "date": "2024-06-04T09:58:50-07:00",
    "message": "Add validation that tests do not change global configs",
    "changes": [
        {
            "name": "test_util.py",
            "path": "jax/_src/test_util.py",
            "patches": [
                {
                    "old_start": 1021,
                    "old_length": 6,
                    "new_start": 1021,
                    "new_length": 26,
                    "hunk": "@@ -1021,6 +1021,26 @@ def config_context(**kwds):\n     config.update(key, value)\n \n \n+class NotPresent:\n+  def __repr__(self):\n+    return \"<not present>\"\n+\n+\n+@contextmanager\n+def assert_global_configs_unchanged():\n+  starting_config = jax.config.values.copy()\n+  yield\n+  ending_config = jax.config.values\n+\n+  if starting_config == ending_config:\n+    return\n+  differing = {k: (starting_config.get(k, NotPresent()), ending_config.get(k, NotPresent()))\n+                for k in (starting_config.keys() | ending_config.keys())\n+                if (k not in starting_config or k not in ending_config\n+                    or starting_config[k] != ending_config[k])}\n+  raise AssertionError(f\"Test changed global config values. Differing values are: {differing}\")\n+\n+\n class JaxTestCase(parameterized.TestCase):\n   \"\"\"Base class for JAX tests including numerical checks and boilerplate.\"\"\"\n   _default_config = {\n"
                },
                {
                    "old_start": 1040,
                    "old_length": 6,
                    "new_start": 1060,
                    "new_length": 8,
                    "hunk": "@@ -1040,6 +1060,8 @@ class JaxTestCase(parameterized.TestCase):\n \n   def setUp(self):\n     super().setUp()\n+    self.enter_context(assert_global_configs_unchanged())\n+\n     # We use the adler32 hash for two reasons.\n     # a) it is deterministic run to run, unlike hash() which is randomized.\n     # b) it returns values in int32 range, which RandomState requires.\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+class NotPresent:\n+  def __repr__(self):\n+    return \"<not present>\"\n+\n+\n+@contextmanager\n+def assert_global_configs_unchanged():\n+  starting_config = jax.config.values.copy()\n+  yield\n+  ending_config = jax.config.values\n+\n+  if starting_config == ending_config:\n+    return\n+  differing = {k: (starting_config.get(k, NotPresent()), ending_config.get(k, NotPresent()))\n+                for k in (starting_config.keys() | ending_config.keys())\n+                if (k not in starting_config or k not in ending_config\n+                    or starting_config[k] != ending_config[k])}\n+  raise AssertionError(f\"Test changed global config values. Differing values are: {differing}\")\n+\n+\n+    self.enter_context(assert_global_configs_unchanged())\n+\n",
            "whole_hunk": "@@ -1021,6 +1021,26 @@ def config_context(**kwds):\n     config.update(key, value)\n \n \n+class NotPresent:\n+  def __repr__(self):\n+    return \"<not present>\"\n+\n+\n+@contextmanager\n+def assert_global_configs_unchanged():\n+  starting_config = jax.config.values.copy()\n+  yield\n+  ending_config = jax.config.values\n+\n+  if starting_config == ending_config:\n+    return\n+  differing = {k: (starting_config.get(k, NotPresent()), ending_config.get(k, NotPresent()))\n+                for k in (starting_config.keys() | ending_config.keys())\n+                if (k not in starting_config or k not in ending_config\n+                    or starting_config[k] != ending_config[k])}\n+  raise AssertionError(f\"Test changed global config values. Differing values are: {differing}\")\n+\n+\n class JaxTestCase(parameterized.TestCase):\n   \"\"\"Base class for JAX tests including numerical checks and boilerplate.\"\"\"\n   _default_config = {\n@@ -1040,6 +1060,8 @@ class JaxTestCase(parameterized.TestCase):\n \n   def setUp(self):\n     super().setUp()\n+    self.enter_context(assert_global_configs_unchanged())\n+\n     # We use the adler32 hash for two reasons.\n     # a) it is deterministic run to run, unlike hash() which is randomized.\n     # b) it returns values in int32 range, which RandomState requires.\n"
        },
        {
            "name": "paged_attention_kernel_test.py",
            "path": "tests/pallas/paged_attention_kernel_test.py",
            "patches": [
                {
                    "old_start": 104,
                    "old_length": 12,
                    "new_start": 104,
                    "new_length": 8,
                    "hunk": "@@ -104,12 +104,8 @@ def _megacore_enabled():\n   )\n \n \n+@jtu.with_config(jax_numpy_dtype_promotion=\"standard\")\n class PagedAttentionKernelTest(jtu.JaxTestCase):\n-\n-  def setUp(self):\n-    super().setUp()\n-    self.enter_context(jax.numpy_dtype_promotion(\"standard\"))\n-\n   @parameterized.product(\n       dtype=(jnp.float32, jnp.bfloat16),\n       page_size=(16, 32, 64),"
                }
            ],
            "whole_deleted": "-\n-  def setUp(self):\n-    super().setUp()\n-    self.enter_context(jax.numpy_dtype_promotion(\"standard\"))\n-\n",
            "whole_added": "+@jtu.with_config(jax_numpy_dtype_promotion=\"standard\")\n",
            "whole_hunk": "@@ -104,12 +104,8 @@ def _megacore_enabled():\n   )\n \n \n+@jtu.with_config(jax_numpy_dtype_promotion=\"standard\")\n class PagedAttentionKernelTest(jtu.JaxTestCase):\n-\n-  def setUp(self):\n-    super().setUp()\n-    self.enter_context(jax.numpy_dtype_promotion(\"standard\"))\n-\n   @parameterized.product(\n       dtype=(jnp.float32, jnp.bfloat16),\n       page_size=(16, 32, 64),"
        }
    ]
},
{
    "Id": 27,
    "commit_link": "https://github.com/google/jax/commit/10d78279661674c11d5a75e384e8f5c791807075",
    "date": "2024-06-03T23:08:34-05:00",
    "message": "[ROCm] Add hip specific checks in threefry test",
    "changes": [
        {
            "name": "random_test.py",
            "path": "tests/random_test.py",
            "patches": [
                {
                    "old_start": 390,
                    "old_length": 10,
                    "new_start": 390,
                    "new_length": 16,
                    "hunk": "@@ -390,10 +390,16 @@ class PrngTest(jtu.JaxTestCase):\n     f = lambda key: jax.random.uniform(key, (1,))\n     with jax._src.config.threefry_gpu_kernel_lowering(False):\n       hlo_text = jax.jit(f).lower(jax.random.key(17)).as_text()\n-      self.assertNotIn(\"cu_threefry2x32\", hlo_text)\n+      if jtu.is_device_rocm():\n+        self.assertNotIn(\"hip_threefry2x32\", hlo_text)\n+      else:\n+        self.assertNotIn(\"cu_threefry2x32\", hlo_text)\n     with jax._src.config.threefry_gpu_kernel_lowering(True):\n       hlo_text = jax.jit(f).lower(jax.random.key(17)).as_text()\n-      self.assertIn(\"cu_threefry2x32\", hlo_text)\n+      if jtu.is_device_rocm():\n+        self.assertIn(\"hip_threefry2x32\", hlo_text)\n+      else:\n+        self.assertIn(\"cu_threefry2x32\", hlo_text)\n \n   @parameterized.parameters([{'make_key': ctor} for ctor in KEY_CTORS])\n   def test_random_seed_offset(self, make_key):"
                }
            ],
            "whole_deleted": "-      self.assertNotIn(\"cu_threefry2x32\", hlo_text)\n-      self.assertIn(\"cu_threefry2x32\", hlo_text)\n",
            "whole_added": "+      if jtu.is_device_rocm():\n+        self.assertNotIn(\"hip_threefry2x32\", hlo_text)\n+      else:\n+        self.assertNotIn(\"cu_threefry2x32\", hlo_text)\n+      if jtu.is_device_rocm():\n+        self.assertIn(\"hip_threefry2x32\", hlo_text)\n+      else:\n+        self.assertIn(\"cu_threefry2x32\", hlo_text)\n",
            "whole_hunk": "@@ -390,10 +390,16 @@ class PrngTest(jtu.JaxTestCase):\n     f = lambda key: jax.random.uniform(key, (1,))\n     with jax._src.config.threefry_gpu_kernel_lowering(False):\n       hlo_text = jax.jit(f).lower(jax.random.key(17)).as_text()\n-      self.assertNotIn(\"cu_threefry2x32\", hlo_text)\n+      if jtu.is_device_rocm():\n+        self.assertNotIn(\"hip_threefry2x32\", hlo_text)\n+      else:\n+        self.assertNotIn(\"cu_threefry2x32\", hlo_text)\n     with jax._src.config.threefry_gpu_kernel_lowering(True):\n       hlo_text = jax.jit(f).lower(jax.random.key(17)).as_text()\n-      self.assertIn(\"cu_threefry2x32\", hlo_text)\n+      if jtu.is_device_rocm():\n+        self.assertIn(\"hip_threefry2x32\", hlo_text)\n+      else:\n+        self.assertIn(\"cu_threefry2x32\", hlo_text)\n \n   @parameterized.parameters([{'make_key': ctor} for ctor in KEY_CTORS])\n   def test_random_seed_offset(self, make_key):"
        }
    ]
},
{
    "Id": 28,
    "commit_link": "https://github.com/google/jax/commit/84dee133ad5a7eebb0f22c5c8fab3a68068b4cef",
    "date": "2024-06-03T04:35:40-07:00",
    "message": "[Mosaic GPU] Skip tests for Python versions < 3.10\n\nWe use the match statement in Mosaic GPU, making it incompatible with Python 3.9.\nSince we're dropping support for 3.9 in about a month, it's better to skip the\nOSS tests for now (especially that we can't test with H100s) than to rewrite the\ncode.\n\nAlso add a check for having at least an sm90 GPU.\n\nPiperOrigin-RevId: 639733266",
    "changes": [
        {
            "name": "gpu_test.py",
            "path": "tests/mosaic/gpu_test.py",
            "patches": [
                {
                    "old_start": 14,
                    "old_length": 14,
                    "new_start": 14,
                    "new_length": 13,
                    "hunk": "@@ -14,14 +14,13 @@\n # ==============================================================================\n \"\"\"Tests for Mosaic GPU DSL functions and utilities.\"\"\"\n \n-import operator\n from functools import partial\n+import operator\n+import sys\n from typing import Optional\n \n from absl.testing import absltest, parameterized\n-import numpy as np\n import jax\n-import jax.numpy as jnp\n from jax._src import config\n from jax._src import test_util as jtu\n from jax._src.interpreters import mlir\n"
                },
                {
                    "old_start": 29,
                    "old_length": 7,
                    "new_start": 28,
                    "new_length": 11,
                    "hunk": "@@ -29,7 +28,11 @@ from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import arith\n from jax._src.lib.mlir.dialects import scf\n from jax._src.lib.mlir.dialects import vector\n+import jax.numpy as jnp\n+import numpy as np\n try:\n+  if sys.version_info < (3, 10):\n+    raise ImportError(\"Mosaic requires Python 3.10\")\n   import jax._src.lib.mosaic_gpu  # noqa: F401\n   HAS_MOSAIC_GPU = True\n except ImportError:\n"
                },
                {
                    "old_start": 156,
                    "old_length": 6,
                    "new_start": 159,
                    "new_length": 9,
                    "hunk": "@@ -156,6 +159,9 @@ class TestCase(parameterized.TestCase):\n   def setUp(self):\n     if not HAS_MOSAIC_GPU:\n       self.skipTest(\"jaxlib built without Mosaic GPU\")\n+    if (not jtu.test_device_matches([\"cuda\"]) or\n+        not jtu.is_cuda_compute_capability_at_least(\"9.0\")):\n+      self.skipTest(\"Only works on GPU with capability >= sm90\")\n     super().setUp()\n     self.prng = np.random.default_rng(1234)\n     self.ctx = mlir.make_ir_context()\n"
                }
            ],
            "whole_deleted": "-import operator\n-import numpy as np\n-import jax.numpy as jnp\n",
            "whole_added": "+import operator\n+import sys\n+import jax.numpy as jnp\n+import numpy as np\n+  if sys.version_info < (3, 10):\n+    raise ImportError(\"Mosaic requires Python 3.10\")\n+    if (not jtu.test_device_matches([\"cuda\"]) or\n+        not jtu.is_cuda_compute_capability_at_least(\"9.0\")):\n+      self.skipTest(\"Only works on GPU with capability >= sm90\")\n",
            "whole_hunk": "@@ -14,14 +14,13 @@\n # ==============================================================================\n \"\"\"Tests for Mosaic GPU DSL functions and utilities.\"\"\"\n \n-import operator\n from functools import partial\n+import operator\n+import sys\n from typing import Optional\n \n from absl.testing import absltest, parameterized\n-import numpy as np\n import jax\n-import jax.numpy as jnp\n from jax._src import config\n from jax._src import test_util as jtu\n from jax._src.interpreters import mlir\n@@ -29,7 +28,11 @@ from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import arith\n from jax._src.lib.mlir.dialects import scf\n from jax._src.lib.mlir.dialects import vector\n+import jax.numpy as jnp\n+import numpy as np\n try:\n+  if sys.version_info < (3, 10):\n+    raise ImportError(\"Mosaic requires Python 3.10\")\n   import jax._src.lib.mosaic_gpu  # noqa: F401\n   HAS_MOSAIC_GPU = True\n except ImportError:\n@@ -156,6 +159,9 @@ class TestCase(parameterized.TestCase):\n   def setUp(self):\n     if not HAS_MOSAIC_GPU:\n       self.skipTest(\"jaxlib built without Mosaic GPU\")\n+    if (not jtu.test_device_matches([\"cuda\"]) or\n+        not jtu.is_cuda_compute_capability_at_least(\"9.0\")):\n+      self.skipTest(\"Only works on GPU with capability >= sm90\")\n     super().setUp()\n     self.prng = np.random.default_rng(1234)\n     self.ctx = mlir.make_ir_context()\n"
        },
        {
            "name": "matmul_test.py",
            "path": "tests/mosaic/matmul_test.py",
            "patches": [
                {
                    "old_start": 15,
                    "old_length": 12,
                    "new_start": 15,
                    "new_length": 15,
                    "hunk": "@@ -15,12 +15,15 @@\n \"\"\"Test different parameterizations of a matmul.\"\"\"\n \n import os\n+import sys\n \n from absl.testing import absltest, parameterized\n from jax._src import config\n from jax._src import test_util as jtu\n import jax.numpy as jnp\n try:\n+  if sys.version_info < (3, 10):\n+    raise ImportError(\"Mosaic GPU requires Python 3.10+\")\n   # We only import this to see if Mosaic is available.\n   import jax.experimental.mosaic.gpu  # noqa: F401\n except ImportError:\n"
                },
                {
                    "old_start": 40,
                    "old_length": 6,
                    "new_start": 44,
                    "new_length": 9,
                    "hunk": "@@ -40,6 +44,9 @@ class MatmulTestCase(jtu.JaxTestCase):\n     super().setUp()\n     if matmul is None:\n       self.skipTest(\"Mosaic GPU not available.\")\n+    if (not jtu.test_device_matches([\"cuda\"]) or\n+        not jtu.is_cuda_compute_capability_at_least(\"9.0\")):\n+      self.skipTest(\"Only works on GPU with capability >= sm90\")\n \n   @parameterized.product(\n       m=(128, 256, 512, 2048),"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+import sys\n+  if sys.version_info < (3, 10):\n+    raise ImportError(\"Mosaic GPU requires Python 3.10+\")\n+    if (not jtu.test_device_matches([\"cuda\"]) or\n+        not jtu.is_cuda_compute_capability_at_least(\"9.0\")):\n+      self.skipTest(\"Only works on GPU with capability >= sm90\")\n",
            "whole_hunk": "@@ -15,12 +15,15 @@\n \"\"\"Test different parameterizations of a matmul.\"\"\"\n \n import os\n+import sys\n \n from absl.testing import absltest, parameterized\n from jax._src import config\n from jax._src import test_util as jtu\n import jax.numpy as jnp\n try:\n+  if sys.version_info < (3, 10):\n+    raise ImportError(\"Mosaic GPU requires Python 3.10+\")\n   # We only import this to see if Mosaic is available.\n   import jax.experimental.mosaic.gpu  # noqa: F401\n except ImportError:\n@@ -40,6 +44,9 @@ class MatmulTestCase(jtu.JaxTestCase):\n     super().setUp()\n     if matmul is None:\n       self.skipTest(\"Mosaic GPU not available.\")\n+    if (not jtu.test_device_matches([\"cuda\"]) or\n+        not jtu.is_cuda_compute_capability_at_least(\"9.0\")):\n+      self.skipTest(\"Only works on GPU with capability >= sm90\")\n \n   @parameterized.product(\n       m=(128, 256, 512, 2048),"
        }
    ]
},
{
    "Id": 29,
    "commit_link": "https://github.com/google/jax/commit/7c471e25335b87182e14de560f700ea98c138128",
    "date": "2024-06-03T03:33:50-07:00",
    "message": "[Pallas] Clean up MLIR compatibility check\n\nPiperOrigin-RevId: 639718864",
    "changes": [
        {
            "name": "lowering.py",
            "path": "jax/_src/pallas/mosaic/lowering.py",
            "patches": [
                {
                    "old_start": 1035,
                    "old_length": 11,
                    "new_start": 1035,
                    "new_length": 7,
                    "hunk": "@@ -1035,11 +1035,7 @@ def _reduce_max_lowering_rule(ctx: LoweringRuleContext, x, *, axes):\n \n   out_type = aval_to_ir_type(ctx.avals_out[0])\n   if jnp.issubdtype(x_aval.dtype, jnp.floating):\n-    # TODO(apaszke): Remove in 03/2024.\n-    if hasattr(vector.CombiningKind, \"MAXIMUMF\"):\n-      kind = vector.CombiningKind.MAXIMUMF\n-    else:\n-      kind = vector.CombiningKind.MAXF\n+    kind = vector.CombiningKind.MAXIMUMF\n     val = ir.FloatAttr.get(ir.F32Type.get(), float(\"-inf\"))\n     identity = ir.DenseElementsAttr.get_splat(out_type, val)\n   elif jnp.issubdtype(x_aval.dtype, jnp.signedinteger):"
                }
            ],
            "whole_deleted": "-    # TODO(apaszke): Remove in 03/2024.\n-    if hasattr(vector.CombiningKind, \"MAXIMUMF\"):\n-      kind = vector.CombiningKind.MAXIMUMF\n-    else:\n-      kind = vector.CombiningKind.MAXF\n",
            "whole_added": "+    kind = vector.CombiningKind.MAXIMUMF\n",
            "whole_hunk": "@@ -1035,11 +1035,7 @@ def _reduce_max_lowering_rule(ctx: LoweringRuleContext, x, *, axes):\n \n   out_type = aval_to_ir_type(ctx.avals_out[0])\n   if jnp.issubdtype(x_aval.dtype, jnp.floating):\n-    # TODO(apaszke): Remove in 03/2024.\n-    if hasattr(vector.CombiningKind, \"MAXIMUMF\"):\n-      kind = vector.CombiningKind.MAXIMUMF\n-    else:\n-      kind = vector.CombiningKind.MAXF\n+    kind = vector.CombiningKind.MAXIMUMF\n     val = ir.FloatAttr.get(ir.F32Type.get(), float(\"-inf\"))\n     identity = ir.DenseElementsAttr.get_splat(out_type, val)\n   elif jnp.issubdtype(x_aval.dtype, jnp.signedinteger):"
        }
    ]
},
{
    "Id": 30,
    "commit_link": "https://github.com/google/jax/commit/3984d822ba7f6ab94beb7bc7d0298c39464920d3",
    "date": "2024-05-30T20:48:11+00:00",
    "message": "add error checks for vmap spmd_axis_name",
    "changes": [
        {
            "name": "pjit.py",
            "path": "jax/_src/pjit.py",
            "patches": [
                {
                    "old_start": 2458,
                    "old_length": 6,
                    "new_start": 2458,
                    "new_length": 13,
                    "hunk": "@@ -2458,6 +2458,13 @@ mlir.register_lowering(sharding_constraint_p,\n def _sharding_constraint_batcher(insert_axis, spmd_axis_name, axis_size,\n                                  axis_name, main_type, vals_in, dims_in,\n                                  sharding, resource_env, unconstrained_dims):\n+  if spmd_axis_name is not None and isinstance(sharding, NamedSharding):\n+    used = {n for ns in sharding.spec\n+            for n in (ns if isinstance(ns, tuple) else (ns,))}\n+    if set(spmd_axis_name) & used:\n+      raise ValueError(\"vmap spmd_axis_name cannot appear in \"\n+                       \"with_sharding_constraint spec, but got spec\"\n+                       f\"{sharding}\")\n   x, = vals_in\n   d, = dims_in\n   # None means unconstrained in ParsedPartitionSpec\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  if spmd_axis_name is not None and isinstance(sharding, NamedSharding):\n+    used = {n for ns in sharding.spec\n+            for n in (ns if isinstance(ns, tuple) else (ns,))}\n+    if set(spmd_axis_name) & used:\n+      raise ValueError(\"vmap spmd_axis_name cannot appear in \"\n+                       \"with_sharding_constraint spec, but got spec\"\n+                       f\"{sharding}\")\n",
            "whole_hunk": "@@ -2458,6 +2458,13 @@ mlir.register_lowering(sharding_constraint_p,\n def _sharding_constraint_batcher(insert_axis, spmd_axis_name, axis_size,\n                                  axis_name, main_type, vals_in, dims_in,\n                                  sharding, resource_env, unconstrained_dims):\n+  if spmd_axis_name is not None and isinstance(sharding, NamedSharding):\n+    used = {n for ns in sharding.spec\n+            for n in (ns if isinstance(ns, tuple) else (ns,))}\n+    if set(spmd_axis_name) & used:\n+      raise ValueError(\"vmap spmd_axis_name cannot appear in \"\n+                       \"with_sharding_constraint spec, but got spec\"\n+                       f\"{sharding}\")\n   x, = vals_in\n   d, = dims_in\n   # None means unconstrained in ParsedPartitionSpec\n"
        },
        {
            "name": "shard_map.py",
            "path": "jax/experimental/shard_map.py",
            "patches": [
                {
                    "old_start": 1274,
                    "old_length": 6,
                    "new_start": 1274,
                    "new_length": 9,
                    "hunk": "@@ -1274,6 +1274,9 @@ def _shard_map_batch(\n                    for ax in names} for names, d in zip(in_names, in_dims)]\n   spmd_axis_name = trace.spmd_axis_name\n   if spmd_axis_name is not None:\n+    used = {n for names in in_names for ns in names.values() for n in ns}\n+    if set(spmd_axis_name) & used:\n+      raise ValueError(\"vmap spmd_axis_name cannot appear in shard_map in_specs\")\n     new_in_names = [{**ns, d:spmd_axis_name} if d is not batching.not_mapped  # type: ignore\n                     else ns for ns, d in zip(new_in_names, in_dims)]\n   @as_hashable_function(closure=out_names_thunk)\n"
                },
                {
                    "old_start": 1306,
                    "old_length": 6,
                    "new_start": 1309,
                    "new_length": 9,
                    "hunk": "@@ -1306,6 +1309,9 @@ def _batch_out_names(spmd_axis_name, dims, out_names):\n   out_names_ = [{ax + (d is not batching.not_mapped and d <= ax): names[ax]\n                   for ax in names} for names, d in zip(out_names, dims)]\n   if spmd_axis_name is not None:\n+    used = {n for names in out_names for ns in names.values() for n in ns}\n+    if set(spmd_axis_name) & used:\n+      raise ValueError(\"vmap spmd_axis_name cannot appear in shard_map out_specs\")\n     out_names_ = [{**ns, d:spmd_axis_name} if d is not batching.not_mapped\n                   else ns for ns, d in zip(out_names_, dims)]\n   return out_names_\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    used = {n for names in in_names for ns in names.values() for n in ns}\n+    if set(spmd_axis_name) & used:\n+      raise ValueError(\"vmap spmd_axis_name cannot appear in shard_map in_specs\")\n+    used = {n for names in out_names for ns in names.values() for n in ns}\n+    if set(spmd_axis_name) & used:\n+      raise ValueError(\"vmap spmd_axis_name cannot appear in shard_map out_specs\")\n",
            "whole_hunk": "@@ -1274,6 +1274,9 @@ def _shard_map_batch(\n                    for ax in names} for names, d in zip(in_names, in_dims)]\n   spmd_axis_name = trace.spmd_axis_name\n   if spmd_axis_name is not None:\n+    used = {n for names in in_names for ns in names.values() for n in ns}\n+    if set(spmd_axis_name) & used:\n+      raise ValueError(\"vmap spmd_axis_name cannot appear in shard_map in_specs\")\n     new_in_names = [{**ns, d:spmd_axis_name} if d is not batching.not_mapped  # type: ignore\n                     else ns for ns, d in zip(new_in_names, in_dims)]\n   @as_hashable_function(closure=out_names_thunk)\n@@ -1306,6 +1309,9 @@ def _batch_out_names(spmd_axis_name, dims, out_names):\n   out_names_ = [{ax + (d is not batching.not_mapped and d <= ax): names[ax]\n                   for ax in names} for names, d in zip(out_names, dims)]\n   if spmd_axis_name is not None:\n+    used = {n for names in out_names for ns in names.values() for n in ns}\n+    if set(spmd_axis_name) & used:\n+      raise ValueError(\"vmap spmd_axis_name cannot appear in shard_map out_specs\")\n     out_names_ = [{**ns, d:spmd_axis_name} if d is not batching.not_mapped\n                   else ns for ns, d in zip(out_names_, dims)]\n   return out_names_\n"
        },
        {
            "name": "pjit_test.py",
            "path": "tests/pjit_test.py",
            "patches": [
                {
                    "old_start": 1303,
                    "old_length": 6,
                    "new_start": 1303,
                    "new_length": 16,
                    "hunk": "@@ -1303,6 +1303,16 @@ class PJitTest(jtu.BufferDonationTestCase):\n             \"\"\").strip(),\n     )\n \n+  def test_with_sharding_constraint_vmap_spmd_axis_name_error(self):\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+\n+    def f(x):\n+      return jax.lax.with_sharding_constraint(x, NamedSharding(mesh, P('x')))\n+\n+    xs = jnp.arange(4 * 16.).reshape(4, 16)\n+    with self.assertRaisesRegex(ValueError, \"spmd_axis_name\"):\n+      jax.vmap(f, spmd_axis_name='x')(xs)\n+\n \n @jtu.pytest_mark_if_available('multiaccelerator')\n class CustomPartitionerTest(jtu.JaxTestCase):\n"
                },
                {
                    "old_start": 4270,
                    "old_length": 8,
                    "new_start": 4280,
                    "new_length": 7,
                    "hunk": "@@ -4270,8 +4280,7 @@ class PJitErrorTest(jtu.JaxTestCase):\n         r\".*rank at least 2, but was applied to a value of rank 1\", re.M | re.S)\n     with self.assertRaisesRegex(ValueError, error):\n       pjit(\n-          lambda x: with_sharding_constraint(x, spec),\n-          in_shardings=None,\n+          lambda x: with_sharding_constraint(x, spec), in_shardings=None,\n           out_shardings=None,\n       )(x)\n \n"
                }
            ],
            "whole_deleted": "-          lambda x: with_sharding_constraint(x, spec),\n-          in_shardings=None,\n",
            "whole_added": "+  def test_with_sharding_constraint_vmap_spmd_axis_name_error(self):\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+\n+    def f(x):\n+      return jax.lax.with_sharding_constraint(x, NamedSharding(mesh, P('x')))\n+\n+    xs = jnp.arange(4 * 16.).reshape(4, 16)\n+    with self.assertRaisesRegex(ValueError, \"spmd_axis_name\"):\n+      jax.vmap(f, spmd_axis_name='x')(xs)\n+\n+          lambda x: with_sharding_constraint(x, spec), in_shardings=None,\n",
            "whole_hunk": "@@ -1303,6 +1303,16 @@ class PJitTest(jtu.BufferDonationTestCase):\n             \"\"\").strip(),\n     )\n \n+  def test_with_sharding_constraint_vmap_spmd_axis_name_error(self):\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+\n+    def f(x):\n+      return jax.lax.with_sharding_constraint(x, NamedSharding(mesh, P('x')))\n+\n+    xs = jnp.arange(4 * 16.).reshape(4, 16)\n+    with self.assertRaisesRegex(ValueError, \"spmd_axis_name\"):\n+      jax.vmap(f, spmd_axis_name='x')(xs)\n+\n \n @jtu.pytest_mark_if_available('multiaccelerator')\n class CustomPartitionerTest(jtu.JaxTestCase):\n@@ -4270,8 +4280,7 @@ class PJitErrorTest(jtu.JaxTestCase):\n         r\".*rank at least 2, but was applied to a value of rank 1\", re.M | re.S)\n     with self.assertRaisesRegex(ValueError, error):\n       pjit(\n-          lambda x: with_sharding_constraint(x, spec),\n-          in_shardings=None,\n+          lambda x: with_sharding_constraint(x, spec), in_shardings=None,\n           out_shardings=None,\n       )(x)\n \n"
        },
        {
            "name": "shard_map_test.py",
            "path": "tests/shard_map_test.py",
            "patches": [
                {
                    "old_start": 1798,
                    "old_length": 6,
                    "new_start": 1798,
                    "new_length": 37,
                    "hunk": "@@ -1798,6 +1798,37 @@ class ShardMapTest(jtu.JaxTestCase):\n       ir.as_text()\n     )\n \n+  def test_vmap_spmd_axis_name_error(self):\n+    mesh = jtu.create_global_mesh((4, 2), ('i', 'j'))\n+\n+    @partial(\n+      shard_map,\n+      mesh=mesh,\n+      in_specs=P('i'),\n+      out_specs=P('i'),\n+      )\n+    def f(x):\n+      return jnp.sin(x)\n+\n+    xs = jnp.arange(4 * 16.).reshape(4, 16)\n+    with self.assertRaisesRegex(ValueError, \"spmd_axis_name cannot appear\"):\n+      jax.vmap(f, spmd_axis_name='i')(xs)\n+\n+    @partial(\n+      shard_map,\n+      mesh=mesh,\n+      in_specs=P('j'),\n+      out_specs=P(('i', 'j')),\n+      check_rep=False,\n+      )\n+    def g(x):\n+      return jnp.sin(x)\n+\n+    xs = jnp.arange(4 * 16.).reshape(4, 16)\n+    with self.assertRaisesRegex(ValueError, \"spmd_axis_name cannot appear\"):\n+      jax.vmap(g, spmd_axis_name='i')(xs)\n+\n+\n class FunSpec(NamedTuple):\n   name: str\n   num_inputs: int"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  def test_vmap_spmd_axis_name_error(self):\n+    mesh = jtu.create_global_mesh((4, 2), ('i', 'j'))\n+\n+    @partial(\n+      shard_map,\n+      mesh=mesh,\n+      in_specs=P('i'),\n+      out_specs=P('i'),\n+      )\n+    def f(x):\n+      return jnp.sin(x)\n+\n+    xs = jnp.arange(4 * 16.).reshape(4, 16)\n+    with self.assertRaisesRegex(ValueError, \"spmd_axis_name cannot appear\"):\n+      jax.vmap(f, spmd_axis_name='i')(xs)\n+\n+    @partial(\n+      shard_map,\n+      mesh=mesh,\n+      in_specs=P('j'),\n+      out_specs=P(('i', 'j')),\n+      check_rep=False,\n+      )\n+    def g(x):\n+      return jnp.sin(x)\n+\n+    xs = jnp.arange(4 * 16.).reshape(4, 16)\n+    with self.assertRaisesRegex(ValueError, \"spmd_axis_name cannot appear\"):\n+      jax.vmap(g, spmd_axis_name='i')(xs)\n+\n+\n",
            "whole_hunk": "@@ -1798,6 +1798,37 @@ class ShardMapTest(jtu.JaxTestCase):\n       ir.as_text()\n     )\n \n+  def test_vmap_spmd_axis_name_error(self):\n+    mesh = jtu.create_global_mesh((4, 2), ('i', 'j'))\n+\n+    @partial(\n+      shard_map,\n+      mesh=mesh,\n+      in_specs=P('i'),\n+      out_specs=P('i'),\n+      )\n+    def f(x):\n+      return jnp.sin(x)\n+\n+    xs = jnp.arange(4 * 16.).reshape(4, 16)\n+    with self.assertRaisesRegex(ValueError, \"spmd_axis_name cannot appear\"):\n+      jax.vmap(f, spmd_axis_name='i')(xs)\n+\n+    @partial(\n+      shard_map,\n+      mesh=mesh,\n+      in_specs=P('j'),\n+      out_specs=P(('i', 'j')),\n+      check_rep=False,\n+      )\n+    def g(x):\n+      return jnp.sin(x)\n+\n+    xs = jnp.arange(4 * 16.).reshape(4, 16)\n+    with self.assertRaisesRegex(ValueError, \"spmd_axis_name cannot appear\"):\n+      jax.vmap(g, spmd_axis_name='i')(xs)\n+\n+\n class FunSpec(NamedTuple):\n   name: str\n   num_inputs: int"
        }
    ]
},
{
    "Id": 31,
    "commit_link": "https://github.com/google/jax/commit/daa99025b9428fd961b184934d55b080811f0706",
    "date": "2024-05-30T07:58:59-07:00",
    "message": "Updated the JVP rule for pallas_call_p to propagate new invar indices to effects\n\nPrior to this change some of the tests in PallasTest were failing under\nJAX_ENABLE_CHECKS=1, because the effects in the JVP jaxpr did not type check.\nPiperOrigin-RevId: 638652928",
    "changes": [
        {
            "name": "pallas_call.py",
            "path": "jax/_src/pallas/pallas_call.py",
            "patches": [
                {
                    "old_start": 16,
                    "old_length": 34,
                    "new_start": 16,
                    "new_length": 34,
                    "hunk": "@@ -16,34 +16,34 @@\n from __future__ import annotations\n \n from collections.abc import Sequence\n-import itertools\n from functools import partial, reduce\n+import itertools\n from typing import Any, Callable\n \n import jax\n from jax import api_util\n-from jax import tree_util\n from jax import lax\n+from jax import tree_util\n+from jax._src import ad_util\n from jax._src import config\n+from jax._src import core as jax_core\n+from jax._src import effects\n+from jax._src import linear_util as lu\n from jax._src import state\n from jax._src.interpreters import ad\n from jax._src.interpreters import batching\n-from jax._src.interpreters import partial_eval as pe\n from jax._src.interpreters import mlir\n+from jax._src.interpreters import partial_eval as pe\n from jax._src.interpreters import xla\n-from jax._src import ad_util\n-from jax._src import core as jax_core\n-from jax._src.state import primitives as sp\n-from jax._src import linear_util as lu\n+from jax._src.pallas import core as pallas_core\n from jax._src.state import discharge as state_discharge\n+from jax._src.state import primitives as sp\n from jax._src.util import (\n     split_list, safe_map, safe_zip, weakref_lru_cache,\n     tuple_insert, partition_list, merge_lists)\n import jax.numpy as jnp\n import numpy as np\n \n-from jax._src.pallas import core as pallas_core\n-\n map, unsafe_map = safe_map, map\n zip, unsafe_zip = safe_zip, zip\n \n"
                },
                {
                    "old_start": 302,
                    "old_length": 8,
                    "new_start": 302,
                    "new_length": 14,
                    "hunk": "@@ -302,8 +302,14 @@ def _pallas_call_jvp_rule(primals, tangents, *, jaxpr, name, which_linear,\n       jvp_jaxpr.invars, [len(primals), len(out_shapes), len(tangents)]\n   )\n   invars = (*primal_refs, *tangent_refs, *primal_out_refs, *tangent_out_refs)\n-  # TODO(sharadmv): Fix state effect tracking after invar switch.\n-  jvp_jaxpr = jvp_jaxpr.replace(invars=invars)\n+  effs = []\n+  for eff in jvp_jaxpr.effects:\n+    if isinstance(eff, effects.JaxprInputEffect):\n+      eff = eff.replace(\n+          input_index=invars.index(jvp_jaxpr.invars[eff.input_index])\n+      )\n+    effs.append(eff)\n+  jvp_jaxpr = jvp_jaxpr.replace(invars=invars, effects=effs)\n   if debug:\n     print(jvp_jaxpr)\n   in_bms, out_bms = split_list(grid_mapping.block_mappings, [len(primals)])\n"
                }
            ],
            "whole_deleted": "-import itertools\n-from jax import tree_util\n-from jax._src.interpreters import partial_eval as pe\n-from jax._src import ad_util\n-from jax._src import core as jax_core\n-from jax._src.state import primitives as sp\n-from jax._src import linear_util as lu\n-from jax._src.pallas import core as pallas_core\n-\n-  # TODO(sharadmv): Fix state effect tracking after invar switch.\n-  jvp_jaxpr = jvp_jaxpr.replace(invars=invars)\n",
            "whole_added": "+import itertools\n+from jax import tree_util\n+from jax._src import ad_util\n+from jax._src import core as jax_core\n+from jax._src import effects\n+from jax._src import linear_util as lu\n+from jax._src.interpreters import partial_eval as pe\n+from jax._src.pallas import core as pallas_core\n+from jax._src.state import primitives as sp\n+  effs = []\n+  for eff in jvp_jaxpr.effects:\n+    if isinstance(eff, effects.JaxprInputEffect):\n+      eff = eff.replace(\n+          input_index=invars.index(jvp_jaxpr.invars[eff.input_index])\n+      )\n+    effs.append(eff)\n+  jvp_jaxpr = jvp_jaxpr.replace(invars=invars, effects=effs)\n",
            "whole_hunk": "@@ -16,34 +16,34 @@\n from __future__ import annotations\n \n from collections.abc import Sequence\n-import itertools\n from functools import partial, reduce\n+import itertools\n from typing import Any, Callable\n \n import jax\n from jax import api_util\n-from jax import tree_util\n from jax import lax\n+from jax import tree_util\n+from jax._src import ad_util\n from jax._src import config\n+from jax._src import core as jax_core\n+from jax._src import effects\n+from jax._src import linear_util as lu\n from jax._src import state\n from jax._src.interpreters import ad\n from jax._src.interpreters import batching\n-from jax._src.interpreters import partial_eval as pe\n from jax._src.interpreters import mlir\n+from jax._src.interpreters import partial_eval as pe\n from jax._src.interpreters import xla\n-from jax._src import ad_util\n-from jax._src import core as jax_core\n-from jax._src.state import primitives as sp\n-from jax._src import linear_util as lu\n+from jax._src.pallas import core as pallas_core\n from jax._src.state import discharge as state_discharge\n+from jax._src.state import primitives as sp\n from jax._src.util import (\n     split_list, safe_map, safe_zip, weakref_lru_cache,\n     tuple_insert, partition_list, merge_lists)\n import jax.numpy as jnp\n import numpy as np\n \n-from jax._src.pallas import core as pallas_core\n-\n map, unsafe_map = safe_map, map\n zip, unsafe_zip = safe_zip, zip\n \n@@ -302,8 +302,14 @@ def _pallas_call_jvp_rule(primals, tangents, *, jaxpr, name, which_linear,\n       jvp_jaxpr.invars, [len(primals), len(out_shapes), len(tangents)]\n   )\n   invars = (*primal_refs, *tangent_refs, *primal_out_refs, *tangent_out_refs)\n-  # TODO(sharadmv): Fix state effect tracking after invar switch.\n-  jvp_jaxpr = jvp_jaxpr.replace(invars=invars)\n+  effs = []\n+  for eff in jvp_jaxpr.effects:\n+    if isinstance(eff, effects.JaxprInputEffect):\n+      eff = eff.replace(\n+          input_index=invars.index(jvp_jaxpr.invars[eff.input_index])\n+      )\n+    effs.append(eff)\n+  jvp_jaxpr = jvp_jaxpr.replace(invars=invars, effects=effs)\n   if debug:\n     print(jvp_jaxpr)\n   in_bms, out_bms = split_list(grid_mapping.block_mappings, [len(primals)])\n"
        },
        {
            "name": "pallas_test.py",
            "path": "tests/pallas/pallas_test.py",
            "patches": [
                {
                    "old_start": 123,
                    "old_length": 7,
                    "new_start": 123,
                    "new_length": 7,
                    "hunk": "@@ -123,7 +123,7 @@ def matmul_block_spec(x, y, *, bm, bn, bk, interpret, debug=False):\n   return matmul_kernel(x, y)\n \n \n-class PallasTest(parameterized.TestCase):\n+class PallasTest(jtu.JaxTestCase):\n   INTERPRET = False\n \n   def setUp(self):\n"
                },
                {
                    "old_start": 459,
                    "old_length": 7,
                    "new_start": 459,
                    "new_length": 7,
                    "hunk": "@@ -459,7 +459,7 @@ class PallasCallTest(PallasTest):\n   )\n   def test_invalid_broadcasted_load(self, x_shape, mask_shape):\n     if self.INTERPRET:\n-      self.skipTest(\"No broadcasting checks in pl.load in interepreter mode\")\n+      self.skipTest(\"No broadcasting checks in pl.load in interpreter mode\")\n \n     @functools.partial(\n         self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32)"
                }
            ],
            "whole_deleted": "-class PallasTest(parameterized.TestCase):\n-      self.skipTest(\"No broadcasting checks in pl.load in interepreter mode\")\n",
            "whole_added": "+class PallasTest(jtu.JaxTestCase):\n+      self.skipTest(\"No broadcasting checks in pl.load in interpreter mode\")\n",
            "whole_hunk": "@@ -123,7 +123,7 @@ def matmul_block_spec(x, y, *, bm, bn, bk, interpret, debug=False):\n   return matmul_kernel(x, y)\n \n \n-class PallasTest(parameterized.TestCase):\n+class PallasTest(jtu.JaxTestCase):\n   INTERPRET = False\n \n   def setUp(self):\n@@ -459,7 +459,7 @@ class PallasCallTest(PallasTest):\n   )\n   def test_invalid_broadcasted_load(self, x_shape, mask_shape):\n     if self.INTERPRET:\n-      self.skipTest(\"No broadcasting checks in pl.load in interepreter mode\")\n+      self.skipTest(\"No broadcasting checks in pl.load in interpreter mode\")\n \n     @functools.partial(\n         self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32)"
        }
    ]
},
{
    "Id": 32,
    "commit_link": "https://github.com/google/jax/commit/cc0a20f4b1300a1366faf0ee77fda973b9f303b2",
    "date": "2024-05-29T07:29:25-07:00",
    "message": "Raise a lowering-time error when broadcasted operand has invalid shape\n\nPreviously, we let these invalid broadcasts through, which led to crashes\nin Triton compiler passes, because Triton does not have a verifier checking\nthat a tt.broadcast op is valid.\n\nPiperOrigin-RevId: 638277527",
    "changes": [
        {
            "name": "lowering.py",
            "path": "jax/_src/pallas/triton/lowering.py",
            "patches": [
                {
                    "old_start": 136,
                    "old_length": 6,
                    "new_start": 136,
                    "new_length": 10,
                    "hunk": "@@ -136,6 +136,10 @@ def _bcast_to(a: ir.Value, shape: tuple[int, ...]) -> ir.Value:\n     a_type = ir.RankedTensorType(a.type)\n     if a_type.shape == [*shape]:\n       return a\n+    if a_type.rank != len(shape) or not all(\n+        a_type.shape[i] in (dim, 1) for i, dim in enumerate(shape)\n+    ):\n+      raise ValueError(f\"Cannot broadcast from {a_type.shape} to {[*shape]}\")\n     return tt_dialect.broadcast(\n         ir.RankedTensorType.get(shape, a_type.element_type, a_type.encoding), a\n     )\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    if a_type.rank != len(shape) or not all(\n+        a_type.shape[i] in (dim, 1) for i, dim in enumerate(shape)\n+    ):\n+      raise ValueError(f\"Cannot broadcast from {a_type.shape} to {[*shape]}\")\n",
            "whole_hunk": "@@ -136,6 +136,10 @@ def _bcast_to(a: ir.Value, shape: tuple[int, ...]) -> ir.Value:\n     a_type = ir.RankedTensorType(a.type)\n     if a_type.shape == [*shape]:\n       return a\n+    if a_type.rank != len(shape) or not all(\n+        a_type.shape[i] in (dim, 1) for i, dim in enumerate(shape)\n+    ):\n+      raise ValueError(f\"Cannot broadcast from {a_type.shape} to {[*shape]}\")\n     return tt_dialect.broadcast(\n         ir.RankedTensorType.get(shape, a_type.element_type, a_type.encoding), a\n     )\n"
        },
        {
            "name": "pallas_test.py",
            "path": "tests/pallas/pallas_test.py",
            "patches": [
                {
                    "old_start": 452,
                    "old_length": 6,
                    "new_start": 452,
                    "new_length": 33,
                    "hunk": "@@ -452,6 +452,33 @@ class PallasCallTest(PallasTest):\n     x = random.normal(key, (m, n))\n     np.testing.assert_allclose(load(x), x + 1., atol=1e-5, rtol=1e-5)\n \n+  @parameterized.parameters(\n+      ((16, 32), (16,)),\n+      ((16, 32), (32,)),\n+      ((16, 32), (16, 31)),\n+  )\n+  def test_invalid_broadcasted_load(self, x_shape, mask_shape):\n+    if self.INTERPRET:\n+      self.skipTest(\"No broadcasting checks in pl.load in interepreter mode\")\n+\n+    @functools.partial(\n+        self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32)\n+    )\n+    def kernel(x_ref, mask_ref, o_ref):\n+      del o_ref  # Unused.\n+      pl.load(x_ref, slice(None), mask=mask_ref[:])\n+\n+    x = jnp.ones(x_shape, dtype=jnp.float32)\n+    mask = jnp.ones(mask_shape, dtype=jnp.bool_)\n+    # assertRaises* methods do not support inspecting the __cause__, so\n+    # we have to check it manually.\n+    try:\n+      kernel(x, mask)\n+    except Exception as e:\n+      self.assertIn(\"Cannot broadcast\", str(e.__cause__))\n+    else:\n+      self.fail(\"Expected exception due to invalid broadcasting\")\n+\n   def test_swap(self):\n     m, n = 16, 32\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  @parameterized.parameters(\n+      ((16, 32), (16,)),\n+      ((16, 32), (32,)),\n+      ((16, 32), (16, 31)),\n+  )\n+  def test_invalid_broadcasted_load(self, x_shape, mask_shape):\n+    if self.INTERPRET:\n+      self.skipTest(\"No broadcasting checks in pl.load in interepreter mode\")\n+\n+    @functools.partial(\n+        self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32)\n+    )\n+    def kernel(x_ref, mask_ref, o_ref):\n+      del o_ref  # Unused.\n+      pl.load(x_ref, slice(None), mask=mask_ref[:])\n+\n+    x = jnp.ones(x_shape, dtype=jnp.float32)\n+    mask = jnp.ones(mask_shape, dtype=jnp.bool_)\n+    # assertRaises* methods do not support inspecting the __cause__, so\n+    # we have to check it manually.\n+    try:\n+      kernel(x, mask)\n+    except Exception as e:\n+      self.assertIn(\"Cannot broadcast\", str(e.__cause__))\n+    else:\n+      self.fail(\"Expected exception due to invalid broadcasting\")\n+\n",
            "whole_hunk": "@@ -452,6 +452,33 @@ class PallasCallTest(PallasTest):\n     x = random.normal(key, (m, n))\n     np.testing.assert_allclose(load(x), x + 1., atol=1e-5, rtol=1e-5)\n \n+  @parameterized.parameters(\n+      ((16, 32), (16,)),\n+      ((16, 32), (32,)),\n+      ((16, 32), (16, 31)),\n+  )\n+  def test_invalid_broadcasted_load(self, x_shape, mask_shape):\n+    if self.INTERPRET:\n+      self.skipTest(\"No broadcasting checks in pl.load in interepreter mode\")\n+\n+    @functools.partial(\n+        self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32)\n+    )\n+    def kernel(x_ref, mask_ref, o_ref):\n+      del o_ref  # Unused.\n+      pl.load(x_ref, slice(None), mask=mask_ref[:])\n+\n+    x = jnp.ones(x_shape, dtype=jnp.float32)\n+    mask = jnp.ones(mask_shape, dtype=jnp.bool_)\n+    # assertRaises* methods do not support inspecting the __cause__, so\n+    # we have to check it manually.\n+    try:\n+      kernel(x, mask)\n+    except Exception as e:\n+      self.assertIn(\"Cannot broadcast\", str(e.__cause__))\n+    else:\n+      self.fail(\"Expected exception due to invalid broadcasting\")\n+\n   def test_swap(self):\n     m, n = 16, 32\n "
        }
    ]
},
{
    "Id": 33,
    "commit_link": "https://github.com/google/jax/commit/818e7d92a4b88d17c73e8eb4d4b26d9e265a4e0d",
    "date": "2024-05-28T13:17:28+00:00",
    "message": "Fix rel_entr behavior at boundary value",
    "changes": [
        {
            "name": "special.py",
            "path": "jax/_src/scipy/special.py",
            "patches": [
                {
                    "old_start": 672,
                    "old_length": 7,
                    "new_start": 672,
                    "new_length": 7,
                    "hunk": "@@ -672,7 +672,7 @@ def rel_entr(\n   safe_q = jnp.where(both_gt_zero_mask, q, 1)\n   log_val = lax.sub(_xlogx(safe_p), xlogy(safe_p, safe_q))\n   result = jnp.where(\n-      both_gt_zero_mask, log_val, jnp.where(one_zero_mask, q, jnp.inf)\n+      both_gt_zero_mask, log_val, jnp.where(one_zero_mask, zero, jnp.inf)\n   )\n   return result\n \n"
                }
            ],
            "whole_deleted": "-      both_gt_zero_mask, log_val, jnp.where(one_zero_mask, q, jnp.inf)\n",
            "whole_added": "+      both_gt_zero_mask, log_val, jnp.where(one_zero_mask, zero, jnp.inf)\n",
            "whole_hunk": "@@ -672,7 +672,7 @@ def rel_entr(\n   safe_q = jnp.where(both_gt_zero_mask, q, 1)\n   log_val = lax.sub(_xlogx(safe_p), xlogy(safe_p, safe_q))\n   result = jnp.where(\n-      both_gt_zero_mask, log_val, jnp.where(one_zero_mask, q, jnp.inf)\n+      both_gt_zero_mask, log_val, jnp.where(one_zero_mask, zero, jnp.inf)\n   )\n   return result\n \n"
        },
        {
            "name": "lax_scipy_special_functions_test.py",
            "path": "tests/lax_scipy_special_functions_test.py",
            "patches": [
                {
                    "old_start": 228,
                    "old_length": 6,
                    "new_start": 228,
                    "new_length": 15,
                    "hunk": "@@ -228,6 +228,15 @@ class LaxScipySpcialFunctionsTest(jtu.JaxTestCase):\n     self._CheckAgainstNumpy(osp_special.ndtri, lsp_special.ndtri, args_maker, rtol=rtol)\n     self._CompileAndCheck(lsp_special.ndtri, args_maker, rtol=rtol)\n \n+  def testRelEntrExtremeValues(self):\n+    # Testing at the extreme values (bounds (0. and 1.) and outside the bounds).\n+    dtype = jax.numpy.zeros(0).dtype  # default float dtype.\n+    args_maker = lambda: [np.array([-2, -2, -2, -1, -1, -1, 0, 0, 0]).astype(dtype),\n+                          np.array([-1, 0, 1, -1, 0, 1, -1, 0, 1]).astype(dtype)]\n+    rtol = 1E-3 if jtu.test_device_matches([\"tpu\"]) else 1e-5\n+    self._CheckAgainstNumpy(osp_special.rel_entr, lsp_special.rel_entr, args_maker, rtol=rtol)\n+    self._CompileAndCheck(lsp_special.rel_entr, args_maker, rtol=rtol)\n+\n \n if __name__ == \"__main__\":\n   absltest.main(testLoader=jtu.JaxTestLoader())"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  def testRelEntrExtremeValues(self):\n+    # Testing at the extreme values (bounds (0. and 1.) and outside the bounds).\n+    dtype = jax.numpy.zeros(0).dtype  # default float dtype.\n+    args_maker = lambda: [np.array([-2, -2, -2, -1, -1, -1, 0, 0, 0]).astype(dtype),\n+                          np.array([-1, 0, 1, -1, 0, 1, -1, 0, 1]).astype(dtype)]\n+    rtol = 1E-3 if jtu.test_device_matches([\"tpu\"]) else 1e-5\n+    self._CheckAgainstNumpy(osp_special.rel_entr, lsp_special.rel_entr, args_maker, rtol=rtol)\n+    self._CompileAndCheck(lsp_special.rel_entr, args_maker, rtol=rtol)\n+\n",
            "whole_hunk": "@@ -228,6 +228,15 @@ class LaxScipySpcialFunctionsTest(jtu.JaxTestCase):\n     self._CheckAgainstNumpy(osp_special.ndtri, lsp_special.ndtri, args_maker, rtol=rtol)\n     self._CompileAndCheck(lsp_special.ndtri, args_maker, rtol=rtol)\n \n+  def testRelEntrExtremeValues(self):\n+    # Testing at the extreme values (bounds (0. and 1.) and outside the bounds).\n+    dtype = jax.numpy.zeros(0).dtype  # default float dtype.\n+    args_maker = lambda: [np.array([-2, -2, -2, -1, -1, -1, 0, 0, 0]).astype(dtype),\n+                          np.array([-1, 0, 1, -1, 0, 1, -1, 0, 1]).astype(dtype)]\n+    rtol = 1E-3 if jtu.test_device_matches([\"tpu\"]) else 1e-5\n+    self._CheckAgainstNumpy(osp_special.rel_entr, lsp_special.rel_entr, args_maker, rtol=rtol)\n+    self._CompileAndCheck(lsp_special.rel_entr, args_maker, rtol=rtol)\n+\n \n if __name__ == \"__main__\":\n   absltest.main(testLoader=jtu.JaxTestLoader())"
        }
    ]
},
{
    "Id": 34,
    "commit_link": "https://github.com/google/jax/commit/7dbab168fd5dc9f4f97e792d6569a2e34625e947",
    "date": "2024-05-28T04:23:56+03:00",
    "message": "[export] Simplify construction of shardings for the VJP\n\nThis is possible now due to improvements in the handling\nof shardings in pjit. At the same time, re-enable the\nchecking of shardings for the arguments and results of\nthe VJP function.",
    "changes": [
        {
            "name": "_export.py",
            "path": "jax/experimental/export/_export.py",
            "patches": [
                {
                    "old_start": 899,
                    "old_length": 27,
                    "new_start": 899,
                    "new_length": 18,
                    "hunk": "@@ -899,27 +899,18 @@ def canonical_shardings(\n     device_assignment: Sequence[jax.Device],\n     in_shardings: Sequence[Sharding],\n     out_shardings: Sequence[Sharding]\n-    ) -> tuple[(pxla.UnspecifiedValue |\n-                     Sequence[sharding.XLACompatibleSharding]),\n-               (pxla.UnspecifiedValue |\n-                     Sequence[sharding.XLACompatibleSharding])]:\n+    ) -> tuple[Sequence[sharding.XLACompatibleSharding | None],\n+               Sequence[sharding.XLACompatibleSharding | None]]:\n   \"\"\"Prepares canonical in_ and out_shardings for a pjit invocation.\n \n-  The pjit front-end is picky about what in- and out-shardings it accepts,\n-  e.g., if all are unspecified then the whole sharding should be the\n-  sharding_impls.UNSPECIFIED object, otherwise the unspecified shardings are\n-  replaced with the replicated sharding.\n+  Turns the HloSharding into XLACompatibleSharding.\n \n   Returns: a pair with the canonicalized input and output shardings.\n   \"\"\"\n-  replicated_s = sharding.GSPMDSharding.get_replicated(device_assignment)\n   def canonicalize(\n-    ss: Sequence[Sharding]) -> (pxla.UnspecifiedValue |\n-                                     Sequence[sharding.XLACompatibleSharding]):\n-    if all(s is None for s in ss):\n-      return sharding_impls.UNSPECIFIED\n+    ss: Sequence[Sharding]) -> Sequence[sharding.XLACompatibleSharding | None]:\n     return tuple(\n-        sharding.GSPMDSharding(device_assignment, s) if s is not None else replicated_s\n+        sharding.GSPMDSharding(device_assignment, s) if s is not None else None\n         for s in ss)\n   return (canonicalize(in_shardings), canonicalize(out_shardings))\n \n"
                }
            ],
            "whole_deleted": "-    ) -> tuple[(pxla.UnspecifiedValue |\n-                     Sequence[sharding.XLACompatibleSharding]),\n-               (pxla.UnspecifiedValue |\n-                     Sequence[sharding.XLACompatibleSharding])]:\n-  The pjit front-end is picky about what in- and out-shardings it accepts,\n-  e.g., if all are unspecified then the whole sharding should be the\n-  sharding_impls.UNSPECIFIED object, otherwise the unspecified shardings are\n-  replaced with the replicated sharding.\n-  replicated_s = sharding.GSPMDSharding.get_replicated(device_assignment)\n-    ss: Sequence[Sharding]) -> (pxla.UnspecifiedValue |\n-                                     Sequence[sharding.XLACompatibleSharding]):\n-    if all(s is None for s in ss):\n-      return sharding_impls.UNSPECIFIED\n-        sharding.GSPMDSharding(device_assignment, s) if s is not None else replicated_s\n",
            "whole_added": "+    ) -> tuple[Sequence[sharding.XLACompatibleSharding | None],\n+               Sequence[sharding.XLACompatibleSharding | None]]:\n+  Turns the HloSharding into XLACompatibleSharding.\n+    ss: Sequence[Sharding]) -> Sequence[sharding.XLACompatibleSharding | None]:\n+        sharding.GSPMDSharding(device_assignment, s) if s is not None else None\n",
            "whole_hunk": "@@ -899,27 +899,18 @@ def canonical_shardings(\n     device_assignment: Sequence[jax.Device],\n     in_shardings: Sequence[Sharding],\n     out_shardings: Sequence[Sharding]\n-    ) -> tuple[(pxla.UnspecifiedValue |\n-                     Sequence[sharding.XLACompatibleSharding]),\n-               (pxla.UnspecifiedValue |\n-                     Sequence[sharding.XLACompatibleSharding])]:\n+    ) -> tuple[Sequence[sharding.XLACompatibleSharding | None],\n+               Sequence[sharding.XLACompatibleSharding | None]]:\n   \"\"\"Prepares canonical in_ and out_shardings for a pjit invocation.\n \n-  The pjit front-end is picky about what in- and out-shardings it accepts,\n-  e.g., if all are unspecified then the whole sharding should be the\n-  sharding_impls.UNSPECIFIED object, otherwise the unspecified shardings are\n-  replaced with the replicated sharding.\n+  Turns the HloSharding into XLACompatibleSharding.\n \n   Returns: a pair with the canonicalized input and output shardings.\n   \"\"\"\n-  replicated_s = sharding.GSPMDSharding.get_replicated(device_assignment)\n   def canonicalize(\n-    ss: Sequence[Sharding]) -> (pxla.UnspecifiedValue |\n-                                     Sequence[sharding.XLACompatibleSharding]):\n-    if all(s is None for s in ss):\n-      return sharding_impls.UNSPECIFIED\n+    ss: Sequence[Sharding]) -> Sequence[sharding.XLACompatibleSharding | None]:\n     return tuple(\n-        sharding.GSPMDSharding(device_assignment, s) if s is not None else replicated_s\n+        sharding.GSPMDSharding(device_assignment, s) if s is not None else None\n         for s in ss)\n   return (canonicalize(in_shardings), canonicalize(out_shardings))\n \n"
        },
        {
            "name": "export_test.py",
            "path": "tests/export_test.py",
            "patches": [
                {
                    "old_start": 1055,
                    "old_length": 24,
                    "new_start": 1055,
                    "new_length": 26,
                    "hunk": "@@ -1055,24 +1055,26 @@ class JaxExportTest(jtu.JaxTestCase):\n   @jtu.parameterized_filterable(\n     kwargs=[\n       dict(in_shardings=in_shardings, out_shardings=out_shardings,\n-           with_mesh=with_mesh)\n+           with_mesh_context=with_mesh_context)\n       for in_shardings in (\"missing\", None, \"P\")\n       for out_shardings in (\"missing\", None, \"P\")\n-      for with_mesh in (True, False)\n+      for with_mesh_context in (True, False)\n   ])\n   def test_grad_with_sharding(self, in_shardings=\"P\", out_shardings=None,\n-                              with_mesh=False):\n+                              with_mesh_context=False):\n     if len(jax.devices()) < 2:\n       self.skipTest(\"Test requires at least 2 devices\")\n     x_shape = (10, 20)\n     x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n+    # The input has shape f32[10,20] and output f32[20,10] in order to\n+    # distinguish them in the HLO.\n     def f_jax(x):  # x: f32[10,20] -> f32[20,10]\n       return jnp.sin(x.T)\n \n     mesh = Mesh(jax.devices()[:2], \"d\")\n     pjit_kwargs = {}\n     # Use NamedShardings if we don't have a mesh_context\n-    if with_mesh:\n+    if with_mesh_context:\n       sharding_None_d = P(None, \"d\")\n       sharding_d_None = P(\"d\", None)\n     else:\n"
                },
                {
                    "old_start": 1088,
                    "old_length": 7,
                    "new_start": 1090,
                    "new_length": 7,
                    "hunk": "@@ -1088,7 +1090,7 @@ class JaxExportTest(jtu.JaxTestCase):\n     f_jax_pjit = pjit.pjit(f_jax, **pjit_kwargs)\n \n     with contextlib.ExitStack() as stack:\n-      if with_mesh:\n+      if with_mesh_context:\n         stack.enter_context(mesh)\n       # Serialize higher-order gradiends\n       exp = get_exported(f_jax_pjit, vjp_order=2)(x)\n"
                },
                {
                    "old_start": 1098,
                    "old_length": 50,
                    "new_start": 1100,
                    "new_length": 62,
                    "hunk": "@@ -1098,50 +1100,62 @@ class JaxExportTest(jtu.JaxTestCase):\n \n     vjp_module_str = str(exp_vjp.mlir_module())\n \n+    # The MHLO attributes of the args and the result of the main function\n+    # Arg0 are the primal inputs, arg1 are the output cotangent, res is the input cotangent\n+    arg0_attrs, arg1_attrs, res_attrs = re.search(\n+        r\"func.func public @main\\(%arg0: tensor<10x20xf32> (.*)\"\n+        r\", %arg1: tensor<20x10xf32> (.*)\"\n+        r\"\\) -> \\(tensor<10x20xf32> (.*)\",  # the result\n+        vjp_module_str).groups()\n+\n     if in_shardings == \"P\":\n+      self.assertRegex(arg0_attrs, re.escape(\"{devices=[1,2]<=[2]}\"))\n+      self.assertRegex(res_attrs, re.escape(\"{devices=[1,2]<=[2]}\"))\n       primal_in_sharding = \"{devices=[1,2]<=[2]}\"\n     else:\n       primal_in_sharding = \"{replicated}\"\n+      if with_mesh_context:\n+        self.assertRegex(arg0_attrs, re.escape(\"replicated\"))\n+        self.assertRegex(res_attrs, re.escape(\"replicated\"))\n+      else:\n+        # If there is no mesh context, we have used NamedSharding(None)\n+        # and then the sharding is unspecified!\n+        self.assertNotIn(\"mhlo.sharding\", arg0_attrs)\n+        self.assertNotIn(\"mhlo.sharding\", res_attrs)\n+\n     if out_shardings == \"P\":\n+      self.assertRegex(arg1_attrs, re.escape(\"{devices=[2,1]<=[2]}\"))\n       primal_out_sharding = \"{devices=[2,1]<=[2]}\"\n     else:\n       primal_out_sharding = \"{replicated}\"\n+      if with_mesh_context:\n+        self.assertRegex(arg1_attrs, re.escape(\"replicated\"))\n+      else:\n+        self.assertNotIn(\"mhlo.sharding\", arg1_attrs)\n \n-    # TODO(b/326476605): Change the condition below if required.\n-    if in_shardings == \"P\":\n-      main = re.compile(\n-        r\"func.func public @main\\(%arg0: tensor<10x20xf32>.*\"\n-        \"mhlo.sharding = \\\"\" + re.escape(primal_in_sharding) + \"\\\"\"\n-        r\".*%arg1: tensor<20x10xf32>.*\"\n-        \"mhlo.sharding = \\\"\" + re.escape(primal_out_sharding) + \"\\\"\"\n-        # result\n-        r\".*->.*\\(tensor<10x20xf32>.*\"\n-        \"mhlo.sharding = \\\"\" + re.escape(primal_in_sharding) + \"\\\"\")\n-      self.assertRegex(vjp_module_str, main)\n-\n-    # Custom calls for the primal input shape all match primal_in_sharding\n-    primal_in_calls = re.findall(\n+    # Sharding custom calls for the primal input shape all match primal_in_sharding\n+    primal_in_sharding_calls = re.findall(\n       r\"custom_call @Sharding.*mhlo.sharding = \\\"(.+)\\\".*:.*tensor<10x20xf32>\",\n       vjp_module_str)\n     self.assertTrue(\n-      all(s == primal_in_sharding for s in primal_in_calls),\n-      primal_in_calls\n+      all(s == primal_in_sharding for s in primal_in_sharding_calls),\n+      primal_in_sharding_calls\n     )\n \n     # Custom calls for the primal output shape all match primal_out_sharding\n-    primal_out_calls = re.findall(\n+    primal_out_sharding_calls = re.findall(\n       r\"custom_call @Sharding.*mhlo.sharding = \\\"(.+)\\\".*:.*tensor<20x10xf32>\",\n       vjp_module_str)\n     self.assertTrue(\n-      all(s == primal_out_sharding for s in primal_out_calls),\n-      primal_in_calls\n+      all(s == primal_out_sharding for s in primal_out_sharding_calls),\n+      primal_out_sharding_calls\n     )\n \n     # Call the exported gradient functions. In order to set the device context\n     # we replicate the inputs. If we don't use a mesh context and there are\n     # no shardings on inputs or outputs, then we have serialized for one\n     # device.\n-    if in_shardings != \"P\" and out_shardings != \"P\" and not with_mesh:\n+    if in_shardings != \"P\" and out_shardings != \"P\" and not with_mesh_context:\n       self.assertEqual(exp_vjp.nr_devices, 1)\n       self.assertEqual(exp_vjp2.nr_devices, 1)\n       call_mesh = Mesh(jax.devices()[:1], \"e\")"
                }
            ],
            "whole_deleted": "-           with_mesh=with_mesh)\n-      for with_mesh in (True, False)\n-                              with_mesh=False):\n-    if with_mesh:\n-      if with_mesh:\n-    # TODO(b/326476605): Change the condition below if required.\n-    if in_shardings == \"P\":\n-      main = re.compile(\n-        r\"func.func public @main\\(%arg0: tensor<10x20xf32>.*\"\n-        \"mhlo.sharding = \\\"\" + re.escape(primal_in_sharding) + \"\\\"\"\n-        r\".*%arg1: tensor<20x10xf32>.*\"\n-        \"mhlo.sharding = \\\"\" + re.escape(primal_out_sharding) + \"\\\"\"\n-        # result\n-        r\".*->.*\\(tensor<10x20xf32>.*\"\n-        \"mhlo.sharding = \\\"\" + re.escape(primal_in_sharding) + \"\\\"\")\n-      self.assertRegex(vjp_module_str, main)\n-\n-    # Custom calls for the primal input shape all match primal_in_sharding\n-    primal_in_calls = re.findall(\n-      all(s == primal_in_sharding for s in primal_in_calls),\n-      primal_in_calls\n-    primal_out_calls = re.findall(\n-      all(s == primal_out_sharding for s in primal_out_calls),\n-      primal_in_calls\n-    if in_shardings != \"P\" and out_shardings != \"P\" and not with_mesh:\n",
            "whole_added": "+           with_mesh_context=with_mesh_context)\n+      for with_mesh_context in (True, False)\n+                              with_mesh_context=False):\n+    # The input has shape f32[10,20] and output f32[20,10] in order to\n+    # distinguish them in the HLO.\n+    if with_mesh_context:\n+      if with_mesh_context:\n+    # The MHLO attributes of the args and the result of the main function\n+    # Arg0 are the primal inputs, arg1 are the output cotangent, res is the input cotangent\n+    arg0_attrs, arg1_attrs, res_attrs = re.search(\n+        r\"func.func public @main\\(%arg0: tensor<10x20xf32> (.*)\"\n+        r\", %arg1: tensor<20x10xf32> (.*)\"\n+        r\"\\) -> \\(tensor<10x20xf32> (.*)\",  # the result\n+        vjp_module_str).groups()\n+\n+      self.assertRegex(arg0_attrs, re.escape(\"{devices=[1,2]<=[2]}\"))\n+      self.assertRegex(res_attrs, re.escape(\"{devices=[1,2]<=[2]}\"))\n+      if with_mesh_context:\n+        self.assertRegex(arg0_attrs, re.escape(\"replicated\"))\n+        self.assertRegex(res_attrs, re.escape(\"replicated\"))\n+      else:\n+        # If there is no mesh context, we have used NamedSharding(None)\n+        # and then the sharding is unspecified!\n+        self.assertNotIn(\"mhlo.sharding\", arg0_attrs)\n+        self.assertNotIn(\"mhlo.sharding\", res_attrs)\n+\n+      self.assertRegex(arg1_attrs, re.escape(\"{devices=[2,1]<=[2]}\"))\n+      if with_mesh_context:\n+        self.assertRegex(arg1_attrs, re.escape(\"replicated\"))\n+      else:\n+        self.assertNotIn(\"mhlo.sharding\", arg1_attrs)\n+    # Sharding custom calls for the primal input shape all match primal_in_sharding\n+    primal_in_sharding_calls = re.findall(\n+      all(s == primal_in_sharding for s in primal_in_sharding_calls),\n+      primal_in_sharding_calls\n+    primal_out_sharding_calls = re.findall(\n+      all(s == primal_out_sharding for s in primal_out_sharding_calls),\n+      primal_out_sharding_calls\n+    if in_shardings != \"P\" and out_shardings != \"P\" and not with_mesh_context:\n",
            "whole_hunk": "@@ -1055,24 +1055,26 @@ class JaxExportTest(jtu.JaxTestCase):\n   @jtu.parameterized_filterable(\n     kwargs=[\n       dict(in_shardings=in_shardings, out_shardings=out_shardings,\n-           with_mesh=with_mesh)\n+           with_mesh_context=with_mesh_context)\n       for in_shardings in (\"missing\", None, \"P\")\n       for out_shardings in (\"missing\", None, \"P\")\n-      for with_mesh in (True, False)\n+      for with_mesh_context in (True, False)\n   ])\n   def test_grad_with_sharding(self, in_shardings=\"P\", out_shardings=None,\n-                              with_mesh=False):\n+                              with_mesh_context=False):\n     if len(jax.devices()) < 2:\n       self.skipTest(\"Test requires at least 2 devices\")\n     x_shape = (10, 20)\n     x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n+    # The input has shape f32[10,20] and output f32[20,10] in order to\n+    # distinguish them in the HLO.\n     def f_jax(x):  # x: f32[10,20] -> f32[20,10]\n       return jnp.sin(x.T)\n \n     mesh = Mesh(jax.devices()[:2], \"d\")\n     pjit_kwargs = {}\n     # Use NamedShardings if we don't have a mesh_context\n-    if with_mesh:\n+    if with_mesh_context:\n       sharding_None_d = P(None, \"d\")\n       sharding_d_None = P(\"d\", None)\n     else:\n@@ -1088,7 +1090,7 @@ class JaxExportTest(jtu.JaxTestCase):\n     f_jax_pjit = pjit.pjit(f_jax, **pjit_kwargs)\n \n     with contextlib.ExitStack() as stack:\n-      if with_mesh:\n+      if with_mesh_context:\n         stack.enter_context(mesh)\n       # Serialize higher-order gradiends\n       exp = get_exported(f_jax_pjit, vjp_order=2)(x)\n@@ -1098,50 +1100,62 @@ class JaxExportTest(jtu.JaxTestCase):\n \n     vjp_module_str = str(exp_vjp.mlir_module())\n \n+    # The MHLO attributes of the args and the result of the main function\n+    # Arg0 are the primal inputs, arg1 are the output cotangent, res is the input cotangent\n+    arg0_attrs, arg1_attrs, res_attrs = re.search(\n+        r\"func.func public @main\\(%arg0: tensor<10x20xf32> (.*)\"\n+        r\", %arg1: tensor<20x10xf32> (.*)\"\n+        r\"\\) -> \\(tensor<10x20xf32> (.*)\",  # the result\n+        vjp_module_str).groups()\n+\n     if in_shardings == \"P\":\n+      self.assertRegex(arg0_attrs, re.escape(\"{devices=[1,2]<=[2]}\"))\n+      self.assertRegex(res_attrs, re.escape(\"{devices=[1,2]<=[2]}\"))\n       primal_in_sharding = \"{devices=[1,2]<=[2]}\"\n     else:\n       primal_in_sharding = \"{replicated}\"\n+      if with_mesh_context:\n+        self.assertRegex(arg0_attrs, re.escape(\"replicated\"))\n+        self.assertRegex(res_attrs, re.escape(\"replicated\"))\n+      else:\n+        # If there is no mesh context, we have used NamedSharding(None)\n+        # and then the sharding is unspecified!\n+        self.assertNotIn(\"mhlo.sharding\", arg0_attrs)\n+        self.assertNotIn(\"mhlo.sharding\", res_attrs)\n+\n     if out_shardings == \"P\":\n+      self.assertRegex(arg1_attrs, re.escape(\"{devices=[2,1]<=[2]}\"))\n       primal_out_sharding = \"{devices=[2,1]<=[2]}\"\n     else:\n       primal_out_sharding = \"{replicated}\"\n+      if with_mesh_context:\n+        self.assertRegex(arg1_attrs, re.escape(\"replicated\"))\n+      else:\n+        self.assertNotIn(\"mhlo.sharding\", arg1_attrs)\n \n-    # TODO(b/326476605): Change the condition below if required.\n-    if in_shardings == \"P\":\n-      main = re.compile(\n-        r\"func.func public @main\\(%arg0: tensor<10x20xf32>.*\"\n-        \"mhlo.sharding = \\\"\" + re.escape(primal_in_sharding) + \"\\\"\"\n-        r\".*%arg1: tensor<20x10xf32>.*\"\n-        \"mhlo.sharding = \\\"\" + re.escape(primal_out_sharding) + \"\\\"\"\n-        # result\n-        r\".*->.*\\(tensor<10x20xf32>.*\"\n-        \"mhlo.sharding = \\\"\" + re.escape(primal_in_sharding) + \"\\\"\")\n-      self.assertRegex(vjp_module_str, main)\n-\n-    # Custom calls for the primal input shape all match primal_in_sharding\n-    primal_in_calls = re.findall(\n+    # Sharding custom calls for the primal input shape all match primal_in_sharding\n+    primal_in_sharding_calls = re.findall(\n       r\"custom_call @Sharding.*mhlo.sharding = \\\"(.+)\\\".*:.*tensor<10x20xf32>\",\n       vjp_module_str)\n     self.assertTrue(\n-      all(s == primal_in_sharding for s in primal_in_calls),\n-      primal_in_calls\n+      all(s == primal_in_sharding for s in primal_in_sharding_calls),\n+      primal_in_sharding_calls\n     )\n \n     # Custom calls for the primal output shape all match primal_out_sharding\n-    primal_out_calls = re.findall(\n+    primal_out_sharding_calls = re.findall(\n       r\"custom_call @Sharding.*mhlo.sharding = \\\"(.+)\\\".*:.*tensor<20x10xf32>\",\n       vjp_module_str)\n     self.assertTrue(\n-      all(s == primal_out_sharding for s in primal_out_calls),\n-      primal_in_calls\n+      all(s == primal_out_sharding for s in primal_out_sharding_calls),\n+      primal_out_sharding_calls\n     )\n \n     # Call the exported gradient functions. In order to set the device context\n     # we replicate the inputs. If we don't use a mesh context and there are\n     # no shardings on inputs or outputs, then we have serialized for one\n     # device.\n-    if in_shardings != \"P\" and out_shardings != \"P\" and not with_mesh:\n+    if in_shardings != \"P\" and out_shardings != \"P\" and not with_mesh_context:\n       self.assertEqual(exp_vjp.nr_devices, 1)\n       self.assertEqual(exp_vjp2.nr_devices, 1)\n       call_mesh = Mesh(jax.devices()[:1], \"e\")"
        }
    ]
},
{
    "Id": 35,
    "commit_link": "https://github.com/google/jax/commit/707cd9a077ca2569c7716f0ef3bfa4d27a80eeaa",
    "date": "2024-05-23T08:13:36-07:00",
    "message": "[Mosaic GPU] More rigorous check for broadcasting.\n\nPiperOrigin-RevId: 636556148",
    "changes": [
        {
            "name": "fragmented_array.py",
            "path": "jax/experimental/mosaic/gpu/fragmented_array.py",
            "patches": [
                {
                    "old_start": 65,
                    "old_length": 8,
                    "new_start": 65,
                    "new_length": 7,
                    "hunk": "@@ -65,8 +65,7 @@ class WGSplatFragLayout:\n     Only dimensions of size 1 can be broadcast. All other dimensions\n     must be the same as the argument shape.\n     \"\"\"\n-    return all(\n-        dim1 == dim2 or dim1 == 1 for dim1, dim2 in zip(self.shape, shape))\n+    return all(dim1 == dim2 or dim1 == 1 for dim1, dim2 in zip(self.shape[::-1], shape[::-1]))\n \n \n @dataclasses.dataclass(frozen=True)"
                }
            ],
            "whole_deleted": "-    return all(\n-        dim1 == dim2 or dim1 == 1 for dim1, dim2 in zip(self.shape, shape))\n",
            "whole_added": "+    return all(dim1 == dim2 or dim1 == 1 for dim1, dim2 in zip(self.shape[::-1], shape[::-1]))\n",
            "whole_hunk": "@@ -65,8 +65,7 @@ class WGSplatFragLayout:\n     Only dimensions of size 1 can be broadcast. All other dimensions\n     must be the same as the argument shape.\n     \"\"\"\n-    return all(\n-        dim1 == dim2 or dim1 == 1 for dim1, dim2 in zip(self.shape, shape))\n+    return all(dim1 == dim2 or dim1 == 1 for dim1, dim2 in zip(self.shape[::-1], shape[::-1]))\n \n \n @dataclasses.dataclass(frozen=True)"
        }
    ]
},
{
    "Id": 36,
    "commit_link": "https://github.com/google/jax/commit/5ae2491853563a47beb942e80808ec1452932de1",
    "date": "2024-05-22T12:11:28-07:00",
    "message": "[Mosaic] Use implicit shape over directly using `implicit_dim()` in layout.{cc, h}\n\nThis CL also fixes a bug in `VectorLayout::join` where `ImplicitDim::kMinor` was considered equivalent to `ImplicitDim::kNone` when the shape's minor dimension is 1 (also needed to check that the second-minor dimension is 1).\n\nOften handling every implicit dim case separately is more complex and error-prone.\n\nThere will be more follow-up changes to do this consistently elsewhere the code. This is also a first step towards 0D layout support.\n\nPiperOrigin-RevId: 636250759",
    "changes": [
        {
            "name": "layout.cc",
            "path": "jaxlib/mosaic/dialect/tpu/layout.cc",
            "patches": [
                {
                    "old_start": 441,
                    "old_length": 8,
                    "new_start": 441,
                    "new_length": 7,
                    "hunk": "@@ -441,8 +441,7 @@ bool VectorLayout::hasNativeTiling(\n SmallVector<int64_t> VectorLayout::implicitShape(\n     ArrayRef<int64_t> shape) const {\n   SmallVector<int64_t> implicit_shape(shape);\n-  const int64_t num_implicit_dims = 2 - layout_rank();\n-  implicit_shape.reserve(shape.size() + num_implicit_dims);\n+  implicit_shape.reserve(shape.size() + num_implicit_dims());\n   insertImplicit(implicit_shape, 1);\n   return implicit_shape;\n }\n"
                },
                {
                    "old_start": 478,
                    "old_length": 29,
                    "new_start": 477,
                    "new_length": 17,
                    "hunk": "@@ -478,29 +477,17 @@ std::unique_ptr<VRegDataBounds> VectorLayout::tileDataBounds(\n   // TODO(apaszke): allow_replicated could have been generalized to specify\n   // what action should be taken when a REPLICATED offset is encountered.\n   // Right now it either disallows replication, or selects the whole dimension.\n-  int64_t s, l;\n-  switch (implicit_dim_) {\n-    case ImplicitDim::kNone:\n-      s = idxs[idxs.size() - 2];\n-      l = idxs[idxs.size() - 1];\n-      break;\n-    case ImplicitDim::kMinor:\n-      s = idxs[idxs.size() - 1];\n-      l = 0;\n-      break;\n-    case ImplicitDim::kSecondMinor:\n-      s = 0;\n-      l = idxs[idxs.size() - 1];\n-      break;\n-  }\n-\n+  const std::array<int64_t, 2> tiled_idxs = getImplicitTiledDims(idxs, 0);\n+  const int64_t s = tiled_idxs[0];\n+  const int64_t l = tiled_idxs[1];\n   const SmallVector<int64_t> tiles_implicit_shape =\n       tileArrayImplicitShape(full_shape, target_shape);\n-  const int64_t ns = tiles_implicit_shape[tiles_implicit_shape.size() - 2];\n-  const int64_t nl = tiles_implicit_shape[tiles_implicit_shape.size() - 1];\n-  const SmallVector<int64_t> implicit_shape = implicitShape(full_shape);\n-  const int64_t is = implicit_shape[implicit_shape.size() - 2];\n-  const int64_t il = implicit_shape[implicit_shape.size() - 1];\n+  const int64_t ns = *(tiles_implicit_shape.end() - 2);\n+  const int64_t nl = *(tiles_implicit_shape.end() - 1);\n+  const std::array<int64_t, 2> shape_tiled_dims =\n+      getImplicitTiledDims(full_shape, 1);\n+  const int64_t is = shape_tiled_dims[0];\n+  const int64_t il = shape_tiled_dims[1];\n \n   if (!hasNaturalTopology(target_shape)) {\n     if (!offsets_[0].has_value() || !offsets_[1].has_value()) {\n"
                },
                {
                    "old_start": 588,
                    "old_length": 27,
                    "new_start": 575,
                    "new_length": 12,
                    "hunk": "@@ -588,27 +575,12 @@ bool VectorLayout::generalizes(\n     if (shape.data() == nullptr) {\n       return false;\n     }\n-    // If the second-minor dimension is of size 1, then it does not matter\n-    // whether we have a second minor implicit dim or not.\n-    bool ok = false;\n-    if (((implicit_dim_ == ImplicitDim::kSecondMinor &&\n-          other.implicit_dim_ == ImplicitDim::kNone) ||\n-         (other.implicit_dim_ == ImplicitDim::kSecondMinor &&\n-          implicit_dim_ == ImplicitDim::kNone)) &&\n-        shape[shape.size() - 2] == 1) {\n-      ok =  true;\n-    }\n-    // If sufficiently many trailing dimensions are of size 1, then it does not\n-    // matter if we use implicit dims to insert more.\n-    int max_rank = std::max(layout_rank(), other.layout_rank());\n-    CHECK_GE(max_rank, 1);\n-    CHECK_LE(max_rank, 2);\n-    if (*(shape.end() - 1) == 1 && (max_rank == 1 || *(shape.end() - 2) == 1)) {\n-      ok = true;\n-    }\n-    if (!ok) {\n-      return false;\n-    }\n+    // Since we do not reorder axes, if the shapes resulting from inserting\n+    // implicit dimensions resulting are the same in the 2 minormost dimensions\n+    // for both layouts, then the elements must be laid out the same way (i.e.\n+    // layouts are equivalent).\n+    return getImplicitTiledDims(shape, 1) ==\n+           other.getImplicitTiledDims(shape, 1);\n   }\n   if (tiling_ != other.tiling_) {\n     // Don't fail yet!\n"
                },
                {
                    "old_start": 658,
                    "old_length": 26,
                    "new_start": 630,
                    "new_length": 8,
                    "hunk": "@@ -658,26 +630,8 @@ std::optional<VectorLayout> VectorLayout::join(const VectorLayout& l,\n   if (l.bitwidth_ != r.bitwidth_ || l.tiling_ != r.tiling_) {\n     return std::nullopt;\n   }\n-  if (l.implicit_dim_ != r.implicit_dim_) {\n-    if (shape.size() < 2) {\n-      return std::nullopt;\n-    }\n-    ImplicitDim dim;\n-    if (l.implicit_dim_ == ImplicitDim::kNone) {\n-      dim = r.implicit_dim_;\n-    } else if (r.implicit_dim_ == ImplicitDim::kNone) {\n-      dim = l.implicit_dim_;\n-    } else {\n-      return std::nullopt;\n-    }\n-    if (dim == ImplicitDim::kMinor && shape[shape.size() - 1] == 1) {\n-      // OK, they are equivalent.\n-    } else if (dim == ImplicitDim::kSecondMinor &&\n-               shape[shape.size() - 2] == 1) {\n-      // OK, they are equivalent.\n-    } else {\n-      return std::nullopt;\n-    }\n+  if (l.getImplicitTiledDims(shape, 1) != r.getImplicitTiledDims(shape, 1)) {\n+    return std::nullopt;\n   }\n   LayoutOffsets offsets;\n   for (int i = 0; i < 2; ++i) {\n"
                }
            ],
            "whole_deleted": "-  const int64_t num_implicit_dims = 2 - layout_rank();\n-  implicit_shape.reserve(shape.size() + num_implicit_dims);\n-  int64_t s, l;\n-  switch (implicit_dim_) {\n-    case ImplicitDim::kNone:\n-      s = idxs[idxs.size() - 2];\n-      l = idxs[idxs.size() - 1];\n-      break;\n-    case ImplicitDim::kMinor:\n-      s = idxs[idxs.size() - 1];\n-      l = 0;\n-      break;\n-    case ImplicitDim::kSecondMinor:\n-      s = 0;\n-      l = idxs[idxs.size() - 1];\n-      break;\n-  }\n-\n-  const int64_t ns = tiles_implicit_shape[tiles_implicit_shape.size() - 2];\n-  const int64_t nl = tiles_implicit_shape[tiles_implicit_shape.size() - 1];\n-  const SmallVector<int64_t> implicit_shape = implicitShape(full_shape);\n-  const int64_t is = implicit_shape[implicit_shape.size() - 2];\n-  const int64_t il = implicit_shape[implicit_shape.size() - 1];\n-    // If the second-minor dimension is of size 1, then it does not matter\n-    // whether we have a second minor implicit dim or not.\n-    bool ok = false;\n-    if (((implicit_dim_ == ImplicitDim::kSecondMinor &&\n-          other.implicit_dim_ == ImplicitDim::kNone) ||\n-         (other.implicit_dim_ == ImplicitDim::kSecondMinor &&\n-          implicit_dim_ == ImplicitDim::kNone)) &&\n-        shape[shape.size() - 2] == 1) {\n-      ok =  true;\n-    }\n-    // If sufficiently many trailing dimensions are of size 1, then it does not\n-    // matter if we use implicit dims to insert more.\n-    int max_rank = std::max(layout_rank(), other.layout_rank());\n-    CHECK_GE(max_rank, 1);\n-    CHECK_LE(max_rank, 2);\n-    if (*(shape.end() - 1) == 1 && (max_rank == 1 || *(shape.end() - 2) == 1)) {\n-      ok = true;\n-    }\n-    if (!ok) {\n-      return false;\n-    }\n-  if (l.implicit_dim_ != r.implicit_dim_) {\n-    if (shape.size() < 2) {\n-      return std::nullopt;\n-    }\n-    ImplicitDim dim;\n-    if (l.implicit_dim_ == ImplicitDim::kNone) {\n-      dim = r.implicit_dim_;\n-    } else if (r.implicit_dim_ == ImplicitDim::kNone) {\n-      dim = l.implicit_dim_;\n-    } else {\n-      return std::nullopt;\n-    }\n-    if (dim == ImplicitDim::kMinor && shape[shape.size() - 1] == 1) {\n-      // OK, they are equivalent.\n-    } else if (dim == ImplicitDim::kSecondMinor &&\n-               shape[shape.size() - 2] == 1) {\n-      // OK, they are equivalent.\n-    } else {\n-      return std::nullopt;\n-    }\n",
            "whole_added": "+  implicit_shape.reserve(shape.size() + num_implicit_dims());\n+  const std::array<int64_t, 2> tiled_idxs = getImplicitTiledDims(idxs, 0);\n+  const int64_t s = tiled_idxs[0];\n+  const int64_t l = tiled_idxs[1];\n+  const int64_t ns = *(tiles_implicit_shape.end() - 2);\n+  const int64_t nl = *(tiles_implicit_shape.end() - 1);\n+  const std::array<int64_t, 2> shape_tiled_dims =\n+      getImplicitTiledDims(full_shape, 1);\n+  const int64_t is = shape_tiled_dims[0];\n+  const int64_t il = shape_tiled_dims[1];\n+    // Since we do not reorder axes, if the shapes resulting from inserting\n+    // implicit dimensions resulting are the same in the 2 minormost dimensions\n+    // for both layouts, then the elements must be laid out the same way (i.e.\n+    // layouts are equivalent).\n+    return getImplicitTiledDims(shape, 1) ==\n+           other.getImplicitTiledDims(shape, 1);\n+  if (l.getImplicitTiledDims(shape, 1) != r.getImplicitTiledDims(shape, 1)) {\n+    return std::nullopt;\n",
            "whole_hunk": "@@ -441,8 +441,7 @@ bool VectorLayout::hasNativeTiling(\n SmallVector<int64_t> VectorLayout::implicitShape(\n     ArrayRef<int64_t> shape) const {\n   SmallVector<int64_t> implicit_shape(shape);\n-  const int64_t num_implicit_dims = 2 - layout_rank();\n-  implicit_shape.reserve(shape.size() + num_implicit_dims);\n+  implicit_shape.reserve(shape.size() + num_implicit_dims());\n   insertImplicit(implicit_shape, 1);\n   return implicit_shape;\n }\n@@ -478,29 +477,17 @@ std::unique_ptr<VRegDataBounds> VectorLayout::tileDataBounds(\n   // TODO(apaszke): allow_replicated could have been generalized to specify\n   // what action should be taken when a REPLICATED offset is encountered.\n   // Right now it either disallows replication, or selects the whole dimension.\n-  int64_t s, l;\n-  switch (implicit_dim_) {\n-    case ImplicitDim::kNone:\n-      s = idxs[idxs.size() - 2];\n-      l = idxs[idxs.size() - 1];\n-      break;\n-    case ImplicitDim::kMinor:\n-      s = idxs[idxs.size() - 1];\n-      l = 0;\n-      break;\n-    case ImplicitDim::kSecondMinor:\n-      s = 0;\n-      l = idxs[idxs.size() - 1];\n-      break;\n-  }\n-\n+  const std::array<int64_t, 2> tiled_idxs = getImplicitTiledDims(idxs, 0);\n+  const int64_t s = tiled_idxs[0];\n+  const int64_t l = tiled_idxs[1];\n   const SmallVector<int64_t> tiles_implicit_shape =\n       tileArrayImplicitShape(full_shape, target_shape);\n-  const int64_t ns = tiles_implicit_shape[tiles_implicit_shape.size() - 2];\n-  const int64_t nl = tiles_implicit_shape[tiles_implicit_shape.size() - 1];\n-  const SmallVector<int64_t> implicit_shape = implicitShape(full_shape);\n-  const int64_t is = implicit_shape[implicit_shape.size() - 2];\n-  const int64_t il = implicit_shape[implicit_shape.size() - 1];\n+  const int64_t ns = *(tiles_implicit_shape.end() - 2);\n+  const int64_t nl = *(tiles_implicit_shape.end() - 1);\n+  const std::array<int64_t, 2> shape_tiled_dims =\n+      getImplicitTiledDims(full_shape, 1);\n+  const int64_t is = shape_tiled_dims[0];\n+  const int64_t il = shape_tiled_dims[1];\n \n   if (!hasNaturalTopology(target_shape)) {\n     if (!offsets_[0].has_value() || !offsets_[1].has_value()) {\n@@ -588,27 +575,12 @@ bool VectorLayout::generalizes(\n     if (shape.data() == nullptr) {\n       return false;\n     }\n-    // If the second-minor dimension is of size 1, then it does not matter\n-    // whether we have a second minor implicit dim or not.\n-    bool ok = false;\n-    if (((implicit_dim_ == ImplicitDim::kSecondMinor &&\n-          other.implicit_dim_ == ImplicitDim::kNone) ||\n-         (other.implicit_dim_ == ImplicitDim::kSecondMinor &&\n-          implicit_dim_ == ImplicitDim::kNone)) &&\n-        shape[shape.size() - 2] == 1) {\n-      ok =  true;\n-    }\n-    // If sufficiently many trailing dimensions are of size 1, then it does not\n-    // matter if we use implicit dims to insert more.\n-    int max_rank = std::max(layout_rank(), other.layout_rank());\n-    CHECK_GE(max_rank, 1);\n-    CHECK_LE(max_rank, 2);\n-    if (*(shape.end() - 1) == 1 && (max_rank == 1 || *(shape.end() - 2) == 1)) {\n-      ok = true;\n-    }\n-    if (!ok) {\n-      return false;\n-    }\n+    // Since we do not reorder axes, if the shapes resulting from inserting\n+    // implicit dimensions resulting are the same in the 2 minormost dimensions\n+    // for both layouts, then the elements must be laid out the same way (i.e.\n+    // layouts are equivalent).\n+    return getImplicitTiledDims(shape, 1) ==\n+           other.getImplicitTiledDims(shape, 1);\n   }\n   if (tiling_ != other.tiling_) {\n     // Don't fail yet!\n@@ -658,26 +630,8 @@ std::optional<VectorLayout> VectorLayout::join(const VectorLayout& l,\n   if (l.bitwidth_ != r.bitwidth_ || l.tiling_ != r.tiling_) {\n     return std::nullopt;\n   }\n-  if (l.implicit_dim_ != r.implicit_dim_) {\n-    if (shape.size() < 2) {\n-      return std::nullopt;\n-    }\n-    ImplicitDim dim;\n-    if (l.implicit_dim_ == ImplicitDim::kNone) {\n-      dim = r.implicit_dim_;\n-    } else if (r.implicit_dim_ == ImplicitDim::kNone) {\n-      dim = l.implicit_dim_;\n-    } else {\n-      return std::nullopt;\n-    }\n-    if (dim == ImplicitDim::kMinor && shape[shape.size() - 1] == 1) {\n-      // OK, they are equivalent.\n-    } else if (dim == ImplicitDim::kSecondMinor &&\n-               shape[shape.size() - 2] == 1) {\n-      // OK, they are equivalent.\n-    } else {\n-      return std::nullopt;\n-    }\n+  if (l.getImplicitTiledDims(shape, 1) != r.getImplicitTiledDims(shape, 1)) {\n+    return std::nullopt;\n   }\n   LayoutOffsets offsets;\n   for (int i = 0; i < 2; ++i) {\n"
        },
        {
            "name": "layout.h",
            "path": "jaxlib/mosaic/dialect/tpu/layout.h",
            "patches": [
                {
                    "old_start": 245,
                    "old_length": 8,
                    "new_start": 245,
                    "new_length": 17,
                    "hunk": "@@ -245,8 +245,17 @@ class VectorLayout {\n   const std::array<int64_t, 2> &tiling() const { return tiling_; }\n   ImplicitDim implicit_dim() const { return implicit_dim_; }\n   int packing() const { return 32 / bitwidth_; }\n+  int num_implicit_dims() const {\n+    switch (implicit_dim_) {\n+      case ImplicitDim::kNone:\n+        return 0;\n+      case ImplicitDim::kMinor:\n+      case ImplicitDim::kSecondMinor:\n+        return 1;\n+    }\n+  }\n   // The number of minormost dimensions tiled by this layout.\n-  int layout_rank() const { return 1 + (implicit_dim_ == ImplicitDim::kNone); }\n+  int layout_rank() const { return 2 - num_implicit_dims(); }\n \n   bool operator==(const VectorLayout &other) const;\n   bool operator!=(const VectorLayout &other) const {\n"
                },
                {
                    "old_start": 302,
                    "old_length": 6,
                    "new_start": 311,
                    "new_length": 27,
                    "hunk": "@@ -302,6 +311,27 @@ class VectorLayout {\n     }\n   }\n \n+  // Returns the value of the tiled (2 minormost) dimensions of the given array\n+  // with implicit dims inserted.\n+  //\n+  // Roughly equivalent to the following (but avoids vector allocation):\n+  //\n+  //   SmallVector<int64_t> vec = arr;\n+  //   insertImplicit(arr, implicit_value);\n+  //   return {*(vec.end() - 2), *(vec.end() - 1)};\n+  std::array<int64_t, 2> getImplicitTiledDims(\n+      const ArrayRef<int64_t> arr, const int64_t implicit_value) const {\n+    CHECK_GE(arr.size(), layout_rank());\n+    switch (implicit_dim_) {\n+      case ImplicitDim::kNone:\n+        return {*(arr.end() - 2), *(arr.end() - 1)};\n+      case ImplicitDim::kMinor:\n+        return {*(arr.end() - 1), implicit_value};\n+      case ImplicitDim::kSecondMinor:\n+        return {implicit_value, *(arr.end() - 1)};\n+    }\n+  }\n+\n   SmallVector<int64_t> implicitShape(ArrayRef<int64_t> shape) const;\n \n   SmallVector<int64_t> tileArrayImplicitShape(\n"
                }
            ],
            "whole_deleted": "-  int layout_rank() const { return 1 + (implicit_dim_ == ImplicitDim::kNone); }\n",
            "whole_added": "+  int num_implicit_dims() const {\n+    switch (implicit_dim_) {\n+      case ImplicitDim::kNone:\n+        return 0;\n+      case ImplicitDim::kMinor:\n+      case ImplicitDim::kSecondMinor:\n+        return 1;\n+    }\n+  }\n+  int layout_rank() const { return 2 - num_implicit_dims(); }\n+  // Returns the value of the tiled (2 minormost) dimensions of the given array\n+  // with implicit dims inserted.\n+  //\n+  // Roughly equivalent to the following (but avoids vector allocation):\n+  //\n+  //   SmallVector<int64_t> vec = arr;\n+  //   insertImplicit(arr, implicit_value);\n+  //   return {*(vec.end() - 2), *(vec.end() - 1)};\n+  std::array<int64_t, 2> getImplicitTiledDims(\n+      const ArrayRef<int64_t> arr, const int64_t implicit_value) const {\n+    CHECK_GE(arr.size(), layout_rank());\n+    switch (implicit_dim_) {\n+      case ImplicitDim::kNone:\n+        return {*(arr.end() - 2), *(arr.end() - 1)};\n+      case ImplicitDim::kMinor:\n+        return {*(arr.end() - 1), implicit_value};\n+      case ImplicitDim::kSecondMinor:\n+        return {implicit_value, *(arr.end() - 1)};\n+    }\n+  }\n+\n",
            "whole_hunk": "@@ -245,8 +245,17 @@ class VectorLayout {\n   const std::array<int64_t, 2> &tiling() const { return tiling_; }\n   ImplicitDim implicit_dim() const { return implicit_dim_; }\n   int packing() const { return 32 / bitwidth_; }\n+  int num_implicit_dims() const {\n+    switch (implicit_dim_) {\n+      case ImplicitDim::kNone:\n+        return 0;\n+      case ImplicitDim::kMinor:\n+      case ImplicitDim::kSecondMinor:\n+        return 1;\n+    }\n+  }\n   // The number of minormost dimensions tiled by this layout.\n-  int layout_rank() const { return 1 + (implicit_dim_ == ImplicitDim::kNone); }\n+  int layout_rank() const { return 2 - num_implicit_dims(); }\n \n   bool operator==(const VectorLayout &other) const;\n   bool operator!=(const VectorLayout &other) const {\n@@ -302,6 +311,27 @@ class VectorLayout {\n     }\n   }\n \n+  // Returns the value of the tiled (2 minormost) dimensions of the given array\n+  // with implicit dims inserted.\n+  //\n+  // Roughly equivalent to the following (but avoids vector allocation):\n+  //\n+  //   SmallVector<int64_t> vec = arr;\n+  //   insertImplicit(arr, implicit_value);\n+  //   return {*(vec.end() - 2), *(vec.end() - 1)};\n+  std::array<int64_t, 2> getImplicitTiledDims(\n+      const ArrayRef<int64_t> arr, const int64_t implicit_value) const {\n+    CHECK_GE(arr.size(), layout_rank());\n+    switch (implicit_dim_) {\n+      case ImplicitDim::kNone:\n+        return {*(arr.end() - 2), *(arr.end() - 1)};\n+      case ImplicitDim::kMinor:\n+        return {*(arr.end() - 1), implicit_value};\n+      case ImplicitDim::kSecondMinor:\n+        return {implicit_value, *(arr.end() - 1)};\n+    }\n+  }\n+\n   SmallVector<int64_t> implicitShape(ArrayRef<int64_t> shape) const;\n \n   SmallVector<int64_t> tileArrayImplicitShape(\n"
        },
        {
            "name": "apply_vector_layout.cc",
            "path": "jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc",
            "patches": [
                {
                    "old_start": 2960,
                    "old_length": 15,
                    "new_start": 2960,
                    "new_length": 14,
                    "hunk": "@@ -2960,15 +2960,14 @@ FailureOr<xla::Array<Value>> vector_extract_slice_impl(\n   TPU_ASSERT_EQ_OP(num_indices, sizes.size());\n \n   SmallVector<int64_t> full_sizes;\n-  const int64_t num_implicit_dims = 2 - layout_in.layout_rank();\n-  full_sizes.reserve(src_vector_rank + num_implicit_dims);\n+  full_sizes.reserve(src_vector_rank + layout_in.num_implicit_dims());\n   full_sizes.append(sizes.begin(), sizes.end());\n   full_sizes.append(src_vector_shape.begin() + num_indices,\n                     src_vector_shape.end());\n-  layout_in.insertImplicit(full_sizes, 1); /*  */\n+  layout_in.insertImplicit(full_sizes, 1);\n \n   SmallVector<int64_t> full_offsets;\n-  full_offsets.reserve(src_vector_rank + num_implicit_dims);\n+  full_offsets.reserve(src_vector_rank + layout_in.num_implicit_dims());\n   full_offsets.append(offsets.begin(), offsets.end());\n   full_offsets.append(src_vector_rank - num_indices, 0);\n   layout_in.insertImplicit(full_offsets, 0);"
                }
            ],
            "whole_deleted": "-  const int64_t num_implicit_dims = 2 - layout_in.layout_rank();\n-  full_sizes.reserve(src_vector_rank + num_implicit_dims);\n-  layout_in.insertImplicit(full_sizes, 1); /*  */\n-  full_offsets.reserve(src_vector_rank + num_implicit_dims);\n",
            "whole_added": "+  full_sizes.reserve(src_vector_rank + layout_in.num_implicit_dims());\n+  layout_in.insertImplicit(full_sizes, 1);\n+  full_offsets.reserve(src_vector_rank + layout_in.num_implicit_dims());\n",
            "whole_hunk": "@@ -2960,15 +2960,14 @@ FailureOr<xla::Array<Value>> vector_extract_slice_impl(\n   TPU_ASSERT_EQ_OP(num_indices, sizes.size());\n \n   SmallVector<int64_t> full_sizes;\n-  const int64_t num_implicit_dims = 2 - layout_in.layout_rank();\n-  full_sizes.reserve(src_vector_rank + num_implicit_dims);\n+  full_sizes.reserve(src_vector_rank + layout_in.num_implicit_dims());\n   full_sizes.append(sizes.begin(), sizes.end());\n   full_sizes.append(src_vector_shape.begin() + num_indices,\n                     src_vector_shape.end());\n-  layout_in.insertImplicit(full_sizes, 1); /*  */\n+  layout_in.insertImplicit(full_sizes, 1);\n \n   SmallVector<int64_t> full_offsets;\n-  full_offsets.reserve(src_vector_rank + num_implicit_dims);\n+  full_offsets.reserve(src_vector_rank + layout_in.num_implicit_dims());\n   full_offsets.append(offsets.begin(), offsets.end());\n   full_offsets.append(src_vector_rank - num_indices, 0);\n   layout_in.insertImplicit(full_offsets, 0);"
        }
    ]
},
{
    "Id": 37,
    "commit_link": "https://github.com/google/jax/commit/99485297350d69658d417a73ffb7277d8be1049b",
    "date": "2024-05-21T11:40:16-07:00",
    "message": "[export] Relax the check that exported modules are used with same number of devices as when exported\n\nNow we allow a module exported for 1 device and not using any sharding annotations\nto be called from a computation that uses multiple devices. Such exported modules\ncan be parallelized trivially point-wise.",
    "changes": [
        {
            "name": "_export.py",
            "path": "jax/experimental/export/_export.py",
            "patches": [
                {
                    "old_start": 298,
                    "old_length": 7,
                    "new_start": 298,
                    "new_length": 7,
                    "hunk": "@@ -298,7 +298,7 @@ class Exported:\n \n   _get_vjp: Callable[[Exported], Exported] | None\n \n-  def mlir_module(self) -> ir.Module:\n+  def mlir_module(self) -> str:\n     return xla_client._xla.mlir.deserialize_portable_artifact(self.mlir_module_serialized)\n \n   def __str__(self):\n"
                },
                {
                    "old_start": 812,
                    "old_length": 14,
                    "new_start": 812,
                    "new_length": 15,
                    "hunk": "@@ -812,14 +812,15 @@ _CUSTOM_CALL_TARGETS_GUARANTEED_STABLE = {\n check_sharding_pattern = re.compile(r\"^({replicated}|{unknown shard_as.*}|\"\")$\")\n \n def _check_module(mod: ir.Module, *,\n-                  disabled_checks: Sequence[DisabledSafetyCheck]) -> None:\n+                  disabled_checks: Sequence[DisabledSafetyCheck]) -> bool:\n   \"\"\"Run a number of checks on the module.\n \n   Args:\n-    allow_non_replicated_sharding: whether the module is allowed to contain\n-      non_replicated sharding annotations.\n     disabled_checks: the safety checks that are disabled.\n+\n+  Returns True if the module uses non-replicated shardings.\n   \"\"\"\n+  sharding_attr = ir.StringAttr.get(\"Sharding\", mod.context)\n   allowed_custom_call_targets: set[str] = copy.copy(_CUSTOM_CALL_TARGETS_GUARANTEED_STABLE)\n   for dc in disabled_checks:\n     target = dc.is_custom_call()\n"
                },
                {
                    "old_start": 830,
                    "old_length": 13,
                    "new_start": 831,
                    "new_length": 36,
                    "hunk": "@@ -830,13 +831,36 @@ def _check_module(mod: ir.Module, *,\n       ir.StringAttr.get(target, mod.context)\n       for target in allowed_custom_call_targets}\n   disallowed_custom_call_ops: list[str] = []\n+  module_uses_non_replicated_sharding = False\n+  def check_sharding(op: ir.Operation, loc: ir.Location):\n+    try:\n+      sharding = op.attributes[\"mhlo.sharding\"]\n+    except KeyError:\n+      pass\n+    else:\n+      nonlocal module_uses_non_replicated_sharding\n+      try:\n+        sharding_value = ir.StringAttr(sharding).value\n+      except UnicodeDecodeError:\n+        # The mhlo.sharding attribute may be in pretty-printed format, or\n+        # as an encoding of an HloSharding protobuf in some rare situations.\n+        # We handle the latter by conservatively assuming it is non-replicated.\n+        module_uses_non_replicated_sharding = True\n+      else:\n+        if not re.match(check_sharding_pattern, sharding_value):\n+          module_uses_non_replicated_sharding = True\n \n   def check_op(op: ir.Operation):\n     op_name = op.operation.name\n-    if op_name == \"stablehlo.custom_call\":\n+    if op_name == \"func.func\":\n+      check_sharding(op.operation, op.location)\n+\n+    elif op_name == \"stablehlo.custom_call\":\n       call_target_name_attr = op.operation.attributes[\"call_target_name\"]\n       if (call_target_name_attr not in allowed_custom_call_targets_attrs):\n         disallowed_custom_call_ops.append(f\"{op} at {op.location}\")\n+      if call_target_name_attr == sharding_attr:\n+        check_sharding(op, op.location)\n \n   def walk_operations(op):\n     check_op(op)\n"
                },
                {
                    "old_start": 853,
                    "old_length": 6,
                    "new_start": 877,
                    "new_length": 7,
                    "hunk": "@@ -853,6 +877,7 @@ def _check_module(mod: ir.Module, *,\n            f\"{disallowed_custom_call_ops_str}.\\n\"\n            \"See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#native-lowering-supports-only-select-custom-calls\")\n     raise ValueError(msg)\n+  return module_uses_non_replicated_sharding\n \n def expand_in_shardings(in_shardings: Sequence[LoweringSharding],\n                         module_kept_var_idx: Sequence[int],\n"
                },
                {
                    "old_start": 1090,
                    "old_length": 6,
                    "new_start": 1115,
                    "new_length": 7,
                    "hunk": "@@ -1090,6 +1115,7 @@ def _call_exported_lowering(ctx: mlir.LoweringRuleContext, *args,\n                             exported: Exported):\n   if exported.uses_shape_polymorphism:\n     ctx.module_context.shape_poly_state.uses_dim_vars = True\n+  submodule = ir.Module.parse(exported.mlir_module())\n \n   axis_context = ctx.module_context.axis_context\n   if isinstance(axis_context, sharding_impls.ShardingContext):\n"
                },
                {
                    "old_start": 1099,
                    "old_length": 17,
                    "new_start": 1125,
                    "new_length": 26,
                    "hunk": "@@ -1099,17 +1125,26 @@ def _call_exported_lowering(ctx: mlir.LoweringRuleContext, *args,\n   else:\n     raise NotImplementedError(type(axis_context))\n   if num_devices != exported.nr_devices:\n-    raise NotImplementedError(\n-      f\"Exported module {exported.fun_name} was lowered for \"\n-      f\"{exported.nr_devices} devices and is called in a context with \"\n-      f\"{num_devices} devices\"\n-    )\n+    # In some special cases we allow running with a different number of devices\n+    # than the function was exported for.\n+    err_msg = \"\"\n+    if exported.nr_devices != 1:\n+      err_msg = \"the module was lowered for more than 1 device.\"\n+    elif (_check_module(submodule, disabled_checks=()) or\n+          any(s is not None and not s.is_replicated()\n+              for s in exported.in_shardings + exported.out_shardings)):\n+      err_msg = \"the module contains non-replicated sharding annotations.\"\n+    if err_msg:\n+      raise NotImplementedError(\n+        f\"Exported module {exported.fun_name} was lowered for \"\n+        f\"{exported.nr_devices} devices and is called in a context with \"\n+        f\"{num_devices} devices. This is disallowed because: {err_msg}\"\n+      )\n \n   # Apply in_shardings\n   args = tuple(\n     wrap_with_sharding(ctx, x, x_aval, x_sharding)\n     for x, x_aval, x_sharding in zip(args, ctx.avals_in, exported.in_shardings))\n-  submodule = ir.Module.parse(exported.mlir_module())  # type: ignore\n   symtab = ir.SymbolTable(submodule.operation)\n   # The called function may have been exported with polymorphic shapes and called\n   # now with more refined shapes. We insert hlo.ConvertOp to ensure the module\n"
                }
            ],
            "whole_deleted": "-  def mlir_module(self) -> ir.Module:\n-                  disabled_checks: Sequence[DisabledSafetyCheck]) -> None:\n-    allow_non_replicated_sharding: whether the module is allowed to contain\n-      non_replicated sharding annotations.\n-    if op_name == \"stablehlo.custom_call\":\n-    raise NotImplementedError(\n-      f\"Exported module {exported.fun_name} was lowered for \"\n-      f\"{exported.nr_devices} devices and is called in a context with \"\n-      f\"{num_devices} devices\"\n-    )\n-  submodule = ir.Module.parse(exported.mlir_module())  # type: ignore\n",
            "whole_added": "+  def mlir_module(self) -> str:\n+                  disabled_checks: Sequence[DisabledSafetyCheck]) -> bool:\n+\n+  Returns True if the module uses non-replicated shardings.\n+  sharding_attr = ir.StringAttr.get(\"Sharding\", mod.context)\n+  module_uses_non_replicated_sharding = False\n+  def check_sharding(op: ir.Operation, loc: ir.Location):\n+    try:\n+      sharding = op.attributes[\"mhlo.sharding\"]\n+    except KeyError:\n+      pass\n+    else:\n+      nonlocal module_uses_non_replicated_sharding\n+      try:\n+        sharding_value = ir.StringAttr(sharding).value\n+      except UnicodeDecodeError:\n+        # The mhlo.sharding attribute may be in pretty-printed format, or\n+        # as an encoding of an HloSharding protobuf in some rare situations.\n+        # We handle the latter by conservatively assuming it is non-replicated.\n+        module_uses_non_replicated_sharding = True\n+      else:\n+        if not re.match(check_sharding_pattern, sharding_value):\n+          module_uses_non_replicated_sharding = True\n+    if op_name == \"func.func\":\n+      check_sharding(op.operation, op.location)\n+\n+    elif op_name == \"stablehlo.custom_call\":\n+      if call_target_name_attr == sharding_attr:\n+        check_sharding(op, op.location)\n+  return module_uses_non_replicated_sharding\n+  submodule = ir.Module.parse(exported.mlir_module())\n+    # In some special cases we allow running with a different number of devices\n+    # than the function was exported for.\n+    err_msg = \"\"\n+    if exported.nr_devices != 1:\n+      err_msg = \"the module was lowered for more than 1 device.\"\n+    elif (_check_module(submodule, disabled_checks=()) or\n+          any(s is not None and not s.is_replicated()\n+              for s in exported.in_shardings + exported.out_shardings)):\n+      err_msg = \"the module contains non-replicated sharding annotations.\"\n+    if err_msg:\n+      raise NotImplementedError(\n+        f\"Exported module {exported.fun_name} was lowered for \"\n+        f\"{exported.nr_devices} devices and is called in a context with \"\n+        f\"{num_devices} devices. This is disallowed because: {err_msg}\"\n+      )\n",
            "whole_hunk": "@@ -298,7 +298,7 @@ class Exported:\n \n   _get_vjp: Callable[[Exported], Exported] | None\n \n-  def mlir_module(self) -> ir.Module:\n+  def mlir_module(self) -> str:\n     return xla_client._xla.mlir.deserialize_portable_artifact(self.mlir_module_serialized)\n \n   def __str__(self):\n@@ -812,14 +812,15 @@ _CUSTOM_CALL_TARGETS_GUARANTEED_STABLE = {\n check_sharding_pattern = re.compile(r\"^({replicated}|{unknown shard_as.*}|\"\")$\")\n \n def _check_module(mod: ir.Module, *,\n-                  disabled_checks: Sequence[DisabledSafetyCheck]) -> None:\n+                  disabled_checks: Sequence[DisabledSafetyCheck]) -> bool:\n   \"\"\"Run a number of checks on the module.\n \n   Args:\n-    allow_non_replicated_sharding: whether the module is allowed to contain\n-      non_replicated sharding annotations.\n     disabled_checks: the safety checks that are disabled.\n+\n+  Returns True if the module uses non-replicated shardings.\n   \"\"\"\n+  sharding_attr = ir.StringAttr.get(\"Sharding\", mod.context)\n   allowed_custom_call_targets: set[str] = copy.copy(_CUSTOM_CALL_TARGETS_GUARANTEED_STABLE)\n   for dc in disabled_checks:\n     target = dc.is_custom_call()\n@@ -830,13 +831,36 @@ def _check_module(mod: ir.Module, *,\n       ir.StringAttr.get(target, mod.context)\n       for target in allowed_custom_call_targets}\n   disallowed_custom_call_ops: list[str] = []\n+  module_uses_non_replicated_sharding = False\n+  def check_sharding(op: ir.Operation, loc: ir.Location):\n+    try:\n+      sharding = op.attributes[\"mhlo.sharding\"]\n+    except KeyError:\n+      pass\n+    else:\n+      nonlocal module_uses_non_replicated_sharding\n+      try:\n+        sharding_value = ir.StringAttr(sharding).value\n+      except UnicodeDecodeError:\n+        # The mhlo.sharding attribute may be in pretty-printed format, or\n+        # as an encoding of an HloSharding protobuf in some rare situations.\n+        # We handle the latter by conservatively assuming it is non-replicated.\n+        module_uses_non_replicated_sharding = True\n+      else:\n+        if not re.match(check_sharding_pattern, sharding_value):\n+          module_uses_non_replicated_sharding = True\n \n   def check_op(op: ir.Operation):\n     op_name = op.operation.name\n-    if op_name == \"stablehlo.custom_call\":\n+    if op_name == \"func.func\":\n+      check_sharding(op.operation, op.location)\n+\n+    elif op_name == \"stablehlo.custom_call\":\n       call_target_name_attr = op.operation.attributes[\"call_target_name\"]\n       if (call_target_name_attr not in allowed_custom_call_targets_attrs):\n         disallowed_custom_call_ops.append(f\"{op} at {op.location}\")\n+      if call_target_name_attr == sharding_attr:\n+        check_sharding(op, op.location)\n \n   def walk_operations(op):\n     check_op(op)\n@@ -853,6 +877,7 @@ def _check_module(mod: ir.Module, *,\n            f\"{disallowed_custom_call_ops_str}.\\n\"\n            \"See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#native-lowering-supports-only-select-custom-calls\")\n     raise ValueError(msg)\n+  return module_uses_non_replicated_sharding\n \n def expand_in_shardings(in_shardings: Sequence[LoweringSharding],\n                         module_kept_var_idx: Sequence[int],\n@@ -1090,6 +1115,7 @@ def _call_exported_lowering(ctx: mlir.LoweringRuleContext, *args,\n                             exported: Exported):\n   if exported.uses_shape_polymorphism:\n     ctx.module_context.shape_poly_state.uses_dim_vars = True\n+  submodule = ir.Module.parse(exported.mlir_module())\n \n   axis_context = ctx.module_context.axis_context\n   if isinstance(axis_context, sharding_impls.ShardingContext):\n@@ -1099,17 +1125,26 @@ def _call_exported_lowering(ctx: mlir.LoweringRuleContext, *args,\n   else:\n     raise NotImplementedError(type(axis_context))\n   if num_devices != exported.nr_devices:\n-    raise NotImplementedError(\n-      f\"Exported module {exported.fun_name} was lowered for \"\n-      f\"{exported.nr_devices} devices and is called in a context with \"\n-      f\"{num_devices} devices\"\n-    )\n+    # In some special cases we allow running with a different number of devices\n+    # than the function was exported for.\n+    err_msg = \"\"\n+    if exported.nr_devices != 1:\n+      err_msg = \"the module was lowered for more than 1 device.\"\n+    elif (_check_module(submodule, disabled_checks=()) or\n+          any(s is not None and not s.is_replicated()\n+              for s in exported.in_shardings + exported.out_shardings)):\n+      err_msg = \"the module contains non-replicated sharding annotations.\"\n+    if err_msg:\n+      raise NotImplementedError(\n+        f\"Exported module {exported.fun_name} was lowered for \"\n+        f\"{exported.nr_devices} devices and is called in a context with \"\n+        f\"{num_devices} devices. This is disallowed because: {err_msg}\"\n+      )\n \n   # Apply in_shardings\n   args = tuple(\n     wrap_with_sharding(ctx, x, x_aval, x_sharding)\n     for x, x_aval, x_sharding in zip(args, ctx.avals_in, exported.in_shardings))\n-  submodule = ir.Module.parse(exported.mlir_module())  # type: ignore\n   symtab = ir.SymbolTable(submodule.operation)\n   # The called function may have been exported with polymorphic shapes and called\n   # now with more refined shapes. We insert hlo.ConvertOp to ensure the module\n"
        },
        {
            "name": "export_test.py",
            "path": "tests/export_test.py",
            "patches": [
                {
                    "old_start": 900,
                    "old_length": 6,
                    "new_start": 900,
                    "new_length": 82,
                    "hunk": "@@ -900,6 +900,82 @@ class JaxExportTest(jtu.JaxTestCase):\n         in_shardings=(jax.sharding.NamedSharding(mesh1, P(\"x\", None)),)\n       )(a)\n \n+  def test_call_with_different_no_of_devices(self):\n+    if jax.local_device_count() < 2:\n+      self.skipTest(\"Need at least 2 devices\")\n+\n+    @jax.jit\n+    def f_without_shardings(x):\n+      return jnp.sum(x ** 2, axis=0)\n+\n+    a = jnp.arange(jax.local_device_count() * 10, dtype=np.float32).reshape(\n+        (jax.local_device_count(), 10)\n+    )\n+    res_native = f_without_shardings(a)\n+    exp = get_exported(f_without_shardings)(a)\n+    self.assertEqual(exp.nr_devices, 1)\n+\n+    run_devices = jax.local_devices()\n+    run_mesh = Mesh(run_devices, \"i\")\n+    b = jax.device_put(a, jax.sharding.NamedSharding(run_mesh, P(\"i\")))\n+\n+    res_exported = export.call_exported(exp)(b)\n+    self.assertAllClose(res_native, res_exported)\n+\n+  def test_call_with_different_no_of_devices_error_has_in_shardings(self):\n+    if jax.local_device_count() < 2:\n+      self.skipTest(\"Need at least 2 devices\")\n+\n+    mesh_1 = Mesh(jax.local_devices()[:1], \"i\")\n+    @functools.partial(pjit.pjit,\n+                       in_shardings=NamedSharding(mesh_1, P(\"i\")))\n+    def f_with_sharding(x):\n+      return jnp.sum(x ** 2, axis=0)\n+\n+    a = jnp.arange(jax.device_count() * 10, dtype=np.float32).reshape(\n+        (jax.device_count(), 10)\n+    )\n+    exp = get_exported(f_with_sharding)(a)\n+    self.assertEqual(exp.nr_devices, 1)\n+\n+    run_devices = jax.local_devices()\n+    run_mesh = Mesh(run_devices, \"i\")\n+    b = jax.device_put(a, jax.sharding.NamedSharding(run_mesh, P(\"i\")))\n+\n+    with self.assertRaisesRegex(\n+        NotImplementedError,\n+        \"Exported module .* was lowered for 1 devices and is called in a \"\n+        f\"context with {jax.local_device_count()} devices.* module contains \"\n+        \"non-replicated sharding annotations\"):\n+      export.call_exported(exp)(b)\n+\n+  def test_call_with_different_no_of_devices_error_has_sharding_constraint(self):\n+    if jax.device_count() < 2:\n+      self.skipTest(\"Need at least 2 devices\")\n+\n+    mesh_1 = Mesh(jax.local_devices()[:1], \"i\")\n+    @jax.jit\n+    def f_with_sharding(x):\n+      x = jax.lax.with_sharding_constraint(x, NamedSharding(mesh_1, P(\"i\")))\n+      return jnp.sum(x ** 2, axis=0)\n+\n+    a = jnp.arange(jax.device_count() * 10, dtype=np.float32).reshape(\n+        (jax.device_count(), 10)\n+    )\n+    exp = get_exported(f_with_sharding)(a)\n+    self.assertEqual(exp.nr_devices, 1)\n+\n+    run_devices = jax.local_devices()\n+    run_mesh = Mesh(run_devices, \"i\")\n+    b = jax.device_put(a, jax.sharding.NamedSharding(run_mesh, P(\"i\")))\n+\n+    with self.assertRaisesRegex(\n+        NotImplementedError,\n+        \"Exported module .* was lowered for 1 devices and is called in a \"\n+        f\"context with {jax.local_device_count()} devices.* module contains \"\n+        \"non-replicated sharding annotations\"):\n+      export.call_exported(exp)(b)\n+\n   @jtu.parameterized_filterable(\n     kwargs=[\n       dict(testcase_name=f\"_poly={poly}\", poly=poly)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  def test_call_with_different_no_of_devices(self):\n+    if jax.local_device_count() < 2:\n+      self.skipTest(\"Need at least 2 devices\")\n+\n+    @jax.jit\n+    def f_without_shardings(x):\n+      return jnp.sum(x ** 2, axis=0)\n+\n+    a = jnp.arange(jax.local_device_count() * 10, dtype=np.float32).reshape(\n+        (jax.local_device_count(), 10)\n+    )\n+    res_native = f_without_shardings(a)\n+    exp = get_exported(f_without_shardings)(a)\n+    self.assertEqual(exp.nr_devices, 1)\n+\n+    run_devices = jax.local_devices()\n+    run_mesh = Mesh(run_devices, \"i\")\n+    b = jax.device_put(a, jax.sharding.NamedSharding(run_mesh, P(\"i\")))\n+\n+    res_exported = export.call_exported(exp)(b)\n+    self.assertAllClose(res_native, res_exported)\n+\n+  def test_call_with_different_no_of_devices_error_has_in_shardings(self):\n+    if jax.local_device_count() < 2:\n+      self.skipTest(\"Need at least 2 devices\")\n+\n+    mesh_1 = Mesh(jax.local_devices()[:1], \"i\")\n+    @functools.partial(pjit.pjit,\n+                       in_shardings=NamedSharding(mesh_1, P(\"i\")))\n+    def f_with_sharding(x):\n+      return jnp.sum(x ** 2, axis=0)\n+\n+    a = jnp.arange(jax.device_count() * 10, dtype=np.float32).reshape(\n+        (jax.device_count(), 10)\n+    )\n+    exp = get_exported(f_with_sharding)(a)\n+    self.assertEqual(exp.nr_devices, 1)\n+\n+    run_devices = jax.local_devices()\n+    run_mesh = Mesh(run_devices, \"i\")\n+    b = jax.device_put(a, jax.sharding.NamedSharding(run_mesh, P(\"i\")))\n+\n+    with self.assertRaisesRegex(\n+        NotImplementedError,\n+        \"Exported module .* was lowered for 1 devices and is called in a \"\n+        f\"context with {jax.local_device_count()} devices.* module contains \"\n+        \"non-replicated sharding annotations\"):\n+      export.call_exported(exp)(b)\n+\n+  def test_call_with_different_no_of_devices_error_has_sharding_constraint(self):\n+    if jax.device_count() < 2:\n+      self.skipTest(\"Need at least 2 devices\")\n+\n+    mesh_1 = Mesh(jax.local_devices()[:1], \"i\")\n+    @jax.jit\n+    def f_with_sharding(x):\n+      x = jax.lax.with_sharding_constraint(x, NamedSharding(mesh_1, P(\"i\")))\n+      return jnp.sum(x ** 2, axis=0)\n+\n+    a = jnp.arange(jax.device_count() * 10, dtype=np.float32).reshape(\n+        (jax.device_count(), 10)\n+    )\n+    exp = get_exported(f_with_sharding)(a)\n+    self.assertEqual(exp.nr_devices, 1)\n+\n+    run_devices = jax.local_devices()\n+    run_mesh = Mesh(run_devices, \"i\")\n+    b = jax.device_put(a, jax.sharding.NamedSharding(run_mesh, P(\"i\")))\n+\n+    with self.assertRaisesRegex(\n+        NotImplementedError,\n+        \"Exported module .* was lowered for 1 devices and is called in a \"\n+        f\"context with {jax.local_device_count()} devices.* module contains \"\n+        \"non-replicated sharding annotations\"):\n+      export.call_exported(exp)(b)\n+\n",
            "whole_hunk": "@@ -900,6 +900,82 @@ class JaxExportTest(jtu.JaxTestCase):\n         in_shardings=(jax.sharding.NamedSharding(mesh1, P(\"x\", None)),)\n       )(a)\n \n+  def test_call_with_different_no_of_devices(self):\n+    if jax.local_device_count() < 2:\n+      self.skipTest(\"Need at least 2 devices\")\n+\n+    @jax.jit\n+    def f_without_shardings(x):\n+      return jnp.sum(x ** 2, axis=0)\n+\n+    a = jnp.arange(jax.local_device_count() * 10, dtype=np.float32).reshape(\n+        (jax.local_device_count(), 10)\n+    )\n+    res_native = f_without_shardings(a)\n+    exp = get_exported(f_without_shardings)(a)\n+    self.assertEqual(exp.nr_devices, 1)\n+\n+    run_devices = jax.local_devices()\n+    run_mesh = Mesh(run_devices, \"i\")\n+    b = jax.device_put(a, jax.sharding.NamedSharding(run_mesh, P(\"i\")))\n+\n+    res_exported = export.call_exported(exp)(b)\n+    self.assertAllClose(res_native, res_exported)\n+\n+  def test_call_with_different_no_of_devices_error_has_in_shardings(self):\n+    if jax.local_device_count() < 2:\n+      self.skipTest(\"Need at least 2 devices\")\n+\n+    mesh_1 = Mesh(jax.local_devices()[:1], \"i\")\n+    @functools.partial(pjit.pjit,\n+                       in_shardings=NamedSharding(mesh_1, P(\"i\")))\n+    def f_with_sharding(x):\n+      return jnp.sum(x ** 2, axis=0)\n+\n+    a = jnp.arange(jax.device_count() * 10, dtype=np.float32).reshape(\n+        (jax.device_count(), 10)\n+    )\n+    exp = get_exported(f_with_sharding)(a)\n+    self.assertEqual(exp.nr_devices, 1)\n+\n+    run_devices = jax.local_devices()\n+    run_mesh = Mesh(run_devices, \"i\")\n+    b = jax.device_put(a, jax.sharding.NamedSharding(run_mesh, P(\"i\")))\n+\n+    with self.assertRaisesRegex(\n+        NotImplementedError,\n+        \"Exported module .* was lowered for 1 devices and is called in a \"\n+        f\"context with {jax.local_device_count()} devices.* module contains \"\n+        \"non-replicated sharding annotations\"):\n+      export.call_exported(exp)(b)\n+\n+  def test_call_with_different_no_of_devices_error_has_sharding_constraint(self):\n+    if jax.device_count() < 2:\n+      self.skipTest(\"Need at least 2 devices\")\n+\n+    mesh_1 = Mesh(jax.local_devices()[:1], \"i\")\n+    @jax.jit\n+    def f_with_sharding(x):\n+      x = jax.lax.with_sharding_constraint(x, NamedSharding(mesh_1, P(\"i\")))\n+      return jnp.sum(x ** 2, axis=0)\n+\n+    a = jnp.arange(jax.device_count() * 10, dtype=np.float32).reshape(\n+        (jax.device_count(), 10)\n+    )\n+    exp = get_exported(f_with_sharding)(a)\n+    self.assertEqual(exp.nr_devices, 1)\n+\n+    run_devices = jax.local_devices()\n+    run_mesh = Mesh(run_devices, \"i\")\n+    b = jax.device_put(a, jax.sharding.NamedSharding(run_mesh, P(\"i\")))\n+\n+    with self.assertRaisesRegex(\n+        NotImplementedError,\n+        \"Exported module .* was lowered for 1 devices and is called in a \"\n+        f\"context with {jax.local_device_count()} devices.* module contains \"\n+        \"non-replicated sharding annotations\"):\n+      export.call_exported(exp)(b)\n+\n   @jtu.parameterized_filterable(\n     kwargs=[\n       dict(testcase_name=f\"_poly={poly}\", poly=poly)"
        }
    ]
},
{
    "Id": 38,
    "commit_link": "https://github.com/google/jax/commit/b197ae527e632b76e438cc3896d75f79b478bfa7",
    "date": "2024-05-20T15:57:08-07:00",
    "message": "[Mosaic] Also check bitwidth in apply-vector-layout's `layoutIsValidForValue`.\n\nPiperOrigin-RevId: 635595321",
    "changes": [
        {
            "name": "apply_vector_layout.cc",
            "path": "jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc",
            "patches": [
                {
                    "old_start": 548,
                    "old_length": 8,
                    "new_start": 548,
                    "new_length": 21,
                    "hunk": "@@ -548,8 +548,21 @@ bool layoutIsValidForValue(const Layout &l, const Value v,\n                            const std::array<int64_t, 2> target_shape) {\n   // l must be non-null iff v is of vector type\n   if (const auto vty = dyn_cast<VectorType>(v.getType())) {\n-    return l.has_value() && l->isValid(target_shape) &&\n-           l->layout_rank() <= vty.getRank();\n+    if (!l.has_value()) {\n+      return false;\n+    }\n+\n+    // Vector type should have the same bitwidth as the layout, except for the\n+    // i1 special case, used for vmasks (see comment for VectorLayout class).\n+    if (!vty.getElementType().isIntOrFloat()) {\n+      return false;\n+    }\n+    const int8_t bitwidth = vty.getElementTypeBitWidth();\n+    if (bitwidth != l->bitwidth() && bitwidth != 1) {\n+      return false;\n+    }\n+\n+    return l->isValid(target_shape) && l->layout_rank() <= vty.getRank();\n   }\n   return !l.has_value();\n }"
                }
            ],
            "whole_deleted": "-    return l.has_value() && l->isValid(target_shape) &&\n-           l->layout_rank() <= vty.getRank();\n",
            "whole_added": "+    if (!l.has_value()) {\n+      return false;\n+    }\n+\n+    // Vector type should have the same bitwidth as the layout, except for the\n+    // i1 special case, used for vmasks (see comment for VectorLayout class).\n+    if (!vty.getElementType().isIntOrFloat()) {\n+      return false;\n+    }\n+    const int8_t bitwidth = vty.getElementTypeBitWidth();\n+    if (bitwidth != l->bitwidth() && bitwidth != 1) {\n+      return false;\n+    }\n+\n+    return l->isValid(target_shape) && l->layout_rank() <= vty.getRank();\n",
            "whole_hunk": "@@ -548,8 +548,21 @@ bool layoutIsValidForValue(const Layout &l, const Value v,\n                            const std::array<int64_t, 2> target_shape) {\n   // l must be non-null iff v is of vector type\n   if (const auto vty = dyn_cast<VectorType>(v.getType())) {\n-    return l.has_value() && l->isValid(target_shape) &&\n-           l->layout_rank() <= vty.getRank();\n+    if (!l.has_value()) {\n+      return false;\n+    }\n+\n+    // Vector type should have the same bitwidth as the layout, except for the\n+    // i1 special case, used for vmasks (see comment for VectorLayout class).\n+    if (!vty.getElementType().isIntOrFloat()) {\n+      return false;\n+    }\n+    const int8_t bitwidth = vty.getElementTypeBitWidth();\n+    if (bitwidth != l->bitwidth() && bitwidth != 1) {\n+      return false;\n+    }\n+\n+    return l->isValid(target_shape) && l->layout_rank() <= vty.getRank();\n   }\n   return !l.has_value();\n }"
        }
    ]
},
{
    "Id": 39,
    "commit_link": "https://github.com/google/jax/commit/cc3a380f7638ec8d0454c1f129b7559a2a578fc2",
    "date": "2024-05-20T09:52:38-07:00",
    "message": "Add unit test to check if the backend serialization/deserialization result equal to the original executable.\n\nPiperOrigin-RevId: 635485374",
    "changes": [
        {
            "name": "compilation_cache_test.py",
            "path": "tests/compilation_cache_test.py",
            "patches": [
                {
                    "old_start": 448,
                    "old_length": 6,
                    "new_start": 448,
                    "new_length": 20,
                    "hunk": "@@ -448,6 +448,20 @@ class CompilationCacheTest(jtu.JaxTestCase):\n       elif process_id == 1:\n         self.assertEqual(files_in_directory, 0)\n \n+  def test_backend_serialization_deserialization(self):\n+    backend = xla_bridge.get_backend()\n+    executable = (\n+        jax.jit(lambda x, y: x + y)\n+        .lower(np.array(1.), np.array(1.))\n+        .compile()\n+        .runtime_executable()\n+    )\n+    serialized_executable = backend.serialize_executable(executable)\n+    deserialized_executable = backend.deserialize_executable(\n+        serialized_executable, None)\n+    self.assertEqual(\n+        executable.fingerprint, deserialized_executable.fingerprint)\n+\n \n @jtu.with_config(\n     jax_enable_compilation_cache=False,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  def test_backend_serialization_deserialization(self):\n+    backend = xla_bridge.get_backend()\n+    executable = (\n+        jax.jit(lambda x, y: x + y)\n+        .lower(np.array(1.), np.array(1.))\n+        .compile()\n+        .runtime_executable()\n+    )\n+    serialized_executable = backend.serialize_executable(executable)\n+    deserialized_executable = backend.deserialize_executable(\n+        serialized_executable, None)\n+    self.assertEqual(\n+        executable.fingerprint, deserialized_executable.fingerprint)\n+\n",
            "whole_hunk": "@@ -448,6 +448,20 @@ class CompilationCacheTest(jtu.JaxTestCase):\n       elif process_id == 1:\n         self.assertEqual(files_in_directory, 0)\n \n+  def test_backend_serialization_deserialization(self):\n+    backend = xla_bridge.get_backend()\n+    executable = (\n+        jax.jit(lambda x, y: x + y)\n+        .lower(np.array(1.), np.array(1.))\n+        .compile()\n+        .runtime_executable()\n+    )\n+    serialized_executable = backend.serialize_executable(executable)\n+    deserialized_executable = backend.deserialize_executable(\n+        serialized_executable, None)\n+    self.assertEqual(\n+        executable.fingerprint, deserialized_executable.fingerprint)\n+\n \n @jtu.with_config(\n     jax_enable_compilation_cache=False,"
        }
    ]
},
{
    "Id": 40,
    "commit_link": "https://github.com/google/jax/commit/bb400756fc1196cbb383bdc26893fc2d61a0ee22",
    "date": "2024-05-15T12:49:27-07:00",
    "message": "[Mosaic GPU] Fix argument checking in case the input shape is not a tuple.\n\nPiperOrigin-RevId: 634044759",
    "changes": [
        {
            "name": "__init__.py",
            "path": "jax/experimental/mosaic/gpu/__init__.py",
            "patches": [
                {
                    "old_start": 537,
                    "old_length": 7,
                    "new_start": 537,
                    "new_length": 7,
                    "hunk": "@@ -537,7 +537,7 @@ def _lower_as_gpu_kernel(\n     body,\n     grid: tuple[int, ...],\n     block: tuple[int, ...],\n-    in_shape,\n+    in_shapes: tuple[Any, ...],\n     out_shape,\n     smem_scratch_shape,\n     prof_spec: profiler.ProfilerSpec | None = None,\n"
                },
                {
                    "old_start": 550,
                    "old_length": 11,
                    "new_start": 550,
                    "new_length": 7,
                    "hunk": "@@ -550,11 +550,7 @@ def _lower_as_gpu_kernel(\n   def _shape_to_ref_ty(shape: jax.ShapeDtypeStruct) -> ir.MemRefType:\n     return ir.MemRefType.get(shape.shape, mlir.dtype_to_ir_type(shape.dtype))\n \n-  if isinstance(in_shape, list):\n-    in_shape = tuple(in_shape)\n-  elif not isinstance(in_shape, tuple):\n-    in_shape = (in_shape,)\n-  in_ref_tys = [_shape_to_ref_ty(t) for t in in_shape]\n+  in_ref_tys = [_shape_to_ref_ty(t) for t in in_shapes]\n \n   unwrap_output_tuple = False\n   if isinstance(out_shape, list):\n"
                },
                {
                    "old_start": 631,
                    "old_length": 6,
                    "new_start": 627,
                    "new_length": 11,
                    "hunk": "@@ -631,6 +627,11 @@ def as_gpu_kernel(\n     smem_scratch_shape,\n     prof_spec: profiler.ProfilerSpec | None = None,\n ):\n+  if isinstance(in_shape, list):\n+    in_shape = tuple(in_shape)\n+  elif not isinstance(in_shape, tuple):\n+    in_shape = (in_shape,)\n+\n   module, out_shape, gmem_scratch_bytes, unwrap_output_tuple = (\n       _lower_as_gpu_kernel(\n           body, grid, block, in_shape, out_shape, smem_scratch_shape, prof_spec\n"
                },
                {
                    "old_start": 638,
                    "old_length": 12,
                    "new_start": 639,
                    "new_length": 12,
                    "hunk": "@@ -638,12 +639,12 @@ def as_gpu_kernel(\n   )\n \n   expected_arg_treedef = jax.tree.structure(in_shape)\n-  def _check_args(args):\n+  def _check_args(*args):\n     arg_treedef = jax.tree.structure(args)\n     if arg_treedef != expected_arg_treedef:\n       raise ValueError(\n           f\"Invalid argument structure: expected {expected_arg_treedef}, got\"\n-          f\" {arg_treedef}\"\n+          f\" {arg_treedef}, ({args=})\"\n       )\n \n   def bind(*args):\n"
                },
                {
                    "old_start": 657,
                    "old_length": 7,
                    "new_start": 658,
                    "new_length": 7,
                    "hunk": "@@ -657,7 +658,7 @@ def as_gpu_kernel(\n   if prof_spec is not None:\n     @jax.jit\n     def prof_kernel(*args):\n-      _check_args(args)\n+      _check_args(*args)\n       *results, prof_buffer = bind(*args)\n       def dump_profile(prof_buffer):\n         out_file = os.path.join(\n"
                },
                {
                    "old_start": 675,
                    "old_length": 7,
                    "new_start": 676,
                    "new_length": 7,
                    "hunk": "@@ -675,7 +676,7 @@ def as_gpu_kernel(\n   else:\n     @jax.jit\n     def kernel(*args):\n-      _check_args(args)\n+      _check_args(*args)\n       results = bind(*args)\n       return results[0] if unwrap_output_tuple else results\n     return kernel"
                }
            ],
            "whole_deleted": "-    in_shape,\n-  if isinstance(in_shape, list):\n-    in_shape = tuple(in_shape)\n-  elif not isinstance(in_shape, tuple):\n-    in_shape = (in_shape,)\n-  in_ref_tys = [_shape_to_ref_ty(t) for t in in_shape]\n-  def _check_args(args):\n-          f\" {arg_treedef}\"\n-      _check_args(args)\n-      _check_args(args)\n",
            "whole_added": "+    in_shapes: tuple[Any, ...],\n+  in_ref_tys = [_shape_to_ref_ty(t) for t in in_shapes]\n+  if isinstance(in_shape, list):\n+    in_shape = tuple(in_shape)\n+  elif not isinstance(in_shape, tuple):\n+    in_shape = (in_shape,)\n+\n+  def _check_args(*args):\n+          f\" {arg_treedef}, ({args=})\"\n+      _check_args(*args)\n+      _check_args(*args)\n",
            "whole_hunk": "@@ -537,7 +537,7 @@ def _lower_as_gpu_kernel(\n     body,\n     grid: tuple[int, ...],\n     block: tuple[int, ...],\n-    in_shape,\n+    in_shapes: tuple[Any, ...],\n     out_shape,\n     smem_scratch_shape,\n     prof_spec: profiler.ProfilerSpec | None = None,\n@@ -550,11 +550,7 @@ def _lower_as_gpu_kernel(\n   def _shape_to_ref_ty(shape: jax.ShapeDtypeStruct) -> ir.MemRefType:\n     return ir.MemRefType.get(shape.shape, mlir.dtype_to_ir_type(shape.dtype))\n \n-  if isinstance(in_shape, list):\n-    in_shape = tuple(in_shape)\n-  elif not isinstance(in_shape, tuple):\n-    in_shape = (in_shape,)\n-  in_ref_tys = [_shape_to_ref_ty(t) for t in in_shape]\n+  in_ref_tys = [_shape_to_ref_ty(t) for t in in_shapes]\n \n   unwrap_output_tuple = False\n   if isinstance(out_shape, list):\n@@ -631,6 +627,11 @@ def as_gpu_kernel(\n     smem_scratch_shape,\n     prof_spec: profiler.ProfilerSpec | None = None,\n ):\n+  if isinstance(in_shape, list):\n+    in_shape = tuple(in_shape)\n+  elif not isinstance(in_shape, tuple):\n+    in_shape = (in_shape,)\n+\n   module, out_shape, gmem_scratch_bytes, unwrap_output_tuple = (\n       _lower_as_gpu_kernel(\n           body, grid, block, in_shape, out_shape, smem_scratch_shape, prof_spec\n@@ -638,12 +639,12 @@ def as_gpu_kernel(\n   )\n \n   expected_arg_treedef = jax.tree.structure(in_shape)\n-  def _check_args(args):\n+  def _check_args(*args):\n     arg_treedef = jax.tree.structure(args)\n     if arg_treedef != expected_arg_treedef:\n       raise ValueError(\n           f\"Invalid argument structure: expected {expected_arg_treedef}, got\"\n-          f\" {arg_treedef}\"\n+          f\" {arg_treedef}, ({args=})\"\n       )\n \n   def bind(*args):\n@@ -657,7 +658,7 @@ def as_gpu_kernel(\n   if prof_spec is not None:\n     @jax.jit\n     def prof_kernel(*args):\n-      _check_args(args)\n+      _check_args(*args)\n       *results, prof_buffer = bind(*args)\n       def dump_profile(prof_buffer):\n         out_file = os.path.join(\n@@ -675,7 +676,7 @@ def as_gpu_kernel(\n   else:\n     @jax.jit\n     def kernel(*args):\n-      _check_args(args)\n+      _check_args(*args)\n       results = bind(*args)\n       return results[0] if unwrap_output_tuple else results\n     return kernel"
        }
    ]
},
{
    "Id": 41,
    "commit_link": "https://github.com/google/jax/commit/09a4b38ae2be601101aac032468ab92c92ec92f5",
    "date": "2024-05-15T15:40:27-04:00",
    "message": "Add informative error for invalid unroll in scan\n\nAs reported in #20481, setting `unroll=0` in `lax.scan` resulted in an\nuninformative `ZeroDivisionError`. This PR adds a check which raises a\n`ValueError` for `unroll<=0`.",
    "changes": [
        {
            "name": "loops.py",
            "path": "jax/_src/lax/control_flow/loops.py",
            "patches": [
                {
                    "old_start": 294,
                    "old_length": 6,
                    "new_start": 294,
                    "new_length": 8,
                    "hunk": "@@ -294,6 +294,8 @@ def scan(f: Callable[[Carry, X], tuple[Carry, Y]],\n \n   if isinstance(unroll, bool):\n     unroll = max(length, 1) if unroll else 1\n+  if unroll < 1:\n+    raise ValueError(\"`unroll` must be a `bool` or a positive `int`.\")\n   if attrs_tracked:\n     in_state = _get_states(attrs_tracked)\n     in_carry, in_ext = split_list(in_flat, [num_carry])\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  if unroll < 1:\n+    raise ValueError(\"`unroll` must be a `bool` or a positive `int`.\")\n",
            "whole_hunk": "@@ -294,6 +294,8 @@ def scan(f: Callable[[Carry, X], tuple[Carry, Y]],\n \n   if isinstance(unroll, bool):\n     unroll = max(length, 1) if unroll else 1\n+  if unroll < 1:\n+    raise ValueError(\"`unroll` must be a `bool` or a positive `int`.\")\n   if attrs_tracked:\n     in_state = _get_states(attrs_tracked)\n     in_carry, in_ext = split_list(in_flat, [num_carry])\n"
        },
        {
            "name": "lax_control_flow_test.py",
            "path": "tests/lax_control_flow_test.py",
            "patches": [
                {
                    "old_start": 1951,
                    "old_length": 6,
                    "new_start": 1951,
                    "new_length": 13,
                    "hunk": "@@ -1951,6 +1951,13 @@ class LaxControlFlowTest(jtu.JaxTestCase):\n                                   x[2]), None),\n                    (jnp.array(0, 'int32'),) * 3, None, length=1)\n \n+  @jax.enable_checks(False)\n+  def testScanInvalidUnrollRaises(self):\n+    with self.assertRaisesRegex(ValueError, \"`unroll` must be\"):\n+      jax.lax.scan(lambda x, _: (x, x), 0, jnp.arange(5), unroll=-1)\n+    with self.assertRaisesRegex(ValueError, \"`unroll` must be\"):\n+      jax.lax.scan(lambda x, _: (x, x), 0, jnp.arange(5), unroll=0)\n+\n   @parameterized.named_parameters(\n       {\"testcase_name\": f\"_{scan_name}\",\n        \"scan\": scan_impl}"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  @jax.enable_checks(False)\n+  def testScanInvalidUnrollRaises(self):\n+    with self.assertRaisesRegex(ValueError, \"`unroll` must be\"):\n+      jax.lax.scan(lambda x, _: (x, x), 0, jnp.arange(5), unroll=-1)\n+    with self.assertRaisesRegex(ValueError, \"`unroll` must be\"):\n+      jax.lax.scan(lambda x, _: (x, x), 0, jnp.arange(5), unroll=0)\n+\n",
            "whole_hunk": "@@ -1951,6 +1951,13 @@ class LaxControlFlowTest(jtu.JaxTestCase):\n                                   x[2]), None),\n                    (jnp.array(0, 'int32'),) * 3, None, length=1)\n \n+  @jax.enable_checks(False)\n+  def testScanInvalidUnrollRaises(self):\n+    with self.assertRaisesRegex(ValueError, \"`unroll` must be\"):\n+      jax.lax.scan(lambda x, _: (x, x), 0, jnp.arange(5), unroll=-1)\n+    with self.assertRaisesRegex(ValueError, \"`unroll` must be\"):\n+      jax.lax.scan(lambda x, _: (x, x), 0, jnp.arange(5), unroll=0)\n+\n   @parameterized.named_parameters(\n       {\"testcase_name\": f\"_{scan_name}\",\n        \"scan\": scan_impl}"
        }
    ]
},
{
    "Id": 42,
    "commit_link": "https://github.com/google/jax/commit/11a45138a5707eedfd05f322ac4e4d749260054b",
    "date": "2024-05-15T10:10:46-07:00",
    "message": "[Mosaic GPU] Explicitly check the smem size so that we get a good error rather than a cryptic cuda error\n\nPiperOrigin-RevId: 633993345",
    "changes": [
        {
            "name": "__init__.py",
            "path": "jax/experimental/mosaic/gpu/__init__.py",
            "patches": [
                {
                    "old_start": 488,
                    "old_length": 6,
                    "new_start": 488,
                    "new_length": 9,
                    "hunk": "@@ -488,6 +488,9 @@ def _launch(\n   if profiler_spec is not None:\n     smem_bytes += profiler_spec.smem_bytes(grid)\n \n+  # TODO(cperivol): Query the shared memory size programmatically.\n+  if smem_bytes > 228000:\n+    raise ValueError(f\"Mosaic GPU kernel exceeds available shared memory {smem_bytes=} > 228000\")\n   launch_op = gpu.LaunchOp(\n       token.type, [token], *grid_vals, *block_vals,\n       dynamicSharedMemorySize=c(smem_bytes, i32))"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  # TODO(cperivol): Query the shared memory size programmatically.\n+  if smem_bytes > 228000:\n+    raise ValueError(f\"Mosaic GPU kernel exceeds available shared memory {smem_bytes=} > 228000\")\n",
            "whole_hunk": "@@ -488,6 +488,9 @@ def _launch(\n   if profiler_spec is not None:\n     smem_bytes += profiler_spec.smem_bytes(grid)\n \n+  # TODO(cperivol): Query the shared memory size programmatically.\n+  if smem_bytes > 228000:\n+    raise ValueError(f\"Mosaic GPU kernel exceeds available shared memory {smem_bytes=} > 228000\")\n   launch_op = gpu.LaunchOp(\n       token.type, [token], *grid_vals, *block_vals,\n       dynamicSharedMemorySize=c(smem_bytes, i32))"
        }
    ]
},
{
    "Id": 43,
    "commit_link": "https://github.com/google/jax/commit/98aead70ebcc184b2ba92b524431efaa5b02171c",
    "date": "2024-05-13T20:09:43+03:00",
    "message": "[export] Relax the check that exported modules are used with same number of devices as when exported\n\nNow we allow a module exported for 1 device and not using any sharding annotations\nto be called from a computation that uses multiple devices. Such exported modules\ncan be parallelized trivially point-wise.",
    "changes": [
        {
            "name": "_export.py",
            "path": "jax/experimental/export/_export.py",
            "patches": [
                {
                    "old_start": 817,
                    "old_length": 14,
                    "new_start": 817,
                    "new_length": 15,
                    "hunk": "@@ -817,14 +817,15 @@ _CUSTOM_CALL_TARGETS_GUARANTEED_STABLE = {\n check_sharding_pattern = re.compile(r\"^({replicated}|{unknown shard_as.*}|\"\")$\")\n \n def _check_module(mod: ir.Module, *,\n-                  disabled_checks: Sequence[DisabledSafetyCheck]) -> None:\n+                  disabled_checks: Sequence[DisabledSafetyCheck]) -> bool:\n   \"\"\"Run a number of checks on the module.\n \n   Args:\n-    allow_non_replicated_sharding: whether the module is allowed to contain\n-      non_replicated sharding annotations.\n     disabled_checks: the safety checks that are disabled.\n+\n+  Returns True if the module uses non-replicated shardings.\n   \"\"\"\n+  sharding_attr = ir.StringAttr.get(\"Sharding\", mod.context)\n   allowed_custom_call_targets: set[str] = copy.copy(_CUSTOM_CALL_TARGETS_GUARANTEED_STABLE)\n   for dc in disabled_checks:\n     target = dc.is_custom_call()\n"
                },
                {
                    "old_start": 835,
                    "old_length": 13,
                    "new_start": 836,
                    "new_length": 28,
                    "hunk": "@@ -835,13 +836,28 @@ def _check_module(mod: ir.Module, *,\n       ir.StringAttr.get(target, mod.context)\n       for target in allowed_custom_call_targets}\n   disallowed_custom_call_ops: list[str] = []\n+  module_uses_non_replicated_sharding = False\n+  def check_sharding(op: ir.Operation, loc: ir.Location):\n+    try:\n+      sharding = op.attributes[\"mhlo.sharding\"]\n+    except KeyError:\n+      pass\n+    else:\n+      if not re.match(check_sharding_pattern, ir.StringAttr(sharding).value):\n+        nonlocal module_uses_non_replicated_sharding\n+        module_uses_non_replicated_sharding = True\n \n   def check_op(op: ir.Operation):\n     op_name = op.operation.name\n-    if op_name == \"stablehlo.custom_call\":\n+    if op_name == \"func.func\":\n+      check_sharding(op.operation, op.location)\n+\n+    elif op_name == \"stablehlo.custom_call\":\n       call_target_name_attr = op.operation.attributes[\"call_target_name\"]\n       if (call_target_name_attr not in allowed_custom_call_targets_attrs):\n         disallowed_custom_call_ops.append(f\"{op} at {op.location}\")\n+      if call_target_name_attr == sharding_attr:\n+        check_sharding(op, op.location)\n \n   def walk_operations(op):\n     check_op(op)\n"
                },
                {
                    "old_start": 858,
                    "old_length": 6,
                    "new_start": 874,
                    "new_length": 7,
                    "hunk": "@@ -858,6 +874,7 @@ def _check_module(mod: ir.Module, *,\n            f\"{disallowed_custom_call_ops_str}.\\n\"\n            \"See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#native-lowering-supports-only-select-custom-calls\")\n     raise ValueError(msg)\n+  return module_uses_non_replicated_sharding\n \n def expand_in_shardings(in_shardings: Sequence[LoweringSharding],\n                         module_kept_var_idx: Sequence[int],\n"
                },
                {
                    "old_start": 1094,
                    "old_length": 6,
                    "new_start": 1111,
                    "new_length": 7,
                    "hunk": "@@ -1094,6 +1111,7 @@ def _call_exported_lowering(ctx: mlir.LoweringRuleContext, *args,\n                             exported: Exported):\n   if exported.uses_shape_polymorphism:\n     ctx.module_context.shape_poly_state.uses_dim_vars = True\n+  submodule = ir.Module.parse(exported.mlir_module())\n \n   axis_context = ctx.module_context.axis_context\n   if isinstance(axis_context, sharding_impls.ShardingContext):\n"
                },
                {
                    "old_start": 1103,
                    "old_length": 17,
                    "new_start": 1121,
                    "new_length": 27,
                    "hunk": "@@ -1103,17 +1121,27 @@ def _call_exported_lowering(ctx: mlir.LoweringRuleContext, *args,\n   else:\n     raise NotImplementedError(type(axis_context))\n   if num_devices != exported.nr_devices:\n-    raise NotImplementedError(\n-      f\"Exported module {exported.fun_name} was lowered for \"\n-      f\"{exported.nr_devices} devices and is called in a context with \"\n-      f\"{num_devices} devices\"\n-    )\n+    # In some special cases we allow running with a different number of devices\n+    # than the function was exported for.\n+    err_msg = \"\"\n+    if exported.nr_devices != 1:\n+      err_msg = \"the module was lowered for more than 1 device.\"\n+    elif (_check_module(submodule, disabled_checks=()) or\n+          any(s is not None and not s.is_replicated()\n+              for s in exported.in_shardings + exported.out_shardings)):\n+      err_msg = \"the module contains non-replicated sharding annotations.\"\n+    if err_msg:\n+      raise NotImplementedError(\n+        f\"Exported module {exported.fun_name} was lowered for \"\n+        f\"{exported.nr_devices} devices and is called in a context with \"\n+        f\"{num_devices} devices. This is disallowed because: {err_msg}\"\n+      )\n \n   # Apply in_shardings\n   args = tuple(\n     wrap_with_sharding(ctx, x, x_aval, x_sharding)\n     for x, x_aval, x_sharding in zip(args, ctx.avals_in, exported.in_shardings))\n-  submodule = ir.Module.parse(exported.mlir_module())\n+\n   symtab = ir.SymbolTable(submodule.operation)\n   # The called function may have been exported with polymorphic shapes and called\n   # now with more refined shapes. We insert hlo.ConvertOp to ensure the module\n"
                }
            ],
            "whole_deleted": "-                  disabled_checks: Sequence[DisabledSafetyCheck]) -> None:\n-    allow_non_replicated_sharding: whether the module is allowed to contain\n-      non_replicated sharding annotations.\n-    if op_name == \"stablehlo.custom_call\":\n-    raise NotImplementedError(\n-      f\"Exported module {exported.fun_name} was lowered for \"\n-      f\"{exported.nr_devices} devices and is called in a context with \"\n-      f\"{num_devices} devices\"\n-    )\n-  submodule = ir.Module.parse(exported.mlir_module())\n",
            "whole_added": "+                  disabled_checks: Sequence[DisabledSafetyCheck]) -> bool:\n+\n+  Returns True if the module uses non-replicated shardings.\n+  sharding_attr = ir.StringAttr.get(\"Sharding\", mod.context)\n+  module_uses_non_replicated_sharding = False\n+  def check_sharding(op: ir.Operation, loc: ir.Location):\n+    try:\n+      sharding = op.attributes[\"mhlo.sharding\"]\n+    except KeyError:\n+      pass\n+    else:\n+      if not re.match(check_sharding_pattern, ir.StringAttr(sharding).value):\n+        nonlocal module_uses_non_replicated_sharding\n+        module_uses_non_replicated_sharding = True\n+    if op_name == \"func.func\":\n+      check_sharding(op.operation, op.location)\n+\n+    elif op_name == \"stablehlo.custom_call\":\n+      if call_target_name_attr == sharding_attr:\n+        check_sharding(op, op.location)\n+  return module_uses_non_replicated_sharding\n+  submodule = ir.Module.parse(exported.mlir_module())\n+    # In some special cases we allow running with a different number of devices\n+    # than the function was exported for.\n+    err_msg = \"\"\n+    if exported.nr_devices != 1:\n+      err_msg = \"the module was lowered for more than 1 device.\"\n+    elif (_check_module(submodule, disabled_checks=()) or\n+          any(s is not None and not s.is_replicated()\n+              for s in exported.in_shardings + exported.out_shardings)):\n+      err_msg = \"the module contains non-replicated sharding annotations.\"\n+    if err_msg:\n+      raise NotImplementedError(\n+        f\"Exported module {exported.fun_name} was lowered for \"\n+        f\"{exported.nr_devices} devices and is called in a context with \"\n+        f\"{num_devices} devices. This is disallowed because: {err_msg}\"\n+      )\n+\n",
            "whole_hunk": "@@ -817,14 +817,15 @@ _CUSTOM_CALL_TARGETS_GUARANTEED_STABLE = {\n check_sharding_pattern = re.compile(r\"^({replicated}|{unknown shard_as.*}|\"\")$\")\n \n def _check_module(mod: ir.Module, *,\n-                  disabled_checks: Sequence[DisabledSafetyCheck]) -> None:\n+                  disabled_checks: Sequence[DisabledSafetyCheck]) -> bool:\n   \"\"\"Run a number of checks on the module.\n \n   Args:\n-    allow_non_replicated_sharding: whether the module is allowed to contain\n-      non_replicated sharding annotations.\n     disabled_checks: the safety checks that are disabled.\n+\n+  Returns True if the module uses non-replicated shardings.\n   \"\"\"\n+  sharding_attr = ir.StringAttr.get(\"Sharding\", mod.context)\n   allowed_custom_call_targets: set[str] = copy.copy(_CUSTOM_CALL_TARGETS_GUARANTEED_STABLE)\n   for dc in disabled_checks:\n     target = dc.is_custom_call()\n@@ -835,13 +836,28 @@ def _check_module(mod: ir.Module, *,\n       ir.StringAttr.get(target, mod.context)\n       for target in allowed_custom_call_targets}\n   disallowed_custom_call_ops: list[str] = []\n+  module_uses_non_replicated_sharding = False\n+  def check_sharding(op: ir.Operation, loc: ir.Location):\n+    try:\n+      sharding = op.attributes[\"mhlo.sharding\"]\n+    except KeyError:\n+      pass\n+    else:\n+      if not re.match(check_sharding_pattern, ir.StringAttr(sharding).value):\n+        nonlocal module_uses_non_replicated_sharding\n+        module_uses_non_replicated_sharding = True\n \n   def check_op(op: ir.Operation):\n     op_name = op.operation.name\n-    if op_name == \"stablehlo.custom_call\":\n+    if op_name == \"func.func\":\n+      check_sharding(op.operation, op.location)\n+\n+    elif op_name == \"stablehlo.custom_call\":\n       call_target_name_attr = op.operation.attributes[\"call_target_name\"]\n       if (call_target_name_attr not in allowed_custom_call_targets_attrs):\n         disallowed_custom_call_ops.append(f\"{op} at {op.location}\")\n+      if call_target_name_attr == sharding_attr:\n+        check_sharding(op, op.location)\n \n   def walk_operations(op):\n     check_op(op)\n@@ -858,6 +874,7 @@ def _check_module(mod: ir.Module, *,\n            f\"{disallowed_custom_call_ops_str}.\\n\"\n            \"See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#native-lowering-supports-only-select-custom-calls\")\n     raise ValueError(msg)\n+  return module_uses_non_replicated_sharding\n \n def expand_in_shardings(in_shardings: Sequence[LoweringSharding],\n                         module_kept_var_idx: Sequence[int],\n@@ -1094,6 +1111,7 @@ def _call_exported_lowering(ctx: mlir.LoweringRuleContext, *args,\n                             exported: Exported):\n   if exported.uses_shape_polymorphism:\n     ctx.module_context.shape_poly_state.uses_dim_vars = True\n+  submodule = ir.Module.parse(exported.mlir_module())\n \n   axis_context = ctx.module_context.axis_context\n   if isinstance(axis_context, sharding_impls.ShardingContext):\n@@ -1103,17 +1121,27 @@ def _call_exported_lowering(ctx: mlir.LoweringRuleContext, *args,\n   else:\n     raise NotImplementedError(type(axis_context))\n   if num_devices != exported.nr_devices:\n-    raise NotImplementedError(\n-      f\"Exported module {exported.fun_name} was lowered for \"\n-      f\"{exported.nr_devices} devices and is called in a context with \"\n-      f\"{num_devices} devices\"\n-    )\n+    # In some special cases we allow running with a different number of devices\n+    # than the function was exported for.\n+    err_msg = \"\"\n+    if exported.nr_devices != 1:\n+      err_msg = \"the module was lowered for more than 1 device.\"\n+    elif (_check_module(submodule, disabled_checks=()) or\n+          any(s is not None and not s.is_replicated()\n+              for s in exported.in_shardings + exported.out_shardings)):\n+      err_msg = \"the module contains non-replicated sharding annotations.\"\n+    if err_msg:\n+      raise NotImplementedError(\n+        f\"Exported module {exported.fun_name} was lowered for \"\n+        f\"{exported.nr_devices} devices and is called in a context with \"\n+        f\"{num_devices} devices. This is disallowed because: {err_msg}\"\n+      )\n \n   # Apply in_shardings\n   args = tuple(\n     wrap_with_sharding(ctx, x, x_aval, x_sharding)\n     for x, x_aval, x_sharding in zip(args, ctx.avals_in, exported.in_shardings))\n-  submodule = ir.Module.parse(exported.mlir_module())\n+\n   symtab = ir.SymbolTable(submodule.operation)\n   # The called function may have been exported with polymorphic shapes and called\n   # now with more refined shapes. We insert hlo.ConvertOp to ensure the module\n"
        },
        {
            "name": "export_test.py",
            "path": "tests/export_test.py",
            "patches": [
                {
                    "old_start": 900,
                    "old_length": 6,
                    "new_start": 900,
                    "new_length": 82,
                    "hunk": "@@ -900,6 +900,82 @@ class JaxExportTest(jtu.JaxTestCase):\n         in_shardings=(jax.sharding.NamedSharding(mesh1, P(\"x\", None)),)\n       )(a)\n \n+  def test_call_with_different_no_of_devices(self):\n+    if jax.local_device_count() < 2:\n+      self.skipTest(\"Need at least 2 devices\")\n+\n+    @jax.jit\n+    def f_without_shardings(x):\n+      return jnp.sum(x ** 2, axis=0)\n+\n+    a = jnp.arange(jax.local_device_count() * 10, dtype=np.float32).reshape(\n+        (jax.local_device_count(), 10)\n+    )\n+    res_native = f_without_shardings(a)\n+    exp = get_exported(f_without_shardings)(a)\n+    self.assertEqual(exp.nr_devices, 1)\n+\n+    run_devices = jax.local_devices()\n+    run_mesh = Mesh(run_devices, \"i\")\n+    b = jax.device_put(a, jax.sharding.NamedSharding(run_mesh, P(\"i\")))\n+\n+    res_exported = export.call_exported(exp)(b)\n+    self.assertAllClose(res_native, res_exported)\n+\n+  def test_call_with_different_no_of_devices_error_has_in_shardings(self):\n+    if jax.local_device_count() < 2:\n+      self.skipTest(\"Need at least 2 devices\")\n+\n+    mesh_1 = Mesh(jax.local_devices()[:1], \"i\")\n+    @functools.partial(pjit.pjit,\n+                       in_shardings=NamedSharding(mesh_1, P(\"i\")))\n+    def f_with_sharding(x):\n+      return jnp.sum(x ** 2, axis=0)\n+\n+    a = jnp.arange(jax.device_count() * 10, dtype=np.float32).reshape(\n+        (jax.device_count(), 10)\n+    )\n+    exp = get_exported(f_with_sharding)(a)\n+    self.assertEqual(exp.nr_devices, 1)\n+\n+    run_devices = jax.local_devices()\n+    run_mesh = Mesh(run_devices, \"i\")\n+    b = jax.device_put(a, jax.sharding.NamedSharding(run_mesh, P(\"i\")))\n+\n+    with self.assertRaisesRegex(\n+        NotImplementedError,\n+        \"Exported module .* was lowered for 1 devices and is called in a \"\n+        f\"context with {jax.local_device_count()} devices.* module contains \"\n+        \"non-replicated sharding annotations\"):\n+      export.call_exported(exp)(b)\n+\n+  def test_call_with_different_no_of_devices_error_has_sharding_constraint(self):\n+    if jax.device_count() < 2:\n+      self.skipTest(\"Need at least 2 devices\")\n+\n+    mesh_1 = Mesh(jax.local_devices()[:1], \"i\")\n+    @jax.jit\n+    def f_with_sharding(x):\n+      x = jax.lax.with_sharding_constraint(x, NamedSharding(mesh_1, P(\"i\")))\n+      return jnp.sum(x ** 2, axis=0)\n+\n+    a = jnp.arange(jax.device_count() * 10, dtype=np.float32).reshape(\n+        (jax.device_count(), 10)\n+    )\n+    exp = get_exported(f_with_sharding)(a)\n+    self.assertEqual(exp.nr_devices, 1)\n+\n+    run_devices = jax.local_devices()\n+    run_mesh = Mesh(run_devices, \"i\")\n+    b = jax.device_put(a, jax.sharding.NamedSharding(run_mesh, P(\"i\")))\n+\n+    with self.assertRaisesRegex(\n+        NotImplementedError,\n+        \"Exported module .* was lowered for 1 devices and is called in a \"\n+        f\"context with {jax.local_device_count()} devices.* module contains \"\n+        \"non-replicated sharding annotations\"):\n+      export.call_exported(exp)(b)\n+\n   @jtu.parameterized_filterable(\n     kwargs=[\n       dict(testcase_name=f\"_poly={poly}\", poly=poly)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  def test_call_with_different_no_of_devices(self):\n+    if jax.local_device_count() < 2:\n+      self.skipTest(\"Need at least 2 devices\")\n+\n+    @jax.jit\n+    def f_without_shardings(x):\n+      return jnp.sum(x ** 2, axis=0)\n+\n+    a = jnp.arange(jax.local_device_count() * 10, dtype=np.float32).reshape(\n+        (jax.local_device_count(), 10)\n+    )\n+    res_native = f_without_shardings(a)\n+    exp = get_exported(f_without_shardings)(a)\n+    self.assertEqual(exp.nr_devices, 1)\n+\n+    run_devices = jax.local_devices()\n+    run_mesh = Mesh(run_devices, \"i\")\n+    b = jax.device_put(a, jax.sharding.NamedSharding(run_mesh, P(\"i\")))\n+\n+    res_exported = export.call_exported(exp)(b)\n+    self.assertAllClose(res_native, res_exported)\n+\n+  def test_call_with_different_no_of_devices_error_has_in_shardings(self):\n+    if jax.local_device_count() < 2:\n+      self.skipTest(\"Need at least 2 devices\")\n+\n+    mesh_1 = Mesh(jax.local_devices()[:1], \"i\")\n+    @functools.partial(pjit.pjit,\n+                       in_shardings=NamedSharding(mesh_1, P(\"i\")))\n+    def f_with_sharding(x):\n+      return jnp.sum(x ** 2, axis=0)\n+\n+    a = jnp.arange(jax.device_count() * 10, dtype=np.float32).reshape(\n+        (jax.device_count(), 10)\n+    )\n+    exp = get_exported(f_with_sharding)(a)\n+    self.assertEqual(exp.nr_devices, 1)\n+\n+    run_devices = jax.local_devices()\n+    run_mesh = Mesh(run_devices, \"i\")\n+    b = jax.device_put(a, jax.sharding.NamedSharding(run_mesh, P(\"i\")))\n+\n+    with self.assertRaisesRegex(\n+        NotImplementedError,\n+        \"Exported module .* was lowered for 1 devices and is called in a \"\n+        f\"context with {jax.local_device_count()} devices.* module contains \"\n+        \"non-replicated sharding annotations\"):\n+      export.call_exported(exp)(b)\n+\n+  def test_call_with_different_no_of_devices_error_has_sharding_constraint(self):\n+    if jax.device_count() < 2:\n+      self.skipTest(\"Need at least 2 devices\")\n+\n+    mesh_1 = Mesh(jax.local_devices()[:1], \"i\")\n+    @jax.jit\n+    def f_with_sharding(x):\n+      x = jax.lax.with_sharding_constraint(x, NamedSharding(mesh_1, P(\"i\")))\n+      return jnp.sum(x ** 2, axis=0)\n+\n+    a = jnp.arange(jax.device_count() * 10, dtype=np.float32).reshape(\n+        (jax.device_count(), 10)\n+    )\n+    exp = get_exported(f_with_sharding)(a)\n+    self.assertEqual(exp.nr_devices, 1)\n+\n+    run_devices = jax.local_devices()\n+    run_mesh = Mesh(run_devices, \"i\")\n+    b = jax.device_put(a, jax.sharding.NamedSharding(run_mesh, P(\"i\")))\n+\n+    with self.assertRaisesRegex(\n+        NotImplementedError,\n+        \"Exported module .* was lowered for 1 devices and is called in a \"\n+        f\"context with {jax.local_device_count()} devices.* module contains \"\n+        \"non-replicated sharding annotations\"):\n+      export.call_exported(exp)(b)\n+\n",
            "whole_hunk": "@@ -900,6 +900,82 @@ class JaxExportTest(jtu.JaxTestCase):\n         in_shardings=(jax.sharding.NamedSharding(mesh1, P(\"x\", None)),)\n       )(a)\n \n+  def test_call_with_different_no_of_devices(self):\n+    if jax.local_device_count() < 2:\n+      self.skipTest(\"Need at least 2 devices\")\n+\n+    @jax.jit\n+    def f_without_shardings(x):\n+      return jnp.sum(x ** 2, axis=0)\n+\n+    a = jnp.arange(jax.local_device_count() * 10, dtype=np.float32).reshape(\n+        (jax.local_device_count(), 10)\n+    )\n+    res_native = f_without_shardings(a)\n+    exp = get_exported(f_without_shardings)(a)\n+    self.assertEqual(exp.nr_devices, 1)\n+\n+    run_devices = jax.local_devices()\n+    run_mesh = Mesh(run_devices, \"i\")\n+    b = jax.device_put(a, jax.sharding.NamedSharding(run_mesh, P(\"i\")))\n+\n+    res_exported = export.call_exported(exp)(b)\n+    self.assertAllClose(res_native, res_exported)\n+\n+  def test_call_with_different_no_of_devices_error_has_in_shardings(self):\n+    if jax.local_device_count() < 2:\n+      self.skipTest(\"Need at least 2 devices\")\n+\n+    mesh_1 = Mesh(jax.local_devices()[:1], \"i\")\n+    @functools.partial(pjit.pjit,\n+                       in_shardings=NamedSharding(mesh_1, P(\"i\")))\n+    def f_with_sharding(x):\n+      return jnp.sum(x ** 2, axis=0)\n+\n+    a = jnp.arange(jax.device_count() * 10, dtype=np.float32).reshape(\n+        (jax.device_count(), 10)\n+    )\n+    exp = get_exported(f_with_sharding)(a)\n+    self.assertEqual(exp.nr_devices, 1)\n+\n+    run_devices = jax.local_devices()\n+    run_mesh = Mesh(run_devices, \"i\")\n+    b = jax.device_put(a, jax.sharding.NamedSharding(run_mesh, P(\"i\")))\n+\n+    with self.assertRaisesRegex(\n+        NotImplementedError,\n+        \"Exported module .* was lowered for 1 devices and is called in a \"\n+        f\"context with {jax.local_device_count()} devices.* module contains \"\n+        \"non-replicated sharding annotations\"):\n+      export.call_exported(exp)(b)\n+\n+  def test_call_with_different_no_of_devices_error_has_sharding_constraint(self):\n+    if jax.device_count() < 2:\n+      self.skipTest(\"Need at least 2 devices\")\n+\n+    mesh_1 = Mesh(jax.local_devices()[:1], \"i\")\n+    @jax.jit\n+    def f_with_sharding(x):\n+      x = jax.lax.with_sharding_constraint(x, NamedSharding(mesh_1, P(\"i\")))\n+      return jnp.sum(x ** 2, axis=0)\n+\n+    a = jnp.arange(jax.device_count() * 10, dtype=np.float32).reshape(\n+        (jax.device_count(), 10)\n+    )\n+    exp = get_exported(f_with_sharding)(a)\n+    self.assertEqual(exp.nr_devices, 1)\n+\n+    run_devices = jax.local_devices()\n+    run_mesh = Mesh(run_devices, \"i\")\n+    b = jax.device_put(a, jax.sharding.NamedSharding(run_mesh, P(\"i\")))\n+\n+    with self.assertRaisesRegex(\n+        NotImplementedError,\n+        \"Exported module .* was lowered for 1 devices and is called in a \"\n+        f\"context with {jax.local_device_count()} devices.* module contains \"\n+        \"non-replicated sharding annotations\"):\n+      export.call_exported(exp)(b)\n+\n   @jtu.parameterized_filterable(\n     kwargs=[\n       dict(testcase_name=f\"_poly={poly}\", poly=poly)"
        }
    ]
},
{
    "Id": 44,
    "commit_link": "https://github.com/google/jax/commit/78d4d0a498082547166f549c568a9727588625d1",
    "date": "2024-05-13T14:41:51+03:00",
    "message": "[export] Simplify export internals, prepare for integration with AOT APIs\n\nIn preparation for a better integration of the jax.experimental.export with\nthe AOT APIs, we make several simplifications:\n\n  * turn on always the generation of shape assertions in presence of shape\n  polymorphism. Previously, shape assertions were turned on unless the\n  serialization version was less than 7 (possible only before March 27th, 2024\n  when the minimum serialization version was bumped to 9), or if the\n  user specified explicitly that shape assertions should be turned off. It is\n  not safe to turn off shape assertions and I am not aware of an instance where\n  somebody had to turn them off, except for temporary debugging. We keep the\n  `DisabledSafetyCheck.shape_assertions` API for now, for backwards compatibility,\n  but it has no effect and it emits a deprecation warning.\n\n  * remove the code that was conditional on the serialization version\n  being less than 9, e.g., for the lowering in presence of effects.\n\n  * remove a safety check that ensures that when `export` is used on JAX\n  callables, i.e., not the result of `jax.jit`, the code should not\n  contain non-replicated sharding annotations. This usage of `export` is\n  rare and will be removed once `export` will be integrated with the AOT\n  APIs.\n\n  * remove code that was needed only for older jaxlib to replace_tokens_with_dummy.",
    "changes": [
        {
            "name": "mlir.py",
            "path": "jax/_src/interpreters/mlir.py",
            "patches": [
                {
                    "old_start": 571,
                    "old_length": 18,
                    "new_start": 571,
                    "new_length": 6,
                    "hunk": "@@ -571,18 +571,6 @@ class LoweringParameters:\n   # or multi-platform lowering.\n   global_constant_computation: bool = False\n \n-  # TODO(b/302258959): in JAX native execution we cannot lower the tokens\n-  # to stablehlo.token for the top-level function, due to runtime limitations.\n-  # Instead, we use dummy bool[0] arrays. This is controlled by setting\n-  # replace_tokens_with_dummy to True (default). However, when exporting StableHLO\n-  # we can use real tokens, because the resulting StableHLO will not be\n-  # executed directly, but will be embedded as an inner function in a larger\n-  # JAX or TensorFlow program. In these cases, replace_tokens_with_dummy must\n-  # be set to False (for serialization versions >= 9).\n-  # Once the PJRT is extended to use tokens, we can use tokens even in the\n-  # native execution (and we can remove this parameter).\n-  # This parameter can be removed when minimum xla_extension_version is >= 260.\n-  replace_tokens_with_dummy: bool = True\n \n @dataclasses.dataclass\n class TracebackCaches:\n"
                },
                {
                    "old_start": 971,
                    "old_length": 13,
                    "new_start": 959,
                    "new_length": 10,
                    "hunk": "@@ -971,13 +959,10 @@ def lower_jaxpr_to_module(\n     attrs[\"sym_name\"] = ir.StringAttr.get(module_name)\n     attrs[\"mhlo.num_replicas\"] = i32_attr(num_replicas)\n     attrs[\"mhlo.num_partitions\"] = i32_attr(num_partitions)\n-    replace_tokens_with_dummy = False\n     lower_jaxpr_to_fun(\n         ctx, \"main\", jaxpr, ordered_effects,\n         name_stack=name_stack,\n         public=True,\n-        create_tokens=replace_tokens_with_dummy,\n-        replace_tokens_with_dummy=replace_tokens_with_dummy,\n         num_output_tokens=0,\n         replicated_args=replicated_args,\n         arg_shardings=arg_shardings,\n"
                }
            ],
            "whole_deleted": "-  # TODO(b/302258959): in JAX native execution we cannot lower the tokens\n-  # to stablehlo.token for the top-level function, due to runtime limitations.\n-  # Instead, we use dummy bool[0] arrays. This is controlled by setting\n-  # replace_tokens_with_dummy to True (default). However, when exporting StableHLO\n-  # we can use real tokens, because the resulting StableHLO will not be\n-  # executed directly, but will be embedded as an inner function in a larger\n-  # JAX or TensorFlow program. In these cases, replace_tokens_with_dummy must\n-  # be set to False (for serialization versions >= 9).\n-  # Once the PJRT is extended to use tokens, we can use tokens even in the\n-  # native execution (and we can remove this parameter).\n-  # This parameter can be removed when minimum xla_extension_version is >= 260.\n-  replace_tokens_with_dummy: bool = True\n-    replace_tokens_with_dummy = False\n-        create_tokens=replace_tokens_with_dummy,\n-        replace_tokens_with_dummy=replace_tokens_with_dummy,\n",
            "whole_added": "",
            "whole_hunk": "@@ -571,18 +571,6 @@ class LoweringParameters:\n   # or multi-platform lowering.\n   global_constant_computation: bool = False\n \n-  # TODO(b/302258959): in JAX native execution we cannot lower the tokens\n-  # to stablehlo.token for the top-level function, due to runtime limitations.\n-  # Instead, we use dummy bool[0] arrays. This is controlled by setting\n-  # replace_tokens_with_dummy to True (default). However, when exporting StableHLO\n-  # we can use real tokens, because the resulting StableHLO will not be\n-  # executed directly, but will be embedded as an inner function in a larger\n-  # JAX or TensorFlow program. In these cases, replace_tokens_with_dummy must\n-  # be set to False (for serialization versions >= 9).\n-  # Once the PJRT is extended to use tokens, we can use tokens even in the\n-  # native execution (and we can remove this parameter).\n-  # This parameter can be removed when minimum xla_extension_version is >= 260.\n-  replace_tokens_with_dummy: bool = True\n \n @dataclasses.dataclass\n class TracebackCaches:\n@@ -971,13 +959,10 @@ def lower_jaxpr_to_module(\n     attrs[\"sym_name\"] = ir.StringAttr.get(module_name)\n     attrs[\"mhlo.num_replicas\"] = i32_attr(num_replicas)\n     attrs[\"mhlo.num_partitions\"] = i32_attr(num_partitions)\n-    replace_tokens_with_dummy = False\n     lower_jaxpr_to_fun(\n         ctx, \"main\", jaxpr, ordered_effects,\n         name_stack=name_stack,\n         public=True,\n-        create_tokens=replace_tokens_with_dummy,\n-        replace_tokens_with_dummy=replace_tokens_with_dummy,\n         num_output_tokens=0,\n         replicated_args=replicated_args,\n         arg_shardings=arg_shardings,\n"
        },
        {
            "name": "_export.py",
            "path": "jax/experimental/export/_export.py",
            "patches": [
                {
                    "old_start": 69,
                    "old_length": 9,
                    "new_start": 69,
                    "new_length": 6,
                    "hunk": "@@ -69,9 +69,6 @@ Sharding = Union[xla_client.HloSharding, None]\n minimum_supported_serialization_version = 9\n maximum_supported_serialization_version = 9\n \n-_VERSION_START_SUPPORT_SHAPE_ASSERTIONS = 7\n-_VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS = 9\n-\n \n class DisabledSafetyCheck:\n   \"\"\"A safety check should be skipped on (de)serialization.\n"
                },
                {
                    "old_start": 106,
                    "old_length": 11,
                    "new_start": 103,
                    "new_length": 16,
                    "hunk": "@@ -106,11 +103,16 @@ class DisabledSafetyCheck:\n \n   @classmethod\n   def shape_assertions(cls) -> DisabledSafetyCheck:\n-    \"\"\"Allows invocations with shapes that do not meet the constraints.\n+    \"\"\"A noop. DEPRECATED.\n \n-    Has effect on serialization (to suppress the generation of the assertions)\n-    and also on deserialization (to suppress the checking of the assertions).\n+    Was used previously to allow invocations with shapes that do not meet the\n+    constraints. Has no effect anymore, shape assertions cannot be disabled.\n     \"\"\"\n+    # TODO(necula): remove this after compatibility period. Was deprecated in\n+    # May 2024.\n+    warnings.warn(\n+        \"DisabledSafetyCheck.shape_assertions is deprecated, has no effect anymore\",\n+        DeprecationWarning, stacklevel=2)\n     return DisabledSafetyCheck(\"shape_assertions\")\n \n   def is_custom_call(self) -> str | None:\n"
                },
                {
                    "old_start": 344,
                    "old_length": 10,
                    "new_start": 346,
                    "new_length": 6,
                    "hunk": "@@ -344,10 +346,6 @@ def args_specs(\n   return _shape_poly.symbolic_args_specs(args, polymorphic_shapes)\n \n \n-\n-def _keep_main_tokens(serialization_version: int) -> bool:\n-  return serialization_version >= _VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS\n-\n def export(fun_jax: Callable,\n            *,\n            lowering_platforms: Sequence[str] | None = None,\n"
                },
                {
                    "old_start": 391,
                    "old_length": 71,
                    "new_start": 389,
                    "new_length": 58,
                    "hunk": "@@ -391,71 +389,58 @@ def export(fun_jax: Callable,\n       # convert(f_jax), in which case a \"jit\" is implied. In that case we raise\n       # an error if the lowered function contains non-replicated sharding annotations.\n       wrapped_fun_jax = jax.jit(fun_jax)\n-      allow_non_replicated_sharding = False\n     else:\n       # If we have a pjit or pmap already we do not wrap with another, and we\n       # allow shardings.\n       wrapped_fun_jax = fun_jax  # type: ignore\n-      allow_non_replicated_sharding = True\n \n     if lowering_platforms is not None:\n       actual_lowering_platforms = tuple(lowering_platforms)\n     else:\n       actual_lowering_platforms = (default_lowering_platform(),)\n \n-    # Do not include shape assertions if the version is < 7.\n-    enable_shape_assertions = (\n-        DisabledSafetyCheck.shape_assertions() not in disabled_checks and\n-        version >= _VERSION_START_SUPPORT_SHAPE_ASSERTIONS)  # type: ignore\n-    try:\n-      prev_enable_shape_assertions = _shape_poly.thread_local_state.enable_shape_assertions\n-      _shape_poly.thread_local_state.enable_shape_assertions = enable_shape_assertions\n-      replace_tokens_with_dummy = not _keep_main_tokens(version)\n-\n-      symbolic_scope: tuple[_shape_poly.SymbolicScope, tree_util.KeyPath] | None = None\n-      for k_path, aval in tree_util.tree_flatten_with_path((args_specs, kwargs_specs))[0]:\n-        # Static args may has no `shape` attribute.\n-        if not hasattr(aval, \"shape\"):\n-          continue\n-        for d in aval.shape:\n-          if _shape_poly.is_symbolic_dim(d):\n-            if symbolic_scope is None:\n-              symbolic_scope = (d.scope, k_path)\n-              continue\n-            symbolic_scope[0]._check_same_scope(\n-                d, when=f\"when exporting {fun_name}\",\n-                self_descr=f\"current (from {_shape_poly.args_kwargs_path_to_str(symbolic_scope[1])}) \",\n-                other_descr=_shape_poly.args_kwargs_path_to_str(k_path))\n-\n-      lowered = wrapped_fun_jax.lower(\n-          *args_specs, **kwargs_specs,\n-          _experimental_lowering_parameters=mlir.LoweringParameters(\n-            platforms=actual_lowering_platforms,\n-            replace_tokens_with_dummy=replace_tokens_with_dummy,\n-          ))\n-\n-      lowering = lowered._lowering  # type: ignore\n-      _check_lowering(lowering)\n-      mlir_module = lowering.stablehlo()\n-\n-      args_avals_flat, _ = tree_util.tree_flatten(lowered.in_avals)\n-      if \"mut\" in lowering.compile_args:\n-        if lowering.compile_args[\"mut\"]: raise NotImplementedError\n-      if \"kept_var_idx\" in lowering.compile_args:\n-        module_kept_var_idx = tuple(sorted(lowering.compile_args[\"kept_var_idx\"]))\n-      else:\n-        # For pmap\n-        module_kept_var_idx = tuple(range(len(args_avals_flat)))\n-      shape_poly_state = lowering.compile_args[\"shape_poly_state\"]\n-      if (not all(core.is_constant_shape(a.shape) for a in args_avals_flat)\n-          or lowering.compile_args.get(\"ordered_effects\", [])):\n-        mlir_module = _wrap_main_func(\n-            mlir_module, args_avals_flat, args_kwargs_tree=lowered.in_tree,\n-            has_platform_index_argument=shape_poly_state.has_platform_index_argument,\n-            module_kept_var_idx=module_kept_var_idx,\n-            serialization_version=version)\n-    finally:\n-      _shape_poly.thread_local_state.enable_shape_assertions = prev_enable_shape_assertions\n+    # TODO: move to `lower`\n+    symbolic_scope: tuple[_shape_poly.SymbolicScope, tree_util.KeyPath] | None = None\n+    for k_path, aval in tree_util.tree_flatten_with_path((args_specs, kwargs_specs))[0]:\n+      # Static args may has no `shape` attribute.\n+      if not hasattr(aval, \"shape\"):\n+        continue\n+      for d in aval.shape:\n+        if _shape_poly.is_symbolic_dim(d):\n+          if symbolic_scope is None:\n+            symbolic_scope = (d.scope, k_path)\n+            continue\n+          symbolic_scope[0]._check_same_scope(\n+              d, when=f\"when exporting {fun_name}\",\n+              self_descr=f\"current (from {_shape_poly.args_kwargs_path_to_str(symbolic_scope[1])}) \",\n+              other_descr=_shape_poly.args_kwargs_path_to_str(k_path))\n+\n+    lowered = wrapped_fun_jax.lower(\n+        *args_specs, **kwargs_specs,\n+        _experimental_lowering_parameters=mlir.LoweringParameters(\n+          platforms=actual_lowering_platforms,\n+        ))\n+\n+    lowering = lowered._lowering  # type: ignore\n+    _check_lowering(lowering)\n+    mlir_module = lowering.stablehlo()\n+\n+    args_avals_flat, _ = tree_util.tree_flatten(lowered.in_avals)\n+    if \"mut\" in lowering.compile_args:\n+      if lowering.compile_args[\"mut\"]: raise NotImplementedError\n+    if \"kept_var_idx\" in lowering.compile_args:\n+      module_kept_var_idx = tuple(sorted(lowering.compile_args[\"kept_var_idx\"]))\n+    else:\n+      # For pmap\n+      module_kept_var_idx = tuple(range(len(args_avals_flat)))\n+    shape_poly_state = lowering.compile_args[\"shape_poly_state\"]\n+    if (not all(core.is_constant_shape(a.shape) for a in args_avals_flat)\n+        or lowering.compile_args.get(\"ordered_effects\", [])):\n+      mlir_module = _wrap_main_func(\n+          mlir_module, args_avals_flat, args_kwargs_tree=lowered.in_tree,\n+          has_platform_index_argument=shape_poly_state.has_platform_index_argument,\n+          module_kept_var_idx=module_kept_var_idx,\n+          serialization_version=version)\n \n     with mlir_module.context:\n       mlir_module_attrs = mlir_module.operation.attributes\n"
                },
                {
                    "old_start": 483,
                    "old_length": 13,
                    "new_start": 468,
                    "new_length": 10,
                    "hunk": "@@ -483,13 +468,10 @@ def export(fun_jax: Callable,\n         logging.info(\"Dumped the exported MLIR module to %s\", dumped_to)\n \n     _check_module(mlir_module,\n-                  allow_non_replicated_sharding=allow_non_replicated_sharding,\n                   disabled_checks=disabled_checks)\n \n     ordered_effects = tuple(lowering.compile_args[\"ordered_effects\"])\n     unordered_effects = tuple(lowering.compile_args[\"unordered_effects\"])\n-    if version < _VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      ordered_effects = unordered_effects = ()\n \n     nr_devices = len(lowering.compile_args[\"device_assignment\"])\n     def export_sharding(s: LoweringSharding,\n"
                },
                {
                    "old_start": 636,
                    "old_length": 17,
                    "new_start": 618,
                    "new_length": 11,
                    "hunk": "@@ -636,17 +618,11 @@ def _wrap_main_func(\n     assert token_result_idxs == list(range(0, nr_token_results))\n     nr_array_results = len(orig_output_types) - nr_token_results\n     assert nr_array_results >= 0\n-    if _keep_main_tokens(serialization_version):\n-      new_main_arg_indices = (tuple(range(0, nr_platform_index_args)) +\n-                              tuple(range(nr_platform_index_args + nr_dim_args,\n-                                          len(orig_input_types))))\n-      new_main_result_indices = tuple(range(0, len(orig_output_types)))\n-    else:\n-      new_main_arg_indices = (\n-        tuple(range(0, nr_platform_index_args)) +\n-        tuple(range(nr_platform_index_args + nr_dim_args + nr_token_args,\n-                    len(orig_input_types))))\n-      new_main_result_indices = tuple(range(nr_token_results, len(orig_output_types)))\n+    new_main_arg_indices = (\n+        *range(nr_platform_index_args),\n+        *range(nr_platform_index_args + nr_dim_args, len(orig_input_types)))\n+    new_main_result_indices = tuple(range(0, len(orig_output_types)))\n+\n     new_main_input_types = [orig_input_types[idx] for idx in new_main_arg_indices]\n     new_main_output_types = [orig_output_types[idx] for idx in new_main_result_indices]\n     new_main_ftype = ir.FunctionType.get(new_main_input_types, new_main_output_types)\n"
                },
                {
                    "old_start": 714,
                    "old_length": 11,
                    "new_start": 690,
                    "new_length": 8,
                    "hunk": "@@ -714,11 +690,8 @@ def _wrap_main_func(\n         else:\n           orig_main_args.append(arg)\n       # Then the token arguments\n-      if _keep_main_tokens(serialization_version):\n-        orig_main_args.extend(\n-          new_main_op.arguments[nr_platform_index_args: nr_platform_index_args + nr_token_args])\n-      else:\n-        orig_main_args.extend(list(mlir.dummy_token()) * nr_token_args)\n+      orig_main_args.extend(\n+        new_main_op.arguments[nr_platform_index_args: nr_platform_index_args + nr_token_args])\n       # Then the array arguments. We insert a ConvertOp as the only use of\n       # an input argument. This helps the downstream shape refinement because\n       # it will set the type of input arguments to static shapes, and this\n"
                },
                {
                    "old_start": 844,
                    "old_length": 7,
                    "new_start": 817,
                    "new_length": 6,
                    "hunk": "@@ -844,7 +817,6 @@ _CUSTOM_CALL_TARGETS_GUARANTEED_STABLE = {\n check_sharding_pattern = re.compile(r\"^({replicated}|{unknown shard_as.*}|\"\")$\")\n \n def _check_module(mod: ir.Module, *,\n-                  allow_non_replicated_sharding: bool,\n                   disabled_checks: Sequence[DisabledSafetyCheck]) -> None:\n   \"\"\"Run a number of checks on the module.\n \n"
                },
                {
                    "old_start": 853,
                    "old_length": 8,
                    "new_start": 825,
                    "new_length": 6,
                    "hunk": "@@ -853,8 +825,6 @@ def _check_module(mod: ir.Module, *,\n       non_replicated sharding annotations.\n     disabled_checks: the safety checks that are disabled.\n   \"\"\"\n-  sharding_attr = ir.StringAttr.get(\"Sharding\", mod.context)\n-  shape_assertion_attr = ir.StringAttr.get(\"shape_assertion\", mod.context)\n   allowed_custom_call_targets: set[str] = copy.copy(_CUSTOM_CALL_TARGETS_GUARANTEED_STABLE)\n   for dc in disabled_checks:\n     target = dc.is_custom_call()\n"
                },
                {
                    "old_start": 865,
                    "old_length": 34,
                    "new_start": 835,
                    "new_length": 13,
                    "hunk": "@@ -865,34 +835,13 @@ def _check_module(mod: ir.Module, *,\n       ir.StringAttr.get(target, mod.context)\n       for target in allowed_custom_call_targets}\n   disallowed_custom_call_ops: list[str] = []\n-  def check_sharding(op: ir.Operation, loc: ir.Location):\n-    if not allow_non_replicated_sharding:\n-      try:\n-        sharding = op.attributes[\"mhlo.sharding\"]\n-      except KeyError:\n-        pass\n-      else:\n-        if not re.match(check_sharding_pattern, ir.StringAttr(sharding).value):\n-          raise ValueError(\n-              \"Lowered function does not have a top-level pjit but it has\"\n-              f\" non-replicated sharding annotations, e.g., {op} at {loc}.\\nSee\"\n-              \" https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#support-for-partitioning\"\n-              \" for a discussion.\"\n-          )\n \n   def check_op(op: ir.Operation):\n     op_name = op.operation.name\n-    if op_name == \"func.func\":\n-      check_sharding(op.operation, op.location)\n-\n-    elif op_name == \"stablehlo.custom_call\":\n+    if op_name == \"stablehlo.custom_call\":\n       call_target_name_attr = op.operation.attributes[\"call_target_name\"]\n       if (call_target_name_attr not in allowed_custom_call_targets_attrs):\n         disallowed_custom_call_ops.append(f\"{op} at {op.location}\")\n-      if call_target_name_attr == sharding_attr:\n-        check_sharding(op, op.location)\n-      elif call_target_name_attr == shape_assertion_attr:\n-        assert (DisabledSafetyCheck.shape_assertions() not in disabled_checks)\n \n   def walk_operations(op):\n     check_op(op)\n"
                },
                {
                    "old_start": 1227,
                    "old_length": 10,
                    "new_start": 1176,
                    "new_length": 7,
                    "hunk": "@@ -1227,10 +1176,7 @@ def _call_exported_lowering(ctx: mlir.LoweringRuleContext, *args,\n   else:\n     assert len(lowering_platforms) == 1\n \n-  if _keep_main_tokens(exported.mlir_module_serialization_version):\n-    ordered_effects = exported.ordered_effects\n-  else:\n-    ordered_effects = ()\n+  ordered_effects = exported.ordered_effects\n   for eff in ordered_effects:\n     token_in = ctx.tokens_in.get(eff)[0]\n     submodule_args.append(token_in)\n"
                }
            ],
            "whole_deleted": "-_VERSION_START_SUPPORT_SHAPE_ASSERTIONS = 7\n-_VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS = 9\n-\n-    \"\"\"Allows invocations with shapes that do not meet the constraints.\n-    Has effect on serialization (to suppress the generation of the assertions)\n-    and also on deserialization (to suppress the checking of the assertions).\n-\n-def _keep_main_tokens(serialization_version: int) -> bool:\n-  return serialization_version >= _VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS\n-\n-      allow_non_replicated_sharding = False\n-      allow_non_replicated_sharding = True\n-    # Do not include shape assertions if the version is < 7.\n-    enable_shape_assertions = (\n-        DisabledSafetyCheck.shape_assertions() not in disabled_checks and\n-        version >= _VERSION_START_SUPPORT_SHAPE_ASSERTIONS)  # type: ignore\n-    try:\n-      prev_enable_shape_assertions = _shape_poly.thread_local_state.enable_shape_assertions\n-      _shape_poly.thread_local_state.enable_shape_assertions = enable_shape_assertions\n-      replace_tokens_with_dummy = not _keep_main_tokens(version)\n-\n-      symbolic_scope: tuple[_shape_poly.SymbolicScope, tree_util.KeyPath] | None = None\n-      for k_path, aval in tree_util.tree_flatten_with_path((args_specs, kwargs_specs))[0]:\n-        # Static args may has no `shape` attribute.\n-        if not hasattr(aval, \"shape\"):\n-          continue\n-        for d in aval.shape:\n-          if _shape_poly.is_symbolic_dim(d):\n-            if symbolic_scope is None:\n-              symbolic_scope = (d.scope, k_path)\n-              continue\n-            symbolic_scope[0]._check_same_scope(\n-                d, when=f\"when exporting {fun_name}\",\n-                self_descr=f\"current (from {_shape_poly.args_kwargs_path_to_str(symbolic_scope[1])}) \",\n-                other_descr=_shape_poly.args_kwargs_path_to_str(k_path))\n-\n-      lowered = wrapped_fun_jax.lower(\n-          *args_specs, **kwargs_specs,\n-          _experimental_lowering_parameters=mlir.LoweringParameters(\n-            platforms=actual_lowering_platforms,\n-            replace_tokens_with_dummy=replace_tokens_with_dummy,\n-          ))\n-\n-      lowering = lowered._lowering  # type: ignore\n-      _check_lowering(lowering)\n-      mlir_module = lowering.stablehlo()\n-\n-      args_avals_flat, _ = tree_util.tree_flatten(lowered.in_avals)\n-      if \"mut\" in lowering.compile_args:\n-        if lowering.compile_args[\"mut\"]: raise NotImplementedError\n-      if \"kept_var_idx\" in lowering.compile_args:\n-        module_kept_var_idx = tuple(sorted(lowering.compile_args[\"kept_var_idx\"]))\n-      else:\n-        # For pmap\n-        module_kept_var_idx = tuple(range(len(args_avals_flat)))\n-      shape_poly_state = lowering.compile_args[\"shape_poly_state\"]\n-      if (not all(core.is_constant_shape(a.shape) for a in args_avals_flat)\n-          or lowering.compile_args.get(\"ordered_effects\", [])):\n-        mlir_module = _wrap_main_func(\n-            mlir_module, args_avals_flat, args_kwargs_tree=lowered.in_tree,\n-            has_platform_index_argument=shape_poly_state.has_platform_index_argument,\n-            module_kept_var_idx=module_kept_var_idx,\n-            serialization_version=version)\n-    finally:\n-      _shape_poly.thread_local_state.enable_shape_assertions = prev_enable_shape_assertions\n-                  allow_non_replicated_sharding=allow_non_replicated_sharding,\n-    if version < _VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      ordered_effects = unordered_effects = ()\n-    if _keep_main_tokens(serialization_version):\n-      new_main_arg_indices = (tuple(range(0, nr_platform_index_args)) +\n-                              tuple(range(nr_platform_index_args + nr_dim_args,\n-                                          len(orig_input_types))))\n-      new_main_result_indices = tuple(range(0, len(orig_output_types)))\n-    else:\n-      new_main_arg_indices = (\n-        tuple(range(0, nr_platform_index_args)) +\n-        tuple(range(nr_platform_index_args + nr_dim_args + nr_token_args,\n-                    len(orig_input_types))))\n-      new_main_result_indices = tuple(range(nr_token_results, len(orig_output_types)))\n-      if _keep_main_tokens(serialization_version):\n-        orig_main_args.extend(\n-          new_main_op.arguments[nr_platform_index_args: nr_platform_index_args + nr_token_args])\n-      else:\n-        orig_main_args.extend(list(mlir.dummy_token()) * nr_token_args)\n-                  allow_non_replicated_sharding: bool,\n-  sharding_attr = ir.StringAttr.get(\"Sharding\", mod.context)\n-  shape_assertion_attr = ir.StringAttr.get(\"shape_assertion\", mod.context)\n-  def check_sharding(op: ir.Operation, loc: ir.Location):\n-    if not allow_non_replicated_sharding:\n-      try:\n-        sharding = op.attributes[\"mhlo.sharding\"]\n-      except KeyError:\n-        pass\n-      else:\n-        if not re.match(check_sharding_pattern, ir.StringAttr(sharding).value):\n-          raise ValueError(\n-              \"Lowered function does not have a top-level pjit but it has\"\n-              f\" non-replicated sharding annotations, e.g., {op} at {loc}.\\nSee\"\n-              \" https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#support-for-partitioning\"\n-              \" for a discussion.\"\n-          )\n-    if op_name == \"func.func\":\n-      check_sharding(op.operation, op.location)\n-\n-    elif op_name == \"stablehlo.custom_call\":\n-      if call_target_name_attr == sharding_attr:\n-        check_sharding(op, op.location)\n-      elif call_target_name_attr == shape_assertion_attr:\n-        assert (DisabledSafetyCheck.shape_assertions() not in disabled_checks)\n-  if _keep_main_tokens(exported.mlir_module_serialization_version):\n-    ordered_effects = exported.ordered_effects\n-  else:\n-    ordered_effects = ()\n",
            "whole_added": "+    \"\"\"A noop. DEPRECATED.\n+    Was used previously to allow invocations with shapes that do not meet the\n+    constraints. Has no effect anymore, shape assertions cannot be disabled.\n+    # TODO(necula): remove this after compatibility period. Was deprecated in\n+    # May 2024.\n+    warnings.warn(\n+        \"DisabledSafetyCheck.shape_assertions is deprecated, has no effect anymore\",\n+        DeprecationWarning, stacklevel=2)\n+    # TODO: move to `lower`\n+    symbolic_scope: tuple[_shape_poly.SymbolicScope, tree_util.KeyPath] | None = None\n+    for k_path, aval in tree_util.tree_flatten_with_path((args_specs, kwargs_specs))[0]:\n+      # Static args may has no `shape` attribute.\n+      if not hasattr(aval, \"shape\"):\n+        continue\n+      for d in aval.shape:\n+        if _shape_poly.is_symbolic_dim(d):\n+          if symbolic_scope is None:\n+            symbolic_scope = (d.scope, k_path)\n+            continue\n+          symbolic_scope[0]._check_same_scope(\n+              d, when=f\"when exporting {fun_name}\",\n+              self_descr=f\"current (from {_shape_poly.args_kwargs_path_to_str(symbolic_scope[1])}) \",\n+              other_descr=_shape_poly.args_kwargs_path_to_str(k_path))\n+\n+    lowered = wrapped_fun_jax.lower(\n+        *args_specs, **kwargs_specs,\n+        _experimental_lowering_parameters=mlir.LoweringParameters(\n+          platforms=actual_lowering_platforms,\n+        ))\n+\n+    lowering = lowered._lowering  # type: ignore\n+    _check_lowering(lowering)\n+    mlir_module = lowering.stablehlo()\n+\n+    args_avals_flat, _ = tree_util.tree_flatten(lowered.in_avals)\n+    if \"mut\" in lowering.compile_args:\n+      if lowering.compile_args[\"mut\"]: raise NotImplementedError\n+    if \"kept_var_idx\" in lowering.compile_args:\n+      module_kept_var_idx = tuple(sorted(lowering.compile_args[\"kept_var_idx\"]))\n+    else:\n+      # For pmap\n+      module_kept_var_idx = tuple(range(len(args_avals_flat)))\n+    shape_poly_state = lowering.compile_args[\"shape_poly_state\"]\n+    if (not all(core.is_constant_shape(a.shape) for a in args_avals_flat)\n+        or lowering.compile_args.get(\"ordered_effects\", [])):\n+      mlir_module = _wrap_main_func(\n+          mlir_module, args_avals_flat, args_kwargs_tree=lowered.in_tree,\n+          has_platform_index_argument=shape_poly_state.has_platform_index_argument,\n+          module_kept_var_idx=module_kept_var_idx,\n+          serialization_version=version)\n+    new_main_arg_indices = (\n+        *range(nr_platform_index_args),\n+        *range(nr_platform_index_args + nr_dim_args, len(orig_input_types)))\n+    new_main_result_indices = tuple(range(0, len(orig_output_types)))\n+\n+      orig_main_args.extend(\n+        new_main_op.arguments[nr_platform_index_args: nr_platform_index_args + nr_token_args])\n+    if op_name == \"stablehlo.custom_call\":\n+  ordered_effects = exported.ordered_effects\n",
            "whole_hunk": "@@ -69,9 +69,6 @@ Sharding = Union[xla_client.HloSharding, None]\n minimum_supported_serialization_version = 9\n maximum_supported_serialization_version = 9\n \n-_VERSION_START_SUPPORT_SHAPE_ASSERTIONS = 7\n-_VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS = 9\n-\n \n class DisabledSafetyCheck:\n   \"\"\"A safety check should be skipped on (de)serialization.\n@@ -106,11 +103,16 @@ class DisabledSafetyCheck:\n \n   @classmethod\n   def shape_assertions(cls) -> DisabledSafetyCheck:\n-    \"\"\"Allows invocations with shapes that do not meet the constraints.\n+    \"\"\"A noop. DEPRECATED.\n \n-    Has effect on serialization (to suppress the generation of the assertions)\n-    and also on deserialization (to suppress the checking of the assertions).\n+    Was used previously to allow invocations with shapes that do not meet the\n+    constraints. Has no effect anymore, shape assertions cannot be disabled.\n     \"\"\"\n+    # TODO(necula): remove this after compatibility period. Was deprecated in\n+    # May 2024.\n+    warnings.warn(\n+        \"DisabledSafetyCheck.shape_assertions is deprecated, has no effect anymore\",\n+        DeprecationWarning, stacklevel=2)\n     return DisabledSafetyCheck(\"shape_assertions\")\n \n   def is_custom_call(self) -> str | None:\n@@ -344,10 +346,6 @@ def args_specs(\n   return _shape_poly.symbolic_args_specs(args, polymorphic_shapes)\n \n \n-\n-def _keep_main_tokens(serialization_version: int) -> bool:\n-  return serialization_version >= _VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS\n-\n def export(fun_jax: Callable,\n            *,\n            lowering_platforms: Sequence[str] | None = None,\n@@ -391,71 +389,58 @@ def export(fun_jax: Callable,\n       # convert(f_jax), in which case a \"jit\" is implied. In that case we raise\n       # an error if the lowered function contains non-replicated sharding annotations.\n       wrapped_fun_jax = jax.jit(fun_jax)\n-      allow_non_replicated_sharding = False\n     else:\n       # If we have a pjit or pmap already we do not wrap with another, and we\n       # allow shardings.\n       wrapped_fun_jax = fun_jax  # type: ignore\n-      allow_non_replicated_sharding = True\n \n     if lowering_platforms is not None:\n       actual_lowering_platforms = tuple(lowering_platforms)\n     else:\n       actual_lowering_platforms = (default_lowering_platform(),)\n \n-    # Do not include shape assertions if the version is < 7.\n-    enable_shape_assertions = (\n-        DisabledSafetyCheck.shape_assertions() not in disabled_checks and\n-        version >= _VERSION_START_SUPPORT_SHAPE_ASSERTIONS)  # type: ignore\n-    try:\n-      prev_enable_shape_assertions = _shape_poly.thread_local_state.enable_shape_assertions\n-      _shape_poly.thread_local_state.enable_shape_assertions = enable_shape_assertions\n-      replace_tokens_with_dummy = not _keep_main_tokens(version)\n-\n-      symbolic_scope: tuple[_shape_poly.SymbolicScope, tree_util.KeyPath] | None = None\n-      for k_path, aval in tree_util.tree_flatten_with_path((args_specs, kwargs_specs))[0]:\n-        # Static args may has no `shape` attribute.\n-        if not hasattr(aval, \"shape\"):\n-          continue\n-        for d in aval.shape:\n-          if _shape_poly.is_symbolic_dim(d):\n-            if symbolic_scope is None:\n-              symbolic_scope = (d.scope, k_path)\n-              continue\n-            symbolic_scope[0]._check_same_scope(\n-                d, when=f\"when exporting {fun_name}\",\n-                self_descr=f\"current (from {_shape_poly.args_kwargs_path_to_str(symbolic_scope[1])}) \",\n-                other_descr=_shape_poly.args_kwargs_path_to_str(k_path))\n-\n-      lowered = wrapped_fun_jax.lower(\n-          *args_specs, **kwargs_specs,\n-          _experimental_lowering_parameters=mlir.LoweringParameters(\n-            platforms=actual_lowering_platforms,\n-            replace_tokens_with_dummy=replace_tokens_with_dummy,\n-          ))\n-\n-      lowering = lowered._lowering  # type: ignore\n-      _check_lowering(lowering)\n-      mlir_module = lowering.stablehlo()\n-\n-      args_avals_flat, _ = tree_util.tree_flatten(lowered.in_avals)\n-      if \"mut\" in lowering.compile_args:\n-        if lowering.compile_args[\"mut\"]: raise NotImplementedError\n-      if \"kept_var_idx\" in lowering.compile_args:\n-        module_kept_var_idx = tuple(sorted(lowering.compile_args[\"kept_var_idx\"]))\n-      else:\n-        # For pmap\n-        module_kept_var_idx = tuple(range(len(args_avals_flat)))\n-      shape_poly_state = lowering.compile_args[\"shape_poly_state\"]\n-      if (not all(core.is_constant_shape(a.shape) for a in args_avals_flat)\n-          or lowering.compile_args.get(\"ordered_effects\", [])):\n-        mlir_module = _wrap_main_func(\n-            mlir_module, args_avals_flat, args_kwargs_tree=lowered.in_tree,\n-            has_platform_index_argument=shape_poly_state.has_platform_index_argument,\n-            module_kept_var_idx=module_kept_var_idx,\n-            serialization_version=version)\n-    finally:\n-      _shape_poly.thread_local_state.enable_shape_assertions = prev_enable_shape_assertions\n+    # TODO: move to `lower`\n+    symbolic_scope: tuple[_shape_poly.SymbolicScope, tree_util.KeyPath] | None = None\n+    for k_path, aval in tree_util.tree_flatten_with_path((args_specs, kwargs_specs))[0]:\n+      # Static args may has no `shape` attribute.\n+      if not hasattr(aval, \"shape\"):\n+        continue\n+      for d in aval.shape:\n+        if _shape_poly.is_symbolic_dim(d):\n+          if symbolic_scope is None:\n+            symbolic_scope = (d.scope, k_path)\n+            continue\n+          symbolic_scope[0]._check_same_scope(\n+              d, when=f\"when exporting {fun_name}\",\n+              self_descr=f\"current (from {_shape_poly.args_kwargs_path_to_str(symbolic_scope[1])}) \",\n+              other_descr=_shape_poly.args_kwargs_path_to_str(k_path))\n+\n+    lowered = wrapped_fun_jax.lower(\n+        *args_specs, **kwargs_specs,\n+        _experimental_lowering_parameters=mlir.LoweringParameters(\n+          platforms=actual_lowering_platforms,\n+        ))\n+\n+    lowering = lowered._lowering  # type: ignore\n+    _check_lowering(lowering)\n+    mlir_module = lowering.stablehlo()\n+\n+    args_avals_flat, _ = tree_util.tree_flatten(lowered.in_avals)\n+    if \"mut\" in lowering.compile_args:\n+      if lowering.compile_args[\"mut\"]: raise NotImplementedError\n+    if \"kept_var_idx\" in lowering.compile_args:\n+      module_kept_var_idx = tuple(sorted(lowering.compile_args[\"kept_var_idx\"]))\n+    else:\n+      # For pmap\n+      module_kept_var_idx = tuple(range(len(args_avals_flat)))\n+    shape_poly_state = lowering.compile_args[\"shape_poly_state\"]\n+    if (not all(core.is_constant_shape(a.shape) for a in args_avals_flat)\n+        or lowering.compile_args.get(\"ordered_effects\", [])):\n+      mlir_module = _wrap_main_func(\n+          mlir_module, args_avals_flat, args_kwargs_tree=lowered.in_tree,\n+          has_platform_index_argument=shape_poly_state.has_platform_index_argument,\n+          module_kept_var_idx=module_kept_var_idx,\n+          serialization_version=version)\n \n     with mlir_module.context:\n       mlir_module_attrs = mlir_module.operation.attributes\n@@ -483,13 +468,10 @@ def export(fun_jax: Callable,\n         logging.info(\"Dumped the exported MLIR module to %s\", dumped_to)\n \n     _check_module(mlir_module,\n-                  allow_non_replicated_sharding=allow_non_replicated_sharding,\n                   disabled_checks=disabled_checks)\n \n     ordered_effects = tuple(lowering.compile_args[\"ordered_effects\"])\n     unordered_effects = tuple(lowering.compile_args[\"unordered_effects\"])\n-    if version < _VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      ordered_effects = unordered_effects = ()\n \n     nr_devices = len(lowering.compile_args[\"device_assignment\"])\n     def export_sharding(s: LoweringSharding,\n@@ -636,17 +618,11 @@ def _wrap_main_func(\n     assert token_result_idxs == list(range(0, nr_token_results))\n     nr_array_results = len(orig_output_types) - nr_token_results\n     assert nr_array_results >= 0\n-    if _keep_main_tokens(serialization_version):\n-      new_main_arg_indices = (tuple(range(0, nr_platform_index_args)) +\n-                              tuple(range(nr_platform_index_args + nr_dim_args,\n-                                          len(orig_input_types))))\n-      new_main_result_indices = tuple(range(0, len(orig_output_types)))\n-    else:\n-      new_main_arg_indices = (\n-        tuple(range(0, nr_platform_index_args)) +\n-        tuple(range(nr_platform_index_args + nr_dim_args + nr_token_args,\n-                    len(orig_input_types))))\n-      new_main_result_indices = tuple(range(nr_token_results, len(orig_output_types)))\n+    new_main_arg_indices = (\n+        *range(nr_platform_index_args),\n+        *range(nr_platform_index_args + nr_dim_args, len(orig_input_types)))\n+    new_main_result_indices = tuple(range(0, len(orig_output_types)))\n+\n     new_main_input_types = [orig_input_types[idx] for idx in new_main_arg_indices]\n     new_main_output_types = [orig_output_types[idx] for idx in new_main_result_indices]\n     new_main_ftype = ir.FunctionType.get(new_main_input_types, new_main_output_types)\n@@ -714,11 +690,8 @@ def _wrap_main_func(\n         else:\n           orig_main_args.append(arg)\n       # Then the token arguments\n-      if _keep_main_tokens(serialization_version):\n-        orig_main_args.extend(\n-          new_main_op.arguments[nr_platform_index_args: nr_platform_index_args + nr_token_args])\n-      else:\n-        orig_main_args.extend(list(mlir.dummy_token()) * nr_token_args)\n+      orig_main_args.extend(\n+        new_main_op.arguments[nr_platform_index_args: nr_platform_index_args + nr_token_args])\n       # Then the array arguments. We insert a ConvertOp as the only use of\n       # an input argument. This helps the downstream shape refinement because\n       # it will set the type of input arguments to static shapes, and this\n@@ -844,7 +817,6 @@ _CUSTOM_CALL_TARGETS_GUARANTEED_STABLE = {\n check_sharding_pattern = re.compile(r\"^({replicated}|{unknown shard_as.*}|\"\")$\")\n \n def _check_module(mod: ir.Module, *,\n-                  allow_non_replicated_sharding: bool,\n                   disabled_checks: Sequence[DisabledSafetyCheck]) -> None:\n   \"\"\"Run a number of checks on the module.\n \n@@ -853,8 +825,6 @@ def _check_module(mod: ir.Module, *,\n       non_replicated sharding annotations.\n     disabled_checks: the safety checks that are disabled.\n   \"\"\"\n-  sharding_attr = ir.StringAttr.get(\"Sharding\", mod.context)\n-  shape_assertion_attr = ir.StringAttr.get(\"shape_assertion\", mod.context)\n   allowed_custom_call_targets: set[str] = copy.copy(_CUSTOM_CALL_TARGETS_GUARANTEED_STABLE)\n   for dc in disabled_checks:\n     target = dc.is_custom_call()\n@@ -865,34 +835,13 @@ def _check_module(mod: ir.Module, *,\n       ir.StringAttr.get(target, mod.context)\n       for target in allowed_custom_call_targets}\n   disallowed_custom_call_ops: list[str] = []\n-  def check_sharding(op: ir.Operation, loc: ir.Location):\n-    if not allow_non_replicated_sharding:\n-      try:\n-        sharding = op.attributes[\"mhlo.sharding\"]\n-      except KeyError:\n-        pass\n-      else:\n-        if not re.match(check_sharding_pattern, ir.StringAttr(sharding).value):\n-          raise ValueError(\n-              \"Lowered function does not have a top-level pjit but it has\"\n-              f\" non-replicated sharding annotations, e.g., {op} at {loc}.\\nSee\"\n-              \" https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#support-for-partitioning\"\n-              \" for a discussion.\"\n-          )\n \n   def check_op(op: ir.Operation):\n     op_name = op.operation.name\n-    if op_name == \"func.func\":\n-      check_sharding(op.operation, op.location)\n-\n-    elif op_name == \"stablehlo.custom_call\":\n+    if op_name == \"stablehlo.custom_call\":\n       call_target_name_attr = op.operation.attributes[\"call_target_name\"]\n       if (call_target_name_attr not in allowed_custom_call_targets_attrs):\n         disallowed_custom_call_ops.append(f\"{op} at {op.location}\")\n-      if call_target_name_attr == sharding_attr:\n-        check_sharding(op, op.location)\n-      elif call_target_name_attr == shape_assertion_attr:\n-        assert (DisabledSafetyCheck.shape_assertions() not in disabled_checks)\n \n   def walk_operations(op):\n     check_op(op)\n@@ -1227,10 +1176,7 @@ def _call_exported_lowering(ctx: mlir.LoweringRuleContext, *args,\n   else:\n     assert len(lowering_platforms) == 1\n \n-  if _keep_main_tokens(exported.mlir_module_serialization_version):\n-    ordered_effects = exported.ordered_effects\n-  else:\n-    ordered_effects = ()\n+  ordered_effects = exported.ordered_effects\n   for eff in ordered_effects:\n     token_in = ctx.tokens_in.get(eff)[0]\n     submodule_args.append(token_in)\n"
        },
        {
            "name": "_shape_poly.py",
            "path": "jax/experimental/export/_shape_poly.py",
            "patches": [
                {
                    "old_start": 42,
                    "old_length": 7,
                    "new_start": 42,
                    "new_length": 6,
                    "hunk": "@@ -42,7 +42,6 @@ import itertools\n import io\n import copy\n import operator as op\n-import threading\n import tokenize\n from typing import Any, Callable, Union, overload\n import warnings\n"
                },
                {
                    "old_start": 96,
                    "old_length": 15,
                    "new_start": 95,
                    "new_length": 6,
                    "hunk": "@@ -96,15 +95,6 @@ for more details.\n     # https://github.com/python/mypy/issues/5887\n     super().__init__(error_msg)  # type: ignore\n \n-class _ShapePolyThreadLocalState(threading.local):\n-\n-  def __init__(self):\n-    # TODO(necula): this does not play well with some lowering caches, because\n-    # this state is not part of the cache key.\n-    self.enable_shape_assertions = True\n-\n-thread_local_state = _ShapePolyThreadLocalState()\n-\n \n class Comparator(Enum):\n   EQ = 1\n"
                },
                {
                    "old_start": 1311,
                    "old_length": 9,
                    "new_start": 1301,
                    "new_length": 8,
                    "hunk": "@@ -1311,9 +1301,8 @@ def shape_assertion(assert_what: jax.Array,\n       The format specifiers are sometimes processed with Python's\n       `string::format` method, and sometimes with `llvm::formatv`.\n   \"\"\"\n-  if thread_local_state.enable_shape_assertions:\n-    shape_assertion_p.bind(assert_what, *error_message_inputs,\n-                           error_message=error_message)\n+  shape_assertion_p.bind(assert_what, *error_message_inputs,\n+                         error_message=error_message)\n \n # A JAX primitive with no array arguments but with a dimension parameter\n # that is a DimExpr. The value of the primitive is the value of the dimension,\n"
                }
            ],
            "whole_deleted": "-import threading\n-class _ShapePolyThreadLocalState(threading.local):\n-\n-  def __init__(self):\n-    # TODO(necula): this does not play well with some lowering caches, because\n-    # this state is not part of the cache key.\n-    self.enable_shape_assertions = True\n-\n-thread_local_state = _ShapePolyThreadLocalState()\n-\n-  if thread_local_state.enable_shape_assertions:\n-    shape_assertion_p.bind(assert_what, *error_message_inputs,\n-                           error_message=error_message)\n",
            "whole_added": "+  shape_assertion_p.bind(assert_what, *error_message_inputs,\n+                         error_message=error_message)\n",
            "whole_hunk": "@@ -42,7 +42,6 @@ import itertools\n import io\n import copy\n import operator as op\n-import threading\n import tokenize\n from typing import Any, Callable, Union, overload\n import warnings\n@@ -96,15 +95,6 @@ for more details.\n     # https://github.com/python/mypy/issues/5887\n     super().__init__(error_msg)  # type: ignore\n \n-class _ShapePolyThreadLocalState(threading.local):\n-\n-  def __init__(self):\n-    # TODO(necula): this does not play well with some lowering caches, because\n-    # this state is not part of the cache key.\n-    self.enable_shape_assertions = True\n-\n-thread_local_state = _ShapePolyThreadLocalState()\n-\n \n class Comparator(Enum):\n   EQ = 1\n@@ -1311,9 +1301,8 @@ def shape_assertion(assert_what: jax.Array,\n       The format specifiers are sometimes processed with Python's\n       `string::format` method, and sometimes with `llvm::formatv`.\n   \"\"\"\n-  if thread_local_state.enable_shape_assertions:\n-    shape_assertion_p.bind(assert_what, *error_message_inputs,\n-                           error_message=error_message)\n+  shape_assertion_p.bind(assert_what, *error_message_inputs,\n+                         error_message=error_message)\n \n # A JAX primitive with no array arguments but with a dimension parameter\n # that is a DimExpr. The value of the primitive is the value of the dimension,\n"
        },
        {
            "name": "sharding_test.py",
            "path": "jax/experimental/jax2tf/tests/sharding_test.py",
            "patches": [
                {
                    "old_start": 454,
                    "old_length": 55,
                    "new_start": 454,
                    "new_length": 6,
                    "hunk": "@@ -454,55 +454,6 @@ class ShardingTest(tf_test_util.JaxToTfTestCase):\n             (r\"f32\\[20,10\\].*custom_call_target.*Sharding.*sharding.*devices=\\[2,1\\]\", count_out_P),\n         ])\n \n-  @jtu.parameterized_filterable(\n-    kwargs=[\n-      dict(testcase_name=f\"_kind={kind}_in_shardings={in_shardings}_out_shardings={out_shardings}\",\n-           kind=kind, in_shardings=in_shardings, out_shardings=out_shardings)\n-      for kind in (\"pjit\", \"jit\", \"sharding_constraint\")\n-      for in_shardings in (\n-          (\"none\", \"P\") if kind == \"sharding_constraint\" else\n-          (\"unspecified\",) if kind == \"jit\" else\n-          (\"unspecified\", \"none\", \"P\"))\n-      for out_shardings in (\n-          (\"unspecified\",) if kind in [\"sharding_constraint\", \"jit\"] else\n-          (\"unspecified\", \"none\", \"P\"))\n-  ])\n-  def test_pjit_error_inner_sharding(self, kind=\"pjit\", in_shardings=\"P\",\n-                                     out_shardings=\"none\"):\n-    # Check that we raise an error if there is no top-level pjit but we convert\n-    # a function with non-replicated shardings (with native lowering).\n-    shardings_map = dict(none=None, P=P(\"x\"))\n-\n-    def f_jax(x):\n-      if kind == \"pjit\":\n-        pjit_kwargs = {}\n-        if in_shardings != \"unspecified\":\n-          pjit_kwargs[\"in_shardings\"] = shardings_map[in_shardings]\n-        if out_shardings != \"unspecified\":\n-          pjit_kwargs[\"out_shardings\"] = shardings_map[out_shardings]\n-        res = pjit.pjit(lambda x: x * 2., **pjit_kwargs)(x)\n-      elif kind == \"jit\":\n-        res = jax.jit(lambda x: x * 2.)(x)\n-      elif kind == \"sharding_constraint\":\n-        res = jax.lax.with_sharding_constraint(x * 2., shardings_map[in_shardings])\n-      else:\n-        assert False\n-      return res\n-\n-    expect_error = (in_shardings == \"P\" or out_shardings == \"P\")\n-    shape = (8, 10)\n-    x = np.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n-\n-    f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True),\n-                       autograph=False, jit_compile=True)\n-    with contextlib.ExitStack() as stack:\n-      if expect_error:\n-        stack.enter_context(self.assertRaisesRegex(\n-            ValueError,\n-            \"Lowered function does not have a top-level pjit but it has non-replicated sharding annotations\"))\n-      with Mesh(self.devices, axis_names=(\"x\",)):\n-        f_tf(x)\n-\n   @jtu.parameterized_filterable(\n     kwargs=[\n       dict(testcase_name=f\"_func={func}\", func=func)\n"
                }
            ],
            "whole_deleted": "-  @jtu.parameterized_filterable(\n-    kwargs=[\n-      dict(testcase_name=f\"_kind={kind}_in_shardings={in_shardings}_out_shardings={out_shardings}\",\n-           kind=kind, in_shardings=in_shardings, out_shardings=out_shardings)\n-      for kind in (\"pjit\", \"jit\", \"sharding_constraint\")\n-      for in_shardings in (\n-          (\"none\", \"P\") if kind == \"sharding_constraint\" else\n-          (\"unspecified\",) if kind == \"jit\" else\n-          (\"unspecified\", \"none\", \"P\"))\n-      for out_shardings in (\n-          (\"unspecified\",) if kind in [\"sharding_constraint\", \"jit\"] else\n-          (\"unspecified\", \"none\", \"P\"))\n-  ])\n-  def test_pjit_error_inner_sharding(self, kind=\"pjit\", in_shardings=\"P\",\n-                                     out_shardings=\"none\"):\n-    # Check that we raise an error if there is no top-level pjit but we convert\n-    # a function with non-replicated shardings (with native lowering).\n-    shardings_map = dict(none=None, P=P(\"x\"))\n-\n-    def f_jax(x):\n-      if kind == \"pjit\":\n-        pjit_kwargs = {}\n-        if in_shardings != \"unspecified\":\n-          pjit_kwargs[\"in_shardings\"] = shardings_map[in_shardings]\n-        if out_shardings != \"unspecified\":\n-          pjit_kwargs[\"out_shardings\"] = shardings_map[out_shardings]\n-        res = pjit.pjit(lambda x: x * 2., **pjit_kwargs)(x)\n-      elif kind == \"jit\":\n-        res = jax.jit(lambda x: x * 2.)(x)\n-      elif kind == \"sharding_constraint\":\n-        res = jax.lax.with_sharding_constraint(x * 2., shardings_map[in_shardings])\n-      else:\n-        assert False\n-      return res\n-\n-    expect_error = (in_shardings == \"P\" or out_shardings == \"P\")\n-    shape = (8, 10)\n-    x = np.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n-\n-    f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True),\n-                       autograph=False, jit_compile=True)\n-    with contextlib.ExitStack() as stack:\n-      if expect_error:\n-        stack.enter_context(self.assertRaisesRegex(\n-            ValueError,\n-            \"Lowered function does not have a top-level pjit but it has non-replicated sharding annotations\"))\n-      with Mesh(self.devices, axis_names=(\"x\",)):\n-        f_tf(x)\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -454,55 +454,6 @@ class ShardingTest(tf_test_util.JaxToTfTestCase):\n             (r\"f32\\[20,10\\].*custom_call_target.*Sharding.*sharding.*devices=\\[2,1\\]\", count_out_P),\n         ])\n \n-  @jtu.parameterized_filterable(\n-    kwargs=[\n-      dict(testcase_name=f\"_kind={kind}_in_shardings={in_shardings}_out_shardings={out_shardings}\",\n-           kind=kind, in_shardings=in_shardings, out_shardings=out_shardings)\n-      for kind in (\"pjit\", \"jit\", \"sharding_constraint\")\n-      for in_shardings in (\n-          (\"none\", \"P\") if kind == \"sharding_constraint\" else\n-          (\"unspecified\",) if kind == \"jit\" else\n-          (\"unspecified\", \"none\", \"P\"))\n-      for out_shardings in (\n-          (\"unspecified\",) if kind in [\"sharding_constraint\", \"jit\"] else\n-          (\"unspecified\", \"none\", \"P\"))\n-  ])\n-  def test_pjit_error_inner_sharding(self, kind=\"pjit\", in_shardings=\"P\",\n-                                     out_shardings=\"none\"):\n-    # Check that we raise an error if there is no top-level pjit but we convert\n-    # a function with non-replicated shardings (with native lowering).\n-    shardings_map = dict(none=None, P=P(\"x\"))\n-\n-    def f_jax(x):\n-      if kind == \"pjit\":\n-        pjit_kwargs = {}\n-        if in_shardings != \"unspecified\":\n-          pjit_kwargs[\"in_shardings\"] = shardings_map[in_shardings]\n-        if out_shardings != \"unspecified\":\n-          pjit_kwargs[\"out_shardings\"] = shardings_map[out_shardings]\n-        res = pjit.pjit(lambda x: x * 2., **pjit_kwargs)(x)\n-      elif kind == \"jit\":\n-        res = jax.jit(lambda x: x * 2.)(x)\n-      elif kind == \"sharding_constraint\":\n-        res = jax.lax.with_sharding_constraint(x * 2., shardings_map[in_shardings])\n-      else:\n-        assert False\n-      return res\n-\n-    expect_error = (in_shardings == \"P\" or out_shardings == \"P\")\n-    shape = (8, 10)\n-    x = np.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n-\n-    f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True),\n-                       autograph=False, jit_compile=True)\n-    with contextlib.ExitStack() as stack:\n-      if expect_error:\n-        stack.enter_context(self.assertRaisesRegex(\n-            ValueError,\n-            \"Lowered function does not have a top-level pjit but it has non-replicated sharding annotations\"))\n-      with Mesh(self.devices, axis_names=(\"x\",)):\n-        f_tf(x)\n-\n   @jtu.parameterized_filterable(\n     kwargs=[\n       dict(testcase_name=f\"_func={func}\", func=func)\n"
        },
        {
            "name": "export_test.py",
            "path": "tests/export_test.py",
            "patches": [
                {
                    "old_start": 26,
                    "old_length": 7,
                    "new_start": 26,
                    "new_length": 6,
                    "hunk": "@@ -26,7 +26,6 @@ import jax\n from jax import lax\n from jax import numpy as jnp\n from jax.experimental import export\n-from jax.experimental.export import _export\n from jax.experimental import pjit\n from jax.experimental.shard_map import shard_map\n from jax.sharding import NamedSharding\n"
                },
                {
                    "old_start": 1197,
                    "old_length": 14,
                    "new_start": 1196,
                    "new_length": 10,
                    "hunk": "@@ -1197,14 +1196,10 @@ class JaxExportTest(jtu.JaxTestCase):\n       )\n \n     exp = get_exported(f_jax)(x)\n-    if exp.mlir_module_serialization_version >= _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      self.assertEqual([\"ForTestingOrderedEffect1()\", \"ForTestingOrderedEffect2()\"],\n-                       sorted(str(e) for e in exp.ordered_effects))\n-      self.assertEqual([\"ForTestingUnorderedEffect1()\"],\n-                       [str(e) for e in exp.unordered_effects])\n-    else:\n-      self.assertEqual([], [str(e) for e in exp.ordered_effects])\n-      self.assertEqual([], [str(e) for e in exp.unordered_effects])\n+    self.assertEqual([\"ForTestingOrderedEffect1()\", \"ForTestingOrderedEffect2()\"],\n+                     sorted(str(e) for e in exp.ordered_effects))\n+    self.assertEqual([\"ForTestingUnorderedEffect1()\"],\n+                     [str(e) for e in exp.unordered_effects])\n     mlir_module_str = str(exp.mlir_module())\n \n     # Inner functions use stablehlo.token for all versions\n"
                },
                {
                    "old_start": 1227,
                    "old_length": 17,
                    "new_start": 1222,
                    "new_length": 11,
                    "hunk": "@@ -1227,17 +1222,11 @@ class JaxExportTest(jtu.JaxTestCase):\n       # Results\n       r\"!stablehlo.token .*jax.token = true.*\"\n       r\"!stablehlo.token .*jax.token = true.*\")\n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      wrapped_main_expected_re = wrapped_main_expected_re.replace(\"!stablehlo.token\", \"tensor<0xi1>\")\n     self.assertRegex(mlir_module_str, wrapped_main_expected_re)\n \n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      # The main function does not have tokens\n-      self.assertNotRegex(mlir_module_str, r\"@main.*token\")\n-    else:\n-      # The main function takes tokens and has the same type as the wrapped main\n-      main_expected_re = wrapped_main_expected_re.replace(\"@_wrapped_jax_export_main\", \"@main\")\n-      self.assertRegex(mlir_module_str, main_expected_re)\n+    # The main function takes tokens and has the same type as the wrapped main\n+    main_expected_re = wrapped_main_expected_re.replace(\"@_wrapped_jax_export_main\", \"@main\")\n+    self.assertRegex(mlir_module_str, main_expected_re)\n \n     # Now call the exported from a function that uses its own effects\n     def f_outer(x):\n"
                },
                {
                    "old_start": 1249,
                    "old_length": 18,
                    "new_start": 1238,
                    "new_length": 13,
                    "hunk": "@@ -1249,18 +1238,13 @@ class JaxExportTest(jtu.JaxTestCase):\n         export.call_exported(exp)(x))\n \n     lowered_outer = jax.jit(f_outer).lower(x)\n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      self.assertEqual([\"ForTestingOrderedEffect2()\"],\n-                       [str(e) for e in lowered_outer._lowering.compile_args[\"ordered_effects\"]])\n-    else:\n-      self.assertEqual([\"ForTestingOrderedEffect1()\", \"ForTestingOrderedEffect2()\"],\n-                       sorted(str(e) for e in lowered_outer._lowering.compile_args[\"ordered_effects\"]))\n+    self.assertEqual([\"ForTestingOrderedEffect1()\", \"ForTestingOrderedEffect2()\"],\n+                     sorted(str(e) for e in lowered_outer._lowering.compile_args[\"ordered_effects\"]))\n     self.assertEqual([\"ForTestingUnorderedEffect1()\"],\n                      sorted([str(e) for e in lowered_outer._lowering.compile_args[\"unordered_effects\"]]))\n \n     mlir_outer_module_str = str(lowered_outer.compiler_ir())\n-    if exp.mlir_module_serialization_version >= _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      self.assertRegex(mlir_outer_module_str, main_expected_re)\n+    self.assertRegex(mlir_outer_module_str, main_expected_re)\n \n     res = jax.jit(f_outer)(x)\n     self.assertAllClose(2. * 2. * x + 10. + 4. * 2. * x, res)\n"
                },
                {
                    "old_start": 1286,
                    "old_length": 21,
                    "new_start": 1270,
                    "new_length": 15,
                    "hunk": "@@ -1286,21 +1270,15 @@ class JaxExportTest(jtu.JaxTestCase):\n       r\"%arg3: tensor<\\?x\\?xf32>.*\\) -> \\(\"\n       # Results\n       r\"!stablehlo.token {jax.token = true.*, tensor<\\?x\\?xf32>.*\\)\")\n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      wrapped_main_expected_re = wrapped_main_expected_re.replace(\"!stablehlo.token\", \"tensor<0xi1>\")\n     self.assertRegex(mlir_module_str, wrapped_main_expected_re)\n \n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      # The main function does not have tokens\n-      self.assertNotRegex(mlir_module_str, r\"@main.*token\")\n-    else:\n-      main_expected_re = (\n-        r\"@main\\(\"\n-        r\"%arg0: !stablehlo.token {jax.token = true.*, \"\n-        r\"%arg1: tensor<\\?x\\?xf32>.*\\) -> \\(\"\n-        # Results\n-        r\"!stablehlo.token {jax.token = true.*, tensor<\\?x\\?xf32>.*\\)\")\n-      self.assertRegex(mlir_module_str, main_expected_re)\n+    main_expected_re = (\n+      r\"@main\\(\"\n+      r\"%arg0: !stablehlo.token {jax.token = true.*, \"\n+      r\"%arg1: tensor<\\?x\\?xf32>.*\\) -> \\(\"\n+      # Results\n+      r\"!stablehlo.token {jax.token = true.*, tensor<\\?x\\?xf32>.*\\)\")\n+    self.assertRegex(mlir_module_str, main_expected_re)\n \n     res = export.call_exported(exp)(x)\n     self.assertAllClose(10. + 2. * x, res)\n"
                },
                {
                    "old_start": 1333,
                    "old_length": 22,
                    "new_start": 1311,
                    "new_length": 16,
                    "hunk": "@@ -1333,22 +1311,16 @@ class JaxExportTest(jtu.JaxTestCase):\n       r\"%arg4: tensor<\\?x\\?xf32>.*\\) -> \\(\"\n       # Results\n       r\"!stablehlo.token {jax.token = true.*, tensor<\\?x\\?xf32>.*\\)\")\n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      wrapped_main_expected_re = wrapped_main_expected_re.replace(\"!stablehlo.token\", \"tensor<0xi1>\")\n     self.assertRegex(mlir_module_str, wrapped_main_expected_re)\n \n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      # The main function does not have tokens\n-      self.assertNotRegex(mlir_module_str, r\"@main.*token\")\n-    else:\n-      main_expected_re = (\n-        r\"@main\\(\"\n-        r\"%arg0: tensor<i..> {jax.global_constant = \\\"_platform_index\\\".*, \"\n-        r\"%arg1: !stablehlo.token {jax.token = true.*, \"\n-        r\"%arg2: tensor<\\?x\\?xf32>.*\\) -> \\(\"\n-        # Results\n-        r\"!stablehlo.token {jax.token = true.*, tensor<\\?x\\?xf32>.*\\)\")\n-      self.assertRegex(mlir_module_str, main_expected_re)\n+    main_expected_re = (\n+      r\"@main\\(\"\n+      r\"%arg0: tensor<i..> {jax.global_constant = \\\"_platform_index\\\".*, \"\n+      r\"%arg1: !stablehlo.token {jax.token = true.*, \"\n+      r\"%arg2: tensor<\\?x\\?xf32>.*\\) -> \\(\"\n+      # Results\n+      r\"!stablehlo.token {jax.token = true.*, tensor<\\?x\\?xf32>.*\\)\")\n+    self.assertRegex(mlir_module_str, main_expected_re)\n     res = export.call_exported(exp)(x)\n     self.assertAllClose(10. + _testing_multi_platform_fun_expected(x),\n                         res)\n"
                },
                {
                    "old_start": 1370,
                    "old_length": 12,
                    "new_start": 1342,
                    "new_length": 8,
                    "hunk": "@@ -1370,12 +1342,8 @@ class JaxExportTest(jtu.JaxTestCase):\n     f_jax = jax.jit(f_jax, donate_argnums=(0,))\n     exp = export.export(f_jax)(x)\n     mlir_module_str = str(exp.mlir_module())\n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      self.assertRegex(mlir_module_str, r\"@main.*tf.aliasing_output = 0\")\n-      self.assertRegex(mlir_module_str, r\"@_wrapped_jax_export_main.*tf.aliasing_output = 1\")\n-    else:\n-      self.assertRegex(mlir_module_str, r\"@main.*tf.aliasing_output = 1\")\n-      self.assertRegex(mlir_module_str, r\"@_wrapped_jax_export_main.*tf.aliasing_output = 1\")\n+    self.assertRegex(mlir_module_str, r\"@main.*tf.aliasing_output = 1\")\n+    self.assertRegex(mlir_module_str, r\"@_wrapped_jax_export_main.*tf.aliasing_output = 1\")\n \n   @jtu.parameterized_filterable(\n     kwargs=["
                }
            ],
            "whole_deleted": "-from jax.experimental.export import _export\n-    if exp.mlir_module_serialization_version >= _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      self.assertEqual([\"ForTestingOrderedEffect1()\", \"ForTestingOrderedEffect2()\"],\n-                       sorted(str(e) for e in exp.ordered_effects))\n-      self.assertEqual([\"ForTestingUnorderedEffect1()\"],\n-                       [str(e) for e in exp.unordered_effects])\n-    else:\n-      self.assertEqual([], [str(e) for e in exp.ordered_effects])\n-      self.assertEqual([], [str(e) for e in exp.unordered_effects])\n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      wrapped_main_expected_re = wrapped_main_expected_re.replace(\"!stablehlo.token\", \"tensor<0xi1>\")\n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      # The main function does not have tokens\n-      self.assertNotRegex(mlir_module_str, r\"@main.*token\")\n-    else:\n-      # The main function takes tokens and has the same type as the wrapped main\n-      main_expected_re = wrapped_main_expected_re.replace(\"@_wrapped_jax_export_main\", \"@main\")\n-      self.assertRegex(mlir_module_str, main_expected_re)\n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      self.assertEqual([\"ForTestingOrderedEffect2()\"],\n-                       [str(e) for e in lowered_outer._lowering.compile_args[\"ordered_effects\"]])\n-    else:\n-      self.assertEqual([\"ForTestingOrderedEffect1()\", \"ForTestingOrderedEffect2()\"],\n-                       sorted(str(e) for e in lowered_outer._lowering.compile_args[\"ordered_effects\"]))\n-    if exp.mlir_module_serialization_version >= _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      self.assertRegex(mlir_outer_module_str, main_expected_re)\n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      wrapped_main_expected_re = wrapped_main_expected_re.replace(\"!stablehlo.token\", \"tensor<0xi1>\")\n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      # The main function does not have tokens\n-      self.assertNotRegex(mlir_module_str, r\"@main.*token\")\n-    else:\n-      main_expected_re = (\n-        r\"@main\\(\"\n-        r\"%arg0: !stablehlo.token {jax.token = true.*, \"\n-        r\"%arg1: tensor<\\?x\\?xf32>.*\\) -> \\(\"\n-        # Results\n-        r\"!stablehlo.token {jax.token = true.*, tensor<\\?x\\?xf32>.*\\)\")\n-      self.assertRegex(mlir_module_str, main_expected_re)\n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      wrapped_main_expected_re = wrapped_main_expected_re.replace(\"!stablehlo.token\", \"tensor<0xi1>\")\n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      # The main function does not have tokens\n-      self.assertNotRegex(mlir_module_str, r\"@main.*token\")\n-    else:\n-      main_expected_re = (\n-        r\"@main\\(\"\n-        r\"%arg0: tensor<i..> {jax.global_constant = \\\"_platform_index\\\".*, \"\n-        r\"%arg1: !stablehlo.token {jax.token = true.*, \"\n-        r\"%arg2: tensor<\\?x\\?xf32>.*\\) -> \\(\"\n-        # Results\n-        r\"!stablehlo.token {jax.token = true.*, tensor<\\?x\\?xf32>.*\\)\")\n-      self.assertRegex(mlir_module_str, main_expected_re)\n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      self.assertRegex(mlir_module_str, r\"@main.*tf.aliasing_output = 0\")\n-      self.assertRegex(mlir_module_str, r\"@_wrapped_jax_export_main.*tf.aliasing_output = 1\")\n-    else:\n-      self.assertRegex(mlir_module_str, r\"@main.*tf.aliasing_output = 1\")\n-      self.assertRegex(mlir_module_str, r\"@_wrapped_jax_export_main.*tf.aliasing_output = 1\")\n",
            "whole_added": "+    self.assertEqual([\"ForTestingOrderedEffect1()\", \"ForTestingOrderedEffect2()\"],\n+                     sorted(str(e) for e in exp.ordered_effects))\n+    self.assertEqual([\"ForTestingUnorderedEffect1()\"],\n+                     [str(e) for e in exp.unordered_effects])\n+    # The main function takes tokens and has the same type as the wrapped main\n+    main_expected_re = wrapped_main_expected_re.replace(\"@_wrapped_jax_export_main\", \"@main\")\n+    self.assertRegex(mlir_module_str, main_expected_re)\n+    self.assertEqual([\"ForTestingOrderedEffect1()\", \"ForTestingOrderedEffect2()\"],\n+                     sorted(str(e) for e in lowered_outer._lowering.compile_args[\"ordered_effects\"]))\n+    self.assertRegex(mlir_outer_module_str, main_expected_re)\n+    main_expected_re = (\n+      r\"@main\\(\"\n+      r\"%arg0: !stablehlo.token {jax.token = true.*, \"\n+      r\"%arg1: tensor<\\?x\\?xf32>.*\\) -> \\(\"\n+      # Results\n+      r\"!stablehlo.token {jax.token = true.*, tensor<\\?x\\?xf32>.*\\)\")\n+    self.assertRegex(mlir_module_str, main_expected_re)\n+    main_expected_re = (\n+      r\"@main\\(\"\n+      r\"%arg0: tensor<i..> {jax.global_constant = \\\"_platform_index\\\".*, \"\n+      r\"%arg1: !stablehlo.token {jax.token = true.*, \"\n+      r\"%arg2: tensor<\\?x\\?xf32>.*\\) -> \\(\"\n+      # Results\n+      r\"!stablehlo.token {jax.token = true.*, tensor<\\?x\\?xf32>.*\\)\")\n+    self.assertRegex(mlir_module_str, main_expected_re)\n+    self.assertRegex(mlir_module_str, r\"@main.*tf.aliasing_output = 1\")\n+    self.assertRegex(mlir_module_str, r\"@_wrapped_jax_export_main.*tf.aliasing_output = 1\")\n",
            "whole_hunk": "@@ -26,7 +26,6 @@ import jax\n from jax import lax\n from jax import numpy as jnp\n from jax.experimental import export\n-from jax.experimental.export import _export\n from jax.experimental import pjit\n from jax.experimental.shard_map import shard_map\n from jax.sharding import NamedSharding\n@@ -1197,14 +1196,10 @@ class JaxExportTest(jtu.JaxTestCase):\n       )\n \n     exp = get_exported(f_jax)(x)\n-    if exp.mlir_module_serialization_version >= _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      self.assertEqual([\"ForTestingOrderedEffect1()\", \"ForTestingOrderedEffect2()\"],\n-                       sorted(str(e) for e in exp.ordered_effects))\n-      self.assertEqual([\"ForTestingUnorderedEffect1()\"],\n-                       [str(e) for e in exp.unordered_effects])\n-    else:\n-      self.assertEqual([], [str(e) for e in exp.ordered_effects])\n-      self.assertEqual([], [str(e) for e in exp.unordered_effects])\n+    self.assertEqual([\"ForTestingOrderedEffect1()\", \"ForTestingOrderedEffect2()\"],\n+                     sorted(str(e) for e in exp.ordered_effects))\n+    self.assertEqual([\"ForTestingUnorderedEffect1()\"],\n+                     [str(e) for e in exp.unordered_effects])\n     mlir_module_str = str(exp.mlir_module())\n \n     # Inner functions use stablehlo.token for all versions\n@@ -1227,17 +1222,11 @@ class JaxExportTest(jtu.JaxTestCase):\n       # Results\n       r\"!stablehlo.token .*jax.token = true.*\"\n       r\"!stablehlo.token .*jax.token = true.*\")\n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      wrapped_main_expected_re = wrapped_main_expected_re.replace(\"!stablehlo.token\", \"tensor<0xi1>\")\n     self.assertRegex(mlir_module_str, wrapped_main_expected_re)\n \n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      # The main function does not have tokens\n-      self.assertNotRegex(mlir_module_str, r\"@main.*token\")\n-    else:\n-      # The main function takes tokens and has the same type as the wrapped main\n-      main_expected_re = wrapped_main_expected_re.replace(\"@_wrapped_jax_export_main\", \"@main\")\n-      self.assertRegex(mlir_module_str, main_expected_re)\n+    # The main function takes tokens and has the same type as the wrapped main\n+    main_expected_re = wrapped_main_expected_re.replace(\"@_wrapped_jax_export_main\", \"@main\")\n+    self.assertRegex(mlir_module_str, main_expected_re)\n \n     # Now call the exported from a function that uses its own effects\n     def f_outer(x):\n@@ -1249,18 +1238,13 @@ class JaxExportTest(jtu.JaxTestCase):\n         export.call_exported(exp)(x))\n \n     lowered_outer = jax.jit(f_outer).lower(x)\n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      self.assertEqual([\"ForTestingOrderedEffect2()\"],\n-                       [str(e) for e in lowered_outer._lowering.compile_args[\"ordered_effects\"]])\n-    else:\n-      self.assertEqual([\"ForTestingOrderedEffect1()\", \"ForTestingOrderedEffect2()\"],\n-                       sorted(str(e) for e in lowered_outer._lowering.compile_args[\"ordered_effects\"]))\n+    self.assertEqual([\"ForTestingOrderedEffect1()\", \"ForTestingOrderedEffect2()\"],\n+                     sorted(str(e) for e in lowered_outer._lowering.compile_args[\"ordered_effects\"]))\n     self.assertEqual([\"ForTestingUnorderedEffect1()\"],\n                      sorted([str(e) for e in lowered_outer._lowering.compile_args[\"unordered_effects\"]]))\n \n     mlir_outer_module_str = str(lowered_outer.compiler_ir())\n-    if exp.mlir_module_serialization_version >= _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      self.assertRegex(mlir_outer_module_str, main_expected_re)\n+    self.assertRegex(mlir_outer_module_str, main_expected_re)\n \n     res = jax.jit(f_outer)(x)\n     self.assertAllClose(2. * 2. * x + 10. + 4. * 2. * x, res)\n@@ -1286,21 +1270,15 @@ class JaxExportTest(jtu.JaxTestCase):\n       r\"%arg3: tensor<\\?x\\?xf32>.*\\) -> \\(\"\n       # Results\n       r\"!stablehlo.token {jax.token = true.*, tensor<\\?x\\?xf32>.*\\)\")\n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      wrapped_main_expected_re = wrapped_main_expected_re.replace(\"!stablehlo.token\", \"tensor<0xi1>\")\n     self.assertRegex(mlir_module_str, wrapped_main_expected_re)\n \n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      # The main function does not have tokens\n-      self.assertNotRegex(mlir_module_str, r\"@main.*token\")\n-    else:\n-      main_expected_re = (\n-        r\"@main\\(\"\n-        r\"%arg0: !stablehlo.token {jax.token = true.*, \"\n-        r\"%arg1: tensor<\\?x\\?xf32>.*\\) -> \\(\"\n-        # Results\n-        r\"!stablehlo.token {jax.token = true.*, tensor<\\?x\\?xf32>.*\\)\")\n-      self.assertRegex(mlir_module_str, main_expected_re)\n+    main_expected_re = (\n+      r\"@main\\(\"\n+      r\"%arg0: !stablehlo.token {jax.token = true.*, \"\n+      r\"%arg1: tensor<\\?x\\?xf32>.*\\) -> \\(\"\n+      # Results\n+      r\"!stablehlo.token {jax.token = true.*, tensor<\\?x\\?xf32>.*\\)\")\n+    self.assertRegex(mlir_module_str, main_expected_re)\n \n     res = export.call_exported(exp)(x)\n     self.assertAllClose(10. + 2. * x, res)\n@@ -1333,22 +1311,16 @@ class JaxExportTest(jtu.JaxTestCase):\n       r\"%arg4: tensor<\\?x\\?xf32>.*\\) -> \\(\"\n       # Results\n       r\"!stablehlo.token {jax.token = true.*, tensor<\\?x\\?xf32>.*\\)\")\n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      wrapped_main_expected_re = wrapped_main_expected_re.replace(\"!stablehlo.token\", \"tensor<0xi1>\")\n     self.assertRegex(mlir_module_str, wrapped_main_expected_re)\n \n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      # The main function does not have tokens\n-      self.assertNotRegex(mlir_module_str, r\"@main.*token\")\n-    else:\n-      main_expected_re = (\n-        r\"@main\\(\"\n-        r\"%arg0: tensor<i..> {jax.global_constant = \\\"_platform_index\\\".*, \"\n-        r\"%arg1: !stablehlo.token {jax.token = true.*, \"\n-        r\"%arg2: tensor<\\?x\\?xf32>.*\\) -> \\(\"\n-        # Results\n-        r\"!stablehlo.token {jax.token = true.*, tensor<\\?x\\?xf32>.*\\)\")\n-      self.assertRegex(mlir_module_str, main_expected_re)\n+    main_expected_re = (\n+      r\"@main\\(\"\n+      r\"%arg0: tensor<i..> {jax.global_constant = \\\"_platform_index\\\".*, \"\n+      r\"%arg1: !stablehlo.token {jax.token = true.*, \"\n+      r\"%arg2: tensor<\\?x\\?xf32>.*\\) -> \\(\"\n+      # Results\n+      r\"!stablehlo.token {jax.token = true.*, tensor<\\?x\\?xf32>.*\\)\")\n+    self.assertRegex(mlir_module_str, main_expected_re)\n     res = export.call_exported(exp)(x)\n     self.assertAllClose(10. + _testing_multi_platform_fun_expected(x),\n                         res)\n@@ -1370,12 +1342,8 @@ class JaxExportTest(jtu.JaxTestCase):\n     f_jax = jax.jit(f_jax, donate_argnums=(0,))\n     exp = export.export(f_jax)(x)\n     mlir_module_str = str(exp.mlir_module())\n-    if exp.mlir_module_serialization_version < _export._VERSION_START_SUPPORT_EFFECTS_WITH_REAL_TOKENS:\n-      self.assertRegex(mlir_module_str, r\"@main.*tf.aliasing_output = 0\")\n-      self.assertRegex(mlir_module_str, r\"@_wrapped_jax_export_main.*tf.aliasing_output = 1\")\n-    else:\n-      self.assertRegex(mlir_module_str, r\"@main.*tf.aliasing_output = 1\")\n-      self.assertRegex(mlir_module_str, r\"@_wrapped_jax_export_main.*tf.aliasing_output = 1\")\n+    self.assertRegex(mlir_module_str, r\"@main.*tf.aliasing_output = 1\")\n+    self.assertRegex(mlir_module_str, r\"@_wrapped_jax_export_main.*tf.aliasing_output = 1\")\n \n   @jtu.parameterized_filterable(\n     kwargs=["
        }
    ]
},
{
    "Id": 45,
    "commit_link": "https://github.com/google/jax/commit/ba8480a2129916d7a54cc2631f444b3cbf020994",
    "date": "2024-05-12T20:50:41-07:00",
    "message": "Register TPU profiler plugin when get_topology_desc is called with tpu platform.\n\nThis allows the TPU profiler to work with other plugin backends.\n\nTested on a GPU VM:\n$ pip install -U \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n$ pip install -e .\n$ TPU_SKIP_MDS_QUERY=1 python tests/cross_aot_test.py\nRunning tests under Python 3.10.12: /usr/bin/python\n[ RUN      ] JaxAotTest.test_tpu_profiler_registered_get_topology_from_devices\nNOT_FOUND: WARNING: could not determine TPU accelerator type. Set env var `TPU_ACCELERATOR_TYPE` to set manually. TPU runtime may not be properly initialized.\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:285\n\nNOT_FOUND: WARNING: could not determine TPU worker number. Set env var `TPU_WORKER_ID` to set manually. TPU runtime may not be properly initialized.\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:285\n\nNOT_FOUND: WARNING: could not determine TPU worker hostnames or internal IP addresses. Set env var `TPU_WORKER_HOSTNAMES` to set manually. TPU runtime may not be properly initialized.\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:285\nlearning/45eac/tfrc/runtime/common_lib.cc:341\n\nI0510 00:32:03.063246 130900437979136 cross_aot_test.py:58] Expected to fail to get topology\nI0510 00:32:03.079923 130900437979136 xla_bridge.py:884] Unable to initialize backend 'cuda':\nI0510 00:32:03.080080 130900437979136 xla_bridge.py:884] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\nI0510 00:32:03.089399 130900437979136 xla_bridge.py:884] Unable to initialize backend 'tpu': UNKNOWN: TPU initialization failed: No ba16c7433 device found.\nW0510 00:32:03.089633 130900437979136 xla_bridge.py:931] An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n/home/jieying/.local/lib/python3.10/site-packages/tensorflow/__init__.py:30: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n  import distutils as _distutils\n2024-05-10 00:32:03.359597: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-10 00:32:03.359652: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-10 00:32:03.361368: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-05-10 00:32:04.562557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n[       OK ] JaxAotTest.test_tpu_profiler_registered_get_topology_from_devices\n----------------------------------------------------------------------\nRan 1 test in 2.549s\n\nOK\n\nIn tests/cross_aot_test.py\nclass JaxAotTest(jtu.JaxTestCase):\n  def test_tpu_profiler_registered_get_topology_from_devices(self):\n    try:\n      _ = topologies.get_topology_desc(\n          topology_name='fake_topology',\n          platform='tpu',\n      )\n    except xla_extension.XlaRuntimeError:\n      logging.info('Expected to fail to get topology')\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n      try:\n        jax.profiler.start_trace(tmpdir)\n        jax.pmap(lambda x: jax.lax.psum(x + 1, 'i'), axis_name='i')(\n            jnp.ones(jax.local_device_count())\n        )\n      finally:\n        jax.profiler.stop_trace()\n\n      proto_path = glob.glob(\n          os.path.join(tmpdir, '**/*.xplane.pb'), recursive=True\n      )\n      self.assertLen(proto_path, 1)\n      with open(proto_path[0], 'rb') as f:\n        proto = f.read()\n      # Sanity check that serialized proto contains host, and Python traces\n      # without deserializing.\n      self.assertIn(b'/host:metadata', proto)\n      if jtu.test_device_matches(['tpu']):\n        self.assertNotIn(b'/device:TPU', proto)\n      self.assertIn(b'pxla.py', proto)\n\nPiperOrigin-RevId: 633076007",
    "changes": [
        {
            "name": "xla_bridge.py",
            "path": "jax/_src/xla_bridge.py",
            "patches": [
                {
                    "old_start": 1212,
                    "old_length": 7,
                    "new_start": 1212,
                    "new_length": 8,
                    "hunk": "@@ -1212,7 +1212,8 @@ def make_pjrt_tpu_topology(topology_name='', **kwargs):\n       raise RuntimeError(\n           \"JAX TPU support not installed; cannot generate TPU topology. See\"\n           \" https://github.com/google/jax#installation\")\n-    xla_client.load_pjrt_plugin_dynamically(\"tpu\", library_path)\n+    c_api = xla_client.load_pjrt_plugin_dynamically(\"tpu\", library_path)\n+    xla_client.profiler.register_plugin_profiler(c_api)\n   assert xla_client.pjrt_plugin_loaded(\"tpu\")\n   if not xla_client.pjrt_plugin_initialized(\"tpu\"):\n     xla_client.initialize_pjrt_plugin(\"tpu\")"
                }
            ],
            "whole_deleted": "-    xla_client.load_pjrt_plugin_dynamically(\"tpu\", library_path)\n",
            "whole_added": "+    c_api = xla_client.load_pjrt_plugin_dynamically(\"tpu\", library_path)\n+    xla_client.profiler.register_plugin_profiler(c_api)\n",
            "whole_hunk": "@@ -1212,7 +1212,8 @@ def make_pjrt_tpu_topology(topology_name='', **kwargs):\n       raise RuntimeError(\n           \"JAX TPU support not installed; cannot generate TPU topology. See\"\n           \" https://github.com/google/jax#installation\")\n-    xla_client.load_pjrt_plugin_dynamically(\"tpu\", library_path)\n+    c_api = xla_client.load_pjrt_plugin_dynamically(\"tpu\", library_path)\n+    xla_client.profiler.register_plugin_profiler(c_api)\n   assert xla_client.pjrt_plugin_loaded(\"tpu\")\n   if not xla_client.pjrt_plugin_initialized(\"tpu\"):\n     xla_client.initialize_pjrt_plugin(\"tpu\")"
        }
    ]
},
{
    "Id": 46,
    "commit_link": "https://github.com/google/jax/commit/6f79093cffdc5a14ab1cedaeef269482908b1897",
    "date": "2024-05-09T17:34:21-07:00",
    "message": "[XLA:TPU] Support output streaming and refactor TryOutputStreaming into a bottoms-up approach.\n\nPreviously, output streaming took a top-down approach which indiscriminately checks if a MoveToHost custom call would trace down to an output marked with host memory space. This did not work when a dynamic-update-slice existed between the MTH call and the output. This CL fixes this problem by handling output streaming before other MTH calls, while also improving efficiency with the bottoms-up approach so we only trace a single path in the graph.\n\nPiperOrigin-RevId: 632318740",
    "changes": [
        {
            "name": "memories_test.py",
            "path": "tests/memories_test.py",
            "patches": [
                {
                    "old_start": 1195,
                    "old_length": 6,
                    "new_start": 1195,
                    "new_length": 22,
                    "hunk": "@@ -1195,6 +1195,22 @@ class DevicePutTest(jtu.JaxTestCase):\n     out_s = NamedSharding(mesh, P(None, None, \"z\"), memory_kind=\"device\")\n     self.assertEqual(out_hbm.sharding, out_s)\n \n+  def test_output_streaming(self):\n+    mesh = jtu.create_global_mesh((1, 1), (\"x\", \"y\"))\n+    np_inp = np.arange(16.0).reshape(8, 2)\n+    s_hbm = NamedSharding(mesh, P(\"x\", \"y\"), memory_kind=\"device\")\n+    s_host = NamedSharding(mesh, P(\"x\", \"y\"), memory_kind=\"pinned_host\")\n+    arr_hbm = jax.device_put(np_inp, s_hbm)\n+\n+    @functools.partial(jax.jit, out_shardings=s_host)\n+    def f(xs):\n+      out_tpu = xs + 1.0\n+      return out_tpu\n+\n+    out_host = f(arr_hbm)\n+    self.assertArraysEqual(out_host, np_inp + 1.0)\n+    self.assertEqual(out_host.sharding, s_host)\n+\n \n class ActivationOffloadingTest(jtu.JaxTestCase):\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  def test_output_streaming(self):\n+    mesh = jtu.create_global_mesh((1, 1), (\"x\", \"y\"))\n+    np_inp = np.arange(16.0).reshape(8, 2)\n+    s_hbm = NamedSharding(mesh, P(\"x\", \"y\"), memory_kind=\"device\")\n+    s_host = NamedSharding(mesh, P(\"x\", \"y\"), memory_kind=\"pinned_host\")\n+    arr_hbm = jax.device_put(np_inp, s_hbm)\n+\n+    @functools.partial(jax.jit, out_shardings=s_host)\n+    def f(xs):\n+      out_tpu = xs + 1.0\n+      return out_tpu\n+\n+    out_host = f(arr_hbm)\n+    self.assertArraysEqual(out_host, np_inp + 1.0)\n+    self.assertEqual(out_host.sharding, s_host)\n+\n",
            "whole_hunk": "@@ -1195,6 +1195,22 @@ class DevicePutTest(jtu.JaxTestCase):\n     out_s = NamedSharding(mesh, P(None, None, \"z\"), memory_kind=\"device\")\n     self.assertEqual(out_hbm.sharding, out_s)\n \n+  def test_output_streaming(self):\n+    mesh = jtu.create_global_mesh((1, 1), (\"x\", \"y\"))\n+    np_inp = np.arange(16.0).reshape(8, 2)\n+    s_hbm = NamedSharding(mesh, P(\"x\", \"y\"), memory_kind=\"device\")\n+    s_host = NamedSharding(mesh, P(\"x\", \"y\"), memory_kind=\"pinned_host\")\n+    arr_hbm = jax.device_put(np_inp, s_hbm)\n+\n+    @functools.partial(jax.jit, out_shardings=s_host)\n+    def f(xs):\n+      out_tpu = xs + 1.0\n+      return out_tpu\n+\n+    out_host = f(arr_hbm)\n+    self.assertArraysEqual(out_host, np_inp + 1.0)\n+    self.assertEqual(out_host.sharding, s_host)\n+\n \n class ActivationOffloadingTest(jtu.JaxTestCase):\n "
        }
    ]
},
{
    "Id": 47,
    "commit_link": "https://github.com/google/jax/commit/65d4c688e06bd4b22d7e1c2235b8a1aaa7af3195",
    "date": "2024-05-08T14:00:39-07:00",
    "message": "Generic reduce window jvp\n\nThe problem is that we want to generically jvp and tranpose over any reduction_fn. Jax already handles some of the hard parts for us, namely, ensuring that the user provided fn is jax capturable. All that is left then, is to write a jvp and tranpose fn that utilize the jax utils correctly.\n\nHowever, this is not so straightforward because in order to get the transpose of a reduction window, we need to be able to use both the tangents and primals. The current reduce_fn operates on (x, y) - but we actually need is, under jvp, to operate on `(x_primal, y_primal, x_tangent, y_tangent)`. In turn, this means we need to push down notions of a jvp-specific reduction_fn (captured via the usual machinery of as_fun `as_fun(jvp_fn(closed(user_reduction_jaxp)))`).\n\nFor the jvp fn, we stack the primal operand and the tangent operand together, and we stack their respective initial values together - this means a good deal of changes to safety checks and assurances downstream (as well as unpacking) as the shape of the operand has changed from [K,...Kt] to [K, ...Kt, 2] where the last dim is the stacked primal and tangent values.\n\nIn following CLs, we will add (1) re-entrant/recursive is_jvp and (2) transposition\n\nPiperOrigin-RevId: 631916764",
    "changes": [
        {
            "name": "windowed_reductions.py",
            "path": "jax/_src/lax/windowed_reductions.py",
            "patches": [
                {
                    "old_start": 19,
                    "old_length": 36,
                    "new_start": 19,
                    "new_length": 42,
                    "hunk": "@@ -19,36 +19,42 @@ from functools import partial\n from typing import Callable\n import warnings\n \n-import numpy as np\n-\n from jax import tree_util\n-\n-from jax._src import ad_util\n from jax._src import core\n from jax._src import dispatch\n from jax._src import dtypes\n from jax._src import util\n-from jax._src.core import ShapedArray, ConcreteArray\n+from jax._src.core import ConcreteArray, ShapedArray\n from jax._src.interpreters import ad\n from jax._src.interpreters import batching\n from jax._src.interpreters import mlir\n-from jax._src.lax import lax\n from jax._src.lax import convolution\n+from jax._src.lax import lax\n from jax._src.lax import slicing\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import hlo\n from jax._src.numpy.ufuncs import logaddexp\n from jax._src.typing import Array\n+import numpy as np\n+from jax._src.core import ClosedJaxpr\n+from jax._src.core import jaxpr_as_fun\n+from jax._src.interpreters.ad import jvp_jaxpr\n+from jax._src import ad_util\n \n map = util.safe_map\n zip = util.safe_zip\n \n \n-def reduce_window(operand, init_value, computation: Callable,\n-                  window_dimensions: core.Shape, window_strides: Sequence[int],\n-                  padding: str | Sequence[tuple[int, int]],\n-                  base_dilation: Sequence[int] | None = None,\n-                  window_dilation: Sequence[int] | None = None) -> Array:\n+def _reduce_window(\n+    operand,\n+    init_value,\n+    computation,\n+    window_dimensions: core.Shape,\n+    window_strides: Sequence[int],\n+    padding: str | Sequence[tuple[int, int]],\n+    base_dilation: Sequence[int] | None = None,\n+    window_dilation: Sequence[int] | None = None,\n+):\n   \"\"\"Wraps XLA's `ReduceWindowWithGeneralPadding\n   <https://www.tensorflow.org/xla/operation_semantics#reducewindow>`_\n   operator.\n"
                },
                {
                    "old_start": 56,
                    "old_length": 13,
                    "new_start": 62,
                    "new_length": 18,
                    "hunk": "@@ -56,13 +62,18 @@ def reduce_window(operand, init_value, computation: Callable,\n   flat_operands, operand_tree = tree_util.tree_flatten(operand)\n   flat_init_values, init_value_tree = tree_util.tree_flatten(init_value)\n   if operand_tree != init_value_tree:\n-    raise ValueError('Operands must have the same tree structure as '\n-                     f'init_values: {operand_tree} vs. {init_value_tree}')\n-  if len(flat_operands) == 0:\n-    raise ValueError('reduce_window must have at least one operand.')\n+    raise ValueError(\n+        \"Operands must have the same tree structure as \"\n+        f\"init_values: {operand_tree} vs. {init_value_tree}\"\n+    )\n   if len(flat_operands) != len(flat_init_values):\n-    raise ValueError('Must have same total number of operands as init_values: '\n-                     f' {len(flat_operands)} vs. {len(flat_init_values)}')\n+    raise ValueError(\n+        \"Must have same total number of operands as init_values: \"\n+        f\" {len(flat_operands)} vs. {len(flat_init_values)}\"\n+    )\n+\n+  if len(flat_operands) == 0:\n+    raise ValueError(\"reduce_window must have at least one operand.\")\n   if isinstance(padding, str):\n     dilated_window_dims = (\n         window_dimensions if window_dilation is None else\n"
                },
                {
                    "old_start": 82,
                    "old_length": 21,
                    "new_start": 93,
                    "new_length": 52,
                    "hunk": "@@ -82,21 +93,52 @@ def reduce_window(operand, init_value, computation: Callable,\n   else:\n     flat_init_avals = map(lax._abstractify, flat_init_values)\n     jaxpr, out_tree = lax._variadic_reduction_jaxpr(\n-        computation, tuple(flat_init_avals), init_value_tree)\n+        computation, tuple(flat_init_avals), init_value_tree\n+    )\n     if operand_tree != out_tree:\n       raise ValueError(\n         'reduce_window output must have the same tree structure as the operands'\n         f' {operand_tree} vs. {out_tree}')\n     out_flat = reduce_window_p.bind(\n-        *flat_operands, *flat_init_values, jaxpr=jaxpr.jaxpr,\n-        consts=tuple(jaxpr.consts), window_dimensions=tuple(window_dimensions),\n-        window_strides=tuple(window_strides), padding=padding,\n+        *flat_operands,\n+        *flat_init_values,\n+        jaxpr=jaxpr.jaxpr,\n+        consts=tuple(jaxpr.consts),\n+        window_dimensions=tuple(window_dimensions),\n+        window_strides=tuple(window_strides),\n+        padding=padding,\n         base_dilation=tuple(base_dilation),\n-        window_dilation=tuple(window_dilation))\n+        window_dilation=tuple(window_dilation),\n+    )\n     return tree_util.tree_unflatten(out_tree, out_flat)\n \n-def _get_monoid_window_reducer(monoid_op: Callable,\n-                               xs: Sequence[Array]) -> Callable | None:\n+\n+\n+def reduce_window(\n+    operand,\n+    init_value,\n+    computation: Callable,\n+    window_dimensions: core.Shape,\n+    window_strides: Sequence[int],\n+    padding: str | Sequence[tuple[int, int]],\n+    base_dilation: Sequence[int] | None = None,\n+    window_dilation: Sequence[int] | None = None,\n+) -> Array:\n+  return _reduce_window(\n+      operand,\n+      init_value,\n+      computation,\n+      window_dimensions,\n+      window_strides,\n+      padding,\n+      base_dilation,\n+      window_dilation,\n+  )\n+\n+\n+def _get_monoid_window_reducer(\n+    monoid_op, xs: Sequence[Array]\n+) -> Callable | None:\n   if len(xs) != 1:\n     return None\n   x, = xs\n"
                },
                {
                    "old_start": 260,
                    "old_length": 10,
                    "new_start": 303,
                    "new_length": 19,
                    "hunk": "@@ -260,10 +303,19 @@ def _select_and_gather_add(tangents: Array, operand: Array,\n \n \n def _reduce_window_abstract_eval_rule(\n-    *avals, jaxpr, consts, window_dimensions, window_strides, padding,\n-    base_dilation, window_dilation):\n+    *avals,\n+    jaxpr,\n+    consts,\n+    window_dimensions,\n+    window_strides,\n+    padding,\n+    base_dilation,\n+    window_dilation,\n+):\n   operand_avals, init_val_avals = util.split_list(avals, [len(avals) // 2])\n-  if any(o.dtype != iv.dtype for o, iv in zip(operand_avals, init_val_avals)):\n+  if any(\n+      o.dtype != iv.dtype for o, iv in zip(operand_avals, init_val_avals)\n+  ):\n     msg = (\"reduce_window got inconsistent dtypes for operands and init_values:\"\n            \" got operand dtypes {} and init_value dtypes {}.\")\n     raise TypeError(msg.format([o.dtype for o in operand_avals],\n"
                },
                {
                    "old_start": 273,
                    "old_length": 13,
                    "new_start": 325,
                    "new_length": 28,
                    "hunk": "@@ -273,13 +325,28 @@ def _reduce_window_abstract_eval_rule(\n            \"have shapes {}.\")\n     raise TypeError(msg.format([v.shape for v in init_val_avals]))\n   out_shape = _common_reduce_window_shape_rule(\n-    operand_avals[0], window_dimensions, window_strides, padding,\n-    base_dilation, window_dilation)\n+      operand_avals[0],\n+      window_dimensions,\n+      window_strides,\n+      padding,\n+      base_dilation,\n+      window_dilation,\n+  )\n   return tuple(ShapedArray(out_shape, op.dtype) for op in operand_avals)\n \n+\n def _generic_reduce_window_batch_rule(\n-    batched_args, batch_dims, *, jaxpr, consts, window_dimensions,\n-    window_strides, padding, base_dilation, window_dilation):\n+    batched_args,\n+    batch_dims,\n+    *,\n+    jaxpr,\n+    consts,\n+    window_dimensions,\n+    window_strides,\n+    padding,\n+    base_dilation,\n+    window_dilation,\n+):\n   num_operands = len(batched_args) // 2\n   operands, init_values = util.split_list(batched_args, [num_operands])\n   operand_bdims, init_value_bdims = util.split_list(batch_dims, [num_operands])\n"
                },
                {
                    "old_start": 306,
                    "old_length": 14,
                    "new_start": 373,
                    "new_length": 68,
                    "hunk": "@@ -306,14 +373,68 @@ def _generic_reduce_window_batch_rule(\n \n \n reduce_window_p = core.Primitive('reduce_window')\n+\n+\n+def reduce_window_jvp(\n+    primals,\n+    tangents,\n+    window_dimensions,\n+    window_strides,\n+    padding,\n+    base_dilation,\n+    window_dilation,\n+    jaxpr,\n+    consts,\n+):\n+\n+  reduction_jaxpr = jaxpr\n+\n+  n = len(primals) // 2  # number of primal operands\n+  operand, init_value = util.split_list(primals, [n])\n+  operand_tangent, init_value_tangent = util.split_list(tangents, [n])\n+  if not all(isinstance(t, ad.Zero) for t in init_value_tangent):\n+    raise TypeError(\"reduce_window jvp does not support non-zero init_value_tangent.\")\n+\n+  init_value_tangent = map(ad_util.instantiate, init_value_tangent)\n+  c_reduction_jaxpr = ClosedJaxpr(reduction_jaxpr, consts)\n+  jvp_reduction = jvp_jaxpr(c_reduction_jaxpr, (True,) * len(tangents), [False] * len(init_value_tangent))[0]\n+\n+  def wrapper(left, right):\n+    pl, tl = util.split_list(left, [n])\n+    pr, tr = util.split_list(right, [n])\n+    return jaxpr_as_fun(jvp_reduction)(*pl, *pr, *tl, *tr)\n+\n+  jvp_primals_tangents = _reduce_window(\n+      operand=[*operand, *operand_tangent],\n+      init_value=[*init_value, *init_value_tangent],\n+      computation=wrapper,\n+      window_dimensions=window_dimensions,\n+      window_strides=window_strides,\n+      padding=padding,\n+      base_dilation=base_dilation,\n+      window_dilation=window_dilation,\n+  )\n+  primals, tangents = util.split_list(jvp_primals_tangents, [len(jvp_primals_tangents) // 2])\n+  return [*primals], [*tangents]\n+\n+ad.primitive_jvps[reduce_window_p] = reduce_window_jvp\n reduce_window_p.multiple_results = True\n reduce_window_p.def_impl(partial(dispatch.apply_primitive, reduce_window_p))\n reduce_window_p.def_abstract_eval(_reduce_window_abstract_eval_rule)\n batching.primitive_batchers[reduce_window_p] = _generic_reduce_window_batch_rule\n \n-def _generic_reduce_window_lower(ctx, *args, jaxpr, consts,\n-                                 window_dimensions, window_strides, padding,\n-                                 base_dilation, window_dilation):\n+\n+def _generic_reduce_window_lower(\n+    ctx,\n+    *args,\n+    jaxpr,\n+    consts,\n+    window_dimensions,\n+    window_strides,\n+    padding,\n+    base_dilation,\n+    window_dilation,\n+):\n   operands, init_values = util.split_list(args, [len(args) // 2])\n   _, init_value_avals = util.split_list(ctx.avals_in, [len(operands)])\n \n"
                },
                {
                    "old_start": 330,
                    "old_length": 11,
                    "new_start": 451,
                    "new_length": 15,
                    "hunk": "@@ -330,11 +451,15 @@ def _generic_reduce_window_lower(ctx, *args, jaxpr, consts,\n       reducer_name=\"generic_reduce_window_reducer\",\n       reducer_body=reducer_body,\n       operands=operands,\n-      init_values=init_values, init_values_avals=init_value_avals,\n+      init_values=init_values,\n+      init_values_avals=init_value_avals,\n       out_avals=ctx.avals_out,\n-      window_dimensions=window_dimensions, window_strides=window_strides,\n-      base_dilation=base_dilation, window_dilation=window_dilation,\n-      padding=padding)\n+      window_dimensions=window_dimensions,\n+      window_strides=window_strides,\n+      base_dilation=base_dilation,\n+      window_dilation=window_dilation,\n+      padding=padding,\n+  )\n \n \n mlir.register_lowering(reduce_window_p, _generic_reduce_window_lower)\n"
                },
                {
                    "old_start": 402,
                    "old_length": 9,
                    "new_start": 527,
                    "new_length": 14,
                    "hunk": "@@ -402,9 +527,14 @@ def _reduce_window_chooser_jvp_rule(prim, g, operand, *, window_dimensions,\n                                 window_dilation)\n \n \n-def _common_reduce_window_shape_rule(operand, window_dimensions,\n-                                     window_strides, padding, base_dilation,\n-                                     window_dilation):\n+def _common_reduce_window_shape_rule(\n+    operand,\n+    window_dimensions,\n+    window_strides,\n+    padding,\n+    base_dilation,\n+    window_dilation,\n+):\n   lax._check_shapelike(\"reduce_window\", \"window_dimensions\", window_dimensions,\n                        non_zero_shape=True)\n   lax._check_shapelike(\"reduce_window\", \"window_strides\", window_strides,\n"
                },
                {
                    "old_start": 412,
                    "old_length": 8,
                    "new_start": 542,
                    "new_length": 10,
                    "hunk": "@@ -412,8 +542,10 @@ def _common_reduce_window_shape_rule(operand, window_dimensions,\n   lax._check_shapelike(\"reduce_window\", \"base_dilation\", base_dilation)\n   lax._check_shapelike(\"reduce_window\", \"window_dilation\", window_dilation)\n   if operand.ndim != len(window_dimensions):\n-    msg = (\"reduce_window got the wrong number of window_dimensions for \"\n-           \"operand: got operand shape {} with window_dimensions {}.\")\n+    msg = (\n+        \"reduce_window got the wrong number of window_dimensions for \"\n+        \"operand: got operand shape {} with window_dimensions {}.\"\n+    )\n     raise TypeError(msg.format(operand.shape, window_dimensions))\n   if len(window_strides) != len(window_dimensions):\n     msg = (\"reduce_window got inconsistent window_strides and \"\n"
                },
                {
                    "old_start": 463,
                    "old_length": 24,
                    "new_start": 596,
                    "new_length": 36,
                    "hunk": "@@ -463,24 +596,36 @@ batching.primitive_batchers[reduce_window_min_p] = partial(\n \n def _reduce_window_lower(\n     reduce_op,\n-    init_value, ctx, operand, *,\n-    window_dimensions, window_strides, padding, base_dilation,\n-    window_dilation):\n+    init_value,\n+    ctx,\n+    operand,\n+    *,\n+    window_dimensions,\n+    window_strides,\n+    padding,\n+    base_dilation,\n+    window_dilation,\n+):\n \n   operand_aval, = ctx.avals_in\n   scalar_aval = operand_aval.update(shape=())\n \n-  return mlir.reduce_window(ctx,\n+  return mlir.reduce_window(\n+      ctx,\n       reducer_name=f\"reduce_window_{scalar_aval.dtype}_reducer\",\n       reducer_body=lambda reducer: [reduce_op(*reducer.arguments)],\n       operands=[operand],\n-      init_values=[mlir.full_like_aval(ctx, init_value(scalar_aval.dtype),\n-                                       scalar_aval)],\n+      init_values=[\n+          mlir.full_like_aval(ctx, init_value(scalar_aval.dtype), scalar_aval)\n+      ],\n       init_values_avals=[scalar_aval],\n       out_avals=ctx.avals_out,\n       window_dimensions=window_dimensions,\n-      window_strides=window_strides, base_dilation=base_dilation,\n-      window_dilation=window_dilation, padding=padding)\n+      window_strides=window_strides,\n+      base_dilation=base_dilation,\n+      window_dilation=window_dilation,\n+      padding=padding,\n+  )\n \n \n mlir.register_lowering(reduce_window_sum_p, partial(\n"
                }
            ],
            "whole_deleted": "-import numpy as np\n-\n-\n-from jax._src import ad_util\n-from jax._src.core import ShapedArray, ConcreteArray\n-from jax._src.lax import lax\n-def reduce_window(operand, init_value, computation: Callable,\n-                  window_dimensions: core.Shape, window_strides: Sequence[int],\n-                  padding: str | Sequence[tuple[int, int]],\n-                  base_dilation: Sequence[int] | None = None,\n-                  window_dilation: Sequence[int] | None = None) -> Array:\n-    raise ValueError('Operands must have the same tree structure as '\n-                     f'init_values: {operand_tree} vs. {init_value_tree}')\n-  if len(flat_operands) == 0:\n-    raise ValueError('reduce_window must have at least one operand.')\n-    raise ValueError('Must have same total number of operands as init_values: '\n-                     f' {len(flat_operands)} vs. {len(flat_init_values)}')\n-        computation, tuple(flat_init_avals), init_value_tree)\n-        *flat_operands, *flat_init_values, jaxpr=jaxpr.jaxpr,\n-        consts=tuple(jaxpr.consts), window_dimensions=tuple(window_dimensions),\n-        window_strides=tuple(window_strides), padding=padding,\n-        window_dilation=tuple(window_dilation))\n-def _get_monoid_window_reducer(monoid_op: Callable,\n-                               xs: Sequence[Array]) -> Callable | None:\n-    *avals, jaxpr, consts, window_dimensions, window_strides, padding,\n-    base_dilation, window_dilation):\n-  if any(o.dtype != iv.dtype for o, iv in zip(operand_avals, init_val_avals)):\n-    operand_avals[0], window_dimensions, window_strides, padding,\n-    base_dilation, window_dilation)\n-    batched_args, batch_dims, *, jaxpr, consts, window_dimensions,\n-    window_strides, padding, base_dilation, window_dilation):\n-def _generic_reduce_window_lower(ctx, *args, jaxpr, consts,\n-                                 window_dimensions, window_strides, padding,\n-                                 base_dilation, window_dilation):\n-      init_values=init_values, init_values_avals=init_value_avals,\n-      window_dimensions=window_dimensions, window_strides=window_strides,\n-      base_dilation=base_dilation, window_dilation=window_dilation,\n-      padding=padding)\n-def _common_reduce_window_shape_rule(operand, window_dimensions,\n-                                     window_strides, padding, base_dilation,\n-                                     window_dilation):\n-    msg = (\"reduce_window got the wrong number of window_dimensions for \"\n-           \"operand: got operand shape {} with window_dimensions {}.\")\n-    init_value, ctx, operand, *,\n-    window_dimensions, window_strides, padding, base_dilation,\n-    window_dilation):\n-  return mlir.reduce_window(ctx,\n-      init_values=[mlir.full_like_aval(ctx, init_value(scalar_aval.dtype),\n-                                       scalar_aval)],\n-      window_strides=window_strides, base_dilation=base_dilation,\n-      window_dilation=window_dilation, padding=padding)\n",
            "whole_added": "+from jax._src.core import ConcreteArray, ShapedArray\n+from jax._src.lax import lax\n+import numpy as np\n+from jax._src.core import ClosedJaxpr\n+from jax._src.core import jaxpr_as_fun\n+from jax._src.interpreters.ad import jvp_jaxpr\n+from jax._src import ad_util\n+def _reduce_window(\n+    operand,\n+    init_value,\n+    computation,\n+    window_dimensions: core.Shape,\n+    window_strides: Sequence[int],\n+    padding: str | Sequence[tuple[int, int]],\n+    base_dilation: Sequence[int] | None = None,\n+    window_dilation: Sequence[int] | None = None,\n+):\n+    raise ValueError(\n+        \"Operands must have the same tree structure as \"\n+        f\"init_values: {operand_tree} vs. {init_value_tree}\"\n+    )\n+    raise ValueError(\n+        \"Must have same total number of operands as init_values: \"\n+        f\" {len(flat_operands)} vs. {len(flat_init_values)}\"\n+    )\n+\n+  if len(flat_operands) == 0:\n+    raise ValueError(\"reduce_window must have at least one operand.\")\n+        computation, tuple(flat_init_avals), init_value_tree\n+    )\n+        *flat_operands,\n+        *flat_init_values,\n+        jaxpr=jaxpr.jaxpr,\n+        consts=tuple(jaxpr.consts),\n+        window_dimensions=tuple(window_dimensions),\n+        window_strides=tuple(window_strides),\n+        padding=padding,\n+        window_dilation=tuple(window_dilation),\n+    )\n+\n+\n+def reduce_window(\n+    operand,\n+    init_value,\n+    computation: Callable,\n+    window_dimensions: core.Shape,\n+    window_strides: Sequence[int],\n+    padding: str | Sequence[tuple[int, int]],\n+    base_dilation: Sequence[int] | None = None,\n+    window_dilation: Sequence[int] | None = None,\n+) -> Array:\n+  return _reduce_window(\n+      operand,\n+      init_value,\n+      computation,\n+      window_dimensions,\n+      window_strides,\n+      padding,\n+      base_dilation,\n+      window_dilation,\n+  )\n+\n+\n+def _get_monoid_window_reducer(\n+    monoid_op, xs: Sequence[Array]\n+) -> Callable | None:\n+    *avals,\n+    jaxpr,\n+    consts,\n+    window_dimensions,\n+    window_strides,\n+    padding,\n+    base_dilation,\n+    window_dilation,\n+):\n+  if any(\n+      o.dtype != iv.dtype for o, iv in zip(operand_avals, init_val_avals)\n+  ):\n+      operand_avals[0],\n+      window_dimensions,\n+      window_strides,\n+      padding,\n+      base_dilation,\n+      window_dilation,\n+  )\n+\n+    batched_args,\n+    batch_dims,\n+    *,\n+    jaxpr,\n+    consts,\n+    window_dimensions,\n+    window_strides,\n+    padding,\n+    base_dilation,\n+    window_dilation,\n+):\n+\n+\n+def reduce_window_jvp(\n+    primals,\n+    tangents,\n+    window_dimensions,\n+    window_strides,\n+    padding,\n+    base_dilation,\n+    window_dilation,\n+    jaxpr,\n+    consts,\n+):\n+\n+  reduction_jaxpr = jaxpr\n+\n+  n = len(primals) // 2  # number of primal operands\n+  operand, init_value = util.split_list(primals, [n])\n+  operand_tangent, init_value_tangent = util.split_list(tangents, [n])\n+  if not all(isinstance(t, ad.Zero) for t in init_value_tangent):\n+    raise TypeError(\"reduce_window jvp does not support non-zero init_value_tangent.\")\n+\n+  init_value_tangent = map(ad_util.instantiate, init_value_tangent)\n+  c_reduction_jaxpr = ClosedJaxpr(reduction_jaxpr, consts)\n+  jvp_reduction = jvp_jaxpr(c_reduction_jaxpr, (True,) * len(tangents), [False] * len(init_value_tangent))[0]\n+\n+  def wrapper(left, right):\n+    pl, tl = util.split_list(left, [n])\n+    pr, tr = util.split_list(right, [n])\n+    return jaxpr_as_fun(jvp_reduction)(*pl, *pr, *tl, *tr)\n+\n+  jvp_primals_tangents = _reduce_window(\n+      operand=[*operand, *operand_tangent],\n+      init_value=[*init_value, *init_value_tangent],\n+      computation=wrapper,\n+      window_dimensions=window_dimensions,\n+      window_strides=window_strides,\n+      padding=padding,\n+      base_dilation=base_dilation,\n+      window_dilation=window_dilation,\n+  )\n+  primals, tangents = util.split_list(jvp_primals_tangents, [len(jvp_primals_tangents) // 2])\n+  return [*primals], [*tangents]\n+\n+ad.primitive_jvps[reduce_window_p] = reduce_window_jvp\n+\n+def _generic_reduce_window_lower(\n+    ctx,\n+    *args,\n+    jaxpr,\n+    consts,\n+    window_dimensions,\n+    window_strides,\n+    padding,\n+    base_dilation,\n+    window_dilation,\n+):\n+      init_values=init_values,\n+      init_values_avals=init_value_avals,\n+      window_dimensions=window_dimensions,\n+      window_strides=window_strides,\n+      base_dilation=base_dilation,\n+      window_dilation=window_dilation,\n+      padding=padding,\n+  )\n+def _common_reduce_window_shape_rule(\n+    operand,\n+    window_dimensions,\n+    window_strides,\n+    padding,\n+    base_dilation,\n+    window_dilation,\n+):\n+    msg = (\n+        \"reduce_window got the wrong number of window_dimensions for \"\n+        \"operand: got operand shape {} with window_dimensions {}.\"\n+    )\n+    init_value,\n+    ctx,\n+    operand,\n+    *,\n+    window_dimensions,\n+    window_strides,\n+    padding,\n+    base_dilation,\n+    window_dilation,\n+):\n+  return mlir.reduce_window(\n+      ctx,\n+      init_values=[\n+          mlir.full_like_aval(ctx, init_value(scalar_aval.dtype), scalar_aval)\n+      ],\n+      window_strides=window_strides,\n+      base_dilation=base_dilation,\n+      window_dilation=window_dilation,\n+      padding=padding,\n+  )\n",
            "whole_hunk": "@@ -19,36 +19,42 @@ from functools import partial\n from typing import Callable\n import warnings\n \n-import numpy as np\n-\n from jax import tree_util\n-\n-from jax._src import ad_util\n from jax._src import core\n from jax._src import dispatch\n from jax._src import dtypes\n from jax._src import util\n-from jax._src.core import ShapedArray, ConcreteArray\n+from jax._src.core import ConcreteArray, ShapedArray\n from jax._src.interpreters import ad\n from jax._src.interpreters import batching\n from jax._src.interpreters import mlir\n-from jax._src.lax import lax\n from jax._src.lax import convolution\n+from jax._src.lax import lax\n from jax._src.lax import slicing\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import hlo\n from jax._src.numpy.ufuncs import logaddexp\n from jax._src.typing import Array\n+import numpy as np\n+from jax._src.core import ClosedJaxpr\n+from jax._src.core import jaxpr_as_fun\n+from jax._src.interpreters.ad import jvp_jaxpr\n+from jax._src import ad_util\n \n map = util.safe_map\n zip = util.safe_zip\n \n \n-def reduce_window(operand, init_value, computation: Callable,\n-                  window_dimensions: core.Shape, window_strides: Sequence[int],\n-                  padding: str | Sequence[tuple[int, int]],\n-                  base_dilation: Sequence[int] | None = None,\n-                  window_dilation: Sequence[int] | None = None) -> Array:\n+def _reduce_window(\n+    operand,\n+    init_value,\n+    computation,\n+    window_dimensions: core.Shape,\n+    window_strides: Sequence[int],\n+    padding: str | Sequence[tuple[int, int]],\n+    base_dilation: Sequence[int] | None = None,\n+    window_dilation: Sequence[int] | None = None,\n+):\n   \"\"\"Wraps XLA's `ReduceWindowWithGeneralPadding\n   <https://www.tensorflow.org/xla/operation_semantics#reducewindow>`_\n   operator.\n@@ -56,13 +62,18 @@ def reduce_window(operand, init_value, computation: Callable,\n   flat_operands, operand_tree = tree_util.tree_flatten(operand)\n   flat_init_values, init_value_tree = tree_util.tree_flatten(init_value)\n   if operand_tree != init_value_tree:\n-    raise ValueError('Operands must have the same tree structure as '\n-                     f'init_values: {operand_tree} vs. {init_value_tree}')\n-  if len(flat_operands) == 0:\n-    raise ValueError('reduce_window must have at least one operand.')\n+    raise ValueError(\n+        \"Operands must have the same tree structure as \"\n+        f\"init_values: {operand_tree} vs. {init_value_tree}\"\n+    )\n   if len(flat_operands) != len(flat_init_values):\n-    raise ValueError('Must have same total number of operands as init_values: '\n-                     f' {len(flat_operands)} vs. {len(flat_init_values)}')\n+    raise ValueError(\n+        \"Must have same total number of operands as init_values: \"\n+        f\" {len(flat_operands)} vs. {len(flat_init_values)}\"\n+    )\n+\n+  if len(flat_operands) == 0:\n+    raise ValueError(\"reduce_window must have at least one operand.\")\n   if isinstance(padding, str):\n     dilated_window_dims = (\n         window_dimensions if window_dilation is None else\n@@ -82,21 +93,52 @@ def reduce_window(operand, init_value, computation: Callable,\n   else:\n     flat_init_avals = map(lax._abstractify, flat_init_values)\n     jaxpr, out_tree = lax._variadic_reduction_jaxpr(\n-        computation, tuple(flat_init_avals), init_value_tree)\n+        computation, tuple(flat_init_avals), init_value_tree\n+    )\n     if operand_tree != out_tree:\n       raise ValueError(\n         'reduce_window output must have the same tree structure as the operands'\n         f' {operand_tree} vs. {out_tree}')\n     out_flat = reduce_window_p.bind(\n-        *flat_operands, *flat_init_values, jaxpr=jaxpr.jaxpr,\n-        consts=tuple(jaxpr.consts), window_dimensions=tuple(window_dimensions),\n-        window_strides=tuple(window_strides), padding=padding,\n+        *flat_operands,\n+        *flat_init_values,\n+        jaxpr=jaxpr.jaxpr,\n+        consts=tuple(jaxpr.consts),\n+        window_dimensions=tuple(window_dimensions),\n+        window_strides=tuple(window_strides),\n+        padding=padding,\n         base_dilation=tuple(base_dilation),\n-        window_dilation=tuple(window_dilation))\n+        window_dilation=tuple(window_dilation),\n+    )\n     return tree_util.tree_unflatten(out_tree, out_flat)\n \n-def _get_monoid_window_reducer(monoid_op: Callable,\n-                               xs: Sequence[Array]) -> Callable | None:\n+\n+\n+def reduce_window(\n+    operand,\n+    init_value,\n+    computation: Callable,\n+    window_dimensions: core.Shape,\n+    window_strides: Sequence[int],\n+    padding: str | Sequence[tuple[int, int]],\n+    base_dilation: Sequence[int] | None = None,\n+    window_dilation: Sequence[int] | None = None,\n+) -> Array:\n+  return _reduce_window(\n+      operand,\n+      init_value,\n+      computation,\n+      window_dimensions,\n+      window_strides,\n+      padding,\n+      base_dilation,\n+      window_dilation,\n+  )\n+\n+\n+def _get_monoid_window_reducer(\n+    monoid_op, xs: Sequence[Array]\n+) -> Callable | None:\n   if len(xs) != 1:\n     return None\n   x, = xs\n@@ -260,10 +303,19 @@ def _select_and_gather_add(tangents: Array, operand: Array,\n \n \n def _reduce_window_abstract_eval_rule(\n-    *avals, jaxpr, consts, window_dimensions, window_strides, padding,\n-    base_dilation, window_dilation):\n+    *avals,\n+    jaxpr,\n+    consts,\n+    window_dimensions,\n+    window_strides,\n+    padding,\n+    base_dilation,\n+    window_dilation,\n+):\n   operand_avals, init_val_avals = util.split_list(avals, [len(avals) // 2])\n-  if any(o.dtype != iv.dtype for o, iv in zip(operand_avals, init_val_avals)):\n+  if any(\n+      o.dtype != iv.dtype for o, iv in zip(operand_avals, init_val_avals)\n+  ):\n     msg = (\"reduce_window got inconsistent dtypes for operands and init_values:\"\n            \" got operand dtypes {} and init_value dtypes {}.\")\n     raise TypeError(msg.format([o.dtype for o in operand_avals],\n@@ -273,13 +325,28 @@ def _reduce_window_abstract_eval_rule(\n            \"have shapes {}.\")\n     raise TypeError(msg.format([v.shape for v in init_val_avals]))\n   out_shape = _common_reduce_window_shape_rule(\n-    operand_avals[0], window_dimensions, window_strides, padding,\n-    base_dilation, window_dilation)\n+      operand_avals[0],\n+      window_dimensions,\n+      window_strides,\n+      padding,\n+      base_dilation,\n+      window_dilation,\n+  )\n   return tuple(ShapedArray(out_shape, op.dtype) for op in operand_avals)\n \n+\n def _generic_reduce_window_batch_rule(\n-    batched_args, batch_dims, *, jaxpr, consts, window_dimensions,\n-    window_strides, padding, base_dilation, window_dilation):\n+    batched_args,\n+    batch_dims,\n+    *,\n+    jaxpr,\n+    consts,\n+    window_dimensions,\n+    window_strides,\n+    padding,\n+    base_dilation,\n+    window_dilation,\n+):\n   num_operands = len(batched_args) // 2\n   operands, init_values = util.split_list(batched_args, [num_operands])\n   operand_bdims, init_value_bdims = util.split_list(batch_dims, [num_operands])\n@@ -306,14 +373,68 @@ def _generic_reduce_window_batch_rule(\n \n \n reduce_window_p = core.Primitive('reduce_window')\n+\n+\n+def reduce_window_jvp(\n+    primals,\n+    tangents,\n+    window_dimensions,\n+    window_strides,\n+    padding,\n+    base_dilation,\n+    window_dilation,\n+    jaxpr,\n+    consts,\n+):\n+\n+  reduction_jaxpr = jaxpr\n+\n+  n = len(primals) // 2  # number of primal operands\n+  operand, init_value = util.split_list(primals, [n])\n+  operand_tangent, init_value_tangent = util.split_list(tangents, [n])\n+  if not all(isinstance(t, ad.Zero) for t in init_value_tangent):\n+    raise TypeError(\"reduce_window jvp does not support non-zero init_value_tangent.\")\n+\n+  init_value_tangent = map(ad_util.instantiate, init_value_tangent)\n+  c_reduction_jaxpr = ClosedJaxpr(reduction_jaxpr, consts)\n+  jvp_reduction = jvp_jaxpr(c_reduction_jaxpr, (True,) * len(tangents), [False] * len(init_value_tangent))[0]\n+\n+  def wrapper(left, right):\n+    pl, tl = util.split_list(left, [n])\n+    pr, tr = util.split_list(right, [n])\n+    return jaxpr_as_fun(jvp_reduction)(*pl, *pr, *tl, *tr)\n+\n+  jvp_primals_tangents = _reduce_window(\n+      operand=[*operand, *operand_tangent],\n+      init_value=[*init_value, *init_value_tangent],\n+      computation=wrapper,\n+      window_dimensions=window_dimensions,\n+      window_strides=window_strides,\n+      padding=padding,\n+      base_dilation=base_dilation,\n+      window_dilation=window_dilation,\n+  )\n+  primals, tangents = util.split_list(jvp_primals_tangents, [len(jvp_primals_tangents) // 2])\n+  return [*primals], [*tangents]\n+\n+ad.primitive_jvps[reduce_window_p] = reduce_window_jvp\n reduce_window_p.multiple_results = True\n reduce_window_p.def_impl(partial(dispatch.apply_primitive, reduce_window_p))\n reduce_window_p.def_abstract_eval(_reduce_window_abstract_eval_rule)\n batching.primitive_batchers[reduce_window_p] = _generic_reduce_window_batch_rule\n \n-def _generic_reduce_window_lower(ctx, *args, jaxpr, consts,\n-                                 window_dimensions, window_strides, padding,\n-                                 base_dilation, window_dilation):\n+\n+def _generic_reduce_window_lower(\n+    ctx,\n+    *args,\n+    jaxpr,\n+    consts,\n+    window_dimensions,\n+    window_strides,\n+    padding,\n+    base_dilation,\n+    window_dilation,\n+):\n   operands, init_values = util.split_list(args, [len(args) // 2])\n   _, init_value_avals = util.split_list(ctx.avals_in, [len(operands)])\n \n@@ -330,11 +451,15 @@ def _generic_reduce_window_lower(ctx, *args, jaxpr, consts,\n       reducer_name=\"generic_reduce_window_reducer\",\n       reducer_body=reducer_body,\n       operands=operands,\n-      init_values=init_values, init_values_avals=init_value_avals,\n+      init_values=init_values,\n+      init_values_avals=init_value_avals,\n       out_avals=ctx.avals_out,\n-      window_dimensions=window_dimensions, window_strides=window_strides,\n-      base_dilation=base_dilation, window_dilation=window_dilation,\n-      padding=padding)\n+      window_dimensions=window_dimensions,\n+      window_strides=window_strides,\n+      base_dilation=base_dilation,\n+      window_dilation=window_dilation,\n+      padding=padding,\n+  )\n \n \n mlir.register_lowering(reduce_window_p, _generic_reduce_window_lower)\n@@ -402,9 +527,14 @@ def _reduce_window_chooser_jvp_rule(prim, g, operand, *, window_dimensions,\n                                 window_dilation)\n \n \n-def _common_reduce_window_shape_rule(operand, window_dimensions,\n-                                     window_strides, padding, base_dilation,\n-                                     window_dilation):\n+def _common_reduce_window_shape_rule(\n+    operand,\n+    window_dimensions,\n+    window_strides,\n+    padding,\n+    base_dilation,\n+    window_dilation,\n+):\n   lax._check_shapelike(\"reduce_window\", \"window_dimensions\", window_dimensions,\n                        non_zero_shape=True)\n   lax._check_shapelike(\"reduce_window\", \"window_strides\", window_strides,\n@@ -412,8 +542,10 @@ def _common_reduce_window_shape_rule(operand, window_dimensions,\n   lax._check_shapelike(\"reduce_window\", \"base_dilation\", base_dilation)\n   lax._check_shapelike(\"reduce_window\", \"window_dilation\", window_dilation)\n   if operand.ndim != len(window_dimensions):\n-    msg = (\"reduce_window got the wrong number of window_dimensions for \"\n-           \"operand: got operand shape {} with window_dimensions {}.\")\n+    msg = (\n+        \"reduce_window got the wrong number of window_dimensions for \"\n+        \"operand: got operand shape {} with window_dimensions {}.\"\n+    )\n     raise TypeError(msg.format(operand.shape, window_dimensions))\n   if len(window_strides) != len(window_dimensions):\n     msg = (\"reduce_window got inconsistent window_strides and \"\n@@ -463,24 +596,36 @@ batching.primitive_batchers[reduce_window_min_p] = partial(\n \n def _reduce_window_lower(\n     reduce_op,\n-    init_value, ctx, operand, *,\n-    window_dimensions, window_strides, padding, base_dilation,\n-    window_dilation):\n+    init_value,\n+    ctx,\n+    operand,\n+    *,\n+    window_dimensions,\n+    window_strides,\n+    padding,\n+    base_dilation,\n+    window_dilation,\n+):\n \n   operand_aval, = ctx.avals_in\n   scalar_aval = operand_aval.update(shape=())\n \n-  return mlir.reduce_window(ctx,\n+  return mlir.reduce_window(\n+      ctx,\n       reducer_name=f\"reduce_window_{scalar_aval.dtype}_reducer\",\n       reducer_body=lambda reducer: [reduce_op(*reducer.arguments)],\n       operands=[operand],\n-      init_values=[mlir.full_like_aval(ctx, init_value(scalar_aval.dtype),\n-                                       scalar_aval)],\n+      init_values=[\n+          mlir.full_like_aval(ctx, init_value(scalar_aval.dtype), scalar_aval)\n+      ],\n       init_values_avals=[scalar_aval],\n       out_avals=ctx.avals_out,\n       window_dimensions=window_dimensions,\n-      window_strides=window_strides, base_dilation=base_dilation,\n-      window_dilation=window_dilation, padding=padding)\n+      window_strides=window_strides,\n+      base_dilation=base_dilation,\n+      window_dilation=window_dilation,\n+      padding=padding,\n+  )\n \n \n mlir.register_lowering(reduce_window_sum_p, partial(\n"
        },
        {
            "name": "test_util.py",
            "path": "jax/_src/test_util.py",
            "patches": [
                {
                    "old_start": 57,
                    "old_length": 7,
                    "new_start": 57,
                    "new_length": 7,
                    "hunk": "@@ -57,7 +57,7 @@ from jax._src.numpy.util import promote_dtypes, promote_dtypes_inexact\n from jax._src.util import unzip2\n from jax._src.public_test_util import (  # noqa: F401\n     _assert_numpy_allclose, _check_dtypes_match, _default_tolerance, _dtype, check_close, check_grads,\n-    check_jvp, check_vjp, default_gradient_tolerance, default_tolerance, tolerance)\n+    check_jvp, check_vjp, default_gradient_tolerance, default_tolerance, tolerance, rand_like)\n from jax._src import xla_bridge\n \n \n"
                }
            ],
            "whole_deleted": "-    check_jvp, check_vjp, default_gradient_tolerance, default_tolerance, tolerance)\n",
            "whole_added": "+    check_jvp, check_vjp, default_gradient_tolerance, default_tolerance, tolerance, rand_like)\n",
            "whole_hunk": "@@ -57,7 +57,7 @@ from jax._src.numpy.util import promote_dtypes, promote_dtypes_inexact\n from jax._src.util import unzip2\n from jax._src.public_test_util import (  # noqa: F401\n     _assert_numpy_allclose, _check_dtypes_match, _default_tolerance, _dtype, check_close, check_grads,\n-    check_jvp, check_vjp, default_gradient_tolerance, default_tolerance, tolerance)\n+    check_jvp, check_vjp, default_gradient_tolerance, default_tolerance, tolerance, rand_like)\n from jax._src import xla_bridge\n \n \n"
        },
        {
            "name": "lax_test.py",
            "path": "tests/lax_test.py",
            "patches": [
                {
                    "old_start": 46,
                    "old_length": 6,
                    "new_start": 46,
                    "new_length": 7,
                    "hunk": "@@ -46,6 +46,7 @@ from jax._src.interpreters import pxla\n from jax._src.internal_test_util import lax_test_util\n from jax._src.lax import lax as lax_internal\n from jax._src.util import NumpyComplexWarning\n+from jax._src.tree_util import tree_map\n \n config.parse_flags_with_absl()\n \n"
                },
                {
                    "old_start": 68,
                    "old_length": 6,
                    "new_start": 69,
                    "new_length": 22,
                    "hunk": "@@ -68,6 +69,22 @@ preferred_type_combinations = [\n   (np.int32, np.float32), (np.int32, np.float64), (np.int64, np.float64)]\n \n \n+def _reduce_custom_add(x, y):\n+  return x + y\n+\n+def _reduce_custom_mul(x, y):\n+  return x * y\n+\n+def _reduce_custom_sub(x, y):\n+  return x - y\n+\n+def _reduce_custom_min(x, y):\n+  return jnp.minimum(x, y)\n+\n+def _reduce_custom_max(x, y):\n+  return jnp.maximum(x, y)\n+\n+\n class LaxTest(jtu.JaxTestCase):\n   \"\"\"Numerical tests for LAX operations.\"\"\"\n \n"
                },
                {
                    "old_start": 1285,
                    "old_length": 7,
                    "new_start": 1302,
                    "new_length": 7,
                    "hunk": "@@ -1285,7 +1302,7 @@ class LaxTest(jtu.JaxTestCase):\n     numpy_op = lambda x: lax_reference.squeeze(x, dimensions)\n     self._CompileAndCheck(op, args_maker)\n     self._CheckAgainstNumpy(numpy_op, op, args_maker)\n-    check_grads(op, args_maker(), 2, [\"fwd\", \"rev\"], eps=1.)\n+    check_grads(op, args_maker(), 3, [\"fwd\", \"rev\"], eps=1.)\n \n   @jtu.sample_product(\n     input_type=[\"np.array\", \"jnp.array\", \"float\", \"np.float32\"],\n"
                },
                {
                    "old_start": 1799,
                    "old_length": 35,
                    "new_start": 1816,
                    "new_length": 287,
                    "hunk": "@@ -1799,35 +1816,287 @@ class LaxTest(jtu.JaxTestCase):\n     # we separately test the version that uses a concrete init_val because it\n     # can hit different code paths\n     def fun(operand):\n-      return lax.reduce_window(operand, init_val, op, dims, strides, padding,\n-                               base_dilation, window_dilation)\n+      return lax.reduce_window(\n+          operand,\n+          init_val,\n+          op,\n+          dims,\n+          strides,\n+          padding,\n+          base_dilation,\n+          window_dilation,\n+      )\n \n     args_maker = lambda: [rng(shape, dtype)]\n     self._CompileAndCheck(fun, args_maker)\n \n+  # TODO(voz): I broke these out to their own test for 2 reasons:\n+  # 1. I wanted to show that general ops work, there's a small subset of\n+  # ops, specifically, the ones used in the test above, lax.add, lax.max, and\n+  # lax.min that actually route to a monoid operator that *doesn't* pass JVP\n+  # tests.\n+  # 2. Slightly different parameterization.\n   @jtu.sample_product(\n-    [dict(shape=shape, dims=dims, strides=strides, padding=padding,\n-          base_dilation=base_dilation, window_dilation=window_dilation)\n-      for shape, dims, strides, padding, base_dilation, window_dilation in (\n-        itertools.chain(\n-          itertools.product(\n-            [(4, 6)],\n-            [(2, 1), (1, 2)],\n-            [(1, 1), (2, 1), (1, 2)],\n-            [\"VALID\", \"SAME\", [(0, 3), (1, 2)]],\n-            [(1, 1), (2, 3)],\n-            [(1, 1), (1, 2)]),\n-          itertools.product(\n-            [(3, 2, 4, 6)], [(1, 1, 2, 1), (2, 1, 2, 1)],\n-            [(1, 2, 2, 1), (1, 1, 1, 1)],\n-            [\"VALID\", \"SAME\", [(0, 1), (1, 0), (2, 3), (0, 2)]],\n-            [(1, 1, 1, 1), (2, 1, 3, 2)],\n-            [(1, 1, 1, 1), (1, 2, 2, 1)])))\n-    ],\n-    dtype=[np.float32],\n+      [\n+          dict(init_val=init_val, op=op, dtype=dtype)\n+          for init_val, op, dtypes in [\n+              (1, _reduce_custom_add, [np.float32]),\n+              (0, _reduce_custom_mul, [np.float32]),\n+              (0, _reduce_custom_sub, [np.float32]),\n+          ]\n+          for dtype in dtypes\n+      ],\n+      [\n+          dict(\n+              shape=shape,\n+              dims=dims,\n+              strides=strides,\n+              padding=padding,\n+              base_dilation=base_dilation,\n+              window_dilation=window_dilation,\n+          )\n+          for shape, dims, strides, padding, base_dilation, window_dilation in (\n+              itertools.chain(\n+                  itertools.product(\n+                      [(4, 6)],\n+                      [(2, 1), (1, 2)],\n+                      [(1, 1), (2, 1), (1, 2)],\n+                      ['VALID', 'SAME', [(0, 3), (1, 2)]],\n+                      [(1, 1), (2, 3)],\n+                      [(1, 1), (1, 2)],\n+                  ),\n+                  itertools.product(\n+                      [(3, 2, 4, 6)],\n+                      [(1, 1, 2, 1), (2, 1, 2, 1)],\n+                      [(1, 2, 2, 1), (1, 1, 1, 1)],\n+                      ['VALID', 'SAME', [(0, 1), (1, 0), (2, 3), (0, 2)]],\n+                      [(1, 1, 1, 1), (2, 1, 3, 2)],\n+                      [(1, 1, 1, 1), (1, 2, 2, 1)],\n+                  ),\n+              )\n+          )\n+      ],\n   )\n+  @jtu.skip_on_devices('gpu') # jax.lax.mul has an XLA bug on GPU b/339071103\n+  @jtu.skip_on_devices('tpu') # b/39342488\n+  def testReduceWindowGeneralJVP(\n+      self,\n+      op,\n+      init_val,\n+      dtype,\n+      shape,\n+      dims,\n+      strides,\n+      padding,\n+      base_dilation,\n+      window_dilation,\n+  ):\n+    rng = jtu.rand_small(self.rng())\n+    init_val = np.asarray(init_val, dtype=dtype)\n+\n+    def fun(operand, init_val):\n+      return lax.reduce_window(\n+          operand,\n+          init_val,\n+          op,\n+          dims,\n+          strides,\n+          padding,\n+          base_dilation,\n+          window_dilation,\n+      )\n+\n+    args_maker = lambda: [rng(shape, dtype), init_val]\n+    self._CompileAndCheck(fun, args_maker)\n+    args = args_maker()\n+    init_val = args[1]\n+\n+    # we separately test the version that uses a concrete init_val because it\n+    # can hit different code paths\n+    def fun2(operand):\n+      return lax.reduce_window(\n+          operand,\n+          init_val,\n+          op,\n+          dims,\n+          strides,\n+          padding,\n+          base_dilation,\n+          window_dilation,\n+      )\n+\n+    args_maker = lambda: [rng(shape, dtype)]\n+    self._CompileAndCheck(fun2, args_maker)\n+\n+    operand = args_maker()[0]\n+    jtu.check_jvp(fun2, partial(jax.jvp, fun2), (operand,))\n+    check_grads(fun2, (operand,), 3, [\"fwd\"], eps=1.)\n+\n+  @jtu.sample_product(\n+      [\n+          dict(init_val=init_val, op=op, dtype=dtype)\n+          for init_val, op, dtypes in [\n+              (-np.inf, lax.max, [np.float32]),\n+              (np.inf, lax.min, [np.float32]),\n+              (0, lax.add, [np.float32]),\n+          ]\n+          for dtype in dtypes\n+      ],\n+      [\n+          dict(\n+              shape=shape,\n+              dims=dims,\n+              strides=strides,\n+              padding=padding,\n+              base_dilation=base_dilation,\n+              window_dilation=window_dilation,\n+          )\n+          for shape, dims, strides, padding, base_dilation, window_dilation in (\n+              itertools.chain(\n+                  itertools.product(\n+                      [(4, 6)],\n+                      [(2, 1), (1, 2)],\n+                      [(1, 1), (2, 1), (1, 2)],\n+                      ['VALID', 'SAME', [(0, 3), (1, 2)]],\n+                      [(1, 1), (2, 3)],\n+                      [(1, 1), (1, 2)],\n+                  ),\n+                  itertools.product(\n+                      [(3, 2, 4, 6)],\n+                      [(1, 1, 2, 1), (2, 1, 2, 1)],\n+                      [(1, 2, 2, 1), (1, 1, 1, 1)],\n+                      ['VALID', 'SAME', [(0, 1), (1, 0), (2, 3), (0, 2)]],\n+                      [(1, 1, 1, 1), (2, 1, 3, 2)],\n+                      [(1, 1, 1, 1), (1, 2, 2, 1)],\n+                  ),\n+              )\n+          )\n+      ],\n+  )\n+  @jtu.skip_on_devices('gpu') # jax.lax.mul has an XLA bug on GPU b/339071103\n+  @jtu.skip_on_devices('tpu') # b/39342488\n+  def testReduceWindowCustomSameAsMonoid(\n+      self,\n+      op,\n+      init_val,\n+      dtype,\n+      shape,\n+      dims,\n+      strides,\n+      padding,\n+      base_dilation,\n+      window_dilation,\n+  ):\n+    rng = jtu.rand_small(self.rng())\n+    init_val = np.asarray(init_val, dtype=dtype)\n+\n+    def fun(op_, operand_):\n+      return lax.reduce_window(\n+          operand_,\n+          init_val,\n+          op_,\n+          dims,\n+          strides,\n+          padding,\n+          base_dilation,\n+          window_dilation,\n+      )\n+\n+    args_maker = lambda: [rng(shape, dtype)]\n+    args = args_maker()\n+    operand = args[0]\n+    rng = np.random.RandomState(0)\n+    tangent = tree_map(partial(jtu.rand_like, rng), operand)\n+\n+    # There are \"special\" paths for \"monoid\" ops that have\n+    # their jvp defined separately, either for legacy reasons\n+    # or for optimization - compare across both and prove\n+    # that their jvp is the same.\n+    # TODO(voz): Look into the \"monoid\" paths and collapse them as necessary.\n+    # Especially when we go to add support for (1) recursive is_jvp (hessians),\n+    # and (2) transpose?\n+    custom_equiv = {\n+        lax.max: _reduce_custom_max,\n+        lax.min: _reduce_custom_min,\n+        lax.add: _reduce_custom_add,\n+    }\n+    custom_op = custom_equiv[op]\n+    custom_primals, custom_tangents = jax.jvp(\n+        partial(fun, custom_op),\n+        primals=(operand,),\n+        tangents=(tangent,),\n+    )\n+    lax_primals, lax_tangents = jax.jvp(\n+        partial(fun, op),\n+        primals=(operand,),\n+        tangents=(tangent,),\n+    )\n+    # tol = 1e-4\n+    # None is sane defaults, but useful to have here for debugging.\n+    tol = None\n+    jtu.check_close(\n+        lax_primals,\n+        custom_primals,\n+        atol=tol,\n+        rtol=tol,\n+        err_msg='Mismatched primal',\n+    )\n+    jtu.check_close(\n+        lax_tangents,\n+        custom_tangents,\n+        atol=tol,\n+        rtol=tol,\n+        err_msg='Mismatched tangents',\n+    )\n+    # Numerical jvp comparison for min and max values\n+    # does not work - the underlying implementation of the test util\n+    # nans on infs.\n+    if init_val.item() in (np.inf, -np.inf):\n+      return\n+    op_bound_fn = partial(fun, op)\n+    jtu.check_jvp(\n+        op_bound_fn,\n+        partial(jax.jvp, op_bound_fn),\n+        (operand,),\n+    )\n+    check_grads(partial(fun, op), [operand], 3, [\"fwd\"], eps=1.)\n+    check_grads(partial(fun, custom_op), [operand], 3, [\"fwd\"], eps=1.)\n+\n   # TODO(b/183233858): variadic reduce-window is not implemented on XLA:GPU\n-  @jtu.skip_on_devices(\"gpu\")\n+  @jtu.sample_product(\n+      [\n+          dict(\n+              shape=shape,\n+              dims=dims,\n+              strides=strides,\n+              padding=padding,\n+              base_dilation=base_dilation,\n+              window_dilation=window_dilation,\n+          )\n+          for shape, dims, strides, padding, base_dilation, window_dilation in (\n+              itertools.chain(\n+                  itertools.product(\n+                      [(4, 6)],\n+                      [(2, 1), (1, 2)],\n+                      [(1, 1), (2, 1), (1, 2)],\n+                      ['VALID', 'SAME', [(0, 3), (1, 2)]],\n+                      [(1, 1), (2, 3)],\n+                      [(1, 1), (1, 2)],\n+                  ),\n+                  itertools.product(\n+                      [(3, 2, 4, 6)],\n+                      [(1, 1, 2, 1), (2, 1, 2, 1)],\n+                      [(1, 2, 2, 1), (1, 1, 1, 1)],\n+                      ['VALID', 'SAME', [(0, 1), (1, 0), (2, 3), (0, 2)]],\n+                      [(1, 1, 1, 1), (2, 1, 3, 2)],\n+                      [(1, 1, 1, 1), (1, 2, 2, 1)],\n+                  ),\n+              )\n+          )\n+      ],\n+      dtype=[np.float32],\n+  )\n+  @jtu.skip_on_devices('gpu')\n   def testReduceWindowVariadic(self, dtype, shape, dims, strides, padding,\n                                base_dilation, window_dilation):\n     if (jtu.test_device_matches([\"tpu\"]) and\n"
                },
                {
                    "old_start": 2726,
                    "old_length": 21,
                    "new_start": 2995,
                    "new_length": 20,
                    "hunk": "@@ -2726,21 +2995,20 @@ class LaxTest(jtu.JaxTestCase):\n   def test_constant_folding_complex_to_real_scan_regression(self):\n     # regression test for github.com/google/jax/issues/19059\n     def g(hiddens):\n-        hiddens_aug = jnp.vstack((hiddens[0], hiddens))\n-        new_hiddens = hiddens_aug.copy()\n-        diff = new_hiddens[:-1] - hiddens\n-        diff = new_hiddens[:-1] - hiddens\n-        out = jnp.trace(jnp.conj(diff).T @ diff).real\n-        return jnp.array(out, dtype=jnp.complex64)\n-\n+      hiddens_aug = jnp.vstack((hiddens[0], hiddens))\n+      new_hiddens = hiddens_aug.copy()\n+      diff = new_hiddens[:-1] - hiddens\n+      diff = new_hiddens[:-1] - hiddens\n+      out = jnp.trace(jnp.conj(diff).T @ diff).real\n+      return jnp.array(out, dtype=jnp.complex64)\n \n     def _step(carry, arg):\n-        primals, f_vjp = jax.vjp(\n-            g,\n-            jax.random.normal(jax.random.key(0), (9, 8), dtype=jnp.complex64),\n-        )\n-        out = f_vjp(np.array(1.0 + 0j, 'complex64'))[0]\n-        return carry, carry\n+      primals, f_vjp = jax.vjp(\n+          g,\n+          jax.random.normal(jax.random.key(0), (9, 8), dtype=jnp.complex64),\n+      )\n+      out = f_vjp(np.array(1.0 + 0j, 'complex64'))[0]\n+      return carry, carry\n \n     a, b = jax.lax.scan(_step, 0, jnp.arange(4, dtype=jnp.complex64))\n "
                }
            ],
            "whole_deleted": "-    check_grads(op, args_maker(), 2, [\"fwd\", \"rev\"], eps=1.)\n-      return lax.reduce_window(operand, init_val, op, dims, strides, padding,\n-                               base_dilation, window_dilation)\n-    [dict(shape=shape, dims=dims, strides=strides, padding=padding,\n-          base_dilation=base_dilation, window_dilation=window_dilation)\n-      for shape, dims, strides, padding, base_dilation, window_dilation in (\n-        itertools.chain(\n-          itertools.product(\n-            [(4, 6)],\n-            [(2, 1), (1, 2)],\n-            [(1, 1), (2, 1), (1, 2)],\n-            [\"VALID\", \"SAME\", [(0, 3), (1, 2)]],\n-            [(1, 1), (2, 3)],\n-            [(1, 1), (1, 2)]),\n-          itertools.product(\n-            [(3, 2, 4, 6)], [(1, 1, 2, 1), (2, 1, 2, 1)],\n-            [(1, 2, 2, 1), (1, 1, 1, 1)],\n-            [\"VALID\", \"SAME\", [(0, 1), (1, 0), (2, 3), (0, 2)]],\n-            [(1, 1, 1, 1), (2, 1, 3, 2)],\n-            [(1, 1, 1, 1), (1, 2, 2, 1)])))\n-    ],\n-    dtype=[np.float32],\n-  @jtu.skip_on_devices(\"gpu\")\n-        hiddens_aug = jnp.vstack((hiddens[0], hiddens))\n-        new_hiddens = hiddens_aug.copy()\n-        diff = new_hiddens[:-1] - hiddens\n-        diff = new_hiddens[:-1] - hiddens\n-        out = jnp.trace(jnp.conj(diff).T @ diff).real\n-        return jnp.array(out, dtype=jnp.complex64)\n-\n-        primals, f_vjp = jax.vjp(\n-            g,\n-            jax.random.normal(jax.random.key(0), (9, 8), dtype=jnp.complex64),\n-        )\n-        out = f_vjp(np.array(1.0 + 0j, 'complex64'))[0]\n-        return carry, carry\n",
            "whole_added": "+from jax._src.tree_util import tree_map\n+def _reduce_custom_add(x, y):\n+  return x + y\n+\n+def _reduce_custom_mul(x, y):\n+  return x * y\n+\n+def _reduce_custom_sub(x, y):\n+  return x - y\n+\n+def _reduce_custom_min(x, y):\n+  return jnp.minimum(x, y)\n+\n+def _reduce_custom_max(x, y):\n+  return jnp.maximum(x, y)\n+\n+\n+    check_grads(op, args_maker(), 3, [\"fwd\", \"rev\"], eps=1.)\n+      return lax.reduce_window(\n+          operand,\n+          init_val,\n+          op,\n+          dims,\n+          strides,\n+          padding,\n+          base_dilation,\n+          window_dilation,\n+      )\n+  # TODO(voz): I broke these out to their own test for 2 reasons:\n+  # 1. I wanted to show that general ops work, there's a small subset of\n+  # ops, specifically, the ones used in the test above, lax.add, lax.max, and\n+  # lax.min that actually route to a monoid operator that *doesn't* pass JVP\n+  # tests.\n+  # 2. Slightly different parameterization.\n+      [\n+          dict(init_val=init_val, op=op, dtype=dtype)\n+          for init_val, op, dtypes in [\n+              (1, _reduce_custom_add, [np.float32]),\n+              (0, _reduce_custom_mul, [np.float32]),\n+              (0, _reduce_custom_sub, [np.float32]),\n+          ]\n+          for dtype in dtypes\n+      ],\n+      [\n+          dict(\n+              shape=shape,\n+              dims=dims,\n+              strides=strides,\n+              padding=padding,\n+              base_dilation=base_dilation,\n+              window_dilation=window_dilation,\n+          )\n+          for shape, dims, strides, padding, base_dilation, window_dilation in (\n+              itertools.chain(\n+                  itertools.product(\n+                      [(4, 6)],\n+                      [(2, 1), (1, 2)],\n+                      [(1, 1), (2, 1), (1, 2)],\n+                      ['VALID', 'SAME', [(0, 3), (1, 2)]],\n+                      [(1, 1), (2, 3)],\n+                      [(1, 1), (1, 2)],\n+                  ),\n+                  itertools.product(\n+                      [(3, 2, 4, 6)],\n+                      [(1, 1, 2, 1), (2, 1, 2, 1)],\n+                      [(1, 2, 2, 1), (1, 1, 1, 1)],\n+                      ['VALID', 'SAME', [(0, 1), (1, 0), (2, 3), (0, 2)]],\n+                      [(1, 1, 1, 1), (2, 1, 3, 2)],\n+                      [(1, 1, 1, 1), (1, 2, 2, 1)],\n+                  ),\n+              )\n+          )\n+      ],\n+  @jtu.skip_on_devices('gpu') # jax.lax.mul has an XLA bug on GPU b/339071103\n+  @jtu.skip_on_devices('tpu') # b/39342488\n+  def testReduceWindowGeneralJVP(\n+      self,\n+      op,\n+      init_val,\n+      dtype,\n+      shape,\n+      dims,\n+      strides,\n+      padding,\n+      base_dilation,\n+      window_dilation,\n+  ):\n+    rng = jtu.rand_small(self.rng())\n+    init_val = np.asarray(init_val, dtype=dtype)\n+\n+    def fun(operand, init_val):\n+      return lax.reduce_window(\n+          operand,\n+          init_val,\n+          op,\n+          dims,\n+          strides,\n+          padding,\n+          base_dilation,\n+          window_dilation,\n+      )\n+\n+    args_maker = lambda: [rng(shape, dtype), init_val]\n+    self._CompileAndCheck(fun, args_maker)\n+    args = args_maker()\n+    init_val = args[1]\n+\n+    # we separately test the version that uses a concrete init_val because it\n+    # can hit different code paths\n+    def fun2(operand):\n+      return lax.reduce_window(\n+          operand,\n+          init_val,\n+          op,\n+          dims,\n+          strides,\n+          padding,\n+          base_dilation,\n+          window_dilation,\n+      )\n+\n+    args_maker = lambda: [rng(shape, dtype)]\n+    self._CompileAndCheck(fun2, args_maker)\n+\n+    operand = args_maker()[0]\n+    jtu.check_jvp(fun2, partial(jax.jvp, fun2), (operand,))\n+    check_grads(fun2, (operand,), 3, [\"fwd\"], eps=1.)\n+\n+  @jtu.sample_product(\n+      [\n+          dict(init_val=init_val, op=op, dtype=dtype)\n+          for init_val, op, dtypes in [\n+              (-np.inf, lax.max, [np.float32]),\n+              (np.inf, lax.min, [np.float32]),\n+              (0, lax.add, [np.float32]),\n+          ]\n+          for dtype in dtypes\n+      ],\n+      [\n+          dict(\n+              shape=shape,\n+              dims=dims,\n+              strides=strides,\n+              padding=padding,\n+              base_dilation=base_dilation,\n+              window_dilation=window_dilation,\n+          )\n+          for shape, dims, strides, padding, base_dilation, window_dilation in (\n+              itertools.chain(\n+                  itertools.product(\n+                      [(4, 6)],\n+                      [(2, 1), (1, 2)],\n+                      [(1, 1), (2, 1), (1, 2)],\n+                      ['VALID', 'SAME', [(0, 3), (1, 2)]],\n+                      [(1, 1), (2, 3)],\n+                      [(1, 1), (1, 2)],\n+                  ),\n+                  itertools.product(\n+                      [(3, 2, 4, 6)],\n+                      [(1, 1, 2, 1), (2, 1, 2, 1)],\n+                      [(1, 2, 2, 1), (1, 1, 1, 1)],\n+                      ['VALID', 'SAME', [(0, 1), (1, 0), (2, 3), (0, 2)]],\n+                      [(1, 1, 1, 1), (2, 1, 3, 2)],\n+                      [(1, 1, 1, 1), (1, 2, 2, 1)],\n+                  ),\n+              )\n+          )\n+      ],\n+  )\n+  @jtu.skip_on_devices('gpu') # jax.lax.mul has an XLA bug on GPU b/339071103\n+  @jtu.skip_on_devices('tpu') # b/39342488\n+  def testReduceWindowCustomSameAsMonoid(\n+      self,\n+      op,\n+      init_val,\n+      dtype,\n+      shape,\n+      dims,\n+      strides,\n+      padding,\n+      base_dilation,\n+      window_dilation,\n+  ):\n+    rng = jtu.rand_small(self.rng())\n+    init_val = np.asarray(init_val, dtype=dtype)\n+\n+    def fun(op_, operand_):\n+      return lax.reduce_window(\n+          operand_,\n+          init_val,\n+          op_,\n+          dims,\n+          strides,\n+          padding,\n+          base_dilation,\n+          window_dilation,\n+      )\n+\n+    args_maker = lambda: [rng(shape, dtype)]\n+    args = args_maker()\n+    operand = args[0]\n+    rng = np.random.RandomState(0)\n+    tangent = tree_map(partial(jtu.rand_like, rng), operand)\n+\n+    # There are \"special\" paths for \"monoid\" ops that have\n+    # their jvp defined separately, either for legacy reasons\n+    # or for optimization - compare across both and prove\n+    # that their jvp is the same.\n+    # TODO(voz): Look into the \"monoid\" paths and collapse them as necessary.\n+    # Especially when we go to add support for (1) recursive is_jvp (hessians),\n+    # and (2) transpose?\n+    custom_equiv = {\n+        lax.max: _reduce_custom_max,\n+        lax.min: _reduce_custom_min,\n+        lax.add: _reduce_custom_add,\n+    }\n+    custom_op = custom_equiv[op]\n+    custom_primals, custom_tangents = jax.jvp(\n+        partial(fun, custom_op),\n+        primals=(operand,),\n+        tangents=(tangent,),\n+    )\n+    lax_primals, lax_tangents = jax.jvp(\n+        partial(fun, op),\n+        primals=(operand,),\n+        tangents=(tangent,),\n+    )\n+    # tol = 1e-4\n+    # None is sane defaults, but useful to have here for debugging.\n+    tol = None\n+    jtu.check_close(\n+        lax_primals,\n+        custom_primals,\n+        atol=tol,\n+        rtol=tol,\n+        err_msg='Mismatched primal',\n+    )\n+    jtu.check_close(\n+        lax_tangents,\n+        custom_tangents,\n+        atol=tol,\n+        rtol=tol,\n+        err_msg='Mismatched tangents',\n+    )\n+    # Numerical jvp comparison for min and max values\n+    # does not work - the underlying implementation of the test util\n+    # nans on infs.\n+    if init_val.item() in (np.inf, -np.inf):\n+      return\n+    op_bound_fn = partial(fun, op)\n+    jtu.check_jvp(\n+        op_bound_fn,\n+        partial(jax.jvp, op_bound_fn),\n+        (operand,),\n+    )\n+    check_grads(partial(fun, op), [operand], 3, [\"fwd\"], eps=1.)\n+    check_grads(partial(fun, custom_op), [operand], 3, [\"fwd\"], eps=1.)\n+\n+  @jtu.sample_product(\n+      [\n+          dict(\n+              shape=shape,\n+              dims=dims,\n+              strides=strides,\n+              padding=padding,\n+              base_dilation=base_dilation,\n+              window_dilation=window_dilation,\n+          )\n+          for shape, dims, strides, padding, base_dilation, window_dilation in (\n+              itertools.chain(\n+                  itertools.product(\n+                      [(4, 6)],\n+                      [(2, 1), (1, 2)],\n+                      [(1, 1), (2, 1), (1, 2)],\n+                      ['VALID', 'SAME', [(0, 3), (1, 2)]],\n+                      [(1, 1), (2, 3)],\n+                      [(1, 1), (1, 2)],\n+                  ),\n+                  itertools.product(\n+                      [(3, 2, 4, 6)],\n+                      [(1, 1, 2, 1), (2, 1, 2, 1)],\n+                      [(1, 2, 2, 1), (1, 1, 1, 1)],\n+                      ['VALID', 'SAME', [(0, 1), (1, 0), (2, 3), (0, 2)]],\n+                      [(1, 1, 1, 1), (2, 1, 3, 2)],\n+                      [(1, 1, 1, 1), (1, 2, 2, 1)],\n+                  ),\n+              )\n+          )\n+      ],\n+      dtype=[np.float32],\n+  )\n+  @jtu.skip_on_devices('gpu')\n+      hiddens_aug = jnp.vstack((hiddens[0], hiddens))\n+      new_hiddens = hiddens_aug.copy()\n+      diff = new_hiddens[:-1] - hiddens\n+      diff = new_hiddens[:-1] - hiddens\n+      out = jnp.trace(jnp.conj(diff).T @ diff).real\n+      return jnp.array(out, dtype=jnp.complex64)\n+      primals, f_vjp = jax.vjp(\n+          g,\n+          jax.random.normal(jax.random.key(0), (9, 8), dtype=jnp.complex64),\n+      )\n+      out = f_vjp(np.array(1.0 + 0j, 'complex64'))[0]\n+      return carry, carry\n",
            "whole_hunk": "@@ -46,6 +46,7 @@ from jax._src.interpreters import pxla\n from jax._src.internal_test_util import lax_test_util\n from jax._src.lax import lax as lax_internal\n from jax._src.util import NumpyComplexWarning\n+from jax._src.tree_util import tree_map\n \n config.parse_flags_with_absl()\n \n@@ -68,6 +69,22 @@ preferred_type_combinations = [\n   (np.int32, np.float32), (np.int32, np.float64), (np.int64, np.float64)]\n \n \n+def _reduce_custom_add(x, y):\n+  return x + y\n+\n+def _reduce_custom_mul(x, y):\n+  return x * y\n+\n+def _reduce_custom_sub(x, y):\n+  return x - y\n+\n+def _reduce_custom_min(x, y):\n+  return jnp.minimum(x, y)\n+\n+def _reduce_custom_max(x, y):\n+  return jnp.maximum(x, y)\n+\n+\n class LaxTest(jtu.JaxTestCase):\n   \"\"\"Numerical tests for LAX operations.\"\"\"\n \n@@ -1285,7 +1302,7 @@ class LaxTest(jtu.JaxTestCase):\n     numpy_op = lambda x: lax_reference.squeeze(x, dimensions)\n     self._CompileAndCheck(op, args_maker)\n     self._CheckAgainstNumpy(numpy_op, op, args_maker)\n-    check_grads(op, args_maker(), 2, [\"fwd\", \"rev\"], eps=1.)\n+    check_grads(op, args_maker(), 3, [\"fwd\", \"rev\"], eps=1.)\n \n   @jtu.sample_product(\n     input_type=[\"np.array\", \"jnp.array\", \"float\", \"np.float32\"],\n@@ -1799,35 +1816,287 @@ class LaxTest(jtu.JaxTestCase):\n     # we separately test the version that uses a concrete init_val because it\n     # can hit different code paths\n     def fun(operand):\n-      return lax.reduce_window(operand, init_val, op, dims, strides, padding,\n-                               base_dilation, window_dilation)\n+      return lax.reduce_window(\n+          operand,\n+          init_val,\n+          op,\n+          dims,\n+          strides,\n+          padding,\n+          base_dilation,\n+          window_dilation,\n+      )\n \n     args_maker = lambda: [rng(shape, dtype)]\n     self._CompileAndCheck(fun, args_maker)\n \n+  # TODO(voz): I broke these out to their own test for 2 reasons:\n+  # 1. I wanted to show that general ops work, there's a small subset of\n+  # ops, specifically, the ones used in the test above, lax.add, lax.max, and\n+  # lax.min that actually route to a monoid operator that *doesn't* pass JVP\n+  # tests.\n+  # 2. Slightly different parameterization.\n   @jtu.sample_product(\n-    [dict(shape=shape, dims=dims, strides=strides, padding=padding,\n-          base_dilation=base_dilation, window_dilation=window_dilation)\n-      for shape, dims, strides, padding, base_dilation, window_dilation in (\n-        itertools.chain(\n-          itertools.product(\n-            [(4, 6)],\n-            [(2, 1), (1, 2)],\n-            [(1, 1), (2, 1), (1, 2)],\n-            [\"VALID\", \"SAME\", [(0, 3), (1, 2)]],\n-            [(1, 1), (2, 3)],\n-            [(1, 1), (1, 2)]),\n-          itertools.product(\n-            [(3, 2, 4, 6)], [(1, 1, 2, 1), (2, 1, 2, 1)],\n-            [(1, 2, 2, 1), (1, 1, 1, 1)],\n-            [\"VALID\", \"SAME\", [(0, 1), (1, 0), (2, 3), (0, 2)]],\n-            [(1, 1, 1, 1), (2, 1, 3, 2)],\n-            [(1, 1, 1, 1), (1, 2, 2, 1)])))\n-    ],\n-    dtype=[np.float32],\n+      [\n+          dict(init_val=init_val, op=op, dtype=dtype)\n+          for init_val, op, dtypes in [\n+              (1, _reduce_custom_add, [np.float32]),\n+              (0, _reduce_custom_mul, [np.float32]),\n+              (0, _reduce_custom_sub, [np.float32]),\n+          ]\n+          for dtype in dtypes\n+      ],\n+      [\n+          dict(\n+              shape=shape,\n+              dims=dims,\n+              strides=strides,\n+              padding=padding,\n+              base_dilation=base_dilation,\n+              window_dilation=window_dilation,\n+          )\n+          for shape, dims, strides, padding, base_dilation, window_dilation in (\n+              itertools.chain(\n+                  itertools.product(\n+                      [(4, 6)],\n+                      [(2, 1), (1, 2)],\n+                      [(1, 1), (2, 1), (1, 2)],\n+                      ['VALID', 'SAME', [(0, 3), (1, 2)]],\n+                      [(1, 1), (2, 3)],\n+                      [(1, 1), (1, 2)],\n+                  ),\n+                  itertools.product(\n+                      [(3, 2, 4, 6)],\n+                      [(1, 1, 2, 1), (2, 1, 2, 1)],\n+                      [(1, 2, 2, 1), (1, 1, 1, 1)],\n+                      ['VALID', 'SAME', [(0, 1), (1, 0), (2, 3), (0, 2)]],\n+                      [(1, 1, 1, 1), (2, 1, 3, 2)],\n+                      [(1, 1, 1, 1), (1, 2, 2, 1)],\n+                  ),\n+              )\n+          )\n+      ],\n   )\n+  @jtu.skip_on_devices('gpu') # jax.lax.mul has an XLA bug on GPU b/339071103\n+  @jtu.skip_on_devices('tpu') # b/39342488\n+  def testReduceWindowGeneralJVP(\n+      self,\n+      op,\n+      init_val,\n+      dtype,\n+      shape,\n+      dims,\n+      strides,\n+      padding,\n+      base_dilation,\n+      window_dilation,\n+  ):\n+    rng = jtu.rand_small(self.rng())\n+    init_val = np.asarray(init_val, dtype=dtype)\n+\n+    def fun(operand, init_val):\n+      return lax.reduce_window(\n+          operand,\n+          init_val,\n+          op,\n+          dims,\n+          strides,\n+          padding,\n+          base_dilation,\n+          window_dilation,\n+      )\n+\n+    args_maker = lambda: [rng(shape, dtype), init_val]\n+    self._CompileAndCheck(fun, args_maker)\n+    args = args_maker()\n+    init_val = args[1]\n+\n+    # we separately test the version that uses a concrete init_val because it\n+    # can hit different code paths\n+    def fun2(operand):\n+      return lax.reduce_window(\n+          operand,\n+          init_val,\n+          op,\n+          dims,\n+          strides,\n+          padding,\n+          base_dilation,\n+          window_dilation,\n+      )\n+\n+    args_maker = lambda: [rng(shape, dtype)]\n+    self._CompileAndCheck(fun2, args_maker)\n+\n+    operand = args_maker()[0]\n+    jtu.check_jvp(fun2, partial(jax.jvp, fun2), (operand,))\n+    check_grads(fun2, (operand,), 3, [\"fwd\"], eps=1.)\n+\n+  @jtu.sample_product(\n+      [\n+          dict(init_val=init_val, op=op, dtype=dtype)\n+          for init_val, op, dtypes in [\n+              (-np.inf, lax.max, [np.float32]),\n+              (np.inf, lax.min, [np.float32]),\n+              (0, lax.add, [np.float32]),\n+          ]\n+          for dtype in dtypes\n+      ],\n+      [\n+          dict(\n+              shape=shape,\n+              dims=dims,\n+              strides=strides,\n+              padding=padding,\n+              base_dilation=base_dilation,\n+              window_dilation=window_dilation,\n+          )\n+          for shape, dims, strides, padding, base_dilation, window_dilation in (\n+              itertools.chain(\n+                  itertools.product(\n+                      [(4, 6)],\n+                      [(2, 1), (1, 2)],\n+                      [(1, 1), (2, 1), (1, 2)],\n+                      ['VALID', 'SAME', [(0, 3), (1, 2)]],\n+                      [(1, 1), (2, 3)],\n+                      [(1, 1), (1, 2)],\n+                  ),\n+                  itertools.product(\n+                      [(3, 2, 4, 6)],\n+                      [(1, 1, 2, 1), (2, 1, 2, 1)],\n+                      [(1, 2, 2, 1), (1, 1, 1, 1)],\n+                      ['VALID', 'SAME', [(0, 1), (1, 0), (2, 3), (0, 2)]],\n+                      [(1, 1, 1, 1), (2, 1, 3, 2)],\n+                      [(1, 1, 1, 1), (1, 2, 2, 1)],\n+                  ),\n+              )\n+          )\n+      ],\n+  )\n+  @jtu.skip_on_devices('gpu') # jax.lax.mul has an XLA bug on GPU b/339071103\n+  @jtu.skip_on_devices('tpu') # b/39342488\n+  def testReduceWindowCustomSameAsMonoid(\n+      self,\n+      op,\n+      init_val,\n+      dtype,\n+      shape,\n+      dims,\n+      strides,\n+      padding,\n+      base_dilation,\n+      window_dilation,\n+  ):\n+    rng = jtu.rand_small(self.rng())\n+    init_val = np.asarray(init_val, dtype=dtype)\n+\n+    def fun(op_, operand_):\n+      return lax.reduce_window(\n+          operand_,\n+          init_val,\n+          op_,\n+          dims,\n+          strides,\n+          padding,\n+          base_dilation,\n+          window_dilation,\n+      )\n+\n+    args_maker = lambda: [rng(shape, dtype)]\n+    args = args_maker()\n+    operand = args[0]\n+    rng = np.random.RandomState(0)\n+    tangent = tree_map(partial(jtu.rand_like, rng), operand)\n+\n+    # There are \"special\" paths for \"monoid\" ops that have\n+    # their jvp defined separately, either for legacy reasons\n+    # or for optimization - compare across both and prove\n+    # that their jvp is the same.\n+    # TODO(voz): Look into the \"monoid\" paths and collapse them as necessary.\n+    # Especially when we go to add support for (1) recursive is_jvp (hessians),\n+    # and (2) transpose?\n+    custom_equiv = {\n+        lax.max: _reduce_custom_max,\n+        lax.min: _reduce_custom_min,\n+        lax.add: _reduce_custom_add,\n+    }\n+    custom_op = custom_equiv[op]\n+    custom_primals, custom_tangents = jax.jvp(\n+        partial(fun, custom_op),\n+        primals=(operand,),\n+        tangents=(tangent,),\n+    )\n+    lax_primals, lax_tangents = jax.jvp(\n+        partial(fun, op),\n+        primals=(operand,),\n+        tangents=(tangent,),\n+    )\n+    # tol = 1e-4\n+    # None is sane defaults, but useful to have here for debugging.\n+    tol = None\n+    jtu.check_close(\n+        lax_primals,\n+        custom_primals,\n+        atol=tol,\n+        rtol=tol,\n+        err_msg='Mismatched primal',\n+    )\n+    jtu.check_close(\n+        lax_tangents,\n+        custom_tangents,\n+        atol=tol,\n+        rtol=tol,\n+        err_msg='Mismatched tangents',\n+    )\n+    # Numerical jvp comparison for min and max values\n+    # does not work - the underlying implementation of the test util\n+    # nans on infs.\n+    if init_val.item() in (np.inf, -np.inf):\n+      return\n+    op_bound_fn = partial(fun, op)\n+    jtu.check_jvp(\n+        op_bound_fn,\n+        partial(jax.jvp, op_bound_fn),\n+        (operand,),\n+    )\n+    check_grads(partial(fun, op), [operand], 3, [\"fwd\"], eps=1.)\n+    check_grads(partial(fun, custom_op), [operand], 3, [\"fwd\"], eps=1.)\n+\n   # TODO(b/183233858): variadic reduce-window is not implemented on XLA:GPU\n-  @jtu.skip_on_devices(\"gpu\")\n+  @jtu.sample_product(\n+      [\n+          dict(\n+              shape=shape,\n+              dims=dims,\n+              strides=strides,\n+              padding=padding,\n+              base_dilation=base_dilation,\n+              window_dilation=window_dilation,\n+          )\n+          for shape, dims, strides, padding, base_dilation, window_dilation in (\n+              itertools.chain(\n+                  itertools.product(\n+                      [(4, 6)],\n+                      [(2, 1), (1, 2)],\n+                      [(1, 1), (2, 1), (1, 2)],\n+                      ['VALID', 'SAME', [(0, 3), (1, 2)]],\n+                      [(1, 1), (2, 3)],\n+                      [(1, 1), (1, 2)],\n+                  ),\n+                  itertools.product(\n+                      [(3, 2, 4, 6)],\n+                      [(1, 1, 2, 1), (2, 1, 2, 1)],\n+                      [(1, 2, 2, 1), (1, 1, 1, 1)],\n+                      ['VALID', 'SAME', [(0, 1), (1, 0), (2, 3), (0, 2)]],\n+                      [(1, 1, 1, 1), (2, 1, 3, 2)],\n+                      [(1, 1, 1, 1), (1, 2, 2, 1)],\n+                  ),\n+              )\n+          )\n+      ],\n+      dtype=[np.float32],\n+  )\n+  @jtu.skip_on_devices('gpu')\n   def testReduceWindowVariadic(self, dtype, shape, dims, strides, padding,\n                                base_dilation, window_dilation):\n     if (jtu.test_device_matches([\"tpu\"]) and\n@@ -2726,21 +2995,20 @@ class LaxTest(jtu.JaxTestCase):\n   def test_constant_folding_complex_to_real_scan_regression(self):\n     # regression test for github.com/google/jax/issues/19059\n     def g(hiddens):\n-        hiddens_aug = jnp.vstack((hiddens[0], hiddens))\n-        new_hiddens = hiddens_aug.copy()\n-        diff = new_hiddens[:-1] - hiddens\n-        diff = new_hiddens[:-1] - hiddens\n-        out = jnp.trace(jnp.conj(diff).T @ diff).real\n-        return jnp.array(out, dtype=jnp.complex64)\n-\n+      hiddens_aug = jnp.vstack((hiddens[0], hiddens))\n+      new_hiddens = hiddens_aug.copy()\n+      diff = new_hiddens[:-1] - hiddens\n+      diff = new_hiddens[:-1] - hiddens\n+      out = jnp.trace(jnp.conj(diff).T @ diff).real\n+      return jnp.array(out, dtype=jnp.complex64)\n \n     def _step(carry, arg):\n-        primals, f_vjp = jax.vjp(\n-            g,\n-            jax.random.normal(jax.random.key(0), (9, 8), dtype=jnp.complex64),\n-        )\n-        out = f_vjp(np.array(1.0 + 0j, 'complex64'))[0]\n-        return carry, carry\n+      primals, f_vjp = jax.vjp(\n+          g,\n+          jax.random.normal(jax.random.key(0), (9, 8), dtype=jnp.complex64),\n+      )\n+      out = f_vjp(np.array(1.0 + 0j, 'complex64'))[0]\n+      return carry, carry\n \n     a, b = jax.lax.scan(_step, 0, jnp.arange(4, dtype=jnp.complex64))\n "
        }
    ]
},
{
    "Id": 48,
    "commit_link": "https://github.com/google/jax/commit/7a87010f843adf44bbfaf1739187c61fa26672ea",
    "date": "2024-05-04T01:31:15+00:00",
    "message": "[shard_map] better fix for spmd_axis_name issues with shmap residuals\n\nThe fix in #21032 was not correct because it assumed that the set of all mesh\naxis names appearing in in_specs was an upper bound on the set of mesh axes\nover which residuals could be device-varying. But collectives can introduce\ndevice variance! So it's not an upper bound.\n\nWe track device variance when check_rep=True, but often people set\ncheck_rep=False (e.g. when using pallas_call in a shard_map). So relying on our\ndevice variance tracking would be limiting. That may be a decent long term\nsolution, if we can make it easy to annotate pallas_calls with device variance\ninformation. But it's not a great short term one to unblock things.\n\nSo instead I temporrarily went with context sensitivity: instead of making\nresiduals sharded over all mesh.axis_names (as we did before these patches), we\nmake them sharded over all mesh axis names _excluding_ any spmd_axis_names in\nour dynamic context (by looking at the traces in our trace stack). It's illegal\nto mention any spmd_axis_names in collectives (indeed anywhere in the body of\nthe function being vmapped), but I don't think we check it.\n\nTODO(mattjj): add more testing (maybe in follow-ups)",
    "changes": [
        {
            "name": "shard_map.py",
            "path": "jax/experimental/shard_map.py",
            "patches": [
                {
                    "old_start": 1300,
                    "old_length": 7,
                    "new_start": 1300,
                    "new_length": 7,
                    "hunk": "@@ -1300,7 +1300,7 @@ def _shard_map_partial_eval(trace, shard_map_p, f, tracers, mesh, in_names,\n   in_pvals = [t.pval for t in tracers]\n   in_knowns, in_avals, in_consts = pe.partition_pvals(in_pvals)\n   unk_in_names, known_in_names = pe.partition_list(in_knowns, in_names)\n-  used_names = {n for names in known_in_names for ns in names.values() for n in ns}\n+  all_names = _all_mesh_names(mesh)\n   in_avals_sharded = map(partial(_shard_aval, mesh), unk_in_names, in_avals)\n   f = pe.trace_to_subjaxpr_nounits_fwd2(f, trace.main, False)\n   f = _promote_scalar_residuals(f)\n"
                },
                {
                    "old_start": 1312,
                    "old_length": 7,
                    "new_start": 1312,
                    "new_length": 7,
                    "hunk": "@@ -1312,7 +1312,7 @@ def _shard_map_partial_eval(trace, shard_map_p, f, tracers, mesh, in_names,\n     in_fwd, out_fwd, out_knowns, _, jaxpr, _ = aux()\n     _, out_known_names = pe.partition_list(out_knowns, out_names_thunk())\n     num_res = sum(f1 is None and f2 is None for f1, f2 in zip(in_fwd, out_fwd))\n-    return (*out_known_names, *({0: (*used_names,)},) * num_res)\n+    return (*out_known_names, *({0: (*all_names,)},) * num_res)\n \n   known_params = dict(mesh=mesh, in_names=(*known_in_names,),\n                       out_names_thunk=known_out_names, check_rep=check_rep,\n"
                },
                {
                    "old_start": 1327,
                    "old_length": 7,
                    "new_start": 1327,
                    "new_length": 7,
                    "hunk": "@@ -1327,7 +1327,7 @@ def _shard_map_partial_eval(trace, shard_map_p, f, tracers, mesh, in_names,\n   res = subs_list2(in_fwd, out_fwd, in_consts, out_consts, non_fwd_res)\n   res_names = [known_in_names[f1] if f1 is not None else\n                known_out_names_[f2] if f2 is not None else\n-               {0: (*used_names,)} for f1, f2 in zip(in_fwd, out_fwd)]\n+               {0: (*all_names,)} for f1, f2 in zip(in_fwd, out_fwd)]\n   unk_in_names = (*res_names,) + ({},) * len(env) + (*unk_in_names,)\n   const_tracers = map(trace.new_instantiated_const, res)\n   env_tracers = map(trace.full_raise, env)\n"
                },
                {
                    "old_start": 1349,
                    "old_length": 7,
                    "new_start": 1349,
                    "new_length": 7,
                    "hunk": "@@ -1349,7 +1349,7 @@ pe.JaxprTrace.process_shard_map = _shard_map_partial_eval\n def _shard_map_partial_eval_post_process(\n     trace, tracers, mesh, in_names, out_names_thunk, check_rep, rewrite, auto):\n   del check_rep\n-  used_names = {n for names in in_names for ns in names.values() for n in ns}\n+  all_names = _all_mesh_names(mesh)\n   unk_tracers = [t for t in tracers if not t.is_known()]\n   jaxpr, res, env = pe.tracers_to_jaxpr([], unk_tracers)\n   # TODO(mattjj): output forwarding optimization\n"
                },
                {
                    "old_start": 1370,
                    "old_length": 7,
                    "new_start": 1370,
                    "new_length": 7,
                    "hunk": "@@ -1370,7 +1370,7 @@ def _shard_map_partial_eval_post_process(\n     const_tracers = map(trace.new_instantiated_const, res_)\n     env_tracers = map(trace.full_raise, env)\n \n-    staged_in_names = ({0: (*used_names,)},) * len(res_) + ({},) * len(env)\n+    staged_in_names = ({0: (*all_names,)},) * len(res_) + ({},) * len(env)\n     staged_params = dict(jaxpr=jaxpr_, mesh=mesh, in_names=staged_in_names,\n                          out_names=(*out_names_unknown,), check_rep=False,\n                          rewrite=rewrite, auto=auto)\n"
                },
                {
                    "old_start": 1389,
                    "old_length": 7,
                    "new_start": 1389,
                    "new_length": 7,
                    "hunk": "@@ -1389,7 +1389,7 @@ def _shard_map_partial_eval_post_process(\n   def out_names_transform(out_names):\n     nonlocal out_names_unknown\n     out_names_unknown, out_names_known = partition_list(out_knowns, out_names)\n-    return (*out_names_known,) + ({0: (*used_names,)},) * len(res)\n+    return (*out_names_known,) + ({0: (*all_names,)},) * len(res)\n   out_names_unknown: list | None = None\n \n   return out, (todo, out_names_transform)\n"
                },
                {
                    "old_start": 1499,
                    "old_length": 10,
                    "new_start": 1499,
                    "new_length": 10,
                    "hunk": "@@ -1499,10 +1499,10 @@ def _partial_eval_jaxpr_custom_rule(\n   _, ins_staged = partition_list(inst_in, eqn.invars)\n   _, out_binders_staged = partition_list(inst_out, eqn.outvars)\n   newvar = core.gensym()\n-  params_known, params_staged = _pe_custom_params(\n+  params_known, params_staged, all_names = _pe_custom_params(\n       unks_in, inst_in, map(op.not_, unks_out), inst_out, in_fwd, out_fwd, which,\n       dict(eqn.params, jaxpr=jaxpr_known), dict(eqn.params, jaxpr=jaxpr_staged))\n-  residuals = [newvar(_unshard_aval(mesh, {0: (*mesh.axis_names,)}, var.aval))\n+  residuals = [newvar(_unshard_aval(mesh, {0: (*all_names,)}, var.aval))\n                for var, w in zip(jaxpr_staged.invars[:num_res], which) if w]\n   eqn_known = pe.new_jaxpr_eqn(ins_known, [*out_binders_known, *residuals],\n                                eqn.primitive, params_known, jaxpr_known.effects,\n"
                },
                {
                    "old_start": 1551,
                    "old_length": 9,
                    "new_start": 1551,
                    "new_length": 10,
                    "hunk": "@@ -1551,9 +1551,10 @@ def _pe_custom_params(unks_in, inst_in, kept_outs_known, kept_outs_staged,\n                       in_fwd, out_fwd, which, params_known, params_staged):\n   # prune inputs to jaxpr_known according to unks_in\n   mesh = params_known['mesh']\n+  all_names = _all_mesh_names(mesh)\n   in_names_known, _ = partition_list(unks_in, params_known['in_names'])\n   _, out_names_known = partition_list(kept_outs_known, params_known['out_names'])\n-  out_names_known = out_names_known + [{0: (*mesh.axis_names,)}] * sum(which)\n+  out_names_known = out_names_known + [{0: (*all_names,)}] * sum(which)\n   new_params_known = dict(params_known, in_names=tuple(in_names_known),\n                           out_names=tuple(out_names_known))\n \n"
                },
                {
                    "old_start": 1561,
                    "old_length": 12,
                    "new_start": 1562,
                    "new_length": 22,
                    "hunk": "@@ -1561,12 +1562,22 @@ def _pe_custom_params(unks_in, inst_in, kept_outs_known, kept_outs_staged,\n   _, in_names_staged = partition_list(inst_in, params_staged['in_names'])\n   res_names = [in_names_known[f1] if f1 is not None else\n                out_names_known[f2] if f2 is not None else\n-               {0: (*mesh.axis_names,)} for f1, f2 in zip(in_fwd, out_fwd)]\n+               {0: (*all_names,)} for f1, f2 in zip(in_fwd, out_fwd)]\n   in_names_staged = res_names + in_names_staged\n   _, out_names_staged = partition_list(kept_outs_staged, params_staged['out_names'])\n   new_params_staged = dict(params_staged, in_names=tuple(in_names_staged),\n                            out_names=tuple(out_names_staged), check_rep=False)\n-  return new_params_known, new_params_staged\n+  return new_params_known, new_params_staged, all_names\n+\n+\n+# TODO(mattjj): remove this mechanism when we revise mesh scopes\n+def _all_mesh_names(mesh: Mesh) -> set[AxisName]:\n+  stack = core.thread_local_state.trace_state.trace_stack.stack\n+  names = {n for frame in stack\n+           if (ns := frame.payload.get('spmd_axis_name', ())) is not None\n+           for n in ns}\n+  return set(mesh.axis_names) - names\n+\n \n # DCE\n \n"
                }
            ],
            "whole_deleted": "-  used_names = {n for names in known_in_names for ns in names.values() for n in ns}\n-    return (*out_known_names, *({0: (*used_names,)},) * num_res)\n-               {0: (*used_names,)} for f1, f2 in zip(in_fwd, out_fwd)]\n-  used_names = {n for names in in_names for ns in names.values() for n in ns}\n-    staged_in_names = ({0: (*used_names,)},) * len(res_) + ({},) * len(env)\n-    return (*out_names_known,) + ({0: (*used_names,)},) * len(res)\n-  params_known, params_staged = _pe_custom_params(\n-  residuals = [newvar(_unshard_aval(mesh, {0: (*mesh.axis_names,)}, var.aval))\n-  out_names_known = out_names_known + [{0: (*mesh.axis_names,)}] * sum(which)\n-               {0: (*mesh.axis_names,)} for f1, f2 in zip(in_fwd, out_fwd)]\n-  return new_params_known, new_params_staged\n",
            "whole_added": "+  all_names = _all_mesh_names(mesh)\n+    return (*out_known_names, *({0: (*all_names,)},) * num_res)\n+               {0: (*all_names,)} for f1, f2 in zip(in_fwd, out_fwd)]\n+  all_names = _all_mesh_names(mesh)\n+    staged_in_names = ({0: (*all_names,)},) * len(res_) + ({},) * len(env)\n+    return (*out_names_known,) + ({0: (*all_names,)},) * len(res)\n+  params_known, params_staged, all_names = _pe_custom_params(\n+  residuals = [newvar(_unshard_aval(mesh, {0: (*all_names,)}, var.aval))\n+  all_names = _all_mesh_names(mesh)\n+  out_names_known = out_names_known + [{0: (*all_names,)}] * sum(which)\n+               {0: (*all_names,)} for f1, f2 in zip(in_fwd, out_fwd)]\n+  return new_params_known, new_params_staged, all_names\n+\n+\n+# TODO(mattjj): remove this mechanism when we revise mesh scopes\n+def _all_mesh_names(mesh: Mesh) -> set[AxisName]:\n+  stack = core.thread_local_state.trace_state.trace_stack.stack\n+  names = {n for frame in stack\n+           if (ns := frame.payload.get('spmd_axis_name', ())) is not None\n+           for n in ns}\n+  return set(mesh.axis_names) - names\n+\n",
            "whole_hunk": "@@ -1300,7 +1300,7 @@ def _shard_map_partial_eval(trace, shard_map_p, f, tracers, mesh, in_names,\n   in_pvals = [t.pval for t in tracers]\n   in_knowns, in_avals, in_consts = pe.partition_pvals(in_pvals)\n   unk_in_names, known_in_names = pe.partition_list(in_knowns, in_names)\n-  used_names = {n for names in known_in_names for ns in names.values() for n in ns}\n+  all_names = _all_mesh_names(mesh)\n   in_avals_sharded = map(partial(_shard_aval, mesh), unk_in_names, in_avals)\n   f = pe.trace_to_subjaxpr_nounits_fwd2(f, trace.main, False)\n   f = _promote_scalar_residuals(f)\n@@ -1312,7 +1312,7 @@ def _shard_map_partial_eval(trace, shard_map_p, f, tracers, mesh, in_names,\n     in_fwd, out_fwd, out_knowns, _, jaxpr, _ = aux()\n     _, out_known_names = pe.partition_list(out_knowns, out_names_thunk())\n     num_res = sum(f1 is None and f2 is None for f1, f2 in zip(in_fwd, out_fwd))\n-    return (*out_known_names, *({0: (*used_names,)},) * num_res)\n+    return (*out_known_names, *({0: (*all_names,)},) * num_res)\n \n   known_params = dict(mesh=mesh, in_names=(*known_in_names,),\n                       out_names_thunk=known_out_names, check_rep=check_rep,\n@@ -1327,7 +1327,7 @@ def _shard_map_partial_eval(trace, shard_map_p, f, tracers, mesh, in_names,\n   res = subs_list2(in_fwd, out_fwd, in_consts, out_consts, non_fwd_res)\n   res_names = [known_in_names[f1] if f1 is not None else\n                known_out_names_[f2] if f2 is not None else\n-               {0: (*used_names,)} for f1, f2 in zip(in_fwd, out_fwd)]\n+               {0: (*all_names,)} for f1, f2 in zip(in_fwd, out_fwd)]\n   unk_in_names = (*res_names,) + ({},) * len(env) + (*unk_in_names,)\n   const_tracers = map(trace.new_instantiated_const, res)\n   env_tracers = map(trace.full_raise, env)\n@@ -1349,7 +1349,7 @@ pe.JaxprTrace.process_shard_map = _shard_map_partial_eval\n def _shard_map_partial_eval_post_process(\n     trace, tracers, mesh, in_names, out_names_thunk, check_rep, rewrite, auto):\n   del check_rep\n-  used_names = {n for names in in_names for ns in names.values() for n in ns}\n+  all_names = _all_mesh_names(mesh)\n   unk_tracers = [t for t in tracers if not t.is_known()]\n   jaxpr, res, env = pe.tracers_to_jaxpr([], unk_tracers)\n   # TODO(mattjj): output forwarding optimization\n@@ -1370,7 +1370,7 @@ def _shard_map_partial_eval_post_process(\n     const_tracers = map(trace.new_instantiated_const, res_)\n     env_tracers = map(trace.full_raise, env)\n \n-    staged_in_names = ({0: (*used_names,)},) * len(res_) + ({},) * len(env)\n+    staged_in_names = ({0: (*all_names,)},) * len(res_) + ({},) * len(env)\n     staged_params = dict(jaxpr=jaxpr_, mesh=mesh, in_names=staged_in_names,\n                          out_names=(*out_names_unknown,), check_rep=False,\n                          rewrite=rewrite, auto=auto)\n@@ -1389,7 +1389,7 @@ def _shard_map_partial_eval_post_process(\n   def out_names_transform(out_names):\n     nonlocal out_names_unknown\n     out_names_unknown, out_names_known = partition_list(out_knowns, out_names)\n-    return (*out_names_known,) + ({0: (*used_names,)},) * len(res)\n+    return (*out_names_known,) + ({0: (*all_names,)},) * len(res)\n   out_names_unknown: list | None = None\n \n   return out, (todo, out_names_transform)\n@@ -1499,10 +1499,10 @@ def _partial_eval_jaxpr_custom_rule(\n   _, ins_staged = partition_list(inst_in, eqn.invars)\n   _, out_binders_staged = partition_list(inst_out, eqn.outvars)\n   newvar = core.gensym()\n-  params_known, params_staged = _pe_custom_params(\n+  params_known, params_staged, all_names = _pe_custom_params(\n       unks_in, inst_in, map(op.not_, unks_out), inst_out, in_fwd, out_fwd, which,\n       dict(eqn.params, jaxpr=jaxpr_known), dict(eqn.params, jaxpr=jaxpr_staged))\n-  residuals = [newvar(_unshard_aval(mesh, {0: (*mesh.axis_names,)}, var.aval))\n+  residuals = [newvar(_unshard_aval(mesh, {0: (*all_names,)}, var.aval))\n                for var, w in zip(jaxpr_staged.invars[:num_res], which) if w]\n   eqn_known = pe.new_jaxpr_eqn(ins_known, [*out_binders_known, *residuals],\n                                eqn.primitive, params_known, jaxpr_known.effects,\n@@ -1551,9 +1551,10 @@ def _pe_custom_params(unks_in, inst_in, kept_outs_known, kept_outs_staged,\n                       in_fwd, out_fwd, which, params_known, params_staged):\n   # prune inputs to jaxpr_known according to unks_in\n   mesh = params_known['mesh']\n+  all_names = _all_mesh_names(mesh)\n   in_names_known, _ = partition_list(unks_in, params_known['in_names'])\n   _, out_names_known = partition_list(kept_outs_known, params_known['out_names'])\n-  out_names_known = out_names_known + [{0: (*mesh.axis_names,)}] * sum(which)\n+  out_names_known = out_names_known + [{0: (*all_names,)}] * sum(which)\n   new_params_known = dict(params_known, in_names=tuple(in_names_known),\n                           out_names=tuple(out_names_known))\n \n@@ -1561,12 +1562,22 @@ def _pe_custom_params(unks_in, inst_in, kept_outs_known, kept_outs_staged,\n   _, in_names_staged = partition_list(inst_in, params_staged['in_names'])\n   res_names = [in_names_known[f1] if f1 is not None else\n                out_names_known[f2] if f2 is not None else\n-               {0: (*mesh.axis_names,)} for f1, f2 in zip(in_fwd, out_fwd)]\n+               {0: (*all_names,)} for f1, f2 in zip(in_fwd, out_fwd)]\n   in_names_staged = res_names + in_names_staged\n   _, out_names_staged = partition_list(kept_outs_staged, params_staged['out_names'])\n   new_params_staged = dict(params_staged, in_names=tuple(in_names_staged),\n                            out_names=tuple(out_names_staged), check_rep=False)\n-  return new_params_known, new_params_staged\n+  return new_params_known, new_params_staged, all_names\n+\n+\n+# TODO(mattjj): remove this mechanism when we revise mesh scopes\n+def _all_mesh_names(mesh: Mesh) -> set[AxisName]:\n+  stack = core.thread_local_state.trace_state.trace_stack.stack\n+  names = {n for frame in stack\n+           if (ns := frame.payload.get('spmd_axis_name', ())) is not None\n+           for n in ns}\n+  return set(mesh.axis_names) - names\n+\n \n # DCE\n \n"
        },
        {
            "name": "shard_map_test.py",
            "path": "tests/shard_map_test.py",
            "patches": [
                {
                    "old_start": 1716,
                    "old_length": 6,
                    "new_start": 1716,
                    "new_length": 41,
                    "hunk": "@@ -1716,6 +1716,41 @@ class ShardMapTest(jtu.JaxTestCase):\n     with self.assertRaisesRegex(ValueError, \"in_specs refers to 'j'\"):\n       f(v)\n \n+  def test_vmap_grad_shmap_spmd_axis_name_residuals(self):\n+    # https://github.com/google/jax/pull/21032\n+    mesh = jtu.create_global_mesh((4, 2), ('i', 'j'))\n+\n+    @partial(\n+      shard_map,\n+      mesh=mesh,\n+      in_specs=P('j'),\n+      out_specs=P('j'),\n+      )\n+    def f(x):\n+      return jnp.sin(x)\n+\n+    xs = jnp.arange(4 * 16.).reshape(4, 16)\n+\n+    jax.vmap(jax.grad(lambda x: f(x).sum()), spmd_axis_name='i')(xs)  # don't crash\n+\n+  def test_vmap_grad_remat_shmap_spmd_axis_name_residuals(self):\n+    # https://github.com/google/jax/pull/21056\n+    mesh = jtu.create_global_mesh((4, 2), ('i', 'j'))\n+\n+    @partial(jax.remat, policy=lambda *_, **__: True)\n+    @partial(\n+      shard_map,\n+      mesh=mesh,\n+      in_specs=P('j'),\n+      out_specs=P('j'),\n+      )\n+    def f(x):\n+      return jnp.sin(x)\n+\n+    xs = jnp.arange(4 * 16.).reshape(4, 16)\n+\n+    jax.vmap(jax.grad(lambda x: f(x).sum()), spmd_axis_name='i')(xs)  # don't crash\n+\n \n class FunSpec(NamedTuple):\n   name: str"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  def test_vmap_grad_shmap_spmd_axis_name_residuals(self):\n+    # https://github.com/google/jax/pull/21032\n+    mesh = jtu.create_global_mesh((4, 2), ('i', 'j'))\n+\n+    @partial(\n+      shard_map,\n+      mesh=mesh,\n+      in_specs=P('j'),\n+      out_specs=P('j'),\n+      )\n+    def f(x):\n+      return jnp.sin(x)\n+\n+    xs = jnp.arange(4 * 16.).reshape(4, 16)\n+\n+    jax.vmap(jax.grad(lambda x: f(x).sum()), spmd_axis_name='i')(xs)  # don't crash\n+\n+  def test_vmap_grad_remat_shmap_spmd_axis_name_residuals(self):\n+    # https://github.com/google/jax/pull/21056\n+    mesh = jtu.create_global_mesh((4, 2), ('i', 'j'))\n+\n+    @partial(jax.remat, policy=lambda *_, **__: True)\n+    @partial(\n+      shard_map,\n+      mesh=mesh,\n+      in_specs=P('j'),\n+      out_specs=P('j'),\n+      )\n+    def f(x):\n+      return jnp.sin(x)\n+\n+    xs = jnp.arange(4 * 16.).reshape(4, 16)\n+\n+    jax.vmap(jax.grad(lambda x: f(x).sum()), spmd_axis_name='i')(xs)  # don't crash\n+\n",
            "whole_hunk": "@@ -1716,6 +1716,41 @@ class ShardMapTest(jtu.JaxTestCase):\n     with self.assertRaisesRegex(ValueError, \"in_specs refers to 'j'\"):\n       f(v)\n \n+  def test_vmap_grad_shmap_spmd_axis_name_residuals(self):\n+    # https://github.com/google/jax/pull/21032\n+    mesh = jtu.create_global_mesh((4, 2), ('i', 'j'))\n+\n+    @partial(\n+      shard_map,\n+      mesh=mesh,\n+      in_specs=P('j'),\n+      out_specs=P('j'),\n+      )\n+    def f(x):\n+      return jnp.sin(x)\n+\n+    xs = jnp.arange(4 * 16.).reshape(4, 16)\n+\n+    jax.vmap(jax.grad(lambda x: f(x).sum()), spmd_axis_name='i')(xs)  # don't crash\n+\n+  def test_vmap_grad_remat_shmap_spmd_axis_name_residuals(self):\n+    # https://github.com/google/jax/pull/21056\n+    mesh = jtu.create_global_mesh((4, 2), ('i', 'j'))\n+\n+    @partial(jax.remat, policy=lambda *_, **__: True)\n+    @partial(\n+      shard_map,\n+      mesh=mesh,\n+      in_specs=P('j'),\n+      out_specs=P('j'),\n+      )\n+    def f(x):\n+      return jnp.sin(x)\n+\n+    xs = jnp.arange(4 * 16.).reshape(4, 16)\n+\n+    jax.vmap(jax.grad(lambda x: f(x).sum()), spmd_axis_name='i')(xs)  # don't crash\n+\n \n class FunSpec(NamedTuple):\n   name: str"
        }
    ]
},
{
    "Id": 49,
    "commit_link": "https://github.com/google/jax/commit/51fc4f85ad03149a7250703d4b9e72b2fcb27d3b",
    "date": "2024-05-02T08:12:05-07:00",
    "message": "Ported LuPivotsToPermutation to the typed XLA FFI\n\nThe typed FFI\n\n* allows passing custom call attributes directly to backend_config= instead\n  of serializing them into a C++ struct.\n* It also handles validation and deserialization of custom call operands.\n\nPiperOrigin-RevId: 630067005",
    "changes": [
        {
            "name": "BUILD",
            "path": "jaxlib/cuda/BUILD",
            "patches": [
                {
                    "old_start": 281,
                    "old_length": 8,
                    "new_start": 281,
                    "new_length": 8,
                    "hunk": "@@ -281,8 +281,8 @@ cc_library(\n         \":cuda_gpu_kernel_helpers\",\n         \":cuda_lu_pivot_kernels_impl\",\n         \":cuda_vendor\",\n-        \"//jaxlib:kernel_helpers\",\n-        \"@xla//xla/service:custom_call_status\",\n+        \"@xla//xla/ffi/api:c_api\",\n+        \"@xla//xla/ffi/api:ffi\",\n         \"@local_config_cuda//cuda:cuda_headers\",\n     ],\n )\n"
                },
                {
                    "old_start": 296,
                    "old_length": 7,
                    "new_start": 296,
                    "new_length": 7,
                    "hunk": "@@ -296,7 +296,7 @@ cuda_library(\n     deps = [\n         \":cuda_gpu_kernel_helpers\",\n         \":cuda_vendor\",\n-        \"//jaxlib:kernel_helpers\",\n+        \"@xla//xla/ffi/api:c_api\",\n         \"@xla//xla/service:custom_call_status\",\n         \"@local_config_cuda//cuda:cuda_headers\",\n     ],\n"
                }
            ],
            "whole_deleted": "-        \"//jaxlib:kernel_helpers\",\n-        \"@xla//xla/service:custom_call_status\",\n-        \"//jaxlib:kernel_helpers\",\n",
            "whole_added": "+        \"@xla//xla/ffi/api:c_api\",\n+        \"@xla//xla/ffi/api:ffi\",\n+        \"@xla//xla/ffi/api:c_api\",\n",
            "whole_hunk": "@@ -281,8 +281,8 @@ cc_library(\n         \":cuda_gpu_kernel_helpers\",\n         \":cuda_lu_pivot_kernels_impl\",\n         \":cuda_vendor\",\n-        \"//jaxlib:kernel_helpers\",\n-        \"@xla//xla/service:custom_call_status\",\n+        \"@xla//xla/ffi/api:c_api\",\n+        \"@xla//xla/ffi/api:ffi\",\n         \"@local_config_cuda//cuda:cuda_headers\",\n     ],\n )\n@@ -296,7 +296,7 @@ cuda_library(\n     deps = [\n         \":cuda_gpu_kernel_helpers\",\n         \":cuda_vendor\",\n-        \"//jaxlib:kernel_helpers\",\n+        \"@xla//xla/ffi/api:c_api\",\n         \"@xla//xla/service:custom_call_status\",\n         \"@local_config_cuda//cuda:cuda_headers\",\n     ],\n"
        },
        {
            "name": "linalg.cc",
            "path": "jaxlib/gpu/linalg.cc",
            "patches": [
                {
                    "old_start": 14,
                    "old_length": 9,
                    "new_start": 14,
                    "new_length": 8,
                    "hunk": "@@ -14,9 +14,8 @@ limitations under the License.\n ==============================================================================*/\n \n #include \"nanobind/nanobind.h\"\n-#include \"jaxlib/gpu/gpu_kernel_helpers.h\"\n #include \"jaxlib/gpu/lu_pivot_kernels.h\"\n-#include \"jaxlib/kernel_nanobind_helpers.h\"\n+#include \"jaxlib/gpu/vendor.h\"\n \n namespace jax {\n namespace JAX_GPU_NAMESPACE {\n"
                },
                {
                    "old_start": 24,
                    "old_length": 29,
                    "new_start": 23,
                    "new_length": 13,
                    "hunk": "@@ -24,29 +23,13 @@ namespace {\n \n namespace nb = nanobind;\n \n-std::string BuildLuPivotsToPermutationDescriptor(\n-    std::int64_t batch_size, std::int32_t pivot_size,\n-    std::int32_t permutation_size) {\n-  return PackDescriptorAsString(LuPivotsToPermutationDescriptor{\n-      batch_size, pivot_size, permutation_size});\n-}\n-\n-nb::dict Registrations() {\n-  nb::dict dict;\n-  dict[JAX_GPU_PREFIX \"_lu_pivots_to_permutation\"] =\n-      EncapsulateFunction(LuPivotsToPermutation);\n-  return dict;\n-}\n-\n NB_MODULE(_linalg, m) {\n-  m.def(\"registrations\", &Registrations);\n-  m.def(\"lu_pivots_to_permutation_descriptor\",\n-        [](std::int64_t batch_size, std::int32_t pivot_size,\n-           std::int32_t permutation_size) {\n-          std::string result = BuildLuPivotsToPermutationDescriptor(\n-              batch_size, pivot_size, permutation_size);\n-          return nb::bytes(result.data(), result.size());\n-        });\n+  m.def(\"registrations\", []() {\n+    nb::dict dict;\n+    dict[JAX_GPU_PREFIX \"_lu_pivots_to_permutation\"] =\n+        nb::capsule(reinterpret_cast<void*>(+LuPivotsToPermutation));\n+    return dict;\n+  });\n }\n \n }  // namespace\n"
                }
            ],
            "whole_deleted": "-#include \"jaxlib/gpu/gpu_kernel_helpers.h\"\n-#include \"jaxlib/kernel_nanobind_helpers.h\"\n-std::string BuildLuPivotsToPermutationDescriptor(\n-    std::int64_t batch_size, std::int32_t pivot_size,\n-    std::int32_t permutation_size) {\n-  return PackDescriptorAsString(LuPivotsToPermutationDescriptor{\n-      batch_size, pivot_size, permutation_size});\n-}\n-\n-nb::dict Registrations() {\n-  nb::dict dict;\n-  dict[JAX_GPU_PREFIX \"_lu_pivots_to_permutation\"] =\n-      EncapsulateFunction(LuPivotsToPermutation);\n-  return dict;\n-}\n-\n-  m.def(\"registrations\", &Registrations);\n-  m.def(\"lu_pivots_to_permutation_descriptor\",\n-        [](std::int64_t batch_size, std::int32_t pivot_size,\n-           std::int32_t permutation_size) {\n-          std::string result = BuildLuPivotsToPermutationDescriptor(\n-              batch_size, pivot_size, permutation_size);\n-          return nb::bytes(result.data(), result.size());\n-        });\n",
            "whole_added": "+#include \"jaxlib/gpu/vendor.h\"\n+  m.def(\"registrations\", []() {\n+    nb::dict dict;\n+    dict[JAX_GPU_PREFIX \"_lu_pivots_to_permutation\"] =\n+        nb::capsule(reinterpret_cast<void*>(+LuPivotsToPermutation));\n+    return dict;\n+  });\n",
            "whole_hunk": "@@ -14,9 +14,8 @@ limitations under the License.\n ==============================================================================*/\n \n #include \"nanobind/nanobind.h\"\n-#include \"jaxlib/gpu/gpu_kernel_helpers.h\"\n #include \"jaxlib/gpu/lu_pivot_kernels.h\"\n-#include \"jaxlib/kernel_nanobind_helpers.h\"\n+#include \"jaxlib/gpu/vendor.h\"\n \n namespace jax {\n namespace JAX_GPU_NAMESPACE {\n@@ -24,29 +23,13 @@ namespace {\n \n namespace nb = nanobind;\n \n-std::string BuildLuPivotsToPermutationDescriptor(\n-    std::int64_t batch_size, std::int32_t pivot_size,\n-    std::int32_t permutation_size) {\n-  return PackDescriptorAsString(LuPivotsToPermutationDescriptor{\n-      batch_size, pivot_size, permutation_size});\n-}\n-\n-nb::dict Registrations() {\n-  nb::dict dict;\n-  dict[JAX_GPU_PREFIX \"_lu_pivots_to_permutation\"] =\n-      EncapsulateFunction(LuPivotsToPermutation);\n-  return dict;\n-}\n-\n NB_MODULE(_linalg, m) {\n-  m.def(\"registrations\", &Registrations);\n-  m.def(\"lu_pivots_to_permutation_descriptor\",\n-        [](std::int64_t batch_size, std::int32_t pivot_size,\n-           std::int32_t permutation_size) {\n-          std::string result = BuildLuPivotsToPermutationDescriptor(\n-              batch_size, pivot_size, permutation_size);\n-          return nb::bytes(result.data(), result.size());\n-        });\n+  m.def(\"registrations\", []() {\n+    nb::dict dict;\n+    dict[JAX_GPU_PREFIX \"_lu_pivots_to_permutation\"] =\n+        nb::capsule(reinterpret_cast<void*>(+LuPivotsToPermutation));\n+    return dict;\n+  });\n }\n \n }  // namespace\n"
        },
        {
            "name": "lu_pivot_kernels.cc",
            "path": "jaxlib/gpu/lu_pivot_kernels.cc",
            "patches": [
                {
                    "old_start": 15,
                    "old_length": 38,
                    "new_start": 15,
                    "new_length": 42,
                    "hunk": "@@ -15,38 +15,42 @@ limitations under the License.\n \n #include \"jaxlib/gpu/lu_pivot_kernels.h\"\n \n-#include <string_view>\n+#include <cstdint>\n+#include <string>\n \n #include \"jaxlib/gpu/gpu_kernel_helpers.h\"\n #include \"jaxlib/gpu/vendor.h\"\n-#include \"jaxlib/kernel_helpers.h\"\n-#include \"xla/service/custom_call_status.h\"\n+#include \"xla/ffi/api/c_api.h\"\n+#include \"xla/ffi/api/ffi.h\"\n \n namespace jax {\n namespace JAX_GPU_NAMESPACE {\n-namespace {\n-\n-absl::Status LuPivotsToPermutation_(gpuStream_t stream, void** buffers,\n-                                    const char* opaque,\n-                                    std::size_t opaque_len) {\n-  auto s =\n-      UnpackDescriptor<LuPivotsToPermutationDescriptor>(opaque, opaque_len);\n-  JAX_RETURN_IF_ERROR(s.status());\n-  LaunchLuPivotsToPermutationKernel(stream, buffers, **s);\n-  JAX_RETURN_IF_ERROR(JAX_AS_STATUS(gpuGetLastError()));\n-  return absl::OkStatus();\n-}\n-\n-}  // namespace\n \n-void LuPivotsToPermutation(gpuStream_t stream, void** buffers,\n-                           const char* opaque, size_t opaque_len,\n-                           XlaCustomCallStatus* status) {\n-  auto s = LuPivotsToPermutation_(stream, buffers, opaque, opaque_len);\n-  if (!s.ok()) {\n-    std::string_view message = s.message();\n-    XlaCustomCallStatusSetFailure(status, message.data(), message.length());\n-  }\n+namespace ffi = xla::ffi;\n+\n+XLA_FFI_Error* LuPivotsToPermutation(XLA_FFI_CallFrame* call_frame) {\n+  static const auto* kImpl =\n+      ffi::Ffi::Bind()\n+          .Ctx<ffi::PlatformStream<gpuStream_t>>()\n+          .Attr<std::int64_t>(\"batch_size\")\n+          .Attr<std::int32_t>(\"pivot_size\")\n+          .Attr<std::int32_t>(\"permutation_size\")\n+          .Arg<ffi::Buffer<ffi::DataType::S32>>()\n+          .Ret<ffi::Buffer<ffi::DataType::S32>>()\n+          .To([](gpuStream_t stream, std::int64_t batch_size,\n+                 std::int32_t pivot_size, std::int32_t permutation_size,\n+                 auto pivots, auto permutation) -> ffi::Error {\n+            LaunchLuPivotsToPermutationKernel(stream, batch_size, pivot_size,\n+                                              permutation_size, pivots.data,\n+                                              permutation->data);\n+            if (auto status = JAX_AS_STATUS(gpuGetLastError()); !status.ok()) {\n+              return ffi::Error(static_cast<XLA_FFI_Error_Code>(status.code()),\n+                                std::string(status.message()));\n+            }\n+            return ffi::Error::Success();\n+          })\n+          .release();\n+  return kImpl->Call(call_frame);\n }\n \n }  // namespace JAX_GPU_NAMESPACE\n"
                }
            ],
            "whole_deleted": "-#include <string_view>\n-#include \"jaxlib/kernel_helpers.h\"\n-#include \"xla/service/custom_call_status.h\"\n-namespace {\n-\n-absl::Status LuPivotsToPermutation_(gpuStream_t stream, void** buffers,\n-                                    const char* opaque,\n-                                    std::size_t opaque_len) {\n-  auto s =\n-      UnpackDescriptor<LuPivotsToPermutationDescriptor>(opaque, opaque_len);\n-  JAX_RETURN_IF_ERROR(s.status());\n-  LaunchLuPivotsToPermutationKernel(stream, buffers, **s);\n-  JAX_RETURN_IF_ERROR(JAX_AS_STATUS(gpuGetLastError()));\n-  return absl::OkStatus();\n-}\n-\n-}  // namespace\n-void LuPivotsToPermutation(gpuStream_t stream, void** buffers,\n-                           const char* opaque, size_t opaque_len,\n-                           XlaCustomCallStatus* status) {\n-  auto s = LuPivotsToPermutation_(stream, buffers, opaque, opaque_len);\n-  if (!s.ok()) {\n-    std::string_view message = s.message();\n-    XlaCustomCallStatusSetFailure(status, message.data(), message.length());\n-  }\n",
            "whole_added": "+#include <cstdint>\n+#include <string>\n+#include \"xla/ffi/api/c_api.h\"\n+#include \"xla/ffi/api/ffi.h\"\n+namespace ffi = xla::ffi;\n+\n+XLA_FFI_Error* LuPivotsToPermutation(XLA_FFI_CallFrame* call_frame) {\n+  static const auto* kImpl =\n+      ffi::Ffi::Bind()\n+          .Ctx<ffi::PlatformStream<gpuStream_t>>()\n+          .Attr<std::int64_t>(\"batch_size\")\n+          .Attr<std::int32_t>(\"pivot_size\")\n+          .Attr<std::int32_t>(\"permutation_size\")\n+          .Arg<ffi::Buffer<ffi::DataType::S32>>()\n+          .Ret<ffi::Buffer<ffi::DataType::S32>>()\n+          .To([](gpuStream_t stream, std::int64_t batch_size,\n+                 std::int32_t pivot_size, std::int32_t permutation_size,\n+                 auto pivots, auto permutation) -> ffi::Error {\n+            LaunchLuPivotsToPermutationKernel(stream, batch_size, pivot_size,\n+                                              permutation_size, pivots.data,\n+                                              permutation->data);\n+            if (auto status = JAX_AS_STATUS(gpuGetLastError()); !status.ok()) {\n+              return ffi::Error(static_cast<XLA_FFI_Error_Code>(status.code()),\n+                                std::string(status.message()));\n+            }\n+            return ffi::Error::Success();\n+          })\n+          .release();\n+  return kImpl->Call(call_frame);\n",
            "whole_hunk": "@@ -15,38 +15,42 @@ limitations under the License.\n \n #include \"jaxlib/gpu/lu_pivot_kernels.h\"\n \n-#include <string_view>\n+#include <cstdint>\n+#include <string>\n \n #include \"jaxlib/gpu/gpu_kernel_helpers.h\"\n #include \"jaxlib/gpu/vendor.h\"\n-#include \"jaxlib/kernel_helpers.h\"\n-#include \"xla/service/custom_call_status.h\"\n+#include \"xla/ffi/api/c_api.h\"\n+#include \"xla/ffi/api/ffi.h\"\n \n namespace jax {\n namespace JAX_GPU_NAMESPACE {\n-namespace {\n-\n-absl::Status LuPivotsToPermutation_(gpuStream_t stream, void** buffers,\n-                                    const char* opaque,\n-                                    std::size_t opaque_len) {\n-  auto s =\n-      UnpackDescriptor<LuPivotsToPermutationDescriptor>(opaque, opaque_len);\n-  JAX_RETURN_IF_ERROR(s.status());\n-  LaunchLuPivotsToPermutationKernel(stream, buffers, **s);\n-  JAX_RETURN_IF_ERROR(JAX_AS_STATUS(gpuGetLastError()));\n-  return absl::OkStatus();\n-}\n-\n-}  // namespace\n \n-void LuPivotsToPermutation(gpuStream_t stream, void** buffers,\n-                           const char* opaque, size_t opaque_len,\n-                           XlaCustomCallStatus* status) {\n-  auto s = LuPivotsToPermutation_(stream, buffers, opaque, opaque_len);\n-  if (!s.ok()) {\n-    std::string_view message = s.message();\n-    XlaCustomCallStatusSetFailure(status, message.data(), message.length());\n-  }\n+namespace ffi = xla::ffi;\n+\n+XLA_FFI_Error* LuPivotsToPermutation(XLA_FFI_CallFrame* call_frame) {\n+  static const auto* kImpl =\n+      ffi::Ffi::Bind()\n+          .Ctx<ffi::PlatformStream<gpuStream_t>>()\n+          .Attr<std::int64_t>(\"batch_size\")\n+          .Attr<std::int32_t>(\"pivot_size\")\n+          .Attr<std::int32_t>(\"permutation_size\")\n+          .Arg<ffi::Buffer<ffi::DataType::S32>>()\n+          .Ret<ffi::Buffer<ffi::DataType::S32>>()\n+          .To([](gpuStream_t stream, std::int64_t batch_size,\n+                 std::int32_t pivot_size, std::int32_t permutation_size,\n+                 auto pivots, auto permutation) -> ffi::Error {\n+            LaunchLuPivotsToPermutationKernel(stream, batch_size, pivot_size,\n+                                              permutation_size, pivots.data,\n+                                              permutation->data);\n+            if (auto status = JAX_AS_STATUS(gpuGetLastError()); !status.ok()) {\n+              return ffi::Error(static_cast<XLA_FFI_Error_Code>(status.code()),\n+                                std::string(status.message()));\n+            }\n+            return ffi::Error::Success();\n+          })\n+          .release();\n+  return kImpl->Call(call_frame);\n }\n \n }  // namespace JAX_GPU_NAMESPACE\n"
        },
        {
            "name": "lu_pivot_kernels.cu.cc",
            "path": "jaxlib/gpu/lu_pivot_kernels.cu.cc",
            "patches": [
                {
                    "old_start": 61,
                    "old_length": 21,
                    "new_start": 61,
                    "new_length": 19,
                    "hunk": "@@ -61,21 +61,19 @@ __global__ void LuPivotsToPermutationKernel(\n \n }  // namespace\n \n-void LaunchLuPivotsToPermutationKernel(\n-    gpuStream_t stream, void** buffers,\n-    LuPivotsToPermutationDescriptor descriptor) {\n-  const std::int32_t* pivots =\n-      reinterpret_cast<const std::int32_t*>(buffers[0]);\n-  std::int32_t* permutation_out = reinterpret_cast<std::int32_t*>(buffers[1]);\n-\n+void LaunchLuPivotsToPermutationKernel(gpuStream_t stream,\n+                                       std::int64_t batch_size,\n+                                       std::int32_t pivot_size,\n+                                       std::int32_t permutation_size,\n+                                       const std::int32_t* pivots,\n+                                       std::int32_t* permutation) {\n   const int block_dim = 128;\n-  const std::int64_t grid_dim = std::min<std::int64_t>(\n-      1024, (descriptor.batch_size + block_dim - 1) / block_dim);\n+  const std::int64_t grid_dim =\n+      std::min<std::int64_t>(1024, (batch_size + block_dim - 1) / block_dim);\n \n   LuPivotsToPermutationKernel<<<grid_dim, block_dim,\n                                 /*dynamic_shared_mem_bytes=*/0, stream>>>(\n-      pivots, permutation_out, descriptor.batch_size, descriptor.pivot_size,\n-      descriptor.permutation_size);\n+      pivots, permutation, batch_size, pivot_size, permutation_size);\n }\n \n }  // namespace JAX_GPU_NAMESPACE\n"
                }
            ],
            "whole_deleted": "-void LaunchLuPivotsToPermutationKernel(\n-    gpuStream_t stream, void** buffers,\n-    LuPivotsToPermutationDescriptor descriptor) {\n-  const std::int32_t* pivots =\n-      reinterpret_cast<const std::int32_t*>(buffers[0]);\n-  std::int32_t* permutation_out = reinterpret_cast<std::int32_t*>(buffers[1]);\n-\n-  const std::int64_t grid_dim = std::min<std::int64_t>(\n-      1024, (descriptor.batch_size + block_dim - 1) / block_dim);\n-      pivots, permutation_out, descriptor.batch_size, descriptor.pivot_size,\n-      descriptor.permutation_size);\n",
            "whole_added": "+void LaunchLuPivotsToPermutationKernel(gpuStream_t stream,\n+                                       std::int64_t batch_size,\n+                                       std::int32_t pivot_size,\n+                                       std::int32_t permutation_size,\n+                                       const std::int32_t* pivots,\n+                                       std::int32_t* permutation) {\n+  const std::int64_t grid_dim =\n+      std::min<std::int64_t>(1024, (batch_size + block_dim - 1) / block_dim);\n+      pivots, permutation, batch_size, pivot_size, permutation_size);\n",
            "whole_hunk": "@@ -61,21 +61,19 @@ __global__ void LuPivotsToPermutationKernel(\n \n }  // namespace\n \n-void LaunchLuPivotsToPermutationKernel(\n-    gpuStream_t stream, void** buffers,\n-    LuPivotsToPermutationDescriptor descriptor) {\n-  const std::int32_t* pivots =\n-      reinterpret_cast<const std::int32_t*>(buffers[0]);\n-  std::int32_t* permutation_out = reinterpret_cast<std::int32_t*>(buffers[1]);\n-\n+void LaunchLuPivotsToPermutationKernel(gpuStream_t stream,\n+                                       std::int64_t batch_size,\n+                                       std::int32_t pivot_size,\n+                                       std::int32_t permutation_size,\n+                                       const std::int32_t* pivots,\n+                                       std::int32_t* permutation) {\n   const int block_dim = 128;\n-  const std::int64_t grid_dim = std::min<std::int64_t>(\n-      1024, (descriptor.batch_size + block_dim - 1) / block_dim);\n+  const std::int64_t grid_dim =\n+      std::min<std::int64_t>(1024, (batch_size + block_dim - 1) / block_dim);\n \n   LuPivotsToPermutationKernel<<<grid_dim, block_dim,\n                                 /*dynamic_shared_mem_bytes=*/0, stream>>>(\n-      pivots, permutation_out, descriptor.batch_size, descriptor.pivot_size,\n-      descriptor.permutation_size);\n+      pivots, permutation, batch_size, pivot_size, permutation_size);\n }\n \n }  // namespace JAX_GPU_NAMESPACE\n"
        },
        {
            "name": "lu_pivot_kernels.h",
            "path": "jaxlib/gpu/lu_pivot_kernels.h",
            "patches": [
                {
                    "old_start": 16,
                    "old_length": 29,
                    "new_start": 16,
                    "new_length": 22,
                    "hunk": "@@ -16,29 +16,22 @@ limitations under the License.\n #ifndef JAXLIB_GPU_LU_PIVOT_KERNELS_H_\n #define JAXLIB_GPU_LU_PIVOT_KERNELS_H_\n \n-#include <cstddef>\n #include <cstdint>\n-#include <string>\n \n #include \"jaxlib/gpu/vendor.h\"\n-#include \"xla/service/custom_call_status.h\"\n+#include \"xla/ffi/api/c_api.h\"\n \n namespace jax {\n namespace JAX_GPU_NAMESPACE {\n \n-struct LuPivotsToPermutationDescriptor {\n-  std::int64_t batch_size;\n-  std::int32_t pivot_size;\n-  std::int32_t permutation_size;\n-};\n+void LaunchLuPivotsToPermutationKernel(gpuStream_t stream,\n+                                       std::int64_t batch_size,\n+                                       std::int32_t pivot_size,\n+                                       std::int32_t permutation_size,\n+                                       const std::int32_t* pivots,\n+                                       std::int32_t* permutation);\n \n-void LaunchLuPivotsToPermutationKernel(\n-    gpuStream_t stream, void** buffers,\n-    LuPivotsToPermutationDescriptor descriptor);\n-\n-void LuPivotsToPermutation(gpuStream_t stream, void** buffers,\n-                           const char* opaque, size_t opaque_len,\n-                           XlaCustomCallStatus* status);\n+XLA_FFI_Error* LuPivotsToPermutation(XLA_FFI_CallFrame* call_frame);\n \n }  // namespace JAX_GPU_NAMESPACE\n }  // namespace jax\n"
                }
            ],
            "whole_deleted": "-#include <cstddef>\n-#include <string>\n-#include \"xla/service/custom_call_status.h\"\n-struct LuPivotsToPermutationDescriptor {\n-  std::int64_t batch_size;\n-  std::int32_t pivot_size;\n-  std::int32_t permutation_size;\n-};\n-void LaunchLuPivotsToPermutationKernel(\n-    gpuStream_t stream, void** buffers,\n-    LuPivotsToPermutationDescriptor descriptor);\n-\n-void LuPivotsToPermutation(gpuStream_t stream, void** buffers,\n-                           const char* opaque, size_t opaque_len,\n-                           XlaCustomCallStatus* status);\n",
            "whole_added": "+#include \"xla/ffi/api/c_api.h\"\n+void LaunchLuPivotsToPermutationKernel(gpuStream_t stream,\n+                                       std::int64_t batch_size,\n+                                       std::int32_t pivot_size,\n+                                       std::int32_t permutation_size,\n+                                       const std::int32_t* pivots,\n+                                       std::int32_t* permutation);\n+XLA_FFI_Error* LuPivotsToPermutation(XLA_FFI_CallFrame* call_frame);\n",
            "whole_hunk": "@@ -16,29 +16,22 @@ limitations under the License.\n #ifndef JAXLIB_GPU_LU_PIVOT_KERNELS_H_\n #define JAXLIB_GPU_LU_PIVOT_KERNELS_H_\n \n-#include <cstddef>\n #include <cstdint>\n-#include <string>\n \n #include \"jaxlib/gpu/vendor.h\"\n-#include \"xla/service/custom_call_status.h\"\n+#include \"xla/ffi/api/c_api.h\"\n \n namespace jax {\n namespace JAX_GPU_NAMESPACE {\n \n-struct LuPivotsToPermutationDescriptor {\n-  std::int64_t batch_size;\n-  std::int32_t pivot_size;\n-  std::int32_t permutation_size;\n-};\n+void LaunchLuPivotsToPermutationKernel(gpuStream_t stream,\n+                                       std::int64_t batch_size,\n+                                       std::int32_t pivot_size,\n+                                       std::int32_t permutation_size,\n+                                       const std::int32_t* pivots,\n+                                       std::int32_t* permutation);\n \n-void LaunchLuPivotsToPermutationKernel(\n-    gpuStream_t stream, void** buffers,\n-    LuPivotsToPermutationDescriptor descriptor);\n-\n-void LuPivotsToPermutation(gpuStream_t stream, void** buffers,\n-                           const char* opaque, size_t opaque_len,\n-                           XlaCustomCallStatus* status);\n+XLA_FFI_Error* LuPivotsToPermutation(XLA_FFI_CallFrame* call_frame);\n \n }  // namespace JAX_GPU_NAMESPACE\n }  // namespace jax\n"
        },
        {
            "name": "gpu_linalg.py",
            "path": "jaxlib/gpu_linalg.py",
            "patches": [
                {
                    "old_start": 36,
                    "old_length": 12,
                    "new_start": 36,
                    "new_length": 16,
                    "hunk": "@@ -36,12 +36,16 @@ for cuda_module_name in [\".cuda\", \"jax_cuda12_plugin\"]:\n \n if _cuda_linalg:\n   for _name, _value in _cuda_linalg.registrations().items():\n-    xla_client.register_custom_call_target(_name, _value, platform=\"CUDA\")\n+    xla_client.register_custom_call_target(\n+        _name, _value, platform=\"CUDA\", api_version=1\n+    )\n \n try:\n   from .rocm import _linalg as _hip_linalg  # pytype: disable=import-error\n   for _name, _value in _hip_linalg.registrations().items():\n-    xla_client.register_custom_call_target(_name, _value, platform=\"ROCM\")\n+    xla_client.register_custom_call_target(\n+        _name, _value, platform=\"ROCM\", api_version=1\n+    )\n except ImportError:\n   _hip_linalg = None\n \n"
                },
                {
                    "old_start": 53,
                    "old_length": 6,
                    "new_start": 57,
                    "new_length": 7,
                    "hunk": "@@ -53,6 +57,7 @@ def _lu_pivots_to_permutation_hlo(platform, gpu_linalg, pivots, *, permutation_s\n   typ = ir.RankedTensorType(pivots.type)\n   dims = typ.shape\n   i32_type = ir.IntegerType.get_signless(32)\n+  i64_type = ir.IntegerType.get_signless(64)\n \n   assert typ.element_type == i32_type, typ\n \n"
                },
                {
                    "old_start": 62,
                    "old_length": 8,
                    "new_start": 67,
                    "new_length": 6,
                    "hunk": "@@ -62,8 +67,6 @@ def _lu_pivots_to_permutation_hlo(platform, gpu_linalg, pivots, *, permutation_s\n   if not gpu_linalg:\n     raise GpuLibNotLinkedError()\n \n-  opaque = gpu_linalg.lu_pivots_to_permutation_descriptor(\n-      batch_size, pivot_size, permutation_size)\n   pivots_layout = tuple(range(len(dims) - 1, -1, -1))\n   permutations_layout = pivots_layout\n   permutations_dims = list(dims)\n"
                },
                {
                    "old_start": 71,
                    "old_length": 11,
                    "new_start": 74,
                    "new_length": 17,
                    "hunk": "@@ -71,11 +74,17 @@ def _lu_pivots_to_permutation_hlo(platform, gpu_linalg, pivots, *, permutation_s\n   permutations_type = ir.RankedTensorType.get(permutations_dims, i32_type)\n   return custom_call(\n       f\"{platform}_lu_pivots_to_permutation\",\n+      api_version=4,\n       result_types=[permutations_type],\n       operands=[pivots],\n-      backend_config=opaque,\n+      backend_config=dict(\n+          batch_size=ir.IntegerAttr.get(i64_type, batch_size),\n+          pivot_size=ir.IntegerAttr.get(i32_type, pivot_size),\n+          permutation_size=ir.IntegerAttr.get(i32_type, permutation_size),\n+      ),\n       operand_layouts=[pivots_layout],\n-      result_layouts=[permutations_layout]).results\n+      result_layouts=[permutations_layout],\n+  ).results\n \n cuda_lu_pivots_to_permutation = partial(_lu_pivots_to_permutation_hlo, \"cu\",\n                                         _cuda_linalg)\n"
                }
            ],
            "whole_deleted": "-    xla_client.register_custom_call_target(_name, _value, platform=\"CUDA\")\n-    xla_client.register_custom_call_target(_name, _value, platform=\"ROCM\")\n-  opaque = gpu_linalg.lu_pivots_to_permutation_descriptor(\n-      batch_size, pivot_size, permutation_size)\n-      backend_config=opaque,\n-      result_layouts=[permutations_layout]).results\n",
            "whole_added": "+    xla_client.register_custom_call_target(\n+        _name, _value, platform=\"CUDA\", api_version=1\n+    )\n+    xla_client.register_custom_call_target(\n+        _name, _value, platform=\"ROCM\", api_version=1\n+    )\n+  i64_type = ir.IntegerType.get_signless(64)\n+      api_version=4,\n+      backend_config=dict(\n+          batch_size=ir.IntegerAttr.get(i64_type, batch_size),\n+          pivot_size=ir.IntegerAttr.get(i32_type, pivot_size),\n+          permutation_size=ir.IntegerAttr.get(i32_type, permutation_size),\n+      ),\n+      result_layouts=[permutations_layout],\n+  ).results\n",
            "whole_hunk": "@@ -36,12 +36,16 @@ for cuda_module_name in [\".cuda\", \"jax_cuda12_plugin\"]:\n \n if _cuda_linalg:\n   for _name, _value in _cuda_linalg.registrations().items():\n-    xla_client.register_custom_call_target(_name, _value, platform=\"CUDA\")\n+    xla_client.register_custom_call_target(\n+        _name, _value, platform=\"CUDA\", api_version=1\n+    )\n \n try:\n   from .rocm import _linalg as _hip_linalg  # pytype: disable=import-error\n   for _name, _value in _hip_linalg.registrations().items():\n-    xla_client.register_custom_call_target(_name, _value, platform=\"ROCM\")\n+    xla_client.register_custom_call_target(\n+        _name, _value, platform=\"ROCM\", api_version=1\n+    )\n except ImportError:\n   _hip_linalg = None\n \n@@ -53,6 +57,7 @@ def _lu_pivots_to_permutation_hlo(platform, gpu_linalg, pivots, *, permutation_s\n   typ = ir.RankedTensorType(pivots.type)\n   dims = typ.shape\n   i32_type = ir.IntegerType.get_signless(32)\n+  i64_type = ir.IntegerType.get_signless(64)\n \n   assert typ.element_type == i32_type, typ\n \n@@ -62,8 +67,6 @@ def _lu_pivots_to_permutation_hlo(platform, gpu_linalg, pivots, *, permutation_s\n   if not gpu_linalg:\n     raise GpuLibNotLinkedError()\n \n-  opaque = gpu_linalg.lu_pivots_to_permutation_descriptor(\n-      batch_size, pivot_size, permutation_size)\n   pivots_layout = tuple(range(len(dims) - 1, -1, -1))\n   permutations_layout = pivots_layout\n   permutations_dims = list(dims)\n@@ -71,11 +74,17 @@ def _lu_pivots_to_permutation_hlo(platform, gpu_linalg, pivots, *, permutation_s\n   permutations_type = ir.RankedTensorType.get(permutations_dims, i32_type)\n   return custom_call(\n       f\"{platform}_lu_pivots_to_permutation\",\n+      api_version=4,\n       result_types=[permutations_type],\n       operands=[pivots],\n-      backend_config=opaque,\n+      backend_config=dict(\n+          batch_size=ir.IntegerAttr.get(i64_type, batch_size),\n+          pivot_size=ir.IntegerAttr.get(i32_type, pivot_size),\n+          permutation_size=ir.IntegerAttr.get(i32_type, permutation_size),\n+      ),\n       operand_layouts=[pivots_layout],\n-      result_layouts=[permutations_layout]).results\n+      result_layouts=[permutations_layout],\n+  ).results\n \n cuda_lu_pivots_to_permutation = partial(_lu_pivots_to_permutation_hlo, \"cu\",\n                                         _cuda_linalg)\n"
        },
        {
            "name": "BUILD.bazel",
            "path": "jaxlib/rocm/BUILD.bazel",
            "patches": [
                {
                    "old_start": 201,
                    "old_length": 9,
                    "new_start": 201,
                    "new_length": 9,
                    "hunk": "@@ -201,9 +201,9 @@ cc_library(\n         \":hip_gpu_kernel_helpers\",\n         \":hip_lu_pivot_kernels_impl\",\n         \":hip_vendor\",\n-        \"//jaxlib:kernel_helpers\",\n         \"@local_config_rocm//rocm:rocm_headers\",\n-        \"@xla//xla/service:custom_call_status\",\n+        \"@xla//xla/ffi/api:c_api\",\n+        \"@xla//xla/ffi/api:ffi\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 214,
                    "old_length": 9,
                    "new_start": 214,
                    "new_length": 8,
                    "hunk": "@@ -214,9 +214,8 @@ rocm_library(\n     deps = [\n         \":hip_gpu_kernel_helpers\",\n         \":hip_vendor\",\n-        \"//jaxlib:kernel_helpers\",\n         \"@local_config_rocm//rocm:rocm_headers\",\n-        \"@xla//xla/service:custom_call_status\",\n+        \"@xla//xla/ffi/api:c_api\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 295,
                    "old_length": 8,
                    "new_start": 294,
                    "new_length": 6,
                    "hunk": "@@ -295,8 +294,6 @@ cc_library(\n         \":hip_vendor\",\n         \":triton_utils\",\n         \"//jaxlib/gpu:triton_cc_proto\",\n-        \"@xla//xla/service:custom_call_status\",\n-        \"@xla//xla/stream_executor/gpu:asm_compiler\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/cleanup\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n"
                },
                {
                    "old_start": 306,
                    "old_length": 6,
                    "new_start": 303,
                    "new_length": 8,
                    "hunk": "@@ -306,6 +303,8 @@ cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/synchronization\",\n+        \"@xla//xla/service:custom_call_status\",\n+        \"@xla//xla/stream_executor/gpu:asm_compiler\",\n         \"@xla//xla/tsl/util:env_var\",\n     ],\n )"
                }
            ],
            "whole_deleted": "-        \"//jaxlib:kernel_helpers\",\n-        \"@xla//xla/service:custom_call_status\",\n-        \"//jaxlib:kernel_helpers\",\n-        \"@xla//xla/service:custom_call_status\",\n-        \"@xla//xla/service:custom_call_status\",\n-        \"@xla//xla/stream_executor/gpu:asm_compiler\",\n",
            "whole_added": "+        \"@xla//xla/ffi/api:c_api\",\n+        \"@xla//xla/ffi/api:ffi\",\n+        \"@xla//xla/ffi/api:c_api\",\n+        \"@xla//xla/service:custom_call_status\",\n+        \"@xla//xla/stream_executor/gpu:asm_compiler\",\n",
            "whole_hunk": "@@ -201,9 +201,9 @@ cc_library(\n         \":hip_gpu_kernel_helpers\",\n         \":hip_lu_pivot_kernels_impl\",\n         \":hip_vendor\",\n-        \"//jaxlib:kernel_helpers\",\n         \"@local_config_rocm//rocm:rocm_headers\",\n-        \"@xla//xla/service:custom_call_status\",\n+        \"@xla//xla/ffi/api:c_api\",\n+        \"@xla//xla/ffi/api:ffi\",\n     ],\n )\n \n@@ -214,9 +214,8 @@ rocm_library(\n     deps = [\n         \":hip_gpu_kernel_helpers\",\n         \":hip_vendor\",\n-        \"//jaxlib:kernel_helpers\",\n         \"@local_config_rocm//rocm:rocm_headers\",\n-        \"@xla//xla/service:custom_call_status\",\n+        \"@xla//xla/ffi/api:c_api\",\n     ],\n )\n \n@@ -295,8 +294,6 @@ cc_library(\n         \":hip_vendor\",\n         \":triton_utils\",\n         \"//jaxlib/gpu:triton_cc_proto\",\n-        \"@xla//xla/service:custom_call_status\",\n-        \"@xla//xla/stream_executor/gpu:asm_compiler\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/cleanup\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n@@ -306,6 +303,8 @@ cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/synchronization\",\n+        \"@xla//xla/service:custom_call_status\",\n+        \"@xla//xla/stream_executor/gpu:asm_compiler\",\n         \"@xla//xla/tsl/util:env_var\",\n     ],\n )"
        }
    ]
},
{
    "Id": 50,
    "commit_link": "https://github.com/google/jax/commit/582c56a7070ff972ac66848e245592103596c948",
    "date": "2024-05-02T07:22:56-07:00",
    "message": "Change determination of cloud TPU to check for TPU chips.\n\nThis is useful in the case of ahead of time compilation, when libtpu is present but there may not be any TPU chips, so we shouldn't attempt to initialize a TPU backend.\n\nPiperOrigin-RevId: 630055511",
    "changes": [
        {
            "name": "cloud_tpu_init.py",
            "path": "jax/_src/cloud_tpu_init.py",
            "patches": [
                {
                    "old_start": 54,
                    "old_length": 11,
                    "new_start": 54,
                    "new_length": 10,
                    "hunk": "@@ -54,11 +54,10 @@ def cloud_tpu_init() -> None:\n   \"\"\"\n   global running_in_cloud_tpu_vm\n \n-  # We assume we are in a correctly-configured Cloud TPU environment\n-  # if the following hold: a) libtpu is installed b) JAX_FORCE_TPU_INIT is set\n-  # Exit early if we're not running on Cloud TPU.\n+  # Exit early if we're not running on a Cloud TPU VM or libtpu isn't installed.\n   libtpu_module = maybe_import_libtpu()\n-  if libtpu_module is None and not jax_force_tpu_init():\n+  num_tpu_chips = hardware_utils.num_available_tpu_chips_and_device_id()[0]\n+  if (libtpu_module is None or num_tpu_chips == 0) and not jax_force_tpu_init():\n     return\n \n   running_in_cloud_tpu_vm = True"
                }
            ],
            "whole_deleted": "-  # We assume we are in a correctly-configured Cloud TPU environment\n-  # if the following hold: a) libtpu is installed b) JAX_FORCE_TPU_INIT is set\n-  # Exit early if we're not running on Cloud TPU.\n-  if libtpu_module is None and not jax_force_tpu_init():\n",
            "whole_added": "+  # Exit early if we're not running on a Cloud TPU VM or libtpu isn't installed.\n+  num_tpu_chips = hardware_utils.num_available_tpu_chips_and_device_id()[0]\n+  if (libtpu_module is None or num_tpu_chips == 0) and not jax_force_tpu_init():\n",
            "whole_hunk": "@@ -54,11 +54,10 @@ def cloud_tpu_init() -> None:\n   \"\"\"\n   global running_in_cloud_tpu_vm\n \n-  # We assume we are in a correctly-configured Cloud TPU environment\n-  # if the following hold: a) libtpu is installed b) JAX_FORCE_TPU_INIT is set\n-  # Exit early if we're not running on Cloud TPU.\n+  # Exit early if we're not running on a Cloud TPU VM or libtpu isn't installed.\n   libtpu_module = maybe_import_libtpu()\n-  if libtpu_module is None and not jax_force_tpu_init():\n+  num_tpu_chips = hardware_utils.num_available_tpu_chips_and_device_id()[0]\n+  if (libtpu_module is None or num_tpu_chips == 0) and not jax_force_tpu_init():\n     return\n \n   running_in_cloud_tpu_vm = True"
        }
    ]
},
{
    "Id": 51,
    "commit_link": "https://github.com/google/jax/commit/442526869f638637e4a60ee0aaf26aef8c04f953",
    "date": "2024-05-01T19:37:26+01:00",
    "message": "Bundle MLIR .pyi files with jaxlib\n\nThis allows mypy and pyright to type check the code using MLIR Python APIs.",
    "changes": [
        {
            "name": "BUILD.bazel",
            "path": "jaxlib/mlir/BUILD.bazel",
            "patches": [
                {
                    "old_start": 44,
                    "old_length": 7,
                    "new_start": 44,
                    "new_length": 10,
                    "hunk": "@@ -44,7 +44,10 @@ symlink_inputs(\n     name = \"ir\",\n     rule = py_library,\n     symlinked_inputs = {\"srcs\": {\n-        \".\": [\"@llvm-project//mlir/python:IRPyFiles\"],\n+        \".\": [\n+            \"@llvm-project//mlir/python:IRPyFiles\",\n+            \"@llvm-project//mlir/python:IRPyIFiles\",\n+        ],\n     }},\n     deps = [\n         \":mlir\",\n"
                },
                {
                    "old_start": 197,
                    "old_length": 7,
                    "new_start": 200,
                    "new_length": 10,
                    "hunk": "@@ -197,7 +200,10 @@ symlink_inputs(\n     name = \"pass_manager\",\n     rule = py_library,\n     symlinked_inputs = {\"srcs\": {\n-        \".\": [\"@llvm-project//mlir/python:PassManagerPyFiles\"],\n+        \".\": [\n+            \"@llvm-project//mlir/python:PassManagerPyFiles\",\n+            \"@llvm-project//mlir/python:PassManagerPyIFiles\",\n+        ],\n     }},\n     deps = [\n         \":mlir\",\n"
                },
                {
                    "old_start": 224,
                    "old_length": 6,
                    "new_start": 230,
                    "new_length": 7,
                    "hunk": "@@ -224,6 +230,7 @@ symlink_inputs(\n     symlinked_inputs = {\"srcs\": {\n         \".\": [\n             \"@llvm-project//mlir/python:ExecutionEnginePyFiles\",\n+            \"@llvm-project//mlir/python:ExecutionEnginePyIFiles\",\n         ],\n     }},\n     deps = [\n"
                }
            ],
            "whole_deleted": "-        \".\": [\"@llvm-project//mlir/python:IRPyFiles\"],\n-        \".\": [\"@llvm-project//mlir/python:PassManagerPyFiles\"],\n",
            "whole_added": "+        \".\": [\n+            \"@llvm-project//mlir/python:IRPyFiles\",\n+            \"@llvm-project//mlir/python:IRPyIFiles\",\n+        ],\n+        \".\": [\n+            \"@llvm-project//mlir/python:PassManagerPyFiles\",\n+            \"@llvm-project//mlir/python:PassManagerPyIFiles\",\n+        ],\n+            \"@llvm-project//mlir/python:ExecutionEnginePyIFiles\",\n",
            "whole_hunk": "@@ -44,7 +44,10 @@ symlink_inputs(\n     name = \"ir\",\n     rule = py_library,\n     symlinked_inputs = {\"srcs\": {\n-        \".\": [\"@llvm-project//mlir/python:IRPyFiles\"],\n+        \".\": [\n+            \"@llvm-project//mlir/python:IRPyFiles\",\n+            \"@llvm-project//mlir/python:IRPyIFiles\",\n+        ],\n     }},\n     deps = [\n         \":mlir\",\n@@ -197,7 +200,10 @@ symlink_inputs(\n     name = \"pass_manager\",\n     rule = py_library,\n     symlinked_inputs = {\"srcs\": {\n-        \".\": [\"@llvm-project//mlir/python:PassManagerPyFiles\"],\n+        \".\": [\n+            \"@llvm-project//mlir/python:PassManagerPyFiles\",\n+            \"@llvm-project//mlir/python:PassManagerPyIFiles\",\n+        ],\n     }},\n     deps = [\n         \":mlir\",\n@@ -224,6 +230,7 @@ symlink_inputs(\n     symlinked_inputs = {\"srcs\": {\n         \".\": [\n             \"@llvm-project//mlir/python:ExecutionEnginePyFiles\",\n+            \"@llvm-project//mlir/python:ExecutionEnginePyIFiles\",\n         ],\n     }},\n     deps = [\n"
        },
        {
            "name": "setup.py",
            "path": "jaxlib/setup.py",
            "patches": [
                {
                    "old_start": 102,
                    "old_length": 6,
                    "new_start": 102,
                    "new_length": 7,
                    "hunk": "@@ -102,6 +102,7 @@ setup(\n             'mosaic/python/*.py',\n             'mosaic/python/*.so',\n             'mlir/*.py',\n+            'mlir/*.pyi',\n             'mlir/dialects/*.py',\n             'mlir/dialects/gpu/*.py',\n             'mlir/dialects/gpu/passes/*.py',\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+            'mlir/*.pyi',\n",
            "whole_hunk": "@@ -102,6 +102,7 @@ setup(\n             'mosaic/python/*.py',\n             'mosaic/python/*.so',\n             'mlir/*.py',\n+            'mlir/*.pyi',\n             'mlir/dialects/*.py',\n             'mlir/dialects/gpu/*.py',\n             'mlir/dialects/gpu/passes/*.py',\n"
        },
        {
            "name": "build_wheel.py",
            "path": "jaxlib/tools/build_wheel.py",
            "patches": [
                {
                    "old_start": 283,
                    "old_length": 9,
                    "new_start": 283,
                    "new_length": 12,
                    "hunk": "@@ -283,9 +283,12 @@ def prepare_wheel(sources_path: pathlib.Path, *, cpu, skip_gpu_kernels):\n       dst_dir=jaxlib_dir / \"mlir\",\n       src_files=[\n           \"__main__/jaxlib/mlir/ir.py\",\n+          \"__main__/jaxlib/mlir/ir.pyi\",\n           \"__main__/jaxlib/mlir/passmanager.py\",\n+          \"__main__/jaxlib/mlir/passmanager.pyi\",\n       ] + if_has_mosaic_gpu([\n           \"__main__/jaxlib/mlir/execution_engine.py\",\n+          \"__main__/jaxlib/mlir/execution_engine.pyi\",\n       ]),\n   )\n   copy_runfiles("
                }
            ],
            "whole_deleted": "",
            "whole_added": "+          \"__main__/jaxlib/mlir/ir.pyi\",\n+          \"__main__/jaxlib/mlir/passmanager.pyi\",\n+          \"__main__/jaxlib/mlir/execution_engine.pyi\",\n",
            "whole_hunk": "@@ -283,9 +283,12 @@ def prepare_wheel(sources_path: pathlib.Path, *, cpu, skip_gpu_kernels):\n       dst_dir=jaxlib_dir / \"mlir\",\n       src_files=[\n           \"__main__/jaxlib/mlir/ir.py\",\n+          \"__main__/jaxlib/mlir/ir.pyi\",\n           \"__main__/jaxlib/mlir/passmanager.py\",\n+          \"__main__/jaxlib/mlir/passmanager.pyi\",\n       ] + if_has_mosaic_gpu([\n           \"__main__/jaxlib/mlir/execution_engine.py\",\n+          \"__main__/jaxlib/mlir/execution_engine.pyi\",\n       ]),\n   )\n   copy_runfiles("
        }
    ]
},
{
    "Id": 52,
    "commit_link": "https://github.com/google/jax/commit/9bf1148e74a4190bf5d466488f11d7d3cde4887c",
    "date": "2024-05-01T09:37:48-07:00",
    "message": "[Mosaic] Always define tiling as (1, 128) for 1D loaded or stored vectors (not for the memref), instead of sometimes using (1, 128 * n).\n\nThey are equivalent - the way values are laid out is the same - but relayouts check specifically for (1, 128). We define (1, 128) to be canonical.\n\nPiperOrigin-RevId: 629748121",
    "changes": [
        {
            "name": "infer_vector_layout.cc",
            "path": "jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc",
            "patches": [
                {
                    "old_start": 1025,
                    "old_length": 10,
                    "new_start": 1025,
                    "new_length": 12,
                    "hunk": "@@ -1025,10 +1025,12 @@ class VectorLayoutInferer {\n       TPU_CHECK_OP(tile % target_shape_[1] == 0,\n                    \"Unsupported tiling for 1D load\");\n       CHECK_EQ(tile_offsets.size(), 1);\n+      // TODO(tlongeri): Also pick a unique (canonical) tiling for packed types\n+      const int64_t lane_tiling = bitwidth == 32 ? target_shape_[1] : tile;\n       // TODO(apaszke): We could generate replicated loads for short values.\n       setLayout(op, in_layout,\n-                VectorLayout(bitwidth, {0, tile_offsets[0]}, {1, tile},\n-                             ImplicitDim::kSecondMinor));\n+                VectorLayout(bitwidth, {0, tile_offsets[0] % lane_tiling},\n+                             {1, lane_tiling}, ImplicitDim::kSecondMinor));\n     } else {  // rank >= 2\n       TPU_CHECK_OP(tiling.size() == 2, \"Expected 2D tiling in 2D+ loads\");\n       CHECK_EQ(tile_offsets.size(), 2);\n"
                },
                {
                    "old_start": 1366,
                    "old_length": 9,
                    "new_start": 1368,
                    "new_length": 12,
                    "hunk": "@@ -1366,9 +1368,12 @@ class VectorLayoutInferer {\n       auto tile = tiling.front();\n       TPU_CHECK_OP(tile % target_shape_[1] == 0,\n                    \"Unsupported 1D tiling for 1D store\");\n+      // TODO(tlongeri): Also pick a unique (canonical) tiling for packed types\n+      const int64_t lane_tiling = bitwidth == 32 ? target_shape_[1] : tile;\n       CHECK_EQ(tile_offsets.size(), 1);\n-      store_layout = VectorLayout(bitwidth, {0, tile_offsets[0]}, {1, tile},\n-                                  ImplicitDim::kSecondMinor);\n+      store_layout =\n+          VectorLayout(bitwidth, {0, tile_offsets[0] % lane_tiling},\n+                       {1, lane_tiling}, ImplicitDim::kSecondMinor);\n     } else {  // rank >= 2  // NOLINT(readability-else-after-return)\n       TPU_CHECK_OP(tiling.size() == 2, \"Expected 2D tiling in 2D+ store\");\n       CHECK_EQ(tile_offsets.size(), 2);"
                }
            ],
            "whole_deleted": "-                VectorLayout(bitwidth, {0, tile_offsets[0]}, {1, tile},\n-                             ImplicitDim::kSecondMinor));\n-      store_layout = VectorLayout(bitwidth, {0, tile_offsets[0]}, {1, tile},\n-                                  ImplicitDim::kSecondMinor);\n",
            "whole_added": "+      // TODO(tlongeri): Also pick a unique (canonical) tiling for packed types\n+      const int64_t lane_tiling = bitwidth == 32 ? target_shape_[1] : tile;\n+                VectorLayout(bitwidth, {0, tile_offsets[0] % lane_tiling},\n+                             {1, lane_tiling}, ImplicitDim::kSecondMinor));\n+      // TODO(tlongeri): Also pick a unique (canonical) tiling for packed types\n+      const int64_t lane_tiling = bitwidth == 32 ? target_shape_[1] : tile;\n+      store_layout =\n+          VectorLayout(bitwidth, {0, tile_offsets[0] % lane_tiling},\n+                       {1, lane_tiling}, ImplicitDim::kSecondMinor);\n",
            "whole_hunk": "@@ -1025,10 +1025,12 @@ class VectorLayoutInferer {\n       TPU_CHECK_OP(tile % target_shape_[1] == 0,\n                    \"Unsupported tiling for 1D load\");\n       CHECK_EQ(tile_offsets.size(), 1);\n+      // TODO(tlongeri): Also pick a unique (canonical) tiling for packed types\n+      const int64_t lane_tiling = bitwidth == 32 ? target_shape_[1] : tile;\n       // TODO(apaszke): We could generate replicated loads for short values.\n       setLayout(op, in_layout,\n-                VectorLayout(bitwidth, {0, tile_offsets[0]}, {1, tile},\n-                             ImplicitDim::kSecondMinor));\n+                VectorLayout(bitwidth, {0, tile_offsets[0] % lane_tiling},\n+                             {1, lane_tiling}, ImplicitDim::kSecondMinor));\n     } else {  // rank >= 2\n       TPU_CHECK_OP(tiling.size() == 2, \"Expected 2D tiling in 2D+ loads\");\n       CHECK_EQ(tile_offsets.size(), 2);\n@@ -1366,9 +1368,12 @@ class VectorLayoutInferer {\n       auto tile = tiling.front();\n       TPU_CHECK_OP(tile % target_shape_[1] == 0,\n                    \"Unsupported 1D tiling for 1D store\");\n+      // TODO(tlongeri): Also pick a unique (canonical) tiling for packed types\n+      const int64_t lane_tiling = bitwidth == 32 ? target_shape_[1] : tile;\n       CHECK_EQ(tile_offsets.size(), 1);\n-      store_layout = VectorLayout(bitwidth, {0, tile_offsets[0]}, {1, tile},\n-                                  ImplicitDim::kSecondMinor);\n+      store_layout =\n+          VectorLayout(bitwidth, {0, tile_offsets[0] % lane_tiling},\n+                       {1, lane_tiling}, ImplicitDim::kSecondMinor);\n     } else {  // rank >= 2  // NOLINT(readability-else-after-return)\n       TPU_CHECK_OP(tiling.size() == 2, \"Expected 2D tiling in 2D+ store\");\n       CHECK_EQ(tile_offsets.size(), 2);"
        }
    ]
},
{
    "Id": 53,
    "commit_link": "https://github.com/google/jax/commit/26a3d3dc020406ca87ede46c5ad8a27952d28a19",
    "date": "2024-04-23T18:02:07-07:00",
    "message": "Only perform checks on slice sizes if they're static.\n\nPiperOrigin-RevId: 627560765",
    "changes": [
        {
            "name": "indexing.py",
            "path": "jax/_src/state/indexing.py",
            "patches": [
                {
                    "old_start": 139,
                    "old_length": 11,
                    "new_start": 139,
                    "new_length": 12,
                    "hunk": "@@ -139,11 +139,12 @@ class NDIndexer:\n         if value := _maybe_concretize(start):\n           if value >= s:\n             raise ValueError(f\"Out of bound slice: start={value}, dim={s}.\")\n-          if value + (idx.size - 1) * idx.stride >= s:\n-            raise ValueError(\n-                f\"Out of bound slice: start={value}, size={idx.size},\"\n-                f\" stride={idx.stride}, dim={s}.\"\n-            )\n+          if size := _maybe_concretize(idx.size):\n+            if value + (size - 1) * idx.stride >= s:\n+              raise ValueError(\n+                  f\"Out of bound slice: start={value}, size={size},\"\n+                  f\" stride={idx.stride}, dim={s}.\"\n+              )\n         continue\n       # The shape of indexer integers should be broadcastable up to the\n       # int_indexer_shape of the whole NDIndexer"
                }
            ],
            "whole_deleted": "-          if value + (idx.size - 1) * idx.stride >= s:\n-            raise ValueError(\n-                f\"Out of bound slice: start={value}, size={idx.size},\"\n-                f\" stride={idx.stride}, dim={s}.\"\n-            )\n",
            "whole_added": "+          if size := _maybe_concretize(idx.size):\n+            if value + (size - 1) * idx.stride >= s:\n+              raise ValueError(\n+                  f\"Out of bound slice: start={value}, size={size},\"\n+                  f\" stride={idx.stride}, dim={s}.\"\n+              )\n",
            "whole_hunk": "@@ -139,11 +139,12 @@ class NDIndexer:\n         if value := _maybe_concretize(start):\n           if value >= s:\n             raise ValueError(f\"Out of bound slice: start={value}, dim={s}.\")\n-          if value + (idx.size - 1) * idx.stride >= s:\n-            raise ValueError(\n-                f\"Out of bound slice: start={value}, size={idx.size},\"\n-                f\" stride={idx.stride}, dim={s}.\"\n-            )\n+          if size := _maybe_concretize(idx.size):\n+            if value + (size - 1) * idx.stride >= s:\n+              raise ValueError(\n+                  f\"Out of bound slice: start={value}, size={size},\"\n+                  f\" stride={idx.stride}, dim={s}.\"\n+              )\n         continue\n       # The shape of indexer integers should be broadcastable up to the\n       # int_indexer_shape of the whole NDIndexer"
        }
    ]
},
{
    "Id": 54,
    "commit_link": "https://github.com/google/jax/commit/837f0bbf6f667bacb85243e84d9a75e6b6eef1c7",
    "date": "2024-04-18T21:26:35-07:00",
    "message": "Cache the `_check_sharding` check in device_put. If aval and sharding are the same, no need to check multiple times\n\nPiperOrigin-RevId: 626244240",
    "changes": [
        {
            "name": "api.py",
            "path": "jax/_src/api.py",
            "patches": [
                {
                    "old_start": 24,
                    "old_length": 7,
                    "new_start": 24,
                    "new_length": 7,
                    "hunk": "@@ -24,7 +24,7 @@ from __future__ import annotations\n \n import collections\n from collections.abc import Generator, Hashable, Iterable, Sequence\n-from functools import partial\n+from functools import partial, lru_cache\n import inspect\n import math\n import typing\n"
                },
                {
                    "old_start": 2451,
                    "old_length": 9,
                    "new_start": 2451,
                    "new_length": 9,
                    "hunk": "@@ -2451,9 +2451,9 @@ def _infer_src_sharding(src, x) -> Sharding | None:\n \n # TODO(yashkatariya): Generalize is_compatible_aval (maybe renamed) and use that\n # to check if shardings are compatible with the input.\n-def _check_sharding(x, s):\n+@lru_cache(maxsize=2048)\n+def _check_sharding(aval, s):\n   if isinstance(s, Sharding):\n-    aval = shaped_abstractify(x)\n     if isinstance(aval, core.AbstractToken):\n       aval = core.token_shaped_array\n     if isinstance(s, XLACompatibleSharding) and not isinstance(s, PmapSharding):\n"
                },
                {
                    "old_start": 2494,
                    "old_length": 7,
                    "new_start": 2494,
                    "new_length": 7,
                    "hunk": "@@ -2494,7 +2494,7 @@ def device_put(\n         (src is None or\n          isinstance(src, (xc.Device, Sharding, TransferToMemoryKind)))):\n       for leaf in tree_leaves(x):\n-        _check_sharding(leaf, s=device)\n+        _check_sharding(shaped_abstractify(leaf), s=device)\n       return tree_map(\n           lambda y: dispatch.device_put_p.bind(\n               y, device=device, src=_infer_src_sharding(src, y)), x)\n"
                },
                {
                    "old_start": 2503,
                    "old_length": 7,
                    "new_start": 2503,
                    "new_length": 7,
                    "hunk": "@@ -2503,7 +2503,7 @@ def device_put(\n     device_flat = flatten_axes(\"device_put device\", treedef, device)\n     src_flat = flatten_axes(\"device_put source\", treedef, src)\n     for x_leaf, device_leaf in zip(x_flat, device_flat):\n-      _check_sharding(x_leaf, device_leaf)\n+      _check_sharding(shaped_abstractify(x_leaf), device_leaf)\n     out_flat = [\n         dispatch.device_put_p.bind(xf, device=d, src=_infer_src_sharding(s, xf))\n         for xf, d, s in zip(x_flat, device_flat, src_flat)"
                }
            ],
            "whole_deleted": "-from functools import partial\n-def _check_sharding(x, s):\n-    aval = shaped_abstractify(x)\n-        _check_sharding(leaf, s=device)\n-      _check_sharding(x_leaf, device_leaf)\n",
            "whole_added": "+from functools import partial, lru_cache\n+@lru_cache(maxsize=2048)\n+def _check_sharding(aval, s):\n+        _check_sharding(shaped_abstractify(leaf), s=device)\n+      _check_sharding(shaped_abstractify(x_leaf), device_leaf)\n",
            "whole_hunk": "@@ -24,7 +24,7 @@ from __future__ import annotations\n \n import collections\n from collections.abc import Generator, Hashable, Iterable, Sequence\n-from functools import partial\n+from functools import partial, lru_cache\n import inspect\n import math\n import typing\n@@ -2451,9 +2451,9 @@ def _infer_src_sharding(src, x) -> Sharding | None:\n \n # TODO(yashkatariya): Generalize is_compatible_aval (maybe renamed) and use that\n # to check if shardings are compatible with the input.\n-def _check_sharding(x, s):\n+@lru_cache(maxsize=2048)\n+def _check_sharding(aval, s):\n   if isinstance(s, Sharding):\n-    aval = shaped_abstractify(x)\n     if isinstance(aval, core.AbstractToken):\n       aval = core.token_shaped_array\n     if isinstance(s, XLACompatibleSharding) and not isinstance(s, PmapSharding):\n@@ -2494,7 +2494,7 @@ def device_put(\n         (src is None or\n          isinstance(src, (xc.Device, Sharding, TransferToMemoryKind)))):\n       for leaf in tree_leaves(x):\n-        _check_sharding(leaf, s=device)\n+        _check_sharding(shaped_abstractify(leaf), s=device)\n       return tree_map(\n           lambda y: dispatch.device_put_p.bind(\n               y, device=device, src=_infer_src_sharding(src, y)), x)\n@@ -2503,7 +2503,7 @@ def device_put(\n     device_flat = flatten_axes(\"device_put device\", treedef, device)\n     src_flat = flatten_axes(\"device_put source\", treedef, src)\n     for x_leaf, device_leaf in zip(x_flat, device_flat):\n-      _check_sharding(x_leaf, device_leaf)\n+      _check_sharding(shaped_abstractify(x_leaf), device_leaf)\n     out_flat = [\n         dispatch.device_put_p.bind(xf, device=d, src=_infer_src_sharding(s, xf))\n         for xf, d, s in zip(x_flat, device_flat, src_flat)"
        }
    ]
},
{
    "Id": 55,
    "commit_link": "https://github.com/google/jax/commit/7cb0e601de85e5b949ed07b0969cb658052e09ac",
    "date": "2024-04-17T12:08:49-07:00",
    "message": "Remove the spmd_mode check from OSS JAX since enhanced barrier is switched on for OSS JAX\n\nPiperOrigin-RevId: 625763988",
    "changes": [
        {
            "name": "pxla.py",
            "path": "jax/_src/interpreters/pxla.py",
            "patches": [
                {
                    "old_start": 2037,
                    "old_length": 6,
                    "new_start": 2037,
                    "new_length": 12,
                    "hunk": "@@ -2037,6 +2037,12 @@ def to_gspmd_sharding(s: sharding_impls.XLACompatibleSharding,\n                          memory_kind=s.memory_kind)\n \n \n+# Dummy function which is a no-op in OSS since enhanced barrier is switched on\n+# in OSS.\n+def spmd_mode_check(da_object, inline):\n+  return\n+\n+\n @profiler.annotate_function\n def lower_sharding_computation(\n     closed_jaxpr: core.ClosedJaxpr,\n"
                },
                {
                    "old_start": 2127,
                    "old_length": 19,
                    "new_start": 2133,
                    "new_length": 7,
                    "hunk": "@@ -2127,19 +2133,7 @@ def lower_sharding_computation(\n       da_object,\n       it.chain(in_shardings, out_shardings, [js for js, _ in jaxpr_sharding]))  # type: ignore\n \n-  if not da_object.is_fully_addressable:  # type: ignore\n-    if inline and config.spmd_mode.value != 'allow_all':\n-      raise RuntimeError(\n-          \"Running operations on `Array`s that are not fully addressable by this \"\n-          \"process (i.e. `Array`s with data sharded across multiple devices and \"\n-          \"processes.) is dangerous. It\u2019s very important that all processes run \"\n-          \"the same cross-process computations in the same order otherwise it \"\n-          \"can lead to hangs. \"\n-          \"If you\u2019re not already familiar with JAX\u2019s multi-process \"\n-          \"programming model, please read \"\n-          \"https://jax.readthedocs.io/en/latest/multi_process.html. \"\n-          \"To fix this error, run your `jitted` computation inside \"\n-          \"`with jax.spmd_mode('allow_all'):` context manager.\")\n+  spmd_mode_check(da_object, inline)\n \n   # 2. Build up the HLO\n   semantic_in_shardings = SemanticallyEqualShardings("
                }
            ],
            "whole_deleted": "-  if not da_object.is_fully_addressable:  # type: ignore\n-    if inline and config.spmd_mode.value != 'allow_all':\n-      raise RuntimeError(\n-          \"Running operations on `Array`s that are not fully addressable by this \"\n-          \"process (i.e. `Array`s with data sharded across multiple devices and \"\n-          \"processes.) is dangerous. It\u2019s very important that all processes run \"\n-          \"the same cross-process computations in the same order otherwise it \"\n-          \"can lead to hangs. \"\n-          \"If you\u2019re not already familiar with JAX\u2019s multi-process \"\n-          \"programming model, please read \"\n-          \"https://jax.readthedocs.io/en/latest/multi_process.html. \"\n-          \"To fix this error, run your `jitted` computation inside \"\n-          \"`with jax.spmd_mode('allow_all'):` context manager.\")\n",
            "whole_added": "+# Dummy function which is a no-op in OSS since enhanced barrier is switched on\n+# in OSS.\n+def spmd_mode_check(da_object, inline):\n+  return\n+\n+\n+  spmd_mode_check(da_object, inline)\n",
            "whole_hunk": "@@ -2037,6 +2037,12 @@ def to_gspmd_sharding(s: sharding_impls.XLACompatibleSharding,\n                          memory_kind=s.memory_kind)\n \n \n+# Dummy function which is a no-op in OSS since enhanced barrier is switched on\n+# in OSS.\n+def spmd_mode_check(da_object, inline):\n+  return\n+\n+\n @profiler.annotate_function\n def lower_sharding_computation(\n     closed_jaxpr: core.ClosedJaxpr,\n@@ -2127,19 +2133,7 @@ def lower_sharding_computation(\n       da_object,\n       it.chain(in_shardings, out_shardings, [js for js, _ in jaxpr_sharding]))  # type: ignore\n \n-  if not da_object.is_fully_addressable:  # type: ignore\n-    if inline and config.spmd_mode.value != 'allow_all':\n-      raise RuntimeError(\n-          \"Running operations on `Array`s that are not fully addressable by this \"\n-          \"process (i.e. `Array`s with data sharded across multiple devices and \"\n-          \"processes.) is dangerous. It\u2019s very important that all processes run \"\n-          \"the same cross-process computations in the same order otherwise it \"\n-          \"can lead to hangs. \"\n-          \"If you\u2019re not already familiar with JAX\u2019s multi-process \"\n-          \"programming model, please read \"\n-          \"https://jax.readthedocs.io/en/latest/multi_process.html. \"\n-          \"To fix this error, run your `jitted` computation inside \"\n-          \"`with jax.spmd_mode('allow_all'):` context manager.\")\n+  spmd_mode_check(da_object, inline)\n \n   # 2. Build up the HLO\n   semantic_in_shardings = SemanticallyEqualShardings("
        }
    ]
},
{
    "Id": 56,
    "commit_link": "https://github.com/google/jax/commit/192e2b6ce9176e302e3ecab6f8e154dcfa857602",
    "date": "2024-04-15T17:13:35-07:00",
    "message": "relax a side-effects test that was erroneously checking against a canonical order\n\nPiperOrigin-RevId: 625131535",
    "changes": [
        {
            "name": "python_callback_test.py",
            "path": "tests/python_callback_test.py",
            "patches": [
                {
                    "old_start": 1367,
                    "old_length": 9,
                    "new_start": 1367,
                    "new_length": 8,
                    "hunk": "@@ -1367,9 +1367,8 @@ class IOCallbackTest(jtu.JaxTestCase):\n         return i + 1\n       jax.lax.while_loop(lambda i: i < 2, body, 0)\n \n-    jax.vmap(f)(jnp.arange(3.))\n+    jax.vmap(f)(jnp.arange(3.))  # don't crash\n     jax.effects_barrier()\n-    self.assertAllClose(x_lst, [0., 1., 2., 0., 2., 4.] * 2, check_dtypes=False)\n \n \n if __name__ == \"__main__\":"
                }
            ],
            "whole_deleted": "-    jax.vmap(f)(jnp.arange(3.))\n-    self.assertAllClose(x_lst, [0., 1., 2., 0., 2., 4.] * 2, check_dtypes=False)\n",
            "whole_added": "+    jax.vmap(f)(jnp.arange(3.))  # don't crash\n",
            "whole_hunk": "@@ -1367,9 +1367,8 @@ class IOCallbackTest(jtu.JaxTestCase):\n         return i + 1\n       jax.lax.while_loop(lambda i: i < 2, body, 0)\n \n-    jax.vmap(f)(jnp.arange(3.))\n+    jax.vmap(f)(jnp.arange(3.))  # don't crash\n     jax.effects_barrier()\n-    self.assertAllClose(x_lst, [0., 1., 2., 0., 2., 4.] * 2, check_dtypes=False)\n \n \n if __name__ == \"__main__\":"
        }
    ]
},
{
    "Id": 57,
    "commit_link": "https://github.com/google/jax/commit/2be72052ae5e04f11bff61b0133f54997f825a17",
    "date": "2024-04-11T07:32:50-07:00",
    "message": "lax.Precision now uses _missing_ to handle aliases\n\nNote that pytype does not support _missing_, so unfortunately we still have\nto have a separate definition for type checkers for the time being.\n\nPiperOrigin-RevId: 623820876",
    "changes": [
        {
            "name": "lax.py",
            "path": "jax/_src/lax/lax.py",
            "patches": [
                {
                    "old_start": 22,
                    "old_length": 7,
                    "new_start": 22,
                    "new_length": 7,
                    "hunk": "@@ -22,7 +22,7 @@ from functools import partial\n import itertools\n import math\n import operator\n-from typing import Any, Callable, TypeVar, Union, cast as type_cast, overload, TYPE_CHECKING\n+from typing import Any, Callable, ClassVar, TypeVar, Union, cast as type_cast, overload, TYPE_CHECKING\n import warnings\n \n import numpy as np\n"
                },
                {
                    "old_start": 624,
                    "old_length": 14,
                    "new_start": 624,
                    "new_length": 14,
                    "hunk": "@@ -624,14 +624,14 @@ def concatenate(operands: Array | Sequence[ArrayLike], dimension: int) -> Array:\n \n _precision_strings: dict[Any, Precision] = {}\n \n-# TODO(b/328046715): pytype appears unable to handle overriding __new__ in an\n-# enum class. Doing this crashes Pytype. For now, just write an explicit type\n-# for type checkers.\n+# TODO(b/333851820): pytype does not properly handle _missing_ in enums.\n+# We work around that by defining `Precision` as a normal class.\n if TYPE_CHECKING:\n+\n   class Precision:\n-    DEFAULT: Precision\n-    HIGH: Precision\n-    HIGHEST: Precision\n+    DEFAULT: ClassVar[Precision]\n+    HIGH: ClassVar[Precision]\n+    HIGHEST: ClassVar[Precision]\n \n     def __new__(cls, value: Precision | int | str | None) -> Precision:\n       raise NotImplementedError\n"
                },
                {
                    "old_start": 667,
                    "old_length": 19,
                    "new_start": 669,
                    "new_length": 16,
                    "hunk": "@@ -667,19 +669,16 @@ else:\n     HIGH = 1\n     HIGHEST = 2\n \n+    @classmethod\n+    def _missing_(cls, value: object) -> Precision | None:\n+      return _precision_strings.get(value)\n+\n     def __repr__(self) -> str:\n-      return f\"{self.__class__.__name__}.{self.name}\"\n+      return f'{self.__class__.__name__}.{self.name}'\n \n     def __str__(self) -> str:\n       return self.name\n \n-  # You can't define __new__ on an enum class directly, but you can monkey-patch\n-  # it after the fact. Another way to do this might be using a metaclass.\n-  def _precision_new(cls, value: Precision | int | str | None) -> Precision:\n-    return super(Precision, cls).__new__(cls, _precision_strings.get(value, value))\n-\n-  Precision.__new__ = _precision_new\n-\n \n _precision_strings['highest'] = Precision.HIGHEST\n _precision_strings['float32'] = Precision.HIGHEST"
                }
            ],
            "whole_deleted": "-from typing import Any, Callable, TypeVar, Union, cast as type_cast, overload, TYPE_CHECKING\n-# TODO(b/328046715): pytype appears unable to handle overriding __new__ in an\n-# enum class. Doing this crashes Pytype. For now, just write an explicit type\n-# for type checkers.\n-    DEFAULT: Precision\n-    HIGH: Precision\n-    HIGHEST: Precision\n-      return f\"{self.__class__.__name__}.{self.name}\"\n-  # You can't define __new__ on an enum class directly, but you can monkey-patch\n-  # it after the fact. Another way to do this might be using a metaclass.\n-  def _precision_new(cls, value: Precision | int | str | None) -> Precision:\n-    return super(Precision, cls).__new__(cls, _precision_strings.get(value, value))\n-\n-  Precision.__new__ = _precision_new\n-\n",
            "whole_added": "+from typing import Any, Callable, ClassVar, TypeVar, Union, cast as type_cast, overload, TYPE_CHECKING\n+# TODO(b/333851820): pytype does not properly handle _missing_ in enums.\n+# We work around that by defining `Precision` as a normal class.\n+\n+    DEFAULT: ClassVar[Precision]\n+    HIGH: ClassVar[Precision]\n+    HIGHEST: ClassVar[Precision]\n+    @classmethod\n+    def _missing_(cls, value: object) -> Precision | None:\n+      return _precision_strings.get(value)\n+\n+      return f'{self.__class__.__name__}.{self.name}'\n",
            "whole_hunk": "@@ -22,7 +22,7 @@ from functools import partial\n import itertools\n import math\n import operator\n-from typing import Any, Callable, TypeVar, Union, cast as type_cast, overload, TYPE_CHECKING\n+from typing import Any, Callable, ClassVar, TypeVar, Union, cast as type_cast, overload, TYPE_CHECKING\n import warnings\n \n import numpy as np\n@@ -624,14 +624,14 @@ def concatenate(operands: Array | Sequence[ArrayLike], dimension: int) -> Array:\n \n _precision_strings: dict[Any, Precision] = {}\n \n-# TODO(b/328046715): pytype appears unable to handle overriding __new__ in an\n-# enum class. Doing this crashes Pytype. For now, just write an explicit type\n-# for type checkers.\n+# TODO(b/333851820): pytype does not properly handle _missing_ in enums.\n+# We work around that by defining `Precision` as a normal class.\n if TYPE_CHECKING:\n+\n   class Precision:\n-    DEFAULT: Precision\n-    HIGH: Precision\n-    HIGHEST: Precision\n+    DEFAULT: ClassVar[Precision]\n+    HIGH: ClassVar[Precision]\n+    HIGHEST: ClassVar[Precision]\n \n     def __new__(cls, value: Precision | int | str | None) -> Precision:\n       raise NotImplementedError\n@@ -667,19 +669,16 @@ else:\n     HIGH = 1\n     HIGHEST = 2\n \n+    @classmethod\n+    def _missing_(cls, value: object) -> Precision | None:\n+      return _precision_strings.get(value)\n+\n     def __repr__(self) -> str:\n-      return f\"{self.__class__.__name__}.{self.name}\"\n+      return f'{self.__class__.__name__}.{self.name}'\n \n     def __str__(self) -> str:\n       return self.name\n \n-  # You can't define __new__ on an enum class directly, but you can monkey-patch\n-  # it after the fact. Another way to do this might be using a metaclass.\n-  def _precision_new(cls, value: Precision | int | str | None) -> Precision:\n-    return super(Precision, cls).__new__(cls, _precision_strings.get(value, value))\n-\n-  Precision.__new__ = _precision_new\n-\n \n _precision_strings['highest'] = Precision.HIGHEST\n _precision_strings['float32'] = Precision.HIGHEST"
        }
    ]
},
{
    "Id": 58,
    "commit_link": "https://github.com/google/jax/commit/10dbfcf7476532fc334e7aa62755c9d93e66c95d",
    "date": "2024-04-10T17:49:16-07:00",
    "message": "Fix incorrect sequence length in batch megacore mode and enable megacore tests which were incorrectly disabled before.\n\nAlso configure sequence lengths in the unit test to cover edge cases (zero length, divisible/non-divisible by block size).\n\nPiperOrigin-RevId: 623657472",
    "changes": [
        {
            "name": "paged_attention_kernel.py",
            "path": "jax/experimental/pallas/ops/tpu/paged_attention/paged_attention_kernel.py",
            "patches": [
                {
                    "old_start": 281,
                    "old_length": 7,
                    "new_start": 281,
                    "new_length": 14,
                    "hunk": "@@ -281,7 +281,14 @@ def paged_flash_attention_kernel_inline_seq_dim(\n     return ()\n \n   bk = pages_per_compute_block * k_pages_hbm_ref.shape[-2]\n-  lax.fori_loop(0, lax.div(lengths_ref[b] + bk - 1, bk), body, ())\n+\n+  if megacore_mode == \"batch\":\n+    num_cores = pl.num_programs(0)\n+    length = lengths_ref[b * num_cores + core_index]\n+  else:\n+    length = lengths_ref[b]\n+\n+  lax.fori_loop(0, lax.div(length + bk - 1, bk), body, ())\n \n \n @functools.partial(\n"
                },
                {
                    "old_start": 304,
                    "old_length": 7,
                    "new_start": 311,
                    "new_length": 7,
                    "hunk": "@@ -304,7 +311,7 @@ def paged_attention(\n     pages_per_compute_block: int,\n     megacore_mode: Optional[str] = None,\n     inline_seq_dim: bool = True,\n-) -> tuple[jax.Array, tuple[jax.Array, jax.Array]]:\n+) -> jax.Array:\n   \"\"\"Paged grouped query attention.\n \n   Args:\n"
                }
            ],
            "whole_deleted": "-  lax.fori_loop(0, lax.div(lengths_ref[b] + bk - 1, bk), body, ())\n-) -> tuple[jax.Array, tuple[jax.Array, jax.Array]]:\n",
            "whole_added": "+\n+  if megacore_mode == \"batch\":\n+    num_cores = pl.num_programs(0)\n+    length = lengths_ref[b * num_cores + core_index]\n+  else:\n+    length = lengths_ref[b]\n+\n+  lax.fori_loop(0, lax.div(length + bk - 1, bk), body, ())\n+) -> jax.Array:\n",
            "whole_hunk": "@@ -281,7 +281,14 @@ def paged_flash_attention_kernel_inline_seq_dim(\n     return ()\n \n   bk = pages_per_compute_block * k_pages_hbm_ref.shape[-2]\n-  lax.fori_loop(0, lax.div(lengths_ref[b] + bk - 1, bk), body, ())\n+\n+  if megacore_mode == \"batch\":\n+    num_cores = pl.num_programs(0)\n+    length = lengths_ref[b * num_cores + core_index]\n+  else:\n+    length = lengths_ref[b]\n+\n+  lax.fori_loop(0, lax.div(length + bk - 1, bk), body, ())\n \n \n @functools.partial(\n@@ -304,7 +311,7 @@ def paged_attention(\n     pages_per_compute_block: int,\n     megacore_mode: Optional[str] = None,\n     inline_seq_dim: bool = True,\n-) -> tuple[jax.Array, tuple[jax.Array, jax.Array]]:\n+) -> jax.Array:\n   \"\"\"Paged grouped query attention.\n \n   Args:\n"
        },
        {
            "name": "BUILD",
            "path": "tests/pallas/BUILD",
            "patches": [
                {
                    "old_start": 217,
                    "old_length": 6,
                    "new_start": 217,
                    "new_length": 8,
                    "hunk": "@@ -217,6 +217,8 @@ jax_test(\n     shard_count = 2,\n     tags = [\n         \"noasan\",  # Times out.\n+        \"nomsan\",  # Times out.\n+        \"notsan\",  # Times out.\n     ],\n     deps = [\n         \"//jax:pallas_tpu_ops\",\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"nomsan\",  # Times out.\n+        \"notsan\",  # Times out.\n",
            "whole_hunk": "@@ -217,6 +217,8 @@ jax_test(\n     shard_count = 2,\n     tags = [\n         \"noasan\",  # Times out.\n+        \"nomsan\",  # Times out.\n+        \"notsan\",  # Times out.\n     ],\n     deps = [\n         \"//jax:pallas_tpu_ops\",\n"
        },
        {
            "name": "paged_attention_kernel_test.py",
            "path": "tests/pallas/paged_attention_kernel_test.py",
            "patches": [
                {
                    "old_start": 83,
                    "old_length": 7,
                    "new_start": 83,
                    "new_length": 7,
                    "hunk": "@@ -83,7 +83,7 @@ def _grouped_query_attention_reference(q, k, v, lengths):\n \n \n def _megacore_enabled():\n-  return jax.devices()[0].device_kind == \"TPU V4\" and jtu.is_device_tpu(\n+  return jax.devices()[0].device_kind == \"TPU v4\" or jtu.is_device_tpu(\n       version=5, variant=\"p\"\n   )\n \n"
                },
                {
                    "old_start": 114,
                    "old_length": 15,
                    "new_start": 114,
                    "new_length": 12,
                    "hunk": "@@ -114,15 +114,12 @@ class PagedAttentionKernelTest(jtu.JaxTestCase):\n     if not jtu.is_device_tpu_at_least(4):\n       self.skipTest(\"Only supports TPU generation 4 or above\")\n     if megacore_mode and not _megacore_enabled():\n-      self.skipTest(\"Megacore is only available on TPU v4 and TPU v5p\")\n+      self.skipTest(\"Megacore is only available on TPU v4 or TPU v5p\")\n     if num_kv_heads % 2 != 0 and megacore_mode == \"kv_head\":\n       self.skipTest(\"Skip kv_head megacore mode when num_kv_heads is odd\")\n-    batch_size = 4\n     max_kv_len = 2048\n     block_size = 512\n-    seq_lens = np.asarray(\n-        [max_kv_len // batch_size * (i + 1) for i in range(batch_size)]\n-    )\n+    seq_lens = np.asarray([0, 3, 256, 513, 1023, 2048])\n     q, k_pages, v_pages, page_indices = _generate_qkv(\n         seq_lens,\n         page_size,\n"
                },
                {
                    "old_start": 151,
                    "old_length": 7,
                    "new_start": 148,
                    "new_length": 10,
                    "hunk": "@@ -151,7 +148,10 @@ class PagedAttentionKernelTest(jtu.JaxTestCase):\n     else:\n       atol, rtol = 1e-1, 1e-1\n     np.testing.assert_allclose(\n-        o.astype(jnp.float32), o_ref.astype(jnp.float32), atol=atol, rtol=rtol\n+        o[np.where(seq_lens > 0)].astype(jnp.float32),\n+        o_ref[np.where(seq_lens > 0)].astype(jnp.float32),\n+        atol=atol,\n+        rtol=rtol,\n     )\n \n "
                }
            ],
            "whole_deleted": "-  return jax.devices()[0].device_kind == \"TPU V4\" and jtu.is_device_tpu(\n-      self.skipTest(\"Megacore is only available on TPU v4 and TPU v5p\")\n-    batch_size = 4\n-    seq_lens = np.asarray(\n-        [max_kv_len // batch_size * (i + 1) for i in range(batch_size)]\n-    )\n-        o.astype(jnp.float32), o_ref.astype(jnp.float32), atol=atol, rtol=rtol\n",
            "whole_added": "+  return jax.devices()[0].device_kind == \"TPU v4\" or jtu.is_device_tpu(\n+      self.skipTest(\"Megacore is only available on TPU v4 or TPU v5p\")\n+    seq_lens = np.asarray([0, 3, 256, 513, 1023, 2048])\n+        o[np.where(seq_lens > 0)].astype(jnp.float32),\n+        o_ref[np.where(seq_lens > 0)].astype(jnp.float32),\n+        atol=atol,\n+        rtol=rtol,\n",
            "whole_hunk": "@@ -83,7 +83,7 @@ def _grouped_query_attention_reference(q, k, v, lengths):\n \n \n def _megacore_enabled():\n-  return jax.devices()[0].device_kind == \"TPU V4\" and jtu.is_device_tpu(\n+  return jax.devices()[0].device_kind == \"TPU v4\" or jtu.is_device_tpu(\n       version=5, variant=\"p\"\n   )\n \n@@ -114,15 +114,12 @@ class PagedAttentionKernelTest(jtu.JaxTestCase):\n     if not jtu.is_device_tpu_at_least(4):\n       self.skipTest(\"Only supports TPU generation 4 or above\")\n     if megacore_mode and not _megacore_enabled():\n-      self.skipTest(\"Megacore is only available on TPU v4 and TPU v5p\")\n+      self.skipTest(\"Megacore is only available on TPU v4 or TPU v5p\")\n     if num_kv_heads % 2 != 0 and megacore_mode == \"kv_head\":\n       self.skipTest(\"Skip kv_head megacore mode when num_kv_heads is odd\")\n-    batch_size = 4\n     max_kv_len = 2048\n     block_size = 512\n-    seq_lens = np.asarray(\n-        [max_kv_len // batch_size * (i + 1) for i in range(batch_size)]\n-    )\n+    seq_lens = np.asarray([0, 3, 256, 513, 1023, 2048])\n     q, k_pages, v_pages, page_indices = _generate_qkv(\n         seq_lens,\n         page_size,\n@@ -151,7 +148,10 @@ class PagedAttentionKernelTest(jtu.JaxTestCase):\n     else:\n       atol, rtol = 1e-1, 1e-1\n     np.testing.assert_allclose(\n-        o.astype(jnp.float32), o_ref.astype(jnp.float32), atol=atol, rtol=rtol\n+        o[np.where(seq_lens > 0)].astype(jnp.float32),\n+        o_ref[np.where(seq_lens > 0)].astype(jnp.float32),\n+        atol=atol,\n+        rtol=rtol,\n     )\n \n "
        }
    ]
},
{
    "Id": 59,
    "commit_link": "https://github.com/google/jax/commit/0d8eb45c2062be98059bdcbd3c7b8a2cc99c13a6",
    "date": "2024-04-09T22:12:05-07:00",
    "message": "Remove the sharding and layout checks for non-DCE'd arguments during AOT safe call.\n\nThis is because the tracing, lowering and compilation caches do not register a miss if sharding/layout of a DCE'd arg changes when it's passed again to a jitted function.\n\nThis is not true for avals so that check still exists.\n\nPiperOrigin-RevId: 623375760",
    "changes": [
        {
            "name": "pxla.py",
            "path": "jax/_src/interpreters/pxla.py",
            "patches": [
                {
                    "old_start": 1989,
                    "old_length": 10,
                    "new_start": 1989,
                    "new_length": 8,
                    "hunk": "@@ -1989,10 +1989,8 @@ MaybeLayout = Sequence[Union[DeviceLocalLayout, AutoLayout, None]]\n \n \n class AllArgsInfo(NamedTuple):\n-  \"\"\"Avals, shardings, layouts and debug_info for all arguments prior to DCE.\"\"\"\n+  \"\"\"Avals and debug_info for all arguments prior to DCE.\"\"\"\n   in_avals: Sequence[core.ShapedArray]\n-  in_shardings: Any\n-  in_layouts: Any\n   debug_info: core.JaxprDebugInfo | None\n \n \n"
                },
                {
                    "old_start": 2038,
                    "old_length": 8,
                    "new_start": 2036,
                    "new_length": 7,
                    "hunk": "@@ -2038,8 +2036,7 @@ def lower_sharding_computation(\n   auto_spmd_lowering = check_if_any_auto(\n       it.chain.from_iterable([in_shardings, out_shardings]))  # type: ignore\n \n-  all_args_info = AllArgsInfo(global_in_avals, in_shardings, in_layouts,\n-                              closed_jaxpr.jaxpr.debug_info)\n+  all_args_info = AllArgsInfo(global_in_avals, closed_jaxpr.jaxpr.debug_info)\n \n   (closed_jaxpr, global_in_avals, global_out_avals, donated_invars,\n    kept_var_idx, name_stack) = _dce_jaxpr(\n"
                },
                {
                    "old_start": 3013,
                    "old_length": 28,
                    "new_start": 3010,
                    "new_length": 22,
                    "hunk": "@@ -3013,28 +3010,22 @@ class MeshExecutable(stages.XlaExecutable):\n     return self.xla_executable\n \n   def call(self, *args):\n+    args_after_dce = [a for i, a in enumerate(args) if i in self._kept_var_idx]\n     if self._all_args_info is None:\n-      kept_args = [a for i, a in enumerate(args) if i in self._kept_var_idx]\n+      kept_args = args_after_dce\n       ref_avals = self.in_avals\n-      in_shardings = self._in_shardings\n-      in_layouts = self._in_layouts\n       debug_info = None\n     else:\n       kept_args = args\n       ref_avals = self._all_args_info.in_avals\n-      iter_in_shardings = iter(self._in_shardings)\n-      in_shardings = [next(iter_in_shardings) if i in self._kept_var_idx else s\n-                      for i, s in enumerate(self._all_args_info.in_shardings)]\n-      iter_in_layouts = iter(self._in_layouts)\n-      in_layouts = [next(iter_in_layouts) if i in self._kept_var_idx else s\n-                    for i, s in enumerate(self._all_args_info.in_layouts)]\n       debug_info = self._all_args_info.debug_info\n \n-    arg_avals = map(xla.abstractify, kept_args)\n-    check_arg_avals_for_call(ref_avals, arg_avals, debug_info)\n+    all_arg_avals = map(xla.abstractify, kept_args)\n+    check_arg_avals_for_call(ref_avals, all_arg_avals, debug_info)\n     # Check the GDA sharding and the input sharding.\n-    check_array_xla_sharding_layout_match(kept_args, in_shardings,\n-                                          in_layouts, debug_info)\n+    check_array_xla_sharding_layout_match(\n+        args_after_dce, self._in_shardings, self._in_layouts, debug_info,\n+        self._kept_var_idx)\n     return self.unsafe_call(*args)  # pylint: disable=not-callable\n \n   def input_shardings(self) -> Sequence[sharding_impls.XLACompatibleSharding]:\n"
                },
                {
                    "old_start": 3163,
                    "old_length": 16,
                    "new_start": 3154,
                    "new_length": 22,
                    "hunk": "@@ -3163,16 +3154,22 @@ def check_device_backend_on_shardings(shardings) -> bool:\n \n \n def check_array_xla_sharding_layout_match(\n-    args, in_xla_shardings: Sequence[sharding_impls.XLACompatibleSharding],\n+    args_after_dce,\n+    in_xla_shardings: Sequence[sharding_impls.XLACompatibleSharding],\n     in_xla_layouts: Sequence[DeviceLocalLayout],\n-    jaxpr_debug_info: core.JaxprDebugInfo | None) -> None:\n+    jaxpr_debug_info: core.JaxprDebugInfo | None,\n+    kept_var_idx: set[int]) -> None:\n   from jax._src.array import ArrayImpl\n-  arg_names = ([''] * len(args) if jaxpr_debug_info is None else\n-               jaxpr_debug_info.arg_names)\n+  # jaxpr_debug_info.arg_names are before DCE, so need to DCE them.\n+  arg_names = (\n+      [\"\"] * len(args_after_dce) if jaxpr_debug_info is None\n+      else [a for i, a in enumerate(jaxpr_debug_info.arg_names)  # type: ignore\n+            if i in kept_var_idx]\n+  )\n   errors = []\n   num_errors = 5\n-  for arg, xs, xl, name in safe_zip(args, in_xla_shardings, in_xla_layouts,\n-                                    arg_names):\n+  for arg, xs, xl, name in safe_zip(\n+      args_after_dce, in_xla_shardings, in_xla_layouts, arg_names):\n     if not isinstance(arg, ArrayImpl):\n       continue\n     if is_unspecified_or_auto(xs):\n"
                },
                {
                    "old_start": 3200,
                    "old_length": 7,
                    "new_start": 3197,
                    "new_length": 6,
                    "hunk": "@@ -3200,7 +3197,6 @@ def check_array_xla_sharding_layout_match(\n \n     if (xla_extension_version >= 249 and not db_xs and arg._committed and\n         arg.layout.device_local_layout is not None and xl is not None and\n-        not isinstance(xl, AutoLayout) and\n         arg.layout.device_local_layout != xl):\n       errors.append(\n           (\"Got input layout(s) that compiled object was called with: \"\n"
                }
            ],
            "whole_deleted": "-  \"\"\"Avals, shardings, layouts and debug_info for all arguments prior to DCE.\"\"\"\n-  in_shardings: Any\n-  in_layouts: Any\n-  all_args_info = AllArgsInfo(global_in_avals, in_shardings, in_layouts,\n-                              closed_jaxpr.jaxpr.debug_info)\n-      kept_args = [a for i, a in enumerate(args) if i in self._kept_var_idx]\n-      in_shardings = self._in_shardings\n-      in_layouts = self._in_layouts\n-      iter_in_shardings = iter(self._in_shardings)\n-      in_shardings = [next(iter_in_shardings) if i in self._kept_var_idx else s\n-                      for i, s in enumerate(self._all_args_info.in_shardings)]\n-      iter_in_layouts = iter(self._in_layouts)\n-      in_layouts = [next(iter_in_layouts) if i in self._kept_var_idx else s\n-                    for i, s in enumerate(self._all_args_info.in_layouts)]\n-    arg_avals = map(xla.abstractify, kept_args)\n-    check_arg_avals_for_call(ref_avals, arg_avals, debug_info)\n-    check_array_xla_sharding_layout_match(kept_args, in_shardings,\n-                                          in_layouts, debug_info)\n-    args, in_xla_shardings: Sequence[sharding_impls.XLACompatibleSharding],\n-    jaxpr_debug_info: core.JaxprDebugInfo | None) -> None:\n-  arg_names = ([''] * len(args) if jaxpr_debug_info is None else\n-               jaxpr_debug_info.arg_names)\n-  for arg, xs, xl, name in safe_zip(args, in_xla_shardings, in_xla_layouts,\n-                                    arg_names):\n-        not isinstance(xl, AutoLayout) and\n",
            "whole_added": "+  \"\"\"Avals and debug_info for all arguments prior to DCE.\"\"\"\n+  all_args_info = AllArgsInfo(global_in_avals, closed_jaxpr.jaxpr.debug_info)\n+    args_after_dce = [a for i, a in enumerate(args) if i in self._kept_var_idx]\n+      kept_args = args_after_dce\n+    all_arg_avals = map(xla.abstractify, kept_args)\n+    check_arg_avals_for_call(ref_avals, all_arg_avals, debug_info)\n+    check_array_xla_sharding_layout_match(\n+        args_after_dce, self._in_shardings, self._in_layouts, debug_info,\n+        self._kept_var_idx)\n+    args_after_dce,\n+    in_xla_shardings: Sequence[sharding_impls.XLACompatibleSharding],\n+    jaxpr_debug_info: core.JaxprDebugInfo | None,\n+    kept_var_idx: set[int]) -> None:\n+  # jaxpr_debug_info.arg_names are before DCE, so need to DCE them.\n+  arg_names = (\n+      [\"\"] * len(args_after_dce) if jaxpr_debug_info is None\n+      else [a for i, a in enumerate(jaxpr_debug_info.arg_names)  # type: ignore\n+            if i in kept_var_idx]\n+  )\n+  for arg, xs, xl, name in safe_zip(\n+      args_after_dce, in_xla_shardings, in_xla_layouts, arg_names):\n",
            "whole_hunk": "@@ -1989,10 +1989,8 @@ MaybeLayout = Sequence[Union[DeviceLocalLayout, AutoLayout, None]]\n \n \n class AllArgsInfo(NamedTuple):\n-  \"\"\"Avals, shardings, layouts and debug_info for all arguments prior to DCE.\"\"\"\n+  \"\"\"Avals and debug_info for all arguments prior to DCE.\"\"\"\n   in_avals: Sequence[core.ShapedArray]\n-  in_shardings: Any\n-  in_layouts: Any\n   debug_info: core.JaxprDebugInfo | None\n \n \n@@ -2038,8 +2036,7 @@ def lower_sharding_computation(\n   auto_spmd_lowering = check_if_any_auto(\n       it.chain.from_iterable([in_shardings, out_shardings]))  # type: ignore\n \n-  all_args_info = AllArgsInfo(global_in_avals, in_shardings, in_layouts,\n-                              closed_jaxpr.jaxpr.debug_info)\n+  all_args_info = AllArgsInfo(global_in_avals, closed_jaxpr.jaxpr.debug_info)\n \n   (closed_jaxpr, global_in_avals, global_out_avals, donated_invars,\n    kept_var_idx, name_stack) = _dce_jaxpr(\n@@ -3013,28 +3010,22 @@ class MeshExecutable(stages.XlaExecutable):\n     return self.xla_executable\n \n   def call(self, *args):\n+    args_after_dce = [a for i, a in enumerate(args) if i in self._kept_var_idx]\n     if self._all_args_info is None:\n-      kept_args = [a for i, a in enumerate(args) if i in self._kept_var_idx]\n+      kept_args = args_after_dce\n       ref_avals = self.in_avals\n-      in_shardings = self._in_shardings\n-      in_layouts = self._in_layouts\n       debug_info = None\n     else:\n       kept_args = args\n       ref_avals = self._all_args_info.in_avals\n-      iter_in_shardings = iter(self._in_shardings)\n-      in_shardings = [next(iter_in_shardings) if i in self._kept_var_idx else s\n-                      for i, s in enumerate(self._all_args_info.in_shardings)]\n-      iter_in_layouts = iter(self._in_layouts)\n-      in_layouts = [next(iter_in_layouts) if i in self._kept_var_idx else s\n-                    for i, s in enumerate(self._all_args_info.in_layouts)]\n       debug_info = self._all_args_info.debug_info\n \n-    arg_avals = map(xla.abstractify, kept_args)\n-    check_arg_avals_for_call(ref_avals, arg_avals, debug_info)\n+    all_arg_avals = map(xla.abstractify, kept_args)\n+    check_arg_avals_for_call(ref_avals, all_arg_avals, debug_info)\n     # Check the GDA sharding and the input sharding.\n-    check_array_xla_sharding_layout_match(kept_args, in_shardings,\n-                                          in_layouts, debug_info)\n+    check_array_xla_sharding_layout_match(\n+        args_after_dce, self._in_shardings, self._in_layouts, debug_info,\n+        self._kept_var_idx)\n     return self.unsafe_call(*args)  # pylint: disable=not-callable\n \n   def input_shardings(self) -> Sequence[sharding_impls.XLACompatibleSharding]:\n@@ -3163,16 +3154,22 @@ def check_device_backend_on_shardings(shardings) -> bool:\n \n \n def check_array_xla_sharding_layout_match(\n-    args, in_xla_shardings: Sequence[sharding_impls.XLACompatibleSharding],\n+    args_after_dce,\n+    in_xla_shardings: Sequence[sharding_impls.XLACompatibleSharding],\n     in_xla_layouts: Sequence[DeviceLocalLayout],\n-    jaxpr_debug_info: core.JaxprDebugInfo | None) -> None:\n+    jaxpr_debug_info: core.JaxprDebugInfo | None,\n+    kept_var_idx: set[int]) -> None:\n   from jax._src.array import ArrayImpl\n-  arg_names = ([''] * len(args) if jaxpr_debug_info is None else\n-               jaxpr_debug_info.arg_names)\n+  # jaxpr_debug_info.arg_names are before DCE, so need to DCE them.\n+  arg_names = (\n+      [\"\"] * len(args_after_dce) if jaxpr_debug_info is None\n+      else [a for i, a in enumerate(jaxpr_debug_info.arg_names)  # type: ignore\n+            if i in kept_var_idx]\n+  )\n   errors = []\n   num_errors = 5\n-  for arg, xs, xl, name in safe_zip(args, in_xla_shardings, in_xla_layouts,\n-                                    arg_names):\n+  for arg, xs, xl, name in safe_zip(\n+      args_after_dce, in_xla_shardings, in_xla_layouts, arg_names):\n     if not isinstance(arg, ArrayImpl):\n       continue\n     if is_unspecified_or_auto(xs):\n@@ -3200,7 +3197,6 @@ def check_array_xla_sharding_layout_match(\n \n     if (xla_extension_version >= 249 and not db_xs and arg._committed and\n         arg.layout.device_local_layout is not None and xl is not None and\n-        not isinstance(xl, AutoLayout) and\n         arg.layout.device_local_layout != xl):\n       errors.append(\n           (\"Got input layout(s) that compiled object was called with: \"\n"
        },
        {
            "name": "pjit.py",
            "path": "jax/_src/pjit.py",
            "patches": [
                {
                    "old_start": 1486,
                    "old_length": 7,
                    "new_start": 1486,
                    "new_length": 7,
                    "hunk": "@@ -1486,7 +1486,7 @@ def _pjit_call_impl_python(\n   if compiled._auto_spmd_lowering and config.enable_checks.value:\n     pxla.check_array_xla_sharding_layout_match(\n         args, compiled._in_shardings, compiled._in_layouts,\n-        jaxpr.jaxpr.debug_info)\n+        jaxpr.jaxpr.debug_info, compiled._kept_var_idx)\n   if config.distributed_debug.value:\n     # Defensively only perform fingerprint logic if debug logging is enabled\n     # NOTE(skyewm): I didn't benchmark this\n"
                }
            ],
            "whole_deleted": "-        jaxpr.jaxpr.debug_info)\n",
            "whole_added": "+        jaxpr.jaxpr.debug_info, compiled._kept_var_idx)\n",
            "whole_hunk": "@@ -1486,7 +1486,7 @@ def _pjit_call_impl_python(\n   if compiled._auto_spmd_lowering and config.enable_checks.value:\n     pxla.check_array_xla_sharding_layout_match(\n         args, compiled._in_shardings, compiled._in_layouts,\n-        jaxpr.jaxpr.debug_info)\n+        jaxpr.jaxpr.debug_info, compiled._kept_var_idx)\n   if config.distributed_debug.value:\n     # Defensively only perform fingerprint logic if debug logging is enabled\n     # NOTE(skyewm): I didn't benchmark this\n"
        },
        {
            "name": "pjit_test.py",
            "path": "tests/pjit_test.py",
            "patches": [
                {
                    "old_start": 4368,
                    "old_length": 32,
                    "new_start": 4368,
                    "new_length": 6,
                    "hunk": "@@ -4368,32 +4368,6 @@ class PJitErrorTest(jtu.JaxTestCase):\n         ' compiled'):\n       g(x, y2)\n \n-  def test_aot_error_on_dced_shardings_mismatch(self):\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n-    shape = (8, 2)\n-    np_inp = np.arange(math.prod(shape)).reshape(shape)\n-\n-    x = jax.device_put(np_inp, NamedSharding(mesh, P('x', 'y')))\n-    y1 = jax.device_put(np_inp, NamedSharding(mesh, P('x')))\n-    y2 = jax.device_put(np_inp, NamedSharding(mesh, P('y')))\n-\n-    @jax.jit\n-    def f(x, y):\n-      return x + 1\n-\n-    f_out1 = f(x, y1)\n-    f(x, y2)\n-\n-    g = f.lower(x, y1).compile()\n-    g_out1 = g(x, y1)\n-    self.assertArraysEqual(f_out1, g_out1)\n-\n-    with self.assertRaisesRegex(\n-        ValueError,\n-        r\"Compiled object called with input sharding.*does not match the \"\n-        r\"sharding.*the computation was compiled with\"):\n-      g(x, y2)\n-\n   def test_dce_no_array(self):\n     mesh = jtu.create_global_mesh((2,), ('x',))\n     arr = jax.device_put(np.arange(8.), NamedSharding(mesh, P('x')))"
                }
            ],
            "whole_deleted": "-  def test_aot_error_on_dced_shardings_mismatch(self):\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n-    shape = (8, 2)\n-    np_inp = np.arange(math.prod(shape)).reshape(shape)\n-\n-    x = jax.device_put(np_inp, NamedSharding(mesh, P('x', 'y')))\n-    y1 = jax.device_put(np_inp, NamedSharding(mesh, P('x')))\n-    y2 = jax.device_put(np_inp, NamedSharding(mesh, P('y')))\n-\n-    @jax.jit\n-    def f(x, y):\n-      return x + 1\n-\n-    f_out1 = f(x, y1)\n-    f(x, y2)\n-\n-    g = f.lower(x, y1).compile()\n-    g_out1 = g(x, y1)\n-    self.assertArraysEqual(f_out1, g_out1)\n-\n-    with self.assertRaisesRegex(\n-        ValueError,\n-        r\"Compiled object called with input sharding.*does not match the \"\n-        r\"sharding.*the computation was compiled with\"):\n-      g(x, y2)\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -4368,32 +4368,6 @@ class PJitErrorTest(jtu.JaxTestCase):\n         ' compiled'):\n       g(x, y2)\n \n-  def test_aot_error_on_dced_shardings_mismatch(self):\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n-    shape = (8, 2)\n-    np_inp = np.arange(math.prod(shape)).reshape(shape)\n-\n-    x = jax.device_put(np_inp, NamedSharding(mesh, P('x', 'y')))\n-    y1 = jax.device_put(np_inp, NamedSharding(mesh, P('x')))\n-    y2 = jax.device_put(np_inp, NamedSharding(mesh, P('y')))\n-\n-    @jax.jit\n-    def f(x, y):\n-      return x + 1\n-\n-    f_out1 = f(x, y1)\n-    f(x, y2)\n-\n-    g = f.lower(x, y1).compile()\n-    g_out1 = g(x, y1)\n-    self.assertArraysEqual(f_out1, g_out1)\n-\n-    with self.assertRaisesRegex(\n-        ValueError,\n-        r\"Compiled object called with input sharding.*does not match the \"\n-        r\"sharding.*the computation was compiled with\"):\n-      g(x, y2)\n-\n   def test_dce_no_array(self):\n     mesh = jtu.create_global_mesh((2,), ('x',))\n     arr = jax.device_put(np.arange(8.), NamedSharding(mesh, P('x')))"
        }
    ]
},
{
    "Id": 60,
    "commit_link": "https://github.com/google/jax/commit/69bf3b866cd75551e01e801bcb97b413c4d065ba",
    "date": "2024-04-09T19:34:25-07:00",
    "message": "Don't do layout checks during compiled safe call on DCE'd args.\n\nPiperOrigin-RevId: 623347380",
    "changes": [
        {
            "name": "pxla.py",
            "path": "jax/_src/interpreters/pxla.py",
            "patches": [
                {
                    "old_start": 3200,
                    "old_length": 6,
                    "new_start": 3200,
                    "new_length": 7,
                    "hunk": "@@ -3200,6 +3200,7 @@ def check_array_xla_sharding_layout_match(\n \n     if (xla_extension_version >= 249 and not db_xs and arg._committed and\n         arg.layout.device_local_layout is not None and xl is not None and\n+        not isinstance(xl, AutoLayout) and\n         arg.layout.device_local_layout != xl):\n       errors.append(\n           (\"Got input layout(s) that compiled object was called with: \"\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        not isinstance(xl, AutoLayout) and\n",
            "whole_hunk": "@@ -3200,6 +3200,7 @@ def check_array_xla_sharding_layout_match(\n \n     if (xla_extension_version >= 249 and not db_xs and arg._committed and\n         arg.layout.device_local_layout is not None and xl is not None and\n+        not isinstance(xl, AutoLayout) and\n         arg.layout.device_local_layout != xl):\n       errors.append(\n           (\"Got input layout(s) that compiled object was called with: \"\n"
        },
        {
            "name": "layout_test.py",
            "path": "tests/layout_test.py",
            "patches": [
                {
                    "old_start": 233,
                    "old_length": 6,
                    "new_start": 233,
                    "new_length": 25,
                    "hunk": "@@ -233,6 +233,25 @@ class LayoutTest(jtu.JaxTestCase):\n     self.assertArraysEqual(out1, out5)\n     self.assertArraysEqual(out2, out6)\n \n+  def test_no_error_dced_args(self):\n+    mesh = jtu.create_global_mesh((2, 1), ('x', 'y'))\n+    shape = (8, 2)\n+    s = NamedSharding(mesh, P('x', 'y'))\n+    np_inp = np.arange(math.prod(shape)).reshape(shape)\n+    arr1 = jax.device_put(np_inp, s)\n+    arr2 = jax.device_put(np_inp, s)\n+    arrs = [arr1, arr2]\n+\n+    def f(x, y):\n+      return x * 2\n+\n+    jf = jax.jit(f, in_shardings=Layout(DLL.AUTO, s),\n+                 out_shardings=Layout(DLL.AUTO, s))\n+    compiled = jf.lower(np_inp, np_inp).compile()\n+    arg_layouts, _ = compiled.input_layouts()\n+    arrs = [jax.device_put(i, l) for i, l in zip(arrs, arg_layouts)]\n+    compiled(*arrs)\n+\n   def test_aot_layout_mismatch(self):\n     mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n     shape = (256, 4, 2)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  def test_no_error_dced_args(self):\n+    mesh = jtu.create_global_mesh((2, 1), ('x', 'y'))\n+    shape = (8, 2)\n+    s = NamedSharding(mesh, P('x', 'y'))\n+    np_inp = np.arange(math.prod(shape)).reshape(shape)\n+    arr1 = jax.device_put(np_inp, s)\n+    arr2 = jax.device_put(np_inp, s)\n+    arrs = [arr1, arr2]\n+\n+    def f(x, y):\n+      return x * 2\n+\n+    jf = jax.jit(f, in_shardings=Layout(DLL.AUTO, s),\n+                 out_shardings=Layout(DLL.AUTO, s))\n+    compiled = jf.lower(np_inp, np_inp).compile()\n+    arg_layouts, _ = compiled.input_layouts()\n+    arrs = [jax.device_put(i, l) for i, l in zip(arrs, arg_layouts)]\n+    compiled(*arrs)\n+\n",
            "whole_hunk": "@@ -233,6 +233,25 @@ class LayoutTest(jtu.JaxTestCase):\n     self.assertArraysEqual(out1, out5)\n     self.assertArraysEqual(out2, out6)\n \n+  def test_no_error_dced_args(self):\n+    mesh = jtu.create_global_mesh((2, 1), ('x', 'y'))\n+    shape = (8, 2)\n+    s = NamedSharding(mesh, P('x', 'y'))\n+    np_inp = np.arange(math.prod(shape)).reshape(shape)\n+    arr1 = jax.device_put(np_inp, s)\n+    arr2 = jax.device_put(np_inp, s)\n+    arrs = [arr1, arr2]\n+\n+    def f(x, y):\n+      return x * 2\n+\n+    jf = jax.jit(f, in_shardings=Layout(DLL.AUTO, s),\n+                 out_shardings=Layout(DLL.AUTO, s))\n+    compiled = jf.lower(np_inp, np_inp).compile()\n+    arg_layouts, _ = compiled.input_layouts()\n+    arrs = [jax.device_put(i, l) for i, l in zip(arrs, arg_layouts)]\n+    compiled(*arrs)\n+\n   def test_aot_layout_mismatch(self):\n     mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n     shape = (256, 4, 2)"
        }
    ]
},
{
    "Id": 61,
    "commit_link": "https://github.com/google/jax/commit/7413894b86e5bc3d869b743fd5619547cf69e91b",
    "date": "2024-04-05T10:52:45-07:00",
    "message": "Merge pull request #20599 from mattjj:temp-config-to-disable-custom-vjp-shape-check\n\nPiperOrigin-RevId: 622224003",
    "changes": [
        {
            "name": "config.py",
            "path": "jax/_src/config.py",
            "patches": [
                {
                    "old_start": 1390,
                    "old_length": 6,
                    "new_start": 1390,
                    "new_length": 13,
                    "hunk": "@@ -1390,6 +1390,13 @@ eager_pmap = define_bool_state(\n     upgrade=True,\n     help='Enable eager-mode pmap when jax_disable_jit is activated.')\n \n+# TODO(mattjj): remove once we land mutable array plumbing, or face great shame\n+custom_vjp_disable_shape_check = define_bool_state(\n+    name='jax_custom_vjp_disable_shape_check',\n+    default=False,\n+    upgrade=True,\n+    help='Disable the check from #19009 to enable some custom_vjp hacks.')\n+\n xla_runtime_errors = define_bool_state(\n     name='jax_experimental_unsafe_xla_runtime_errors',\n     default=False,\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+# TODO(mattjj): remove once we land mutable array plumbing, or face great shame\n+custom_vjp_disable_shape_check = define_bool_state(\n+    name='jax_custom_vjp_disable_shape_check',\n+    default=False,\n+    upgrade=True,\n+    help='Disable the check from #19009 to enable some custom_vjp hacks.')\n+\n",
            "whole_hunk": "@@ -1390,6 +1390,13 @@ eager_pmap = define_bool_state(\n     upgrade=True,\n     help='Enable eager-mode pmap when jax_disable_jit is activated.')\n \n+# TODO(mattjj): remove once we land mutable array plumbing, or face great shame\n+custom_vjp_disable_shape_check = define_bool_state(\n+    name='jax_custom_vjp_disable_shape_check',\n+    default=False,\n+    upgrade=True,\n+    help='Disable the check from #19009 to enable some custom_vjp hacks.')\n+\n xla_runtime_errors = define_bool_state(\n     name='jax_experimental_unsafe_xla_runtime_errors',\n     default=False,\n"
        },
        {
            "name": "custom_derivatives.py",
            "path": "jax/_src/custom_derivatives.py",
            "patches": [
                {
                    "old_start": 772,
                    "old_length": 7,
                    "new_start": 772,
                    "new_length": 8,
                    "hunk": "@@ -772,7 +772,8 @@ def _flatten_bwd(in_tree, in_avals, out_trees, *args):\n       results.append(Zero(ct.aval))\n     else:\n       if (not core.typecompat(a.at_least_vspace(), a_ := core.get_aval(ct))\n-          and not _temporary_dtype_exception(a, a_)):\n+          and not (_temporary_dtype_exception(a, a_) or\n+                   _temporary_shape_exception(a, a_))):\n         msg = (\"Custom VJP bwd rule must produce an output with the same \"\n                \"shape/dtypes as the args tuple of the primal function, but at \"\n                f\"output{keystr(kp)} the bwd rule produced an output of \"\n"
                },
                {
                    "old_start": 790,
                    "old_length": 6,
                    "new_start": 791,
                    "new_length": 9,
                    "hunk": "@@ -790,6 +791,9 @@ def _temporary_dtype_exception(a, a_) -> bool:\n              dtypes.issubdtype(a.dtype, dtypes.np.inexact)))\n   return False\n \n+# TODO(mattjj): remove both these exceptions to cotangent compatibility check\n+def _temporary_shape_exception(a, a_) -> bool:\n+  return config.custom_vjp_disable_shape_check.value\n \n class CustomVJPCallPrimitive(core.CallPrimitive):\n   initial_style: core.Primitive\n"
                }
            ],
            "whole_deleted": "-          and not _temporary_dtype_exception(a, a_)):\n",
            "whole_added": "+          and not (_temporary_dtype_exception(a, a_) or\n+                   _temporary_shape_exception(a, a_))):\n+# TODO(mattjj): remove both these exceptions to cotangent compatibility check\n+def _temporary_shape_exception(a, a_) -> bool:\n+  return config.custom_vjp_disable_shape_check.value\n",
            "whole_hunk": "@@ -772,7 +772,8 @@ def _flatten_bwd(in_tree, in_avals, out_trees, *args):\n       results.append(Zero(ct.aval))\n     else:\n       if (not core.typecompat(a.at_least_vspace(), a_ := core.get_aval(ct))\n-          and not _temporary_dtype_exception(a, a_)):\n+          and not (_temporary_dtype_exception(a, a_) or\n+                   _temporary_shape_exception(a, a_))):\n         msg = (\"Custom VJP bwd rule must produce an output with the same \"\n                \"shape/dtypes as the args tuple of the primal function, but at \"\n                f\"output{keystr(kp)} the bwd rule produced an output of \"\n@@ -790,6 +791,9 @@ def _temporary_dtype_exception(a, a_) -> bool:\n              dtypes.issubdtype(a.dtype, dtypes.np.inexact)))\n   return False\n \n+# TODO(mattjj): remove both these exceptions to cotangent compatibility check\n+def _temporary_shape_exception(a, a_) -> bool:\n+  return config.custom_vjp_disable_shape_check.value\n \n class CustomVJPCallPrimitive(core.CallPrimitive):\n   initial_style: core.Primitive\n"
        },
        {
            "name": "api_test.py",
            "path": "tests/api_test.py",
            "patches": [
                {
                    "old_start": 9372,
                    "old_length": 6,
                    "new_start": 9372,
                    "new_length": 27,
                    "hunk": "@@ -9372,6 +9372,27 @@ class CustomVJPTest(jtu.JaxTestCase):\n         r'output\\[1\\] the bwd rule produced an output of shape/dtype float..\\[3\\]'):\n       jax.grad(lambda x, y: foo(x, y * y).sum(), 1)(jnp.ones(3), jnp.ones(4))\n \n+  def test_bwd_rule_shape_mismatch_disable(self):\n+    # TODO(mattjj): remove this test when the config option is removed\n+    @jax.custom_vjp\n+    def foo(x, y):\n+      return x\n+\n+    def foo_fwd(x, y):\n+      return x, None\n+\n+    def foo_bwd(_, g):\n+      return jnp.zeros(3), jnp.zeros(3)\n+\n+    foo.defvjp(foo_fwd, foo_bwd)\n+\n+    try:\n+      jax.config.update('jax_custom_vjp_disable_shape_check', True)\n+      jax.grad(lambda x, y: foo(x, y).sum(), 1)(jnp.ones(3), jnp.ones(4))\n+    finally:\n+      jax.config.update('jax_custom_vjp_disable_shape_check', False)\n+\n+\n def transpose_unary(f, x_example):\n   def transposed(y):\n     x, = api.linear_transpose(f, x_example)(y)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  def test_bwd_rule_shape_mismatch_disable(self):\n+    # TODO(mattjj): remove this test when the config option is removed\n+    @jax.custom_vjp\n+    def foo(x, y):\n+      return x\n+\n+    def foo_fwd(x, y):\n+      return x, None\n+\n+    def foo_bwd(_, g):\n+      return jnp.zeros(3), jnp.zeros(3)\n+\n+    foo.defvjp(foo_fwd, foo_bwd)\n+\n+    try:\n+      jax.config.update('jax_custom_vjp_disable_shape_check', True)\n+      jax.grad(lambda x, y: foo(x, y).sum(), 1)(jnp.ones(3), jnp.ones(4))\n+    finally:\n+      jax.config.update('jax_custom_vjp_disable_shape_check', False)\n+\n+\n",
            "whole_hunk": "@@ -9372,6 +9372,27 @@ class CustomVJPTest(jtu.JaxTestCase):\n         r'output\\[1\\] the bwd rule produced an output of shape/dtype float..\\[3\\]'):\n       jax.grad(lambda x, y: foo(x, y * y).sum(), 1)(jnp.ones(3), jnp.ones(4))\n \n+  def test_bwd_rule_shape_mismatch_disable(self):\n+    # TODO(mattjj): remove this test when the config option is removed\n+    @jax.custom_vjp\n+    def foo(x, y):\n+      return x\n+\n+    def foo_fwd(x, y):\n+      return x, None\n+\n+    def foo_bwd(_, g):\n+      return jnp.zeros(3), jnp.zeros(3)\n+\n+    foo.defvjp(foo_fwd, foo_bwd)\n+\n+    try:\n+      jax.config.update('jax_custom_vjp_disable_shape_check', True)\n+      jax.grad(lambda x, y: foo(x, y).sum(), 1)(jnp.ones(3), jnp.ones(4))\n+    finally:\n+      jax.config.update('jax_custom_vjp_disable_shape_check', False)\n+\n+\n def transpose_unary(f, x_example):\n   def transposed(y):\n     x, = api.linear_transpose(f, x_example)(y)"
        }
    ]
},
{
    "Id": 62,
    "commit_link": "https://github.com/google/jax/commit/3d4687fbfcee037f4899e12086e4ce8cad285fe0",
    "date": "2024-04-04T18:21:10-07:00",
    "message": "add a temporary config option to disable custom_vjp shape checking",
    "changes": [
        {
            "name": "config.py",
            "path": "jax/_src/config.py",
            "patches": [
                {
                    "old_start": 1390,
                    "old_length": 6,
                    "new_start": 1390,
                    "new_length": 13,
                    "hunk": "@@ -1390,6 +1390,13 @@ eager_pmap = define_bool_state(\n     upgrade=True,\n     help='Enable eager-mode pmap when jax_disable_jit is activated.')\n \n+# TODO(mattjj): remove once we land mutable array plumbing, or face great shame\n+custom_vjp_disable_shape_check = define_bool_state(\n+    name='jax_custom_vjp_disable_shape_check',\n+    default=False,\n+    upgrade=True,\n+    help='Disable the check from #19009 to enable some custom_vjp hacks.')\n+\n xla_runtime_errors = define_bool_state(\n     name='jax_experimental_unsafe_xla_runtime_errors',\n     default=False,\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+# TODO(mattjj): remove once we land mutable array plumbing, or face great shame\n+custom_vjp_disable_shape_check = define_bool_state(\n+    name='jax_custom_vjp_disable_shape_check',\n+    default=False,\n+    upgrade=True,\n+    help='Disable the check from #19009 to enable some custom_vjp hacks.')\n+\n",
            "whole_hunk": "@@ -1390,6 +1390,13 @@ eager_pmap = define_bool_state(\n     upgrade=True,\n     help='Enable eager-mode pmap when jax_disable_jit is activated.')\n \n+# TODO(mattjj): remove once we land mutable array plumbing, or face great shame\n+custom_vjp_disable_shape_check = define_bool_state(\n+    name='jax_custom_vjp_disable_shape_check',\n+    default=False,\n+    upgrade=True,\n+    help='Disable the check from #19009 to enable some custom_vjp hacks.')\n+\n xla_runtime_errors = define_bool_state(\n     name='jax_experimental_unsafe_xla_runtime_errors',\n     default=False,\n"
        },
        {
            "name": "custom_derivatives.py",
            "path": "jax/_src/custom_derivatives.py",
            "patches": [
                {
                    "old_start": 772,
                    "old_length": 7,
                    "new_start": 772,
                    "new_length": 8,
                    "hunk": "@@ -772,7 +772,8 @@ def _flatten_bwd(in_tree, in_avals, out_trees, *args):\n       results.append(Zero(ct.aval))\n     else:\n       if (not core.typecompat(a.at_least_vspace(), a_ := core.get_aval(ct))\n-          and not _temporary_dtype_exception(a, a_)):\n+          and not (_temporary_dtype_exception(a, a_) or\n+                   _temporary_shape_exception(a, a_))):\n         msg = (\"Custom VJP bwd rule must produce an output with the same \"\n                \"shape/dtypes as the args tuple of the primal function, but at \"\n                f\"output{keystr(kp)} the bwd rule produced an output of \"\n"
                },
                {
                    "old_start": 790,
                    "old_length": 6,
                    "new_start": 791,
                    "new_length": 9,
                    "hunk": "@@ -790,6 +791,9 @@ def _temporary_dtype_exception(a, a_) -> bool:\n              dtypes.issubdtype(a.dtype, dtypes.np.inexact)))\n   return False\n \n+# TODO(mattjj): remove both these exceptions to cotangent compatibility check\n+def _temporary_shape_exception(a, a_) -> bool:\n+  return config.custom_vjp_disable_shape_check.value\n \n class CustomVJPCallPrimitive(core.CallPrimitive):\n   initial_style: core.Primitive\n"
                }
            ],
            "whole_deleted": "-          and not _temporary_dtype_exception(a, a_)):\n",
            "whole_added": "+          and not (_temporary_dtype_exception(a, a_) or\n+                   _temporary_shape_exception(a, a_))):\n+# TODO(mattjj): remove both these exceptions to cotangent compatibility check\n+def _temporary_shape_exception(a, a_) -> bool:\n+  return config.custom_vjp_disable_shape_check.value\n",
            "whole_hunk": "@@ -772,7 +772,8 @@ def _flatten_bwd(in_tree, in_avals, out_trees, *args):\n       results.append(Zero(ct.aval))\n     else:\n       if (not core.typecompat(a.at_least_vspace(), a_ := core.get_aval(ct))\n-          and not _temporary_dtype_exception(a, a_)):\n+          and not (_temporary_dtype_exception(a, a_) or\n+                   _temporary_shape_exception(a, a_))):\n         msg = (\"Custom VJP bwd rule must produce an output with the same \"\n                \"shape/dtypes as the args tuple of the primal function, but at \"\n                f\"output{keystr(kp)} the bwd rule produced an output of \"\n@@ -790,6 +791,9 @@ def _temporary_dtype_exception(a, a_) -> bool:\n              dtypes.issubdtype(a.dtype, dtypes.np.inexact)))\n   return False\n \n+# TODO(mattjj): remove both these exceptions to cotangent compatibility check\n+def _temporary_shape_exception(a, a_) -> bool:\n+  return config.custom_vjp_disable_shape_check.value\n \n class CustomVJPCallPrimitive(core.CallPrimitive):\n   initial_style: core.Primitive\n"
        },
        {
            "name": "api_test.py",
            "path": "tests/api_test.py",
            "patches": [
                {
                    "old_start": 9372,
                    "old_length": 6,
                    "new_start": 9372,
                    "new_length": 27,
                    "hunk": "@@ -9372,6 +9372,27 @@ class CustomVJPTest(jtu.JaxTestCase):\n         r'output\\[1\\] the bwd rule produced an output of shape/dtype float..\\[3\\]'):\n       jax.grad(lambda x, y: foo(x, y * y).sum(), 1)(jnp.ones(3), jnp.ones(4))\n \n+  def test_bwd_rule_shape_mismatch_disable(self):\n+    # TODO(mattjj): remove this test when the config option is removed\n+    @jax.custom_vjp\n+    def foo(x, y):\n+      return x\n+\n+    def foo_fwd(x, y):\n+      return x, None\n+\n+    def foo_bwd(_, g):\n+      return jnp.zeros(3), jnp.zeros(3)\n+\n+    foo.defvjp(foo_fwd, foo_bwd)\n+\n+    try:\n+      jax.config.update('jax_custom_vjp_disable_shape_check', True)\n+      jax.grad(lambda x, y: foo(x, y).sum(), 1)(jnp.ones(3), jnp.ones(4))\n+    finally:\n+      jax.config.update('jax_custom_vjp_disable_shape_check', False)\n+\n+\n def transpose_unary(f, x_example):\n   def transposed(y):\n     x, = api.linear_transpose(f, x_example)(y)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  def test_bwd_rule_shape_mismatch_disable(self):\n+    # TODO(mattjj): remove this test when the config option is removed\n+    @jax.custom_vjp\n+    def foo(x, y):\n+      return x\n+\n+    def foo_fwd(x, y):\n+      return x, None\n+\n+    def foo_bwd(_, g):\n+      return jnp.zeros(3), jnp.zeros(3)\n+\n+    foo.defvjp(foo_fwd, foo_bwd)\n+\n+    try:\n+      jax.config.update('jax_custom_vjp_disable_shape_check', True)\n+      jax.grad(lambda x, y: foo(x, y).sum(), 1)(jnp.ones(3), jnp.ones(4))\n+    finally:\n+      jax.config.update('jax_custom_vjp_disable_shape_check', False)\n+\n+\n",
            "whole_hunk": "@@ -9372,6 +9372,27 @@ class CustomVJPTest(jtu.JaxTestCase):\n         r'output\\[1\\] the bwd rule produced an output of shape/dtype float..\\[3\\]'):\n       jax.grad(lambda x, y: foo(x, y * y).sum(), 1)(jnp.ones(3), jnp.ones(4))\n \n+  def test_bwd_rule_shape_mismatch_disable(self):\n+    # TODO(mattjj): remove this test when the config option is removed\n+    @jax.custom_vjp\n+    def foo(x, y):\n+      return x\n+\n+    def foo_fwd(x, y):\n+      return x, None\n+\n+    def foo_bwd(_, g):\n+      return jnp.zeros(3), jnp.zeros(3)\n+\n+    foo.defvjp(foo_fwd, foo_bwd)\n+\n+    try:\n+      jax.config.update('jax_custom_vjp_disable_shape_check', True)\n+      jax.grad(lambda x, y: foo(x, y).sum(), 1)(jnp.ones(3), jnp.ones(4))\n+    finally:\n+      jax.config.update('jax_custom_vjp_disable_shape_check', False)\n+\n+\n def transpose_unary(f, x_example):\n   def transposed(y):\n     x, = api.linear_transpose(f, x_example)(y)"
        }
    ]
},
{
    "Id": 63,
    "commit_link": "https://github.com/google/jax/commit/55233a00291716fc291287b4d2fc5372bafc5dd1",
    "date": "2024-04-04T16:42:27-07:00",
    "message": "`device_local_layout` can be None on a jax.Array for backends that don't implement certain required methods for a jax.Array to populate the `device_local_layout`.\n\nSkip the error checks when arr.layout.device_local_layout is None.\n\nPiperOrigin-RevId: 622007598",
    "changes": [
        {
            "name": "pjit.py",
            "path": "jax/_src/pjit.py",
            "patches": [
                {
                    "old_start": 1279,
                    "old_length": 11,
                    "new_start": 1279,
                    "new_length": 14,
                    "hunk": "@@ -1279,11 +1279,14 @@ def _resolve_in_layouts(args, jit_in_layouts, jit_in_shardings):\n       else:\n         resolved_in_layouts.append(None)\n     else:\n-      if committed and arg_layout != jit_in_l:\n+      # arg_layout can be None because some backends don't implement the\n+      # required layout methods. Hence `arr.layout` can return\n+      # `Layout(None, sharding)`\n+      if committed and arg_layout is not None and arg_layout != jit_in_l:\n         raise ValueError('Layout passed to jit does not match the layout '\n                           'on the respective arg. '\n                           f'Got pjit layout: {jit_in_l},\\n'\n-                          f'arg sharding: {arg_layout} for '\n+                          f'arg layout: {arg_layout} for '\n                           f'arg shape: {shaped_abstractify(arg).str_short()}')\n       resolved_in_layouts.append(jit_in_l)\n   return tuple(resolved_in_layouts)"
                }
            ],
            "whole_deleted": "-      if committed and arg_layout != jit_in_l:\n-                          f'arg sharding: {arg_layout} for '\n",
            "whole_added": "+      # arg_layout can be None because some backends don't implement the\n+      # required layout methods. Hence `arr.layout` can return\n+      # `Layout(None, sharding)`\n+      if committed and arg_layout is not None and arg_layout != jit_in_l:\n+                          f'arg layout: {arg_layout} for '\n",
            "whole_hunk": "@@ -1279,11 +1279,14 @@ def _resolve_in_layouts(args, jit_in_layouts, jit_in_shardings):\n       else:\n         resolved_in_layouts.append(None)\n     else:\n-      if committed and arg_layout != jit_in_l:\n+      # arg_layout can be None because some backends don't implement the\n+      # required layout methods. Hence `arr.layout` can return\n+      # `Layout(None, sharding)`\n+      if committed and arg_layout is not None and arg_layout != jit_in_l:\n         raise ValueError('Layout passed to jit does not match the layout '\n                           'on the respective arg. '\n                           f'Got pjit layout: {jit_in_l},\\n'\n-                          f'arg sharding: {arg_layout} for '\n+                          f'arg layout: {arg_layout} for '\n                           f'arg shape: {shaped_abstractify(arg).str_short()}')\n       resolved_in_layouts.append(jit_in_l)\n   return tuple(resolved_in_layouts)"
        }
    ]
},
{
    "Id": 64,
    "commit_link": "https://github.com/google/jax/commit/011ced4431d0f58837f9c48bda854df78664c0af",
    "date": "2024-04-01T12:32:54-07:00",
    "message": "Guard test that requires two devices with device_count() check.\n\nPiperOrigin-RevId: 620921563",
    "changes": [
        {
            "name": "pmap_test.py",
            "path": "tests/pmap_test.py",
            "patches": [
                {
                    "old_start": 2206,
                    "old_length": 6,
                    "new_start": 2206,
                    "new_length": 8,
                    "hunk": "@@ -2206,6 +2206,8 @@ class PythonPmapTest(jtu.JaxTestCase):\n     # Regression test for https://github.com/google/jax/issues/20428\n     # pmap isn't particularly important here, but it guarantees that the CPU\n     # client runs the computation on a threadpool rather than inline.\n+    if jax.device_count() < 2:\n+      raise SkipTest(\"test requires at least two devices\")\n     x = jnp.eye(200)\n     y = jax.pmap(jax.scipy.linalg.expm)(jnp.array([x, x]))\n     y.block_until_ready()  # doesn't crash"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    if jax.device_count() < 2:\n+      raise SkipTest(\"test requires at least two devices\")\n",
            "whole_hunk": "@@ -2206,6 +2206,8 @@ class PythonPmapTest(jtu.JaxTestCase):\n     # Regression test for https://github.com/google/jax/issues/20428\n     # pmap isn't particularly important here, but it guarantees that the CPU\n     # client runs the computation on a threadpool rather than inline.\n+    if jax.device_count() < 2:\n+      raise SkipTest(\"test requires at least two devices\")\n     x = jnp.eye(200)\n     y = jax.pmap(jax.scipy.linalg.expm)(jnp.array([x, x]))\n     y.block_until_ready()  # doesn't crash"
        }
    ]
},
{
    "Id": 65,
    "commit_link": "https://github.com/google/jax/commit/75db481299cf774cefbd271539fbccb91a93d5d4",
    "date": "2024-03-26T05:32:41-07:00",
    "message": "[callback] Fix io_callback for callbacks that return Python literals.\n\nThe internal implementation of io_callback and friends currently use .shape and .dtype on the result of the callback. This fails if the callback returns a Python literal.\n\nFixed the checks that the callback returns values of expected shape and dtype,\nand added tests.\n\nReverts 19e6156ccec0df7a900471df7840bc421da2898b\n\nPiperOrigin-RevId: 619156176",
    "changes": [
        {
            "name": "mlir.py",
            "path": "jax/_src/interpreters/mlir.py",
            "patches": [
                {
                    "old_start": 2461,
                    "old_length": 15,
                    "new_start": 2461,
                    "new_length": 22,
                    "hunk": "@@ -2461,15 +2461,22 @@ def emit_python_callback(\n       raise RuntimeError(\n           \"Mismatched number of outputs from callback. \"\n           \"Expected: {}, Actual: {}\".format(len(result_avals), len(out_vals)))\n+    # Handle Python literals, and custom arrays, e.g., tf.Tensor.\n+    out_vals = tuple(np.asarray(a) for a in out_vals)\n     for i, (out_val, out_aval) in enumerate(zip(out_vals, result_avals)):\n       if out_val.shape != out_aval.shape:\n         raise RuntimeError(\n-            f\"Incorrect output shape for return value {i}: \"\n-            \"Expected: {}, Actual: {}\".format(out_aval.shape, out_val.shape))\n+            f\"Incorrect output shape for return value #{i}: \"\n+            f\"Expected: {out_aval.shape}, Actual: {out_val.shape}\")\n+      if out_val.dtype != dtypes.canonicalize_dtype(out_val.dtype):\n+        raise RuntimeError(\n+            \"Cannot return 64-bit values when `jax_enable_x64` is disabled. \"\n+            f\"Actual: {out_val.dtype}\")\n       if out_val.dtype != out_aval.dtype:\n         raise RuntimeError(\n-            f\"Incorrect output dtype for return value {i}: \"\n-            \"Expected: {}, Actual: {}\".format(out_aval.dtype, out_val.dtype))\n+            f\"Incorrect output dtype for return value #{i}: \"\n+            f\"Expected: {out_aval.dtype}, Actual: {out_val.dtype}\")\n+\n     if platform == \"tpu\":\n       # On TPU we cannot receive empty arrays. So, we return from the wrapped\n       # callback only the non-empty results, and we will create empty constants\n"
                }
            ],
            "whole_deleted": "-            f\"Incorrect output shape for return value {i}: \"\n-            \"Expected: {}, Actual: {}\".format(out_aval.shape, out_val.shape))\n-            f\"Incorrect output dtype for return value {i}: \"\n-            \"Expected: {}, Actual: {}\".format(out_aval.dtype, out_val.dtype))\n",
            "whole_added": "+    # Handle Python literals, and custom arrays, e.g., tf.Tensor.\n+    out_vals = tuple(np.asarray(a) for a in out_vals)\n+            f\"Incorrect output shape for return value #{i}: \"\n+            f\"Expected: {out_aval.shape}, Actual: {out_val.shape}\")\n+      if out_val.dtype != dtypes.canonicalize_dtype(out_val.dtype):\n+        raise RuntimeError(\n+            \"Cannot return 64-bit values when `jax_enable_x64` is disabled. \"\n+            f\"Actual: {out_val.dtype}\")\n+            f\"Incorrect output dtype for return value #{i}: \"\n+            f\"Expected: {out_aval.dtype}, Actual: {out_val.dtype}\")\n+\n",
            "whole_hunk": "@@ -2461,15 +2461,22 @@ def emit_python_callback(\n       raise RuntimeError(\n           \"Mismatched number of outputs from callback. \"\n           \"Expected: {}, Actual: {}\".format(len(result_avals), len(out_vals)))\n+    # Handle Python literals, and custom arrays, e.g., tf.Tensor.\n+    out_vals = tuple(np.asarray(a) for a in out_vals)\n     for i, (out_val, out_aval) in enumerate(zip(out_vals, result_avals)):\n       if out_val.shape != out_aval.shape:\n         raise RuntimeError(\n-            f\"Incorrect output shape for return value {i}: \"\n-            \"Expected: {}, Actual: {}\".format(out_aval.shape, out_val.shape))\n+            f\"Incorrect output shape for return value #{i}: \"\n+            f\"Expected: {out_aval.shape}, Actual: {out_val.shape}\")\n+      if out_val.dtype != dtypes.canonicalize_dtype(out_val.dtype):\n+        raise RuntimeError(\n+            \"Cannot return 64-bit values when `jax_enable_x64` is disabled. \"\n+            f\"Actual: {out_val.dtype}\")\n       if out_val.dtype != out_aval.dtype:\n         raise RuntimeError(\n-            f\"Incorrect output dtype for return value {i}: \"\n-            \"Expected: {}, Actual: {}\".format(out_aval.dtype, out_val.dtype))\n+            f\"Incorrect output dtype for return value #{i}: \"\n+            f\"Expected: {out_aval.dtype}, Actual: {out_val.dtype}\")\n+\n     if platform == \"tpu\":\n       # On TPU we cannot receive empty arrays. So, we return from the wrapped\n       # callback only the non-empty results, and we will create empty constants\n"
        },
        {
            "name": "python_callback_test.py",
            "path": "tests/python_callback_test.py",
            "patches": [
                {
                    "old_start": 13,
                    "old_length": 6,
                    "new_start": 13,
                    "new_length": 7,
                    "hunk": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \n import collections\n+import contextlib\n import functools\n import logging\n import textwrap\n"
                },
                {
                    "old_start": 27,
                    "old_length": 11,
                    "new_start": 28,
                    "new_length": 11,
                    "hunk": "@@ -27,11 +28,11 @@ from jax._src import config\n from jax._src import core\n from jax._src import dispatch\n from jax._src import maps\n-from jax._src.maps import xmap\n from jax._src import test_util as jtu\n from jax._src import util\n from jax._src.lib import xla_client\n from jax._src.lib import xla_extension_version\n+from jax._src.maps import xmap\n from jax.experimental import io_callback\n from jax.experimental import pjit\n from jax.experimental.shard_map import shard_map\n"
                },
                {
                    "old_start": 93,
                    "old_length": 6,
                    "new_start": 95,
                    "new_length": 73,
                    "hunk": "@@ -93,6 +95,73 @@ class PythonCallbackTest(jtu.JaxTestCase):\n     out = f(0.)\n     self.assertEqual(out, 1.)\n \n+  @parameterized.named_parameters(\n+      dict(\n+          testcase_name=f\"{flavor}_expect_dtype_{expect_dtype}\",\n+          callback=dict(\n+              io_unordered=io_calback_unordered,\n+              io_ordered=io_callback_ordered,\n+              pure=jax.pure_callback,\n+          )[flavor],\n+          expect_dtype=expect_dtype,\n+      )\n+      for flavor in (\"io_unordered\", \"io_ordered\", \"pure\")\n+      for expect_dtype in (np.int32, np.int64, np.float32, np.float64)\n+  )\n+  def test_callback_returning_python_literal(self, *, callback, expect_dtype):\n+    returned_literal = 42 if expect_dtype in (np.int32, np.int64) else 42.0\n+\n+    @jax.jit\n+    def f(x):\n+      return callback(\n+          lambda x: returned_literal, core.ShapedArray((), expect_dtype), x\n+      )\n+\n+    if not config.enable_x64.value:\n+      ctx = self.assertRaisesRegex(Exception, \"Cannot return 64-bit values\")\n+    elif expect_dtype in (np.int32, np.float32):\n+      ctx = self.assertRaisesRegex(Exception, \"Incorrect output dtype\")\n+    else:\n+      ctx = contextlib.nullcontext()\n+\n+    with ctx:\n+      out = f(0.0)\n+      jax.effects_barrier()\n+      self.assertEqual(out, returned_literal)\n+\n+  @with_pure_and_io_callbacks\n+  def test_callback_returning_custom_array(self, *, callback):\n+    # Some users write the callback in TF, returning a tf.Tensor. We don't\n+    # want to add TF as a dependency, but simulate that use case with a\n+    # custom array class.\n+    class CustomArray:\n+\n+      def __init__(self, a: np.ndarray):\n+        self.a = a\n+\n+      @property\n+      def shape(self):\n+        return self.a.shape\n+\n+      @property\n+      def dtype(self):\n+        return self.a.dtype\n+\n+      def __array__(self):\n+        return self.a\n+\n+    @jax.jit\n+    def f(x):\n+      return callback(\n+          lambda x: CustomArray(np.array(42.0, dtype=np.float32)),\n+          core.ShapedArray((), np.float32),\n+          x,\n+      )\n+\n+    out = f(0.0)\n+    jax.effects_barrier()\n+    self.assertEqual(out, 42.0)\n+\n   @parameterized.named_parameters(\n     dict(testcase_name=f\"{flavor}_{dtype}\",\n          dtype=dtype,"
                }
            ],
            "whole_deleted": "-from jax._src.maps import xmap\n",
            "whole_added": "+import contextlib\n+from jax._src.maps import xmap\n+  @parameterized.named_parameters(\n+      dict(\n+          testcase_name=f\"{flavor}_expect_dtype_{expect_dtype}\",\n+          callback=dict(\n+              io_unordered=io_calback_unordered,\n+              io_ordered=io_callback_ordered,\n+              pure=jax.pure_callback,\n+          )[flavor],\n+          expect_dtype=expect_dtype,\n+      )\n+      for flavor in (\"io_unordered\", \"io_ordered\", \"pure\")\n+      for expect_dtype in (np.int32, np.int64, np.float32, np.float64)\n+  )\n+  def test_callback_returning_python_literal(self, *, callback, expect_dtype):\n+    returned_literal = 42 if expect_dtype in (np.int32, np.int64) else 42.0\n+\n+    @jax.jit\n+    def f(x):\n+      return callback(\n+          lambda x: returned_literal, core.ShapedArray((), expect_dtype), x\n+      )\n+\n+    if not config.enable_x64.value:\n+      ctx = self.assertRaisesRegex(Exception, \"Cannot return 64-bit values\")\n+    elif expect_dtype in (np.int32, np.float32):\n+      ctx = self.assertRaisesRegex(Exception, \"Incorrect output dtype\")\n+    else:\n+      ctx = contextlib.nullcontext()\n+\n+    with ctx:\n+      out = f(0.0)\n+      jax.effects_barrier()\n+      self.assertEqual(out, returned_literal)\n+\n+  @with_pure_and_io_callbacks\n+  def test_callback_returning_custom_array(self, *, callback):\n+    # Some users write the callback in TF, returning a tf.Tensor. We don't\n+    # want to add TF as a dependency, but simulate that use case with a\n+    # custom array class.\n+    class CustomArray:\n+\n+      def __init__(self, a: np.ndarray):\n+        self.a = a\n+\n+      @property\n+      def shape(self):\n+        return self.a.shape\n+\n+      @property\n+      def dtype(self):\n+        return self.a.dtype\n+\n+      def __array__(self):\n+        return self.a\n+\n+    @jax.jit\n+    def f(x):\n+      return callback(\n+          lambda x: CustomArray(np.array(42.0, dtype=np.float32)),\n+          core.ShapedArray((), np.float32),\n+          x,\n+      )\n+\n+    out = f(0.0)\n+    jax.effects_barrier()\n+    self.assertEqual(out, 42.0)\n+\n",
            "whole_hunk": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \n import collections\n+import contextlib\n import functools\n import logging\n import textwrap\n@@ -27,11 +28,11 @@ from jax._src import config\n from jax._src import core\n from jax._src import dispatch\n from jax._src import maps\n-from jax._src.maps import xmap\n from jax._src import test_util as jtu\n from jax._src import util\n from jax._src.lib import xla_client\n from jax._src.lib import xla_extension_version\n+from jax._src.maps import xmap\n from jax.experimental import io_callback\n from jax.experimental import pjit\n from jax.experimental.shard_map import shard_map\n@@ -93,6 +95,73 @@ class PythonCallbackTest(jtu.JaxTestCase):\n     out = f(0.)\n     self.assertEqual(out, 1.)\n \n+  @parameterized.named_parameters(\n+      dict(\n+          testcase_name=f\"{flavor}_expect_dtype_{expect_dtype}\",\n+          callback=dict(\n+              io_unordered=io_calback_unordered,\n+              io_ordered=io_callback_ordered,\n+              pure=jax.pure_callback,\n+          )[flavor],\n+          expect_dtype=expect_dtype,\n+      )\n+      for flavor in (\"io_unordered\", \"io_ordered\", \"pure\")\n+      for expect_dtype in (np.int32, np.int64, np.float32, np.float64)\n+  )\n+  def test_callback_returning_python_literal(self, *, callback, expect_dtype):\n+    returned_literal = 42 if expect_dtype in (np.int32, np.int64) else 42.0\n+\n+    @jax.jit\n+    def f(x):\n+      return callback(\n+          lambda x: returned_literal, core.ShapedArray((), expect_dtype), x\n+      )\n+\n+    if not config.enable_x64.value:\n+      ctx = self.assertRaisesRegex(Exception, \"Cannot return 64-bit values\")\n+    elif expect_dtype in (np.int32, np.float32):\n+      ctx = self.assertRaisesRegex(Exception, \"Incorrect output dtype\")\n+    else:\n+      ctx = contextlib.nullcontext()\n+\n+    with ctx:\n+      out = f(0.0)\n+      jax.effects_barrier()\n+      self.assertEqual(out, returned_literal)\n+\n+  @with_pure_and_io_callbacks\n+  def test_callback_returning_custom_array(self, *, callback):\n+    # Some users write the callback in TF, returning a tf.Tensor. We don't\n+    # want to add TF as a dependency, but simulate that use case with a\n+    # custom array class.\n+    class CustomArray:\n+\n+      def __init__(self, a: np.ndarray):\n+        self.a = a\n+\n+      @property\n+      def shape(self):\n+        return self.a.shape\n+\n+      @property\n+      def dtype(self):\n+        return self.a.dtype\n+\n+      def __array__(self):\n+        return self.a\n+\n+    @jax.jit\n+    def f(x):\n+      return callback(\n+          lambda x: CustomArray(np.array(42.0, dtype=np.float32)),\n+          core.ShapedArray((), np.float32),\n+          x,\n+      )\n+\n+    out = f(0.0)\n+    jax.effects_barrier()\n+    self.assertEqual(out, 42.0)\n+\n   @parameterized.named_parameters(\n     dict(testcase_name=f\"{flavor}_{dtype}\",\n          dtype=dtype,"
        }
    ]
},
{
    "Id": 66,
    "commit_link": "https://github.com/google/jax/commit/f51d80ed1eda737da9353d7b52cfa0ad0ab0b59f",
    "date": "2024-03-25T14:11:11-07:00",
    "message": "move checks to setup",
    "changes": [
        {
            "name": "fused_attention_stablehlo_test.py",
            "path": "tests/fused_attention_stablehlo_test.py",
            "patches": [
                {
                    "old_start": 26,
                    "old_length": 7,
                    "new_start": 26,
                    "new_length": 6,
                    "hunk": "@@ -26,7 +26,6 @@ from jax.sharding import PartitionSpec, NamedSharding\n from jax._src import config\n from jax._src import test_util as jtu\n from jax._src.cudnn.fused_attention_stablehlo import dot_product_attention, check_is_flash_attention\n-import unittest\n \n config.parse_flags_with_absl()\n Array = jnp.ndarray\n"
                },
                {
                    "old_start": 112,
                    "old_length": 6,
                    "new_start": 111,
                    "new_length": 10,
                    "hunk": "@@ -112,6 +111,10 @@ def sdpa_train_ref(query: Array,\n   return out_ref, (query_grad_ref, key_grad_ref, value_grad_ref)\n \n class DotProductAttentionTest(jtu.JaxTestCase):\n+  def setUp(self):\n+    if jax.device_count() < 4:\n+      self.skipTest(\"Requires more than 4 devices.\")\n+\n   @jtu.sample_product(\n       batch_size=[4],\n       seq_len=[256, 1024],\n"
                },
                {
                    "old_start": 203,
                    "old_length": 7,
                    "new_start": 206,
                    "new_length": 6,
                    "hunk": "@@ -203,7 +206,6 @@ class DotProductAttentionTest(jtu.JaxTestCase):\n       self.assertArraysAllClose(key_grad_ref, key_grad, rtol=1e-5, atol=1e-5)\n       self.assertArraysAllClose(value_grad_ref, value_grad, rtol=1e-5, atol=1e-5)\n \n-  @unittest.skipIf(len(jax.local_devices()) < 4, '4 gpus required')\n   @jtu.run_on_devices(\"cuda\")\n   def test_sdpa_inference(self):\n     k1, k2, k3 = jax.random.split(jax.random.key(0), 3)"
                }
            ],
            "whole_deleted": "-import unittest\n-  @unittest.skipIf(len(jax.local_devices()) < 4, '4 gpus required')\n",
            "whole_added": "+  def setUp(self):\n+    if jax.device_count() < 4:\n+      self.skipTest(\"Requires more than 4 devices.\")\n+\n",
            "whole_hunk": "@@ -26,7 +26,6 @@ from jax.sharding import PartitionSpec, NamedSharding\n from jax._src import config\n from jax._src import test_util as jtu\n from jax._src.cudnn.fused_attention_stablehlo import dot_product_attention, check_is_flash_attention\n-import unittest\n \n config.parse_flags_with_absl()\n Array = jnp.ndarray\n@@ -112,6 +111,10 @@ def sdpa_train_ref(query: Array,\n   return out_ref, (query_grad_ref, key_grad_ref, value_grad_ref)\n \n class DotProductAttentionTest(jtu.JaxTestCase):\n+  def setUp(self):\n+    if jax.device_count() < 4:\n+      self.skipTest(\"Requires more than 4 devices.\")\n+\n   @jtu.sample_product(\n       batch_size=[4],\n       seq_len=[256, 1024],\n@@ -203,7 +206,6 @@ class DotProductAttentionTest(jtu.JaxTestCase):\n       self.assertArraysAllClose(key_grad_ref, key_grad, rtol=1e-5, atol=1e-5)\n       self.assertArraysAllClose(value_grad_ref, value_grad, rtol=1e-5, atol=1e-5)\n \n-  @unittest.skipIf(len(jax.local_devices()) < 4, '4 gpus required')\n   @jtu.run_on_devices(\"cuda\")\n   def test_sdpa_inference(self):\n     k1, k2, k3 = jax.random.split(jax.random.key(0), 3)"
        }
    ]
},
{
    "Id": 67,
    "commit_link": "https://github.com/google/jax/commit/25d01e983cab99bed8482944c40de7a9139242e7",
    "date": "2024-03-25T10:08:43-07:00",
    "message": "[Take 2] Expose .layout on jax.Array. Also add checks in the AOT path to make sure that the input Array's layout matches the layout given to jax.jit.\n\nReverts cd79e71d85621a8d6dede9a710bdb2a29bb380fd\n\nPiperOrigin-RevId: 618878870",
    "changes": [
        {
            "name": "array.py",
            "path": "jax/_src/array.py",
            "patches": [
                {
                    "old_start": 34,
                    "old_length": 10,
                    "new_start": 34,
                    "new_length": 12,
                    "hunk": "@@ -34,10 +34,12 @@ from jax._src import deprecations\n from jax._src import dispatch\n from jax._src import dtypes\n from jax._src import errors\n+from jax._src import layout\n from jax._src import profiler\n from jax._src import tree_util\n from jax._src import xla_bridge\n from jax._src.lib import xla_client as xc\n+from jax._src.lib import xla_extension as xe\n from jax._src.interpreters import mlir\n from jax._src.interpreters import pxla\n from jax._src.interpreters import xla\n"
                },
                {
                    "old_start": 527,
                    "old_length": 6,
                    "new_start": 529,
                    "new_length": 18,
                    "hunk": "@@ -527,6 +529,18 @@ class ArrayImpl(basearray.Array):\n       out.append(Shard(_get_device(a), self.sharding, self.shape, a))\n     return out\n \n+  @property\n+  def layout(self):\n+    # TODO(yashkatariya): Remove the try;except when pathways supports layouts.\n+    try:\n+      return layout.SpecifiedLayout(self._pjrt_layout)\n+    except xe.XlaRuntimeError as e:\n+      msg, *_ = e.args\n+      if type(msg) is str and msg.startswith(\"UNIMPLEMENTED\"):\n+        return None\n+      else:\n+        raise\n+\n   @property\n   def global_shards(self) -> Sequence[Shard]:\n     \"\"\"Returns list of all `Shard`s of the Array across all devices.\n"
                },
                {
                    "old_start": 637,
                    "old_length": 7,
                    "new_start": 651,
                    "new_length": 7,
                    "hunk": "@@ -637,7 +651,7 @@ if not TYPE_CHECKING:\n   ArrayImpl = use_cpp_class(xc.ArrayImpl)(ArrayImpl)\n \n \n-# explicitly set to be unhashable. Same as what device_array.py does.\n+# explicitly set to be unhashable.\n setattr(ArrayImpl, \"__hash__\", None)\n setattr(ArrayImpl, \"__array_priority__\", 100)\n \n"
                }
            ],
            "whole_deleted": "-# explicitly set to be unhashable. Same as what device_array.py does.\n",
            "whole_added": "+from jax._src import layout\n+from jax._src.lib import xla_extension as xe\n+  @property\n+  def layout(self):\n+    # TODO(yashkatariya): Remove the try;except when pathways supports layouts.\n+    try:\n+      return layout.SpecifiedLayout(self._pjrt_layout)\n+    except xe.XlaRuntimeError as e:\n+      msg, *_ = e.args\n+      if type(msg) is str and msg.startswith(\"UNIMPLEMENTED\"):\n+        return None\n+      else:\n+        raise\n+\n+# explicitly set to be unhashable.\n",
            "whole_hunk": "@@ -34,10 +34,12 @@ from jax._src import deprecations\n from jax._src import dispatch\n from jax._src import dtypes\n from jax._src import errors\n+from jax._src import layout\n from jax._src import profiler\n from jax._src import tree_util\n from jax._src import xla_bridge\n from jax._src.lib import xla_client as xc\n+from jax._src.lib import xla_extension as xe\n from jax._src.interpreters import mlir\n from jax._src.interpreters import pxla\n from jax._src.interpreters import xla\n@@ -527,6 +529,18 @@ class ArrayImpl(basearray.Array):\n       out.append(Shard(_get_device(a), self.sharding, self.shape, a))\n     return out\n \n+  @property\n+  def layout(self):\n+    # TODO(yashkatariya): Remove the try;except when pathways supports layouts.\n+    try:\n+      return layout.SpecifiedLayout(self._pjrt_layout)\n+    except xe.XlaRuntimeError as e:\n+      msg, *_ = e.args\n+      if type(msg) is str and msg.startswith(\"UNIMPLEMENTED\"):\n+        return None\n+      else:\n+        raise\n+\n   @property\n   def global_shards(self) -> Sequence[Shard]:\n     \"\"\"Returns list of all `Shard`s of the Array across all devices.\n@@ -637,7 +651,7 @@ if not TYPE_CHECKING:\n   ArrayImpl = use_cpp_class(xc.ArrayImpl)(ArrayImpl)\n \n \n-# explicitly set to be unhashable. Same as what device_array.py does.\n+# explicitly set to be unhashable.\n setattr(ArrayImpl, \"__hash__\", None)\n setattr(ArrayImpl, \"__array_priority__\", 100)\n \n"
        },
        {
            "name": "mlir.py",
            "path": "jax/_src/interpreters/mlir.py",
            "patches": [
                {
                    "old_start": 46,
                    "old_length": 7,
                    "new_start": 46,
                    "new_length": 7,
                    "hunk": "@@ -46,7 +46,7 @@ from jax._src import util\n from jax._src import xla_bridge as xb\n from jax._src.interpreters import partial_eval as pe\n from jax._src.interpreters import xla\n-from jax._src.layout import XLACompatibleLayout, LayoutRequest\n+from jax._src.layout import AutoLayout, SpecifiedLayout\n from jax._src.lib import xla_client as xc\n from jax._src.lib import xla_extension\n from jax._src.lib import xla_extension_version\n"
                },
                {
                    "old_start": 834,
                    "old_length": 10,
                    "new_start": 834,
                    "new_length": 10,
                    "hunk": "@@ -834,10 +834,10 @@ def _to_physical_op_sharding(\n   return sharding._to_xla_hlo_sharding(aval.ndim).to_proto()  # type: ignore\n \n \n-def _to_xla_layout(layout: XLACompatibleLayout | None | LayoutRequest) -> str | None:\n+def _to_xla_layout(layout: SpecifiedLayout | None | AutoLayout) -> str | None:\n   if layout is None:\n     return \"default\"\n-  if isinstance(layout, LayoutRequest):\n+  if isinstance(layout, AutoLayout):\n     return \"auto\"\n   return layout._to_xla_layout()\n \n"
                },
                {
                    "old_start": 862,
                    "old_length": 8,
                    "new_start": 862,
                    "new_length": 8,
                    "hunk": "@@ -862,8 +862,8 @@ def lower_jaxpr_to_module(\n     replicated_args: Sequence[bool] | None = None,\n     arg_shardings: Sequence[XLACompatibleSharding | None] | None = None,\n     result_shardings: Sequence[XLACompatibleSharding | None] | None = None,\n-    in_layouts: Sequence[XLACompatibleLayout | None | LayoutRequest] | None = None,\n-    out_layouts: Sequence[XLACompatibleLayout | None | LayoutRequest] | None = None,\n+    in_layouts: Sequence[SpecifiedLayout | None | AutoLayout] | None = None,\n+    out_layouts: Sequence[SpecifiedLayout | None | AutoLayout] | None = None,\n     arg_names: Sequence[str | None] | None = None,\n     result_names: Sequence[str | None] | None = None,\n     num_replicas: int = 1,\n"
                }
            ],
            "whole_deleted": "-from jax._src.layout import XLACompatibleLayout, LayoutRequest\n-def _to_xla_layout(layout: XLACompatibleLayout | None | LayoutRequest) -> str | None:\n-  if isinstance(layout, LayoutRequest):\n-    in_layouts: Sequence[XLACompatibleLayout | None | LayoutRequest] | None = None,\n-    out_layouts: Sequence[XLACompatibleLayout | None | LayoutRequest] | None = None,\n",
            "whole_added": "+from jax._src.layout import AutoLayout, SpecifiedLayout\n+def _to_xla_layout(layout: SpecifiedLayout | None | AutoLayout) -> str | None:\n+  if isinstance(layout, AutoLayout):\n+    in_layouts: Sequence[SpecifiedLayout | None | AutoLayout] | None = None,\n+    out_layouts: Sequence[SpecifiedLayout | None | AutoLayout] | None = None,\n",
            "whole_hunk": "@@ -46,7 +46,7 @@ from jax._src import util\n from jax._src import xla_bridge as xb\n from jax._src.interpreters import partial_eval as pe\n from jax._src.interpreters import xla\n-from jax._src.layout import XLACompatibleLayout, LayoutRequest\n+from jax._src.layout import AutoLayout, SpecifiedLayout\n from jax._src.lib import xla_client as xc\n from jax._src.lib import xla_extension\n from jax._src.lib import xla_extension_version\n@@ -834,10 +834,10 @@ def _to_physical_op_sharding(\n   return sharding._to_xla_hlo_sharding(aval.ndim).to_proto()  # type: ignore\n \n \n-def _to_xla_layout(layout: XLACompatibleLayout | None | LayoutRequest) -> str | None:\n+def _to_xla_layout(layout: SpecifiedLayout | None | AutoLayout) -> str | None:\n   if layout is None:\n     return \"default\"\n-  if isinstance(layout, LayoutRequest):\n+  if isinstance(layout, AutoLayout):\n     return \"auto\"\n   return layout._to_xla_layout()\n \n@@ -862,8 +862,8 @@ def lower_jaxpr_to_module(\n     replicated_args: Sequence[bool] | None = None,\n     arg_shardings: Sequence[XLACompatibleSharding | None] | None = None,\n     result_shardings: Sequence[XLACompatibleSharding | None] | None = None,\n-    in_layouts: Sequence[XLACompatibleLayout | None | LayoutRequest] | None = None,\n-    out_layouts: Sequence[XLACompatibleLayout | None | LayoutRequest] | None = None,\n+    in_layouts: Sequence[SpecifiedLayout | None | AutoLayout] | None = None,\n+    out_layouts: Sequence[SpecifiedLayout | None | AutoLayout] | None = None,\n     arg_names: Sequence[str | None] | None = None,\n     result_names: Sequence[str | None] | None = None,\n     num_replicas: int = 1,\n"
        },
        {
            "name": "pxla.py",
            "path": "jax/_src/interpreters/pxla.py",
            "patches": [
                {
                    "old_start": 60,
                    "old_length": 7,
                    "new_start": 60,
                    "new_length": 7,
                    "hunk": "@@ -60,7 +60,7 @@ from jax._src.interpreters import batching\n from jax._src.interpreters import partial_eval as pe\n from jax._src.interpreters import mlir\n from jax._src.interpreters import xla\n-from jax._src.layout import XLACompatibleLayout, SpecifiedLayout, LayoutRequest\n+from jax._src.layout import SpecifiedLayout, AutoLayout\n from jax._src.lib import xla_client as xc\n from jax._src.lib import xla_extension_version\n from jax._src.lib.mlir import ir\n"
                },
                {
                    "old_start": 1985,
                    "old_length": 13,
                    "new_start": 1985,
                    "new_length": 14,
                    "hunk": "@@ -1985,13 +1985,14 @@ def are_all_shardings_default_mem_kind(da_object, shardings):\n       return False\n   return True\n \n-MaybeLayout = Sequence[Union[XLACompatibleLayout, LayoutRequest, None]]\n+MaybeLayout = Sequence[Union[SpecifiedLayout, AutoLayout, None]]\n \n \n class AllArgsInfo(NamedTuple):\n   \"\"\"Avals, shardings, layouts and debug_info for all arguments prior to DCE.\"\"\"\n   in_avals: Sequence[core.ShapedArray]\n   in_shardings: Any\n+  in_layouts: Any\n   debug_info: core.JaxprDebugInfo | None\n \n \n"
                },
                {
                    "old_start": 2023,
                    "old_length": 7,
                    "new_start": 2024,
                    "new_length": 7,
                    "hunk": "@@ -2023,7 +2024,7 @@ def lower_sharding_computation(\n   auto_spmd_lowering = check_if_any_auto(\n       it.chain.from_iterable([in_shardings, out_shardings]))  # type: ignore\n \n-  all_args_info = AllArgsInfo(global_in_avals, in_shardings,\n+  all_args_info = AllArgsInfo(global_in_avals, in_shardings, in_layouts,\n                               closed_jaxpr.jaxpr.debug_info)\n \n   (closed_jaxpr, global_in_avals, global_out_avals, donated_invars,\n"
                },
                {
                    "old_start": 2227,
                    "old_length": 8,
                    "new_start": 2228,
                    "new_length": 6,
                    "hunk": "@@ -2227,8 +2228,6 @@ def lower_mesh_computation(\n       out_jaxpr_avals = fun_or_jaxpr.out_avals\n       consts = fun_or_jaxpr.consts\n \n-  all_args_info = AllArgsInfo(global_in_avals, in_shardings, jaxpr.debug_info)\n-\n   assert len(out_shardings) == len(out_jaxpr_avals)\n   if spmd_lowering:\n     global_out_avals = out_jaxpr_avals\n"
                },
                {
                    "old_start": 2319,
                    "old_length": 7,
                    "new_start": 2318,
                    "new_length": 7,
                    "hunk": "@@ -2319,7 +2318,7 @@ def lower_mesh_computation(\n       in_layouts=(None,) * len(global_in_avals),\n       out_layouts=(None,) * len(global_out_avals),\n       shape_poly_state=lowering_result.shape_poly_state,\n-      all_args_info=all_args_info)\n+      all_args_info=None)\n \n class MeshComputation(stages.XlaLowering):\n   _hlo: ir.Module | None\n"
                },
                {
                    "old_start": 2599,
                    "old_length": 7,
                    "new_start": 2598,
                    "new_length": 7,
                    "hunk": "@@ -2599,7 +2598,7 @@ def _get_layouts_from_executable(\n     if isinstance(i, SpecifiedLayout):\n       if i != x:\n         raise AssertionError(\n-            f\"Unexpected XLA layout override: (XLA) {x} != {i} (User sharding)\")\n+            f\"Unexpected XLA layout override: (XLA) {x} != {i} (User layout)\")\n       new_in_layouts.append(i)\n     else:\n       new_in_layouts.append(x)\n"
                },
                {
                    "old_start": 2610,
                    "old_length": 7,
                    "new_start": 2609,
                    "new_length": 7,
                    "hunk": "@@ -2610,7 +2609,7 @@ def _get_layouts_from_executable(\n     if isinstance(o, SpecifiedLayout):\n       if o != x:\n         raise AssertionError(\n-            f\"Unexpected XLA layout override: (XLA) {x} != {o} (User sharding)\")\n+            f\"Unexpected XLA layout override: (XLA) {x} != {o} (User layout)\")\n       new_out_layouts.append(o)\n     else:\n       new_out_layouts.append(x)\n"
                },
                {
                    "old_start": 3016,
                    "old_length": 6,
                    "new_start": 3015,
                    "new_length": 7,
                    "hunk": "@@ -3016,6 +3015,7 @@ class MeshExecutable(stages.XlaExecutable):\n       kept_args = [a for i, a in enumerate(args) if i in self._kept_var_idx]\n       ref_avals = self.in_avals\n       in_shardings = self._in_shardings\n+      in_layouts = self._in_layouts\n       debug_info = None\n     else:\n       kept_args = args\n"
                },
                {
                    "old_start": 3023,
                    "old_length": 12,
                    "new_start": 3023,
                    "new_length": 16,
                    "hunk": "@@ -3023,12 +3023,16 @@ class MeshExecutable(stages.XlaExecutable):\n       iter_in_shardings = iter(self._in_shardings)\n       in_shardings = [next(iter_in_shardings) if i in self._kept_var_idx else s\n                       for i, s in enumerate(self._all_args_info.in_shardings)]\n+      iter_in_layouts = iter(self._in_layouts)\n+      in_layouts = [next(iter_in_layouts) if i in self._kept_var_idx else s\n+                    for i, s in enumerate(self._all_args_info.in_layouts)]\n       debug_info = self._all_args_info.debug_info\n \n     arg_avals = map(xla.abstractify, kept_args)\n     check_arg_avals_for_call(ref_avals, arg_avals, debug_info)\n     # Check the GDA sharding and the input sharding.\n-    check_gda_or_array_xla_sharding_match(kept_args, in_shardings, debug_info)\n+    check_array_xla_sharding_layout_match(kept_args, in_shardings,\n+                                          in_layouts, debug_info)\n     return self.unsafe_call(*args)  # pylint: disable=not-callable\n \n   def input_shardings(self) -> Sequence[sharding_impls.XLACompatibleSharding]:\n"
                },
                {
                    "old_start": 3184,
                    "old_length": 15,
                    "new_start": 3188,
                    "new_length": 17,
                    "hunk": "@@ -3184,15 +3188,17 @@ def check_device_backend_on_shardings(shardings) -> bool:\n   return False\n \n \n-def check_gda_or_array_xla_sharding_match(\n+def check_array_xla_sharding_layout_match(\n     args, in_xla_shardings: Sequence[sharding_impls.XLACompatibleSharding],\n+    in_xla_layouts: Sequence[SpecifiedLayout],\n     jaxpr_debug_info: core.JaxprDebugInfo | None) -> None:\n   from jax._src.array import ArrayImpl\n   arg_names = ([''] * len(args) if jaxpr_debug_info is None else\n                jaxpr_debug_info.arg_names)\n   errors = []\n   num_errors = 5\n-  for arg, xs, name in safe_zip(args, in_xla_shardings, arg_names):\n+  for arg, xs, xl, name in safe_zip(args, in_xla_shardings, in_xla_layouts,\n+                                    arg_names):\n     if not isinstance(arg, ArrayImpl):\n       continue\n     if is_unspecified_or_auto(xs):\n"
                },
                {
                    "old_start": 3205,
                    "old_length": 27,
                    "new_start": 3211,
                    "new_length": 45,
                    "hunk": "@@ -3205,27 +3211,45 @@ def check_gda_or_array_xla_sharding_match(\n     # Raise memory kind mismatch error even if the arg is uncommitted.\n     if arg.sharding.memory_kind != xs.memory_kind:\n       errors.append(\n-          \"Got input sharding(s) that compiled object was called with: \"\n+          (\"Got input sharding(s) that compiled object was called with: \"\n           f\"{arg.sharding} and sharding(s) the computation was compiled \"\n-          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\")\n+          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\",\n+          'sharding'))\n \n     if (not db_xs and arg._committed and\n         not op_shardings.are_op_shardings_equal(\n             arg.sharding._to_xla_hlo_sharding(arg.ndim),\n             xs._to_xla_hlo_sharding(arg.ndim))):\n       errors.append(\n-          \"Got input sharding(s) that compiled object was called with: \"\n+          (\"Got input sharding(s) that compiled object was called with: \"\n           f\"{arg.sharding} and sharding(s) the computation was compiled \"\n-          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\")\n+          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\",\n+          'sharding'))\n+\n+    if (xla_extension_version >= 249 and not db_xs and arg._committed and\n+        arg.layout is not None and xl is not None and arg.layout != xl):\n+      errors.append(\n+          (\"Got input layout(s) that compiled object was called with: \"\n+          f\"{arg.layout} and layout(s) the computation was compiled \"\n+          f\"with: {xl} for arg {name} with shape: {arg.aval.str_short()}\",\n+          'layout'))\n \n   if errors:\n-    str_errors = '\\n'.join(errors[:num_errors])\n+    first_errors, error_kinds = unzip2(errors[:num_errors])\n+    str_errors = '\\n'.join(first_errors)\n+    if all(k == 'sharding' for k in error_kinds):\n+      kind_str = r'sharding(s)'\n+    elif all(k == 'layout' for k in error_kinds):\n+      kind_str = 'layout(s)'\n+    else:\n+      kind_str = 'sharding(s) and layout(s)'\n     num_mismatch_str = (\n         f'the {len(errors)} mismatches' if len(errors) < num_errors else\n         f\"{num_errors} mismatches out of {len(errors)}\")\n     raise ValueError(\n-          \"Compiled object called with input sharding(s) does not match the \"\n-          \"sharding(s) the computation was compiled with. \"\n+          f\"Compiled object called with input {kind_str} does \"\n+          f\"not match the {kind_str} the computation was \"\n+          \"compiled with. \"\n           f\"Here are {num_mismatch_str}:\\n{str_errors}\")\n \n \n"
                }
            ],
            "whole_deleted": "-from jax._src.layout import XLACompatibleLayout, SpecifiedLayout, LayoutRequest\n-MaybeLayout = Sequence[Union[XLACompatibleLayout, LayoutRequest, None]]\n-  all_args_info = AllArgsInfo(global_in_avals, in_shardings,\n-  all_args_info = AllArgsInfo(global_in_avals, in_shardings, jaxpr.debug_info)\n-\n-      all_args_info=all_args_info)\n-            f\"Unexpected XLA layout override: (XLA) {x} != {i} (User sharding)\")\n-            f\"Unexpected XLA layout override: (XLA) {x} != {o} (User sharding)\")\n-    check_gda_or_array_xla_sharding_match(kept_args, in_shardings, debug_info)\n-def check_gda_or_array_xla_sharding_match(\n-  for arg, xs, name in safe_zip(args, in_xla_shardings, arg_names):\n-          \"Got input sharding(s) that compiled object was called with: \"\n-          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\")\n-          \"Got input sharding(s) that compiled object was called with: \"\n-          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\")\n-    str_errors = '\\n'.join(errors[:num_errors])\n-          \"Compiled object called with input sharding(s) does not match the \"\n-          \"sharding(s) the computation was compiled with. \"\n",
            "whole_added": "+from jax._src.layout import SpecifiedLayout, AutoLayout\n+MaybeLayout = Sequence[Union[SpecifiedLayout, AutoLayout, None]]\n+  in_layouts: Any\n+  all_args_info = AllArgsInfo(global_in_avals, in_shardings, in_layouts,\n+      all_args_info=None)\n+            f\"Unexpected XLA layout override: (XLA) {x} != {i} (User layout)\")\n+            f\"Unexpected XLA layout override: (XLA) {x} != {o} (User layout)\")\n+      in_layouts = self._in_layouts\n+      iter_in_layouts = iter(self._in_layouts)\n+      in_layouts = [next(iter_in_layouts) if i in self._kept_var_idx else s\n+                    for i, s in enumerate(self._all_args_info.in_layouts)]\n+    check_array_xla_sharding_layout_match(kept_args, in_shardings,\n+                                          in_layouts, debug_info)\n+def check_array_xla_sharding_layout_match(\n+    in_xla_layouts: Sequence[SpecifiedLayout],\n+  for arg, xs, xl, name in safe_zip(args, in_xla_shardings, in_xla_layouts,\n+                                    arg_names):\n+          (\"Got input sharding(s) that compiled object was called with: \"\n+          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\",\n+          'sharding'))\n+          (\"Got input sharding(s) that compiled object was called with: \"\n+          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\",\n+          'sharding'))\n+\n+    if (xla_extension_version >= 249 and not db_xs and arg._committed and\n+        arg.layout is not None and xl is not None and arg.layout != xl):\n+      errors.append(\n+          (\"Got input layout(s) that compiled object was called with: \"\n+          f\"{arg.layout} and layout(s) the computation was compiled \"\n+          f\"with: {xl} for arg {name} with shape: {arg.aval.str_short()}\",\n+          'layout'))\n+    first_errors, error_kinds = unzip2(errors[:num_errors])\n+    str_errors = '\\n'.join(first_errors)\n+    if all(k == 'sharding' for k in error_kinds):\n+      kind_str = r'sharding(s)'\n+    elif all(k == 'layout' for k in error_kinds):\n+      kind_str = 'layout(s)'\n+    else:\n+      kind_str = 'sharding(s) and layout(s)'\n+          f\"Compiled object called with input {kind_str} does \"\n+          f\"not match the {kind_str} the computation was \"\n+          \"compiled with. \"\n",
            "whole_hunk": "@@ -60,7 +60,7 @@ from jax._src.interpreters import batching\n from jax._src.interpreters import partial_eval as pe\n from jax._src.interpreters import mlir\n from jax._src.interpreters import xla\n-from jax._src.layout import XLACompatibleLayout, SpecifiedLayout, LayoutRequest\n+from jax._src.layout import SpecifiedLayout, AutoLayout\n from jax._src.lib import xla_client as xc\n from jax._src.lib import xla_extension_version\n from jax._src.lib.mlir import ir\n@@ -1985,13 +1985,14 @@ def are_all_shardings_default_mem_kind(da_object, shardings):\n       return False\n   return True\n \n-MaybeLayout = Sequence[Union[XLACompatibleLayout, LayoutRequest, None]]\n+MaybeLayout = Sequence[Union[SpecifiedLayout, AutoLayout, None]]\n \n \n class AllArgsInfo(NamedTuple):\n   \"\"\"Avals, shardings, layouts and debug_info for all arguments prior to DCE.\"\"\"\n   in_avals: Sequence[core.ShapedArray]\n   in_shardings: Any\n+  in_layouts: Any\n   debug_info: core.JaxprDebugInfo | None\n \n \n@@ -2023,7 +2024,7 @@ def lower_sharding_computation(\n   auto_spmd_lowering = check_if_any_auto(\n       it.chain.from_iterable([in_shardings, out_shardings]))  # type: ignore\n \n-  all_args_info = AllArgsInfo(global_in_avals, in_shardings,\n+  all_args_info = AllArgsInfo(global_in_avals, in_shardings, in_layouts,\n                               closed_jaxpr.jaxpr.debug_info)\n \n   (closed_jaxpr, global_in_avals, global_out_avals, donated_invars,\n@@ -2227,8 +2228,6 @@ def lower_mesh_computation(\n       out_jaxpr_avals = fun_or_jaxpr.out_avals\n       consts = fun_or_jaxpr.consts\n \n-  all_args_info = AllArgsInfo(global_in_avals, in_shardings, jaxpr.debug_info)\n-\n   assert len(out_shardings) == len(out_jaxpr_avals)\n   if spmd_lowering:\n     global_out_avals = out_jaxpr_avals\n@@ -2319,7 +2318,7 @@ def lower_mesh_computation(\n       in_layouts=(None,) * len(global_in_avals),\n       out_layouts=(None,) * len(global_out_avals),\n       shape_poly_state=lowering_result.shape_poly_state,\n-      all_args_info=all_args_info)\n+      all_args_info=None)\n \n class MeshComputation(stages.XlaLowering):\n   _hlo: ir.Module | None\n@@ -2599,7 +2598,7 @@ def _get_layouts_from_executable(\n     if isinstance(i, SpecifiedLayout):\n       if i != x:\n         raise AssertionError(\n-            f\"Unexpected XLA layout override: (XLA) {x} != {i} (User sharding)\")\n+            f\"Unexpected XLA layout override: (XLA) {x} != {i} (User layout)\")\n       new_in_layouts.append(i)\n     else:\n       new_in_layouts.append(x)\n@@ -2610,7 +2609,7 @@ def _get_layouts_from_executable(\n     if isinstance(o, SpecifiedLayout):\n       if o != x:\n         raise AssertionError(\n-            f\"Unexpected XLA layout override: (XLA) {x} != {o} (User sharding)\")\n+            f\"Unexpected XLA layout override: (XLA) {x} != {o} (User layout)\")\n       new_out_layouts.append(o)\n     else:\n       new_out_layouts.append(x)\n@@ -3016,6 +3015,7 @@ class MeshExecutable(stages.XlaExecutable):\n       kept_args = [a for i, a in enumerate(args) if i in self._kept_var_idx]\n       ref_avals = self.in_avals\n       in_shardings = self._in_shardings\n+      in_layouts = self._in_layouts\n       debug_info = None\n     else:\n       kept_args = args\n@@ -3023,12 +3023,16 @@ class MeshExecutable(stages.XlaExecutable):\n       iter_in_shardings = iter(self._in_shardings)\n       in_shardings = [next(iter_in_shardings) if i in self._kept_var_idx else s\n                       for i, s in enumerate(self._all_args_info.in_shardings)]\n+      iter_in_layouts = iter(self._in_layouts)\n+      in_layouts = [next(iter_in_layouts) if i in self._kept_var_idx else s\n+                    for i, s in enumerate(self._all_args_info.in_layouts)]\n       debug_info = self._all_args_info.debug_info\n \n     arg_avals = map(xla.abstractify, kept_args)\n     check_arg_avals_for_call(ref_avals, arg_avals, debug_info)\n     # Check the GDA sharding and the input sharding.\n-    check_gda_or_array_xla_sharding_match(kept_args, in_shardings, debug_info)\n+    check_array_xla_sharding_layout_match(kept_args, in_shardings,\n+                                          in_layouts, debug_info)\n     return self.unsafe_call(*args)  # pylint: disable=not-callable\n \n   def input_shardings(self) -> Sequence[sharding_impls.XLACompatibleSharding]:\n@@ -3184,15 +3188,17 @@ def check_device_backend_on_shardings(shardings) -> bool:\n   return False\n \n \n-def check_gda_or_array_xla_sharding_match(\n+def check_array_xla_sharding_layout_match(\n     args, in_xla_shardings: Sequence[sharding_impls.XLACompatibleSharding],\n+    in_xla_layouts: Sequence[SpecifiedLayout],\n     jaxpr_debug_info: core.JaxprDebugInfo | None) -> None:\n   from jax._src.array import ArrayImpl\n   arg_names = ([''] * len(args) if jaxpr_debug_info is None else\n                jaxpr_debug_info.arg_names)\n   errors = []\n   num_errors = 5\n-  for arg, xs, name in safe_zip(args, in_xla_shardings, arg_names):\n+  for arg, xs, xl, name in safe_zip(args, in_xla_shardings, in_xla_layouts,\n+                                    arg_names):\n     if not isinstance(arg, ArrayImpl):\n       continue\n     if is_unspecified_or_auto(xs):\n@@ -3205,27 +3211,45 @@ def check_gda_or_array_xla_sharding_match(\n     # Raise memory kind mismatch error even if the arg is uncommitted.\n     if arg.sharding.memory_kind != xs.memory_kind:\n       errors.append(\n-          \"Got input sharding(s) that compiled object was called with: \"\n+          (\"Got input sharding(s) that compiled object was called with: \"\n           f\"{arg.sharding} and sharding(s) the computation was compiled \"\n-          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\")\n+          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\",\n+          'sharding'))\n \n     if (not db_xs and arg._committed and\n         not op_shardings.are_op_shardings_equal(\n             arg.sharding._to_xla_hlo_sharding(arg.ndim),\n             xs._to_xla_hlo_sharding(arg.ndim))):\n       errors.append(\n-          \"Got input sharding(s) that compiled object was called with: \"\n+          (\"Got input sharding(s) that compiled object was called with: \"\n           f\"{arg.sharding} and sharding(s) the computation was compiled \"\n-          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\")\n+          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\",\n+          'sharding'))\n+\n+    if (xla_extension_version >= 249 and not db_xs and arg._committed and\n+        arg.layout is not None and xl is not None and arg.layout != xl):\n+      errors.append(\n+          (\"Got input layout(s) that compiled object was called with: \"\n+          f\"{arg.layout} and layout(s) the computation was compiled \"\n+          f\"with: {xl} for arg {name} with shape: {arg.aval.str_short()}\",\n+          'layout'))\n \n   if errors:\n-    str_errors = '\\n'.join(errors[:num_errors])\n+    first_errors, error_kinds = unzip2(errors[:num_errors])\n+    str_errors = '\\n'.join(first_errors)\n+    if all(k == 'sharding' for k in error_kinds):\n+      kind_str = r'sharding(s)'\n+    elif all(k == 'layout' for k in error_kinds):\n+      kind_str = 'layout(s)'\n+    else:\n+      kind_str = 'sharding(s) and layout(s)'\n     num_mismatch_str = (\n         f'the {len(errors)} mismatches' if len(errors) < num_errors else\n         f\"{num_errors} mismatches out of {len(errors)}\")\n     raise ValueError(\n-          \"Compiled object called with input sharding(s) does not match the \"\n-          \"sharding(s) the computation was compiled with. \"\n+          f\"Compiled object called with input {kind_str} does \"\n+          f\"not match the {kind_str} the computation was \"\n+          \"compiled with. \"\n           f\"Here are {num_mismatch_str}:\\n{str_errors}\")\n \n \n"
        },
        {
            "name": "layout.py",
            "path": "jax/_src/layout.py",
            "patches": [
                {
                    "old_start": 14,
                    "old_length": 8,
                    "new_start": 14,
                    "new_length": 6,
                    "hunk": "@@ -14,8 +14,6 @@\n \n from __future__ import annotations\n \n-import re\n-\n from jax._src.lib import xla_client as xc\n \n \n"
                },
                {
                    "old_start": 24,
                    "old_length": 16,
                    "new_start": 22,
                    "new_length": 10,
                    "hunk": "@@ -24,16 +22,10 @@ class Layout:\n   pass\n \n \n-class XLACompatibleLayout(Layout):\n-\n-  def _to_xla_layout(self) -> str:\n-    raise NotImplementedError(\"Subclasses should implement this method.\")\n-\n-\n-class SpecifiedLayout(XLACompatibleLayout):\n-  layout: xc.Layout\n+class SpecifiedLayout(Layout):\n+  layout: xc.PjRtLayout\n \n-  def __init__(self, layout: xc.Layout):\n+  def __init__(self, layout: xc.PjRtLayout):\n     self._layout = layout\n     self._layout_str = str(self._layout)\n \n"
                },
                {
                    "old_start": 51,
                    "old_length": 19,
                    "new_start": 43,
                    "new_length": 10,
                    "hunk": "@@ -51,19 +43,10 @@ class SpecifiedLayout(XLACompatibleLayout):\n   def _to_xla_layout(self) -> str:\n     return self._layout_str\n \n-  @property\n-  def _minor_to_major(self):\n-    m = re.search(\"{([0-9,]*):\", str(self))\n-    assert m is not None\n-    m2m_str = m.group(1)\n-    if m2m_str == \"\":\n-      return ()\n-    return tuple(int(x) for x in m2m_str.split(\",\"))\n-\n \n-class LayoutRequest:\n+class AutoLayout:\n \n   def __repr__(self):\n-    return \"Request a layout from the compiler\"\n+    return \"AUTO\"\n \n-AUTO = LayoutRequest()\n+AUTO = AutoLayout()\n"
                }
            ],
            "whole_deleted": "-import re\n-\n-class XLACompatibleLayout(Layout):\n-\n-  def _to_xla_layout(self) -> str:\n-    raise NotImplementedError(\"Subclasses should implement this method.\")\n-\n-\n-class SpecifiedLayout(XLACompatibleLayout):\n-  layout: xc.Layout\n-  def __init__(self, layout: xc.Layout):\n-  @property\n-  def _minor_to_major(self):\n-    m = re.search(\"{([0-9,]*):\", str(self))\n-    assert m is not None\n-    m2m_str = m.group(1)\n-    if m2m_str == \"\":\n-      return ()\n-    return tuple(int(x) for x in m2m_str.split(\",\"))\n-\n-class LayoutRequest:\n-    return \"Request a layout from the compiler\"\n-AUTO = LayoutRequest()\n",
            "whole_added": "+class SpecifiedLayout(Layout):\n+  layout: xc.PjRtLayout\n+  def __init__(self, layout: xc.PjRtLayout):\n+class AutoLayout:\n+    return \"AUTO\"\n+AUTO = AutoLayout()\n",
            "whole_hunk": "@@ -14,8 +14,6 @@\n \n from __future__ import annotations\n \n-import re\n-\n from jax._src.lib import xla_client as xc\n \n \n@@ -24,16 +22,10 @@ class Layout:\n   pass\n \n \n-class XLACompatibleLayout(Layout):\n-\n-  def _to_xla_layout(self) -> str:\n-    raise NotImplementedError(\"Subclasses should implement this method.\")\n-\n-\n-class SpecifiedLayout(XLACompatibleLayout):\n-  layout: xc.Layout\n+class SpecifiedLayout(Layout):\n+  layout: xc.PjRtLayout\n \n-  def __init__(self, layout: xc.Layout):\n+  def __init__(self, layout: xc.PjRtLayout):\n     self._layout = layout\n     self._layout_str = str(self._layout)\n \n@@ -51,19 +43,10 @@ class SpecifiedLayout(XLACompatibleLayout):\n   def _to_xla_layout(self) -> str:\n     return self._layout_str\n \n-  @property\n-  def _minor_to_major(self):\n-    m = re.search(\"{([0-9,]*):\", str(self))\n-    assert m is not None\n-    m2m_str = m.group(1)\n-    if m2m_str == \"\":\n-      return ()\n-    return tuple(int(x) for x in m2m_str.split(\",\"))\n-\n \n-class LayoutRequest:\n+class AutoLayout:\n \n   def __repr__(self):\n-    return \"Request a layout from the compiler\"\n+    return \"AUTO\"\n \n-AUTO = LayoutRequest()\n+AUTO = AutoLayout()\n"
        },
        {
            "name": "pjit.py",
            "path": "jax/_src/pjit.py",
            "patches": [
                {
                    "old_start": 435,
                    "old_length": 6,
                    "new_start": 435,
                    "new_length": 8,
                    "hunk": "@@ -435,6 +435,8 @@ def _make_jit_wrapper(jit_info: PjitInfo):\n     try:\n       in_shardings = _resolve_in_shardings(\n           args_flat, params['in_shardings'], params['out_shardings'], mesh)\n+      in_layouts_flat = _resolve_in_layouts(\n+          args_flat, in_layouts_flat, in_shardings)\n       lowering = _pjit_lower(\n           params['jaxpr'], in_shardings, params['out_shardings'],\n           params['resource_env'], params['donated_invars'], params['name'],\n"
                },
                {
                    "old_start": 1264,
                    "old_length": 6,
                    "new_start": 1265,
                    "new_length": 35,
                    "hunk": "@@ -1264,6 +1265,35 @@ pjit_p = core.AxisPrimitive(\"pjit\")\n pjit_p.multiple_results = True\n \n \n+def _resolve_in_layouts(args, jit_in_layouts, jit_in_shardings):\n+  # If device or backend is set, return the default layout. This is because you\n+  # can pass arrays on cpu (with untiled layouts) to jit with backend='tpu'\n+  # which causes error checks to fail. Returning the default layout allows\n+  # this to exist. It's the same for handling shardings.\n+  if pxla.check_device_backend_on_shardings(jit_in_shardings):\n+    return (None,) * len(jit_in_layouts)\n+\n+  resolved_in_layouts = []\n+  for arg, jit_in_l in safe_zip(args, jit_in_layouts):\n+    arg_layout, committed = (\n+        (arg.layout, getattr(arg, '_committed', True))\n+        if getattr(arg, 'layout', None) is not None else (None, False))\n+    if jit_in_l is None:\n+      if committed:\n+        resolved_in_layouts.append(arg_layout)\n+      else:\n+        resolved_in_layouts.append(None)\n+    else:\n+      if committed and arg_layout != jit_in_l:\n+        raise ValueError('Layout passed to jit does not match the layout '\n+                          'on the respective arg. '\n+                          f'Got pjit layout: {jit_in_l},\\n'\n+                          f'arg sharding: {arg_layout} for '\n+                          f'arg shape: {shaped_abstractify(arg).str_short()}')\n+      resolved_in_layouts.append(jit_in_l)\n+  return tuple(resolved_in_layouts)\n+\n+\n def _resolve_in_shardings(\n     args, pjit_in_shardings: Sequence[PjitSharding],\n     out_shardings: Sequence[PjitSharding],\n"
                },
                {
                    "old_start": 1387,
                    "old_length": 8,
                    "new_start": 1417,
                    "new_length": 9,
                    "hunk": "@@ -1387,8 +1417,9 @@ def _pjit_call_impl_python(\n   _most_recent_pjit_call_executable.weak_key_dict[jaxpr] = compiled\n   # This check is expensive so only do it if enable_checks is on.\n   if compiled._auto_spmd_lowering and config.enable_checks.value:\n-    pxla.check_gda_or_array_xla_sharding_match(args, compiled._in_shardings,\n-                                               jaxpr.jaxpr.debug_info)\n+    pxla.check_array_xla_sharding_layout_match(\n+        args, compiled._in_shardings, compiled._in_layouts,\n+        jaxpr.jaxpr.debug_info)\n   if config.distributed_debug.value:\n     # Defensively only perform fingerprint logic if debug logging is enabled\n     # NOTE(skyewm): I didn't benchmark this\n"
                }
            ],
            "whole_deleted": "-    pxla.check_gda_or_array_xla_sharding_match(args, compiled._in_shardings,\n-                                               jaxpr.jaxpr.debug_info)\n",
            "whole_added": "+      in_layouts_flat = _resolve_in_layouts(\n+          args_flat, in_layouts_flat, in_shardings)\n+def _resolve_in_layouts(args, jit_in_layouts, jit_in_shardings):\n+  # If device or backend is set, return the default layout. This is because you\n+  # can pass arrays on cpu (with untiled layouts) to jit with backend='tpu'\n+  # which causes error checks to fail. Returning the default layout allows\n+  # this to exist. It's the same for handling shardings.\n+  if pxla.check_device_backend_on_shardings(jit_in_shardings):\n+    return (None,) * len(jit_in_layouts)\n+\n+  resolved_in_layouts = []\n+  for arg, jit_in_l in safe_zip(args, jit_in_layouts):\n+    arg_layout, committed = (\n+        (arg.layout, getattr(arg, '_committed', True))\n+        if getattr(arg, 'layout', None) is not None else (None, False))\n+    if jit_in_l is None:\n+      if committed:\n+        resolved_in_layouts.append(arg_layout)\n+      else:\n+        resolved_in_layouts.append(None)\n+    else:\n+      if committed and arg_layout != jit_in_l:\n+        raise ValueError('Layout passed to jit does not match the layout '\n+                          'on the respective arg. '\n+                          f'Got pjit layout: {jit_in_l},\\n'\n+                          f'arg sharding: {arg_layout} for '\n+                          f'arg shape: {shaped_abstractify(arg).str_short()}')\n+      resolved_in_layouts.append(jit_in_l)\n+  return tuple(resolved_in_layouts)\n+\n+\n+    pxla.check_array_xla_sharding_layout_match(\n+        args, compiled._in_shardings, compiled._in_layouts,\n+        jaxpr.jaxpr.debug_info)\n",
            "whole_hunk": "@@ -435,6 +435,8 @@ def _make_jit_wrapper(jit_info: PjitInfo):\n     try:\n       in_shardings = _resolve_in_shardings(\n           args_flat, params['in_shardings'], params['out_shardings'], mesh)\n+      in_layouts_flat = _resolve_in_layouts(\n+          args_flat, in_layouts_flat, in_shardings)\n       lowering = _pjit_lower(\n           params['jaxpr'], in_shardings, params['out_shardings'],\n           params['resource_env'], params['donated_invars'], params['name'],\n@@ -1264,6 +1265,35 @@ pjit_p = core.AxisPrimitive(\"pjit\")\n pjit_p.multiple_results = True\n \n \n+def _resolve_in_layouts(args, jit_in_layouts, jit_in_shardings):\n+  # If device or backend is set, return the default layout. This is because you\n+  # can pass arrays on cpu (with untiled layouts) to jit with backend='tpu'\n+  # which causes error checks to fail. Returning the default layout allows\n+  # this to exist. It's the same for handling shardings.\n+  if pxla.check_device_backend_on_shardings(jit_in_shardings):\n+    return (None,) * len(jit_in_layouts)\n+\n+  resolved_in_layouts = []\n+  for arg, jit_in_l in safe_zip(args, jit_in_layouts):\n+    arg_layout, committed = (\n+        (arg.layout, getattr(arg, '_committed', True))\n+        if getattr(arg, 'layout', None) is not None else (None, False))\n+    if jit_in_l is None:\n+      if committed:\n+        resolved_in_layouts.append(arg_layout)\n+      else:\n+        resolved_in_layouts.append(None)\n+    else:\n+      if committed and arg_layout != jit_in_l:\n+        raise ValueError('Layout passed to jit does not match the layout '\n+                          'on the respective arg. '\n+                          f'Got pjit layout: {jit_in_l},\\n'\n+                          f'arg sharding: {arg_layout} for '\n+                          f'arg shape: {shaped_abstractify(arg).str_short()}')\n+      resolved_in_layouts.append(jit_in_l)\n+  return tuple(resolved_in_layouts)\n+\n+\n def _resolve_in_shardings(\n     args, pjit_in_shardings: Sequence[PjitSharding],\n     out_shardings: Sequence[PjitSharding],\n@@ -1387,8 +1417,9 @@ def _pjit_call_impl_python(\n   _most_recent_pjit_call_executable.weak_key_dict[jaxpr] = compiled\n   # This check is expensive so only do it if enable_checks is on.\n   if compiled._auto_spmd_lowering and config.enable_checks.value:\n-    pxla.check_gda_or_array_xla_sharding_match(args, compiled._in_shardings,\n-                                               jaxpr.jaxpr.debug_info)\n+    pxla.check_array_xla_sharding_layout_match(\n+        args, compiled._in_shardings, compiled._in_layouts,\n+        jaxpr.jaxpr.debug_info)\n   if config.distributed_debug.value:\n     # Defensively only perform fingerprint logic if debug logging is enabled\n     # NOTE(skyewm): I didn't benchmark this\n"
        },
        {
            "name": "layout_test.py",
            "path": "tests/layout_test.py",
            "patches": [
                {
                    "old_start": 14,
                    "old_length": 10,
                    "new_start": 14,
                    "new_length": 12,
                    "hunk": "@@ -14,10 +14,12 @@\n \n import math\n import os\n+import re\n from absl.testing import absltest\n import numpy as np\n \n import jax\n+import jax.numpy as jnp\n from jax.sharding import NamedSharding, PartitionSpec as P\n from jax._src import config\n from jax._src import layout\n"
                },
                {
                    "old_start": 50,
                    "old_length": 6,
                    "new_start": 52,
                    "new_length": 15,
                    "hunk": "@@ -50,6 +52,15 @@ def tearDownModule():\n   xla_bridge.get_backend.cache_clear()\n \n \n+pattern = re.compile(r\"\\{(.*?):\")\n+\n+# Extract minor_to_major from str(layout) because layout doesn't have a\n+# minor_to_major property yet.\n+def extract_minor_to_major(l):\n+  match = re.search(pattern, str(l))\n+  return tuple(int(i) for i in match.groups()[0].split(','))\n+\n+\n class LayoutTest(jtu.JaxTestCase):\n \n   def setUp(self):\n"
                },
                {
                    "old_start": 60,
                    "old_length": 9,
                    "new_start": 71,
                    "new_length": 11,
                    "hunk": "@@ -60,9 +71,11 @@ class LayoutTest(jtu.JaxTestCase):\n     super().setUp()\n \n   def test_auto_layout(self):\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n     shape1 = (128, 128)\n     shape2 = (128, 128)\n+    s1 = NamedSharding(mesh, P('x', 'y'))\n+    s2 = NamedSharding(mesh, P('x'))\n \n     def apply(x, y):\n       return x.T, y.T\n"
                },
                {
                    "old_start": 71,
                    "old_length": 70,
                    "new_start": 84,
                    "new_length": 89,
                    "hunk": "@@ -71,70 +84,89 @@ class LayoutTest(jtu.JaxTestCase):\n       return x * 2, y * 2\n \n     np_inp1 = np.arange(math.prod(shape1)).reshape(shape1)\n-    arr1 = jax.device_put(np_inp1, NamedSharding(mesh, P('x', 'y')))\n     np_inp2 = np.arange(math.prod(shape2)).reshape(shape2)\n-    arr2 = jax.device_put(np_inp2, NamedSharding(mesh, P('x')))\n+    sds1 = jax.ShapeDtypeStruct(np_inp1.shape, np_inp1.dtype, sharding=s1)\n+    sds2 = jax.ShapeDtypeStruct(np_inp2.shape, np_inp2.dtype, sharding=s2)\n \n-    lowered_apply = jax.jit(apply).lower(arr1, arr2, _in_layouts=layout.AUTO,\n-                                         _out_layouts=layout.AUTO)\n+    lowered_apply = jax.jit(apply).lower(\n+        sds1, sds2, _in_layouts=layout.AUTO, _out_layouts=layout.AUTO)\n     compiled_apply = lowered_apply.compile()\n \n     arg_layouts, kw_layouts = compiled_apply._input_layouts()\n     self.assertEmpty(kw_layouts)\n+\n     for i, o in zip(arg_layouts, compiled_apply._output_layouts()):\n-      self.assertEqual(i._minor_to_major, o._minor_to_major[::-1])\n+      self.assertEqual(extract_minor_to_major(i),\n+                       extract_minor_to_major(o)[::-1])\n \n     init_compiled = jax.jit(init).lower(\n-        arr1, arr2, _out_layouts=arg_layouts).compile()\n+        sds1, sds2, _out_layouts=arg_layouts).compile()\n \n     for i, o in zip(init_compiled._input_layouts()[0],\n                     init_compiled._output_layouts()):\n-      self.assertEqual(i._minor_to_major, o._minor_to_major)\n+      self.assertEqual(i, o)\n+\n+    arr1 = jax.device_put(np_inp1, s1)\n+    arr2 = jax.device_put(np_inp2, s2)\n \n     with jtu.count_aot_jit_cpp_cache_miss() as init_count:\n       init_out = init_compiled(arr1, arr2)\n       init_compiled(arr1, arr2)\n     self.assertEqual(init_count[0], 1)\n \n+    self.assertEqual(init_out[0].layout, init_compiled._output_layouts()[0])\n+    self.assertEqual(init_out[1].layout, init_compiled._output_layouts()[0])\n+\n     with jtu.count_aot_jit_cpp_cache_miss() as apply_count:\n       apply_out = compiled_apply(*init_out)\n       compiled_apply(*init_out)\n     self.assertEqual(apply_count[0], 1)\n \n+    self.assertEqual(apply_out[0].layout, compiled_apply._output_layouts()[0])\n+    self.assertEqual(apply_out[1].layout, compiled_apply._output_layouts()[1])\n+\n+    self.assertTupleEqual(extract_minor_to_major(apply_out[0].layout),\n+                          extract_minor_to_major(init_out[0].layout)[::-1])\n+    self.assertTupleEqual(extract_minor_to_major(apply_out[1].layout),\n+                          extract_minor_to_major(init_out[1].layout)[::-1])\n+\n     self.assertArraysEqual(init_out[0], np_inp1 * 2)\n     self.assertArraysEqual(init_out[1], np_inp2 * 2)\n     self.assertArraysEqual(apply_out[0], (np_inp1 * 2).T)\n     self.assertArraysEqual(apply_out[1], (np_inp2 * 2).T)\n \n   def test_default_layout(self):\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n-    shape = (8, 4, 2)\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+    shape = (4, 4, 2)\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n     s = NamedSharding(mesh, P('x', 'y'))\n+    sds = jax.ShapeDtypeStruct(np_inp.shape, np_inp.dtype, sharding=s)\n     arr = jax.device_put(np_inp, s)\n \n     def f(x):\n       return x.T\n \n-    lowered = jax.jit(f).lower(arr, _in_layouts=None, _out_layouts=None)\n+    lowered = jax.jit(f).lower(sds, _in_layouts=None, _out_layouts=None)\n     self.assertIn(\"default\", lowered.as_text())\n     compiled = lowered.compile()\n     out = compiled(arr)\n \n-    self.assertTupleEqual(compiled._input_layouts()[0][0]._minor_to_major, (2, 1, 0))\n-    self.assertTupleEqual(compiled._output_layouts()._minor_to_major, (2, 1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._input_layouts()[0][0]), (2, 1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._output_layouts()), (2, 1, 0))\n     self.assertArraysEqual(out, np_inp.T)\n     self.assertEqual(out.sharding, NamedSharding(mesh, P(None, 'y', 'x')))\n \n-    compiled_auto = jax.jit(f).lower(arr, _in_layouts=layout.AUTO,\n+    compiled_auto = jax.jit(f).lower(sds, _in_layouts=layout.AUTO,\n                                      _out_layouts=layout.AUTO).compile()\n-    self.assertTupleEqual(compiled_auto._input_layouts()[0][0]._minor_to_major,\n-                          (2, 1, 0))\n-    self.assertTupleEqual(compiled_auto._output_layouts()._minor_to_major,\n-                          (0, 1, 2))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled_auto._input_layouts()[0][0]), (2, 1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled_auto._output_layouts()), (0, 1, 2))\n \n   def test_in_layouts_out_layouts(self):\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n     shape = (8, 8)\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n     s = NamedSharding(mesh, P('x', 'y'))\n"
                },
                {
                    "old_start": 142,
                    "old_length": 17,
                    "new_start": 174,
                    "new_length": 21,
                    "hunk": "@@ -142,17 +174,21 @@ class LayoutTest(jtu.JaxTestCase):\n \n     def f(x):\n       return x.T\n+\n     compiled = jax.jit(f).lower(\n         arr, _in_layouts=None, _out_layouts=layout.AUTO).compile()\n-    self.assertTupleEqual(compiled._input_layouts()[0][0]._minor_to_major, (1, 0))\n-    self.assertTupleEqual(compiled._output_layouts()._minor_to_major, (0, 1))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._input_layouts()[0][0]), (1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._output_layouts()), (0, 1))\n \n     out = compiled(arr)\n     self.assertArraysEqual(out, np_inp.T)\n+    self.assertEqual(out.layout, compiled._output_layouts())\n     self.assertEqual(out.sharding, NamedSharding(mesh, P('y', 'x')))\n \n   def test_sharding_and_layouts(self):\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n     shape = (4, 8)\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n     s = NamedSharding(mesh, P('x', 'y'))\n"
                },
                {
                    "old_start": 160,
                    "old_length": 8,
                    "new_start": 196,
                    "new_length": 10,
                    "hunk": "@@ -160,8 +196,10 @@ class LayoutTest(jtu.JaxTestCase):\n     compiled = jax.jit(lambda x: x.T, in_shardings=s, out_shardings=s).lower(\n         np_inp, _in_layouts=layout.AUTO, _out_layouts=layout.AUTO).compile()\n     out = compiled(np_inp)\n-    self.assertTupleEqual(compiled._input_layouts()[0][0]._minor_to_major, (1, 0))\n-    self.assertTupleEqual(compiled._output_layouts()._minor_to_major, (0, 1))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._input_layouts()[0][0]), (1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._output_layouts()), (0, 1))\n     self.assertArraysEqual(out, np_inp.T)\n     self.assertEqual(out.sharding, s)\n \n"
                },
                {
                    "old_start": 188,
                    "old_length": 6,
                    "new_start": 226,
                    "new_length": 39,
                    "hunk": "@@ -188,6 +226,39 @@ class LayoutTest(jtu.JaxTestCase):\n     # TODO(yashkatariya, frostig): Also use the arg_layouts to create an Array\n     # and then pass that back into compiled.\n \n+  def test_aot_layout_mismatch(self):\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+    shape = (256, 4, 2)\n+    np_inp = np.arange(math.prod(shape)).reshape(shape)\n+    s = NamedSharding(mesh, P('x'))\n+\n+    sds = jax.ShapeDtypeStruct(np_inp.shape, np_inp.dtype, sharding=s)\n+    arr = jax.device_put(np_inp, s)\n+\n+    def f(x):\n+      return (x * 2).T\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        'Layout passed to jit does not match the layout on the respective arg'):\n+      jax.jit(f).lower(arr, _in_layouts=layout.AUTO)\n+\n+    compiled = jax.jit(f).lower(\n+        sds, _in_layouts=layout.AUTO, _out_layouts=layout.AUTO).compile()\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        r'Compiled object called with input layout\\(s\\) does'\n+        r' not match the layout\\(s\\) the computation was'\n+        ' compiled with'):\n+      compiled(arr)\n+\n+  def test_cpu_default_backend_layout(self):\n+    out_cpu = jax.jit(jnp.dot, backend='cpu')(np.ones((8, 8)), np.ones((8, 8)))\n+\n+    jax.jit(jnp.dot, backend=jax.default_backend()).lower(\n+        out_cpu, out_cpu).compile()  # doesn't crash\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n"
                }
            ],
            "whole_deleted": "-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n-    arr1 = jax.device_put(np_inp1, NamedSharding(mesh, P('x', 'y')))\n-    arr2 = jax.device_put(np_inp2, NamedSharding(mesh, P('x')))\n-    lowered_apply = jax.jit(apply).lower(arr1, arr2, _in_layouts=layout.AUTO,\n-                                         _out_layouts=layout.AUTO)\n-      self.assertEqual(i._minor_to_major, o._minor_to_major[::-1])\n-        arr1, arr2, _out_layouts=arg_layouts).compile()\n-      self.assertEqual(i._minor_to_major, o._minor_to_major)\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n-    shape = (8, 4, 2)\n-    lowered = jax.jit(f).lower(arr, _in_layouts=None, _out_layouts=None)\n-    self.assertTupleEqual(compiled._input_layouts()[0][0]._minor_to_major, (2, 1, 0))\n-    self.assertTupleEqual(compiled._output_layouts()._minor_to_major, (2, 1, 0))\n-    compiled_auto = jax.jit(f).lower(arr, _in_layouts=layout.AUTO,\n-    self.assertTupleEqual(compiled_auto._input_layouts()[0][0]._minor_to_major,\n-                          (2, 1, 0))\n-    self.assertTupleEqual(compiled_auto._output_layouts()._minor_to_major,\n-                          (0, 1, 2))\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n-    self.assertTupleEqual(compiled._input_layouts()[0][0]._minor_to_major, (1, 0))\n-    self.assertTupleEqual(compiled._output_layouts()._minor_to_major, (0, 1))\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n-    self.assertTupleEqual(compiled._input_layouts()[0][0]._minor_to_major, (1, 0))\n-    self.assertTupleEqual(compiled._output_layouts()._minor_to_major, (0, 1))\n",
            "whole_added": "+import re\n+import jax.numpy as jnp\n+pattern = re.compile(r\"\\{(.*?):\")\n+\n+# Extract minor_to_major from str(layout) because layout doesn't have a\n+# minor_to_major property yet.\n+def extract_minor_to_major(l):\n+  match = re.search(pattern, str(l))\n+  return tuple(int(i) for i in match.groups()[0].split(','))\n+\n+\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+    s1 = NamedSharding(mesh, P('x', 'y'))\n+    s2 = NamedSharding(mesh, P('x'))\n+    sds1 = jax.ShapeDtypeStruct(np_inp1.shape, np_inp1.dtype, sharding=s1)\n+    sds2 = jax.ShapeDtypeStruct(np_inp2.shape, np_inp2.dtype, sharding=s2)\n+    lowered_apply = jax.jit(apply).lower(\n+        sds1, sds2, _in_layouts=layout.AUTO, _out_layouts=layout.AUTO)\n+\n+      self.assertEqual(extract_minor_to_major(i),\n+                       extract_minor_to_major(o)[::-1])\n+        sds1, sds2, _out_layouts=arg_layouts).compile()\n+      self.assertEqual(i, o)\n+\n+    arr1 = jax.device_put(np_inp1, s1)\n+    arr2 = jax.device_put(np_inp2, s2)\n+    self.assertEqual(init_out[0].layout, init_compiled._output_layouts()[0])\n+    self.assertEqual(init_out[1].layout, init_compiled._output_layouts()[0])\n+\n+    self.assertEqual(apply_out[0].layout, compiled_apply._output_layouts()[0])\n+    self.assertEqual(apply_out[1].layout, compiled_apply._output_layouts()[1])\n+\n+    self.assertTupleEqual(extract_minor_to_major(apply_out[0].layout),\n+                          extract_minor_to_major(init_out[0].layout)[::-1])\n+    self.assertTupleEqual(extract_minor_to_major(apply_out[1].layout),\n+                          extract_minor_to_major(init_out[1].layout)[::-1])\n+\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+    shape = (4, 4, 2)\n+    sds = jax.ShapeDtypeStruct(np_inp.shape, np_inp.dtype, sharding=s)\n+    lowered = jax.jit(f).lower(sds, _in_layouts=None, _out_layouts=None)\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._input_layouts()[0][0]), (2, 1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._output_layouts()), (2, 1, 0))\n+    compiled_auto = jax.jit(f).lower(sds, _in_layouts=layout.AUTO,\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled_auto._input_layouts()[0][0]), (2, 1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled_auto._output_layouts()), (0, 1, 2))\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._input_layouts()[0][0]), (1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._output_layouts()), (0, 1))\n+    self.assertEqual(out.layout, compiled._output_layouts())\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._input_layouts()[0][0]), (1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._output_layouts()), (0, 1))\n+  def test_aot_layout_mismatch(self):\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+    shape = (256, 4, 2)\n+    np_inp = np.arange(math.prod(shape)).reshape(shape)\n+    s = NamedSharding(mesh, P('x'))\n+\n+    sds = jax.ShapeDtypeStruct(np_inp.shape, np_inp.dtype, sharding=s)\n+    arr = jax.device_put(np_inp, s)\n+\n+    def f(x):\n+      return (x * 2).T\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        'Layout passed to jit does not match the layout on the respective arg'):\n+      jax.jit(f).lower(arr, _in_layouts=layout.AUTO)\n+\n+    compiled = jax.jit(f).lower(\n+        sds, _in_layouts=layout.AUTO, _out_layouts=layout.AUTO).compile()\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        r'Compiled object called with input layout\\(s\\) does'\n+        r' not match the layout\\(s\\) the computation was'\n+        ' compiled with'):\n+      compiled(arr)\n+\n+  def test_cpu_default_backend_layout(self):\n+    out_cpu = jax.jit(jnp.dot, backend='cpu')(np.ones((8, 8)), np.ones((8, 8)))\n+\n+    jax.jit(jnp.dot, backend=jax.default_backend()).lower(\n+        out_cpu, out_cpu).compile()  # doesn't crash\n+\n",
            "whole_hunk": "@@ -14,10 +14,12 @@\n \n import math\n import os\n+import re\n from absl.testing import absltest\n import numpy as np\n \n import jax\n+import jax.numpy as jnp\n from jax.sharding import NamedSharding, PartitionSpec as P\n from jax._src import config\n from jax._src import layout\n@@ -50,6 +52,15 @@ def tearDownModule():\n   xla_bridge.get_backend.cache_clear()\n \n \n+pattern = re.compile(r\"\\{(.*?):\")\n+\n+# Extract minor_to_major from str(layout) because layout doesn't have a\n+# minor_to_major property yet.\n+def extract_minor_to_major(l):\n+  match = re.search(pattern, str(l))\n+  return tuple(int(i) for i in match.groups()[0].split(','))\n+\n+\n class LayoutTest(jtu.JaxTestCase):\n \n   def setUp(self):\n@@ -60,9 +71,11 @@ class LayoutTest(jtu.JaxTestCase):\n     super().setUp()\n \n   def test_auto_layout(self):\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n     shape1 = (128, 128)\n     shape2 = (128, 128)\n+    s1 = NamedSharding(mesh, P('x', 'y'))\n+    s2 = NamedSharding(mesh, P('x'))\n \n     def apply(x, y):\n       return x.T, y.T\n@@ -71,70 +84,89 @@ class LayoutTest(jtu.JaxTestCase):\n       return x * 2, y * 2\n \n     np_inp1 = np.arange(math.prod(shape1)).reshape(shape1)\n-    arr1 = jax.device_put(np_inp1, NamedSharding(mesh, P('x', 'y')))\n     np_inp2 = np.arange(math.prod(shape2)).reshape(shape2)\n-    arr2 = jax.device_put(np_inp2, NamedSharding(mesh, P('x')))\n+    sds1 = jax.ShapeDtypeStruct(np_inp1.shape, np_inp1.dtype, sharding=s1)\n+    sds2 = jax.ShapeDtypeStruct(np_inp2.shape, np_inp2.dtype, sharding=s2)\n \n-    lowered_apply = jax.jit(apply).lower(arr1, arr2, _in_layouts=layout.AUTO,\n-                                         _out_layouts=layout.AUTO)\n+    lowered_apply = jax.jit(apply).lower(\n+        sds1, sds2, _in_layouts=layout.AUTO, _out_layouts=layout.AUTO)\n     compiled_apply = lowered_apply.compile()\n \n     arg_layouts, kw_layouts = compiled_apply._input_layouts()\n     self.assertEmpty(kw_layouts)\n+\n     for i, o in zip(arg_layouts, compiled_apply._output_layouts()):\n-      self.assertEqual(i._minor_to_major, o._minor_to_major[::-1])\n+      self.assertEqual(extract_minor_to_major(i),\n+                       extract_minor_to_major(o)[::-1])\n \n     init_compiled = jax.jit(init).lower(\n-        arr1, arr2, _out_layouts=arg_layouts).compile()\n+        sds1, sds2, _out_layouts=arg_layouts).compile()\n \n     for i, o in zip(init_compiled._input_layouts()[0],\n                     init_compiled._output_layouts()):\n-      self.assertEqual(i._minor_to_major, o._minor_to_major)\n+      self.assertEqual(i, o)\n+\n+    arr1 = jax.device_put(np_inp1, s1)\n+    arr2 = jax.device_put(np_inp2, s2)\n \n     with jtu.count_aot_jit_cpp_cache_miss() as init_count:\n       init_out = init_compiled(arr1, arr2)\n       init_compiled(arr1, arr2)\n     self.assertEqual(init_count[0], 1)\n \n+    self.assertEqual(init_out[0].layout, init_compiled._output_layouts()[0])\n+    self.assertEqual(init_out[1].layout, init_compiled._output_layouts()[0])\n+\n     with jtu.count_aot_jit_cpp_cache_miss() as apply_count:\n       apply_out = compiled_apply(*init_out)\n       compiled_apply(*init_out)\n     self.assertEqual(apply_count[0], 1)\n \n+    self.assertEqual(apply_out[0].layout, compiled_apply._output_layouts()[0])\n+    self.assertEqual(apply_out[1].layout, compiled_apply._output_layouts()[1])\n+\n+    self.assertTupleEqual(extract_minor_to_major(apply_out[0].layout),\n+                          extract_minor_to_major(init_out[0].layout)[::-1])\n+    self.assertTupleEqual(extract_minor_to_major(apply_out[1].layout),\n+                          extract_minor_to_major(init_out[1].layout)[::-1])\n+\n     self.assertArraysEqual(init_out[0], np_inp1 * 2)\n     self.assertArraysEqual(init_out[1], np_inp2 * 2)\n     self.assertArraysEqual(apply_out[0], (np_inp1 * 2).T)\n     self.assertArraysEqual(apply_out[1], (np_inp2 * 2).T)\n \n   def test_default_layout(self):\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n-    shape = (8, 4, 2)\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+    shape = (4, 4, 2)\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n     s = NamedSharding(mesh, P('x', 'y'))\n+    sds = jax.ShapeDtypeStruct(np_inp.shape, np_inp.dtype, sharding=s)\n     arr = jax.device_put(np_inp, s)\n \n     def f(x):\n       return x.T\n \n-    lowered = jax.jit(f).lower(arr, _in_layouts=None, _out_layouts=None)\n+    lowered = jax.jit(f).lower(sds, _in_layouts=None, _out_layouts=None)\n     self.assertIn(\"default\", lowered.as_text())\n     compiled = lowered.compile()\n     out = compiled(arr)\n \n-    self.assertTupleEqual(compiled._input_layouts()[0][0]._minor_to_major, (2, 1, 0))\n-    self.assertTupleEqual(compiled._output_layouts()._minor_to_major, (2, 1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._input_layouts()[0][0]), (2, 1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._output_layouts()), (2, 1, 0))\n     self.assertArraysEqual(out, np_inp.T)\n     self.assertEqual(out.sharding, NamedSharding(mesh, P(None, 'y', 'x')))\n \n-    compiled_auto = jax.jit(f).lower(arr, _in_layouts=layout.AUTO,\n+    compiled_auto = jax.jit(f).lower(sds, _in_layouts=layout.AUTO,\n                                      _out_layouts=layout.AUTO).compile()\n-    self.assertTupleEqual(compiled_auto._input_layouts()[0][0]._minor_to_major,\n-                          (2, 1, 0))\n-    self.assertTupleEqual(compiled_auto._output_layouts()._minor_to_major,\n-                          (0, 1, 2))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled_auto._input_layouts()[0][0]), (2, 1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled_auto._output_layouts()), (0, 1, 2))\n \n   def test_in_layouts_out_layouts(self):\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n     shape = (8, 8)\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n     s = NamedSharding(mesh, P('x', 'y'))\n@@ -142,17 +174,21 @@ class LayoutTest(jtu.JaxTestCase):\n \n     def f(x):\n       return x.T\n+\n     compiled = jax.jit(f).lower(\n         arr, _in_layouts=None, _out_layouts=layout.AUTO).compile()\n-    self.assertTupleEqual(compiled._input_layouts()[0][0]._minor_to_major, (1, 0))\n-    self.assertTupleEqual(compiled._output_layouts()._minor_to_major, (0, 1))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._input_layouts()[0][0]), (1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._output_layouts()), (0, 1))\n \n     out = compiled(arr)\n     self.assertArraysEqual(out, np_inp.T)\n+    self.assertEqual(out.layout, compiled._output_layouts())\n     self.assertEqual(out.sharding, NamedSharding(mesh, P('y', 'x')))\n \n   def test_sharding_and_layouts(self):\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n     shape = (4, 8)\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n     s = NamedSharding(mesh, P('x', 'y'))\n@@ -160,8 +196,10 @@ class LayoutTest(jtu.JaxTestCase):\n     compiled = jax.jit(lambda x: x.T, in_shardings=s, out_shardings=s).lower(\n         np_inp, _in_layouts=layout.AUTO, _out_layouts=layout.AUTO).compile()\n     out = compiled(np_inp)\n-    self.assertTupleEqual(compiled._input_layouts()[0][0]._minor_to_major, (1, 0))\n-    self.assertTupleEqual(compiled._output_layouts()._minor_to_major, (0, 1))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._input_layouts()[0][0]), (1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._output_layouts()), (0, 1))\n     self.assertArraysEqual(out, np_inp.T)\n     self.assertEqual(out.sharding, s)\n \n@@ -188,6 +226,39 @@ class LayoutTest(jtu.JaxTestCase):\n     # TODO(yashkatariya, frostig): Also use the arg_layouts to create an Array\n     # and then pass that back into compiled.\n \n+  def test_aot_layout_mismatch(self):\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+    shape = (256, 4, 2)\n+    np_inp = np.arange(math.prod(shape)).reshape(shape)\n+    s = NamedSharding(mesh, P('x'))\n+\n+    sds = jax.ShapeDtypeStruct(np_inp.shape, np_inp.dtype, sharding=s)\n+    arr = jax.device_put(np_inp, s)\n+\n+    def f(x):\n+      return (x * 2).T\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        'Layout passed to jit does not match the layout on the respective arg'):\n+      jax.jit(f).lower(arr, _in_layouts=layout.AUTO)\n+\n+    compiled = jax.jit(f).lower(\n+        sds, _in_layouts=layout.AUTO, _out_layouts=layout.AUTO).compile()\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        r'Compiled object called with input layout\\(s\\) does'\n+        r' not match the layout\\(s\\) the computation was'\n+        ' compiled with'):\n+      compiled(arr)\n+\n+  def test_cpu_default_backend_layout(self):\n+    out_cpu = jax.jit(jnp.dot, backend='cpu')(np.ones((8, 8)), np.ones((8, 8)))\n+\n+    jax.jit(jnp.dot, backend=jax.default_backend()).lower(\n+        out_cpu, out_cpu).compile()  # doesn't crash\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n"
        },
        {
            "name": "memories_test.py",
            "path": "tests/memories_test.py",
            "patches": [
                {
                    "old_start": 198,
                    "old_length": 8,
                    "new_start": 198,
                    "new_length": 10,
                    "hunk": "@@ -198,8 +198,10 @@ class ShardingMemoriesTest(jtu.JaxTestCase):\n     self.assertEqual(dev.default_memory().kind, \"device\")\n \n   def test_parameter_streaming(self):\n-    _, s_host, _, inp_host = _create_inputs(\n-        (8, 2), P(\"x\", \"y\"), mem_kind=\"unpinned_host\")\n+    self.skipTest(\"Enable after pinned_host support exists\")\n+\n+    _, s_host, np_inp, inp_host = _create_inputs(\n+        (8, 2), P(\"x\", \"y\"), mem_kind=\"pinned_host\")\n     s_dev = s_host.with_memory_kind('device')\n     inp_dev = jax.device_put(inp_host, s_dev)\n \n"
                }
            ],
            "whole_deleted": "-    _, s_host, _, inp_host = _create_inputs(\n-        (8, 2), P(\"x\", \"y\"), mem_kind=\"unpinned_host\")\n",
            "whole_added": "+    self.skipTest(\"Enable after pinned_host support exists\")\n+\n+    _, s_host, np_inp, inp_host = _create_inputs(\n+        (8, 2), P(\"x\", \"y\"), mem_kind=\"pinned_host\")\n",
            "whole_hunk": "@@ -198,8 +198,10 @@ class ShardingMemoriesTest(jtu.JaxTestCase):\n     self.assertEqual(dev.default_memory().kind, \"device\")\n \n   def test_parameter_streaming(self):\n-    _, s_host, _, inp_host = _create_inputs(\n-        (8, 2), P(\"x\", \"y\"), mem_kind=\"unpinned_host\")\n+    self.skipTest(\"Enable after pinned_host support exists\")\n+\n+    _, s_host, np_inp, inp_host = _create_inputs(\n+        (8, 2), P(\"x\", \"y\"), mem_kind=\"pinned_host\")\n     s_dev = s_host.with_memory_kind('device')\n     inp_dev = jax.device_put(inp_host, s_dev)\n \n"
        },
        {
            "name": "pjit_test.py",
            "path": "tests/pjit_test.py",
            "patches": [
                {
                    "old_start": 2848,
                    "old_length": 6,
                    "new_start": 2848,
                    "new_length": 20,
                    "hunk": "@@ -2848,6 +2848,20 @@ class ArrayPjitTest(jtu.JaxTestCase):\n     self.assertEqual(compiled._executable._kept_var_idx, {5})\n     self.assertLen(compiled._executable.in_avals, 1)\n \n+  def test_pjit_relayout_multi_slice(self):\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+\n+    @jax.jit\n+    def mul(x):\n+      return x @ x.T\n+\n+    x = jnp.arange(8).reshape(4, 2)\n+    y = jax.device_put(x, jax.sharding.NamedSharding(mesh, P('x', 'y')))\n+    compiled = mul.lower(jax.ShapeDtypeStruct(\n+        y.shape, y.dtype, sharding=y.sharding)).compile()\n+    out = compiled(y)\n+    self.assertArraysEqual(out, x @ x.T)\n+\n   def test_pjit_with_device_arg(self):\n     def mul(x):\n       return x @ x.T"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  def test_pjit_relayout_multi_slice(self):\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+\n+    @jax.jit\n+    def mul(x):\n+      return x @ x.T\n+\n+    x = jnp.arange(8).reshape(4, 2)\n+    y = jax.device_put(x, jax.sharding.NamedSharding(mesh, P('x', 'y')))\n+    compiled = mul.lower(jax.ShapeDtypeStruct(\n+        y.shape, y.dtype, sharding=y.sharding)).compile()\n+    out = compiled(y)\n+    self.assertArraysEqual(out, x @ x.T)\n+\n",
            "whole_hunk": "@@ -2848,6 +2848,20 @@ class ArrayPjitTest(jtu.JaxTestCase):\n     self.assertEqual(compiled._executable._kept_var_idx, {5})\n     self.assertLen(compiled._executable.in_avals, 1)\n \n+  def test_pjit_relayout_multi_slice(self):\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+\n+    @jax.jit\n+    def mul(x):\n+      return x @ x.T\n+\n+    x = jnp.arange(8).reshape(4, 2)\n+    y = jax.device_put(x, jax.sharding.NamedSharding(mesh, P('x', 'y')))\n+    compiled = mul.lower(jax.ShapeDtypeStruct(\n+        y.shape, y.dtype, sharding=y.sharding)).compile()\n+    out = compiled(y)\n+    self.assertArraysEqual(out, x @ x.T)\n+\n   def test_pjit_with_device_arg(self):\n     def mul(x):\n       return x @ x.T"
        }
    ]
},
{
    "Id": 68,
    "commit_link": "https://github.com/google/jax/commit/2a4e1caac465bb4cb448e7370b23a822cce699e2",
    "date": "2024-03-25T05:53:52-07:00",
    "message": "[callback] Fix io_callback for callbacks that return Python literals.\n\nThe internal implementation of io_callback and friends currently use .shape and .dtype on the result of the callback. This fails if the callback returns a Python literal.\n\nFixed the checks that the callback returns values of expected shape and dtype,\nand added tests.\n\nPiperOrigin-RevId: 618814787",
    "changes": [
        {
            "name": "mlir.py",
            "path": "jax/_src/interpreters/mlir.py",
            "patches": [
                {
                    "old_start": 2462,
                    "old_length": 14,
                    "new_start": 2462,
                    "new_length": 20,
                    "hunk": "@@ -2462,14 +2462,20 @@ def emit_python_callback(\n           \"Mismatched number of outputs from callback. \"\n           \"Expected: {}, Actual: {}\".format(len(result_avals), len(out_vals)))\n     for i, (out_val, out_aval) in enumerate(zip(out_vals, result_avals)):\n-      if out_val.shape != out_aval.shape:\n+      actual_shape = np.shape(out_val)\n+      if actual_shape != out_aval.shape:\n         raise RuntimeError(\n             f\"Incorrect output shape for return value {i}: \"\n-            \"Expected: {}, Actual: {}\".format(out_aval.shape, out_val.shape))\n-      if out_val.dtype != out_aval.dtype:\n+            \"Expected: {}, Actual: {}\".format(out_aval.shape, actual_shape))\n+      actual_dtype = np.result_type(out_val)\n+      if actual_dtype != dtypes.canonicalize_dtype(actual_dtype):\n+        raise ValueError(\n+            \"Cannot return 64-bit values when `jax_enable_x64` is disabled\")\n+      if actual_dtype != out_aval.dtype:\n         raise RuntimeError(\n             f\"Incorrect output dtype for return value {i}: \"\n-            \"Expected: {}, Actual: {}\".format(out_aval.dtype, out_val.dtype))\n+            \"Expected: {}, Actual: {}\".format(out_aval.dtype, actual_dtype))\n+\n     if platform == \"tpu\":\n       # On TPU we cannot receive empty arrays. So, we return from the wrapped\n       # callback only the non-empty results, and we will create empty constants\n"
                }
            ],
            "whole_deleted": "-      if out_val.shape != out_aval.shape:\n-            \"Expected: {}, Actual: {}\".format(out_aval.shape, out_val.shape))\n-      if out_val.dtype != out_aval.dtype:\n-            \"Expected: {}, Actual: {}\".format(out_aval.dtype, out_val.dtype))\n",
            "whole_added": "+      actual_shape = np.shape(out_val)\n+      if actual_shape != out_aval.shape:\n+            \"Expected: {}, Actual: {}\".format(out_aval.shape, actual_shape))\n+      actual_dtype = np.result_type(out_val)\n+      if actual_dtype != dtypes.canonicalize_dtype(actual_dtype):\n+        raise ValueError(\n+            \"Cannot return 64-bit values when `jax_enable_x64` is disabled\")\n+      if actual_dtype != out_aval.dtype:\n+            \"Expected: {}, Actual: {}\".format(out_aval.dtype, actual_dtype))\n+\n",
            "whole_hunk": "@@ -2462,14 +2462,20 @@ def emit_python_callback(\n           \"Mismatched number of outputs from callback. \"\n           \"Expected: {}, Actual: {}\".format(len(result_avals), len(out_vals)))\n     for i, (out_val, out_aval) in enumerate(zip(out_vals, result_avals)):\n-      if out_val.shape != out_aval.shape:\n+      actual_shape = np.shape(out_val)\n+      if actual_shape != out_aval.shape:\n         raise RuntimeError(\n             f\"Incorrect output shape for return value {i}: \"\n-            \"Expected: {}, Actual: {}\".format(out_aval.shape, out_val.shape))\n-      if out_val.dtype != out_aval.dtype:\n+            \"Expected: {}, Actual: {}\".format(out_aval.shape, actual_shape))\n+      actual_dtype = np.result_type(out_val)\n+      if actual_dtype != dtypes.canonicalize_dtype(actual_dtype):\n+        raise ValueError(\n+            \"Cannot return 64-bit values when `jax_enable_x64` is disabled\")\n+      if actual_dtype != out_aval.dtype:\n         raise RuntimeError(\n             f\"Incorrect output dtype for return value {i}: \"\n-            \"Expected: {}, Actual: {}\".format(out_aval.dtype, out_val.dtype))\n+            \"Expected: {}, Actual: {}\".format(out_aval.dtype, actual_dtype))\n+\n     if platform == \"tpu\":\n       # On TPU we cannot receive empty arrays. So, we return from the wrapped\n       # callback only the non-empty results, and we will create empty constants\n"
        },
        {
            "name": "python_callback_test.py",
            "path": "tests/python_callback_test.py",
            "patches": [
                {
                    "old_start": 13,
                    "old_length": 6,
                    "new_start": 13,
                    "new_length": 7,
                    "hunk": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \n import collections\n+import contextlib\n import functools\n import logging\n import textwrap\n"
                },
                {
                    "old_start": 93,
                    "old_length": 6,
                    "new_start": 95,
                    "new_length": 34,
                    "hunk": "@@ -93,6 +95,34 @@ class PythonCallbackTest(jtu.JaxTestCase):\n     out = f(0.)\n     self.assertEqual(out, 1.)\n \n+  @parameterized.named_parameters(\n+    dict(testcase_name=f\"{flavor}_expect_dtype_{expect_dtype}\",\n+         callback=dict(io_unordered=io_calback_unordered,\n+                       io_ordered=io_callback_ordered,\n+                       pure=jax.pure_callback)[flavor],\n+         expect_dtype=expect_dtype)\n+    for flavor in (\"io_unordered\", \"io_ordered\", \"pure\")\n+    for expect_dtype in (np.int32, np.int64, np.float32, np.float64)\n+  )\n+  def test_callback_returning_python_literal(self, *, callback, expect_dtype):\n+    returned_literal = 42 if expect_dtype in (np.int32, np.int64) else 42.\n+    @jax.jit\n+    def f(x):\n+      return callback(lambda x: returned_literal,\n+                      core.ShapedArray((), expect_dtype), x)\n+\n+    if not config.enable_x64.value:\n+      ctx = self.assertRaisesRegex(Exception, \"Cannot return 64-bit values\")\n+    elif expect_dtype in (np.int32, np.float32):\n+      ctx = self.assertRaisesRegex(Exception, \"Incorrect output dtype\")\n+    else:\n+      ctx = contextlib.nullcontext()\n+\n+    with ctx:\n+      out = f(0.)\n+      jax.effects_barrier()\n+      self.assertEqual(out, returned_literal)\n+\n   @parameterized.named_parameters(\n     dict(testcase_name=f\"{flavor}_{dtype}\",\n          dtype=dtype,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+import contextlib\n+  @parameterized.named_parameters(\n+    dict(testcase_name=f\"{flavor}_expect_dtype_{expect_dtype}\",\n+         callback=dict(io_unordered=io_calback_unordered,\n+                       io_ordered=io_callback_ordered,\n+                       pure=jax.pure_callback)[flavor],\n+         expect_dtype=expect_dtype)\n+    for flavor in (\"io_unordered\", \"io_ordered\", \"pure\")\n+    for expect_dtype in (np.int32, np.int64, np.float32, np.float64)\n+  )\n+  def test_callback_returning_python_literal(self, *, callback, expect_dtype):\n+    returned_literal = 42 if expect_dtype in (np.int32, np.int64) else 42.\n+    @jax.jit\n+    def f(x):\n+      return callback(lambda x: returned_literal,\n+                      core.ShapedArray((), expect_dtype), x)\n+\n+    if not config.enable_x64.value:\n+      ctx = self.assertRaisesRegex(Exception, \"Cannot return 64-bit values\")\n+    elif expect_dtype in (np.int32, np.float32):\n+      ctx = self.assertRaisesRegex(Exception, \"Incorrect output dtype\")\n+    else:\n+      ctx = contextlib.nullcontext()\n+\n+    with ctx:\n+      out = f(0.)\n+      jax.effects_barrier()\n+      self.assertEqual(out, returned_literal)\n+\n",
            "whole_hunk": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \n import collections\n+import contextlib\n import functools\n import logging\n import textwrap\n@@ -93,6 +95,34 @@ class PythonCallbackTest(jtu.JaxTestCase):\n     out = f(0.)\n     self.assertEqual(out, 1.)\n \n+  @parameterized.named_parameters(\n+    dict(testcase_name=f\"{flavor}_expect_dtype_{expect_dtype}\",\n+         callback=dict(io_unordered=io_calback_unordered,\n+                       io_ordered=io_callback_ordered,\n+                       pure=jax.pure_callback)[flavor],\n+         expect_dtype=expect_dtype)\n+    for flavor in (\"io_unordered\", \"io_ordered\", \"pure\")\n+    for expect_dtype in (np.int32, np.int64, np.float32, np.float64)\n+  )\n+  def test_callback_returning_python_literal(self, *, callback, expect_dtype):\n+    returned_literal = 42 if expect_dtype in (np.int32, np.int64) else 42.\n+    @jax.jit\n+    def f(x):\n+      return callback(lambda x: returned_literal,\n+                      core.ShapedArray((), expect_dtype), x)\n+\n+    if not config.enable_x64.value:\n+      ctx = self.assertRaisesRegex(Exception, \"Cannot return 64-bit values\")\n+    elif expect_dtype in (np.int32, np.float32):\n+      ctx = self.assertRaisesRegex(Exception, \"Incorrect output dtype\")\n+    else:\n+      ctx = contextlib.nullcontext()\n+\n+    with ctx:\n+      out = f(0.)\n+      jax.effects_barrier()\n+      self.assertEqual(out, returned_literal)\n+\n   @parameterized.named_parameters(\n     dict(testcase_name=f\"{flavor}_{dtype}\",\n          dtype=dtype,"
        }
    ]
},
{
    "Id": 69,
    "commit_link": "https://github.com/google/jax/commit/1f81477214176bd14922b47800942a09d6c86f3f",
    "date": "2024-03-23T13:33:02-07:00",
    "message": "Adds additional check for x is a Tracer, as\nlooking up sharding attribute on a tracer is expensive.\n\nPiperOrigin-RevId: 618489738",
    "changes": [
        {
            "name": "lax.py",
            "path": "jax/_src/lax/lax.py",
            "patches": [
                {
                    "old_start": 1417,
                    "old_length": 9,
                    "new_start": 1415,
                    "new_length": 16,
                    "hunk": "@@ -1417,9 +1415,16 @@ def full_like(x: ArrayLike | DuckTypedArray,\n   # If `x` has a sharding but no `_committed` attribute\n   # (in case of ShapeDtypeStruct), default it to True.\n   use_x_sharding = (\n-      sharding is None and\n-      hasattr(x, 'sharding') and getattr(x, '_committed', True) and\n-      not weak_type and fill_shape == x.shape)  # type: ignore\n+      sharding is None\n+      # Tracer have special logic in handling sharding and even\n+      # though hasattr(x, 'sharding') returns False, it is very slow.\n+      # This bypasses the check.\n+      and not isinstance(x, core.Tracer)\n+      and hasattr(x, 'sharding')\n+      and getattr(x, '_committed', True)\n+      and not weak_type\n+      and fill_shape == np.shape(x)  # type: ignore[arg-type]\n+  )  # type: ignore\n   if use_x_sharding:\n     # TODO(yashkatariya): Use shard_alike in tracing_mode once it is supported.\n     sharding = x.sharding  # type: ignore"
                }
            ],
            "whole_deleted": "-      sharding is None and\n-      hasattr(x, 'sharding') and getattr(x, '_committed', True) and\n-      not weak_type and fill_shape == x.shape)  # type: ignore\n",
            "whole_added": "+      sharding is None\n+      # Tracer have special logic in handling sharding and even\n+      # though hasattr(x, 'sharding') returns False, it is very slow.\n+      # This bypasses the check.\n+      and not isinstance(x, core.Tracer)\n+      and hasattr(x, 'sharding')\n+      and getattr(x, '_committed', True)\n+      and not weak_type\n+      and fill_shape == np.shape(x)  # type: ignore[arg-type]\n+  )  # type: ignore\n",
            "whole_hunk": "@@ -1417,9 +1415,16 @@ def full_like(x: ArrayLike | DuckTypedArray,\n   # If `x` has a sharding but no `_committed` attribute\n   # (in case of ShapeDtypeStruct), default it to True.\n   use_x_sharding = (\n-      sharding is None and\n-      hasattr(x, 'sharding') and getattr(x, '_committed', True) and\n-      not weak_type and fill_shape == x.shape)  # type: ignore\n+      sharding is None\n+      # Tracer have special logic in handling sharding and even\n+      # though hasattr(x, 'sharding') returns False, it is very slow.\n+      # This bypasses the check.\n+      and not isinstance(x, core.Tracer)\n+      and hasattr(x, 'sharding')\n+      and getattr(x, '_committed', True)\n+      and not weak_type\n+      and fill_shape == np.shape(x)  # type: ignore[arg-type]\n+  )  # type: ignore\n   if use_x_sharding:\n     # TODO(yashkatariya): Use shard_alike in tracing_mode once it is supported.\n     sharding = x.sharding  # type: ignore"
        }
    ]
},
{
    "Id": 70,
    "commit_link": "https://github.com/google/jax/commit/0e092a77067dbbce33cfd6d54a46e743b779919b",
    "date": "2024-03-21T21:02:40-07:00",
    "message": "Expose `.layout` on jax.Array. Also add checks in the AOT path to make sure that the input `Array`'s layout matches the layout given to `jax.jit`.\n\nPiperOrigin-RevId: 618050680",
    "changes": [
        {
            "name": "array.py",
            "path": "jax/_src/array.py",
            "patches": [
                {
                    "old_start": 34,
                    "old_length": 10,
                    "new_start": 34,
                    "new_length": 12,
                    "hunk": "@@ -34,10 +34,12 @@ from jax._src import deprecations\n from jax._src import dispatch\n from jax._src import dtypes\n from jax._src import errors\n+from jax._src import layout\n from jax._src import profiler\n from jax._src import tree_util\n from jax._src import xla_bridge\n from jax._src.lib import xla_client as xc\n+from jax._src.lib import xla_extension as xe\n from jax._src.interpreters import mlir\n from jax._src.interpreters import pxla\n from jax._src.interpreters import xla\n"
                },
                {
                    "old_start": 527,
                    "old_length": 6,
                    "new_start": 529,
                    "new_length": 18,
                    "hunk": "@@ -527,6 +529,18 @@ class ArrayImpl(basearray.Array):\n       out.append(Shard(_get_device(a), self.sharding, self.shape, a))\n     return out\n \n+  @property\n+  def layout(self):\n+    # TODO(yashkatariya): Remove the try;except when pathways supports layouts.\n+    try:\n+      return layout.SpecifiedLayout(self._pjrt_layout)\n+    except xe.XlaRuntimeError as e:\n+      msg, *_ = e.args\n+      if type(msg) is str and msg.startswith(\"UNIMPLEMENTED\"):\n+        return None\n+      else:\n+        raise\n+\n   @property\n   def global_shards(self) -> Sequence[Shard]:\n     \"\"\"Returns list of all `Shard`s of the Array across all devices.\n"
                },
                {
                    "old_start": 637,
                    "old_length": 7,
                    "new_start": 651,
                    "new_length": 7,
                    "hunk": "@@ -637,7 +651,7 @@ if not TYPE_CHECKING:\n   ArrayImpl = use_cpp_class(xc.ArrayImpl)(ArrayImpl)\n \n \n-# explicitly set to be unhashable. Same as what device_array.py does.\n+# explicitly set to be unhashable.\n setattr(ArrayImpl, \"__hash__\", None)\n setattr(ArrayImpl, \"__array_priority__\", 100)\n \n"
                }
            ],
            "whole_deleted": "-# explicitly set to be unhashable. Same as what device_array.py does.\n",
            "whole_added": "+from jax._src import layout\n+from jax._src.lib import xla_extension as xe\n+  @property\n+  def layout(self):\n+    # TODO(yashkatariya): Remove the try;except when pathways supports layouts.\n+    try:\n+      return layout.SpecifiedLayout(self._pjrt_layout)\n+    except xe.XlaRuntimeError as e:\n+      msg, *_ = e.args\n+      if type(msg) is str and msg.startswith(\"UNIMPLEMENTED\"):\n+        return None\n+      else:\n+        raise\n+\n+# explicitly set to be unhashable.\n",
            "whole_hunk": "@@ -34,10 +34,12 @@ from jax._src import deprecations\n from jax._src import dispatch\n from jax._src import dtypes\n from jax._src import errors\n+from jax._src import layout\n from jax._src import profiler\n from jax._src import tree_util\n from jax._src import xla_bridge\n from jax._src.lib import xla_client as xc\n+from jax._src.lib import xla_extension as xe\n from jax._src.interpreters import mlir\n from jax._src.interpreters import pxla\n from jax._src.interpreters import xla\n@@ -527,6 +529,18 @@ class ArrayImpl(basearray.Array):\n       out.append(Shard(_get_device(a), self.sharding, self.shape, a))\n     return out\n \n+  @property\n+  def layout(self):\n+    # TODO(yashkatariya): Remove the try;except when pathways supports layouts.\n+    try:\n+      return layout.SpecifiedLayout(self._pjrt_layout)\n+    except xe.XlaRuntimeError as e:\n+      msg, *_ = e.args\n+      if type(msg) is str and msg.startswith(\"UNIMPLEMENTED\"):\n+        return None\n+      else:\n+        raise\n+\n   @property\n   def global_shards(self) -> Sequence[Shard]:\n     \"\"\"Returns list of all `Shard`s of the Array across all devices.\n@@ -637,7 +651,7 @@ if not TYPE_CHECKING:\n   ArrayImpl = use_cpp_class(xc.ArrayImpl)(ArrayImpl)\n \n \n-# explicitly set to be unhashable. Same as what device_array.py does.\n+# explicitly set to be unhashable.\n setattr(ArrayImpl, \"__hash__\", None)\n setattr(ArrayImpl, \"__array_priority__\", 100)\n \n"
        },
        {
            "name": "mlir.py",
            "path": "jax/_src/interpreters/mlir.py",
            "patches": [
                {
                    "old_start": 46,
                    "old_length": 7,
                    "new_start": 46,
                    "new_length": 7,
                    "hunk": "@@ -46,7 +46,7 @@ from jax._src import util\n from jax._src import xla_bridge as xb\n from jax._src.interpreters import partial_eval as pe\n from jax._src.interpreters import xla\n-from jax._src.layout import XLACompatibleLayout, LayoutRequest\n+from jax._src.layout import AutoLayout, SpecifiedLayout\n from jax._src.lib import xla_client as xc\n from jax._src.lib import xla_extension\n from jax._src.lib import xla_extension_version\n"
                },
                {
                    "old_start": 834,
                    "old_length": 10,
                    "new_start": 834,
                    "new_length": 10,
                    "hunk": "@@ -834,10 +834,10 @@ def _to_physical_op_sharding(\n   return sharding._to_xla_hlo_sharding(aval.ndim).to_proto()  # type: ignore\n \n \n-def _to_xla_layout(layout: XLACompatibleLayout | None | LayoutRequest) -> str | None:\n+def _to_xla_layout(layout: SpecifiedLayout | None | AutoLayout) -> str | None:\n   if layout is None:\n     return \"default\"\n-  if isinstance(layout, LayoutRequest):\n+  if isinstance(layout, AutoLayout):\n     return \"auto\"\n   return layout._to_xla_layout()\n \n"
                },
                {
                    "old_start": 862,
                    "old_length": 8,
                    "new_start": 862,
                    "new_length": 8,
                    "hunk": "@@ -862,8 +862,8 @@ def lower_jaxpr_to_module(\n     replicated_args: Sequence[bool] | None = None,\n     arg_shardings: Sequence[XLACompatibleSharding | None] | None = None,\n     result_shardings: Sequence[XLACompatibleSharding | None] | None = None,\n-    in_layouts: Sequence[XLACompatibleLayout | None | LayoutRequest] | None = None,\n-    out_layouts: Sequence[XLACompatibleLayout | None | LayoutRequest] | None = None,\n+    in_layouts: Sequence[SpecifiedLayout | None | AutoLayout] | None = None,\n+    out_layouts: Sequence[SpecifiedLayout | None | AutoLayout] | None = None,\n     arg_names: Sequence[str | None] | None = None,\n     result_names: Sequence[str | None] | None = None,\n     num_replicas: int = 1,\n"
                }
            ],
            "whole_deleted": "-from jax._src.layout import XLACompatibleLayout, LayoutRequest\n-def _to_xla_layout(layout: XLACompatibleLayout | None | LayoutRequest) -> str | None:\n-  if isinstance(layout, LayoutRequest):\n-    in_layouts: Sequence[XLACompatibleLayout | None | LayoutRequest] | None = None,\n-    out_layouts: Sequence[XLACompatibleLayout | None | LayoutRequest] | None = None,\n",
            "whole_added": "+from jax._src.layout import AutoLayout, SpecifiedLayout\n+def _to_xla_layout(layout: SpecifiedLayout | None | AutoLayout) -> str | None:\n+  if isinstance(layout, AutoLayout):\n+    in_layouts: Sequence[SpecifiedLayout | None | AutoLayout] | None = None,\n+    out_layouts: Sequence[SpecifiedLayout | None | AutoLayout] | None = None,\n",
            "whole_hunk": "@@ -46,7 +46,7 @@ from jax._src import util\n from jax._src import xla_bridge as xb\n from jax._src.interpreters import partial_eval as pe\n from jax._src.interpreters import xla\n-from jax._src.layout import XLACompatibleLayout, LayoutRequest\n+from jax._src.layout import AutoLayout, SpecifiedLayout\n from jax._src.lib import xla_client as xc\n from jax._src.lib import xla_extension\n from jax._src.lib import xla_extension_version\n@@ -834,10 +834,10 @@ def _to_physical_op_sharding(\n   return sharding._to_xla_hlo_sharding(aval.ndim).to_proto()  # type: ignore\n \n \n-def _to_xla_layout(layout: XLACompatibleLayout | None | LayoutRequest) -> str | None:\n+def _to_xla_layout(layout: SpecifiedLayout | None | AutoLayout) -> str | None:\n   if layout is None:\n     return \"default\"\n-  if isinstance(layout, LayoutRequest):\n+  if isinstance(layout, AutoLayout):\n     return \"auto\"\n   return layout._to_xla_layout()\n \n@@ -862,8 +862,8 @@ def lower_jaxpr_to_module(\n     replicated_args: Sequence[bool] | None = None,\n     arg_shardings: Sequence[XLACompatibleSharding | None] | None = None,\n     result_shardings: Sequence[XLACompatibleSharding | None] | None = None,\n-    in_layouts: Sequence[XLACompatibleLayout | None | LayoutRequest] | None = None,\n-    out_layouts: Sequence[XLACompatibleLayout | None | LayoutRequest] | None = None,\n+    in_layouts: Sequence[SpecifiedLayout | None | AutoLayout] | None = None,\n+    out_layouts: Sequence[SpecifiedLayout | None | AutoLayout] | None = None,\n     arg_names: Sequence[str | None] | None = None,\n     result_names: Sequence[str | None] | None = None,\n     num_replicas: int = 1,\n"
        },
        {
            "name": "pxla.py",
            "path": "jax/_src/interpreters/pxla.py",
            "patches": [
                {
                    "old_start": 60,
                    "old_length": 7,
                    "new_start": 60,
                    "new_length": 7,
                    "hunk": "@@ -60,7 +60,7 @@ from jax._src.interpreters import batching\n from jax._src.interpreters import partial_eval as pe\n from jax._src.interpreters import mlir\n from jax._src.interpreters import xla\n-from jax._src.layout import XLACompatibleLayout, SpecifiedLayout, LayoutRequest\n+from jax._src.layout import SpecifiedLayout, AutoLayout\n from jax._src.lib import xla_client as xc\n from jax._src.lib import xla_extension_version\n from jax._src.lib.mlir import ir\n"
                },
                {
                    "old_start": 1985,
                    "old_length": 13,
                    "new_start": 1985,
                    "new_length": 14,
                    "hunk": "@@ -1985,13 +1985,14 @@ def are_all_shardings_default_mem_kind(da_object, shardings):\n       return False\n   return True\n \n-MaybeLayout = Sequence[Union[XLACompatibleLayout, LayoutRequest, None]]\n+MaybeLayout = Sequence[Union[SpecifiedLayout, AutoLayout, None]]\n \n \n class AllArgsInfo(NamedTuple):\n   \"\"\"Avals, shardings, layouts and debug_info for all arguments prior to DCE.\"\"\"\n   in_avals: Sequence[core.ShapedArray]\n   in_shardings: Any\n+  in_layouts: Any\n   debug_info: core.JaxprDebugInfo | None\n \n \n"
                },
                {
                    "old_start": 2023,
                    "old_length": 7,
                    "new_start": 2024,
                    "new_length": 7,
                    "hunk": "@@ -2023,7 +2024,7 @@ def lower_sharding_computation(\n   auto_spmd_lowering = check_if_any_auto(\n       it.chain.from_iterable([in_shardings, out_shardings]))  # type: ignore\n \n-  all_args_info = AllArgsInfo(global_in_avals, in_shardings,\n+  all_args_info = AllArgsInfo(global_in_avals, in_shardings, in_layouts,\n                               closed_jaxpr.jaxpr.debug_info)\n \n   (closed_jaxpr, global_in_avals, global_out_avals, donated_invars,\n"
                },
                {
                    "old_start": 2227,
                    "old_length": 8,
                    "new_start": 2228,
                    "new_length": 6,
                    "hunk": "@@ -2227,8 +2228,6 @@ def lower_mesh_computation(\n       out_jaxpr_avals = fun_or_jaxpr.out_avals\n       consts = fun_or_jaxpr.consts\n \n-  all_args_info = AllArgsInfo(global_in_avals, in_shardings, jaxpr.debug_info)\n-\n   assert len(out_shardings) == len(out_jaxpr_avals)\n   if spmd_lowering:\n     global_out_avals = out_jaxpr_avals\n"
                },
                {
                    "old_start": 2319,
                    "old_length": 7,
                    "new_start": 2318,
                    "new_length": 7,
                    "hunk": "@@ -2319,7 +2318,7 @@ def lower_mesh_computation(\n       in_layouts=(None,) * len(global_in_avals),\n       out_layouts=(None,) * len(global_out_avals),\n       shape_poly_state=lowering_result.shape_poly_state,\n-      all_args_info=all_args_info)\n+      all_args_info=None)\n \n class MeshComputation(stages.XlaLowering):\n   _hlo: ir.Module | None\n"
                },
                {
                    "old_start": 2599,
                    "old_length": 7,
                    "new_start": 2598,
                    "new_length": 7,
                    "hunk": "@@ -2599,7 +2598,7 @@ def _get_layouts_from_executable(\n     if isinstance(i, SpecifiedLayout):\n       if i != x:\n         raise AssertionError(\n-            f\"Unexpected XLA layout override: (XLA) {x} != {i} (User sharding)\")\n+            f\"Unexpected XLA layout override: (XLA) {x} != {i} (User layout)\")\n       new_in_layouts.append(i)\n     else:\n       new_in_layouts.append(x)\n"
                },
                {
                    "old_start": 2610,
                    "old_length": 7,
                    "new_start": 2609,
                    "new_length": 7,
                    "hunk": "@@ -2610,7 +2609,7 @@ def _get_layouts_from_executable(\n     if isinstance(o, SpecifiedLayout):\n       if o != x:\n         raise AssertionError(\n-            f\"Unexpected XLA layout override: (XLA) {x} != {o} (User sharding)\")\n+            f\"Unexpected XLA layout override: (XLA) {x} != {o} (User layout)\")\n       new_out_layouts.append(o)\n     else:\n       new_out_layouts.append(x)\n"
                },
                {
                    "old_start": 3016,
                    "old_length": 6,
                    "new_start": 3015,
                    "new_length": 7,
                    "hunk": "@@ -3016,6 +3015,7 @@ class MeshExecutable(stages.XlaExecutable):\n       kept_args = [a for i, a in enumerate(args) if i in self._kept_var_idx]\n       ref_avals = self.in_avals\n       in_shardings = self._in_shardings\n+      in_layouts = self._in_layouts\n       debug_info = None\n     else:\n       kept_args = args\n"
                },
                {
                    "old_start": 3023,
                    "old_length": 12,
                    "new_start": 3023,
                    "new_length": 16,
                    "hunk": "@@ -3023,12 +3023,16 @@ class MeshExecutable(stages.XlaExecutable):\n       iter_in_shardings = iter(self._in_shardings)\n       in_shardings = [next(iter_in_shardings) if i in self._kept_var_idx else s\n                       for i, s in enumerate(self._all_args_info.in_shardings)]\n+      iter_in_layouts = iter(self._in_layouts)\n+      in_layouts = [next(iter_in_layouts) if i in self._kept_var_idx else s\n+                    for i, s in enumerate(self._all_args_info.in_layouts)]\n       debug_info = self._all_args_info.debug_info\n \n     arg_avals = map(xla.abstractify, kept_args)\n     check_arg_avals_for_call(ref_avals, arg_avals, debug_info)\n     # Check the GDA sharding and the input sharding.\n-    check_gda_or_array_xla_sharding_match(kept_args, in_shardings, debug_info)\n+    check_array_xla_sharding_layout_match(kept_args, in_shardings,\n+                                          in_layouts, debug_info)\n     return self.unsafe_call(*args)  # pylint: disable=not-callable\n \n   def input_shardings(self) -> Sequence[sharding_impls.XLACompatibleSharding]:\n"
                },
                {
                    "old_start": 3184,
                    "old_length": 15,
                    "new_start": 3188,
                    "new_length": 17,
                    "hunk": "@@ -3184,15 +3188,17 @@ def check_device_backend_on_shardings(shardings) -> bool:\n   return False\n \n \n-def check_gda_or_array_xla_sharding_match(\n+def check_array_xla_sharding_layout_match(\n     args, in_xla_shardings: Sequence[sharding_impls.XLACompatibleSharding],\n+    in_xla_layouts: Sequence[SpecifiedLayout],\n     jaxpr_debug_info: core.JaxprDebugInfo | None) -> None:\n   from jax._src.array import ArrayImpl\n   arg_names = ([''] * len(args) if jaxpr_debug_info is None else\n                jaxpr_debug_info.arg_names)\n   errors = []\n   num_errors = 5\n-  for arg, xs, name in safe_zip(args, in_xla_shardings, arg_names):\n+  for arg, xs, xl, name in safe_zip(args, in_xla_shardings, in_xla_layouts,\n+                                    arg_names):\n     if not isinstance(arg, ArrayImpl):\n       continue\n     if is_unspecified_or_auto(xs):\n"
                },
                {
                    "old_start": 3205,
                    "old_length": 27,
                    "new_start": 3211,
                    "new_length": 47,
                    "hunk": "@@ -3205,27 +3211,47 @@ def check_gda_or_array_xla_sharding_match(\n     # Raise memory kind mismatch error even if the arg is uncommitted.\n     if arg.sharding.memory_kind != xs.memory_kind:\n       errors.append(\n-          \"Got input sharding(s) that compiled object was called with: \"\n+          (\"Got input sharding(s) that compiled object was called with: \"\n           f\"{arg.sharding} and sharding(s) the computation was compiled \"\n-          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\")\n+          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\",\n+          'sharding'))\n \n     if (not db_xs and arg._committed and\n         not op_shardings.are_op_shardings_equal(\n             arg.sharding._to_xla_hlo_sharding(arg.ndim),\n             xs._to_xla_hlo_sharding(arg.ndim))):\n       errors.append(\n-          \"Got input sharding(s) that compiled object was called with: \"\n+          (\"Got input sharding(s) that compiled object was called with: \"\n           f\"{arg.sharding} and sharding(s) the computation was compiled \"\n-          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\")\n+          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\",\n+          'sharding'))\n+\n+    # TODO(yashkatariya): Remove `arg.layout is not None` check after pathways\n+    # supports layout on Array.\n+    if (xla_extension_version >= 249 and not db_xs and arg._committed and\n+        arg.layout is not None and arg.layout != xl):\n+      errors.append(\n+          (\"Got input layout(s) that compiled object was called with: \"\n+          f\"{arg.layout} and layout(s) the computation was compiled \"\n+          f\"with: {xl} for arg {name} with shape: {arg.aval.str_short()}\",\n+          'layout'))\n \n   if errors:\n-    str_errors = '\\n'.join(errors[:num_errors])\n+    first_errors, error_kinds = unzip2(errors[:num_errors])\n+    str_errors = '\\n'.join(first_errors)\n+    if all(k == 'sharding' for k in error_kinds):\n+      kind_str = r'sharding(s)'\n+    elif all(k == 'layout' for k in error_kinds):\n+      kind_str = 'layout(s)'\n+    else:\n+      kind_str = 'sharding(s) and layout(s)'\n     num_mismatch_str = (\n         f'the {len(errors)} mismatches' if len(errors) < num_errors else\n         f\"{num_errors} mismatches out of {len(errors)}\")\n     raise ValueError(\n-          \"Compiled object called with input sharding(s) does not match the \"\n-          \"sharding(s) the computation was compiled with. \"\n+          f\"Compiled object called with input {kind_str} does \"\n+          f\"not match the {kind_str} the computation was \"\n+          \"compiled with. \"\n           f\"Here are {num_mismatch_str}:\\n{str_errors}\")\n \n \n"
                }
            ],
            "whole_deleted": "-from jax._src.layout import XLACompatibleLayout, SpecifiedLayout, LayoutRequest\n-MaybeLayout = Sequence[Union[XLACompatibleLayout, LayoutRequest, None]]\n-  all_args_info = AllArgsInfo(global_in_avals, in_shardings,\n-  all_args_info = AllArgsInfo(global_in_avals, in_shardings, jaxpr.debug_info)\n-\n-      all_args_info=all_args_info)\n-            f\"Unexpected XLA layout override: (XLA) {x} != {i} (User sharding)\")\n-            f\"Unexpected XLA layout override: (XLA) {x} != {o} (User sharding)\")\n-    check_gda_or_array_xla_sharding_match(kept_args, in_shardings, debug_info)\n-def check_gda_or_array_xla_sharding_match(\n-  for arg, xs, name in safe_zip(args, in_xla_shardings, arg_names):\n-          \"Got input sharding(s) that compiled object was called with: \"\n-          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\")\n-          \"Got input sharding(s) that compiled object was called with: \"\n-          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\")\n-    str_errors = '\\n'.join(errors[:num_errors])\n-          \"Compiled object called with input sharding(s) does not match the \"\n-          \"sharding(s) the computation was compiled with. \"\n",
            "whole_added": "+from jax._src.layout import SpecifiedLayout, AutoLayout\n+MaybeLayout = Sequence[Union[SpecifiedLayout, AutoLayout, None]]\n+  in_layouts: Any\n+  all_args_info = AllArgsInfo(global_in_avals, in_shardings, in_layouts,\n+      all_args_info=None)\n+            f\"Unexpected XLA layout override: (XLA) {x} != {i} (User layout)\")\n+            f\"Unexpected XLA layout override: (XLA) {x} != {o} (User layout)\")\n+      in_layouts = self._in_layouts\n+      iter_in_layouts = iter(self._in_layouts)\n+      in_layouts = [next(iter_in_layouts) if i in self._kept_var_idx else s\n+                    for i, s in enumerate(self._all_args_info.in_layouts)]\n+    check_array_xla_sharding_layout_match(kept_args, in_shardings,\n+                                          in_layouts, debug_info)\n+def check_array_xla_sharding_layout_match(\n+    in_xla_layouts: Sequence[SpecifiedLayout],\n+  for arg, xs, xl, name in safe_zip(args, in_xla_shardings, in_xla_layouts,\n+                                    arg_names):\n+          (\"Got input sharding(s) that compiled object was called with: \"\n+          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\",\n+          'sharding'))\n+          (\"Got input sharding(s) that compiled object was called with: \"\n+          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\",\n+          'sharding'))\n+\n+    # TODO(yashkatariya): Remove `arg.layout is not None` check after pathways\n+    # supports layout on Array.\n+    if (xla_extension_version >= 249 and not db_xs and arg._committed and\n+        arg.layout is not None and arg.layout != xl):\n+      errors.append(\n+          (\"Got input layout(s) that compiled object was called with: \"\n+          f\"{arg.layout} and layout(s) the computation was compiled \"\n+          f\"with: {xl} for arg {name} with shape: {arg.aval.str_short()}\",\n+          'layout'))\n+    first_errors, error_kinds = unzip2(errors[:num_errors])\n+    str_errors = '\\n'.join(first_errors)\n+    if all(k == 'sharding' for k in error_kinds):\n+      kind_str = r'sharding(s)'\n+    elif all(k == 'layout' for k in error_kinds):\n+      kind_str = 'layout(s)'\n+    else:\n+      kind_str = 'sharding(s) and layout(s)'\n+          f\"Compiled object called with input {kind_str} does \"\n+          f\"not match the {kind_str} the computation was \"\n+          \"compiled with. \"\n",
            "whole_hunk": "@@ -60,7 +60,7 @@ from jax._src.interpreters import batching\n from jax._src.interpreters import partial_eval as pe\n from jax._src.interpreters import mlir\n from jax._src.interpreters import xla\n-from jax._src.layout import XLACompatibleLayout, SpecifiedLayout, LayoutRequest\n+from jax._src.layout import SpecifiedLayout, AutoLayout\n from jax._src.lib import xla_client as xc\n from jax._src.lib import xla_extension_version\n from jax._src.lib.mlir import ir\n@@ -1985,13 +1985,14 @@ def are_all_shardings_default_mem_kind(da_object, shardings):\n       return False\n   return True\n \n-MaybeLayout = Sequence[Union[XLACompatibleLayout, LayoutRequest, None]]\n+MaybeLayout = Sequence[Union[SpecifiedLayout, AutoLayout, None]]\n \n \n class AllArgsInfo(NamedTuple):\n   \"\"\"Avals, shardings, layouts and debug_info for all arguments prior to DCE.\"\"\"\n   in_avals: Sequence[core.ShapedArray]\n   in_shardings: Any\n+  in_layouts: Any\n   debug_info: core.JaxprDebugInfo | None\n \n \n@@ -2023,7 +2024,7 @@ def lower_sharding_computation(\n   auto_spmd_lowering = check_if_any_auto(\n       it.chain.from_iterable([in_shardings, out_shardings]))  # type: ignore\n \n-  all_args_info = AllArgsInfo(global_in_avals, in_shardings,\n+  all_args_info = AllArgsInfo(global_in_avals, in_shardings, in_layouts,\n                               closed_jaxpr.jaxpr.debug_info)\n \n   (closed_jaxpr, global_in_avals, global_out_avals, donated_invars,\n@@ -2227,8 +2228,6 @@ def lower_mesh_computation(\n       out_jaxpr_avals = fun_or_jaxpr.out_avals\n       consts = fun_or_jaxpr.consts\n \n-  all_args_info = AllArgsInfo(global_in_avals, in_shardings, jaxpr.debug_info)\n-\n   assert len(out_shardings) == len(out_jaxpr_avals)\n   if spmd_lowering:\n     global_out_avals = out_jaxpr_avals\n@@ -2319,7 +2318,7 @@ def lower_mesh_computation(\n       in_layouts=(None,) * len(global_in_avals),\n       out_layouts=(None,) * len(global_out_avals),\n       shape_poly_state=lowering_result.shape_poly_state,\n-      all_args_info=all_args_info)\n+      all_args_info=None)\n \n class MeshComputation(stages.XlaLowering):\n   _hlo: ir.Module | None\n@@ -2599,7 +2598,7 @@ def _get_layouts_from_executable(\n     if isinstance(i, SpecifiedLayout):\n       if i != x:\n         raise AssertionError(\n-            f\"Unexpected XLA layout override: (XLA) {x} != {i} (User sharding)\")\n+            f\"Unexpected XLA layout override: (XLA) {x} != {i} (User layout)\")\n       new_in_layouts.append(i)\n     else:\n       new_in_layouts.append(x)\n@@ -2610,7 +2609,7 @@ def _get_layouts_from_executable(\n     if isinstance(o, SpecifiedLayout):\n       if o != x:\n         raise AssertionError(\n-            f\"Unexpected XLA layout override: (XLA) {x} != {o} (User sharding)\")\n+            f\"Unexpected XLA layout override: (XLA) {x} != {o} (User layout)\")\n       new_out_layouts.append(o)\n     else:\n       new_out_layouts.append(x)\n@@ -3016,6 +3015,7 @@ class MeshExecutable(stages.XlaExecutable):\n       kept_args = [a for i, a in enumerate(args) if i in self._kept_var_idx]\n       ref_avals = self.in_avals\n       in_shardings = self._in_shardings\n+      in_layouts = self._in_layouts\n       debug_info = None\n     else:\n       kept_args = args\n@@ -3023,12 +3023,16 @@ class MeshExecutable(stages.XlaExecutable):\n       iter_in_shardings = iter(self._in_shardings)\n       in_shardings = [next(iter_in_shardings) if i in self._kept_var_idx else s\n                       for i, s in enumerate(self._all_args_info.in_shardings)]\n+      iter_in_layouts = iter(self._in_layouts)\n+      in_layouts = [next(iter_in_layouts) if i in self._kept_var_idx else s\n+                    for i, s in enumerate(self._all_args_info.in_layouts)]\n       debug_info = self._all_args_info.debug_info\n \n     arg_avals = map(xla.abstractify, kept_args)\n     check_arg_avals_for_call(ref_avals, arg_avals, debug_info)\n     # Check the GDA sharding and the input sharding.\n-    check_gda_or_array_xla_sharding_match(kept_args, in_shardings, debug_info)\n+    check_array_xla_sharding_layout_match(kept_args, in_shardings,\n+                                          in_layouts, debug_info)\n     return self.unsafe_call(*args)  # pylint: disable=not-callable\n \n   def input_shardings(self) -> Sequence[sharding_impls.XLACompatibleSharding]:\n@@ -3184,15 +3188,17 @@ def check_device_backend_on_shardings(shardings) -> bool:\n   return False\n \n \n-def check_gda_or_array_xla_sharding_match(\n+def check_array_xla_sharding_layout_match(\n     args, in_xla_shardings: Sequence[sharding_impls.XLACompatibleSharding],\n+    in_xla_layouts: Sequence[SpecifiedLayout],\n     jaxpr_debug_info: core.JaxprDebugInfo | None) -> None:\n   from jax._src.array import ArrayImpl\n   arg_names = ([''] * len(args) if jaxpr_debug_info is None else\n                jaxpr_debug_info.arg_names)\n   errors = []\n   num_errors = 5\n-  for arg, xs, name in safe_zip(args, in_xla_shardings, arg_names):\n+  for arg, xs, xl, name in safe_zip(args, in_xla_shardings, in_xla_layouts,\n+                                    arg_names):\n     if not isinstance(arg, ArrayImpl):\n       continue\n     if is_unspecified_or_auto(xs):\n@@ -3205,27 +3211,47 @@ def check_gda_or_array_xla_sharding_match(\n     # Raise memory kind mismatch error even if the arg is uncommitted.\n     if arg.sharding.memory_kind != xs.memory_kind:\n       errors.append(\n-          \"Got input sharding(s) that compiled object was called with: \"\n+          (\"Got input sharding(s) that compiled object was called with: \"\n           f\"{arg.sharding} and sharding(s) the computation was compiled \"\n-          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\")\n+          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\",\n+          'sharding'))\n \n     if (not db_xs and arg._committed and\n         not op_shardings.are_op_shardings_equal(\n             arg.sharding._to_xla_hlo_sharding(arg.ndim),\n             xs._to_xla_hlo_sharding(arg.ndim))):\n       errors.append(\n-          \"Got input sharding(s) that compiled object was called with: \"\n+          (\"Got input sharding(s) that compiled object was called with: \"\n           f\"{arg.sharding} and sharding(s) the computation was compiled \"\n-          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\")\n+          f\"with: {xs} for arg {name} with shape: {arg.aval.str_short()}\",\n+          'sharding'))\n+\n+    # TODO(yashkatariya): Remove `arg.layout is not None` check after pathways\n+    # supports layout on Array.\n+    if (xla_extension_version >= 249 and not db_xs and arg._committed and\n+        arg.layout is not None and arg.layout != xl):\n+      errors.append(\n+          (\"Got input layout(s) that compiled object was called with: \"\n+          f\"{arg.layout} and layout(s) the computation was compiled \"\n+          f\"with: {xl} for arg {name} with shape: {arg.aval.str_short()}\",\n+          'layout'))\n \n   if errors:\n-    str_errors = '\\n'.join(errors[:num_errors])\n+    first_errors, error_kinds = unzip2(errors[:num_errors])\n+    str_errors = '\\n'.join(first_errors)\n+    if all(k == 'sharding' for k in error_kinds):\n+      kind_str = r'sharding(s)'\n+    elif all(k == 'layout' for k in error_kinds):\n+      kind_str = 'layout(s)'\n+    else:\n+      kind_str = 'sharding(s) and layout(s)'\n     num_mismatch_str = (\n         f'the {len(errors)} mismatches' if len(errors) < num_errors else\n         f\"{num_errors} mismatches out of {len(errors)}\")\n     raise ValueError(\n-          \"Compiled object called with input sharding(s) does not match the \"\n-          \"sharding(s) the computation was compiled with. \"\n+          f\"Compiled object called with input {kind_str} does \"\n+          f\"not match the {kind_str} the computation was \"\n+          \"compiled with. \"\n           f\"Here are {num_mismatch_str}:\\n{str_errors}\")\n \n \n"
        },
        {
            "name": "layout.py",
            "path": "jax/_src/layout.py",
            "patches": [
                {
                    "old_start": 14,
                    "old_length": 8,
                    "new_start": 14,
                    "new_length": 6,
                    "hunk": "@@ -14,8 +14,6 @@\n \n from __future__ import annotations\n \n-import re\n-\n from jax._src.lib import xla_client as xc\n \n \n"
                },
                {
                    "old_start": 24,
                    "old_length": 16,
                    "new_start": 22,
                    "new_length": 10,
                    "hunk": "@@ -24,16 +22,10 @@ class Layout:\n   pass\n \n \n-class XLACompatibleLayout(Layout):\n-\n-  def _to_xla_layout(self) -> str:\n-    raise NotImplementedError(\"Subclasses should implement this method.\")\n-\n-\n-class SpecifiedLayout(XLACompatibleLayout):\n-  layout: xc.Layout\n+class SpecifiedLayout(Layout):\n+  layout: xc.PjRtLayout\n \n-  def __init__(self, layout: xc.Layout):\n+  def __init__(self, layout: xc.PjRtLayout):\n     self._layout = layout\n     self._layout_str = str(self._layout)\n \n"
                },
                {
                    "old_start": 51,
                    "old_length": 19,
                    "new_start": 43,
                    "new_length": 10,
                    "hunk": "@@ -51,19 +43,10 @@ class SpecifiedLayout(XLACompatibleLayout):\n   def _to_xla_layout(self) -> str:\n     return self._layout_str\n \n-  @property\n-  def _minor_to_major(self):\n-    m = re.search(\"{([0-9,]*):\", str(self))\n-    assert m is not None\n-    m2m_str = m.group(1)\n-    if m2m_str == \"\":\n-      return ()\n-    return tuple(int(x) for x in m2m_str.split(\",\"))\n-\n \n-class LayoutRequest:\n+class AutoLayout:\n \n   def __repr__(self):\n-    return \"Request a layout from the compiler\"\n+    return \"AUTO\"\n \n-AUTO = LayoutRequest()\n+AUTO = AutoLayout()\n"
                }
            ],
            "whole_deleted": "-import re\n-\n-class XLACompatibleLayout(Layout):\n-\n-  def _to_xla_layout(self) -> str:\n-    raise NotImplementedError(\"Subclasses should implement this method.\")\n-\n-\n-class SpecifiedLayout(XLACompatibleLayout):\n-  layout: xc.Layout\n-  def __init__(self, layout: xc.Layout):\n-  @property\n-  def _minor_to_major(self):\n-    m = re.search(\"{([0-9,]*):\", str(self))\n-    assert m is not None\n-    m2m_str = m.group(1)\n-    if m2m_str == \"\":\n-      return ()\n-    return tuple(int(x) for x in m2m_str.split(\",\"))\n-\n-class LayoutRequest:\n-    return \"Request a layout from the compiler\"\n-AUTO = LayoutRequest()\n",
            "whole_added": "+class SpecifiedLayout(Layout):\n+  layout: xc.PjRtLayout\n+  def __init__(self, layout: xc.PjRtLayout):\n+class AutoLayout:\n+    return \"AUTO\"\n+AUTO = AutoLayout()\n",
            "whole_hunk": "@@ -14,8 +14,6 @@\n \n from __future__ import annotations\n \n-import re\n-\n from jax._src.lib import xla_client as xc\n \n \n@@ -24,16 +22,10 @@ class Layout:\n   pass\n \n \n-class XLACompatibleLayout(Layout):\n-\n-  def _to_xla_layout(self) -> str:\n-    raise NotImplementedError(\"Subclasses should implement this method.\")\n-\n-\n-class SpecifiedLayout(XLACompatibleLayout):\n-  layout: xc.Layout\n+class SpecifiedLayout(Layout):\n+  layout: xc.PjRtLayout\n \n-  def __init__(self, layout: xc.Layout):\n+  def __init__(self, layout: xc.PjRtLayout):\n     self._layout = layout\n     self._layout_str = str(self._layout)\n \n@@ -51,19 +43,10 @@ class SpecifiedLayout(XLACompatibleLayout):\n   def _to_xla_layout(self) -> str:\n     return self._layout_str\n \n-  @property\n-  def _minor_to_major(self):\n-    m = re.search(\"{([0-9,]*):\", str(self))\n-    assert m is not None\n-    m2m_str = m.group(1)\n-    if m2m_str == \"\":\n-      return ()\n-    return tuple(int(x) for x in m2m_str.split(\",\"))\n-\n \n-class LayoutRequest:\n+class AutoLayout:\n \n   def __repr__(self):\n-    return \"Request a layout from the compiler\"\n+    return \"AUTO\"\n \n-AUTO = LayoutRequest()\n+AUTO = AutoLayout()\n"
        },
        {
            "name": "pjit.py",
            "path": "jax/_src/pjit.py",
            "patches": [
                {
                    "old_start": 435,
                    "old_length": 6,
                    "new_start": 435,
                    "new_length": 7,
                    "hunk": "@@ -435,6 +435,7 @@ def _make_jit_wrapper(jit_info: PjitInfo):\n     try:\n       in_shardings = _resolve_in_shardings(\n           args_flat, params['in_shardings'], params['out_shardings'], mesh)\n+      in_layouts_flat = _resolve_in_layouts(args_flat, in_layouts_flat)\n       lowering = _pjit_lower(\n           params['jaxpr'], in_shardings, params['out_shardings'],\n           params['resource_env'], params['donated_invars'], params['name'],\n"
                },
                {
                    "old_start": 1264,
                    "old_length": 6,
                    "new_start": 1264,
                    "new_length": 28,
                    "hunk": "@@ -1264,6 +1264,28 @@ pjit_p = core.AxisPrimitive(\"pjit\")\n pjit_p.multiple_results = True\n \n \n+def _resolve_in_layouts(args, jit_in_layouts):\n+  resolved_in_layouts = []\n+  for arg, jit_in_l in safe_zip(args, jit_in_layouts):\n+    arg_layout, committed = (\n+        (arg.layout, getattr(arg, '_committed', True))\n+        if getattr(arg, 'layout', None) is not None else (None, False))\n+    if jit_in_l is None:\n+      if committed:\n+        resolved_in_layouts.append(arg_layout)\n+      else:\n+        resolved_in_layouts.append(None)\n+    else:\n+      if committed and arg_layout != jit_in_l:\n+        raise ValueError('Layout passed to jit does not match the layout '\n+                          'on the respective arg. '\n+                          f'Got pjit layout: {jit_in_l},\\n'\n+                          f'arg sharding: {arg_layout} for '\n+                          f'arg shape: {shaped_abstractify(arg).str_short()}')\n+      resolved_in_layouts.append(jit_in_l)\n+  return tuple(resolved_in_layouts)\n+\n+\n def _resolve_in_shardings(\n     args, pjit_in_shardings: Sequence[PjitSharding],\n     out_shardings: Sequence[PjitSharding],\n"
                },
                {
                    "old_start": 1387,
                    "old_length": 8,
                    "new_start": 1409,
                    "new_length": 9,
                    "hunk": "@@ -1387,8 +1409,9 @@ def _pjit_call_impl_python(\n   _most_recent_pjit_call_executable.weak_key_dict[jaxpr] = compiled\n   # This check is expensive so only do it if enable_checks is on.\n   if compiled._auto_spmd_lowering and config.enable_checks.value:\n-    pxla.check_gda_or_array_xla_sharding_match(args, compiled._in_shardings,\n-                                               jaxpr.jaxpr.debug_info)\n+    pxla.check_array_xla_sharding_layout_match(\n+        args, compiled._in_shardings, compiled._in_layouts,\n+        jaxpr.jaxpr.debug_info)\n   if config.distributed_debug.value:\n     # Defensively only perform fingerprint logic if debug logging is enabled\n     # NOTE(skyewm): I didn't benchmark this\n"
                }
            ],
            "whole_deleted": "-    pxla.check_gda_or_array_xla_sharding_match(args, compiled._in_shardings,\n-                                               jaxpr.jaxpr.debug_info)\n",
            "whole_added": "+      in_layouts_flat = _resolve_in_layouts(args_flat, in_layouts_flat)\n+def _resolve_in_layouts(args, jit_in_layouts):\n+  resolved_in_layouts = []\n+  for arg, jit_in_l in safe_zip(args, jit_in_layouts):\n+    arg_layout, committed = (\n+        (arg.layout, getattr(arg, '_committed', True))\n+        if getattr(arg, 'layout', None) is not None else (None, False))\n+    if jit_in_l is None:\n+      if committed:\n+        resolved_in_layouts.append(arg_layout)\n+      else:\n+        resolved_in_layouts.append(None)\n+    else:\n+      if committed and arg_layout != jit_in_l:\n+        raise ValueError('Layout passed to jit does not match the layout '\n+                          'on the respective arg. '\n+                          f'Got pjit layout: {jit_in_l},\\n'\n+                          f'arg sharding: {arg_layout} for '\n+                          f'arg shape: {shaped_abstractify(arg).str_short()}')\n+      resolved_in_layouts.append(jit_in_l)\n+  return tuple(resolved_in_layouts)\n+\n+\n+    pxla.check_array_xla_sharding_layout_match(\n+        args, compiled._in_shardings, compiled._in_layouts,\n+        jaxpr.jaxpr.debug_info)\n",
            "whole_hunk": "@@ -435,6 +435,7 @@ def _make_jit_wrapper(jit_info: PjitInfo):\n     try:\n       in_shardings = _resolve_in_shardings(\n           args_flat, params['in_shardings'], params['out_shardings'], mesh)\n+      in_layouts_flat = _resolve_in_layouts(args_flat, in_layouts_flat)\n       lowering = _pjit_lower(\n           params['jaxpr'], in_shardings, params['out_shardings'],\n           params['resource_env'], params['donated_invars'], params['name'],\n@@ -1264,6 +1264,28 @@ pjit_p = core.AxisPrimitive(\"pjit\")\n pjit_p.multiple_results = True\n \n \n+def _resolve_in_layouts(args, jit_in_layouts):\n+  resolved_in_layouts = []\n+  for arg, jit_in_l in safe_zip(args, jit_in_layouts):\n+    arg_layout, committed = (\n+        (arg.layout, getattr(arg, '_committed', True))\n+        if getattr(arg, 'layout', None) is not None else (None, False))\n+    if jit_in_l is None:\n+      if committed:\n+        resolved_in_layouts.append(arg_layout)\n+      else:\n+        resolved_in_layouts.append(None)\n+    else:\n+      if committed and arg_layout != jit_in_l:\n+        raise ValueError('Layout passed to jit does not match the layout '\n+                          'on the respective arg. '\n+                          f'Got pjit layout: {jit_in_l},\\n'\n+                          f'arg sharding: {arg_layout} for '\n+                          f'arg shape: {shaped_abstractify(arg).str_short()}')\n+      resolved_in_layouts.append(jit_in_l)\n+  return tuple(resolved_in_layouts)\n+\n+\n def _resolve_in_shardings(\n     args, pjit_in_shardings: Sequence[PjitSharding],\n     out_shardings: Sequence[PjitSharding],\n@@ -1387,8 +1409,9 @@ def _pjit_call_impl_python(\n   _most_recent_pjit_call_executable.weak_key_dict[jaxpr] = compiled\n   # This check is expensive so only do it if enable_checks is on.\n   if compiled._auto_spmd_lowering and config.enable_checks.value:\n-    pxla.check_gda_or_array_xla_sharding_match(args, compiled._in_shardings,\n-                                               jaxpr.jaxpr.debug_info)\n+    pxla.check_array_xla_sharding_layout_match(\n+        args, compiled._in_shardings, compiled._in_layouts,\n+        jaxpr.jaxpr.debug_info)\n   if config.distributed_debug.value:\n     # Defensively only perform fingerprint logic if debug logging is enabled\n     # NOTE(skyewm): I didn't benchmark this\n"
        },
        {
            "name": "layout_test.py",
            "path": "tests/layout_test.py",
            "patches": [
                {
                    "old_start": 14,
                    "old_length": 6,
                    "new_start": 14,
                    "new_length": 7,
                    "hunk": "@@ -14,6 +14,7 @@\n \n import math\n import os\n+import re\n from absl.testing import absltest\n import numpy as np\n \n"
                },
                {
                    "old_start": 50,
                    "old_length": 6,
                    "new_start": 51,
                    "new_length": 15,
                    "hunk": "@@ -50,6 +51,15 @@ def tearDownModule():\n   xla_bridge.get_backend.cache_clear()\n \n \n+pattern = re.compile(r\"\\{(.*?):\")\n+\n+# Extract minor_to_major from str(layout) because layout doesn't have a\n+# minor_to_major property yet.\n+def extract_minor_to_major(l):\n+  match = re.search(pattern, str(l))\n+  return tuple(int(i) for i in match.groups()[0].split(','))\n+\n+\n class LayoutTest(jtu.JaxTestCase):\n \n   def setUp(self):\n"
                },
                {
                    "old_start": 60,
                    "old_length": 9,
                    "new_start": 70,
                    "new_length": 11,
                    "hunk": "@@ -60,9 +70,11 @@ class LayoutTest(jtu.JaxTestCase):\n     super().setUp()\n \n   def test_auto_layout(self):\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n     shape1 = (128, 128)\n     shape2 = (128, 128)\n+    s1 = NamedSharding(mesh, P('x', 'y'))\n+    s2 = NamedSharding(mesh, P('x'))\n \n     def apply(x, y):\n       return x.T, y.T\n"
                },
                {
                    "old_start": 71,
                    "old_length": 70,
                    "new_start": 83,
                    "new_length": 89,
                    "hunk": "@@ -71,70 +83,89 @@ class LayoutTest(jtu.JaxTestCase):\n       return x * 2, y * 2\n \n     np_inp1 = np.arange(math.prod(shape1)).reshape(shape1)\n-    arr1 = jax.device_put(np_inp1, NamedSharding(mesh, P('x', 'y')))\n     np_inp2 = np.arange(math.prod(shape2)).reshape(shape2)\n-    arr2 = jax.device_put(np_inp2, NamedSharding(mesh, P('x')))\n+    sds1 = jax.ShapeDtypeStruct(np_inp1.shape, np_inp1.dtype, sharding=s1)\n+    sds2 = jax.ShapeDtypeStruct(np_inp2.shape, np_inp2.dtype, sharding=s2)\n \n-    lowered_apply = jax.jit(apply).lower(arr1, arr2, _in_layouts=layout.AUTO,\n-                                         _out_layouts=layout.AUTO)\n+    lowered_apply = jax.jit(apply).lower(\n+        sds1, sds2, _in_layouts=layout.AUTO, _out_layouts=layout.AUTO)\n     compiled_apply = lowered_apply.compile()\n \n     arg_layouts, kw_layouts = compiled_apply._input_layouts()\n     self.assertEmpty(kw_layouts)\n+\n     for i, o in zip(arg_layouts, compiled_apply._output_layouts()):\n-      self.assertEqual(i._minor_to_major, o._minor_to_major[::-1])\n+      self.assertEqual(extract_minor_to_major(i),\n+                       extract_minor_to_major(o)[::-1])\n \n     init_compiled = jax.jit(init).lower(\n-        arr1, arr2, _out_layouts=arg_layouts).compile()\n+        sds1, sds2, _out_layouts=arg_layouts).compile()\n \n     for i, o in zip(init_compiled._input_layouts()[0],\n                     init_compiled._output_layouts()):\n-      self.assertEqual(i._minor_to_major, o._minor_to_major)\n+      self.assertEqual(i, o)\n+\n+    arr1 = jax.device_put(np_inp1, s1)\n+    arr2 = jax.device_put(np_inp2, s2)\n \n     with jtu.count_aot_jit_cpp_cache_miss() as init_count:\n       init_out = init_compiled(arr1, arr2)\n       init_compiled(arr1, arr2)\n     self.assertEqual(init_count[0], 1)\n \n+    self.assertEqual(init_out[0].layout, init_compiled._output_layouts()[0])\n+    self.assertEqual(init_out[1].layout, init_compiled._output_layouts()[0])\n+\n     with jtu.count_aot_jit_cpp_cache_miss() as apply_count:\n       apply_out = compiled_apply(*init_out)\n       compiled_apply(*init_out)\n     self.assertEqual(apply_count[0], 1)\n \n+    self.assertEqual(apply_out[0].layout, compiled_apply._output_layouts()[0])\n+    self.assertEqual(apply_out[1].layout, compiled_apply._output_layouts()[1])\n+\n+    self.assertTupleEqual(extract_minor_to_major(apply_out[0].layout),\n+                          extract_minor_to_major(init_out[0].layout)[::-1])\n+    self.assertTupleEqual(extract_minor_to_major(apply_out[1].layout),\n+                          extract_minor_to_major(init_out[1].layout)[::-1])\n+\n     self.assertArraysEqual(init_out[0], np_inp1 * 2)\n     self.assertArraysEqual(init_out[1], np_inp2 * 2)\n     self.assertArraysEqual(apply_out[0], (np_inp1 * 2).T)\n     self.assertArraysEqual(apply_out[1], (np_inp2 * 2).T)\n \n   def test_default_layout(self):\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n-    shape = (8, 4, 2)\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+    shape = (4, 4, 2)\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n     s = NamedSharding(mesh, P('x', 'y'))\n+    sds = jax.ShapeDtypeStruct(np_inp.shape, np_inp.dtype, sharding=s)\n     arr = jax.device_put(np_inp, s)\n \n     def f(x):\n       return x.T\n \n-    lowered = jax.jit(f).lower(arr, _in_layouts=None, _out_layouts=None)\n+    lowered = jax.jit(f).lower(sds, _in_layouts=None, _out_layouts=None)\n     self.assertIn(\"default\", lowered.as_text())\n     compiled = lowered.compile()\n     out = compiled(arr)\n \n-    self.assertTupleEqual(compiled._input_layouts()[0][0]._minor_to_major, (2, 1, 0))\n-    self.assertTupleEqual(compiled._output_layouts()._minor_to_major, (2, 1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._input_layouts()[0][0]), (2, 1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._output_layouts()), (2, 1, 0))\n     self.assertArraysEqual(out, np_inp.T)\n     self.assertEqual(out.sharding, NamedSharding(mesh, P(None, 'y', 'x')))\n \n-    compiled_auto = jax.jit(f).lower(arr, _in_layouts=layout.AUTO,\n+    compiled_auto = jax.jit(f).lower(sds, _in_layouts=layout.AUTO,\n                                      _out_layouts=layout.AUTO).compile()\n-    self.assertTupleEqual(compiled_auto._input_layouts()[0][0]._minor_to_major,\n-                          (2, 1, 0))\n-    self.assertTupleEqual(compiled_auto._output_layouts()._minor_to_major,\n-                          (0, 1, 2))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled_auto._input_layouts()[0][0]), (2, 1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled_auto._output_layouts()), (0, 1, 2))\n \n   def test_in_layouts_out_layouts(self):\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n     shape = (8, 8)\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n     s = NamedSharding(mesh, P('x', 'y'))\n"
                },
                {
                    "old_start": 142,
                    "old_length": 17,
                    "new_start": 173,
                    "new_length": 21,
                    "hunk": "@@ -142,17 +173,21 @@ class LayoutTest(jtu.JaxTestCase):\n \n     def f(x):\n       return x.T\n+\n     compiled = jax.jit(f).lower(\n         arr, _in_layouts=None, _out_layouts=layout.AUTO).compile()\n-    self.assertTupleEqual(compiled._input_layouts()[0][0]._minor_to_major, (1, 0))\n-    self.assertTupleEqual(compiled._output_layouts()._minor_to_major, (0, 1))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._input_layouts()[0][0]), (1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._output_layouts()), (0, 1))\n \n     out = compiled(arr)\n     self.assertArraysEqual(out, np_inp.T)\n+    self.assertEqual(out.layout, compiled._output_layouts())\n     self.assertEqual(out.sharding, NamedSharding(mesh, P('y', 'x')))\n \n   def test_sharding_and_layouts(self):\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n     shape = (4, 8)\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n     s = NamedSharding(mesh, P('x', 'y'))\n"
                },
                {
                    "old_start": 160,
                    "old_length": 8,
                    "new_start": 195,
                    "new_length": 10,
                    "hunk": "@@ -160,8 +195,10 @@ class LayoutTest(jtu.JaxTestCase):\n     compiled = jax.jit(lambda x: x.T, in_shardings=s, out_shardings=s).lower(\n         np_inp, _in_layouts=layout.AUTO, _out_layouts=layout.AUTO).compile()\n     out = compiled(np_inp)\n-    self.assertTupleEqual(compiled._input_layouts()[0][0]._minor_to_major, (1, 0))\n-    self.assertTupleEqual(compiled._output_layouts()._minor_to_major, (0, 1))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._input_layouts()[0][0]), (1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._output_layouts()), (0, 1))\n     self.assertArraysEqual(out, np_inp.T)\n     self.assertEqual(out.sharding, s)\n \n"
                },
                {
                    "old_start": 188,
                    "old_length": 6,
                    "new_start": 225,
                    "new_length": 33,
                    "hunk": "@@ -188,6 +225,33 @@ class LayoutTest(jtu.JaxTestCase):\n     # TODO(yashkatariya, frostig): Also use the arg_layouts to create an Array\n     # and then pass that back into compiled.\n \n+  def test_aot_layout_mismatch(self):\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+    shape = (256, 4, 2)\n+    np_inp = np.arange(math.prod(shape)).reshape(shape)\n+    s = NamedSharding(mesh, P('x'))\n+\n+    sds = jax.ShapeDtypeStruct(np_inp.shape, np_inp.dtype, sharding=s)\n+    arr = jax.device_put(np_inp, s)\n+\n+    def f(x):\n+      return (x * 2).T\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        'Layout passed to jit does not match the layout on the respective arg'):\n+      jax.jit(f).lower(arr, _in_layouts=layout.AUTO)\n+\n+    compiled = jax.jit(f).lower(\n+        sds, _in_layouts=layout.AUTO, _out_layouts=layout.AUTO).compile()\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        r'Compiled object called with input layout\\(s\\) does'\n+        r' not match the layout\\(s\\) the computation was'\n+        ' compiled with'):\n+      compiled(arr)\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n"
                }
            ],
            "whole_deleted": "-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n-    arr1 = jax.device_put(np_inp1, NamedSharding(mesh, P('x', 'y')))\n-    arr2 = jax.device_put(np_inp2, NamedSharding(mesh, P('x')))\n-    lowered_apply = jax.jit(apply).lower(arr1, arr2, _in_layouts=layout.AUTO,\n-                                         _out_layouts=layout.AUTO)\n-      self.assertEqual(i._minor_to_major, o._minor_to_major[::-1])\n-        arr1, arr2, _out_layouts=arg_layouts).compile()\n-      self.assertEqual(i._minor_to_major, o._minor_to_major)\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n-    shape = (8, 4, 2)\n-    lowered = jax.jit(f).lower(arr, _in_layouts=None, _out_layouts=None)\n-    self.assertTupleEqual(compiled._input_layouts()[0][0]._minor_to_major, (2, 1, 0))\n-    self.assertTupleEqual(compiled._output_layouts()._minor_to_major, (2, 1, 0))\n-    compiled_auto = jax.jit(f).lower(arr, _in_layouts=layout.AUTO,\n-    self.assertTupleEqual(compiled_auto._input_layouts()[0][0]._minor_to_major,\n-                          (2, 1, 0))\n-    self.assertTupleEqual(compiled_auto._output_layouts()._minor_to_major,\n-                          (0, 1, 2))\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n-    self.assertTupleEqual(compiled._input_layouts()[0][0]._minor_to_major, (1, 0))\n-    self.assertTupleEqual(compiled._output_layouts()._minor_to_major, (0, 1))\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n-    self.assertTupleEqual(compiled._input_layouts()[0][0]._minor_to_major, (1, 0))\n-    self.assertTupleEqual(compiled._output_layouts()._minor_to_major, (0, 1))\n",
            "whole_added": "+import re\n+pattern = re.compile(r\"\\{(.*?):\")\n+\n+# Extract minor_to_major from str(layout) because layout doesn't have a\n+# minor_to_major property yet.\n+def extract_minor_to_major(l):\n+  match = re.search(pattern, str(l))\n+  return tuple(int(i) for i in match.groups()[0].split(','))\n+\n+\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+    s1 = NamedSharding(mesh, P('x', 'y'))\n+    s2 = NamedSharding(mesh, P('x'))\n+    sds1 = jax.ShapeDtypeStruct(np_inp1.shape, np_inp1.dtype, sharding=s1)\n+    sds2 = jax.ShapeDtypeStruct(np_inp2.shape, np_inp2.dtype, sharding=s2)\n+    lowered_apply = jax.jit(apply).lower(\n+        sds1, sds2, _in_layouts=layout.AUTO, _out_layouts=layout.AUTO)\n+\n+      self.assertEqual(extract_minor_to_major(i),\n+                       extract_minor_to_major(o)[::-1])\n+        sds1, sds2, _out_layouts=arg_layouts).compile()\n+      self.assertEqual(i, o)\n+\n+    arr1 = jax.device_put(np_inp1, s1)\n+    arr2 = jax.device_put(np_inp2, s2)\n+    self.assertEqual(init_out[0].layout, init_compiled._output_layouts()[0])\n+    self.assertEqual(init_out[1].layout, init_compiled._output_layouts()[0])\n+\n+    self.assertEqual(apply_out[0].layout, compiled_apply._output_layouts()[0])\n+    self.assertEqual(apply_out[1].layout, compiled_apply._output_layouts()[1])\n+\n+    self.assertTupleEqual(extract_minor_to_major(apply_out[0].layout),\n+                          extract_minor_to_major(init_out[0].layout)[::-1])\n+    self.assertTupleEqual(extract_minor_to_major(apply_out[1].layout),\n+                          extract_minor_to_major(init_out[1].layout)[::-1])\n+\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+    shape = (4, 4, 2)\n+    sds = jax.ShapeDtypeStruct(np_inp.shape, np_inp.dtype, sharding=s)\n+    lowered = jax.jit(f).lower(sds, _in_layouts=None, _out_layouts=None)\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._input_layouts()[0][0]), (2, 1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._output_layouts()), (2, 1, 0))\n+    compiled_auto = jax.jit(f).lower(sds, _in_layouts=layout.AUTO,\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled_auto._input_layouts()[0][0]), (2, 1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled_auto._output_layouts()), (0, 1, 2))\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._input_layouts()[0][0]), (1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._output_layouts()), (0, 1))\n+    self.assertEqual(out.layout, compiled._output_layouts())\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._input_layouts()[0][0]), (1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._output_layouts()), (0, 1))\n+  def test_aot_layout_mismatch(self):\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+    shape = (256, 4, 2)\n+    np_inp = np.arange(math.prod(shape)).reshape(shape)\n+    s = NamedSharding(mesh, P('x'))\n+\n+    sds = jax.ShapeDtypeStruct(np_inp.shape, np_inp.dtype, sharding=s)\n+    arr = jax.device_put(np_inp, s)\n+\n+    def f(x):\n+      return (x * 2).T\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        'Layout passed to jit does not match the layout on the respective arg'):\n+      jax.jit(f).lower(arr, _in_layouts=layout.AUTO)\n+\n+    compiled = jax.jit(f).lower(\n+        sds, _in_layouts=layout.AUTO, _out_layouts=layout.AUTO).compile()\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        r'Compiled object called with input layout\\(s\\) does'\n+        r' not match the layout\\(s\\) the computation was'\n+        ' compiled with'):\n+      compiled(arr)\n+\n",
            "whole_hunk": "@@ -14,6 +14,7 @@\n \n import math\n import os\n+import re\n from absl.testing import absltest\n import numpy as np\n \n@@ -50,6 +51,15 @@ def tearDownModule():\n   xla_bridge.get_backend.cache_clear()\n \n \n+pattern = re.compile(r\"\\{(.*?):\")\n+\n+# Extract minor_to_major from str(layout) because layout doesn't have a\n+# minor_to_major property yet.\n+def extract_minor_to_major(l):\n+  match = re.search(pattern, str(l))\n+  return tuple(int(i) for i in match.groups()[0].split(','))\n+\n+\n class LayoutTest(jtu.JaxTestCase):\n \n   def setUp(self):\n@@ -60,9 +70,11 @@ class LayoutTest(jtu.JaxTestCase):\n     super().setUp()\n \n   def test_auto_layout(self):\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n     shape1 = (128, 128)\n     shape2 = (128, 128)\n+    s1 = NamedSharding(mesh, P('x', 'y'))\n+    s2 = NamedSharding(mesh, P('x'))\n \n     def apply(x, y):\n       return x.T, y.T\n@@ -71,70 +83,89 @@ class LayoutTest(jtu.JaxTestCase):\n       return x * 2, y * 2\n \n     np_inp1 = np.arange(math.prod(shape1)).reshape(shape1)\n-    arr1 = jax.device_put(np_inp1, NamedSharding(mesh, P('x', 'y')))\n     np_inp2 = np.arange(math.prod(shape2)).reshape(shape2)\n-    arr2 = jax.device_put(np_inp2, NamedSharding(mesh, P('x')))\n+    sds1 = jax.ShapeDtypeStruct(np_inp1.shape, np_inp1.dtype, sharding=s1)\n+    sds2 = jax.ShapeDtypeStruct(np_inp2.shape, np_inp2.dtype, sharding=s2)\n \n-    lowered_apply = jax.jit(apply).lower(arr1, arr2, _in_layouts=layout.AUTO,\n-                                         _out_layouts=layout.AUTO)\n+    lowered_apply = jax.jit(apply).lower(\n+        sds1, sds2, _in_layouts=layout.AUTO, _out_layouts=layout.AUTO)\n     compiled_apply = lowered_apply.compile()\n \n     arg_layouts, kw_layouts = compiled_apply._input_layouts()\n     self.assertEmpty(kw_layouts)\n+\n     for i, o in zip(arg_layouts, compiled_apply._output_layouts()):\n-      self.assertEqual(i._minor_to_major, o._minor_to_major[::-1])\n+      self.assertEqual(extract_minor_to_major(i),\n+                       extract_minor_to_major(o)[::-1])\n \n     init_compiled = jax.jit(init).lower(\n-        arr1, arr2, _out_layouts=arg_layouts).compile()\n+        sds1, sds2, _out_layouts=arg_layouts).compile()\n \n     for i, o in zip(init_compiled._input_layouts()[0],\n                     init_compiled._output_layouts()):\n-      self.assertEqual(i._minor_to_major, o._minor_to_major)\n+      self.assertEqual(i, o)\n+\n+    arr1 = jax.device_put(np_inp1, s1)\n+    arr2 = jax.device_put(np_inp2, s2)\n \n     with jtu.count_aot_jit_cpp_cache_miss() as init_count:\n       init_out = init_compiled(arr1, arr2)\n       init_compiled(arr1, arr2)\n     self.assertEqual(init_count[0], 1)\n \n+    self.assertEqual(init_out[0].layout, init_compiled._output_layouts()[0])\n+    self.assertEqual(init_out[1].layout, init_compiled._output_layouts()[0])\n+\n     with jtu.count_aot_jit_cpp_cache_miss() as apply_count:\n       apply_out = compiled_apply(*init_out)\n       compiled_apply(*init_out)\n     self.assertEqual(apply_count[0], 1)\n \n+    self.assertEqual(apply_out[0].layout, compiled_apply._output_layouts()[0])\n+    self.assertEqual(apply_out[1].layout, compiled_apply._output_layouts()[1])\n+\n+    self.assertTupleEqual(extract_minor_to_major(apply_out[0].layout),\n+                          extract_minor_to_major(init_out[0].layout)[::-1])\n+    self.assertTupleEqual(extract_minor_to_major(apply_out[1].layout),\n+                          extract_minor_to_major(init_out[1].layout)[::-1])\n+\n     self.assertArraysEqual(init_out[0], np_inp1 * 2)\n     self.assertArraysEqual(init_out[1], np_inp2 * 2)\n     self.assertArraysEqual(apply_out[0], (np_inp1 * 2).T)\n     self.assertArraysEqual(apply_out[1], (np_inp2 * 2).T)\n \n   def test_default_layout(self):\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n-    shape = (8, 4, 2)\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+    shape = (4, 4, 2)\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n     s = NamedSharding(mesh, P('x', 'y'))\n+    sds = jax.ShapeDtypeStruct(np_inp.shape, np_inp.dtype, sharding=s)\n     arr = jax.device_put(np_inp, s)\n \n     def f(x):\n       return x.T\n \n-    lowered = jax.jit(f).lower(arr, _in_layouts=None, _out_layouts=None)\n+    lowered = jax.jit(f).lower(sds, _in_layouts=None, _out_layouts=None)\n     self.assertIn(\"default\", lowered.as_text())\n     compiled = lowered.compile()\n     out = compiled(arr)\n \n-    self.assertTupleEqual(compiled._input_layouts()[0][0]._minor_to_major, (2, 1, 0))\n-    self.assertTupleEqual(compiled._output_layouts()._minor_to_major, (2, 1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._input_layouts()[0][0]), (2, 1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._output_layouts()), (2, 1, 0))\n     self.assertArraysEqual(out, np_inp.T)\n     self.assertEqual(out.sharding, NamedSharding(mesh, P(None, 'y', 'x')))\n \n-    compiled_auto = jax.jit(f).lower(arr, _in_layouts=layout.AUTO,\n+    compiled_auto = jax.jit(f).lower(sds, _in_layouts=layout.AUTO,\n                                      _out_layouts=layout.AUTO).compile()\n-    self.assertTupleEqual(compiled_auto._input_layouts()[0][0]._minor_to_major,\n-                          (2, 1, 0))\n-    self.assertTupleEqual(compiled_auto._output_layouts()._minor_to_major,\n-                          (0, 1, 2))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled_auto._input_layouts()[0][0]), (2, 1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled_auto._output_layouts()), (0, 1, 2))\n \n   def test_in_layouts_out_layouts(self):\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n     shape = (8, 8)\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n     s = NamedSharding(mesh, P('x', 'y'))\n@@ -142,17 +173,21 @@ class LayoutTest(jtu.JaxTestCase):\n \n     def f(x):\n       return x.T\n+\n     compiled = jax.jit(f).lower(\n         arr, _in_layouts=None, _out_layouts=layout.AUTO).compile()\n-    self.assertTupleEqual(compiled._input_layouts()[0][0]._minor_to_major, (1, 0))\n-    self.assertTupleEqual(compiled._output_layouts()._minor_to_major, (0, 1))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._input_layouts()[0][0]), (1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._output_layouts()), (0, 1))\n \n     out = compiled(arr)\n     self.assertArraysEqual(out, np_inp.T)\n+    self.assertEqual(out.layout, compiled._output_layouts())\n     self.assertEqual(out.sharding, NamedSharding(mesh, P('y', 'x')))\n \n   def test_sharding_and_layouts(self):\n-    mesh = jtu.create_global_mesh((4, 2), ('x', 'y'))\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n     shape = (4, 8)\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n     s = NamedSharding(mesh, P('x', 'y'))\n@@ -160,8 +195,10 @@ class LayoutTest(jtu.JaxTestCase):\n     compiled = jax.jit(lambda x: x.T, in_shardings=s, out_shardings=s).lower(\n         np_inp, _in_layouts=layout.AUTO, _out_layouts=layout.AUTO).compile()\n     out = compiled(np_inp)\n-    self.assertTupleEqual(compiled._input_layouts()[0][0]._minor_to_major, (1, 0))\n-    self.assertTupleEqual(compiled._output_layouts()._minor_to_major, (0, 1))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._input_layouts()[0][0]), (1, 0))\n+    self.assertTupleEqual(\n+        extract_minor_to_major(compiled._output_layouts()), (0, 1))\n     self.assertArraysEqual(out, np_inp.T)\n     self.assertEqual(out.sharding, s)\n \n@@ -188,6 +225,33 @@ class LayoutTest(jtu.JaxTestCase):\n     # TODO(yashkatariya, frostig): Also use the arg_layouts to create an Array\n     # and then pass that back into compiled.\n \n+  def test_aot_layout_mismatch(self):\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+    shape = (256, 4, 2)\n+    np_inp = np.arange(math.prod(shape)).reshape(shape)\n+    s = NamedSharding(mesh, P('x'))\n+\n+    sds = jax.ShapeDtypeStruct(np_inp.shape, np_inp.dtype, sharding=s)\n+    arr = jax.device_put(np_inp, s)\n+\n+    def f(x):\n+      return (x * 2).T\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        'Layout passed to jit does not match the layout on the respective arg'):\n+      jax.jit(f).lower(arr, _in_layouts=layout.AUTO)\n+\n+    compiled = jax.jit(f).lower(\n+        sds, _in_layouts=layout.AUTO, _out_layouts=layout.AUTO).compile()\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        r'Compiled object called with input layout\\(s\\) does'\n+        r' not match the layout\\(s\\) the computation was'\n+        ' compiled with'):\n+      compiled(arr)\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n"
        },
        {
            "name": "memories_test.py",
            "path": "tests/memories_test.py",
            "patches": [
                {
                    "old_start": 198,
                    "old_length": 8,
                    "new_start": 198,
                    "new_length": 10,
                    "hunk": "@@ -198,8 +198,10 @@ class ShardingMemoriesTest(jtu.JaxTestCase):\n     self.assertEqual(dev.default_memory().kind, \"device\")\n \n   def test_parameter_streaming(self):\n-    _, s_host, _, inp_host = _create_inputs(\n-        (8, 2), P(\"x\", \"y\"), mem_kind=\"unpinned_host\")\n+    self.skipTest(\"Enable after pinned_host support exists\")\n+\n+    _, s_host, np_inp, inp_host = _create_inputs(\n+        (8, 2), P(\"x\", \"y\"), mem_kind=\"pinned_host\")\n     s_dev = s_host.with_memory_kind('device')\n     inp_dev = jax.device_put(inp_host, s_dev)\n \n"
                }
            ],
            "whole_deleted": "-    _, s_host, _, inp_host = _create_inputs(\n-        (8, 2), P(\"x\", \"y\"), mem_kind=\"unpinned_host\")\n",
            "whole_added": "+    self.skipTest(\"Enable after pinned_host support exists\")\n+\n+    _, s_host, np_inp, inp_host = _create_inputs(\n+        (8, 2), P(\"x\", \"y\"), mem_kind=\"pinned_host\")\n",
            "whole_hunk": "@@ -198,8 +198,10 @@ class ShardingMemoriesTest(jtu.JaxTestCase):\n     self.assertEqual(dev.default_memory().kind, \"device\")\n \n   def test_parameter_streaming(self):\n-    _, s_host, _, inp_host = _create_inputs(\n-        (8, 2), P(\"x\", \"y\"), mem_kind=\"unpinned_host\")\n+    self.skipTest(\"Enable after pinned_host support exists\")\n+\n+    _, s_host, np_inp, inp_host = _create_inputs(\n+        (8, 2), P(\"x\", \"y\"), mem_kind=\"pinned_host\")\n     s_dev = s_host.with_memory_kind('device')\n     inp_dev = jax.device_put(inp_host, s_dev)\n \n"
        },
        {
            "name": "pjit_test.py",
            "path": "tests/pjit_test.py",
            "patches": [
                {
                    "old_start": 2836,
                    "old_length": 6,
                    "new_start": 2836,
                    "new_length": 20,
                    "hunk": "@@ -2836,6 +2836,20 @@ class ArrayPjitTest(jtu.JaxTestCase):\n     self.assertEqual(compiled._executable._kept_var_idx, {5})\n     self.assertLen(compiled._executable.in_avals, 1)\n \n+  def test_pjit_relayout_multi_slice(self):\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+\n+    @jax.jit\n+    def mul(x):\n+      return x @ x.T\n+\n+    x = jnp.arange(8).reshape(4, 2)\n+    y = jax.device_put(x, jax.sharding.NamedSharding(mesh, P('x', 'y')))\n+    compiled = mul.lower(jax.ShapeDtypeStruct(\n+        y.shape, y.dtype, sharding=y.sharding)).compile()\n+    out = compiled(y)\n+    self.assertArraysEqual(out, x @ x.T)\n+\n   def test_pjit_with_device_arg(self):\n     def mul(x):\n       return x @ x.T"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  def test_pjit_relayout_multi_slice(self):\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+\n+    @jax.jit\n+    def mul(x):\n+      return x @ x.T\n+\n+    x = jnp.arange(8).reshape(4, 2)\n+    y = jax.device_put(x, jax.sharding.NamedSharding(mesh, P('x', 'y')))\n+    compiled = mul.lower(jax.ShapeDtypeStruct(\n+        y.shape, y.dtype, sharding=y.sharding)).compile()\n+    out = compiled(y)\n+    self.assertArraysEqual(out, x @ x.T)\n+\n",
            "whole_hunk": "@@ -2836,6 +2836,20 @@ class ArrayPjitTest(jtu.JaxTestCase):\n     self.assertEqual(compiled._executable._kept_var_idx, {5})\n     self.assertLen(compiled._executable.in_avals, 1)\n \n+  def test_pjit_relayout_multi_slice(self):\n+    mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))\n+\n+    @jax.jit\n+    def mul(x):\n+      return x @ x.T\n+\n+    x = jnp.arange(8).reshape(4, 2)\n+    y = jax.device_put(x, jax.sharding.NamedSharding(mesh, P('x', 'y')))\n+    compiled = mul.lower(jax.ShapeDtypeStruct(\n+        y.shape, y.dtype, sharding=y.sharding)).compile()\n+    out = compiled(y)\n+    self.assertArraysEqual(out, x @ x.T)\n+\n   def test_pjit_with_device_arg(self):\n     def mul(x):\n       return x @ x.T"
        }
    ]
},
{
    "Id": 71,
    "commit_link": "https://github.com/google/jax/commit/d57bb8c7489a77981bae9a9e2dad04de0c681118",
    "date": "2024-03-21T17:46:32-07:00",
    "message": "Raise a better error message when an invalid input is passed to jit call.\n\nBefore:\n\n```\nTypeError: Argument 'ShapeDtypeStruct(shape=(4, 2), dtype=int32)' of type <class 'jax._src.api.ShapeDtypeStruct'> is not a valid JAX type.\n\n```\n\nAfter:\n\n```\nTypeError: Argument 'x['b']['c']' of shape int32[4,2] of type <class 'jax._src.api.ShapeDtypeStruct'> is not a valid JAX type.\n\n```\n\nThe error is raised deep down the stack during `shard_arg`, so we raise an `InvalidInputException` and catch it in `_python_pjit_helper` where we have the `arg_names` information.\n\nPiperOrigin-RevId: 618014044",
    "changes": [
        {
            "name": "xla.py",
            "path": "jax/_src/interpreters/xla.py",
            "patches": [
                {
                    "old_start": 132,
                    "old_length": 6,
                    "new_start": 130,
                    "new_length": 10,
                    "hunk": "@@ -132,6 +130,10 @@ _xla_shape_handlers[core.AbstractToken] = lambda _: (xc.Shape.token_shape(),)\n \n # IR constants\n \n+class InvalidInputException(Exception):\n+  pass\n+\n+\n # TODO(mattjj): try to remove this canonicalize_dtype stuff\n def canonicalize_dtype(x):\n   typ = type(x)\n"
                },
                {
                    "old_start": 142,
                    "old_length": 8,
                    "new_start": 144,
                    "new_length": 8,
                    "hunk": "@@ -142,8 +144,8 @@ def canonicalize_dtype(x):\n     if handler: return handler(x)\n   if hasattr(x, '__jax_array__'):\n     return canonicalize_dtype(x.__jax_array__())\n-  raise TypeError(f\"Argument '{x}' of type {type(x)} is not a valid \"\n-                  \"JAX type.\")\n+  raise InvalidInputException(\n+      f\"Argument '{x}' of type {type(x)} is not a valid JAX type.\")\n \n def _canonicalize_masked_array_dtype(x):\n   raise ValueError(\"numpy masked arrays are not supported as direct inputs to JAX functions. \"\n"
                }
            ],
            "whole_deleted": "-  raise TypeError(f\"Argument '{x}' of type {type(x)} is not a valid \"\n-                  \"JAX type.\")\n",
            "whole_added": "+class InvalidInputException(Exception):\n+  pass\n+\n+\n+  raise InvalidInputException(\n+      f\"Argument '{x}' of type {type(x)} is not a valid JAX type.\")\n",
            "whole_hunk": "@@ -132,6 +130,10 @@ _xla_shape_handlers[core.AbstractToken] = lambda _: (xc.Shape.token_shape(),)\n \n # IR constants\n \n+class InvalidInputException(Exception):\n+  pass\n+\n+\n # TODO(mattjj): try to remove this canonicalize_dtype stuff\n def canonicalize_dtype(x):\n   typ = type(x)\n@@ -142,8 +144,8 @@ def canonicalize_dtype(x):\n     if handler: return handler(x)\n   if hasattr(x, '__jax_array__'):\n     return canonicalize_dtype(x.__jax_array__())\n-  raise TypeError(f\"Argument '{x}' of type {type(x)} is not a valid \"\n-                  \"JAX type.\")\n+  raise InvalidInputException(\n+      f\"Argument '{x}' of type {type(x)} is not a valid JAX type.\")\n \n def _canonicalize_masked_array_dtype(x):\n   raise ValueError(\"numpy masked arrays are not supported as direct inputs to JAX functions. \"\n"
        },
        {
            "name": "pjit.py",
            "path": "jax/_src/pjit.py",
            "patches": [
                {
                    "old_start": 180,
                    "old_length": 6,
                    "new_start": 182,
                    "new_length": 22,
                    "hunk": "@@ -180,6 +182,22 @@ def _python_pjit_helper(jit_info, *args, **kwargs):\n     msg = _device_assignment_mismatch_error(\n         fun_name, fails, args_flat, api_name, arg_names)\n     raise ValueError(msg) from None\n+  except xla.InvalidInputException as e:\n+    arg_names = [''] * len(args_flat) if arg_names is None else arg_names\n+    # Run canonicalization again to figure out which arg failed.\n+    if params['jaxpr'].consts:\n+      raise TypeError(e.args[0]) from e\n+    else:\n+      for arg, name, aval in zip(args_flat, arg_names, params['jaxpr'].in_avals):\n+        try:\n+          xla.canonicalize_dtype(arg)\n+        except xla.InvalidInputException as _:\n+          # Reraise as TypeError with the new message.\n+          raise TypeError(\n+              f\"Argument '{name}' of shape {aval.str_short()} of type\"\n+              f' {type(arg)} is not a valid JAX type.') from e\n+      raise AssertionError(\"Unreachable\") from e\n+\n   if attrs_tracked:\n     final_states, out_flat = split_list(out_flat, [len(attrs_tracked)])\n     _set_states(attrs_tracked, final_states)\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  except xla.InvalidInputException as e:\n+    arg_names = [''] * len(args_flat) if arg_names is None else arg_names\n+    # Run canonicalization again to figure out which arg failed.\n+    if params['jaxpr'].consts:\n+      raise TypeError(e.args[0]) from e\n+    else:\n+      for arg, name, aval in zip(args_flat, arg_names, params['jaxpr'].in_avals):\n+        try:\n+          xla.canonicalize_dtype(arg)\n+        except xla.InvalidInputException as _:\n+          # Reraise as TypeError with the new message.\n+          raise TypeError(\n+              f\"Argument '{name}' of shape {aval.str_short()} of type\"\n+              f' {type(arg)} is not a valid JAX type.') from e\n+      raise AssertionError(\"Unreachable\") from e\n+\n",
            "whole_hunk": "@@ -180,6 +182,22 @@ def _python_pjit_helper(jit_info, *args, **kwargs):\n     msg = _device_assignment_mismatch_error(\n         fun_name, fails, args_flat, api_name, arg_names)\n     raise ValueError(msg) from None\n+  except xla.InvalidInputException as e:\n+    arg_names = [''] * len(args_flat) if arg_names is None else arg_names\n+    # Run canonicalization again to figure out which arg failed.\n+    if params['jaxpr'].consts:\n+      raise TypeError(e.args[0]) from e\n+    else:\n+      for arg, name, aval in zip(args_flat, arg_names, params['jaxpr'].in_avals):\n+        try:\n+          xla.canonicalize_dtype(arg)\n+        except xla.InvalidInputException as _:\n+          # Reraise as TypeError with the new message.\n+          raise TypeError(\n+              f\"Argument '{name}' of shape {aval.str_short()} of type\"\n+              f' {type(arg)} is not a valid JAX type.') from e\n+      raise AssertionError(\"Unreachable\") from e\n+\n   if attrs_tracked:\n     final_states, out_flat = split_list(out_flat, [len(attrs_tracked)])\n     _set_states(attrs_tracked, final_states)\n"
        },
        {
            "name": "pjit_test.py",
            "path": "tests/pjit_test.py",
            "patches": [
                {
                    "old_start": 2927,
                    "old_length": 6,
                    "new_start": 2927,
                    "new_length": 16,
                    "hunk": "@@ -2927,6 +2927,16 @@ class ArrayPjitTest(jtu.JaxTestCase):\n         'out_shardings should not be specified.'):\n       pjit(lambda x: x, out_shardings=s, device=jax.devices()[0])\n \n+  def test_check_arg_error(self):\n+    sds = jax.ShapeDtypeStruct((4, 2), np.int32)\n+    inp = np.arange(8).reshape(4, 2)\n+\n+    with self.assertRaisesRegex(\n+        TypeError,\n+        r\"Argument 'x\\['b'\\]\\['c'\\]' of shape int32\\[4,2\\] of \"\n+        \"type.*ShapeDtypeStruct.*is not a valid JAX type.\"):\n+      jax.jit(lambda x: x)({'a': inp, 'b': {'c': sds}})\n+\n   def test_pjit_device_backend_both_error(self):\n     with self.assertRaisesRegex(\n         ValueError, \"can't specify both a device and a backend for jit\"):"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  def test_check_arg_error(self):\n+    sds = jax.ShapeDtypeStruct((4, 2), np.int32)\n+    inp = np.arange(8).reshape(4, 2)\n+\n+    with self.assertRaisesRegex(\n+        TypeError,\n+        r\"Argument 'x\\['b'\\]\\['c'\\]' of shape int32\\[4,2\\] of \"\n+        \"type.*ShapeDtypeStruct.*is not a valid JAX type.\"):\n+      jax.jit(lambda x: x)({'a': inp, 'b': {'c': sds}})\n+\n",
            "whole_hunk": "@@ -2927,6 +2927,16 @@ class ArrayPjitTest(jtu.JaxTestCase):\n         'out_shardings should not be specified.'):\n       pjit(lambda x: x, out_shardings=s, device=jax.devices()[0])\n \n+  def test_check_arg_error(self):\n+    sds = jax.ShapeDtypeStruct((4, 2), np.int32)\n+    inp = np.arange(8).reshape(4, 2)\n+\n+    with self.assertRaisesRegex(\n+        TypeError,\n+        r\"Argument 'x\\['b'\\]\\['c'\\]' of shape int32\\[4,2\\] of \"\n+        \"type.*ShapeDtypeStruct.*is not a valid JAX type.\"):\n+      jax.jit(lambda x: x)({'a': inp, 'b': {'c': sds}})\n+\n   def test_pjit_device_backend_both_error(self):\n     with self.assertRaisesRegex(\n         ValueError, \"can't specify both a device and a backend for jit\"):"
        }
    ]
},
{
    "Id": 72,
    "commit_link": "https://github.com/google/jax/commit/74f4846d14b12380f6eb742b86abfdea09666f13",
    "date": "2024-03-21T08:08:51-07:00",
    "message": "Allow disabling checking CUDA constraints at runtime.\n\nPiperOrigin-RevId: 617845847",
    "changes": [
        {
            "name": "xla_bridge.py",
            "path": "jax/_src/xla_bridge.py",
            "patches": [
                {
                    "old_start": 333,
                    "old_length": 7,
                    "new_start": 333,
                    "new_length": 8,
                    "hunk": "@@ -333,7 +333,8 @@ def make_gpu_client(\n       else distributed.global_state.num_processes\n   )\n   if platform_name == \"cuda\":\n-    _check_cuda_versions()\n+    if not os.getenv(\"JAX_SKIP_CUDA_CONSTRAINTS_CHECK\"):\n+      _check_cuda_versions()\n     # TODO(micky774): remove this check when minimum jaxlib is v0.4.26\n     if jaxlib.version.__version_info__ >= (0, 4, 26):\n       devices_to_check = (allowed_devices if allowed_devices else"
                }
            ],
            "whole_deleted": "-    _check_cuda_versions()\n",
            "whole_added": "+    if not os.getenv(\"JAX_SKIP_CUDA_CONSTRAINTS_CHECK\"):\n+      _check_cuda_versions()\n",
            "whole_hunk": "@@ -333,7 +333,8 @@ def make_gpu_client(\n       else distributed.global_state.num_processes\n   )\n   if platform_name == \"cuda\":\n-    _check_cuda_versions()\n+    if not os.getenv(\"JAX_SKIP_CUDA_CONSTRAINTS_CHECK\"):\n+      _check_cuda_versions()\n     # TODO(micky774): remove this check when minimum jaxlib is v0.4.26\n     if jaxlib.version.__version_info__ >= (0, 4, 26):\n       devices_to_check = (allowed_devices if allowed_devices else"
        }
    ]
},
{
    "Id": 73,
    "commit_link": "https://github.com/google/jax/commit/f759452219b2b9f3df4f5d89e6af7acbdfde8170",
    "date": "2024-03-18T23:23:59-07:00",
    "message": "[XLA:Python] Improve error checking for the return value of the to_iterable function of custom pytree nodes.\n\nPiperOrigin-RevId: 617066587",
    "changes": [
        {
            "name": "tree_util_test.py",
            "path": "tests/tree_util_test.py",
            "patches": [
                {
                    "old_start": 17,
                    "old_length": 6,
                    "new_start": 17,
                    "new_length": 7,
                    "hunk": "@@ -17,6 +17,7 @@ import dataclasses\n import functools\n import pickle\n import re\n+import unittest\n \n from absl.testing import absltest\n from absl.testing import parameterized\n"
                },
                {
                    "old_start": 25,
                    "old_length": 6,
                    "new_start": 26,
                    "new_length": 7,
                    "hunk": "@@ -25,6 +26,7 @@ import jax\n from jax import tree_util\n from jax import flatten_util\n from jax._src import test_util as jtu\n+from jax._src.lib import xla_extension_version\n from jax._src.tree_util import prefix_errors, flatten_one_level\n import jax.numpy as jnp\n \n"
                },
                {
                    "old_start": 42,
                    "old_length": 6,
                    "new_start": 44,
                    "new_length": 19,
                    "hunk": "@@ -42,6 +44,19 @@ ATuple2 = collections.namedtuple(\"ATuple2\", (\"foo\", \"bar\"))\n tree_util.register_pytree_node(ATuple2, lambda o: ((o.foo,), o.bar),\n                                lambda bar, foo: ATuple2(foo[0], bar))\n \n+BadFlattenNonTuple = collections.namedtuple(\"ATuple2\", (\"foo\", \"bar\"))\n+tree_util.register_pytree_node(BadFlattenNonTuple, lambda o: \"hello\",\n+                               lambda bar, foo: ATuple2(foo[0], bar))\n+\n+BadFlattenBadArityTuple = collections.namedtuple(\"ATuple2\", (\"foo\", \"bar\"))\n+tree_util.register_pytree_node(BadFlattenBadArityTuple, lambda o: (2, 3, 4),\n+                               lambda bar, foo: ATuple2(foo[0], bar))\n+\n+BadFlattenNonIterableLeaves = collections.namedtuple(\"ATuple2\", (\"foo\", \"bar\"))\n+tree_util.register_pytree_node(BadFlattenNonIterableLeaves, lambda o: (7, 7),\n+                               lambda bar, foo: ATuple2(foo[0], bar))\n+\n+\n class AnObject:\n \n   def __init__(self, x, y, z):\n"
                },
                {
                    "old_start": 762,
                    "old_length": 6,
                    "new_start": 777,
                    "new_length": 37,
                    "hunk": "@@ -762,6 +777,37 @@ class TreeTest(jtu.JaxTestCase):\n     leaves, _ = tree_util.tree_flatten_with_path(ATuple2(1, 'hi'))\n     self.assertLen(leaves, 1)\n \n+  @unittest.skipIf(xla_extension_version < 247, \"Requires jaxlib>=0.4.26\")\n+  def testBadFlattenNonTuple(self):\n+    t = BadFlattenNonTuple(3, 4)\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        \"The to_iterable function for a custom PyTree node should return a\"\n+        r\" \\(children, aux_data\\) tuple, got 'hello'\",\n+    ):\n+      tree_util.tree_flatten(t)\n+\n+  @unittest.skipIf(xla_extension_version < 247, \"Requires jaxlib>=0.4.26\")\n+  def testBadFlattenBadArityTuple(self):\n+    t = BadFlattenBadArityTuple(3, 4)\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        \"The to_iterable function for a custom PyTree node should return a\"\n+        r\" \\(children, aux_data\\) tuple, got \\(2, 3, 4\\)\",\n+    ):\n+      tree_util.tree_flatten(t)\n+\n+  @unittest.skipIf(xla_extension_version < 247, \"Requires jaxlib>=0.4.26\")\n+  def testBadFlattenNonIterableLeaves(self):\n+    t = BadFlattenNonIterableLeaves(3, 4)\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        \"The to_iterable function for a custom PyTree node should return a\"\n+        r\" \\(children, aux_data\\) tuple where 'children' is iterable, got \"\n+        r\"\\(7, 7\\)\",\n+    ):\n+      tree_util.tree_flatten(t)\n+\n \n class StaticTest(parameterized.TestCase):\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "+import unittest\n+from jax._src.lib import xla_extension_version\n+BadFlattenNonTuple = collections.namedtuple(\"ATuple2\", (\"foo\", \"bar\"))\n+tree_util.register_pytree_node(BadFlattenNonTuple, lambda o: \"hello\",\n+                               lambda bar, foo: ATuple2(foo[0], bar))\n+\n+BadFlattenBadArityTuple = collections.namedtuple(\"ATuple2\", (\"foo\", \"bar\"))\n+tree_util.register_pytree_node(BadFlattenBadArityTuple, lambda o: (2, 3, 4),\n+                               lambda bar, foo: ATuple2(foo[0], bar))\n+\n+BadFlattenNonIterableLeaves = collections.namedtuple(\"ATuple2\", (\"foo\", \"bar\"))\n+tree_util.register_pytree_node(BadFlattenNonIterableLeaves, lambda o: (7, 7),\n+                               lambda bar, foo: ATuple2(foo[0], bar))\n+\n+\n+  @unittest.skipIf(xla_extension_version < 247, \"Requires jaxlib>=0.4.26\")\n+  def testBadFlattenNonTuple(self):\n+    t = BadFlattenNonTuple(3, 4)\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        \"The to_iterable function for a custom PyTree node should return a\"\n+        r\" \\(children, aux_data\\) tuple, got 'hello'\",\n+    ):\n+      tree_util.tree_flatten(t)\n+\n+  @unittest.skipIf(xla_extension_version < 247, \"Requires jaxlib>=0.4.26\")\n+  def testBadFlattenBadArityTuple(self):\n+    t = BadFlattenBadArityTuple(3, 4)\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        \"The to_iterable function for a custom PyTree node should return a\"\n+        r\" \\(children, aux_data\\) tuple, got \\(2, 3, 4\\)\",\n+    ):\n+      tree_util.tree_flatten(t)\n+\n+  @unittest.skipIf(xla_extension_version < 247, \"Requires jaxlib>=0.4.26\")\n+  def testBadFlattenNonIterableLeaves(self):\n+    t = BadFlattenNonIterableLeaves(3, 4)\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        \"The to_iterable function for a custom PyTree node should return a\"\n+        r\" \\(children, aux_data\\) tuple where 'children' is iterable, got \"\n+        r\"\\(7, 7\\)\",\n+    ):\n+      tree_util.tree_flatten(t)\n+\n",
            "whole_hunk": "@@ -17,6 +17,7 @@ import dataclasses\n import functools\n import pickle\n import re\n+import unittest\n \n from absl.testing import absltest\n from absl.testing import parameterized\n@@ -25,6 +26,7 @@ import jax\n from jax import tree_util\n from jax import flatten_util\n from jax._src import test_util as jtu\n+from jax._src.lib import xla_extension_version\n from jax._src.tree_util import prefix_errors, flatten_one_level\n import jax.numpy as jnp\n \n@@ -42,6 +44,19 @@ ATuple2 = collections.namedtuple(\"ATuple2\", (\"foo\", \"bar\"))\n tree_util.register_pytree_node(ATuple2, lambda o: ((o.foo,), o.bar),\n                                lambda bar, foo: ATuple2(foo[0], bar))\n \n+BadFlattenNonTuple = collections.namedtuple(\"ATuple2\", (\"foo\", \"bar\"))\n+tree_util.register_pytree_node(BadFlattenNonTuple, lambda o: \"hello\",\n+                               lambda bar, foo: ATuple2(foo[0], bar))\n+\n+BadFlattenBadArityTuple = collections.namedtuple(\"ATuple2\", (\"foo\", \"bar\"))\n+tree_util.register_pytree_node(BadFlattenBadArityTuple, lambda o: (2, 3, 4),\n+                               lambda bar, foo: ATuple2(foo[0], bar))\n+\n+BadFlattenNonIterableLeaves = collections.namedtuple(\"ATuple2\", (\"foo\", \"bar\"))\n+tree_util.register_pytree_node(BadFlattenNonIterableLeaves, lambda o: (7, 7),\n+                               lambda bar, foo: ATuple2(foo[0], bar))\n+\n+\n class AnObject:\n \n   def __init__(self, x, y, z):\n@@ -762,6 +777,37 @@ class TreeTest(jtu.JaxTestCase):\n     leaves, _ = tree_util.tree_flatten_with_path(ATuple2(1, 'hi'))\n     self.assertLen(leaves, 1)\n \n+  @unittest.skipIf(xla_extension_version < 247, \"Requires jaxlib>=0.4.26\")\n+  def testBadFlattenNonTuple(self):\n+    t = BadFlattenNonTuple(3, 4)\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        \"The to_iterable function for a custom PyTree node should return a\"\n+        r\" \\(children, aux_data\\) tuple, got 'hello'\",\n+    ):\n+      tree_util.tree_flatten(t)\n+\n+  @unittest.skipIf(xla_extension_version < 247, \"Requires jaxlib>=0.4.26\")\n+  def testBadFlattenBadArityTuple(self):\n+    t = BadFlattenBadArityTuple(3, 4)\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        \"The to_iterable function for a custom PyTree node should return a\"\n+        r\" \\(children, aux_data\\) tuple, got \\(2, 3, 4\\)\",\n+    ):\n+      tree_util.tree_flatten(t)\n+\n+  @unittest.skipIf(xla_extension_version < 247, \"Requires jaxlib>=0.4.26\")\n+  def testBadFlattenNonIterableLeaves(self):\n+    t = BadFlattenNonIterableLeaves(3, 4)\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        \"The to_iterable function for a custom PyTree node should return a\"\n+        r\" \\(children, aux_data\\) tuple where 'children' is iterable, got \"\n+        r\"\\(7, 7\\)\",\n+    ):\n+      tree_util.tree_flatten(t)\n+\n \n class StaticTest(parameterized.TestCase):\n "
        }
    ]
},
{
    "Id": 74,
    "commit_link": "https://github.com/google/jax/commit/bc363de8a5849ab7d25912d2df87678d8f23e92d",
    "date": "2024-03-18T13:16:36-07:00",
    "message": "Copybara import of the project:\n\n--\nac2c522dfe1e90ce7999e29f0fdf9660782db73d by Meekail Zain <zainmeekail@gmail.com>:\n\n[FIX] Added jaxlib version guard for CUDA compute capability check\n\nCOPYBARA_INTEGRATE_REVIEW=https://github.com/google/jax/pull/20237 from Micky774:add_version_guard ac2c522dfe1e90ce7999e29f0fdf9660782db73d\nPiperOrigin-RevId: 616925918",
    "changes": [
        {
            "name": "xla_bridge.py",
            "path": "jax/_src/xla_bridge.py",
            "patches": [
                {
                    "old_start": 46,
                    "old_length": 6,
                    "new_start": 46,
                    "new_length": 7,
                    "hunk": "@@ -46,6 +46,7 @@ from jax._src.lib import cuda_versions\n from jax._src.lib import xla_client\n from jax._src.lib import xla_extension\n from jax._src.lib import xla_extension_version\n+from jax._src.lib import jaxlib\n \n logger = logging.getLogger(__name__)\n \n"
                },
                {
                    "old_start": 333,
                    "old_length": 8,
                    "new_start": 334,
                    "new_length": 11,
                    "hunk": "@@ -333,8 +334,11 @@ def make_gpu_client(\n   )\n   if platform_name == \"cuda\":\n     _check_cuda_versions()\n-    devices_to_check = allowed_devices if allowed_devices else range(cuda_versions.cuda_device_count())\n-    _check_cuda_compute_capability(devices_to_check)\n+    # TODO(micky774): remove this check when minimum jaxlib is v0.4.26\n+    if jaxlib.version.__version_info__ >= (0, 4, 26):\n+      devices_to_check = (allowed_devices if allowed_devices else\n+                          range(cuda_versions.cuda_device_count()))\n+      _check_cuda_compute_capability(devices_to_check)\n \n   return xla_client.make_gpu_client(\n       distributed_client=distributed.global_state.client,"
                }
            ],
            "whole_deleted": "-    devices_to_check = allowed_devices if allowed_devices else range(cuda_versions.cuda_device_count())\n-    _check_cuda_compute_capability(devices_to_check)\n",
            "whole_added": "+from jax._src.lib import jaxlib\n+    # TODO(micky774): remove this check when minimum jaxlib is v0.4.26\n+    if jaxlib.version.__version_info__ >= (0, 4, 26):\n+      devices_to_check = (allowed_devices if allowed_devices else\n+                          range(cuda_versions.cuda_device_count()))\n+      _check_cuda_compute_capability(devices_to_check)\n",
            "whole_hunk": "@@ -46,6 +46,7 @@ from jax._src.lib import cuda_versions\n from jax._src.lib import xla_client\n from jax._src.lib import xla_extension\n from jax._src.lib import xla_extension_version\n+from jax._src.lib import jaxlib\n \n logger = logging.getLogger(__name__)\n \n@@ -333,8 +334,11 @@ def make_gpu_client(\n   )\n   if platform_name == \"cuda\":\n     _check_cuda_versions()\n-    devices_to_check = allowed_devices if allowed_devices else range(cuda_versions.cuda_device_count())\n-    _check_cuda_compute_capability(devices_to_check)\n+    # TODO(micky774): remove this check when minimum jaxlib is v0.4.26\n+    if jaxlib.version.__version_info__ >= (0, 4, 26):\n+      devices_to_check = (allowed_devices if allowed_devices else\n+                          range(cuda_versions.cuda_device_count()))\n+      _check_cuda_compute_capability(devices_to_check)\n \n   return xla_client.make_gpu_client(\n       distributed_client=distributed.global_state.client,"
        }
    ]
},
{
    "Id": 75,
    "commit_link": "https://github.com/google/jax/commit/566af12acaef2d638a4a5845b447633d19c46961",
    "date": "2024-03-17T17:55:29-07:00",
    "message": "make print_saved_residuals show more checkpoint_name names\n\nPreviously, we would only show the `checkpoint_name` name for a saved residual\nif the _output_ of `checkpoint_name` was saved. That meant that a policy like\n`save_dots` could allow a value to be saved, but because the saved version was\nthe _input_ to a `checkpoint_name`, we wouldn't print the name.\n\nThis fix is kind of a quick hack, and doesn't handle all cases. For example, if\na value is an input to two different `checkpoint_name` calls, this code\narbitrarily uses the latter name (I think). Moreover there are probably ways\nthe name doesn't get picked up even if it should. A more thorough version might\ncheck all uses, even ones that occur outside a higher-order primitive or\nsomething. But for now this change makes the output slightly nicer for some\nuseful cases.",
    "changes": [
        {
            "name": "ad_checkpoint.py",
            "path": "jax/_src/ad_checkpoint.py",
            "patches": [
                {
                    "old_start": 465,
                    "old_length": 11,
                    "new_start": 465,
                    "new_length": 14,
                    "hunk": "@@ -465,11 +465,14 @@ def _saved_residuals(jaxpr, arg_info) -> list[tuple[core.AbstractValue, str]]:\n         src = 'from the argument at flattened index {i}'\n       results.append((v.aval, src))\n \n+  named_vars = {v: e for e in jaxpr.eqns if e.primitive is name_p\n+                for v in e.invars}\n+\n   for eqn in jaxpr.eqns:\n     src = source_info_util.summarize(eqn.source_info)\n     for v in eqn.outvars:\n       if v in res_vars:\n-        if eqn.primitive is name_p:\n+        if eqn.primitive is name_p or v in named_vars and (eqn := named_vars[v]):\n           results.append((v.aval, f\"named '{eqn.params['name']}' from {src}\"))\n         elif str(eqn.primitive) == 'xla_call':\n           results.append((v.aval,\n"
                }
            ],
            "whole_deleted": "-        if eqn.primitive is name_p:\n",
            "whole_added": "+  named_vars = {v: e for e in jaxpr.eqns if e.primitive is name_p\n+                for v in e.invars}\n+\n+        if eqn.primitive is name_p or v in named_vars and (eqn := named_vars[v]):\n",
            "whole_hunk": "@@ -465,11 +465,14 @@ def _saved_residuals(jaxpr, arg_info) -> list[tuple[core.AbstractValue, str]]:\n         src = 'from the argument at flattened index {i}'\n       results.append((v.aval, src))\n \n+  named_vars = {v: e for e in jaxpr.eqns if e.primitive is name_p\n+                for v in e.invars}\n+\n   for eqn in jaxpr.eqns:\n     src = source_info_util.summarize(eqn.source_info)\n     for v in eqn.outvars:\n       if v in res_vars:\n-        if eqn.primitive is name_p:\n+        if eqn.primitive is name_p or v in named_vars and (eqn := named_vars[v]):\n           results.append((v.aval, f\"named '{eqn.params['name']}' from {src}\"))\n         elif str(eqn.primitive) == 'xla_call':\n           results.append((v.aval,\n"
        },
        {
            "name": "api_test.py",
            "path": "tests/api_test.py",
            "patches": [
                {
                    "old_start": 5568,
                    "old_length": 6,
                    "new_start": 5568,
                    "new_length": 16,
                    "hunk": "@@ -5568,6 +5568,16 @@ class RematTest(jtu.JaxTestCase):\n     res_avals = saved_residuals(f, jnp.ones((2, 2)))\n     self.assertLen(res_avals, 1)\n \n+  def test_name_saveable_input(self):\n+    @partial(jax.remat, policy=lambda p, *_, **__: 'mul' in str(p))\n+    def f(x):\n+      x = checkpoint_name(x * x, 'foo')\n+      x = x * x\n+      return x\n+\n+    res = saved_residuals(f, 3.)\n+    self.assertStartsWith(res[1][1], \"named 'foo'\")\n+\n   def test_name_denylist(self):\n     def f(x):\n       y = checkpoint_name(jnp.multiply(2., 2.), 'y')"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  def test_name_saveable_input(self):\n+    @partial(jax.remat, policy=lambda p, *_, **__: 'mul' in str(p))\n+    def f(x):\n+      x = checkpoint_name(x * x, 'foo')\n+      x = x * x\n+      return x\n+\n+    res = saved_residuals(f, 3.)\n+    self.assertStartsWith(res[1][1], \"named 'foo'\")\n+\n",
            "whole_hunk": "@@ -5568,6 +5568,16 @@ class RematTest(jtu.JaxTestCase):\n     res_avals = saved_residuals(f, jnp.ones((2, 2)))\n     self.assertLen(res_avals, 1)\n \n+  def test_name_saveable_input(self):\n+    @partial(jax.remat, policy=lambda p, *_, **__: 'mul' in str(p))\n+    def f(x):\n+      x = checkpoint_name(x * x, 'foo')\n+      x = x * x\n+      return x\n+\n+    res = saved_residuals(f, 3.)\n+    self.assertStartsWith(res[1][1], \"named 'foo'\")\n+\n   def test_name_denylist(self):\n     def f(x):\n       y = checkpoint_name(jnp.multiply(2., 2.), 'y')"
        }
    ]
},
{
    "Id": 76,
    "commit_link": "https://github.com/google/jax/commit/204ee7ff0b69ddd6a930af495026334bfca1422b",
    "date": "2024-03-14T14:34:40-07:00",
    "message": "add is_training && fix seqlen/head_dim checks",
    "changes": [
        {
            "name": "fused_attention_stablehlo.py",
            "path": "jax/_src/cudnn/fused_attention_stablehlo.py",
            "patches": [
                {
                    "old_start": 200,
                    "old_length": 52,
                    "new_start": 200,
                    "new_length": 57,
                    "hunk": "@@ -200,52 +200,57 @@ def check_qkv_layout(query, key, value):\n       \"query should have layout [batch, q_seq, num_heads, head_dim], \" \\\n       \"key and value should have layout [batch, kv_seq, num_heads, head_dim].\")\n \n-def check_is_flash_attention(query, key):\n+def check_is_flash_attention(query, key, cudnn_version, has_bias, is_training):\n   batch, q_seq_len, num_heads, head_dim = query.shape\n-  _, kv_sqe_len, _, _ = key.shape\n-  is_cross_attention = q_seq_len != kv_sqe_len\n+  _, kv_seq_len, _, _ = key.shape\n+\n   # check if attention pattern is supported by flash attention or fused attention\n-  if q_seq_len > 512 and kv_sqe_len > 512 and head_dim in [64, 128]:\n-    # check if flash attention is supported\n-    is_flash_attention = True\n-  elif q_seq_len <= 512 and kv_sqe_len <= 512 and head_dim == 64:\n+  if q_seq_len <= 512 and kv_seq_len <= 512 and head_dim == 64 \\\n+    and (not is_training or q_seq_len % 64 == 0 and kv_seq_len % 64 == 0):\n     # check if regular fused attention is supported\n+    # for training, seqlen should be divisible by 64\n     is_flash_attention = False\n+  elif head_dim <= 128 and head_dim % 8 == 0 \\\n+    and (not is_training or not has_bias or q_seq_len % 2 == 0 and kv_seq_len % 2 == 0):\n+    # check if flash attention is supported\n+    # for training, for patterns with bias, seqlen should be divisible by 2\n+    is_flash_attention = True\n   else:\n     raise NotImplementedError(\n-      f\"Unsupported sequence length Q {q_seq_len}, KV {kv_sqe_len} and head dim {head_dim}.\")\n-  return is_flash_attention, is_cross_attention\n+      f\"Unsupported sequence length Q {q_seq_len}, KV {kv_seq_len} and head dim {head_dim}.\")\n+  # check if minimum cudnn version requirement is satisfied\n+  if is_flash_attention and cudnn_version < 8904:\n+    raise RuntimeError(\"JAX requires cuDNN >= 8.9.4 to use flash cross attention.\")\n+  elif not is_flash_attention and cudnn_version < 8901:\n+    raise RuntimeError(\"JAX requires cuDNN >= 8.9.1 to use fused attention.\")\n+\n+  return is_flash_attention\n \n-def check_cudnn_version(is_flash_attention, is_cross_attention):\n-  # check if cuDNN is installed and if cuDNN version contraint is satisfied\n+def check_cudnn_version():\n+  # check if cuDNN is installed\n   if cuda_versions is None:\n     raise RuntimeError(\"cuDNN is not detected.\")\n-  elif is_flash_attention:\n-    if not is_cross_attention and cuda_versions.cudnn_get_version() < 8903:\n-      raise RuntimeError(\"JAX requires cuDNN >= 8.9.3 to use flash attention.\")\n-    if is_cross_attention and cuda_versions.cudnn_get_version() < 8904:\n-      raise RuntimeError(\"JAX requires cuDNN >= 8.9.4 to use flash cross attention.\")\n-  elif not is_flash_attention and cuda_versions.cudnn_get_version() < 8901:\n-    raise RuntimeError(\"JAX requires cuDNN >= 8.9.1 to use fused attention.\")\n+  return cuda_versions.cudnn_get_version()\n \n def _dot_product_attention_fwd(query, key, value, bias, mask,\n-  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n-  output, _ = _dot_product_attention_fwd_p_wrapper.bind(\n+  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training):\n+  outputs = _dot_product_attention_fwd_p_wrapper.bind(\n     query, key, value, bias, mask, scale=scale, seed=seed, dropout_rate=dropout_rate,\n     variadic_args=variadic_args, is_flash_attention=is_flash_attention,\n-    is_causal_mask=is_causal_mask)\n+    is_causal_mask=is_causal_mask, is_training=is_training)\n+  output = outputs[0]\n   return output\n \n def _dot_product_attention_fwd_rule(query, key, value, bias, mask,\n-  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n-  output, activation = _dot_product_attention_fwd_p_wrapper.bind(\n+  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training):\n+  outputs = _dot_product_attention_fwd_p_wrapper.bind(\n     query, key, value, bias, mask, scale=scale, seed=seed, dropout_rate=dropout_rate,\n     variadic_args=variadic_args, is_flash_attention=is_flash_attention,\n-    is_causal_mask=is_causal_mask)\n-  res = (query, key, value, bias, mask, activation, output)\n-  return output, res\n+    is_causal_mask=is_causal_mask, is_training=is_training)\n+  res = (query, key, value, bias, mask, outputs[1], outputs[0]) if is_training else None\n+  return outputs[0], res\n \n-def _dot_product_attention_bwd_rule(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, res, grad_output):\n+def _dot_product_attention_bwd_rule(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training, res, grad_output):\n   query, key, value, bias, mask, activation, fwd_output = res\n   grad_query, grad_key, grad_value = _dot_product_attention_bwd_p_wrapper.bind(\n     query, key, value, bias, mask, activation, fwd_output, grad_output,\n"
                },
                {
                    "old_start": 256,
                    "old_length": 13,
                    "new_start": 261,
                    "new_length": 13,
                    "hunk": "@@ -256,13 +261,13 @@ def _dot_product_attention_bwd_rule(scale, seed, dropout_rate, variadic_args, is\n   return grads\n \n def _dot_product_attention_fwd_impl(query, key, value, bias, mask,\n-  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n+  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training):\n   # args: {Q, K, V, mask*, bias*}\n-  output, activation = _dot_product_attention_fwd_p.bind(\n+  outputs = _dot_product_attention_fwd_p.bind(\n     query, key, value, bias, mask, scale=scale, seed=seed, dropout_rate=dropout_rate,\n     variadic_args=variadic_args, is_flash_attention=is_flash_attention,\n-    is_causal_mask=is_causal_mask)\n-  return output, activation\n+    is_causal_mask=is_causal_mask, is_training=is_training)\n+  return outputs\n \n def _dot_product_attention_bwd_impl(query, key, value, bias, mask, activation, fwd_output, grad_output,\n   scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n"
                },
                {
                    "old_start": 275,
                    "old_length": 23,
                    "new_start": 280,
                    "new_length": 34,
                    "hunk": "@@ -275,23 +280,34 @@ def _dot_product_attention_bwd_impl(query, key, value, bias, mask, activation, f\n   return grads\n \n def _dot_product_attention_fwd_abstract(query, key, value, bias, mask,\n-  *, scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n+  *, scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training):\n   query_dtype = dtypes.canonicalize_dtype(query.dtype)\n   batch, q_seq_len, num_heads, head_dim = query.shape\n   _, kv_seq_len, _, _ = key.shape\n   output_shape = (batch, q_seq_len, num_heads, head_dim)\n   activation_shape = (batch, num_heads, q_seq_len, kv_seq_len)\n   softmax_stat_shape = (batch, num_heads, q_seq_len)\n-  if q_seq_len > 512:\n+\n+  if is_flash_attention:\n     # is flash attention\n+    if is_training:\n+      return (\n+        core.ShapedArray(output_shape, query_dtype),  # output\n+        core.ShapedArray(softmax_stat_shape, jnp.float32),  # softmax_stat\n+      )\n+    else:\n+      return (\n+        core.ShapedArray(output_shape, query_dtype),  # output\n+      )\n+  if is_training:\n+    return (\n+      core.ShapedArray(output_shape, query_dtype),  # output\n+      core.ShapedArray(activation_shape, query_dtype),  # activation\n+    )\n+  else:\n     return (\n       core.ShapedArray(output_shape, query_dtype),  # output\n-      core.ShapedArray(softmax_stat_shape, jnp.float32),  # softmax_stat\n     )\n-  return (\n-    core.ShapedArray(output_shape, query_dtype),  # output\n-    core.ShapedArray(activation_shape, query_dtype),  # activation\n-  )\n \n def _dot_product_attention_bwd_abstract(query, key, value, bias, mask, activation, fwd_output, grad_output,\n   *, scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n"
                },
                {
                    "old_start": 312,
                    "old_length": 7,
                    "new_start": 328,
                    "new_length": 7,
                    "hunk": "@@ -312,7 +328,7 @@ def _dot_product_attention_bwd_abstract(query, key, value, bias, mask, activatio\n   )\n \n def _dot_product_attention_fwd_cuda_lowering(ctx, query, key, value, bias, mask,\n-  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n+  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training):\n   query_type = ir.RankedTensorType(query.type)\n   query_shape = query_type.shape\n   key_type = ir.RankedTensorType(key.type)\n"
                },
                {
                    "old_start": 344,
                    "old_length": 19,
                    "new_start": 360,
                    "new_length": 33,
                    "hunk": "@@ -344,19 +360,33 @@ def _dot_product_attention_fwd_cuda_lowering(ctx, query, key, value, bias, mask,\n   custom_call_name = get_custom_call_name(has_bias, has_mask, has_dropout, False)\n   # create output types and layouts\n   if is_flash_attention:\n-    result_types = [\n-      ir.RankedTensorType.get(output_shape, query_type.element_type),\n-      ir.RankedTensorType.get(scratch_shape, scratch_type),\n-      ir.RankedTensorType.get(softmax_stat_shape, ir.F32Type.get()),\n-    ]\n-    result_layouts = [output_layout] + default_layouts(scratch_shape, softmax_stat_shape)\n+    if is_training:\n+      result_types = [\n+        ir.RankedTensorType.get(output_shape, query_type.element_type),\n+        ir.RankedTensorType.get(scratch_shape, scratch_type),\n+        ir.RankedTensorType.get(softmax_stat_shape, ir.F32Type.get()),\n+      ]\n+      result_layouts = [output_layout] + default_layouts(scratch_shape, softmax_stat_shape)\n+    else:\n+      result_types = [\n+        ir.RankedTensorType.get(output_shape, query_type.element_type),\n+        ir.RankedTensorType.get(scratch_shape, scratch_type)\n+      ]\n+      result_layouts = [output_layout] + default_layouts(scratch_shape)\n   else:\n-    result_types = [\n-      ir.RankedTensorType.get(output_shape, query_type.element_type),\n-      ir.RankedTensorType.get(scratch_shape, scratch_type),\n-      ir.RankedTensorType.get(activation_shape, query_type.element_type),\n-    ]\n-    result_layouts = [output_layout] + default_layouts(scratch_shape, activation_shape)\n+    if is_training:\n+      result_types = [\n+        ir.RankedTensorType.get(output_shape, query_type.element_type),\n+        ir.RankedTensorType.get(scratch_shape, scratch_type),\n+        ir.RankedTensorType.get(activation_shape, query_type.element_type),\n+      ]\n+      result_layouts = [output_layout] + default_layouts(scratch_shape, activation_shape)\n+    else:\n+      result_types = [\n+        ir.RankedTensorType.get(output_shape, query_type.element_type),\n+        ir.RankedTensorType.get(scratch_shape, scratch_type),\n+      ]\n+      result_layouts = [output_layout] + default_layouts(scratch_shape)\n   # create custom call here\n   out = mlir.custom_call(\n     custom_call_name,\n"
                },
                {
                    "old_start": 368,
                    "old_length": 7,
                    "new_start": 398,
                    "new_length": 10,
                    "hunk": "@@ -368,7 +398,10 @@ def _dot_product_attention_fwd_cuda_lowering(ctx, query, key, value, bias, mask,\n   )\n   # drop scratch memory\n   # output should be (batch, q_seq_len, num_heads, head_dim) instead of (batch, num_heads, q_seq_len, head_dim)\n-  return [hlo.transpose(out.results[0], output_transpose_perm), out.results[2]]\n+  if is_training:\n+    return [hlo.transpose(out.results[0], output_transpose_perm), out.results[2]]\n+  else:\n+    return [hlo.transpose(out.results[0], output_transpose_perm)]\n \n def _dot_product_attention_bwd_cuda_lowering(ctx, query, key, value, bias, mask, activation, fwd_output, grad_output,\n   scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n"
                },
                {
                    "old_start": 451,
                    "old_length": 11,
                    "new_start": 484,
                    "new_length": 14,
                    "hunk": "@@ -451,11 +484,14 @@ def _check_valid_batch_dims(bdims):\n       raise NotImplementedError(\"Currently only support batch_dim in [0, None], \" \\\n       f\"but got {dim=}\")\n \n-def _dot_product_attention_fwd_batcher(batched_args, batch_dims, *, scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n+def _dot_product_attention_fwd_batcher(batched_args, batch_dims, *, scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training):\n   _check_valid_batch_dims(batch_dims)\n   query, key, value, bias, mask = batched_args\n   query_bdim = batch_dims[0]\n-  out_bdims = query_bdim, query_bdim\n+  if is_training:\n+    out_bdims = query_bdim, query_bdim\n+  else:\n+    out_bdims = (query_bdim,)\n \n   *batch_tuple, q_seq_len, num_heads, head_dim = query.shape\n   *_, kv_seq_len, _, _ = key.shape\n"
                },
                {
                    "old_start": 470,
                    "old_length": 19,
                    "new_start": 506,
                    "new_length": 24,
                    "hunk": "@@ -470,19 +506,24 @@ def _dot_product_attention_fwd_batcher(batched_args, batch_dims, *, scale, seed,\n   if has_mask:\n     mask = jnp.reshape(mask, (new_batch, num_heads, q_seq_len, kv_seq_len))\n \n-  output, activation = _dot_product_attention_fwd_p_wrapper.bind(\n+  outputs = _dot_product_attention_fwd_p_wrapper.bind(\n     query, key, value, bias, mask,\n     scale=scale, seed=seed, dropout_rate=dropout_rate,\n     variadic_args=variadic_args, is_flash_attention=is_flash_attention,\n-    is_causal_mask=is_causal_mask)\n+    is_causal_mask=is_causal_mask, is_training=is_training)\n \n   # reshape to original shape\n+  output = outputs[0]\n   output = jnp.reshape(output, (*batch_tuple, q_seq_len, num_heads, head_dim))\n-  if is_flash_attention:\n-    activation = jnp.reshape(activation, (*batch_tuple, num_heads, q_seq_len))\n+  if is_training:\n+    activation = outputs[1]\n+    if is_flash_attention:\n+      activation = jnp.reshape(activation, (*batch_tuple, num_heads, q_seq_len))\n+    else:\n+      activation = jnp.reshape(activation, (*batch_tuple, num_heads, q_seq_len, kv_seq_len))\n+    return (output, activation), out_bdims\n   else:\n-    activation = jnp.reshape(activation, (*batch_tuple, num_heads, q_seq_len, kv_seq_len))\n-  return (output, activation), out_bdims\n+    return (output,), out_bdims\n \n def _dot_product_attention_bwd_batcher(batched_args, batch_dims, *, scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n   _check_valid_batch_dims(batch_dims)\n"
                },
                {
                    "old_start": 556,
                    "old_length": 7,
                    "new_start": 597,
                    "new_length": 7,
                    "hunk": "@@ -556,7 +597,7 @@ def _check_qkv_bias_mask_spec(query_spec, key_spec, value_spec, bias_spec, mask_\n       raise ValueError(\"Sharding on mask sequence dim is not allowed.\")\n \n # fwd custom partition\n-def _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args):\n+def _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args, is_training):\n   # only sharding on batch and num_head dim is allowed\n   # (*batch, q_seq, num_head, head)\n   query_spec = _get_padded_spec(arg_shapes[0])\n"
                },
                {
                    "old_start": 569,
                    "old_length": 21,
                    "new_start": 610,
                    "new_length": 24,
                    "hunk": "@@ -569,21 +610,24 @@ def _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args):\n   _check_qkv_bias_mask_spec(query_spec, key_spec, value_spec, bias_spec, mask_spec)\n   # keep out sharding same as query sharding since they have same shape\n   out_sharding = NamedSharding(mesh, PartitionSpec(*query_spec))\n-  # activation sharding\n-  *batch_spec, q_seq_spec, num_head_spec, head_spec = query_spec\n-  activation_sharding = NamedSharding(mesh, PartitionSpec(*batch_spec, num_head_spec, q_seq_spec, None))\n-  return (out_sharding, activation_sharding)\n-\n-_dot_product_attention_fwd_lower = custom_partitioning(_dot_product_attention_fwd_impl, static_argnums=(5,6,7,8,9,10))\n-def _dot_product_attention_fwd_infer_sharding_from_operands(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, mesh, arg_shapes, result_shape):\n-  return _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args)\n-\n-def _dot_product_attention_fwd_partition(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, mesh, arg_shapes, result_shape):\n+  if is_training:\n+    # activation sharding\n+    *batch_spec, q_seq_spec, num_head_spec, head_spec = query_spec\n+    activation_sharding = NamedSharding(mesh, PartitionSpec(*batch_spec, num_head_spec, q_seq_spec, None))\n+    return [out_sharding, activation_sharding]\n+  return [out_sharding]\n+\n+_dot_product_attention_fwd_lower = custom_partitioning(_dot_product_attention_fwd_impl, static_argnums=(5,6,7,8,9,10,11))\n+def _dot_product_attention_fwd_infer_sharding_from_operands(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training, mesh, arg_shapes, result_shape):\n+  return _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args, is_training)\n+\n+def _dot_product_attention_fwd_partition(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training, mesh, arg_shapes, result_shape):\n   # args sharding\n   arg_shardings = tuple([arg_i.sharding for arg_i in arg_shapes])\n-  out_shardings = _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args)\n+  out_shardings = _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args, is_training)\n   impl = partial(_dot_product_attention_fwd_impl, scale=scale, seed=seed, dropout_rate=dropout_rate,\n-                variadic_args=variadic_args, is_flash_attention=is_flash_attention, is_causal_mask=is_causal_mask)\n+                variadic_args=variadic_args, is_flash_attention=is_flash_attention, is_causal_mask=is_causal_mask,\n+                is_training=is_training)\n   return mesh, impl, out_shardings, arg_shardings\n \n # bwd custom partition\n"
                },
                {
                    "old_start": 673,
                    "old_length": 7,
                    "new_start": 717,
                    "new_length": 7,
                    "hunk": "@@ -673,7 +717,7 @@ dispatch.prim_requires_devices_during_lowering.add(_dot_product_attention_fwd_p_\n dispatch.prim_requires_devices_during_lowering.add(_dot_product_attention_bwd_p)\n dispatch.prim_requires_devices_during_lowering.add(_dot_product_attention_bwd_p_wrapper)\n \n-@partial(jax.custom_vjp, nondiff_argnums=(5, 6, 7, 8, 9, 10))\n+@partial(jax.custom_vjp, nondiff_argnums=(5, 6, 7, 8, 9, 10, 11))\n def _dot_product_attention(query: Array,\n                             key: Array,\n                             value: Array,\n"
                },
                {
                    "old_start": 684,
                    "old_length": 11,
                    "new_start": 728,
                    "new_length": 13,
                    "hunk": "@@ -684,11 +728,13 @@ def _dot_product_attention(query: Array,\n                             dropout_rate: float,\n                             variadic_args: tuple[bool, ...],\n                             is_flash_attention: bool,\n-                            is_causal_mask: bool):\n+                            is_causal_mask: bool,\n+                            is_training: bool):\n   output = _dot_product_attention_fwd(\n     query, key, value, bias, mask,\n     scale=scale, seed=seed, dropout_rate=dropout_rate, variadic_args=variadic_args,\n-    is_flash_attention=is_flash_attention, is_causal_mask=is_causal_mask)\n+    is_flash_attention=is_flash_attention, is_causal_mask=is_causal_mask,\n+    is_training=is_training)\n   return output\n \n # _dot_product_attention_fwd must have the same func signature as _dot_product_attention\n"
                },
                {
                    "old_start": 704,
                    "old_length": 7,
                    "new_start": 750,
                    "new_length": 8,
                    "hunk": "@@ -704,7 +750,8 @@ def dot_product_attention(query: Array,\n                           scale: float = 1.0,\n                           is_causal_mask: bool = False,\n                           seed: int = 42,\n-                          dropout_rate: float = 0.):\n+                          dropout_rate: float = 0.,\n+                          is_training = False):\n   \"\"\"Computes dot-product attention given query, key, and value.\n   This is the core function for applying attention based on\n   https://arxiv.org/abs/1706.03762. It calculates the attention weights given\n"
                },
                {
                    "old_start": 725,
                    "old_length": 16,
                    "new_start": 772,
                    "new_length": 19,
                    "hunk": "@@ -725,16 +772,19 @@ def dot_product_attention(query: Array,\n     mask: mask used mask out logits with shape of `[batch, num_heads,\n     q_length, kv_length]`.\n     scale: scale for the query.\n+    is_causal_mask: choose to apply a causal mask or not.\n+    seed: used for dropout mask generation.\n     dropout_rate: dropout rate.\n+    is_training: choose to save activation or not.\n   Returns:\n     Output of shape `[batch, q_length, num_heads, v_depth_per_head]`.\n   \"\"\"\n-  # check if query, key and value layout meets cuDNN layout requirement\n+  # check if cuDNN is installed\n+  cudnn_version = check_cudnn_version()\n+  # check query, key and value shape and data type\n   check_qkv_layout(query, key, value)\n   # check if flash attention is supported for this attention pattern\n-  is_flash_attention, is_cross_attention = check_is_flash_attention(query, key)\n-  # check if cuDNN is installed and if cuDNN version is sufficient\n-  check_cudnn_version(is_flash_attention, is_cross_attention)\n+  is_flash_attention = check_is_flash_attention(query, key, cudnn_version, bias is not None, is_training)\n   if mask is not None and is_causal_mask:\n     raise ValueError(\"can not apply a mask and generate a causal_mask at the same time.\")\n   if not is_flash_attention and is_causal_mask:\n"
                },
                {
                    "old_start": 747,
                    "old_length": 5,
                    "new_start": 797,
                    "new_length": 5,
                    "hunk": "@@ -747,5 +797,5 @@ def dot_product_attention(query: Array,\n   output = _dot_product_attention(\n     query, key, value, bias, mask,\n     scale, seed, dropout_rate, variadic_args,\n-    is_flash_attention, is_causal_mask)\n+    is_flash_attention, is_causal_mask, is_training)\n   return output\n"
                }
            ],
            "whole_deleted": "-def check_is_flash_attention(query, key):\n-  _, kv_sqe_len, _, _ = key.shape\n-  is_cross_attention = q_seq_len != kv_sqe_len\n-  if q_seq_len > 512 and kv_sqe_len > 512 and head_dim in [64, 128]:\n-    # check if flash attention is supported\n-    is_flash_attention = True\n-  elif q_seq_len <= 512 and kv_sqe_len <= 512 and head_dim == 64:\n-      f\"Unsupported sequence length Q {q_seq_len}, KV {kv_sqe_len} and head dim {head_dim}.\")\n-  return is_flash_attention, is_cross_attention\n-def check_cudnn_version(is_flash_attention, is_cross_attention):\n-  # check if cuDNN is installed and if cuDNN version contraint is satisfied\n-  elif is_flash_attention:\n-    if not is_cross_attention and cuda_versions.cudnn_get_version() < 8903:\n-      raise RuntimeError(\"JAX requires cuDNN >= 8.9.3 to use flash attention.\")\n-    if is_cross_attention and cuda_versions.cudnn_get_version() < 8904:\n-      raise RuntimeError(\"JAX requires cuDNN >= 8.9.4 to use flash cross attention.\")\n-  elif not is_flash_attention and cuda_versions.cudnn_get_version() < 8901:\n-    raise RuntimeError(\"JAX requires cuDNN >= 8.9.1 to use fused attention.\")\n-  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n-  output, _ = _dot_product_attention_fwd_p_wrapper.bind(\n-    is_causal_mask=is_causal_mask)\n-  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n-  output, activation = _dot_product_attention_fwd_p_wrapper.bind(\n-    is_causal_mask=is_causal_mask)\n-  res = (query, key, value, bias, mask, activation, output)\n-  return output, res\n-def _dot_product_attention_bwd_rule(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, res, grad_output):\n-  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n-  output, activation = _dot_product_attention_fwd_p.bind(\n-    is_causal_mask=is_causal_mask)\n-  return output, activation\n-  *, scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n-  if q_seq_len > 512:\n-      core.ShapedArray(softmax_stat_shape, jnp.float32),  # softmax_stat\n-  return (\n-    core.ShapedArray(output_shape, query_dtype),  # output\n-    core.ShapedArray(activation_shape, query_dtype),  # activation\n-  )\n-  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n-    result_types = [\n-      ir.RankedTensorType.get(output_shape, query_type.element_type),\n-      ir.RankedTensorType.get(scratch_shape, scratch_type),\n-      ir.RankedTensorType.get(softmax_stat_shape, ir.F32Type.get()),\n-    ]\n-    result_layouts = [output_layout] + default_layouts(scratch_shape, softmax_stat_shape)\n-    result_types = [\n-      ir.RankedTensorType.get(output_shape, query_type.element_type),\n-      ir.RankedTensorType.get(scratch_shape, scratch_type),\n-      ir.RankedTensorType.get(activation_shape, query_type.element_type),\n-    ]\n-    result_layouts = [output_layout] + default_layouts(scratch_shape, activation_shape)\n-  return [hlo.transpose(out.results[0], output_transpose_perm), out.results[2]]\n-def _dot_product_attention_fwd_batcher(batched_args, batch_dims, *, scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n-  out_bdims = query_bdim, query_bdim\n-  output, activation = _dot_product_attention_fwd_p_wrapper.bind(\n-    is_causal_mask=is_causal_mask)\n-  if is_flash_attention:\n-    activation = jnp.reshape(activation, (*batch_tuple, num_heads, q_seq_len))\n-    activation = jnp.reshape(activation, (*batch_tuple, num_heads, q_seq_len, kv_seq_len))\n-  return (output, activation), out_bdims\n-def _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args):\n-  # activation sharding\n-  *batch_spec, q_seq_spec, num_head_spec, head_spec = query_spec\n-  activation_sharding = NamedSharding(mesh, PartitionSpec(*batch_spec, num_head_spec, q_seq_spec, None))\n-  return (out_sharding, activation_sharding)\n-\n-_dot_product_attention_fwd_lower = custom_partitioning(_dot_product_attention_fwd_impl, static_argnums=(5,6,7,8,9,10))\n-def _dot_product_attention_fwd_infer_sharding_from_operands(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, mesh, arg_shapes, result_shape):\n-  return _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args)\n-\n-def _dot_product_attention_fwd_partition(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, mesh, arg_shapes, result_shape):\n-  out_shardings = _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args)\n-                variadic_args=variadic_args, is_flash_attention=is_flash_attention, is_causal_mask=is_causal_mask)\n-@partial(jax.custom_vjp, nondiff_argnums=(5, 6, 7, 8, 9, 10))\n-                            is_causal_mask: bool):\n-    is_flash_attention=is_flash_attention, is_causal_mask=is_causal_mask)\n-                          dropout_rate: float = 0.):\n-  # check if query, key and value layout meets cuDNN layout requirement\n-  is_flash_attention, is_cross_attention = check_is_flash_attention(query, key)\n-  # check if cuDNN is installed and if cuDNN version is sufficient\n-  check_cudnn_version(is_flash_attention, is_cross_attention)\n-    is_flash_attention, is_causal_mask)\n",
            "whole_added": "+def check_is_flash_attention(query, key, cudnn_version, has_bias, is_training):\n+  _, kv_seq_len, _, _ = key.shape\n+\n+  if q_seq_len <= 512 and kv_seq_len <= 512 and head_dim == 64 \\\n+    and (not is_training or q_seq_len % 64 == 0 and kv_seq_len % 64 == 0):\n+    # for training, seqlen should be divisible by 64\n+  elif head_dim <= 128 and head_dim % 8 == 0 \\\n+    and (not is_training or not has_bias or q_seq_len % 2 == 0 and kv_seq_len % 2 == 0):\n+    # check if flash attention is supported\n+    # for training, for patterns with bias, seqlen should be divisible by 2\n+    is_flash_attention = True\n+      f\"Unsupported sequence length Q {q_seq_len}, KV {kv_seq_len} and head dim {head_dim}.\")\n+  # check if minimum cudnn version requirement is satisfied\n+  if is_flash_attention and cudnn_version < 8904:\n+    raise RuntimeError(\"JAX requires cuDNN >= 8.9.4 to use flash cross attention.\")\n+  elif not is_flash_attention and cudnn_version < 8901:\n+    raise RuntimeError(\"JAX requires cuDNN >= 8.9.1 to use fused attention.\")\n+\n+  return is_flash_attention\n+def check_cudnn_version():\n+  # check if cuDNN is installed\n+  return cuda_versions.cudnn_get_version()\n+  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training):\n+  outputs = _dot_product_attention_fwd_p_wrapper.bind(\n+    is_causal_mask=is_causal_mask, is_training=is_training)\n+  output = outputs[0]\n+  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training):\n+  outputs = _dot_product_attention_fwd_p_wrapper.bind(\n+    is_causal_mask=is_causal_mask, is_training=is_training)\n+  res = (query, key, value, bias, mask, outputs[1], outputs[0]) if is_training else None\n+  return outputs[0], res\n+def _dot_product_attention_bwd_rule(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training, res, grad_output):\n+  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training):\n+  outputs = _dot_product_attention_fwd_p.bind(\n+    is_causal_mask=is_causal_mask, is_training=is_training)\n+  return outputs\n+  *, scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training):\n+\n+  if is_flash_attention:\n+    if is_training:\n+      return (\n+        core.ShapedArray(output_shape, query_dtype),  # output\n+        core.ShapedArray(softmax_stat_shape, jnp.float32),  # softmax_stat\n+      )\n+    else:\n+      return (\n+        core.ShapedArray(output_shape, query_dtype),  # output\n+      )\n+  if is_training:\n+    return (\n+      core.ShapedArray(output_shape, query_dtype),  # output\n+      core.ShapedArray(activation_shape, query_dtype),  # activation\n+    )\n+  else:\n+  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training):\n+    if is_training:\n+      result_types = [\n+        ir.RankedTensorType.get(output_shape, query_type.element_type),\n+        ir.RankedTensorType.get(scratch_shape, scratch_type),\n+        ir.RankedTensorType.get(softmax_stat_shape, ir.F32Type.get()),\n+      ]\n+      result_layouts = [output_layout] + default_layouts(scratch_shape, softmax_stat_shape)\n+    else:\n+      result_types = [\n+        ir.RankedTensorType.get(output_shape, query_type.element_type),\n+        ir.RankedTensorType.get(scratch_shape, scratch_type)\n+      ]\n+      result_layouts = [output_layout] + default_layouts(scratch_shape)\n+    if is_training:\n+      result_types = [\n+        ir.RankedTensorType.get(output_shape, query_type.element_type),\n+        ir.RankedTensorType.get(scratch_shape, scratch_type),\n+        ir.RankedTensorType.get(activation_shape, query_type.element_type),\n+      ]\n+      result_layouts = [output_layout] + default_layouts(scratch_shape, activation_shape)\n+    else:\n+      result_types = [\n+        ir.RankedTensorType.get(output_shape, query_type.element_type),\n+        ir.RankedTensorType.get(scratch_shape, scratch_type),\n+      ]\n+      result_layouts = [output_layout] + default_layouts(scratch_shape)\n+  if is_training:\n+    return [hlo.transpose(out.results[0], output_transpose_perm), out.results[2]]\n+  else:\n+    return [hlo.transpose(out.results[0], output_transpose_perm)]\n+def _dot_product_attention_fwd_batcher(batched_args, batch_dims, *, scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training):\n+  if is_training:\n+    out_bdims = query_bdim, query_bdim\n+  else:\n+    out_bdims = (query_bdim,)\n+  outputs = _dot_product_attention_fwd_p_wrapper.bind(\n+    is_causal_mask=is_causal_mask, is_training=is_training)\n+  output = outputs[0]\n+  if is_training:\n+    activation = outputs[1]\n+    if is_flash_attention:\n+      activation = jnp.reshape(activation, (*batch_tuple, num_heads, q_seq_len))\n+    else:\n+      activation = jnp.reshape(activation, (*batch_tuple, num_heads, q_seq_len, kv_seq_len))\n+    return (output, activation), out_bdims\n+    return (output,), out_bdims\n+def _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args, is_training):\n+  if is_training:\n+    # activation sharding\n+    *batch_spec, q_seq_spec, num_head_spec, head_spec = query_spec\n+    activation_sharding = NamedSharding(mesh, PartitionSpec(*batch_spec, num_head_spec, q_seq_spec, None))\n+    return [out_sharding, activation_sharding]\n+  return [out_sharding]\n+\n+_dot_product_attention_fwd_lower = custom_partitioning(_dot_product_attention_fwd_impl, static_argnums=(5,6,7,8,9,10,11))\n+def _dot_product_attention_fwd_infer_sharding_from_operands(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training, mesh, arg_shapes, result_shape):\n+  return _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args, is_training)\n+\n+def _dot_product_attention_fwd_partition(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training, mesh, arg_shapes, result_shape):\n+  out_shardings = _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args, is_training)\n+                variadic_args=variadic_args, is_flash_attention=is_flash_attention, is_causal_mask=is_causal_mask,\n+                is_training=is_training)\n+@partial(jax.custom_vjp, nondiff_argnums=(5, 6, 7, 8, 9, 10, 11))\n+                            is_causal_mask: bool,\n+                            is_training: bool):\n+    is_flash_attention=is_flash_attention, is_causal_mask=is_causal_mask,\n+    is_training=is_training)\n+                          dropout_rate: float = 0.,\n+                          is_training = False):\n+    is_causal_mask: choose to apply a causal mask or not.\n+    seed: used for dropout mask generation.\n+    is_training: choose to save activation or not.\n+  # check if cuDNN is installed\n+  cudnn_version = check_cudnn_version()\n+  # check query, key and value shape and data type\n+  is_flash_attention = check_is_flash_attention(query, key, cudnn_version, bias is not None, is_training)\n+    is_flash_attention, is_causal_mask, is_training)\n",
            "whole_hunk": "@@ -200,52 +200,57 @@ def check_qkv_layout(query, key, value):\n       \"query should have layout [batch, q_seq, num_heads, head_dim], \" \\\n       \"key and value should have layout [batch, kv_seq, num_heads, head_dim].\")\n \n-def check_is_flash_attention(query, key):\n+def check_is_flash_attention(query, key, cudnn_version, has_bias, is_training):\n   batch, q_seq_len, num_heads, head_dim = query.shape\n-  _, kv_sqe_len, _, _ = key.shape\n-  is_cross_attention = q_seq_len != kv_sqe_len\n+  _, kv_seq_len, _, _ = key.shape\n+\n   # check if attention pattern is supported by flash attention or fused attention\n-  if q_seq_len > 512 and kv_sqe_len > 512 and head_dim in [64, 128]:\n-    # check if flash attention is supported\n-    is_flash_attention = True\n-  elif q_seq_len <= 512 and kv_sqe_len <= 512 and head_dim == 64:\n+  if q_seq_len <= 512 and kv_seq_len <= 512 and head_dim == 64 \\\n+    and (not is_training or q_seq_len % 64 == 0 and kv_seq_len % 64 == 0):\n     # check if regular fused attention is supported\n+    # for training, seqlen should be divisible by 64\n     is_flash_attention = False\n+  elif head_dim <= 128 and head_dim % 8 == 0 \\\n+    and (not is_training or not has_bias or q_seq_len % 2 == 0 and kv_seq_len % 2 == 0):\n+    # check if flash attention is supported\n+    # for training, for patterns with bias, seqlen should be divisible by 2\n+    is_flash_attention = True\n   else:\n     raise NotImplementedError(\n-      f\"Unsupported sequence length Q {q_seq_len}, KV {kv_sqe_len} and head dim {head_dim}.\")\n-  return is_flash_attention, is_cross_attention\n+      f\"Unsupported sequence length Q {q_seq_len}, KV {kv_seq_len} and head dim {head_dim}.\")\n+  # check if minimum cudnn version requirement is satisfied\n+  if is_flash_attention and cudnn_version < 8904:\n+    raise RuntimeError(\"JAX requires cuDNN >= 8.9.4 to use flash cross attention.\")\n+  elif not is_flash_attention and cudnn_version < 8901:\n+    raise RuntimeError(\"JAX requires cuDNN >= 8.9.1 to use fused attention.\")\n+\n+  return is_flash_attention\n \n-def check_cudnn_version(is_flash_attention, is_cross_attention):\n-  # check if cuDNN is installed and if cuDNN version contraint is satisfied\n+def check_cudnn_version():\n+  # check if cuDNN is installed\n   if cuda_versions is None:\n     raise RuntimeError(\"cuDNN is not detected.\")\n-  elif is_flash_attention:\n-    if not is_cross_attention and cuda_versions.cudnn_get_version() < 8903:\n-      raise RuntimeError(\"JAX requires cuDNN >= 8.9.3 to use flash attention.\")\n-    if is_cross_attention and cuda_versions.cudnn_get_version() < 8904:\n-      raise RuntimeError(\"JAX requires cuDNN >= 8.9.4 to use flash cross attention.\")\n-  elif not is_flash_attention and cuda_versions.cudnn_get_version() < 8901:\n-    raise RuntimeError(\"JAX requires cuDNN >= 8.9.1 to use fused attention.\")\n+  return cuda_versions.cudnn_get_version()\n \n def _dot_product_attention_fwd(query, key, value, bias, mask,\n-  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n-  output, _ = _dot_product_attention_fwd_p_wrapper.bind(\n+  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training):\n+  outputs = _dot_product_attention_fwd_p_wrapper.bind(\n     query, key, value, bias, mask, scale=scale, seed=seed, dropout_rate=dropout_rate,\n     variadic_args=variadic_args, is_flash_attention=is_flash_attention,\n-    is_causal_mask=is_causal_mask)\n+    is_causal_mask=is_causal_mask, is_training=is_training)\n+  output = outputs[0]\n   return output\n \n def _dot_product_attention_fwd_rule(query, key, value, bias, mask,\n-  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n-  output, activation = _dot_product_attention_fwd_p_wrapper.bind(\n+  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training):\n+  outputs = _dot_product_attention_fwd_p_wrapper.bind(\n     query, key, value, bias, mask, scale=scale, seed=seed, dropout_rate=dropout_rate,\n     variadic_args=variadic_args, is_flash_attention=is_flash_attention,\n-    is_causal_mask=is_causal_mask)\n-  res = (query, key, value, bias, mask, activation, output)\n-  return output, res\n+    is_causal_mask=is_causal_mask, is_training=is_training)\n+  res = (query, key, value, bias, mask, outputs[1], outputs[0]) if is_training else None\n+  return outputs[0], res\n \n-def _dot_product_attention_bwd_rule(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, res, grad_output):\n+def _dot_product_attention_bwd_rule(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training, res, grad_output):\n   query, key, value, bias, mask, activation, fwd_output = res\n   grad_query, grad_key, grad_value = _dot_product_attention_bwd_p_wrapper.bind(\n     query, key, value, bias, mask, activation, fwd_output, grad_output,\n@@ -256,13 +261,13 @@ def _dot_product_attention_bwd_rule(scale, seed, dropout_rate, variadic_args, is\n   return grads\n \n def _dot_product_attention_fwd_impl(query, key, value, bias, mask,\n-  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n+  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training):\n   # args: {Q, K, V, mask*, bias*}\n-  output, activation = _dot_product_attention_fwd_p.bind(\n+  outputs = _dot_product_attention_fwd_p.bind(\n     query, key, value, bias, mask, scale=scale, seed=seed, dropout_rate=dropout_rate,\n     variadic_args=variadic_args, is_flash_attention=is_flash_attention,\n-    is_causal_mask=is_causal_mask)\n-  return output, activation\n+    is_causal_mask=is_causal_mask, is_training=is_training)\n+  return outputs\n \n def _dot_product_attention_bwd_impl(query, key, value, bias, mask, activation, fwd_output, grad_output,\n   scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n@@ -275,23 +280,34 @@ def _dot_product_attention_bwd_impl(query, key, value, bias, mask, activation, f\n   return grads\n \n def _dot_product_attention_fwd_abstract(query, key, value, bias, mask,\n-  *, scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n+  *, scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training):\n   query_dtype = dtypes.canonicalize_dtype(query.dtype)\n   batch, q_seq_len, num_heads, head_dim = query.shape\n   _, kv_seq_len, _, _ = key.shape\n   output_shape = (batch, q_seq_len, num_heads, head_dim)\n   activation_shape = (batch, num_heads, q_seq_len, kv_seq_len)\n   softmax_stat_shape = (batch, num_heads, q_seq_len)\n-  if q_seq_len > 512:\n+\n+  if is_flash_attention:\n     # is flash attention\n+    if is_training:\n+      return (\n+        core.ShapedArray(output_shape, query_dtype),  # output\n+        core.ShapedArray(softmax_stat_shape, jnp.float32),  # softmax_stat\n+      )\n+    else:\n+      return (\n+        core.ShapedArray(output_shape, query_dtype),  # output\n+      )\n+  if is_training:\n+    return (\n+      core.ShapedArray(output_shape, query_dtype),  # output\n+      core.ShapedArray(activation_shape, query_dtype),  # activation\n+    )\n+  else:\n     return (\n       core.ShapedArray(output_shape, query_dtype),  # output\n-      core.ShapedArray(softmax_stat_shape, jnp.float32),  # softmax_stat\n     )\n-  return (\n-    core.ShapedArray(output_shape, query_dtype),  # output\n-    core.ShapedArray(activation_shape, query_dtype),  # activation\n-  )\n \n def _dot_product_attention_bwd_abstract(query, key, value, bias, mask, activation, fwd_output, grad_output,\n   *, scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n@@ -312,7 +328,7 @@ def _dot_product_attention_bwd_abstract(query, key, value, bias, mask, activatio\n   )\n \n def _dot_product_attention_fwd_cuda_lowering(ctx, query, key, value, bias, mask,\n-  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n+  scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training):\n   query_type = ir.RankedTensorType(query.type)\n   query_shape = query_type.shape\n   key_type = ir.RankedTensorType(key.type)\n@@ -344,19 +360,33 @@ def _dot_product_attention_fwd_cuda_lowering(ctx, query, key, value, bias, mask,\n   custom_call_name = get_custom_call_name(has_bias, has_mask, has_dropout, False)\n   # create output types and layouts\n   if is_flash_attention:\n-    result_types = [\n-      ir.RankedTensorType.get(output_shape, query_type.element_type),\n-      ir.RankedTensorType.get(scratch_shape, scratch_type),\n-      ir.RankedTensorType.get(softmax_stat_shape, ir.F32Type.get()),\n-    ]\n-    result_layouts = [output_layout] + default_layouts(scratch_shape, softmax_stat_shape)\n+    if is_training:\n+      result_types = [\n+        ir.RankedTensorType.get(output_shape, query_type.element_type),\n+        ir.RankedTensorType.get(scratch_shape, scratch_type),\n+        ir.RankedTensorType.get(softmax_stat_shape, ir.F32Type.get()),\n+      ]\n+      result_layouts = [output_layout] + default_layouts(scratch_shape, softmax_stat_shape)\n+    else:\n+      result_types = [\n+        ir.RankedTensorType.get(output_shape, query_type.element_type),\n+        ir.RankedTensorType.get(scratch_shape, scratch_type)\n+      ]\n+      result_layouts = [output_layout] + default_layouts(scratch_shape)\n   else:\n-    result_types = [\n-      ir.RankedTensorType.get(output_shape, query_type.element_type),\n-      ir.RankedTensorType.get(scratch_shape, scratch_type),\n-      ir.RankedTensorType.get(activation_shape, query_type.element_type),\n-    ]\n-    result_layouts = [output_layout] + default_layouts(scratch_shape, activation_shape)\n+    if is_training:\n+      result_types = [\n+        ir.RankedTensorType.get(output_shape, query_type.element_type),\n+        ir.RankedTensorType.get(scratch_shape, scratch_type),\n+        ir.RankedTensorType.get(activation_shape, query_type.element_type),\n+      ]\n+      result_layouts = [output_layout] + default_layouts(scratch_shape, activation_shape)\n+    else:\n+      result_types = [\n+        ir.RankedTensorType.get(output_shape, query_type.element_type),\n+        ir.RankedTensorType.get(scratch_shape, scratch_type),\n+      ]\n+      result_layouts = [output_layout] + default_layouts(scratch_shape)\n   # create custom call here\n   out = mlir.custom_call(\n     custom_call_name,\n@@ -368,7 +398,10 @@ def _dot_product_attention_fwd_cuda_lowering(ctx, query, key, value, bias, mask,\n   )\n   # drop scratch memory\n   # output should be (batch, q_seq_len, num_heads, head_dim) instead of (batch, num_heads, q_seq_len, head_dim)\n-  return [hlo.transpose(out.results[0], output_transpose_perm), out.results[2]]\n+  if is_training:\n+    return [hlo.transpose(out.results[0], output_transpose_perm), out.results[2]]\n+  else:\n+    return [hlo.transpose(out.results[0], output_transpose_perm)]\n \n def _dot_product_attention_bwd_cuda_lowering(ctx, query, key, value, bias, mask, activation, fwd_output, grad_output,\n   scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n@@ -451,11 +484,14 @@ def _check_valid_batch_dims(bdims):\n       raise NotImplementedError(\"Currently only support batch_dim in [0, None], \" \\\n       f\"but got {dim=}\")\n \n-def _dot_product_attention_fwd_batcher(batched_args, batch_dims, *, scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n+def _dot_product_attention_fwd_batcher(batched_args, batch_dims, *, scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training):\n   _check_valid_batch_dims(batch_dims)\n   query, key, value, bias, mask = batched_args\n   query_bdim = batch_dims[0]\n-  out_bdims = query_bdim, query_bdim\n+  if is_training:\n+    out_bdims = query_bdim, query_bdim\n+  else:\n+    out_bdims = (query_bdim,)\n \n   *batch_tuple, q_seq_len, num_heads, head_dim = query.shape\n   *_, kv_seq_len, _, _ = key.shape\n@@ -470,19 +506,24 @@ def _dot_product_attention_fwd_batcher(batched_args, batch_dims, *, scale, seed,\n   if has_mask:\n     mask = jnp.reshape(mask, (new_batch, num_heads, q_seq_len, kv_seq_len))\n \n-  output, activation = _dot_product_attention_fwd_p_wrapper.bind(\n+  outputs = _dot_product_attention_fwd_p_wrapper.bind(\n     query, key, value, bias, mask,\n     scale=scale, seed=seed, dropout_rate=dropout_rate,\n     variadic_args=variadic_args, is_flash_attention=is_flash_attention,\n-    is_causal_mask=is_causal_mask)\n+    is_causal_mask=is_causal_mask, is_training=is_training)\n \n   # reshape to original shape\n+  output = outputs[0]\n   output = jnp.reshape(output, (*batch_tuple, q_seq_len, num_heads, head_dim))\n-  if is_flash_attention:\n-    activation = jnp.reshape(activation, (*batch_tuple, num_heads, q_seq_len))\n+  if is_training:\n+    activation = outputs[1]\n+    if is_flash_attention:\n+      activation = jnp.reshape(activation, (*batch_tuple, num_heads, q_seq_len))\n+    else:\n+      activation = jnp.reshape(activation, (*batch_tuple, num_heads, q_seq_len, kv_seq_len))\n+    return (output, activation), out_bdims\n   else:\n-    activation = jnp.reshape(activation, (*batch_tuple, num_heads, q_seq_len, kv_seq_len))\n-  return (output, activation), out_bdims\n+    return (output,), out_bdims\n \n def _dot_product_attention_bwd_batcher(batched_args, batch_dims, *, scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask):\n   _check_valid_batch_dims(batch_dims)\n@@ -556,7 +597,7 @@ def _check_qkv_bias_mask_spec(query_spec, key_spec, value_spec, bias_spec, mask_\n       raise ValueError(\"Sharding on mask sequence dim is not allowed.\")\n \n # fwd custom partition\n-def _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args):\n+def _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args, is_training):\n   # only sharding on batch and num_head dim is allowed\n   # (*batch, q_seq, num_head, head)\n   query_spec = _get_padded_spec(arg_shapes[0])\n@@ -569,21 +610,24 @@ def _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args):\n   _check_qkv_bias_mask_spec(query_spec, key_spec, value_spec, bias_spec, mask_spec)\n   # keep out sharding same as query sharding since they have same shape\n   out_sharding = NamedSharding(mesh, PartitionSpec(*query_spec))\n-  # activation sharding\n-  *batch_spec, q_seq_spec, num_head_spec, head_spec = query_spec\n-  activation_sharding = NamedSharding(mesh, PartitionSpec(*batch_spec, num_head_spec, q_seq_spec, None))\n-  return (out_sharding, activation_sharding)\n-\n-_dot_product_attention_fwd_lower = custom_partitioning(_dot_product_attention_fwd_impl, static_argnums=(5,6,7,8,9,10))\n-def _dot_product_attention_fwd_infer_sharding_from_operands(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, mesh, arg_shapes, result_shape):\n-  return _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args)\n-\n-def _dot_product_attention_fwd_partition(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, mesh, arg_shapes, result_shape):\n+  if is_training:\n+    # activation sharding\n+    *batch_spec, q_seq_spec, num_head_spec, head_spec = query_spec\n+    activation_sharding = NamedSharding(mesh, PartitionSpec(*batch_spec, num_head_spec, q_seq_spec, None))\n+    return [out_sharding, activation_sharding]\n+  return [out_sharding]\n+\n+_dot_product_attention_fwd_lower = custom_partitioning(_dot_product_attention_fwd_impl, static_argnums=(5,6,7,8,9,10,11))\n+def _dot_product_attention_fwd_infer_sharding_from_operands(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training, mesh, arg_shapes, result_shape):\n+  return _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args, is_training)\n+\n+def _dot_product_attention_fwd_partition(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, is_training, mesh, arg_shapes, result_shape):\n   # args sharding\n   arg_shardings = tuple([arg_i.sharding for arg_i in arg_shapes])\n-  out_shardings = _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args)\n+  out_shardings = _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args, is_training)\n   impl = partial(_dot_product_attention_fwd_impl, scale=scale, seed=seed, dropout_rate=dropout_rate,\n-                variadic_args=variadic_args, is_flash_attention=is_flash_attention, is_causal_mask=is_causal_mask)\n+                variadic_args=variadic_args, is_flash_attention=is_flash_attention, is_causal_mask=is_causal_mask,\n+                is_training=is_training)\n   return mesh, impl, out_shardings, arg_shardings\n \n # bwd custom partition\n@@ -673,7 +717,7 @@ dispatch.prim_requires_devices_during_lowering.add(_dot_product_attention_fwd_p_\n dispatch.prim_requires_devices_during_lowering.add(_dot_product_attention_bwd_p)\n dispatch.prim_requires_devices_during_lowering.add(_dot_product_attention_bwd_p_wrapper)\n \n-@partial(jax.custom_vjp, nondiff_argnums=(5, 6, 7, 8, 9, 10))\n+@partial(jax.custom_vjp, nondiff_argnums=(5, 6, 7, 8, 9, 10, 11))\n def _dot_product_attention(query: Array,\n                             key: Array,\n                             value: Array,\n@@ -684,11 +728,13 @@ def _dot_product_attention(query: Array,\n                             dropout_rate: float,\n                             variadic_args: tuple[bool, ...],\n                             is_flash_attention: bool,\n-                            is_causal_mask: bool):\n+                            is_causal_mask: bool,\n+                            is_training: bool):\n   output = _dot_product_attention_fwd(\n     query, key, value, bias, mask,\n     scale=scale, seed=seed, dropout_rate=dropout_rate, variadic_args=variadic_args,\n-    is_flash_attention=is_flash_attention, is_causal_mask=is_causal_mask)\n+    is_flash_attention=is_flash_attention, is_causal_mask=is_causal_mask,\n+    is_training=is_training)\n   return output\n \n # _dot_product_attention_fwd must have the same func signature as _dot_product_attention\n@@ -704,7 +750,8 @@ def dot_product_attention(query: Array,\n                           scale: float = 1.0,\n                           is_causal_mask: bool = False,\n                           seed: int = 42,\n-                          dropout_rate: float = 0.):\n+                          dropout_rate: float = 0.,\n+                          is_training = False):\n   \"\"\"Computes dot-product attention given query, key, and value.\n   This is the core function for applying attention based on\n   https://arxiv.org/abs/1706.03762. It calculates the attention weights given\n@@ -725,16 +772,19 @@ def dot_product_attention(query: Array,\n     mask: mask used mask out logits with shape of `[batch, num_heads,\n     q_length, kv_length]`.\n     scale: scale for the query.\n+    is_causal_mask: choose to apply a causal mask or not.\n+    seed: used for dropout mask generation.\n     dropout_rate: dropout rate.\n+    is_training: choose to save activation or not.\n   Returns:\n     Output of shape `[batch, q_length, num_heads, v_depth_per_head]`.\n   \"\"\"\n-  # check if query, key and value layout meets cuDNN layout requirement\n+  # check if cuDNN is installed\n+  cudnn_version = check_cudnn_version()\n+  # check query, key and value shape and data type\n   check_qkv_layout(query, key, value)\n   # check if flash attention is supported for this attention pattern\n-  is_flash_attention, is_cross_attention = check_is_flash_attention(query, key)\n-  # check if cuDNN is installed and if cuDNN version is sufficient\n-  check_cudnn_version(is_flash_attention, is_cross_attention)\n+  is_flash_attention = check_is_flash_attention(query, key, cudnn_version, bias is not None, is_training)\n   if mask is not None and is_causal_mask:\n     raise ValueError(\"can not apply a mask and generate a causal_mask at the same time.\")\n   if not is_flash_attention and is_causal_mask:\n@@ -747,5 +797,5 @@ def dot_product_attention(query: Array,\n   output = _dot_product_attention(\n     query, key, value, bias, mask,\n     scale, seed, dropout_rate, variadic_args,\n-    is_flash_attention, is_causal_mask)\n+    is_flash_attention, is_causal_mask, is_training)\n   return output\n"
        },
        {
            "name": "fused_attention_stablehlo_test.py",
            "path": "tests/fused_attention_stablehlo_test.py",
            "patches": [
                {
                    "old_start": 16,
                    "old_length": 7,
                    "new_start": 16,
                    "new_length": 7,
                    "hunk": "@@ -16,7 +16,7 @@ from functools import partial\n from absl.testing import absltest\n from typing import Optional\n import os\n-os.environ['XLA_FLAGS'] = '--xla_gpu_enable_cudnn_fmha=true --xla_gpu_fused_attention_use_cudnn_rng=true'\n+os.environ['XLA_FLAGS'] = '--xla_gpu_enable_cudnn_fmha=true --xla_gpu_fused_attention_use_cudnn_rng=true --xla_dump_hlo_as_text --xla_dump_to=./hlo'\n \n import numpy as np\n import jax\n"
                },
                {
                    "old_start": 25,
                    "old_length": 7,
                    "new_start": 25,
                    "new_length": 7,
                    "hunk": "@@ -25,7 +25,7 @@ from jax.sharding import Mesh\n from jax.sharding import PartitionSpec, NamedSharding\n from jax._src import config\n from jax._src import test_util as jtu\n-from jax._src.cudnn.fused_attention_stablehlo import dot_product_attention\n+from jax._src.cudnn.fused_attention_stablehlo import dot_product_attention, check_is_flash_attention\n \n config.parse_flags_with_absl()\n Array = jnp.ndarray\n"
                },
                {
                    "old_start": 43,
                    "old_length": 7,
                    "new_start": 43,
                    "new_length": 7,
                    "hunk": "@@ -43,7 +43,7 @@ def sdpa_train(query: Array,\n     # convert bool mask to dtype mask\n     mask = mask.astype(query.dtype)\n   out, sdpa_vjp = jax.vjp(\n-    partial(dot_product_attention, scale=scale, is_causal_mask=is_causal_mask, dropout_rate=dropout_rate),\n+    partial(dot_product_attention, scale=scale, is_causal_mask=is_causal_mask, dropout_rate=dropout_rate, is_training=True),\n     query, key, value, bias, mask)\n   query_grad, key_grad, value_grad, _, _ = sdpa_vjp(grad)\n   return out, (query_grad, key_grad, value_grad)\n"
                },
                {
                    "old_start": 202,
                    "old_length": 5,
                    "new_start": 202,
                    "new_length": 57,
                    "hunk": "@@ -202,5 +202,57 @@ class DotProductAttentionTest(jtu.JaxTestCase):\n       self.assertArraysAllClose(key_grad_ref, key_grad, rtol=1e-5, atol=1e-5)\n       self.assertArraysAllClose(value_grad_ref, value_grad, rtol=1e-5, atol=1e-5)\n   \n+  @jtu.run_on_devices(\"cuda\")\n+  def test_sdpa_inference(self):\n+    k1, k2, k3 = jax.random.split(jax.random.key(0), 3)\n+    query = jax.random.normal(\n+        k1, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n+    key = jax.random.normal(\n+        k2, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n+    value = jax.random.normal(\n+        k3, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n+\n+    devices = np.array(jax.local_devices()[:4])\n+    devices = devices.reshape((2, 2))\n+    with Mesh(devices, ('dp', 'tp')) as mesh:\n+      qkv_spec = PartitionSpec('dp', None, 'tp', None)\n+      qkv_sharding = NamedSharding(mesh, qkv_spec)\n+      replicated = NamedSharding(mesh, PartitionSpec())\n+      in_shardings = (qkv_sharding, qkv_sharding, qkv_sharding, replicated, replicated)\n+      out_shardings = replicated\n+      query = jax.device_put(query, qkv_sharding)\n+      key = jax.device_put(key, qkv_sharding)\n+      value = jax.device_put(value, qkv_sharding)\n+      jitted_sdpa_inference = jax.jit(\n+        partial(dot_product_attention, scale=1.0, is_causal_mask=False, dropout_rate=0),\n+        in_shardings=in_shardings,\n+        out_shardings=out_shardings\n+      )\n+\n+      jitted_sdpa_inference_ref = jax.jit(\n+        partial(sdpa_ref, scale=1.0, is_causal_mask=False, dropout_rate=0),\n+        in_shardings=in_shardings,\n+        out_shardings=out_shardings\n+      )\n+\n+      out = jitted_sdpa_inference(query, key, value, None, None)\n+      out_ref = jitted_sdpa_inference_ref(query, key, value, None, None)\n+      self.assertArraysAllClose(out_ref, out, rtol=1e-5, atol=1e-5)\n+\n+  def test_sdpa_utils(self):\n+    test_cases = {\n+      (256, 512, 64, 8905, False, False): False,\n+      (1, 257, 64, 8905, False, True): True,\n+      (1, 1024, 64, 8905, False, False): True,\n+      (1024, 1024, 64, 8905, False, False): True,\n+      (1024, 1024, 128, 8905, False, False): True,\n+    }\n+\n+    for k, v in test_cases.items():\n+      sql_q, sql_v, head_dim, cudnn_version, has_bias, is_training = k\n+      query = jnp.empty((4, sql_q, 4, head_dim))\n+      key = jnp.empty((4, sql_v, 4, head_dim))\n+      self.assertEqual(check_is_flash_attention(query, key, cudnn_version, has_bias, is_training), v)\n+\n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())"
                }
            ],
            "whole_deleted": "-os.environ['XLA_FLAGS'] = '--xla_gpu_enable_cudnn_fmha=true --xla_gpu_fused_attention_use_cudnn_rng=true'\n-from jax._src.cudnn.fused_attention_stablehlo import dot_product_attention\n-    partial(dot_product_attention, scale=scale, is_causal_mask=is_causal_mask, dropout_rate=dropout_rate),\n",
            "whole_added": "+os.environ['XLA_FLAGS'] = '--xla_gpu_enable_cudnn_fmha=true --xla_gpu_fused_attention_use_cudnn_rng=true --xla_dump_hlo_as_text --xla_dump_to=./hlo'\n+from jax._src.cudnn.fused_attention_stablehlo import dot_product_attention, check_is_flash_attention\n+    partial(dot_product_attention, scale=scale, is_causal_mask=is_causal_mask, dropout_rate=dropout_rate, is_training=True),\n+  @jtu.run_on_devices(\"cuda\")\n+  def test_sdpa_inference(self):\n+    k1, k2, k3 = jax.random.split(jax.random.key(0), 3)\n+    query = jax.random.normal(\n+        k1, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n+    key = jax.random.normal(\n+        k2, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n+    value = jax.random.normal(\n+        k3, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n+\n+    devices = np.array(jax.local_devices()[:4])\n+    devices = devices.reshape((2, 2))\n+    with Mesh(devices, ('dp', 'tp')) as mesh:\n+      qkv_spec = PartitionSpec('dp', None, 'tp', None)\n+      qkv_sharding = NamedSharding(mesh, qkv_spec)\n+      replicated = NamedSharding(mesh, PartitionSpec())\n+      in_shardings = (qkv_sharding, qkv_sharding, qkv_sharding, replicated, replicated)\n+      out_shardings = replicated\n+      query = jax.device_put(query, qkv_sharding)\n+      key = jax.device_put(key, qkv_sharding)\n+      value = jax.device_put(value, qkv_sharding)\n+      jitted_sdpa_inference = jax.jit(\n+        partial(dot_product_attention, scale=1.0, is_causal_mask=False, dropout_rate=0),\n+        in_shardings=in_shardings,\n+        out_shardings=out_shardings\n+      )\n+\n+      jitted_sdpa_inference_ref = jax.jit(\n+        partial(sdpa_ref, scale=1.0, is_causal_mask=False, dropout_rate=0),\n+        in_shardings=in_shardings,\n+        out_shardings=out_shardings\n+      )\n+\n+      out = jitted_sdpa_inference(query, key, value, None, None)\n+      out_ref = jitted_sdpa_inference_ref(query, key, value, None, None)\n+      self.assertArraysAllClose(out_ref, out, rtol=1e-5, atol=1e-5)\n+\n+  def test_sdpa_utils(self):\n+    test_cases = {\n+      (256, 512, 64, 8905, False, False): False,\n+      (1, 257, 64, 8905, False, True): True,\n+      (1, 1024, 64, 8905, False, False): True,\n+      (1024, 1024, 64, 8905, False, False): True,\n+      (1024, 1024, 128, 8905, False, False): True,\n+    }\n+\n+    for k, v in test_cases.items():\n+      sql_q, sql_v, head_dim, cudnn_version, has_bias, is_training = k\n+      query = jnp.empty((4, sql_q, 4, head_dim))\n+      key = jnp.empty((4, sql_v, 4, head_dim))\n+      self.assertEqual(check_is_flash_attention(query, key, cudnn_version, has_bias, is_training), v)\n+\n",
            "whole_hunk": "@@ -16,7 +16,7 @@ from functools import partial\n from absl.testing import absltest\n from typing import Optional\n import os\n-os.environ['XLA_FLAGS'] = '--xla_gpu_enable_cudnn_fmha=true --xla_gpu_fused_attention_use_cudnn_rng=true'\n+os.environ['XLA_FLAGS'] = '--xla_gpu_enable_cudnn_fmha=true --xla_gpu_fused_attention_use_cudnn_rng=true --xla_dump_hlo_as_text --xla_dump_to=./hlo'\n \n import numpy as np\n import jax\n@@ -25,7 +25,7 @@ from jax.sharding import Mesh\n from jax.sharding import PartitionSpec, NamedSharding\n from jax._src import config\n from jax._src import test_util as jtu\n-from jax._src.cudnn.fused_attention_stablehlo import dot_product_attention\n+from jax._src.cudnn.fused_attention_stablehlo import dot_product_attention, check_is_flash_attention\n \n config.parse_flags_with_absl()\n Array = jnp.ndarray\n@@ -43,7 +43,7 @@ def sdpa_train(query: Array,\n     # convert bool mask to dtype mask\n     mask = mask.astype(query.dtype)\n   out, sdpa_vjp = jax.vjp(\n-    partial(dot_product_attention, scale=scale, is_causal_mask=is_causal_mask, dropout_rate=dropout_rate),\n+    partial(dot_product_attention, scale=scale, is_causal_mask=is_causal_mask, dropout_rate=dropout_rate, is_training=True),\n     query, key, value, bias, mask)\n   query_grad, key_grad, value_grad, _, _ = sdpa_vjp(grad)\n   return out, (query_grad, key_grad, value_grad)\n@@ -202,5 +202,57 @@ class DotProductAttentionTest(jtu.JaxTestCase):\n       self.assertArraysAllClose(key_grad_ref, key_grad, rtol=1e-5, atol=1e-5)\n       self.assertArraysAllClose(value_grad_ref, value_grad, rtol=1e-5, atol=1e-5)\n   \n+  @jtu.run_on_devices(\"cuda\")\n+  def test_sdpa_inference(self):\n+    k1, k2, k3 = jax.random.split(jax.random.key(0), 3)\n+    query = jax.random.normal(\n+        k1, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n+    key = jax.random.normal(\n+        k2, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n+    value = jax.random.normal(\n+        k3, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n+\n+    devices = np.array(jax.local_devices()[:4])\n+    devices = devices.reshape((2, 2))\n+    with Mesh(devices, ('dp', 'tp')) as mesh:\n+      qkv_spec = PartitionSpec('dp', None, 'tp', None)\n+      qkv_sharding = NamedSharding(mesh, qkv_spec)\n+      replicated = NamedSharding(mesh, PartitionSpec())\n+      in_shardings = (qkv_sharding, qkv_sharding, qkv_sharding, replicated, replicated)\n+      out_shardings = replicated\n+      query = jax.device_put(query, qkv_sharding)\n+      key = jax.device_put(key, qkv_sharding)\n+      value = jax.device_put(value, qkv_sharding)\n+      jitted_sdpa_inference = jax.jit(\n+        partial(dot_product_attention, scale=1.0, is_causal_mask=False, dropout_rate=0),\n+        in_shardings=in_shardings,\n+        out_shardings=out_shardings\n+      )\n+\n+      jitted_sdpa_inference_ref = jax.jit(\n+        partial(sdpa_ref, scale=1.0, is_causal_mask=False, dropout_rate=0),\n+        in_shardings=in_shardings,\n+        out_shardings=out_shardings\n+      )\n+\n+      out = jitted_sdpa_inference(query, key, value, None, None)\n+      out_ref = jitted_sdpa_inference_ref(query, key, value, None, None)\n+      self.assertArraysAllClose(out_ref, out, rtol=1e-5, atol=1e-5)\n+\n+  def test_sdpa_utils(self):\n+    test_cases = {\n+      (256, 512, 64, 8905, False, False): False,\n+      (1, 257, 64, 8905, False, True): True,\n+      (1, 1024, 64, 8905, False, False): True,\n+      (1024, 1024, 64, 8905, False, False): True,\n+      (1024, 1024, 128, 8905, False, False): True,\n+    }\n+\n+    for k, v in test_cases.items():\n+      sql_q, sql_v, head_dim, cudnn_version, has_bias, is_training = k\n+      query = jnp.empty((4, sql_q, 4, head_dim))\n+      key = jnp.empty((4, sql_v, 4, head_dim))\n+      self.assertEqual(check_is_flash_attention(query, key, cudnn_version, has_bias, is_training), v)\n+\n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())"
        }
    ]
},
{
    "Id": 77,
    "commit_link": "https://github.com/google/jax/commit/6f38f277b983c65086515f6e1062253514d5e544",
    "date": "2024-03-14T13:22:42-07:00",
    "message": "temporarily relax the cotangent dtype check introduced in #19009\n\nPiperOrigin-RevId: 615883208",
    "changes": [
        {
            "name": "custom_derivatives.py",
            "path": "jax/_src/custom_derivatives.py",
            "patches": [
                {
                    "old_start": 772,
                    "old_length": 8,
                    "new_start": 772,
                    "new_length": 7,
                    "hunk": "@@ -772,8 +772,7 @@ def _flatten_bwd(in_tree, in_avals, out_trees, *args):\n       results.append(Zero(ct.aval))\n     else:\n       if (not core.typecompat(a.at_least_vspace(), a_ := core.get_aval(ct))\n-          # TODO(mattjj): don't skip check with extended dtype tangent types\n-          and not dtypes.issubdtype(a_.dtype, dtypes.extended)):\n+          and not _temporary_dtype_exception(a, a_)):\n         msg = (\"Custom VJP bwd rule must produce an output with the same \"\n                \"shape/dtypes as the args tuple of the primal function, but at \"\n                f\"output{keystr(kp)} the bwd rule produced an output of \"\n"
                },
                {
                    "old_start": 783,
                    "old_length": 6,
                    "new_start": 782,
                    "new_length": 14,
                    "hunk": "@@ -783,6 +782,14 @@ def _flatten_bwd(in_tree, in_avals, out_trees, *args):\n       results.append(ct)\n   yield results\n \n+# TODO(mattjj): remove both these exceptions to cotangent compatibility check\n+def _temporary_dtype_exception(a, a_) -> bool:\n+  if isinstance(a, core.ShapedArray) and isinstance(a_, core.ShapedArray):\n+    return (a.shape == a_.shape and\n+            (dtypes.issubdtype(a_.dtype, dtypes.extended) or\n+             dtypes.issubdtype(a.dtype, dtypes.np.inexact)))\n+  return False\n+\n \n class CustomVJPCallPrimitive(core.CallPrimitive):\n   initial_style: core.Primitive"
                }
            ],
            "whole_deleted": "-          # TODO(mattjj): don't skip check with extended dtype tangent types\n-          and not dtypes.issubdtype(a_.dtype, dtypes.extended)):\n",
            "whole_added": "+          and not _temporary_dtype_exception(a, a_)):\n+# TODO(mattjj): remove both these exceptions to cotangent compatibility check\n+def _temporary_dtype_exception(a, a_) -> bool:\n+  if isinstance(a, core.ShapedArray) and isinstance(a_, core.ShapedArray):\n+    return (a.shape == a_.shape and\n+            (dtypes.issubdtype(a_.dtype, dtypes.extended) or\n+             dtypes.issubdtype(a.dtype, dtypes.np.inexact)))\n+  return False\n+\n",
            "whole_hunk": "@@ -772,8 +772,7 @@ def _flatten_bwd(in_tree, in_avals, out_trees, *args):\n       results.append(Zero(ct.aval))\n     else:\n       if (not core.typecompat(a.at_least_vspace(), a_ := core.get_aval(ct))\n-          # TODO(mattjj): don't skip check with extended dtype tangent types\n-          and not dtypes.issubdtype(a_.dtype, dtypes.extended)):\n+          and not _temporary_dtype_exception(a, a_)):\n         msg = (\"Custom VJP bwd rule must produce an output with the same \"\n                \"shape/dtypes as the args tuple of the primal function, but at \"\n                f\"output{keystr(kp)} the bwd rule produced an output of \"\n@@ -783,6 +782,14 @@ def _flatten_bwd(in_tree, in_avals, out_trees, *args):\n       results.append(ct)\n   yield results\n \n+# TODO(mattjj): remove both these exceptions to cotangent compatibility check\n+def _temporary_dtype_exception(a, a_) -> bool:\n+  if isinstance(a, core.ShapedArray) and isinstance(a_, core.ShapedArray):\n+    return (a.shape == a_.shape and\n+            (dtypes.issubdtype(a_.dtype, dtypes.extended) or\n+             dtypes.issubdtype(a.dtype, dtypes.np.inexact)))\n+  return False\n+\n \n class CustomVJPCallPrimitive(core.CallPrimitive):\n   initial_style: core.Primitive"
        }
    ]
},
{
    "Id": 78,
    "commit_link": "https://github.com/google/jax/commit/1326c74db7f25b7224412fbe6f69f43d17a90e6a",
    "date": "2024-03-13T19:57:09-07:00",
    "message": "add a shape mismatch check and error to custom_vjp\n\nno idea how we lasted so long without this...",
    "changes": [
        {
            "name": "custom_derivatives.py",
            "path": "jax/_src/custom_derivatives.py",
            "patches": [
                {
                    "old_start": 42,
                    "old_length": 8,
                    "new_start": 42,
                    "new_length": 10,
                    "hunk": "@@ -42,8 +42,10 @@ from jax._src.interpreters.batching import not_mapped\n from jax._src.lax import lax\n from jax._src.tree_util import (tree_flatten, tree_unflatten, tree_map,\n                                 treedef_is_leaf, treedef_tuple,\n-                                register_pytree_node_class, tree_leaves)\n-from jax._src.util import cache, safe_zip, safe_map, split_list, Unhashable\n+                                register_pytree_node_class, tree_leaves,\n+                                tree_flatten_with_path, keystr)\n+from jax._src.util import (cache, safe_zip, safe_map, split_list, Unhashable,\n+                           unzip2)\n \n \n traceback_util.register_exclusion(__file__)\n"
                },
                {
                    "old_start": 733,
                    "old_length": 6,
                    "new_start": 735,
                    "new_length": 7,
                    "hunk": "@@ -733,6 +735,7 @@ def _flatten_bwd(in_tree, in_avals, out_trees, *args):\n   # object, to be replaced with Nones in the final returned result.\n   zero = object()  # non-pytree sentinel to replace Nones in py_cts_in\n   dummy = tree_unflatten(in_tree, [object()] * in_tree.num_leaves)\n+  keypaths, _ = unzip2(tree_flatten_with_path(dummy)[0])\n   cts_in_flat = []\n   def append(x, d):\n     num_leaves = len(tree_flatten(d)[0])\n"
                },
                {
                    "old_start": 747,
                    "old_length": 17,
                    "new_start": 750,
                    "new_length": 38,
                    "hunk": "@@ -747,17 +750,38 @@ def _flatten_bwd(in_tree, in_avals, out_trees, *args):\n     tree_map(append, py_cts_in, dummy, is_leaf=lambda x: x is None)\n   except ValueError:\n     _, in_tree2 = tree_flatten(py_cts_in)\n-    msg = (\"Custom VJP rule must produce an output with the same container \"\n+    msg = (\"Custom VJP bwd rule must produce an output with the same container \"\n            \"(pytree) structure as the args tuple of the primal function, \"\n            \"and in particular must produce a tuple of length equal to the \"\n-           \"number of arguments to the primal function, but got VJP output \"\n+           \"number of arguments to the primal function, but got bwd output \"\n            \"structure {} for primal input structure {}.\")\n     raise TypeError(msg.format(in_tree2, in_tree)) from None\n-  # Ignore any None cotangents, and any corresponding to inputs for which the\n-  # type doesn't equal the tangent type (i.e. float0s)\n-  # TODO(mattjj): change this to check if tangent type represents 0dim vspace\n-  yield [Zero(a.at_least_vspace()) if ct is zero or a != a.at_least_vspace()\n-         else ct for a, ct in zip(in_avals, cts_in_flat)]\n+  results = []\n+  for kp, a, ct in zip(keypaths, in_avals, cts_in_flat):\n+    if ct is zero or a != a.at_least_vspace():\n+      results.append(Zero(a.at_least_vspace()))\n+    elif type(ct) is SymbolicZero:\n+      if not core.typecompat(a.at_least_vspace(), a_ := ct.aval):\n+        msg = (\"Custom VJP bwd rule produced a SymbolicZero with a shape/dtype \"\n+               \"that does not match the corresponding input tangent shape/dtype: \"\n+               f\"the SymbolicZero had shape/dtype {a_.str_short()} while the \"\n+               f\"corresponding input had shape/dtype {a.str_short()}. \"\n+               \"Consider just returning a None here instead of a SymbolicZero \"\n+               \"object.\")\n+        raise ValueError(msg)\n+      results.append(Zero(ct.aval))\n+    else:\n+      if (not core.typecompat(a.at_least_vspace(), a_ := core.get_aval(ct))\n+          # TODO(mattjj): don't skip check with extended dtype tangent types\n+          and not dtypes.issubdtype(a_.dtype, dtypes.extended)):\n+        msg = (\"Custom VJP bwd rule must produce an output with the same \"\n+               \"shape/dtypes as the args tuple of the primal function, but at \"\n+               f\"output{keystr(kp)} the bwd rule produced an output of \"\n+               f\"shape/dtype {raise_to_shaped(a_).str_short()} corresponding \"\n+               f\"to an input of shape/dtype {a.str_short()}.\")\n+        raise ValueError(msg)\n+      results.append(ct)\n+  yield results\n \n \n class CustomVJPCallPrimitive(core.CallPrimitive):\n"
                }
            ],
            "whole_deleted": "-                                register_pytree_node_class, tree_leaves)\n-from jax._src.util import cache, safe_zip, safe_map, split_list, Unhashable\n-    msg = (\"Custom VJP rule must produce an output with the same container \"\n-           \"number of arguments to the primal function, but got VJP output \"\n-  # Ignore any None cotangents, and any corresponding to inputs for which the\n-  # type doesn't equal the tangent type (i.e. float0s)\n-  # TODO(mattjj): change this to check if tangent type represents 0dim vspace\n-  yield [Zero(a.at_least_vspace()) if ct is zero or a != a.at_least_vspace()\n-         else ct for a, ct in zip(in_avals, cts_in_flat)]\n",
            "whole_added": "+                                register_pytree_node_class, tree_leaves,\n+                                tree_flatten_with_path, keystr)\n+from jax._src.util import (cache, safe_zip, safe_map, split_list, Unhashable,\n+                           unzip2)\n+  keypaths, _ = unzip2(tree_flatten_with_path(dummy)[0])\n+    msg = (\"Custom VJP bwd rule must produce an output with the same container \"\n+           \"number of arguments to the primal function, but got bwd output \"\n+  results = []\n+  for kp, a, ct in zip(keypaths, in_avals, cts_in_flat):\n+    if ct is zero or a != a.at_least_vspace():\n+      results.append(Zero(a.at_least_vspace()))\n+    elif type(ct) is SymbolicZero:\n+      if not core.typecompat(a.at_least_vspace(), a_ := ct.aval):\n+        msg = (\"Custom VJP bwd rule produced a SymbolicZero with a shape/dtype \"\n+               \"that does not match the corresponding input tangent shape/dtype: \"\n+               f\"the SymbolicZero had shape/dtype {a_.str_short()} while the \"\n+               f\"corresponding input had shape/dtype {a.str_short()}. \"\n+               \"Consider just returning a None here instead of a SymbolicZero \"\n+               \"object.\")\n+        raise ValueError(msg)\n+      results.append(Zero(ct.aval))\n+    else:\n+      if (not core.typecompat(a.at_least_vspace(), a_ := core.get_aval(ct))\n+          # TODO(mattjj): don't skip check with extended dtype tangent types\n+          and not dtypes.issubdtype(a_.dtype, dtypes.extended)):\n+        msg = (\"Custom VJP bwd rule must produce an output with the same \"\n+               \"shape/dtypes as the args tuple of the primal function, but at \"\n+               f\"output{keystr(kp)} the bwd rule produced an output of \"\n+               f\"shape/dtype {raise_to_shaped(a_).str_short()} corresponding \"\n+               f\"to an input of shape/dtype {a.str_short()}.\")\n+        raise ValueError(msg)\n+      results.append(ct)\n+  yield results\n",
            "whole_hunk": "@@ -42,8 +42,10 @@ from jax._src.interpreters.batching import not_mapped\n from jax._src.lax import lax\n from jax._src.tree_util import (tree_flatten, tree_unflatten, tree_map,\n                                 treedef_is_leaf, treedef_tuple,\n-                                register_pytree_node_class, tree_leaves)\n-from jax._src.util import cache, safe_zip, safe_map, split_list, Unhashable\n+                                register_pytree_node_class, tree_leaves,\n+                                tree_flatten_with_path, keystr)\n+from jax._src.util import (cache, safe_zip, safe_map, split_list, Unhashable,\n+                           unzip2)\n \n \n traceback_util.register_exclusion(__file__)\n@@ -733,6 +735,7 @@ def _flatten_bwd(in_tree, in_avals, out_trees, *args):\n   # object, to be replaced with Nones in the final returned result.\n   zero = object()  # non-pytree sentinel to replace Nones in py_cts_in\n   dummy = tree_unflatten(in_tree, [object()] * in_tree.num_leaves)\n+  keypaths, _ = unzip2(tree_flatten_with_path(dummy)[0])\n   cts_in_flat = []\n   def append(x, d):\n     num_leaves = len(tree_flatten(d)[0])\n@@ -747,17 +750,38 @@ def _flatten_bwd(in_tree, in_avals, out_trees, *args):\n     tree_map(append, py_cts_in, dummy, is_leaf=lambda x: x is None)\n   except ValueError:\n     _, in_tree2 = tree_flatten(py_cts_in)\n-    msg = (\"Custom VJP rule must produce an output with the same container \"\n+    msg = (\"Custom VJP bwd rule must produce an output with the same container \"\n            \"(pytree) structure as the args tuple of the primal function, \"\n            \"and in particular must produce a tuple of length equal to the \"\n-           \"number of arguments to the primal function, but got VJP output \"\n+           \"number of arguments to the primal function, but got bwd output \"\n            \"structure {} for primal input structure {}.\")\n     raise TypeError(msg.format(in_tree2, in_tree)) from None\n-  # Ignore any None cotangents, and any corresponding to inputs for which the\n-  # type doesn't equal the tangent type (i.e. float0s)\n-  # TODO(mattjj): change this to check if tangent type represents 0dim vspace\n-  yield [Zero(a.at_least_vspace()) if ct is zero or a != a.at_least_vspace()\n-         else ct for a, ct in zip(in_avals, cts_in_flat)]\n+  results = []\n+  for kp, a, ct in zip(keypaths, in_avals, cts_in_flat):\n+    if ct is zero or a != a.at_least_vspace():\n+      results.append(Zero(a.at_least_vspace()))\n+    elif type(ct) is SymbolicZero:\n+      if not core.typecompat(a.at_least_vspace(), a_ := ct.aval):\n+        msg = (\"Custom VJP bwd rule produced a SymbolicZero with a shape/dtype \"\n+               \"that does not match the corresponding input tangent shape/dtype: \"\n+               f\"the SymbolicZero had shape/dtype {a_.str_short()} while the \"\n+               f\"corresponding input had shape/dtype {a.str_short()}. \"\n+               \"Consider just returning a None here instead of a SymbolicZero \"\n+               \"object.\")\n+        raise ValueError(msg)\n+      results.append(Zero(ct.aval))\n+    else:\n+      if (not core.typecompat(a.at_least_vspace(), a_ := core.get_aval(ct))\n+          # TODO(mattjj): don't skip check with extended dtype tangent types\n+          and not dtypes.issubdtype(a_.dtype, dtypes.extended)):\n+        msg = (\"Custom VJP bwd rule must produce an output with the same \"\n+               \"shape/dtypes as the args tuple of the primal function, but at \"\n+               f\"output{keystr(kp)} the bwd rule produced an output of \"\n+               f\"shape/dtype {raise_to_shaped(a_).str_short()} corresponding \"\n+               f\"to an input of shape/dtype {a.str_short()}.\")\n+        raise ValueError(msg)\n+      results.append(ct)\n+  yield results\n \n \n class CustomVJPCallPrimitive(core.CallPrimitive):\n"
        },
        {
            "name": "api_test.py",
            "path": "tests/api_test.py",
            "patches": [
                {
                    "old_start": 8244,
                    "old_length": 10,
                    "new_start": 8244,
                    "new_length": 10,
                    "hunk": "@@ -8244,10 +8244,10 @@ class CustomVJPTest(jtu.JaxTestCase):\n     self.assertRaisesRegex(\n         TypeError,\n         re.escape(\n-            \"Custom VJP rule must produce an output with the same container \"\n+            \"Custom VJP bwd rule must produce an output with the same container \"\n             \"(pytree) structure as the args tuple of the primal function, \"\n             \"and in particular must produce a tuple of length equal to the \"\n-            \"number of arguments to the primal function, but got VJP output \"\n+            \"number of arguments to the primal function, but got bwd output \"\n             \"structure {} for primal input structure {}.\".format(\n                 jax.tree.structure((1, 1)),\n                 jax.tree.structure((1,)))\n"
                },
                {
                    "old_start": 8266,
                    "old_length": 7,
                    "new_start": 8266,
                    "new_length": 7,
                    "hunk": "@@ -8266,7 +8266,7 @@ class CustomVJPTest(jtu.JaxTestCase):\n       return 2. * g  # Should be a tuple\n \n     f.defvjp(foo_fwd, foo_bwd)\n-    with self.assertRaisesRegex(TypeError, \"Custom VJP rule .* must produce a tuple\"):\n+    with self.assertRaisesRegex(TypeError, \"Custom VJP bwd rule .* must produce a tuple\"):\n       api.grad(f)(3.)\n \n   def test_fwd_rule_primal_out_type_doesnt_match_primal_error_message(self):\n"
                },
                {
                    "old_start": 8996,
                    "old_length": 6,
                    "new_start": 8996,
                    "new_length": 26,
                    "hunk": "@@ -8996,6 +8996,26 @@ class CustomVJPTest(jtu.JaxTestCase):\n     gx, = vjp(x)\n     self.assertArraysAllClose(gx, zero)\n \n+  def test_symbolic_zero_custom_vjp_bwd_shape_error(self):\n+    @jax.custom_vjp\n+    def f(x, y, z):\n+      return x, y, z\n+\n+    def fwd(x, y, z):\n+      return f(x.value, y.value, z.value), None\n+\n+    def bwd(_, gs):\n+      x_bar, y_bar, z_bar = gs\n+      return y_bar, x_bar, z_bar  # swapped!\n+\n+    f.defvjp(fwd, bwd, symbolic_zeros=True)\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        r'Consider just returning a None here'):\n+      jax.grad(lambda x, y, z: f(x, y, z)[2].sum())(\n+        jnp.ones(1), jnp.ones(2), jnp.ones(3))\n+\n   @parameterized.named_parameters(\n       ('jit_vmap', True, True),\n       ('jit', True, False),\n"
                },
                {
                    "old_start": 9251,
                    "old_length": 6,
                    "new_start": 9271,
                    "new_length": 24,
                    "hunk": "@@ -9251,6 +9271,24 @@ class CustomVJPTest(jtu.JaxTestCase):\n \n     jax.grad(f)((1.0, (2.0, None)))  # don't crash\n \n+  def test_bwd_rule_shape_mismatch(self):\n+    @jax.custom_vjp\n+    def foo(x, y):\n+      return x\n+\n+    def foo_fwd(x, y):\n+      return x, None\n+\n+    def foo_bwd(_, g):\n+      return jnp.zeros(3), jnp.zeros(3)\n+\n+    foo.defvjp(foo_fwd, foo_bwd)\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        r'output\\[1\\] the bwd rule produced an output of shape/dtype float..\\[3\\]'):\n+      jax.grad(lambda x, y: foo(x, y * y).sum(), 1)(jnp.ones(3), jnp.ones(4))\n+\n \n def transpose_unary(f, x_example):\n   def transposed(y):"
                }
            ],
            "whole_deleted": "-            \"Custom VJP rule must produce an output with the same container \"\n-            \"number of arguments to the primal function, but got VJP output \"\n-    with self.assertRaisesRegex(TypeError, \"Custom VJP rule .* must produce a tuple\"):\n",
            "whole_added": "+            \"Custom VJP bwd rule must produce an output with the same container \"\n+            \"number of arguments to the primal function, but got bwd output \"\n+    with self.assertRaisesRegex(TypeError, \"Custom VJP bwd rule .* must produce a tuple\"):\n+  def test_symbolic_zero_custom_vjp_bwd_shape_error(self):\n+    @jax.custom_vjp\n+    def f(x, y, z):\n+      return x, y, z\n+\n+    def fwd(x, y, z):\n+      return f(x.value, y.value, z.value), None\n+\n+    def bwd(_, gs):\n+      x_bar, y_bar, z_bar = gs\n+      return y_bar, x_bar, z_bar  # swapped!\n+\n+    f.defvjp(fwd, bwd, symbolic_zeros=True)\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        r'Consider just returning a None here'):\n+      jax.grad(lambda x, y, z: f(x, y, z)[2].sum())(\n+        jnp.ones(1), jnp.ones(2), jnp.ones(3))\n+\n+  def test_bwd_rule_shape_mismatch(self):\n+    @jax.custom_vjp\n+    def foo(x, y):\n+      return x\n+\n+    def foo_fwd(x, y):\n+      return x, None\n+\n+    def foo_bwd(_, g):\n+      return jnp.zeros(3), jnp.zeros(3)\n+\n+    foo.defvjp(foo_fwd, foo_bwd)\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        r'output\\[1\\] the bwd rule produced an output of shape/dtype float..\\[3\\]'):\n+      jax.grad(lambda x, y: foo(x, y * y).sum(), 1)(jnp.ones(3), jnp.ones(4))\n+\n",
            "whole_hunk": "@@ -8244,10 +8244,10 @@ class CustomVJPTest(jtu.JaxTestCase):\n     self.assertRaisesRegex(\n         TypeError,\n         re.escape(\n-            \"Custom VJP rule must produce an output with the same container \"\n+            \"Custom VJP bwd rule must produce an output with the same container \"\n             \"(pytree) structure as the args tuple of the primal function, \"\n             \"and in particular must produce a tuple of length equal to the \"\n-            \"number of arguments to the primal function, but got VJP output \"\n+            \"number of arguments to the primal function, but got bwd output \"\n             \"structure {} for primal input structure {}.\".format(\n                 jax.tree.structure((1, 1)),\n                 jax.tree.structure((1,)))\n@@ -8266,7 +8266,7 @@ class CustomVJPTest(jtu.JaxTestCase):\n       return 2. * g  # Should be a tuple\n \n     f.defvjp(foo_fwd, foo_bwd)\n-    with self.assertRaisesRegex(TypeError, \"Custom VJP rule .* must produce a tuple\"):\n+    with self.assertRaisesRegex(TypeError, \"Custom VJP bwd rule .* must produce a tuple\"):\n       api.grad(f)(3.)\n \n   def test_fwd_rule_primal_out_type_doesnt_match_primal_error_message(self):\n@@ -8996,6 +8996,26 @@ class CustomVJPTest(jtu.JaxTestCase):\n     gx, = vjp(x)\n     self.assertArraysAllClose(gx, zero)\n \n+  def test_symbolic_zero_custom_vjp_bwd_shape_error(self):\n+    @jax.custom_vjp\n+    def f(x, y, z):\n+      return x, y, z\n+\n+    def fwd(x, y, z):\n+      return f(x.value, y.value, z.value), None\n+\n+    def bwd(_, gs):\n+      x_bar, y_bar, z_bar = gs\n+      return y_bar, x_bar, z_bar  # swapped!\n+\n+    f.defvjp(fwd, bwd, symbolic_zeros=True)\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        r'Consider just returning a None here'):\n+      jax.grad(lambda x, y, z: f(x, y, z)[2].sum())(\n+        jnp.ones(1), jnp.ones(2), jnp.ones(3))\n+\n   @parameterized.named_parameters(\n       ('jit_vmap', True, True),\n       ('jit', True, False),\n@@ -9251,6 +9271,24 @@ class CustomVJPTest(jtu.JaxTestCase):\n \n     jax.grad(f)((1.0, (2.0, None)))  # don't crash\n \n+  def test_bwd_rule_shape_mismatch(self):\n+    @jax.custom_vjp\n+    def foo(x, y):\n+      return x\n+\n+    def foo_fwd(x, y):\n+      return x, None\n+\n+    def foo_bwd(_, g):\n+      return jnp.zeros(3), jnp.zeros(3)\n+\n+    foo.defvjp(foo_fwd, foo_bwd)\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        r'output\\[1\\] the bwd rule produced an output of shape/dtype float..\\[3\\]'):\n+      jax.grad(lambda x, y: foo(x, y * y).sum(), 1)(jnp.ones(3), jnp.ones(4))\n+\n \n def transpose_unary(f, x_example):\n   def transposed(y):"
        }
    ]
},
{
    "Id": 79,
    "commit_link": "https://github.com/google/jax/commit/1cb8d31c665bf88ed2b437431f79ae56bf18e368",
    "date": "2024-03-06T11:42:40-08:00",
    "message": "Convert in_shardings to physical shardings in cpp dispatch path because the same happens with prng arrays.\n\nAlso comment out key reuse check in cpp dispatch since it's True for jax tests which prevent prng keys from taking Cpp dispatch.\n\nPiperOrigin-RevId: 613289252",
    "changes": [
        {
            "name": "pxla.py",
            "path": "jax/_src/interpreters/pxla.py",
            "patches": [
                {
                    "old_start": 2471,
                    "old_length": 6,
                    "new_start": 2471,
                    "new_length": 15,
                    "hunk": "@@ -2471,6 +2471,15 @@ def _register_out_sharding_handler(\n   _orig_out_sharding_handlers[sharding_cls] = handler\n \n \n+def _gspmd_to_named_sharding_via_mesh(\n+    out_s: sharding_impls.GSPMDSharding,\n+    mesh: Mesh) -> sharding_impls.NamedSharding:\n+  parsed_pspec = sharding_impls.parse_flatten_op_sharding(\n+      out_s._hlo_sharding, mesh)[0]\n+  return create_mesh_pspec_sharding(\n+      mesh, parsed_pspec.get_partition_spec(), parsed_pspec,\n+      out_s.memory_kind)\n+\n def _gspmd_to_named_sharding(\n     out_s: sharding_impls.GSPMDSharding,\n     orig_in_s: sharding_impls.NamedSharding) -> sharding_impls.NamedSharding:\n"
                },
                {
                    "old_start": 2688,
                    "old_length": 7,
                    "new_start": 2697,
                    "new_length": 7,
                    "hunk": "@@ -2688,7 +2697,7 @@ def _maybe_get_and_check_in_shardings(\n     if is_unspecified(orig):\n       if (aval is not core.abstract_token and\n           dtypes.issubdtype(aval.dtype, dtypes.extended)):\n-        xla_s = aval.dtype._rules.logical_op_sharding(aval, xla_s)\n+        xla_s = aval.dtype._rules.logical_sharding(aval, xla_s)\n       new_in_shardings.append(xla_s)\n     else:\n       # TODO(yashkatariya): Remove the if branch for abstract_token once\n"
                },
                {
                    "old_start": 2726,
                    "old_length": 7,
                    "new_start": 2735,
                    "new_length": 7,
                    "hunk": "@@ -2726,7 +2735,7 @@ def _maybe_get_and_check_out_shardings(\n     if is_unspecified(orig):\n       if (aval is not core.abstract_token and\n           dtypes.issubdtype(aval.dtype, dtypes.extended)):\n-        xla_s = aval.dtype._rules.logical_op_sharding(aval, xla_s)\n+        xla_s = aval.dtype._rules.logical_sharding(aval, xla_s)\n       new_out_shardings.append(xla_s)\n     else:\n       xla_hlo_s = xla_s._to_xla_hlo_sharding(aval.ndim)  # type: ignore\n"
                },
                {
                    "old_start": 3031,
                    "old_length": 8,
                    "new_start": 3040,
                    "new_length": 14,
                    "hunk": "@@ -3031,8 +3040,14 @@ class MeshExecutable(stages.XlaExecutable):\n         out_committed = [o._committed for o in out_flat]\n         kept_var_bitvec = [i in self._kept_var_idx\n                            for i in range(len(args_flat))]\n+        in_shardings = [\n+            a.dtype._rules.physical_sharding(a, s)\n+            if a is not core.abstract_token and dtypes.issubdtype(a.dtype, dtypes.extended)\n+            else s\n+            for s, a in zip(self._in_shardings, self.in_avals)\n+        ]\n         fastpath_data = MeshExecutableFastpathData(\n-            self.xla_executable, out_tree_dispatch, self._in_shardings,\n+            self.xla_executable, out_tree_dispatch, in_shardings,\n             self._out_shardings, out_avals, out_committed, kept_var_bitvec,\n             self.unsafe_call.in_handler.local_devices,\n             self.unsafe_call.in_handler.input_indices)\n"
                }
            ],
            "whole_deleted": "-        xla_s = aval.dtype._rules.logical_op_sharding(aval, xla_s)\n-        xla_s = aval.dtype._rules.logical_op_sharding(aval, xla_s)\n-            self.xla_executable, out_tree_dispatch, self._in_shardings,\n",
            "whole_added": "+def _gspmd_to_named_sharding_via_mesh(\n+    out_s: sharding_impls.GSPMDSharding,\n+    mesh: Mesh) -> sharding_impls.NamedSharding:\n+  parsed_pspec = sharding_impls.parse_flatten_op_sharding(\n+      out_s._hlo_sharding, mesh)[0]\n+  return create_mesh_pspec_sharding(\n+      mesh, parsed_pspec.get_partition_spec(), parsed_pspec,\n+      out_s.memory_kind)\n+\n+        xla_s = aval.dtype._rules.logical_sharding(aval, xla_s)\n+        xla_s = aval.dtype._rules.logical_sharding(aval, xla_s)\n+        in_shardings = [\n+            a.dtype._rules.physical_sharding(a, s)\n+            if a is not core.abstract_token and dtypes.issubdtype(a.dtype, dtypes.extended)\n+            else s\n+            for s, a in zip(self._in_shardings, self.in_avals)\n+        ]\n+            self.xla_executable, out_tree_dispatch, in_shardings,\n",
            "whole_hunk": "@@ -2471,6 +2471,15 @@ def _register_out_sharding_handler(\n   _orig_out_sharding_handlers[sharding_cls] = handler\n \n \n+def _gspmd_to_named_sharding_via_mesh(\n+    out_s: sharding_impls.GSPMDSharding,\n+    mesh: Mesh) -> sharding_impls.NamedSharding:\n+  parsed_pspec = sharding_impls.parse_flatten_op_sharding(\n+      out_s._hlo_sharding, mesh)[0]\n+  return create_mesh_pspec_sharding(\n+      mesh, parsed_pspec.get_partition_spec(), parsed_pspec,\n+      out_s.memory_kind)\n+\n def _gspmd_to_named_sharding(\n     out_s: sharding_impls.GSPMDSharding,\n     orig_in_s: sharding_impls.NamedSharding) -> sharding_impls.NamedSharding:\n@@ -2688,7 +2697,7 @@ def _maybe_get_and_check_in_shardings(\n     if is_unspecified(orig):\n       if (aval is not core.abstract_token and\n           dtypes.issubdtype(aval.dtype, dtypes.extended)):\n-        xla_s = aval.dtype._rules.logical_op_sharding(aval, xla_s)\n+        xla_s = aval.dtype._rules.logical_sharding(aval, xla_s)\n       new_in_shardings.append(xla_s)\n     else:\n       # TODO(yashkatariya): Remove the if branch for abstract_token once\n@@ -2726,7 +2735,7 @@ def _maybe_get_and_check_out_shardings(\n     if is_unspecified(orig):\n       if (aval is not core.abstract_token and\n           dtypes.issubdtype(aval.dtype, dtypes.extended)):\n-        xla_s = aval.dtype._rules.logical_op_sharding(aval, xla_s)\n+        xla_s = aval.dtype._rules.logical_sharding(aval, xla_s)\n       new_out_shardings.append(xla_s)\n     else:\n       xla_hlo_s = xla_s._to_xla_hlo_sharding(aval.ndim)  # type: ignore\n@@ -3031,8 +3040,14 @@ class MeshExecutable(stages.XlaExecutable):\n         out_committed = [o._committed for o in out_flat]\n         kept_var_bitvec = [i in self._kept_var_idx\n                            for i in range(len(args_flat))]\n+        in_shardings = [\n+            a.dtype._rules.physical_sharding(a, s)\n+            if a is not core.abstract_token and dtypes.issubdtype(a.dtype, dtypes.extended)\n+            else s\n+            for s, a in zip(self._in_shardings, self.in_avals)\n+        ]\n         fastpath_data = MeshExecutableFastpathData(\n-            self.xla_executable, out_tree_dispatch, self._in_shardings,\n+            self.xla_executable, out_tree_dispatch, in_shardings,\n             self._out_shardings, out_avals, out_committed, kept_var_bitvec,\n             self.unsafe_call.in_handler.local_devices,\n             self.unsafe_call.in_handler.input_indices)\n"
        },
        {
            "name": "lax.py",
            "path": "jax/_src/lax/lax.py",
            "patches": [
                {
                    "old_start": 5116,
                    "old_length": 9,
                    "new_start": 5116,
                    "new_length": 13,
                    "hunk": "@@ -5116,9 +5116,13 @@ class BIntRules:\n     return hlo_sharding\n \n   @staticmethod\n-  def logical_op_sharding(aval, phys_sharding):\n+  def logical_sharding(aval, phys_sharding):\n     return phys_sharding\n \n+  @staticmethod\n+  def physical_sharding(aval, sharding):\n+    return sharding\n+\n   @staticmethod\n   def convert_from(bint_dtype, other_dtype) -> bool:\n     return other_dtype in (np.dtype('int32'), np.dtype('int64'))\n"
                }
            ],
            "whole_deleted": "-  def logical_op_sharding(aval, phys_sharding):\n",
            "whole_added": "+  def logical_sharding(aval, phys_sharding):\n+  @staticmethod\n+  def physical_sharding(aval, sharding):\n+    return sharding\n+\n",
            "whole_hunk": "@@ -5116,9 +5116,13 @@ class BIntRules:\n     return hlo_sharding\n \n   @staticmethod\n-  def logical_op_sharding(aval, phys_sharding):\n+  def logical_sharding(aval, phys_sharding):\n     return phys_sharding\n \n+  @staticmethod\n+  def physical_sharding(aval, sharding):\n+    return sharding\n+\n   @staticmethod\n   def convert_from(bint_dtype, other_dtype) -> bool:\n     return other_dtype in (np.dtype('int32'), np.dtype('int64'))\n"
        },
        {
            "name": "pjit.py",
            "path": "jax/_src/pjit.py",
            "patches": [
                {
                    "old_start": 186,
                    "old_length": 20,
                    "new_start": 186,
                    "new_length": 20,
                    "hunk": "@@ -186,20 +186,20 @@ def _get_fastpath_data(\n   out_reflattened, out_tree = pxla.reflatten_outputs_for_dispatch(out_tree, out_flat)\n \n   use_fastpath = (\n-      executable is not None and\n-      isinstance(executable, pxla.MeshExecutable) and\n-      isinstance(executable.unsafe_call, pxla.ExecuteReplicated) and\n+      executable is not None\n+      and isinstance(executable, pxla.MeshExecutable)\n+      and isinstance(executable.unsafe_call, pxla.ExecuteReplicated)\n       # No effects in computation\n-      not executable.unsafe_call.ordered_effects and\n-      not executable.unsafe_call.has_unordered_effects and\n-      not executable.unsafe_call.has_host_callbacks and\n-      all(isinstance(x, xc.ArrayImpl) for x in out_reflattened) and\n+      and not executable.unsafe_call.ordered_effects\n+      and not executable.unsafe_call.has_unordered_effects\n+      and not executable.unsafe_call.has_host_callbacks\n+      and all(isinstance(x, xc.ArrayImpl) for x in out_reflattened)\n       # no attr state effects\n-      not attrs_tracked and\n+      and not attrs_tracked\n       # no ref state effects\n-      not any(isinstance(e, RefEffect) for e in effects) and\n+      and not any(isinstance(e, RefEffect) for e in effects)\n       # no prng reuse checking\n-      not (config.enable_key_reuse_checks.value and any(\n+      and not (config.enable_key_reuse_checks.value and any(\n         hasattr(arg, 'dtype') and dtypes.issubdtype(arg.dtype, dtypes.prng_key)\n         for arg in (*args_flat, *out_flat)))\n       )\n"
                },
                {
                    "old_start": 209,
                    "old_length": 8,
                    "new_start": 209,
                    "new_length": 14,
                    "hunk": "@@ -209,8 +209,14 @@ def _get_fastpath_data(\n     out_committed = [o._committed for o in out_reflattened]\n     kept_var_bitvec = [i in executable._kept_var_idx\n                        for i in range(len(args_flat))]\n+    in_shardings = [\n+        a.dtype._rules.physical_sharding(a, s)\n+        if a is not core.abstract_token and dtypes.issubdtype(a.dtype, dtypes.extended)\n+        else s\n+        for s, a in zip(executable._in_shardings, executable.in_avals)\n+    ]\n     fastpath_data = pxla.MeshExecutableFastpathData(\n-        executable.xla_executable, out_tree, executable._in_shardings,\n+        executable.xla_executable, out_tree, in_shardings,\n         executable._out_shardings, out_avals, out_committed, kept_var_bitvec,\n         executable.unsafe_call.in_handler.local_devices,\n         executable.unsafe_call.in_handler.input_indices)\n"
                }
            ],
            "whole_deleted": "-      executable is not None and\n-      isinstance(executable, pxla.MeshExecutable) and\n-      isinstance(executable.unsafe_call, pxla.ExecuteReplicated) and\n-      not executable.unsafe_call.ordered_effects and\n-      not executable.unsafe_call.has_unordered_effects and\n-      not executable.unsafe_call.has_host_callbacks and\n-      all(isinstance(x, xc.ArrayImpl) for x in out_reflattened) and\n-      not attrs_tracked and\n-      not any(isinstance(e, RefEffect) for e in effects) and\n-      not (config.enable_key_reuse_checks.value and any(\n-        executable.xla_executable, out_tree, executable._in_shardings,\n",
            "whole_added": "+      executable is not None\n+      and isinstance(executable, pxla.MeshExecutable)\n+      and isinstance(executable.unsafe_call, pxla.ExecuteReplicated)\n+      and not executable.unsafe_call.ordered_effects\n+      and not executable.unsafe_call.has_unordered_effects\n+      and not executable.unsafe_call.has_host_callbacks\n+      and all(isinstance(x, xc.ArrayImpl) for x in out_reflattened)\n+      and not attrs_tracked\n+      and not any(isinstance(e, RefEffect) for e in effects)\n+      and not (config.enable_key_reuse_checks.value and any(\n+    in_shardings = [\n+        a.dtype._rules.physical_sharding(a, s)\n+        if a is not core.abstract_token and dtypes.issubdtype(a.dtype, dtypes.extended)\n+        else s\n+        for s, a in zip(executable._in_shardings, executable.in_avals)\n+    ]\n+        executable.xla_executable, out_tree, in_shardings,\n",
            "whole_hunk": "@@ -186,20 +186,20 @@ def _get_fastpath_data(\n   out_reflattened, out_tree = pxla.reflatten_outputs_for_dispatch(out_tree, out_flat)\n \n   use_fastpath = (\n-      executable is not None and\n-      isinstance(executable, pxla.MeshExecutable) and\n-      isinstance(executable.unsafe_call, pxla.ExecuteReplicated) and\n+      executable is not None\n+      and isinstance(executable, pxla.MeshExecutable)\n+      and isinstance(executable.unsafe_call, pxla.ExecuteReplicated)\n       # No effects in computation\n-      not executable.unsafe_call.ordered_effects and\n-      not executable.unsafe_call.has_unordered_effects and\n-      not executable.unsafe_call.has_host_callbacks and\n-      all(isinstance(x, xc.ArrayImpl) for x in out_reflattened) and\n+      and not executable.unsafe_call.ordered_effects\n+      and not executable.unsafe_call.has_unordered_effects\n+      and not executable.unsafe_call.has_host_callbacks\n+      and all(isinstance(x, xc.ArrayImpl) for x in out_reflattened)\n       # no attr state effects\n-      not attrs_tracked and\n+      and not attrs_tracked\n       # no ref state effects\n-      not any(isinstance(e, RefEffect) for e in effects) and\n+      and not any(isinstance(e, RefEffect) for e in effects)\n       # no prng reuse checking\n-      not (config.enable_key_reuse_checks.value and any(\n+      and not (config.enable_key_reuse_checks.value and any(\n         hasattr(arg, 'dtype') and dtypes.issubdtype(arg.dtype, dtypes.prng_key)\n         for arg in (*args_flat, *out_flat)))\n       )\n@@ -209,8 +209,14 @@ def _get_fastpath_data(\n     out_committed = [o._committed for o in out_reflattened]\n     kept_var_bitvec = [i in executable._kept_var_idx\n                        for i in range(len(args_flat))]\n+    in_shardings = [\n+        a.dtype._rules.physical_sharding(a, s)\n+        if a is not core.abstract_token and dtypes.issubdtype(a.dtype, dtypes.extended)\n+        else s\n+        for s, a in zip(executable._in_shardings, executable.in_avals)\n+    ]\n     fastpath_data = pxla.MeshExecutableFastpathData(\n-        executable.xla_executable, out_tree, executable._in_shardings,\n+        executable.xla_executable, out_tree, in_shardings,\n         executable._out_shardings, out_avals, out_committed, kept_var_bitvec,\n         executable.unsafe_call.in_handler.local_devices,\n         executable.unsafe_call.in_handler.input_indices)\n"
        },
        {
            "name": "prng.py",
            "path": "jax/_src/prng.py",
            "patches": [
                {
                    "old_start": 234,
                    "old_length": 7,
                    "new_start": 234,
                    "new_length": 7,
                    "hunk": "@@ -234,7 +234,7 @@ class PRNGKeyArray(jax.Array):\n   @property\n   def sharding(self):\n     phys_sharding = self._base_array.sharding\n-    return KeyTyRules.logical_op_sharding(self.aval, phys_sharding)\n+    return KeyTyRules.logical_sharding(self.aval, phys_sharding)\n \n   def _is_scalar(self):\n     base_ndim = len(self._impl.key_shape)\n"
                },
                {
                    "old_start": 345,
                    "old_length": 6,
                    "new_start": 345,
                    "new_length": 22,
                    "hunk": "@@ -345,6 +345,22 @@ def make_key_array_phys_sharding(aval, sharding):\n         sharding._device_assignment,\n         KeyTyRules.physical_hlo_sharding(aval, hlos))\n \n+\n+def get_logical_gspmd_sharding(aval, phys_sharding):\n+  key_shape = aval.dtype._impl.key_shape\n+  phys_hlo_sharding = phys_sharding._to_xla_hlo_sharding(\n+      aval.ndim + len(key_shape))\n+  partitions, num_replicas = op_shardings.get_num_ways_dim_sharded(\n+      phys_hlo_sharding)\n+  suffix = [] if num_replicas == 1 else [num_replicas]\n+  # Create logical sharding by cutting off the replicated trailing dims.\n+  logical_op_sharding = phys_hlo_sharding.to_proto().clone()\n+  tad = partitions[:-len(key_shape)] + suffix\n+  logical_op_sharding.tile_assignment_dimensions = tad\n+  return GSPMDSharding(phys_sharding._device_assignment,\n+                       xc.HloSharding.from_proto(logical_op_sharding))\n+\n+\n class KeyTyRules:\n \n   @staticmethod\n"
                },
                {
                    "old_start": 378,
                    "old_length": 7,
                    "new_start": 394,
                    "new_length": 12,
                    "hunk": "@@ -378,7 +394,12 @@ class KeyTyRules:\n     return xc.HloSharding.from_proto(new_op_sharding)\n \n   @staticmethod\n-  def logical_op_sharding(aval, phys_sharding) -> XLACompatibleSharding:\n+  def physical_sharding(\n+      aval, sharding: XLACompatibleSharding) -> XLACompatibleSharding:\n+    return make_key_array_phys_sharding(aval, sharding)\n+\n+  @staticmethod\n+  def logical_sharding(aval, phys_sharding) -> XLACompatibleSharding:\n     # The trailing dims should always be replicated.\n     aval.dtype._rules.check_replicated_trailing_dims(phys_sharding, aval)\n \n"
                },
                {
                    "old_start": 392,
                    "old_length": 23,
                    "new_start": 413,
                    "new_length": 11,
                    "hunk": "@@ -392,23 +413,11 @@ class KeyTyRules:\n       return PmapSharding(devices=phys_sharding.devices,\n                           sharding_spec=logical_sharding_spec)\n     elif isinstance(phys_sharding, NamedSharding):\n-      key_shape = aval.dtype._impl.key_shape\n-      return pxla.create_mesh_pspec_sharding(\n-          phys_sharding.mesh,\n-          PartitionSpec(*phys_sharding.spec[:-len(key_shape)]))\n+      logical_gs = get_logical_gspmd_sharding(aval, phys_sharding)\n+      return pxla._gspmd_to_named_sharding_via_mesh(\n+          logical_gs, phys_sharding.mesh)\n     else:\n-      key_shape = aval.dtype._impl.key_shape\n-      phys_hlo_sharding = phys_sharding._to_xla_hlo_sharding(\n-          aval.ndim + len(key_shape))\n-      partitions, num_replicas = op_shardings.get_num_ways_dim_sharded(\n-          phys_hlo_sharding)\n-      suffix = [] if num_replicas == 1 else [num_replicas]\n-      # Create logical sharding by cutting off the replicated trailing dims.\n-      logical_op_sharding = phys_hlo_sharding.to_proto().clone()\n-      tad = partitions[:-len(key_shape)] + suffix\n-      logical_op_sharding.tile_assignment_dimensions = tad\n-      return GSPMDSharding(phys_sharding._device_assignment,\n-                           xc.HloSharding.from_proto(logical_op_sharding))\n+      return get_logical_gspmd_sharding(aval, phys_sharding)\n \n   @staticmethod\n   def result_handler(sticky_device, aval):\n"
                }
            ],
            "whole_deleted": "-    return KeyTyRules.logical_op_sharding(self.aval, phys_sharding)\n-  def logical_op_sharding(aval, phys_sharding) -> XLACompatibleSharding:\n-      key_shape = aval.dtype._impl.key_shape\n-      return pxla.create_mesh_pspec_sharding(\n-          phys_sharding.mesh,\n-          PartitionSpec(*phys_sharding.spec[:-len(key_shape)]))\n-      key_shape = aval.dtype._impl.key_shape\n-      phys_hlo_sharding = phys_sharding._to_xla_hlo_sharding(\n-          aval.ndim + len(key_shape))\n-      partitions, num_replicas = op_shardings.get_num_ways_dim_sharded(\n-          phys_hlo_sharding)\n-      suffix = [] if num_replicas == 1 else [num_replicas]\n-      # Create logical sharding by cutting off the replicated trailing dims.\n-      logical_op_sharding = phys_hlo_sharding.to_proto().clone()\n-      tad = partitions[:-len(key_shape)] + suffix\n-      logical_op_sharding.tile_assignment_dimensions = tad\n-      return GSPMDSharding(phys_sharding._device_assignment,\n-                           xc.HloSharding.from_proto(logical_op_sharding))\n",
            "whole_added": "+    return KeyTyRules.logical_sharding(self.aval, phys_sharding)\n+\n+def get_logical_gspmd_sharding(aval, phys_sharding):\n+  key_shape = aval.dtype._impl.key_shape\n+  phys_hlo_sharding = phys_sharding._to_xla_hlo_sharding(\n+      aval.ndim + len(key_shape))\n+  partitions, num_replicas = op_shardings.get_num_ways_dim_sharded(\n+      phys_hlo_sharding)\n+  suffix = [] if num_replicas == 1 else [num_replicas]\n+  # Create logical sharding by cutting off the replicated trailing dims.\n+  logical_op_sharding = phys_hlo_sharding.to_proto().clone()\n+  tad = partitions[:-len(key_shape)] + suffix\n+  logical_op_sharding.tile_assignment_dimensions = tad\n+  return GSPMDSharding(phys_sharding._device_assignment,\n+                       xc.HloSharding.from_proto(logical_op_sharding))\n+\n+\n+  def physical_sharding(\n+      aval, sharding: XLACompatibleSharding) -> XLACompatibleSharding:\n+    return make_key_array_phys_sharding(aval, sharding)\n+\n+  @staticmethod\n+  def logical_sharding(aval, phys_sharding) -> XLACompatibleSharding:\n+      logical_gs = get_logical_gspmd_sharding(aval, phys_sharding)\n+      return pxla._gspmd_to_named_sharding_via_mesh(\n+          logical_gs, phys_sharding.mesh)\n+      return get_logical_gspmd_sharding(aval, phys_sharding)\n",
            "whole_hunk": "@@ -234,7 +234,7 @@ class PRNGKeyArray(jax.Array):\n   @property\n   def sharding(self):\n     phys_sharding = self._base_array.sharding\n-    return KeyTyRules.logical_op_sharding(self.aval, phys_sharding)\n+    return KeyTyRules.logical_sharding(self.aval, phys_sharding)\n \n   def _is_scalar(self):\n     base_ndim = len(self._impl.key_shape)\n@@ -345,6 +345,22 @@ def make_key_array_phys_sharding(aval, sharding):\n         sharding._device_assignment,\n         KeyTyRules.physical_hlo_sharding(aval, hlos))\n \n+\n+def get_logical_gspmd_sharding(aval, phys_sharding):\n+  key_shape = aval.dtype._impl.key_shape\n+  phys_hlo_sharding = phys_sharding._to_xla_hlo_sharding(\n+      aval.ndim + len(key_shape))\n+  partitions, num_replicas = op_shardings.get_num_ways_dim_sharded(\n+      phys_hlo_sharding)\n+  suffix = [] if num_replicas == 1 else [num_replicas]\n+  # Create logical sharding by cutting off the replicated trailing dims.\n+  logical_op_sharding = phys_hlo_sharding.to_proto().clone()\n+  tad = partitions[:-len(key_shape)] + suffix\n+  logical_op_sharding.tile_assignment_dimensions = tad\n+  return GSPMDSharding(phys_sharding._device_assignment,\n+                       xc.HloSharding.from_proto(logical_op_sharding))\n+\n+\n class KeyTyRules:\n \n   @staticmethod\n@@ -378,7 +394,12 @@ class KeyTyRules:\n     return xc.HloSharding.from_proto(new_op_sharding)\n \n   @staticmethod\n-  def logical_op_sharding(aval, phys_sharding) -> XLACompatibleSharding:\n+  def physical_sharding(\n+      aval, sharding: XLACompatibleSharding) -> XLACompatibleSharding:\n+    return make_key_array_phys_sharding(aval, sharding)\n+\n+  @staticmethod\n+  def logical_sharding(aval, phys_sharding) -> XLACompatibleSharding:\n     # The trailing dims should always be replicated.\n     aval.dtype._rules.check_replicated_trailing_dims(phys_sharding, aval)\n \n@@ -392,23 +413,11 @@ class KeyTyRules:\n       return PmapSharding(devices=phys_sharding.devices,\n                           sharding_spec=logical_sharding_spec)\n     elif isinstance(phys_sharding, NamedSharding):\n-      key_shape = aval.dtype._impl.key_shape\n-      return pxla.create_mesh_pspec_sharding(\n-          phys_sharding.mesh,\n-          PartitionSpec(*phys_sharding.spec[:-len(key_shape)]))\n+      logical_gs = get_logical_gspmd_sharding(aval, phys_sharding)\n+      return pxla._gspmd_to_named_sharding_via_mesh(\n+          logical_gs, phys_sharding.mesh)\n     else:\n-      key_shape = aval.dtype._impl.key_shape\n-      phys_hlo_sharding = phys_sharding._to_xla_hlo_sharding(\n-          aval.ndim + len(key_shape))\n-      partitions, num_replicas = op_shardings.get_num_ways_dim_sharded(\n-          phys_hlo_sharding)\n-      suffix = [] if num_replicas == 1 else [num_replicas]\n-      # Create logical sharding by cutting off the replicated trailing dims.\n-      logical_op_sharding = phys_hlo_sharding.to_proto().clone()\n-      tad = partitions[:-len(key_shape)] + suffix\n-      logical_op_sharding.tile_assignment_dimensions = tad\n-      return GSPMDSharding(phys_sharding._device_assignment,\n-                           xc.HloSharding.from_proto(logical_op_sharding))\n+      return get_logical_gspmd_sharding(aval, phys_sharding)\n \n   @staticmethod\n   def result_handler(sticky_device, aval):\n"
        },
        {
            "name": "test_util.py",
            "path": "jax/_src/test_util.py",
            "patches": [
                {
                    "old_start": 981,
                    "old_length": 7,
                    "new_start": 981,
                    "new_length": 6,
                    "hunk": "@@ -981,7 +981,6 @@ class JaxTestCase(parameterized.TestCase):\n   \"\"\"Base class for JAX tests including numerical checks and boilerplate.\"\"\"\n   _default_config = {\n     'jax_enable_checks': True,\n-    'jax_enable_key_reuse_checks': True,\n     'jax_numpy_dtype_promotion': 'strict',\n     'jax_numpy_rank_promotion': 'raise',\n     'jax_traceback_filtering': 'off',\n"
                }
            ],
            "whole_deleted": "-    'jax_enable_key_reuse_checks': True,\n",
            "whole_added": "",
            "whole_hunk": "@@ -981,7 +981,6 @@ class JaxTestCase(parameterized.TestCase):\n   \"\"\"Base class for JAX tests including numerical checks and boilerplate.\"\"\"\n   _default_config = {\n     'jax_enable_checks': True,\n-    'jax_enable_key_reuse_checks': True,\n     'jax_numpy_dtype_promotion': 'strict',\n     'jax_numpy_rank_promotion': 'raise',\n     'jax_traceback_filtering': 'off',\n"
        },
        {
            "name": "core_test.py",
            "path": "tests/core_test.py",
            "patches": [
                {
                    "old_start": 27,
                    "old_length": 7,
                    "new_start": 27,
                    "new_length": 7,
                    "hunk": "@@ -27,7 +27,7 @@ from jax import lax\n from jax import numpy as jnp\n from jax import jvp, linearize, vjp, jit, make_jaxpr\n from jax.api_util import flatten_fun_nokwargs\n-from jax import config\n+from jax._src import config\n \n from jax._src import core\n from jax._src import linear_util as lu\n"
                },
                {
                    "old_start": 750,
                    "old_length": 16,
                    "new_start": 750,
                    "new_length": 17,
                    "hunk": "@@ -750,16 +750,17 @@ class DynamicShapesTest(jtu.JaxTestCase):\n       core.check_jaxpr(jaxpr)\n \n   def test_check_jaxpr_key_reuse(self):\n-    try:\n-      from jax.experimental.key_reuse import KeyReuseError\n-    except ImportError:\n-      self.skipTest(\"Test requires jax.experimental.key_reuse\")\n-    def f(seed):\n-      key = jax.random.key(seed)\n-      return jax.random.uniform(key) + jax.random.normal(key)\n-    with jax.enable_checks(True):\n-      with self.assertRaises(KeyReuseError):\n-        jax.jit(f)(0)\n+    with config.enable_key_reuse_checks(True):\n+      try:\n+        from jax.experimental.key_reuse import KeyReuseError\n+      except ImportError:\n+        self.skipTest(\"Test requires jax.experimental.key_reuse\")\n+      def f(seed):\n+        key = jax.random.key(seed)\n+        return jax.random.uniform(key) + jax.random.normal(key)\n+      with jax.enable_checks(True):\n+        with self.assertRaises(KeyReuseError):\n+          jax.jit(f)(0)\n \n \n if __name__ == '__main__':\n"
                }
            ],
            "whole_deleted": "-from jax import config\n-    try:\n-      from jax.experimental.key_reuse import KeyReuseError\n-    except ImportError:\n-      self.skipTest(\"Test requires jax.experimental.key_reuse\")\n-    def f(seed):\n-      key = jax.random.key(seed)\n-      return jax.random.uniform(key) + jax.random.normal(key)\n-    with jax.enable_checks(True):\n-      with self.assertRaises(KeyReuseError):\n-        jax.jit(f)(0)\n",
            "whole_added": "+from jax._src import config\n+    with config.enable_key_reuse_checks(True):\n+      try:\n+        from jax.experimental.key_reuse import KeyReuseError\n+      except ImportError:\n+        self.skipTest(\"Test requires jax.experimental.key_reuse\")\n+      def f(seed):\n+        key = jax.random.key(seed)\n+        return jax.random.uniform(key) + jax.random.normal(key)\n+      with jax.enable_checks(True):\n+        with self.assertRaises(KeyReuseError):\n+          jax.jit(f)(0)\n",
            "whole_hunk": "@@ -27,7 +27,7 @@ from jax import lax\n from jax import numpy as jnp\n from jax import jvp, linearize, vjp, jit, make_jaxpr\n from jax.api_util import flatten_fun_nokwargs\n-from jax import config\n+from jax._src import config\n \n from jax._src import core\n from jax._src import linear_util as lu\n@@ -750,16 +750,17 @@ class DynamicShapesTest(jtu.JaxTestCase):\n       core.check_jaxpr(jaxpr)\n \n   def test_check_jaxpr_key_reuse(self):\n-    try:\n-      from jax.experimental.key_reuse import KeyReuseError\n-    except ImportError:\n-      self.skipTest(\"Test requires jax.experimental.key_reuse\")\n-    def f(seed):\n-      key = jax.random.key(seed)\n-      return jax.random.uniform(key) + jax.random.normal(key)\n-    with jax.enable_checks(True):\n-      with self.assertRaises(KeyReuseError):\n-        jax.jit(f)(0)\n+    with config.enable_key_reuse_checks(True):\n+      try:\n+        from jax.experimental.key_reuse import KeyReuseError\n+      except ImportError:\n+        self.skipTest(\"Test requires jax.experimental.key_reuse\")\n+      def f(seed):\n+        key = jax.random.key(seed)\n+        return jax.random.uniform(key) + jax.random.normal(key)\n+      with jax.enable_checks(True):\n+        with self.assertRaises(KeyReuseError):\n+          jax.jit(f)(0)\n \n \n if __name__ == '__main__':\n"
        },
        {
            "name": "key_reuse_test.py",
            "path": "tests/key_reuse_test.py",
            "patches": [
                {
                    "old_start": 586,
                    "old_length": 6,
                    "new_start": 586,
                    "new_length": 7,
                    "hunk": "@@ -586,6 +586,7 @@ class KeyReuseIntegrationTest(jtu.JaxTestCase):\n     self.check_key_reuse(jax.grad(f_good), x, key)\n \n \n+@jtu.with_config(jax_enable_key_reuse_checks=True)\n class KeyReuseEager(jtu.JaxTestCase):\n   jit_msg = \"Previously-consumed key passed to jit-compiled function at index 0\"\n   eager_bits_msg = \"Previously-consumed key passed to random_bits at index 0\"\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+@jtu.with_config(jax_enable_key_reuse_checks=True)\n",
            "whole_hunk": "@@ -586,6 +586,7 @@ class KeyReuseIntegrationTest(jtu.JaxTestCase):\n     self.check_key_reuse(jax.grad(f_good), x, key)\n \n \n+@jtu.with_config(jax_enable_key_reuse_checks=True)\n class KeyReuseEager(jtu.JaxTestCase):\n   jit_msg = \"Previously-consumed key passed to jit-compiled function at index 0\"\n   eager_bits_msg = \"Previously-consumed key passed to random_bits at index 0\"\n"
        },
        {
            "name": "lax_test.py",
            "path": "tests/lax_test.py",
            "patches": [
                {
                    "old_start": 2979,
                    "old_length": 9,
                    "new_start": 2979,
                    "new_length": 13,
                    "hunk": "@@ -2979,9 +2979,13 @@ class FooTyRules:\n     return xc.HloSharding.from_proto(new_op_sharding)\n \n   @staticmethod\n-  def logical_op_sharding(aval, phys_sharding):\n+  def logical_sharding(aval, phys_sharding):\n     return phys_sharding\n \n+  @staticmethod\n+  def physical_sharding(aval, sharding):\n+    return sharding\n+\n   @staticmethod\n   def result_handler(sticky_device, aval):\n     def handler(_, buf):\n"
                }
            ],
            "whole_deleted": "-  def logical_op_sharding(aval, phys_sharding):\n",
            "whole_added": "+  def logical_sharding(aval, phys_sharding):\n+  @staticmethod\n+  def physical_sharding(aval, sharding):\n+    return sharding\n+\n",
            "whole_hunk": "@@ -2979,9 +2979,13 @@ class FooTyRules:\n     return xc.HloSharding.from_proto(new_op_sharding)\n \n   @staticmethod\n-  def logical_op_sharding(aval, phys_sharding):\n+  def logical_sharding(aval, phys_sharding):\n     return phys_sharding\n \n+  @staticmethod\n+  def physical_sharding(aval, sharding):\n+    return sharding\n+\n   @staticmethod\n   def result_handler(sticky_device, aval):\n     def handler(_, buf):\n"
        },
        {
            "name": "pjit_test.py",
            "path": "tests/pjit_test.py",
            "patches": [
                {
                    "old_start": 3897,
                    "old_length": 6,
                    "new_start": 3897,
                    "new_length": 30,
                    "hunk": "@@ -3897,6 +3897,30 @@ class ArrayPjitTest(jtu.JaxTestCase):\n     lowered_text = make_keys.lower(seeds).as_text()\n     self.assertIn('unspecified_dims=[0,1]', lowered_text)\n \n+  def test_partial_sharded_prng_key_inp(self):\n+    input_shape = (8, 2, 2)\n+    mesh = jtu.create_global_mesh((2, 2, 2), ('x', 'y', 'z'))\n+    spec = P('x', 'y', None)\n+\n+    seeds, _ = create_array(input_shape, mesh, spec, dtype=np.uint32)\n+\n+    @jax.jit\n+    def make_keys(seeds):\n+      make_key = partial(prng.random_seed, impl=prng.threefry_prng_impl)\n+      key = make_key(seeds)\n+      return key.T\n+\n+    make_keys(seeds)\n+    out = make_keys(seeds)  # cpp dispatch\n+    self.assertEqual(out.sharding, NamedSharding(mesh, P(None, 'y', 'x')))\n+\n+    base_array = jax.random.key_data(out)\n+    self.assertEqual(base_array.shape, (2, 2, 8, 2))\n+    self.assertEqual(base_array.sharding, NamedSharding(mesh, P(None, 'y', 'x')))\n+\n+    lowered_text = make_keys.lower(seeds).as_text()\n+    self.assertIn('unspecified_dims=[0,1,2]', lowered_text)\n+\n   def test_jit_partially_specified_shardings(self):\n \n     mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  def test_partial_sharded_prng_key_inp(self):\n+    input_shape = (8, 2, 2)\n+    mesh = jtu.create_global_mesh((2, 2, 2), ('x', 'y', 'z'))\n+    spec = P('x', 'y', None)\n+\n+    seeds, _ = create_array(input_shape, mesh, spec, dtype=np.uint32)\n+\n+    @jax.jit\n+    def make_keys(seeds):\n+      make_key = partial(prng.random_seed, impl=prng.threefry_prng_impl)\n+      key = make_key(seeds)\n+      return key.T\n+\n+    make_keys(seeds)\n+    out = make_keys(seeds)  # cpp dispatch\n+    self.assertEqual(out.sharding, NamedSharding(mesh, P(None, 'y', 'x')))\n+\n+    base_array = jax.random.key_data(out)\n+    self.assertEqual(base_array.shape, (2, 2, 8, 2))\n+    self.assertEqual(base_array.sharding, NamedSharding(mesh, P(None, 'y', 'x')))\n+\n+    lowered_text = make_keys.lower(seeds).as_text()\n+    self.assertIn('unspecified_dims=[0,1,2]', lowered_text)\n+\n",
            "whole_hunk": "@@ -3897,6 +3897,30 @@ class ArrayPjitTest(jtu.JaxTestCase):\n     lowered_text = make_keys.lower(seeds).as_text()\n     self.assertIn('unspecified_dims=[0,1]', lowered_text)\n \n+  def test_partial_sharded_prng_key_inp(self):\n+    input_shape = (8, 2, 2)\n+    mesh = jtu.create_global_mesh((2, 2, 2), ('x', 'y', 'z'))\n+    spec = P('x', 'y', None)\n+\n+    seeds, _ = create_array(input_shape, mesh, spec, dtype=np.uint32)\n+\n+    @jax.jit\n+    def make_keys(seeds):\n+      make_key = partial(prng.random_seed, impl=prng.threefry_prng_impl)\n+      key = make_key(seeds)\n+      return key.T\n+\n+    make_keys(seeds)\n+    out = make_keys(seeds)  # cpp dispatch\n+    self.assertEqual(out.sharding, NamedSharding(mesh, P(None, 'y', 'x')))\n+\n+    base_array = jax.random.key_data(out)\n+    self.assertEqual(base_array.shape, (2, 2, 8, 2))\n+    self.assertEqual(base_array.sharding, NamedSharding(mesh, P(None, 'y', 'x')))\n+\n+    lowered_text = make_keys.lower(seeds).as_text()\n+    self.assertIn('unspecified_dims=[0,1,2]', lowered_text)\n+\n   def test_jit_partially_specified_shardings(self):\n \n     mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))"
        }
    ]
},
{
    "Id": 80,
    "commit_link": "https://github.com/google/jax/commit/84d11d7b114485453fca8814364486daf224fc49",
    "date": "2024-03-04T13:32:35-08:00",
    "message": "[key reuse] don't consume on equality check",
    "changes": [
        {
            "name": "_core.py",
            "path": "jax/experimental/key_reuse/_core.py",
            "patches": [
                {
                    "old_start": 83,
                    "old_length": 6,
                    "new_start": 83,
                    "new_length": 9,
                    "hunk": "@@ -83,6 +83,9 @@ key_reuse_signatures[lax.dynamic_slice_p] = KeyReuseSignature([], [], [Forward(0\n key_reuse_signatures[lax.dynamic_update_slice_p] = KeyReuseSignature([Sink(1)], [], [Forward(0, 0)])\n key_reuse_signatures[lax.gather_p] = KeyReuseSignature([], [], [Forward(0, 0)])\n key_reuse_signatures[lax.scatter_p] = KeyReuseSignature([Sink(2)], [], [Forward(0, 0)])\n+# Equality checks don't consume\n+key_reuse_signatures[lax.eq_p] = KeyReuseSignature([], [], [])\n+key_reuse_signatures[lax.ne_p] = KeyReuseSignature([], [], [])\n \n # Rules which require more dynamic logic.\n key_reuse_signatures_dynamic: dict[core.Primitive, Callable[..., KeyReuseSignature]] = {}\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+# Equality checks don't consume\n+key_reuse_signatures[lax.eq_p] = KeyReuseSignature([], [], [])\n+key_reuse_signatures[lax.ne_p] = KeyReuseSignature([], [], [])\n",
            "whole_hunk": "@@ -83,6 +83,9 @@ key_reuse_signatures[lax.dynamic_slice_p] = KeyReuseSignature([], [], [Forward(0\n key_reuse_signatures[lax.dynamic_update_slice_p] = KeyReuseSignature([Sink(1)], [], [Forward(0, 0)])\n key_reuse_signatures[lax.gather_p] = KeyReuseSignature([], [], [Forward(0, 0)])\n key_reuse_signatures[lax.scatter_p] = KeyReuseSignature([Sink(2)], [], [Forward(0, 0)])\n+# Equality checks don't consume\n+key_reuse_signatures[lax.eq_p] = KeyReuseSignature([], [], [])\n+key_reuse_signatures[lax.ne_p] = KeyReuseSignature([], [], [])\n \n # Rules which require more dynamic logic.\n key_reuse_signatures_dynamic: dict[core.Primitive, Callable[..., KeyReuseSignature]] = {}\n"
        },
        {
            "name": "key_reuse_test.py",
            "path": "tests/key_reuse_test.py",
            "patches": [
                {
                    "old_start": 14,
                    "old_length": 6,
                    "new_start": 14,
                    "new_length": 7,
                    "hunk": "@@ -14,6 +14,7 @@\n \n from absl.testing import absltest, parameterized\n from functools import partial\n+import operator\n \n import numpy as np\n import jax\n"
                },
                {
                    "old_start": 216,
                    "old_length": 6,
                    "new_start": 217,
                    "new_length": 17,
                    "hunk": "@@ -216,6 +217,17 @@ class KeyReuseUnitTestWithForwarding(jtu.JaxTestCase):\n       assert_consumed(keys, np.array([True, True]))\n     self.check_key_reuse(f, jax.random.split(jax.random.key(0)))\n \n+  @parameterized.parameters(operator.eq, operator.ne)\n+  def test_equality_checks(self, op):\n+    def f(key1, key2):\n+      assert_unconsumed(key1)\n+      assert_unconsumed(key2)\n+      result = op(key1, key2)\n+      assert_unconsumed(key1)\n+      assert_unconsumed(key2)\n+      return result\n+    self.check_key_reuse(f, jax.random.key(0), jax.random.key(1))\n+\n   def test_jit_can_consume_input(self):\n     def f(key):\n       assert_unconsumed(key)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+import operator\n+  @parameterized.parameters(operator.eq, operator.ne)\n+  def test_equality_checks(self, op):\n+    def f(key1, key2):\n+      assert_unconsumed(key1)\n+      assert_unconsumed(key2)\n+      result = op(key1, key2)\n+      assert_unconsumed(key1)\n+      assert_unconsumed(key2)\n+      return result\n+    self.check_key_reuse(f, jax.random.key(0), jax.random.key(1))\n+\n",
            "whole_hunk": "@@ -14,6 +14,7 @@\n \n from absl.testing import absltest, parameterized\n from functools import partial\n+import operator\n \n import numpy as np\n import jax\n@@ -216,6 +217,17 @@ class KeyReuseUnitTestWithForwarding(jtu.JaxTestCase):\n       assert_consumed(keys, np.array([True, True]))\n     self.check_key_reuse(f, jax.random.split(jax.random.key(0)))\n \n+  @parameterized.parameters(operator.eq, operator.ne)\n+  def test_equality_checks(self, op):\n+    def f(key1, key2):\n+      assert_unconsumed(key1)\n+      assert_unconsumed(key2)\n+      result = op(key1, key2)\n+      assert_unconsumed(key1)\n+      assert_unconsumed(key2)\n+      return result\n+    self.check_key_reuse(f, jax.random.key(0), jax.random.key(1))\n+\n   def test_jit_can_consume_input(self):\n     def f(key):\n       assert_unconsumed(key)"
        }
    ]
},
{
    "Id": 82,
    "commit_link": "https://github.com/google/jax/commit/087f99a31c284df6b0f3a81cab3610e104accc64",
    "date": "2024-02-29T15:15:06-08:00",
    "message": "Support mocking number of GPUs in CUDA plugin.\n\nAlso move reading jax config value to be right before the client is created. Previously they were read before calling register_plugin, which happens during import and before any call of jax.config.update.\n\nThe decorator in mock_gpu_test was used wrongly. jtu.run_on_devices will create the client before jax.config.update is called, which is not desired. Remove the decorator will not fail CPU/TPU tests because the mesh will check the num_shard and the number of devices in the client and skip it if it does not match.\n\ngenerate_pjrt_gpu_plugin_options is only used in places that do not require compatibility so do not need to update xla_client version.\n\nPiperOrigin-RevId: 611610915",
    "changes": [
        {
            "name": "xla_bridge.py",
            "path": "jax/_src/xla_bridge.py",
            "patches": [
                {
                    "old_start": 485,
                    "old_length": 6,
                    "new_start": 485,
                    "new_length": 20,
                    "hunk": "@@ -485,6 +485,20 @@ def discover_pjrt_plugins() -> None:\n                          \"calling %s.initialize()\", plugin_module_name)\n \n \n+def _options_from_jax_configs(plugin_name):\n+  if plugin_name != \"cuda\":\n+    return {}\n+\n+  options = {}\n+  visible_devices = CUDA_VISIBLE_DEVICES.value\n+  if visible_devices != 'all':\n+    options['visible_devices'] = [int(x) for x in visible_devices.split(',')]\n+  options['enable_mock_nccl'] = _USE_MOCK_GPU_CLIENT.value\n+  if options['enable_mock_nccl']:\n+    options['num_nodes'] = _MOCK_NUM_GPUS.value\n+  return options\n+\n+\n # TODO(b/261345120): decide on a public name and expose a public method which is\n # an alias of this method.\n def register_plugin(\n"
                },
                {
                    "old_start": 509,
                    "old_length": 15,
                    "new_start": 523,
                    "new_length": 19,
                    "hunk": "@@ -509,15 +523,19 @@ def register_plugin(\n   def factory():\n     if not xla_client.pjrt_plugin_initialized(plugin_name):\n       xla_client.initialize_pjrt_plugin(plugin_name)\n-\n+    updated_options = {}\n+    if options is not None:\n+      updated_options.update(options)\n+    updated_options.update(_options_from_jax_configs(plugin_name))\n     if distributed.global_state.client is None:\n-      return xla_client.make_c_api_client(plugin_name, options, None)\n+      return xla_client.make_c_api_client(plugin_name, updated_options, None)\n+\n     distribute_options = {\n         'node_id': distributed.global_state.process_id,\n         'num_nodes': distributed.global_state.num_processes,\n     }\n     if options is not None:\n-      distribute_options.update(options)\n+      distribute_options.update(updated_options)\n     return xla_client.make_c_api_client(\n         plugin_name, distribute_options, distributed.global_state.client\n     )\n"
                }
            ],
            "whole_deleted": "-\n-      return xla_client.make_c_api_client(plugin_name, options, None)\n-      distribute_options.update(options)\n",
            "whole_added": "+def _options_from_jax_configs(plugin_name):\n+  if plugin_name != \"cuda\":\n+    return {}\n+\n+  options = {}\n+  visible_devices = CUDA_VISIBLE_DEVICES.value\n+  if visible_devices != 'all':\n+    options['visible_devices'] = [int(x) for x in visible_devices.split(',')]\n+  options['enable_mock_nccl'] = _USE_MOCK_GPU_CLIENT.value\n+  if options['enable_mock_nccl']:\n+    options['num_nodes'] = _MOCK_NUM_GPUS.value\n+  return options\n+\n+\n+    updated_options = {}\n+    if options is not None:\n+      updated_options.update(options)\n+    updated_options.update(_options_from_jax_configs(plugin_name))\n+      return xla_client.make_c_api_client(plugin_name, updated_options, None)\n+\n+      distribute_options.update(updated_options)\n",
            "whole_hunk": "@@ -485,6 +485,20 @@ def discover_pjrt_plugins() -> None:\n                          \"calling %s.initialize()\", plugin_module_name)\n \n \n+def _options_from_jax_configs(plugin_name):\n+  if plugin_name != \"cuda\":\n+    return {}\n+\n+  options = {}\n+  visible_devices = CUDA_VISIBLE_DEVICES.value\n+  if visible_devices != 'all':\n+    options['visible_devices'] = [int(x) for x in visible_devices.split(',')]\n+  options['enable_mock_nccl'] = _USE_MOCK_GPU_CLIENT.value\n+  if options['enable_mock_nccl']:\n+    options['num_nodes'] = _MOCK_NUM_GPUS.value\n+  return options\n+\n+\n # TODO(b/261345120): decide on a public name and expose a public method which is\n # an alias of this method.\n def register_plugin(\n@@ -509,15 +523,19 @@ def register_plugin(\n   def factory():\n     if not xla_client.pjrt_plugin_initialized(plugin_name):\n       xla_client.initialize_pjrt_plugin(plugin_name)\n-\n+    updated_options = {}\n+    if options is not None:\n+      updated_options.update(options)\n+    updated_options.update(_options_from_jax_configs(plugin_name))\n     if distributed.global_state.client is None:\n-      return xla_client.make_c_api_client(plugin_name, options, None)\n+      return xla_client.make_c_api_client(plugin_name, updated_options, None)\n+\n     distribute_options = {\n         'node_id': distributed.global_state.process_id,\n         'num_nodes': distributed.global_state.num_processes,\n     }\n     if options is not None:\n-      distribute_options.update(options)\n+      distribute_options.update(updated_options)\n     return xla_client.make_c_api_client(\n         plugin_name, distribute_options, distributed.global_state.client\n     )\n"
        },
        {
            "name": "__init__.py",
            "path": "jax_plugins/cuda/__init__.py",
            "patches": [
                {
                    "old_start": 75,
                    "old_length": 9,
                    "new_start": 75,
                    "new_length": 7,
                    "hunk": "@@ -75,9 +75,7 @@ def initialize():\n   if path is None:\n     return\n \n-  options = xla_client.generate_pjrt_gpu_plugin_options(\n-      xb.CUDA_VISIBLE_DEVICES.value\n-  )\n+  options = xla_client.generate_pjrt_gpu_plugin_options()\n   c_api = xb.register_plugin(\n       'cuda', priority=500, library_path=str(path), options=options\n   )\n"
                }
            ],
            "whole_deleted": "-  options = xla_client.generate_pjrt_gpu_plugin_options(\n-      xb.CUDA_VISIBLE_DEVICES.value\n-  )\n",
            "whole_added": "+  options = xla_client.generate_pjrt_gpu_plugin_options()\n",
            "whole_hunk": "@@ -75,9 +75,7 @@ def initialize():\n   if path is None:\n     return\n \n-  options = xla_client.generate_pjrt_gpu_plugin_options(\n-      xb.CUDA_VISIBLE_DEVICES.value\n-  )\n+  options = xla_client.generate_pjrt_gpu_plugin_options()\n   c_api = xb.register_plugin(\n       'cuda', priority=500, library_path=str(path), options=options\n   )\n"
        },
        {
            "name": "mock_gpu_test.py",
            "path": "tests/mock_gpu_test.py",
            "patches": [
                {
                    "old_start": 27,
                    "old_length": 7,
                    "new_start": 27,
                    "new_length": 6,
                    "hunk": "@@ -27,7 +27,6 @@ import numpy as np\n config.parse_flags_with_absl()\n \n \n-@jtu.run_on_devices('gpu')\n class MockGPUTest(jtu.JaxTestCase):\n \n   def setUp(self):\n"
                }
            ],
            "whole_deleted": "-@jtu.run_on_devices('gpu')\n",
            "whole_added": "",
            "whole_hunk": "@@ -27,7 +27,6 @@ import numpy as np\n config.parse_flags_with_absl()\n \n \n-@jtu.run_on_devices('gpu')\n class MockGPUTest(jtu.JaxTestCase):\n \n   def setUp(self):\n"
        },
        {
            "name": "xla_bridge_test.py",
            "path": "tests/xla_bridge_test.py",
            "patches": [
                {
                    "old_start": 194,
                    "old_length": 7,
                    "new_start": 194,
                    "new_length": 7,
                    "hunk": "@@ -194,7 +194,7 @@ class XlaBridgeTest(jtu.JaxTestCase):\n     self.assertIn(\"name2\", xb._backend_factories)\n     self.assertEqual(registration.priority, 400)\n     self.assertTrue(registration.experimental)\n-    mock_make.assert_called_once_with(\"name1\", None, None)\n+    mock_make.assert_called_once_with(\"name1\", {}, None)\n \n   def test_register_plugin_with_config(self):\n     test_json_file_path = os.path.join("
                }
            ],
            "whole_deleted": "-    mock_make.assert_called_once_with(\"name1\", None, None)\n",
            "whole_added": "+    mock_make.assert_called_once_with(\"name1\", {}, None)\n",
            "whole_hunk": "@@ -194,7 +194,7 @@ class XlaBridgeTest(jtu.JaxTestCase):\n     self.assertIn(\"name2\", xb._backend_factories)\n     self.assertEqual(registration.priority, 400)\n     self.assertTrue(registration.experimental)\n-    mock_make.assert_called_once_with(\"name1\", None, None)\n+    mock_make.assert_called_once_with(\"name1\", {}, None)\n \n   def test_register_plugin_with_config(self):\n     test_json_file_path = os.path.join("
        }
    ]
},
{
    "Id": 83,
    "commit_link": "https://github.com/google/jax/commit/550ce44afdef0e9ca4e0f80ea1c5a718265f787d",
    "date": "2024-02-28T17:03:37-08:00",
    "message": "Move the replicated trailing dims check inside logical_op_sharding\n\nPiperOrigin-RevId: 611277405",
    "changes": [
        {
            "name": "pxla.py",
            "path": "jax/_src/interpreters/pxla.py",
            "patches": [
                {
                    "old_start": 2637,
                    "old_length": 7,
                    "new_start": 2637,
                    "new_length": 6,
                    "hunk": "@@ -2637,7 +2637,6 @@ def _maybe_get_and_check_in_shardings(\n     if is_unspecified(orig):\n       if (aval is not core.abstract_token and\n           dtypes.issubdtype(aval.dtype, dtypes.extended)):\n-        aval.dtype._rules.check_replicated_trailing_dims(xla_s, aval)\n         xla_s = aval.dtype._rules.logical_op_sharding(aval, xla_s)\n       new_in_shardings.append(xla_s)\n     else:\n"
                },
                {
                    "old_start": 2655,
                    "old_length": 7,
                    "new_start": 2654,
                    "new_length": 7,
                    "hunk": "@@ -2655,7 +2654,7 @@ def _maybe_get_and_check_in_shardings(\n   return new_in_shardings\n \n \n-def _get_out_shardings_from_executable(\n+def _maybe_get_and_check_out_shardings(\n     xla_executable, out_shardings, device_assignment, global_out_avals,\n     num_ordered_effects, all_default_mem_kind\n   ):\n"
                },
                {
                    "old_start": 2671,
                    "old_length": 7,
                    "new_start": 2670,
                    "new_length": 6,
                    "hunk": "@@ -2671,7 +2670,6 @@ def _get_out_shardings_from_executable(\n     if is_unspecified(orig):\n       if (aval is not core.abstract_token and\n           dtypes.issubdtype(aval.dtype, dtypes.extended)):\n-        aval.dtype._rules.check_replicated_trailing_dims(xla_s, aval)\n         xla_s = aval.dtype._rules.logical_op_sharding(aval, xla_s)\n       new_out_shardings.append(xla_s)\n     else:\n"
                },
                {
                    "old_start": 2823,
                    "old_length": 7,
                    "new_start": 2821,
                    "new_length": 7,
                    "hunk": "@@ -2823,7 +2821,7 @@ class UnloadedMeshExecutable:\n           in_shardings = _maybe_get_and_check_in_shardings(\n               xla_executable, in_shardings, tuple(da), global_in_avals,\n               len(ordered_effects))\n-        out_shardings = _get_out_shardings_from_executable(\n+        out_shardings = _maybe_get_and_check_out_shardings(\n             xla_executable, out_shardings, tuple(da), global_out_avals,\n             len(ordered_effects), all_default_mem_kind)\n       else:\n"
                }
            ],
            "whole_deleted": "-        aval.dtype._rules.check_replicated_trailing_dims(xla_s, aval)\n-def _get_out_shardings_from_executable(\n-        aval.dtype._rules.check_replicated_trailing_dims(xla_s, aval)\n-        out_shardings = _get_out_shardings_from_executable(\n",
            "whole_added": "+def _maybe_get_and_check_out_shardings(\n+        out_shardings = _maybe_get_and_check_out_shardings(\n",
            "whole_hunk": "@@ -2637,7 +2637,6 @@ def _maybe_get_and_check_in_shardings(\n     if is_unspecified(orig):\n       if (aval is not core.abstract_token and\n           dtypes.issubdtype(aval.dtype, dtypes.extended)):\n-        aval.dtype._rules.check_replicated_trailing_dims(xla_s, aval)\n         xla_s = aval.dtype._rules.logical_op_sharding(aval, xla_s)\n       new_in_shardings.append(xla_s)\n     else:\n@@ -2655,7 +2654,7 @@ def _maybe_get_and_check_in_shardings(\n   return new_in_shardings\n \n \n-def _get_out_shardings_from_executable(\n+def _maybe_get_and_check_out_shardings(\n     xla_executable, out_shardings, device_assignment, global_out_avals,\n     num_ordered_effects, all_default_mem_kind\n   ):\n@@ -2671,7 +2670,6 @@ def _get_out_shardings_from_executable(\n     if is_unspecified(orig):\n       if (aval is not core.abstract_token and\n           dtypes.issubdtype(aval.dtype, dtypes.extended)):\n-        aval.dtype._rules.check_replicated_trailing_dims(xla_s, aval)\n         xla_s = aval.dtype._rules.logical_op_sharding(aval, xla_s)\n       new_out_shardings.append(xla_s)\n     else:\n@@ -2823,7 +2821,7 @@ class UnloadedMeshExecutable:\n           in_shardings = _maybe_get_and_check_in_shardings(\n               xla_executable, in_shardings, tuple(da), global_in_avals,\n               len(ordered_effects))\n-        out_shardings = _get_out_shardings_from_executable(\n+        out_shardings = _maybe_get_and_check_out_shardings(\n             xla_executable, out_shardings, tuple(da), global_out_avals,\n             len(ordered_effects), all_default_mem_kind)\n       else:\n"
        },
        {
            "name": "prng.py",
            "path": "jax/_src/prng.py",
            "patches": [
                {
                    "old_start": 375,
                    "old_length": 6,
                    "new_start": 375,
                    "new_length": 9,
                    "hunk": "@@ -375,6 +375,9 @@ class KeyTyRules:\n \n   @staticmethod\n   def logical_op_sharding(aval, phys_sharding) -> XLACompatibleSharding:\n+    # The trailing dims should always be replicated.\n+    aval.dtype._rules.check_replicated_trailing_dims(phys_sharding, aval)\n+\n     if dispatch.is_single_device_sharding(phys_sharding):\n       return phys_sharding\n     elif isinstance(phys_sharding, PmapSharding):\n"
                },
                {
                    "old_start": 475,
                    "old_length": 9,
                    "new_start": 478,
                    "new_length": 13,
                    "hunk": "@@ -475,9 +478,13 @@ class KeyTyRules:\n     return random_wrap(physical_result, impl=aval.dtype._impl)\n \n   @staticmethod\n-  def check_replicated_trailing_dims(sharding: GSPMDSharding, aval):\n-    partitions, _ = op_shardings.get_num_ways_dim_sharded(sharding._hlo_sharding)\n-    num_trailing_dims = core.physical_aval(aval).ndim - aval.ndim\n+  def check_replicated_trailing_dims(sharding: XLACompatibleSharding, aval):\n+    if isinstance(sharding, PmapSharding):\n+      return\n+    phys_aval = core.physical_aval(aval)\n+    hlo_s = sharding._to_xla_hlo_sharding(phys_aval.ndim)\n+    partitions, _ = op_shardings.get_num_ways_dim_sharded(hlo_s)\n+    num_trailing_dims = phys_aval.ndim - aval.ndim\n     if not all(i == 1 for i in partitions[-num_trailing_dims:]):\n       raise AssertionError(\n           \"The trailing dims of extended dtypes should be replicated. Got\""
                }
            ],
            "whole_deleted": "-  def check_replicated_trailing_dims(sharding: GSPMDSharding, aval):\n-    partitions, _ = op_shardings.get_num_ways_dim_sharded(sharding._hlo_sharding)\n-    num_trailing_dims = core.physical_aval(aval).ndim - aval.ndim\n",
            "whole_added": "+    # The trailing dims should always be replicated.\n+    aval.dtype._rules.check_replicated_trailing_dims(phys_sharding, aval)\n+\n+  def check_replicated_trailing_dims(sharding: XLACompatibleSharding, aval):\n+    if isinstance(sharding, PmapSharding):\n+      return\n+    phys_aval = core.physical_aval(aval)\n+    hlo_s = sharding._to_xla_hlo_sharding(phys_aval.ndim)\n+    partitions, _ = op_shardings.get_num_ways_dim_sharded(hlo_s)\n+    num_trailing_dims = phys_aval.ndim - aval.ndim\n",
            "whole_hunk": "@@ -375,6 +375,9 @@ class KeyTyRules:\n \n   @staticmethod\n   def logical_op_sharding(aval, phys_sharding) -> XLACompatibleSharding:\n+    # The trailing dims should always be replicated.\n+    aval.dtype._rules.check_replicated_trailing_dims(phys_sharding, aval)\n+\n     if dispatch.is_single_device_sharding(phys_sharding):\n       return phys_sharding\n     elif isinstance(phys_sharding, PmapSharding):\n@@ -475,9 +478,13 @@ class KeyTyRules:\n     return random_wrap(physical_result, impl=aval.dtype._impl)\n \n   @staticmethod\n-  def check_replicated_trailing_dims(sharding: GSPMDSharding, aval):\n-    partitions, _ = op_shardings.get_num_ways_dim_sharded(sharding._hlo_sharding)\n-    num_trailing_dims = core.physical_aval(aval).ndim - aval.ndim\n+  def check_replicated_trailing_dims(sharding: XLACompatibleSharding, aval):\n+    if isinstance(sharding, PmapSharding):\n+      return\n+    phys_aval = core.physical_aval(aval)\n+    hlo_s = sharding._to_xla_hlo_sharding(phys_aval.ndim)\n+    partitions, _ = op_shardings.get_num_ways_dim_sharded(hlo_s)\n+    num_trailing_dims = phys_aval.ndim - aval.ndim\n     if not all(i == 1 for i in partitions[-num_trailing_dims:]):\n       raise AssertionError(\n           \"The trailing dims of extended dtypes should be replicated. Got\""
        }
    ]
},
{
    "Id": 84,
    "commit_link": "https://github.com/google/jax/commit/3dbbfefef83c345de18e581f3e574b3aa8e94041",
    "date": "2024-02-27T15:25:10-08:00",
    "message": "[PJRT C API] Add a helper method to check whether the backend is cloud TPU built after certain date.\n\nSkip tests that are not intended to work with older version libtpu.\n\nPiperOrigin-RevId: 610892754",
    "changes": [
        {
            "name": "test_util.py",
            "path": "jax/_src/test_util.py",
            "patches": [
                {
                    "old_start": 15,
                    "old_length": 6,
                    "new_start": 15,
                    "new_length": 7,
                    "hunk": "@@ -15,6 +15,7 @@ from __future__ import annotations\n \n from collections.abc import Generator, Iterable, Sequence\n from contextlib import contextmanager, ExitStack\n+import datetime\n import inspect\n import io\n import functools\n"
                },
                {
                    "old_start": 366,
                    "old_length": 6,
                    "new_start": 367,
                    "new_length": 24,
                    "hunk": "@@ -366,6 +367,24 @@ def is_device_cuda():\n def is_cloud_tpu():\n   return running_in_cloud_tpu_vm\n \n+# Returns True if it is not cloud TPU. If it is cloud TPU, returns True if it is\n+# built at least `date``.\n+# TODO(b/327203806): after libtpu adds a XLA version and the oldest support\n+# libtpu contains the XLA version, remove using built time to skip tests.\n+def if_cloud_tpu_at_least(date: datetime.date):\n+  if not is_cloud_tpu():\n+    return True\n+  # The format of Cloud TPU platform_version is like:\n+  # PJRT C API\n+  # TFRT TPU v2\n+  # Built on Oct 30 2023 03:04:42 (1698660263) cl/577737722\n+  platform_version = xla_bridge.get_backend().platform_version.split('\\n')[-1]\n+  results = re.findall(r'\\(.*?\\)', platform_version)\n+  if len(results) != 1:\n+    return True\n+  build_date = date.fromtimestamp(int(results[0][1:-1]))\n+  return build_date >= date\n+\n def pjrt_c_api_version_at_least(major_version: int, minor_version: int):\n   pjrt_c_api_versions = xla_bridge.backend_pjrt_c_api_version()\n   if pjrt_c_api_versions is None:\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+import datetime\n+# Returns True if it is not cloud TPU. If it is cloud TPU, returns True if it is\n+# built at least `date``.\n+# TODO(b/327203806): after libtpu adds a XLA version and the oldest support\n+# libtpu contains the XLA version, remove using built time to skip tests.\n+def if_cloud_tpu_at_least(date: datetime.date):\n+  if not is_cloud_tpu():\n+    return True\n+  # The format of Cloud TPU platform_version is like:\n+  # PJRT C API\n+  # TFRT TPU v2\n+  # Built on Oct 30 2023 03:04:42 (1698660263) cl/577737722\n+  platform_version = xla_bridge.get_backend().platform_version.split('\\n')[-1]\n+  results = re.findall(r'\\(.*?\\)', platform_version)\n+  if len(results) != 1:\n+    return True\n+  build_date = date.fromtimestamp(int(results[0][1:-1]))\n+  return build_date >= date\n+\n",
            "whole_hunk": "@@ -15,6 +15,7 @@ from __future__ import annotations\n \n from collections.abc import Generator, Iterable, Sequence\n from contextlib import contextmanager, ExitStack\n+import datetime\n import inspect\n import io\n import functools\n@@ -366,6 +367,24 @@ def is_device_cuda():\n def is_cloud_tpu():\n   return running_in_cloud_tpu_vm\n \n+# Returns True if it is not cloud TPU. If it is cloud TPU, returns True if it is\n+# built at least `date``.\n+# TODO(b/327203806): after libtpu adds a XLA version and the oldest support\n+# libtpu contains the XLA version, remove using built time to skip tests.\n+def if_cloud_tpu_at_least(date: datetime.date):\n+  if not is_cloud_tpu():\n+    return True\n+  # The format of Cloud TPU platform_version is like:\n+  # PJRT C API\n+  # TFRT TPU v2\n+  # Built on Oct 30 2023 03:04:42 (1698660263) cl/577737722\n+  platform_version = xla_bridge.get_backend().platform_version.split('\\n')[-1]\n+  results = re.findall(r'\\(.*?\\)', platform_version)\n+  if len(results) != 1:\n+    return True\n+  build_date = date.fromtimestamp(int(results[0][1:-1]))\n+  return build_date >= date\n+\n def pjrt_c_api_version_at_least(major_version: int, minor_version: int):\n   pjrt_c_api_versions = xla_bridge.backend_pjrt_c_api_version()\n   if pjrt_c_api_versions is None:\n"
        },
        {
            "name": "memories_test.py",
            "path": "tests/memories_test.py",
            "patches": [
                {
                    "old_start": 12,
                    "old_length": 6,
                    "new_start": 12,
                    "new_length": 7,
                    "hunk": "@@ -12,6 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import datetime\n import functools\n import math\n from absl.testing import absltest\n"
                },
                {
                    "old_start": 1079,
                    "old_length": 6,
                    "new_start": 1080,
                    "new_length": 8,
                    "hunk": "@@ -1079,6 +1080,8 @@ class ActivationOffloadingTest(jtu.JaxTestCase):\n   def setUp(self):\n     if not jtu.test_device_matches([\"tpu\"]):\n       self.skipTest(\"Memories do not work on CPU and GPU backends yet.\")\n+    if not jtu.if_cloud_tpu_at_least(datetime.date(2024, 2, 23)):\n+      self.skipTest(\"Memories do not work on Cloud TPU older than 2024/02/23.\")\n     super().setUp()\n \n   def test_remat_jaxpr_offloadable(self):\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+import datetime\n+    if not jtu.if_cloud_tpu_at_least(datetime.date(2024, 2, 23)):\n+      self.skipTest(\"Memories do not work on Cloud TPU older than 2024/02/23.\")\n",
            "whole_hunk": "@@ -12,6 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import datetime\n import functools\n import math\n from absl.testing import absltest\n@@ -1079,6 +1080,8 @@ class ActivationOffloadingTest(jtu.JaxTestCase):\n   def setUp(self):\n     if not jtu.test_device_matches([\"tpu\"]):\n       self.skipTest(\"Memories do not work on CPU and GPU backends yet.\")\n+    if not jtu.if_cloud_tpu_at_least(datetime.date(2024, 2, 23)):\n+      self.skipTest(\"Memories do not work on Cloud TPU older than 2024/02/23.\")\n     super().setUp()\n \n   def test_remat_jaxpr_offloadable(self):\n"
        },
        {
            "name": "pallas_call_tpu_test.py",
            "path": "tests/pallas/pallas_call_tpu_test.py",
            "patches": [
                {
                    "old_start": 14,
                    "old_length": 6,
                    "new_start": 14,
                    "new_length": 7,
                    "hunk": "@@ -14,6 +14,7 @@\n \n \"\"\"Test TPU-specific extensions to pallas_call.\"\"\"\n \n+import datetime\n import functools\n from absl.testing import absltest\n from absl.testing import parameterized\n"
                },
                {
                    "old_start": 48,
                    "old_length": 6,
                    "new_start": 49,
                    "new_length": 8,
                    "hunk": "@@ -48,6 +49,8 @@ class PallasTPUTest(jtu.JaxTestCase):\n     super().setUp()\n     if not self.interpret and jtu.device_under_test() != 'tpu':\n       self.skipTest('Only interpret mode supported on non-TPU')\n+    if not jtu.if_cloud_tpu_at_least(datetime.date(2024, 2, 10)):\n+      self.skipTest('Does not work on Cloud TPU older than 2024/02/10.')\n \n   def pallas_call(self, *args, **kwargs):\n     return pl.pallas_call(*args, **kwargs, interpret=self.interpret)\n"
                },
                {
                    "old_start": 343,
                    "old_length": 6,
                    "new_start": 346,
                    "new_length": 8,
                    "hunk": "@@ -343,6 +346,8 @@ class PallasCallDynamicGridTest(PallasTPUTest):\n \n   # TODO(apaszke): Add tests for scalar_prefetch too\n   def test_dynamic_grid_scalar_input(self):\n+    if not jtu.if_cloud_tpu_at_least(datetime.date(2024, 2, 14)):\n+      self.skipTest('Does not work on Cloud TPU older than 2024/02/14.')\n     shape = (8, 128)\n     result_ty = jax.ShapeDtypeStruct(shape, jnp.float32)\n \n"
                },
                {
                    "old_start": 436,
                    "old_length": 6,
                    "new_start": 441,
                    "new_length": 9,
                    "hunk": "@@ -436,6 +441,9 @@ class PallasCallDynamicGridTest(PallasTPUTest):\n     )\n \n   def test_num_programs(self):\n+    if not jtu.if_cloud_tpu_at_least(datetime.date(2024, 2, 27)):\n+      self.skipTest('Does not work on Cloud TPU older than 2024/02/27.')\n+\n     def kernel(y_ref):\n       y_ref[0, 0] = pl.num_programs(0)\n \n"
                },
                {
                    "old_start": 451,
                    "old_length": 6,
                    "new_start": 459,
                    "new_length": 9,
                    "hunk": "@@ -451,6 +459,9 @@ class PallasCallDynamicGridTest(PallasTPUTest):\n     self.assertEqual(dynamic_kernel(4), 8)\n \n   def test_num_programs_block_spec(self):\n+    if not jtu.if_cloud_tpu_at_least(datetime.date(2024, 2, 27)):\n+      self.skipTest('Does not work on Cloud TPU older than 2024/02/27.')\n+\n     def kernel(x_ref, y_ref):\n       y_ref[...] = x_ref[...]\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+import datetime\n+    if not jtu.if_cloud_tpu_at_least(datetime.date(2024, 2, 10)):\n+      self.skipTest('Does not work on Cloud TPU older than 2024/02/10.')\n+    if not jtu.if_cloud_tpu_at_least(datetime.date(2024, 2, 14)):\n+      self.skipTest('Does not work on Cloud TPU older than 2024/02/14.')\n+    if not jtu.if_cloud_tpu_at_least(datetime.date(2024, 2, 27)):\n+      self.skipTest('Does not work on Cloud TPU older than 2024/02/27.')\n+\n+    if not jtu.if_cloud_tpu_at_least(datetime.date(2024, 2, 27)):\n+      self.skipTest('Does not work on Cloud TPU older than 2024/02/27.')\n+\n",
            "whole_hunk": "@@ -14,6 +14,7 @@\n \n \"\"\"Test TPU-specific extensions to pallas_call.\"\"\"\n \n+import datetime\n import functools\n from absl.testing import absltest\n from absl.testing import parameterized\n@@ -48,6 +49,8 @@ class PallasTPUTest(jtu.JaxTestCase):\n     super().setUp()\n     if not self.interpret and jtu.device_under_test() != 'tpu':\n       self.skipTest('Only interpret mode supported on non-TPU')\n+    if not jtu.if_cloud_tpu_at_least(datetime.date(2024, 2, 10)):\n+      self.skipTest('Does not work on Cloud TPU older than 2024/02/10.')\n \n   def pallas_call(self, *args, **kwargs):\n     return pl.pallas_call(*args, **kwargs, interpret=self.interpret)\n@@ -343,6 +346,8 @@ class PallasCallDynamicGridTest(PallasTPUTest):\n \n   # TODO(apaszke): Add tests for scalar_prefetch too\n   def test_dynamic_grid_scalar_input(self):\n+    if not jtu.if_cloud_tpu_at_least(datetime.date(2024, 2, 14)):\n+      self.skipTest('Does not work on Cloud TPU older than 2024/02/14.')\n     shape = (8, 128)\n     result_ty = jax.ShapeDtypeStruct(shape, jnp.float32)\n \n@@ -436,6 +441,9 @@ class PallasCallDynamicGridTest(PallasTPUTest):\n     )\n \n   def test_num_programs(self):\n+    if not jtu.if_cloud_tpu_at_least(datetime.date(2024, 2, 27)):\n+      self.skipTest('Does not work on Cloud TPU older than 2024/02/27.')\n+\n     def kernel(y_ref):\n       y_ref[0, 0] = pl.num_programs(0)\n \n@@ -451,6 +459,9 @@ class PallasCallDynamicGridTest(PallasTPUTest):\n     self.assertEqual(dynamic_kernel(4), 8)\n \n   def test_num_programs_block_spec(self):\n+    if not jtu.if_cloud_tpu_at_least(datetime.date(2024, 2, 27)):\n+      self.skipTest('Does not work on Cloud TPU older than 2024/02/27.')\n+\n     def kernel(x_ref, y_ref):\n       y_ref[...] = x_ref[...]\n \n"
        },
        {
            "name": "shard_alike_test.py",
            "path": "tests/shard_alike_test.py",
            "patches": [
                {
                    "old_start": 12,
                    "old_length": 6,
                    "new_start": 12,
                    "new_length": 7,
                    "hunk": "@@ -12,6 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import datetime\n import os\n \n import jax\n"
                },
                {
                    "old_start": 66,
                    "old_length": 6,
                    "new_start": 67,
                    "new_length": 8,
                    "hunk": "@@ -66,6 +67,8 @@ class ShardAlikeTest(jtu.JaxTestCase):\n     super().setUp()\n     if xla_extension_version < 227:\n       self.skipTest('Requires xla_extension_version >= 227')\n+    if not jtu.if_cloud_tpu_at_least(datetime.date(2024, 2, 23)):\n+      self.skipTest(\"Requires Cloud TPU older than 2024/02/23.\")\n \n   def test_basic(self):\n     mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+import datetime\n+    if not jtu.if_cloud_tpu_at_least(datetime.date(2024, 2, 23)):\n+      self.skipTest(\"Requires Cloud TPU older than 2024/02/23.\")\n",
            "whole_hunk": "@@ -12,6 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import datetime\n import os\n \n import jax\n@@ -66,6 +67,8 @@ class ShardAlikeTest(jtu.JaxTestCase):\n     super().setUp()\n     if xla_extension_version < 227:\n       self.skipTest('Requires xla_extension_version >= 227')\n+    if not jtu.if_cloud_tpu_at_least(datetime.date(2024, 2, 23)):\n+      self.skipTest(\"Requires Cloud TPU older than 2024/02/23.\")\n \n   def test_basic(self):\n     mesh = jtu.create_global_mesh((2, 2), ('x', 'y'))"
        }
    ]
},
{
    "Id": 85,
    "commit_link": "https://github.com/google/jax/commit/fdbee314d35b4539fd4977e1cf7dab8beb0c0a13",
    "date": "2024-02-27T11:30:10-08:00",
    "message": "Make JAX tests that check for errors from dict key comparators in pytrees more relaxed, in preparation for https://github.com/openxla/xla/pull/9529.\n\nPiperOrigin-RevId: 610819296",
    "changes": [
        {
            "name": "api_test.py",
            "path": "tests/api_test.py",
            "patches": [
                {
                    "old_start": 1329,
                    "old_length": 7,
                    "new_start": 1329,
                    "new_length": 9,
                    "hunk": "@@ -1329,7 +1329,9 @@ class JitTest(jtu.BufferDonationTestCase):\n     def f(d) -> float:\n       return d[E.A]\n \n-    with self.assertRaisesRegex(TypeError, \"'<' not supported.*\"):\n+    with self.assertRaisesRegex(\n+        (TypeError, ValueError),\n+        \"('<' not supported|Comparator raised exception).*\"):\n       f({E.A: 1.0, E.B: 2.0})\n \n   def test_jit_static_argnums_requires_type_equality(self):\n"
                }
            ],
            "whole_deleted": "-    with self.assertRaisesRegex(TypeError, \"'<' not supported.*\"):\n",
            "whole_added": "+    with self.assertRaisesRegex(\n+        (TypeError, ValueError),\n+        \"('<' not supported|Comparator raised exception).*\"):\n",
            "whole_hunk": "@@ -1329,7 +1329,9 @@ class JitTest(jtu.BufferDonationTestCase):\n     def f(d) -> float:\n       return d[E.A]\n \n-    with self.assertRaisesRegex(TypeError, \"'<' not supported.*\"):\n+    with self.assertRaisesRegex(\n+        (TypeError, ValueError),\n+        \"('<' not supported|Comparator raised exception).*\"):\n       f({E.A: 1.0, E.B: 2.0})\n \n   def test_jit_static_argnums_requires_type_equality(self):\n"
        },
        {
            "name": "tree_util_test.py",
            "path": "tests/tree_util_test.py",
            "patches": [
                {
                    "old_start": 582,
                    "old_length": 7,
                    "new_start": 582,
                    "new_length": 9,
                    "hunk": "@@ -582,7 +582,9 @@ class TreeTest(jtu.JaxTestCase):\n \n   def testDictKeysSortable(self):\n     d = {\"a\": 1, 2: \"b\"}\n-    with self.assertRaisesRegex(TypeError, \"'<' not supported\"):\n+    with self.assertRaisesRegex(\n+        (TypeError, ValueError),\n+        \"('<' not supported|Comparator raised exception).*\"):\n       _, _ = tree_util.tree_flatten(d)\n \n   def testFlattenDictKeyOrder(self):"
                }
            ],
            "whole_deleted": "-    with self.assertRaisesRegex(TypeError, \"'<' not supported\"):\n",
            "whole_added": "+    with self.assertRaisesRegex(\n+        (TypeError, ValueError),\n+        \"('<' not supported|Comparator raised exception).*\"):\n",
            "whole_hunk": "@@ -582,7 +582,9 @@ class TreeTest(jtu.JaxTestCase):\n \n   def testDictKeysSortable(self):\n     d = {\"a\": 1, 2: \"b\"}\n-    with self.assertRaisesRegex(TypeError, \"'<' not supported\"):\n+    with self.assertRaisesRegex(\n+        (TypeError, ValueError),\n+        \"('<' not supported|Comparator raised exception).*\"):\n       _, _ = tree_util.tree_flatten(d)\n \n   def testFlattenDictKeyOrder(self):"
        }
    ]
},
{
    "Id": 86,
    "commit_link": "https://github.com/google/jax/commit/57e34e1a2ce4610a93d87931b78b9c6898718549",
    "date": "2024-02-26T13:34:58-08:00",
    "message": "[Mosaic][NFC] Use `TypedValue<VectorType>` instead of `Value` for applicable arguments/return values in `disassemble` and `relayout`\n\nIdeally we would prefer `TypedValue<VectorType>` everywhere possible for static type checking. However, I tried the type for arrays of vregs, `xla::Array<Value>` to `xla::Array<TypedValue<VectorType>>` and ran into issues because MLIR support for arrays/ranges of `TypedValue`s seems lacking.\n\nFor example, I can't find a good way to get a `ValueRange` (which many op constructors take) from an array of `TypedValue`s without creating an intermediate vector of `Value`s. Perhaps an unsafe cast if we make the (probably not guaranteed) assumption that `sizeof(TypedValue)` equals `sizeof(Value)`.\n\nAlso note that MLIR itself uses untyped `Value`s for ranges of op results and operands even when the op definition declares them to be of a specific type.\n\nPiperOrigin-RevId: 610509743",
    "changes": [
        {
            "name": "tpu_dialect.cc",
            "path": "jaxlib/mosaic/dialect/tpu/integrations/c/tpu_dialect.cc",
            "patches": [
                {
                    "old_start": 349,
                    "old_length": 8,
                    "new_start": 349,
                    "new_length": 10,
                    "hunk": "@@ -349,8 +349,10 @@ MlirTpuValueArray mlirTpuDisassemble(MlirTpuInsertionPoint insertion_point,\n                                      MlirTpuVectorLayout layout, MlirValue val,\n                                      MlirTpuI64TargetTuple target_shape) {\n   mlir::OpBuilder builder = mlirTpuInsertionPointToOpBuilder(insertion_point);\n+  // This cast will fail and assert if the caller passed a non-vector\n+  auto vector_val = mlir::cast<mlir::TypedValue<mlir::VectorType>>(unwrap(val));\n   mlir::FailureOr<xla::Array<mlir::Value>> failure_or_vals =\n-      mlir::tpu::disassemble(builder, *unwrap(layout), unwrap(val),\n+      mlir::tpu::disassemble(builder, *unwrap(layout), vector_val,\n                              unwrap(target_shape));\n   if (failed(failure_or_vals)) {\n     return {{nullptr, 0}, nullptr};\n"
                },
                {
                    "old_start": 371,
                    "old_length": 8,
                    "new_start": 373,
                    "new_length": 11,
                    "hunk": "@@ -371,8 +373,11 @@ MlirValue mlirTpuRelayout(MlirTpuInsertionPoint insertion_point, MlirValue val,\n                           MlirTpuVectorLayout src, MlirTpuVectorLayout dst,\n                           MlirTpuI64TargetTuple target_shape) {\n   mlir::OpBuilder builder = mlirTpuInsertionPointToOpBuilder(insertion_point);\n-  mlir::FailureOr<mlir::Value> failure_or_new_val = mlir::tpu::relayout(\n-      builder, unwrap(val), *unwrap(src), *unwrap(dst), unwrap(target_shape));\n+  // This cast will fail and assert if the caller passed a non-vector\n+  auto vector_val = mlir::cast<mlir::TypedValue<mlir::VectorType>>(unwrap(val));\n+  mlir::FailureOr<mlir::TypedValue<mlir::VectorType>> failure_or_new_val =\n+      mlir::tpu::relayout(builder, vector_val, *unwrap(src), *unwrap(dst),\n+                          unwrap(target_shape));\n   if (failed(failure_or_new_val)) {\n     return {nullptr};\n   }\n"
                }
            ],
            "whole_deleted": "-      mlir::tpu::disassemble(builder, *unwrap(layout), unwrap(val),\n-  mlir::FailureOr<mlir::Value> failure_or_new_val = mlir::tpu::relayout(\n-      builder, unwrap(val), *unwrap(src), *unwrap(dst), unwrap(target_shape));\n",
            "whole_added": "+  // This cast will fail and assert if the caller passed a non-vector\n+  auto vector_val = mlir::cast<mlir::TypedValue<mlir::VectorType>>(unwrap(val));\n+      mlir::tpu::disassemble(builder, *unwrap(layout), vector_val,\n+  // This cast will fail and assert if the caller passed a non-vector\n+  auto vector_val = mlir::cast<mlir::TypedValue<mlir::VectorType>>(unwrap(val));\n+  mlir::FailureOr<mlir::TypedValue<mlir::VectorType>> failure_or_new_val =\n+      mlir::tpu::relayout(builder, vector_val, *unwrap(src), *unwrap(dst),\n+                          unwrap(target_shape));\n",
            "whole_hunk": "@@ -349,8 +349,10 @@ MlirTpuValueArray mlirTpuDisassemble(MlirTpuInsertionPoint insertion_point,\n                                      MlirTpuVectorLayout layout, MlirValue val,\n                                      MlirTpuI64TargetTuple target_shape) {\n   mlir::OpBuilder builder = mlirTpuInsertionPointToOpBuilder(insertion_point);\n+  // This cast will fail and assert if the caller passed a non-vector\n+  auto vector_val = mlir::cast<mlir::TypedValue<mlir::VectorType>>(unwrap(val));\n   mlir::FailureOr<xla::Array<mlir::Value>> failure_or_vals =\n-      mlir::tpu::disassemble(builder, *unwrap(layout), unwrap(val),\n+      mlir::tpu::disassemble(builder, *unwrap(layout), vector_val,\n                              unwrap(target_shape));\n   if (failed(failure_or_vals)) {\n     return {{nullptr, 0}, nullptr};\n@@ -371,8 +373,11 @@ MlirValue mlirTpuRelayout(MlirTpuInsertionPoint insertion_point, MlirValue val,\n                           MlirTpuVectorLayout src, MlirTpuVectorLayout dst,\n                           MlirTpuI64TargetTuple target_shape) {\n   mlir::OpBuilder builder = mlirTpuInsertionPointToOpBuilder(insertion_point);\n-  mlir::FailureOr<mlir::Value> failure_or_new_val = mlir::tpu::relayout(\n-      builder, unwrap(val), *unwrap(src), *unwrap(dst), unwrap(target_shape));\n+  // This cast will fail and assert if the caller passed a non-vector\n+  auto vector_val = mlir::cast<mlir::TypedValue<mlir::VectorType>>(unwrap(val));\n+  mlir::FailureOr<mlir::TypedValue<mlir::VectorType>> failure_or_new_val =\n+      mlir::tpu::relayout(builder, vector_val, *unwrap(src), *unwrap(dst),\n+                          unwrap(target_shape));\n   if (failed(failure_or_new_val)) {\n     return {nullptr};\n   }\n"
        },
        {
            "name": "tpu.td",
            "path": "jaxlib/mosaic/dialect/tpu/tpu.td",
            "patches": [
                {
                    "old_start": 325,
                    "old_length": 16,
                    "new_start": 325,
                    "new_length": 16,
                    "hunk": "@@ -325,16 +325,16 @@ def TPU_BitcastVregOp : TPU_Op<\"bitcast_vreg\", [Pure]> {\n }\n \n def TPU_RollVectorsOp : TPU_Op<\"roll_vectors\", [Pure]> {\n-  let arguments = (ins Variadic<AnyType>:$input);\n-  let results = (outs AnyType:$output);\n+  let arguments = (ins Variadic<AnyVector>:$input);\n+  let results = (outs AnyVector:$output);\n   let assemblyFormat = [{\n     $input attr-dict `:` type($input) `->` type($output)\n   }];\n }\n \n def TPU_UnrollVectorsOp : TPU_Op<\"unroll_vectors\", [Pure]> {\n-  let arguments = (ins AnyType:$input);\n-  let results = (outs Variadic<AnyType>:$output);\n+  let arguments = (ins AnyVector:$input);\n+  let results = (outs Variadic<AnyVector>:$output);\n   let hasCanonicalizeMethod = 1;\n   let assemblyFormat = [{\n     $input attr-dict `:` type($input) `->` type($output)\n"
                }
            ],
            "whole_deleted": "-  let arguments = (ins Variadic<AnyType>:$input);\n-  let results = (outs AnyType:$output);\n-  let arguments = (ins AnyType:$input);\n-  let results = (outs Variadic<AnyType>:$output);\n",
            "whole_added": "+  let arguments = (ins Variadic<AnyVector>:$input);\n+  let results = (outs AnyVector:$output);\n+  let arguments = (ins AnyVector:$input);\n+  let results = (outs Variadic<AnyVector>:$output);\n",
            "whole_hunk": "@@ -325,16 +325,16 @@ def TPU_BitcastVregOp : TPU_Op<\"bitcast_vreg\", [Pure]> {\n }\n \n def TPU_RollVectorsOp : TPU_Op<\"roll_vectors\", [Pure]> {\n-  let arguments = (ins Variadic<AnyType>:$input);\n-  let results = (outs AnyType:$output);\n+  let arguments = (ins Variadic<AnyVector>:$input);\n+  let results = (outs AnyVector:$output);\n   let assemblyFormat = [{\n     $input attr-dict `:` type($input) `->` type($output)\n   }];\n }\n \n def TPU_UnrollVectorsOp : TPU_Op<\"unroll_vectors\", [Pure]> {\n-  let arguments = (ins AnyType:$input);\n-  let results = (outs Variadic<AnyType>:$output);\n+  let arguments = (ins AnyVector:$input);\n+  let results = (outs Variadic<AnyVector>:$output);\n   let hasCanonicalizeMethod = 1;\n   let assemblyFormat = [{\n     $input attr-dict `:` type($input) `->` type($output)\n"
        },
        {
            "name": "apply_vector_layout.cc",
            "path": "jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc",
            "patches": [
                {
                    "old_start": 590,
                    "old_length": 9,
                    "new_start": 590,
                    "new_length": 11,
                    "hunk": "@@ -590,9 +590,11 @@ LogicalResult elementwise_op_rule(RewriteContext &ctx, Operation &op,\n   SmallVector<xla::Array<Value>> in_vreg_arrays;\n   in_vreg_arrays.reserve(num_operands);\n   for (unsigned i = 0; i < num_operands; ++i) {\n-    FAILUREOR_ASSIGN_OR_RETURN(xla::Array<Value> tile_array,\n-                               disassemble(builder, *layouts_in[i],\n-                                           op.getOperand(i), ctx.target_shape));\n+    FAILUREOR_ASSIGN_OR_RETURN(\n+        xla::Array<Value> tile_array,\n+        disassemble(builder, *layouts_in[i],\n+                    cast<TypedValue<VectorType>>(op.getOperand(i)),\n+                    ctx.target_shape));\n     in_vreg_arrays.emplace_back(std::move(tile_array));\n   }\n \n"
                },
                {
                    "old_start": 653,
                    "old_length": 15,
                    "new_start": 655,
                    "new_length": 16,
                    "hunk": "@@ -653,15 +655,16 @@ LogicalResult ext_op_rule_impl(RewriteContext &ctx, OpTy op,\n                                const VectorLayout &layout_in,\n                                const VectorLayout &layout_out) {\n   ImplicitLocOpBuilder builder(op.getLoc(), op.getOperation());\n-  auto result_ty = cast<VectorType>(op.getResult().getType());\n-  auto source_ty = cast<VectorType>(op.getIn().getType());\n+  const auto result_ty = cast<VectorType>(op.getResult().getType());\n+  auto source = cast<TypedValue<VectorType>>(op.getIn());\n+  const auto source_ty = source.getType();\n   if (layout_out.bitwidth() != 32) {\n     return op.emitOpError(\n         \"Not implemented: Only extensions to 32-bit supported\");\n   }\n   FAILUREOR_ASSIGN_OR_RETURN(\n       const xla::Array<Value> input_vregs,\n-      disassemble(builder, layout_in, op.getIn(), ctx.target_shape));\n+      disassemble(builder, layout_in, source, ctx.target_shape));\n   xla::Array<Value> output_vregs(\n       layout_out.tileArrayShape(result_ty.getShape(), ctx.target_shape));\n   FAILUREOR_ASSIGN_OR_RETURN(\n"
                },
                {
                    "old_start": 762,
                    "old_length": 7,
                    "new_start": 765,
                    "new_length": 8,
                    "hunk": "@@ -762,7 +765,8 @@ LogicalResult trunc_op_rule_impl(RewriteContext &ctx, OpTy op,\n   auto result_ty = cast<VectorType>(op.getResult().getType());\n   FAILUREOR_ASSIGN_OR_RETURN(\n       const xla::Array<Value> input_vregs,\n-      disassemble(builder, layout_in, op.getIn(), ctx.target_shape));\n+      disassemble(builder, layout_in, cast<TypedValue<VectorType>>(op.getIn()),\n+                  ctx.target_shape));\n   xla::Array<Value> output_vregs(\n       layout_out.tileArrayShape(result_ty.getShape(), ctx.target_shape));\n   if (layout_in.bitwidth() != 32) {\n"
                },
                {
                    "old_start": 905,
                    "old_length": 13,
                    "new_start": 909,
                    "new_length": 13,
                    "hunk": "@@ -905,13 +909,13 @@ LogicalResult scf_for_rule(RewriteContext &ctx, Operation &op,\n       }\n       continue;\n     }\n-    if (auto vty = dyn_cast<VectorType>(operand.getType())) {\n+    if (auto vector_operand = dyn_cast<TypedValue<VectorType>>(operand)) {\n       if (!layout.has_value()) {\n         return op.emitOpError(\"Expected layout for vector operand\");\n       }\n       FAILUREOR_ASSIGN_OR_RETURN(\n           const xla::Array<Value> tiles,\n-          disassemble(builder, *layout, operand, ctx.target_shape));\n+          disassemble(builder, *layout, vector_operand, ctx.target_shape));\n       unrolled_args.append(tiles.begin(), tiles.end());\n     } else {\n       if (layout.has_value()) {\n"
                },
                {
                    "old_start": 1098,
                    "old_length": 12,
                    "new_start": 1102,
                    "new_length": 12,
                    "hunk": "@@ -1098,12 +1102,12 @@ LogicalResult scf_yield_rule(RewriteContext &ctx, Operation &op,\n   SmallVector<Value> unrolled;\n   for (auto [operand, layout] :\n        llvm::zip_equal(yield_op.getOperands(), layouts_in)) {\n-    if (auto vty = dyn_cast<VectorType>(operand.getType())) {\n+    if (auto vector_operand = dyn_cast<TypedValue<VectorType>>(operand)) {\n       // When the operand has vector type, disassemble the operand.\n       TPU_ASSERT_OP(layout.has_value());\n       FAILUREOR_ASSIGN_OR_RETURN(\n           const xla::Array<Value> tiles,\n-          disassemble(builder, *layout, operand, ctx.target_shape));\n+          disassemble(builder, *layout, vector_operand, ctx.target_shape));\n       unrolled.append(tiles.begin(), tiles.end());\n     } else {\n       TPU_ASSERT_OP(!layout.has_value());\n"
                },
                {
                    "old_start": 1745,
                    "old_length": 7,
                    "new_start": 1749,
                    "new_length": 8,
                    "hunk": "@@ -1745,7 +1749,8 @@ LogicalResult tpu_concatenate_rule(RewriteContext &ctx, Operation &op,\n   for (Value operand : concatenate_op.getOperands()) {\n     FAILUREOR_ASSIGN_OR_RETURN(\n         xla::Array<Value> t,\n-        disassemble(builder, layout, operand, ctx.target_shape));\n+        disassemble(builder, layout, cast<TypedValue<VectorType>>(operand),\n+                    ctx.target_shape));\n     tiles.emplace_back(std::move(t));\n   }\n   const xla::Array<Value> res_tiles = concatenate(tiles, dimension);\n"
                },
                {
                    "old_start": 2227,
                    "old_length": 7,
                    "new_start": 2232,
                    "new_length": 8,
                    "hunk": "@@ -2227,7 +2232,8 @@ LogicalResult vector_broadcast_rule(RewriteContext &ctx, Operation &op,\n   const VectorType dst_ty = broadcast_op.getResult().getType();\n   const SmallVector<int64_t> dst_tiles_shape =\n       layout_out.tileArrayShape(dst_ty.getShape(), ctx.target_shape);\n-  if (auto src_ty = dyn_cast<VectorType>(broadcast_op.getSourceType())) {\n+  if (auto src = dyn_cast<TypedValue<VectorType>>(broadcast_op.getSource())) {\n+    VectorType src_ty = src.getType();\n     TPU_ASSERT_OP(maybe_layout_in.has_value());\n     const VectorLayout &layout_in = *maybe_layout_in;\n     if (layout_in.implicit_dim() != layout_out.implicit_dim()) {\n"
                },
                {
                    "old_start": 2301,
                    "old_length": 8,
                    "new_start": 2307,
                    "new_length": 7,
                    "hunk": "@@ -2301,8 +2307,7 @@ LogicalResult vector_broadcast_rule(RewriteContext &ctx, Operation &op,\n \n     FAILUREOR_ASSIGN_OR_RETURN(\n         xla::Array<Value> src_tiles,\n-        disassemble(builder, layout_in, broadcast_op.getSource(),\n-                    ctx.target_shape));\n+        disassemble(builder, layout_in, src, ctx.target_shape));\n     xla::Array<Value> dst_tiles(dst_tiles_shape);\n     if (no_op) {\n       SmallVector<int64_t> reshape_dims(expand_rank, 1);\n"
                },
                {
                    "old_start": 2666,
                    "old_length": 10,
                    "new_start": 2671,
                    "new_length": 9,
                    "hunk": "@@ -2666,10 +2671,9 @@ LogicalResult vector_multi_reduction_rule(RewriteContext &ctx, Operation &op,\n     return multi_reduction_op.emitOpError(\n         \"Not implemented: Can only reduce into vectors\");\n   }\n-  if (!layouts_out.front().has_value()) {\n-    // Shouldn't be empty since result is a vector\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  // Op definition enforces that accumulator type must match result type\n+  auto acc = cast<TypedValue<VectorType>>(multi_reduction_op.getAcc());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n \n   const ArrayAttr dim_attrs = multi_reduction_op.getReductionDims();\n   SmallVector<int64_t> dims;\n"
                },
                {
                    "old_start": 2686,
                    "old_length": 11,
                    "new_start": 2690,
                    "new_length": 9,
                    "hunk": "@@ -2686,11 +2690,9 @@ LogicalResult vector_multi_reduction_rule(RewriteContext &ctx, Operation &op,\n   }\n   FAILUREOR_ASSIGN_OR_RETURN(\n       const xla::Array<Value> acc_vregs,\n-      disassemble(builder, acc_layout, multi_reduction_op.getAcc(),\n-                  ctx.target_shape));\n-  const Value acc_vreg = *acc_vregs.begin();\n-  auto acc_def =\n-      dyn_cast_if_present<arith::ConstantOp>(acc_vreg.getDefiningOp());\n+      disassemble(builder, acc_layout, acc, ctx.target_shape));\n+  auto acc_def = dyn_cast_if_present<arith::ConstantOp>(\n+      acc_vregs.begin()->getDefiningOp());\n   if (acc_def == nullptr) {\n     return multi_reduction_op.emitOpError(\n         \"Not implemented: Only constant accumulator supported\");\n"
                },
                {
                    "old_start": 2838,
                    "old_length": 7,
                    "new_start": 2840,
                    "new_length": 7,
                    "hunk": "@@ -2838,7 +2840,7 @@ LogicalResult vector_multi_reduction_rule(RewriteContext &ctx, Operation &op,\n         }\n         xla::Array<Value> reduced_vregs =\n             src_vregs.Slice(src_slice_start, src_slice_end);\n-        std::optional<Value> acc;\n+        std::optional<Value> acc_vreg;\n         auto reduction_status = reduced_vregs.EachStatus(\n             [&](const absl::Span<const int64_t> red_idx,\n                 Value *const src_vreg) {\n"
                },
                {
                    "old_start": 2860,
                    "old_length": 17,
                    "new_start": 2862,
                    "new_length": 17,
                    "hunk": "@@ -2860,17 +2862,17 @@ LogicalResult vector_multi_reduction_rule(RewriteContext &ctx, Operation &op,\n                 return absl::UnknownError(\"\");\n               }\n               Value vreg = failure_or_vreg.value();\n-              if (!acc.has_value()) {\n-                acc = vreg;\n+              if (!acc_vreg.has_value()) {\n+                acc_vreg = vreg;\n               } else {\n                 switch (tpu_kind) {\n                   case tpu::ReductionKind::SUM:\n-                    acc = builder.create<arith::AddFOp>(vreg.getLoc(), *acc,\n-                                                        vreg);\n+                    acc_vreg = builder.create<arith::AddFOp>(vreg.getLoc(),\n+                                                             *acc_vreg, vreg);\n                     break;\n                   case tpu::ReductionKind::MAX:\n-                    acc = builder.create<arith::MaximumFOp>(vreg.getLoc(), *acc,\n-                                                            vreg);\n+                    acc_vreg = builder.create<arith::MaximumFOp>(\n+                        vreg.getLoc(), *acc_vreg, vreg);\n                     break;\n                 }\n               }\n"
                },
                {
                    "old_start": 2879,
                    "old_length": 16,
                    "new_start": 2881,
                    "new_length": 16,
                    "hunk": "@@ -2879,16 +2881,16 @@ LogicalResult vector_multi_reduction_rule(RewriteContext &ctx, Operation &op,\n         if (!reduction_status.ok()) {\n           return reduction_status;\n         }\n-        TPU_ASSERT_OP(acc.has_value());\n+        TPU_ASSERT_OP(acc_vreg.has_value());\n         if (reduces[1]) {\n-          acc = builder.create<tpu::AllReduceOp>(multi_reduction_op->getLoc(),\n-                                                 *acc, 1, tpu_kind);\n+          acc_vreg = builder.create<tpu::AllReduceOp>(\n+              multi_reduction_op->getLoc(), *acc_vreg, 1, tpu_kind);\n         }\n         if (reduces[0]) {\n-          acc = builder.create<tpu::AllReduceOp>(multi_reduction_op->getLoc(),\n-                                                 *acc, 0, tpu_kind);\n+          acc_vreg = builder.create<tpu::AllReduceOp>(\n+              multi_reduction_op->getLoc(), *acc_vreg, 0, tpu_kind);\n         }\n-        *dst_vreg = *acc;\n+        *dst_vreg = *acc_vreg;\n         return absl::OkStatus();\n       });\n   if (!all_results_ok.ok()) {\n"
                },
                {
                    "old_start": 3478,
                    "old_length": 9,
                    "new_start": 3480,
                    "new_length": 10,
                    "hunk": "@@ -3478,9 +3480,10 @@ RollVectorsOp assemble(OpBuilder &builder, VectorType vty,\n // Returns:\n //   An ndarray of MLIR values representing the tiling of val given by layout.\n FailureOr<xla::Array<Value>> disassemble(\n-    OpBuilder &builder, const VectorLayout &layout, const Value val,\n+    OpBuilder &builder, const VectorLayout &layout,\n+    const TypedValue<VectorType> val,\n     const std::array<int64_t, 2> target_shape) {\n-  const auto vty = cast<VectorType>(val.getType());\n+  const auto vty = val.getType();\n   const auto op_result = dyn_cast<OpResult>(val);\n   if (op_result == nullptr) {\n     return failure();\n"
                },
                {
                    "old_start": 3869,
                    "old_length": 15,
                    "new_start": 3872,
                    "new_length": 15,
                    "hunk": "@@ -3869,15 +3872,15 @@ Value copy_one_sublane(OpBuilder &builder, Value src_vreg, int src_sl_idx,\n }\n \n // TODO(apaszke): Test this function properly\n-FailureOr<Value> relayout(OpBuilder &builder, Value v, VectorLayout src,\n-                          const VectorLayout &dst,\n-                          const std::array<int64_t, 2> target_shape) {\n+FailureOr<TypedValue<VectorType>> relayout(\n+    OpBuilder &builder, TypedValue<VectorType> v, VectorLayout src,\n+    const VectorLayout &dst, const std::array<int64_t, 2> target_shape) {\n   const int8_t bitwidth = src.bitwidth();\n   if (bitwidth != dst.bitwidth()) {\n     return emitError(v.getLoc(), \"Can't change bitwidth during a relayout\");\n   }\n   const int packing = src.packing();\n-  VectorType vty = cast<VectorType>(v.getType());\n+  VectorType vty = v.getType();\n   FAILUREOR_ASSIGN_OR_RETURN(xla::Array<Value> src_tiles,\n                              disassemble(builder, src, v, target_shape));\n   SmallVector<int64_t> dst_tiles_shape =\n"
                },
                {
                    "old_start": 4202,
                    "old_length": 16,
                    "new_start": 4205,
                    "new_length": 17,
                    "hunk": "@@ -4202,16 +4205,17 @@ LogicalResult applyLayoutOp(RewriteContext &ctx, Operation &op) {\n     for (auto [idx, tup] :\n          llvm::enumerate(llvm::zip(op.getOperands(), layouts_in))) {\n       auto [operand, li] = tup;\n-      auto vty = dyn_cast<VectorType>(operand.getType());\n-      TPU_ASSERT_EQ_OP(vty != nullptr, li.has_value());\n-      if (vty == nullptr) {\n+      auto vector_operand = dyn_cast<TypedValue<VectorType>>(operand);\n+      TPU_ASSERT_EQ_OP(vector_operand != nullptr, li.has_value());\n+      if (vector_operand == nullptr) {\n         continue;\n       }\n+      auto vty = vector_operand.getType();\n \n       // The operand should always be an Operation (and not a BlockArgument)\n       // since we expect the FuncOp to have only memrefs and semaphores as\n       // arguments.\n-      auto op_result = dyn_cast<OpResult>(operand);\n+      auto op_result = dyn_cast<OpResult>(vector_operand);\n       if (op_result == nullptr) {\n         return op.emitError(\"Expected operand to be an operation result\");\n       }\n"
                },
                {
                    "old_start": 4227,
                    "old_length": 7,
                    "new_start": 4231,
                    "new_length": 7,
                    "hunk": "@@ -4227,7 +4231,7 @@ LogicalResult applyLayoutOp(RewriteContext &ctx, Operation &op) {\n       }\n       OpBuilder builder(&op);\n       FAILUREOR_ASSIGN_OR_RETURN(Value new_v,\n-                                 relayout(builder, operand, /*src=*/*lo,\n+                                 relayout(builder, vector_operand, /*src=*/*lo,\n                                           /*dst=*/*li, ctx.target_shape));\n       op.setOperand(idx, new_v);\n     }\n"
                }
            ],
            "whole_deleted": "-    FAILUREOR_ASSIGN_OR_RETURN(xla::Array<Value> tile_array,\n-                               disassemble(builder, *layouts_in[i],\n-                                           op.getOperand(i), ctx.target_shape));\n-  auto result_ty = cast<VectorType>(op.getResult().getType());\n-  auto source_ty = cast<VectorType>(op.getIn().getType());\n-      disassemble(builder, layout_in, op.getIn(), ctx.target_shape));\n-      disassemble(builder, layout_in, op.getIn(), ctx.target_shape));\n-    if (auto vty = dyn_cast<VectorType>(operand.getType())) {\n-          disassemble(builder, *layout, operand, ctx.target_shape));\n-    if (auto vty = dyn_cast<VectorType>(operand.getType())) {\n-          disassemble(builder, *layout, operand, ctx.target_shape));\n-        disassemble(builder, layout, operand, ctx.target_shape));\n-  if (auto src_ty = dyn_cast<VectorType>(broadcast_op.getSourceType())) {\n-        disassemble(builder, layout_in, broadcast_op.getSource(),\n-                    ctx.target_shape));\n-  if (!layouts_out.front().has_value()) {\n-    // Shouldn't be empty since result is a vector\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n-      disassemble(builder, acc_layout, multi_reduction_op.getAcc(),\n-                  ctx.target_shape));\n-  const Value acc_vreg = *acc_vregs.begin();\n-  auto acc_def =\n-      dyn_cast_if_present<arith::ConstantOp>(acc_vreg.getDefiningOp());\n-        std::optional<Value> acc;\n-              if (!acc.has_value()) {\n-                acc = vreg;\n-                    acc = builder.create<arith::AddFOp>(vreg.getLoc(), *acc,\n-                                                        vreg);\n-                    acc = builder.create<arith::MaximumFOp>(vreg.getLoc(), *acc,\n-                                                            vreg);\n-        TPU_ASSERT_OP(acc.has_value());\n-          acc = builder.create<tpu::AllReduceOp>(multi_reduction_op->getLoc(),\n-                                                 *acc, 1, tpu_kind);\n-          acc = builder.create<tpu::AllReduceOp>(multi_reduction_op->getLoc(),\n-                                                 *acc, 0, tpu_kind);\n-        *dst_vreg = *acc;\n-    OpBuilder &builder, const VectorLayout &layout, const Value val,\n-  const auto vty = cast<VectorType>(val.getType());\n-FailureOr<Value> relayout(OpBuilder &builder, Value v, VectorLayout src,\n-                          const VectorLayout &dst,\n-                          const std::array<int64_t, 2> target_shape) {\n-  VectorType vty = cast<VectorType>(v.getType());\n-      auto vty = dyn_cast<VectorType>(operand.getType());\n-      TPU_ASSERT_EQ_OP(vty != nullptr, li.has_value());\n-      if (vty == nullptr) {\n-      auto op_result = dyn_cast<OpResult>(operand);\n-                                 relayout(builder, operand, /*src=*/*lo,\n",
            "whole_added": "+    FAILUREOR_ASSIGN_OR_RETURN(\n+        xla::Array<Value> tile_array,\n+        disassemble(builder, *layouts_in[i],\n+                    cast<TypedValue<VectorType>>(op.getOperand(i)),\n+                    ctx.target_shape));\n+  const auto result_ty = cast<VectorType>(op.getResult().getType());\n+  auto source = cast<TypedValue<VectorType>>(op.getIn());\n+  const auto source_ty = source.getType();\n+      disassemble(builder, layout_in, source, ctx.target_shape));\n+      disassemble(builder, layout_in, cast<TypedValue<VectorType>>(op.getIn()),\n+                  ctx.target_shape));\n+    if (auto vector_operand = dyn_cast<TypedValue<VectorType>>(operand)) {\n+          disassemble(builder, *layout, vector_operand, ctx.target_shape));\n+    if (auto vector_operand = dyn_cast<TypedValue<VectorType>>(operand)) {\n+          disassemble(builder, *layout, vector_operand, ctx.target_shape));\n+        disassemble(builder, layout, cast<TypedValue<VectorType>>(operand),\n+                    ctx.target_shape));\n+  if (auto src = dyn_cast<TypedValue<VectorType>>(broadcast_op.getSource())) {\n+    VectorType src_ty = src.getType();\n+        disassemble(builder, layout_in, src, ctx.target_shape));\n+  // Op definition enforces that accumulator type must match result type\n+  auto acc = cast<TypedValue<VectorType>>(multi_reduction_op.getAcc());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n+      disassemble(builder, acc_layout, acc, ctx.target_shape));\n+  auto acc_def = dyn_cast_if_present<arith::ConstantOp>(\n+      acc_vregs.begin()->getDefiningOp());\n+        std::optional<Value> acc_vreg;\n+              if (!acc_vreg.has_value()) {\n+                acc_vreg = vreg;\n+                    acc_vreg = builder.create<arith::AddFOp>(vreg.getLoc(),\n+                                                             *acc_vreg, vreg);\n+                    acc_vreg = builder.create<arith::MaximumFOp>(\n+                        vreg.getLoc(), *acc_vreg, vreg);\n+        TPU_ASSERT_OP(acc_vreg.has_value());\n+          acc_vreg = builder.create<tpu::AllReduceOp>(\n+              multi_reduction_op->getLoc(), *acc_vreg, 1, tpu_kind);\n+          acc_vreg = builder.create<tpu::AllReduceOp>(\n+              multi_reduction_op->getLoc(), *acc_vreg, 0, tpu_kind);\n+        *dst_vreg = *acc_vreg;\n+    OpBuilder &builder, const VectorLayout &layout,\n+    const TypedValue<VectorType> val,\n+  const auto vty = val.getType();\n+FailureOr<TypedValue<VectorType>> relayout(\n+    OpBuilder &builder, TypedValue<VectorType> v, VectorLayout src,\n+    const VectorLayout &dst, const std::array<int64_t, 2> target_shape) {\n+  VectorType vty = v.getType();\n+      auto vector_operand = dyn_cast<TypedValue<VectorType>>(operand);\n+      TPU_ASSERT_EQ_OP(vector_operand != nullptr, li.has_value());\n+      if (vector_operand == nullptr) {\n+      auto vty = vector_operand.getType();\n+      auto op_result = dyn_cast<OpResult>(vector_operand);\n+                                 relayout(builder, vector_operand, /*src=*/*lo,\n",
            "whole_hunk": "@@ -590,9 +590,11 @@ LogicalResult elementwise_op_rule(RewriteContext &ctx, Operation &op,\n   SmallVector<xla::Array<Value>> in_vreg_arrays;\n   in_vreg_arrays.reserve(num_operands);\n   for (unsigned i = 0; i < num_operands; ++i) {\n-    FAILUREOR_ASSIGN_OR_RETURN(xla::Array<Value> tile_array,\n-                               disassemble(builder, *layouts_in[i],\n-                                           op.getOperand(i), ctx.target_shape));\n+    FAILUREOR_ASSIGN_OR_RETURN(\n+        xla::Array<Value> tile_array,\n+        disassemble(builder, *layouts_in[i],\n+                    cast<TypedValue<VectorType>>(op.getOperand(i)),\n+                    ctx.target_shape));\n     in_vreg_arrays.emplace_back(std::move(tile_array));\n   }\n \n@@ -653,15 +655,16 @@ LogicalResult ext_op_rule_impl(RewriteContext &ctx, OpTy op,\n                                const VectorLayout &layout_in,\n                                const VectorLayout &layout_out) {\n   ImplicitLocOpBuilder builder(op.getLoc(), op.getOperation());\n-  auto result_ty = cast<VectorType>(op.getResult().getType());\n-  auto source_ty = cast<VectorType>(op.getIn().getType());\n+  const auto result_ty = cast<VectorType>(op.getResult().getType());\n+  auto source = cast<TypedValue<VectorType>>(op.getIn());\n+  const auto source_ty = source.getType();\n   if (layout_out.bitwidth() != 32) {\n     return op.emitOpError(\n         \"Not implemented: Only extensions to 32-bit supported\");\n   }\n   FAILUREOR_ASSIGN_OR_RETURN(\n       const xla::Array<Value> input_vregs,\n-      disassemble(builder, layout_in, op.getIn(), ctx.target_shape));\n+      disassemble(builder, layout_in, source, ctx.target_shape));\n   xla::Array<Value> output_vregs(\n       layout_out.tileArrayShape(result_ty.getShape(), ctx.target_shape));\n   FAILUREOR_ASSIGN_OR_RETURN(\n@@ -762,7 +765,8 @@ LogicalResult trunc_op_rule_impl(RewriteContext &ctx, OpTy op,\n   auto result_ty = cast<VectorType>(op.getResult().getType());\n   FAILUREOR_ASSIGN_OR_RETURN(\n       const xla::Array<Value> input_vregs,\n-      disassemble(builder, layout_in, op.getIn(), ctx.target_shape));\n+      disassemble(builder, layout_in, cast<TypedValue<VectorType>>(op.getIn()),\n+                  ctx.target_shape));\n   xla::Array<Value> output_vregs(\n       layout_out.tileArrayShape(result_ty.getShape(), ctx.target_shape));\n   if (layout_in.bitwidth() != 32) {\n@@ -905,13 +909,13 @@ LogicalResult scf_for_rule(RewriteContext &ctx, Operation &op,\n       }\n       continue;\n     }\n-    if (auto vty = dyn_cast<VectorType>(operand.getType())) {\n+    if (auto vector_operand = dyn_cast<TypedValue<VectorType>>(operand)) {\n       if (!layout.has_value()) {\n         return op.emitOpError(\"Expected layout for vector operand\");\n       }\n       FAILUREOR_ASSIGN_OR_RETURN(\n           const xla::Array<Value> tiles,\n-          disassemble(builder, *layout, operand, ctx.target_shape));\n+          disassemble(builder, *layout, vector_operand, ctx.target_shape));\n       unrolled_args.append(tiles.begin(), tiles.end());\n     } else {\n       if (layout.has_value()) {\n@@ -1098,12 +1102,12 @@ LogicalResult scf_yield_rule(RewriteContext &ctx, Operation &op,\n   SmallVector<Value> unrolled;\n   for (auto [operand, layout] :\n        llvm::zip_equal(yield_op.getOperands(), layouts_in)) {\n-    if (auto vty = dyn_cast<VectorType>(operand.getType())) {\n+    if (auto vector_operand = dyn_cast<TypedValue<VectorType>>(operand)) {\n       // When the operand has vector type, disassemble the operand.\n       TPU_ASSERT_OP(layout.has_value());\n       FAILUREOR_ASSIGN_OR_RETURN(\n           const xla::Array<Value> tiles,\n-          disassemble(builder, *layout, operand, ctx.target_shape));\n+          disassemble(builder, *layout, vector_operand, ctx.target_shape));\n       unrolled.append(tiles.begin(), tiles.end());\n     } else {\n       TPU_ASSERT_OP(!layout.has_value());\n@@ -1745,7 +1749,8 @@ LogicalResult tpu_concatenate_rule(RewriteContext &ctx, Operation &op,\n   for (Value operand : concatenate_op.getOperands()) {\n     FAILUREOR_ASSIGN_OR_RETURN(\n         xla::Array<Value> t,\n-        disassemble(builder, layout, operand, ctx.target_shape));\n+        disassemble(builder, layout, cast<TypedValue<VectorType>>(operand),\n+                    ctx.target_shape));\n     tiles.emplace_back(std::move(t));\n   }\n   const xla::Array<Value> res_tiles = concatenate(tiles, dimension);\n@@ -2227,7 +2232,8 @@ LogicalResult vector_broadcast_rule(RewriteContext &ctx, Operation &op,\n   const VectorType dst_ty = broadcast_op.getResult().getType();\n   const SmallVector<int64_t> dst_tiles_shape =\n       layout_out.tileArrayShape(dst_ty.getShape(), ctx.target_shape);\n-  if (auto src_ty = dyn_cast<VectorType>(broadcast_op.getSourceType())) {\n+  if (auto src = dyn_cast<TypedValue<VectorType>>(broadcast_op.getSource())) {\n+    VectorType src_ty = src.getType();\n     TPU_ASSERT_OP(maybe_layout_in.has_value());\n     const VectorLayout &layout_in = *maybe_layout_in;\n     if (layout_in.implicit_dim() != layout_out.implicit_dim()) {\n@@ -2301,8 +2307,7 @@ LogicalResult vector_broadcast_rule(RewriteContext &ctx, Operation &op,\n \n     FAILUREOR_ASSIGN_OR_RETURN(\n         xla::Array<Value> src_tiles,\n-        disassemble(builder, layout_in, broadcast_op.getSource(),\n-                    ctx.target_shape));\n+        disassemble(builder, layout_in, src, ctx.target_shape));\n     xla::Array<Value> dst_tiles(dst_tiles_shape);\n     if (no_op) {\n       SmallVector<int64_t> reshape_dims(expand_rank, 1);\n@@ -2666,10 +2671,9 @@ LogicalResult vector_multi_reduction_rule(RewriteContext &ctx, Operation &op,\n     return multi_reduction_op.emitOpError(\n         \"Not implemented: Can only reduce into vectors\");\n   }\n-  if (!layouts_out.front().has_value()) {\n-    // Shouldn't be empty since result is a vector\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  // Op definition enforces that accumulator type must match result type\n+  auto acc = cast<TypedValue<VectorType>>(multi_reduction_op.getAcc());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n \n   const ArrayAttr dim_attrs = multi_reduction_op.getReductionDims();\n   SmallVector<int64_t> dims;\n@@ -2686,11 +2690,9 @@ LogicalResult vector_multi_reduction_rule(RewriteContext &ctx, Operation &op,\n   }\n   FAILUREOR_ASSIGN_OR_RETURN(\n       const xla::Array<Value> acc_vregs,\n-      disassemble(builder, acc_layout, multi_reduction_op.getAcc(),\n-                  ctx.target_shape));\n-  const Value acc_vreg = *acc_vregs.begin();\n-  auto acc_def =\n-      dyn_cast_if_present<arith::ConstantOp>(acc_vreg.getDefiningOp());\n+      disassemble(builder, acc_layout, acc, ctx.target_shape));\n+  auto acc_def = dyn_cast_if_present<arith::ConstantOp>(\n+      acc_vregs.begin()->getDefiningOp());\n   if (acc_def == nullptr) {\n     return multi_reduction_op.emitOpError(\n         \"Not implemented: Only constant accumulator supported\");\n@@ -2838,7 +2840,7 @@ LogicalResult vector_multi_reduction_rule(RewriteContext &ctx, Operation &op,\n         }\n         xla::Array<Value> reduced_vregs =\n             src_vregs.Slice(src_slice_start, src_slice_end);\n-        std::optional<Value> acc;\n+        std::optional<Value> acc_vreg;\n         auto reduction_status = reduced_vregs.EachStatus(\n             [&](const absl::Span<const int64_t> red_idx,\n                 Value *const src_vreg) {\n@@ -2860,17 +2862,17 @@ LogicalResult vector_multi_reduction_rule(RewriteContext &ctx, Operation &op,\n                 return absl::UnknownError(\"\");\n               }\n               Value vreg = failure_or_vreg.value();\n-              if (!acc.has_value()) {\n-                acc = vreg;\n+              if (!acc_vreg.has_value()) {\n+                acc_vreg = vreg;\n               } else {\n                 switch (tpu_kind) {\n                   case tpu::ReductionKind::SUM:\n-                    acc = builder.create<arith::AddFOp>(vreg.getLoc(), *acc,\n-                                                        vreg);\n+                    acc_vreg = builder.create<arith::AddFOp>(vreg.getLoc(),\n+                                                             *acc_vreg, vreg);\n                     break;\n                   case tpu::ReductionKind::MAX:\n-                    acc = builder.create<arith::MaximumFOp>(vreg.getLoc(), *acc,\n-                                                            vreg);\n+                    acc_vreg = builder.create<arith::MaximumFOp>(\n+                        vreg.getLoc(), *acc_vreg, vreg);\n                     break;\n                 }\n               }\n@@ -2879,16 +2881,16 @@ LogicalResult vector_multi_reduction_rule(RewriteContext &ctx, Operation &op,\n         if (!reduction_status.ok()) {\n           return reduction_status;\n         }\n-        TPU_ASSERT_OP(acc.has_value());\n+        TPU_ASSERT_OP(acc_vreg.has_value());\n         if (reduces[1]) {\n-          acc = builder.create<tpu::AllReduceOp>(multi_reduction_op->getLoc(),\n-                                                 *acc, 1, tpu_kind);\n+          acc_vreg = builder.create<tpu::AllReduceOp>(\n+              multi_reduction_op->getLoc(), *acc_vreg, 1, tpu_kind);\n         }\n         if (reduces[0]) {\n-          acc = builder.create<tpu::AllReduceOp>(multi_reduction_op->getLoc(),\n-                                                 *acc, 0, tpu_kind);\n+          acc_vreg = builder.create<tpu::AllReduceOp>(\n+              multi_reduction_op->getLoc(), *acc_vreg, 0, tpu_kind);\n         }\n-        *dst_vreg = *acc;\n+        *dst_vreg = *acc_vreg;\n         return absl::OkStatus();\n       });\n   if (!all_results_ok.ok()) {\n@@ -3478,9 +3480,10 @@ RollVectorsOp assemble(OpBuilder &builder, VectorType vty,\n // Returns:\n //   An ndarray of MLIR values representing the tiling of val given by layout.\n FailureOr<xla::Array<Value>> disassemble(\n-    OpBuilder &builder, const VectorLayout &layout, const Value val,\n+    OpBuilder &builder, const VectorLayout &layout,\n+    const TypedValue<VectorType> val,\n     const std::array<int64_t, 2> target_shape) {\n-  const auto vty = cast<VectorType>(val.getType());\n+  const auto vty = val.getType();\n   const auto op_result = dyn_cast<OpResult>(val);\n   if (op_result == nullptr) {\n     return failure();\n@@ -3869,15 +3872,15 @@ Value copy_one_sublane(OpBuilder &builder, Value src_vreg, int src_sl_idx,\n }\n \n // TODO(apaszke): Test this function properly\n-FailureOr<Value> relayout(OpBuilder &builder, Value v, VectorLayout src,\n-                          const VectorLayout &dst,\n-                          const std::array<int64_t, 2> target_shape) {\n+FailureOr<TypedValue<VectorType>> relayout(\n+    OpBuilder &builder, TypedValue<VectorType> v, VectorLayout src,\n+    const VectorLayout &dst, const std::array<int64_t, 2> target_shape) {\n   const int8_t bitwidth = src.bitwidth();\n   if (bitwidth != dst.bitwidth()) {\n     return emitError(v.getLoc(), \"Can't change bitwidth during a relayout\");\n   }\n   const int packing = src.packing();\n-  VectorType vty = cast<VectorType>(v.getType());\n+  VectorType vty = v.getType();\n   FAILUREOR_ASSIGN_OR_RETURN(xla::Array<Value> src_tiles,\n                              disassemble(builder, src, v, target_shape));\n   SmallVector<int64_t> dst_tiles_shape =\n@@ -4202,16 +4205,17 @@ LogicalResult applyLayoutOp(RewriteContext &ctx, Operation &op) {\n     for (auto [idx, tup] :\n          llvm::enumerate(llvm::zip(op.getOperands(), layouts_in))) {\n       auto [operand, li] = tup;\n-      auto vty = dyn_cast<VectorType>(operand.getType());\n-      TPU_ASSERT_EQ_OP(vty != nullptr, li.has_value());\n-      if (vty == nullptr) {\n+      auto vector_operand = dyn_cast<TypedValue<VectorType>>(operand);\n+      TPU_ASSERT_EQ_OP(vector_operand != nullptr, li.has_value());\n+      if (vector_operand == nullptr) {\n         continue;\n       }\n+      auto vty = vector_operand.getType();\n \n       // The operand should always be an Operation (and not a BlockArgument)\n       // since we expect the FuncOp to have only memrefs and semaphores as\n       // arguments.\n-      auto op_result = dyn_cast<OpResult>(operand);\n+      auto op_result = dyn_cast<OpResult>(vector_operand);\n       if (op_result == nullptr) {\n         return op.emitError(\"Expected operand to be an operation result\");\n       }\n@@ -4227,7 +4231,7 @@ LogicalResult applyLayoutOp(RewriteContext &ctx, Operation &op) {\n       }\n       OpBuilder builder(&op);\n       FAILUREOR_ASSIGN_OR_RETURN(Value new_v,\n-                                 relayout(builder, operand, /*src=*/*lo,\n+                                 relayout(builder, vector_operand, /*src=*/*lo,\n                                           /*dst=*/*li, ctx.target_shape));\n       op.setOperand(idx, new_v);\n     }\n"
        },
        {
            "name": "apply_vector_layout.h",
            "path": "jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.h",
            "patches": [
                {
                    "old_start": 29,
                    "old_length": 7,
                    "new_start": 29,
                    "new_length": 8,
                    "hunk": "@@ -29,7 +29,8 @@ RollVectorsOp assemble(OpBuilder &builder, VectorType vty,\n                        const xla::Array<Value> &vals,\n                        std::array<int64_t, 2> target_shape);\n FailureOr<xla::Array<Value>> disassemble(OpBuilder &builder,\n-                                         const VectorLayout &layout, Value val,\n+                                         const VectorLayout &layout,\n+                                         TypedValue<VectorType> val,\n                                          std::array<int64_t, 2> target_shape);\n \n // Rewrites the operation according to its layout annotations.\n"
                },
                {
                    "old_start": 55,
                    "old_length": 9,
                    "new_start": 56,
                    "new_length": 11,
                    "hunk": "@@ -55,9 +56,11 @@ LogicalResult applyLayoutOp(RewriteContext &ctx, Operation &op);\n //\n // Returns:\n //   A new MLIR vector value, laid out as requested by dst.\n-FailureOr<Value> relayout(OpBuilder &builder, Value v, VectorLayout src,\n-                          const VectorLayout &dst,\n-                          std::array<int64_t, 2> target_shape);\n+FailureOr<TypedValue<VectorType>> relayout(OpBuilder &builder,\n+                                           TypedValue<VectorType> v,\n+                                           VectorLayout src,\n+                                           const VectorLayout &dst,\n+                                           std::array<int64_t, 2> target_shape);\n \n }  // namespace mlir::tpu\n "
                }
            ],
            "whole_deleted": "-                                         const VectorLayout &layout, Value val,\n-FailureOr<Value> relayout(OpBuilder &builder, Value v, VectorLayout src,\n-                          const VectorLayout &dst,\n-                          std::array<int64_t, 2> target_shape);\n",
            "whole_added": "+                                         const VectorLayout &layout,\n+                                         TypedValue<VectorType> val,\n+FailureOr<TypedValue<VectorType>> relayout(OpBuilder &builder,\n+                                           TypedValue<VectorType> v,\n+                                           VectorLayout src,\n+                                           const VectorLayout &dst,\n+                                           std::array<int64_t, 2> target_shape);\n",
            "whole_hunk": "@@ -29,7 +29,8 @@ RollVectorsOp assemble(OpBuilder &builder, VectorType vty,\n                        const xla::Array<Value> &vals,\n                        std::array<int64_t, 2> target_shape);\n FailureOr<xla::Array<Value>> disassemble(OpBuilder &builder,\n-                                         const VectorLayout &layout, Value val,\n+                                         const VectorLayout &layout,\n+                                         TypedValue<VectorType> val,\n                                          std::array<int64_t, 2> target_shape);\n \n // Rewrites the operation according to its layout annotations.\n@@ -55,9 +56,11 @@ LogicalResult applyLayoutOp(RewriteContext &ctx, Operation &op);\n //\n // Returns:\n //   A new MLIR vector value, laid out as requested by dst.\n-FailureOr<Value> relayout(OpBuilder &builder, Value v, VectorLayout src,\n-                          const VectorLayout &dst,\n-                          std::array<int64_t, 2> target_shape);\n+FailureOr<TypedValue<VectorType>> relayout(OpBuilder &builder,\n+                                           TypedValue<VectorType> v,\n+                                           VectorLayout src,\n+                                           const VectorLayout &dst,\n+                                           std::array<int64_t, 2> target_shape);\n \n }  // namespace mlir::tpu\n "
        }
    ]
},
{
    "Id": 87,
    "commit_link": "https://github.com/google/jax/commit/ca1844dd60ea527356028b0f3d91cf3366a02e35",
    "date": "2024-02-26T13:26:01-08:00",
    "message": "[PJRT C API] Bump the minimum support libtpu version and clean up version check.\n\nPiperOrigin-RevId: 610508159",
    "changes": [
        {
            "name": "cloud-tpu-ci-nightly.yml",
            "path": ".github/workflows/cloud-tpu-ci-nightly.yml",
            "patches": [
                {
                    "old_start": 28,
                    "old_length": 7,
                    "new_start": 28,
                    "new_length": 7,
                    "hunk": "@@ -28,7 +28,7 @@ jobs:\n         tpu-type: [\"v3-8\", \"v4-8\", \"v5e-4\"]\n     name: \"TPU test (jaxlib=${{ matrix.jaxlib-version }}, ${{ matrix.tpu-type }})\"\n     env:\n-      LIBTPU_OLDEST_VERSION_DATE: 20230927\n+      LIBTPU_OLDEST_VERSION_DATE: 20231030\n       ENABLE_PJRT_COMPATIBILITY: ${{ matrix.jaxlib-version == 'nightly+oldest_supported_libtpu' }}\n     runs-on: [\"self-hosted\", \"tpu\", \"${{ matrix.tpu-type }}\"]\n     timeout-minutes: 120\n"
                }
            ],
            "whole_deleted": "-      LIBTPU_OLDEST_VERSION_DATE: 20230927\n",
            "whole_added": "+      LIBTPU_OLDEST_VERSION_DATE: 20231030\n",
            "whole_hunk": "@@ -28,7 +28,7 @@ jobs:\n         tpu-type: [\"v3-8\", \"v4-8\", \"v5e-4\"]\n     name: \"TPU test (jaxlib=${{ matrix.jaxlib-version }}, ${{ matrix.tpu-type }})\"\n     env:\n-      LIBTPU_OLDEST_VERSION_DATE: 20230927\n+      LIBTPU_OLDEST_VERSION_DATE: 20231030\n       ENABLE_PJRT_COMPATIBILITY: ${{ matrix.jaxlib-version == 'nightly+oldest_supported_libtpu' }}\n     runs-on: [\"self-hosted\", \"tpu\", \"${{ matrix.tpu-type }}\"]\n     timeout-minutes: 120\n"
        },
        {
            "name": "memories_test.py",
            "path": "tests/memories_test.py",
            "patches": [
                {
                    "old_start": 677,
                    "old_length": 10,
                    "new_start": 677,
                    "new_length": 6,
                    "hunk": "@@ -677,10 +677,6 @@ class MemoriesComputationTest(jtu.BufferDonationTestCase):\n     self.assertEqual(cache_info2.misses, cache_info1.misses)\n \n   def test_device_put_host_to_hbm(self):\n-    # TODO(jieying): remove after 12/26/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 32):\n-      raise unittest.SkipTest(\"CopyToMemorySpace is not supported on PJRT C API version < 0.32.\")\n-\n     mesh = jtu.create_global_mesh((4, 2), (\"x\", \"y\"))\n     s_host = NamedSharding(mesh, P(\"y\"), memory_kind=\"unpinned_host\")\n     np_inp = jnp.arange(16).reshape(8, 2)\n"
                },
                {
                    "old_start": 698,
                    "old_length": 10,
                    "new_start": 694,
                    "new_length": 6,
                    "hunk": "@@ -698,10 +694,6 @@ class MemoriesComputationTest(jtu.BufferDonationTestCase):\n         out_on_hbm, np_inp, s_hbm, \"device\")\n \n   def test_device_put_hbm_to_host(self):\n-    # TODO(jieying): remove after 12/26/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 32):\n-      raise unittest.SkipTest(\"CopyToMemorySpace is not supported on PJRT C API version < 0.32.\")\n-\n     mesh = jtu.create_global_mesh((4, 2), (\"x\", \"y\"))\n     s_host = NamedSharding(mesh, P(\"y\"), memory_kind=\"unpinned_host\")\n     inp = jnp.arange(16).reshape(8, 2)\n"
                },
                {
                    "old_start": 718,
                    "old_length": 9,
                    "new_start": 710,
                    "new_length": 6,
                    "hunk": "@@ -718,9 +710,6 @@ class MemoriesComputationTest(jtu.BufferDonationTestCase):\n   def test_device_put_different_device_and_memory_host_to_hbm(self):\n     if jax.device_count() < 3:\n       raise unittest.SkipTest(\"Test requires >=3 devices\")\n-    # TODO(jieying): remove after 12/26/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 32):\n-      raise unittest.SkipTest(\"CopyToMemorySpace is not supported on PJRT C API version < 0.32.\")\n \n     out_host0 = jax.device_put(\n         jnp.arange(8),\n"
                },
                {
                    "old_start": 738,
                    "old_length": 9,
                    "new_start": 727,
                    "new_length": 6,
                    "hunk": "@@ -738,9 +727,6 @@ class MemoriesComputationTest(jtu.BufferDonationTestCase):\n   def test_device_put_different_device_and_memory_hbm_to_host(self):\n     if jax.device_count() < 3:\n       raise unittest.SkipTest(\"Test requires >=3 devices\")\n-    # TODO(jieying): remove after 12/26/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 32):\n-      raise unittest.SkipTest(\"CopyToMemorySpace is not supported on PJRT C API version < 0.32.\")\n \n     out_hbm0 = jnp.arange(8)\n \n"
                },
                {
                    "old_start": 758,
                    "old_length": 9,
                    "new_start": 744,
                    "new_length": 6,
                    "hunk": "@@ -758,9 +744,6 @@ class MemoriesComputationTest(jtu.BufferDonationTestCase):\n   def test_device_put_on_different_device_with_the_same_memory_kind(self):\n     if len(jax.devices()) < 2:\n       raise unittest.SkipTest(\"Test requires >=2 devices.\")\n-    # TODO(jieying): remove after 12/26/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 32):\n-      raise unittest.SkipTest(\"CopyToMemorySpace is not supported on PJRT C API version < 0.32.\")\n \n     np_inp = np.arange(16).reshape(8, 2)\n \n"
                },
                {
                    "old_start": 779,
                    "old_length": 10,
                    "new_start": 762,
                    "new_length": 6,
                    "hunk": "@@ -779,10 +762,6 @@ class MemoriesComputationTest(jtu.BufferDonationTestCase):\n         out_host_dev_1, np_inp, s_host_dev_1, \"unpinned_host\")\n \n   def test_device_put_resharding(self):\n-    # TODO(jieying): remove after 12/26/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 32):\n-      raise unittest.SkipTest(\"CopyToMemorySpace is not supported on PJRT C API version < 0.32.\")\n-\n     mesh = jtu.create_global_mesh((2, 2), (\"x\", \"y\"))\n     s_host = NamedSharding(mesh, P(\"x\", \"y\"), memory_kind=\"unpinned_host\")\n     s_hbm = s_host.with_memory_kind(\"device\")\n"
                },
                {
                    "old_start": 807,
                    "old_length": 10,
                    "new_start": 786,
                    "new_length": 6,
                    "hunk": "@@ -807,10 +786,6 @@ class MemoriesComputationTest(jtu.BufferDonationTestCase):\n         out_sharded_hbm, np_inp, s_hbm, \"device\")\n \n   def test_jit_host_inputs_via_device_put_outside(self):\n-    # TODO(jieying): remove after 12/26/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 32):\n-      raise unittest.SkipTest(\"CopyToMemorySpace is not supported on PJRT C API version < 0.32.\")\n-\n     mesh = jtu.create_global_mesh((4, 2), (\"x\", \"y\"))\n     s_host = NamedSharding(mesh, P(\"x\", \"y\"), memory_kind=\"unpinned_host\")\n     s_hbm = s_host.with_memory_kind(\"device\")\n"
                }
            ],
            "whole_deleted": "-    # TODO(jieying): remove after 12/26/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 32):\n-      raise unittest.SkipTest(\"CopyToMemorySpace is not supported on PJRT C API version < 0.32.\")\n-\n-    # TODO(jieying): remove after 12/26/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 32):\n-      raise unittest.SkipTest(\"CopyToMemorySpace is not supported on PJRT C API version < 0.32.\")\n-\n-    # TODO(jieying): remove after 12/26/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 32):\n-      raise unittest.SkipTest(\"CopyToMemorySpace is not supported on PJRT C API version < 0.32.\")\n-    # TODO(jieying): remove after 12/26/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 32):\n-      raise unittest.SkipTest(\"CopyToMemorySpace is not supported on PJRT C API version < 0.32.\")\n-    # TODO(jieying): remove after 12/26/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 32):\n-      raise unittest.SkipTest(\"CopyToMemorySpace is not supported on PJRT C API version < 0.32.\")\n-    # TODO(jieying): remove after 12/26/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 32):\n-      raise unittest.SkipTest(\"CopyToMemorySpace is not supported on PJRT C API version < 0.32.\")\n-\n-    # TODO(jieying): remove after 12/26/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 32):\n-      raise unittest.SkipTest(\"CopyToMemorySpace is not supported on PJRT C API version < 0.32.\")\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -677,10 +677,6 @@ class MemoriesComputationTest(jtu.BufferDonationTestCase):\n     self.assertEqual(cache_info2.misses, cache_info1.misses)\n \n   def test_device_put_host_to_hbm(self):\n-    # TODO(jieying): remove after 12/26/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 32):\n-      raise unittest.SkipTest(\"CopyToMemorySpace is not supported on PJRT C API version < 0.32.\")\n-\n     mesh = jtu.create_global_mesh((4, 2), (\"x\", \"y\"))\n     s_host = NamedSharding(mesh, P(\"y\"), memory_kind=\"unpinned_host\")\n     np_inp = jnp.arange(16).reshape(8, 2)\n@@ -698,10 +694,6 @@ class MemoriesComputationTest(jtu.BufferDonationTestCase):\n         out_on_hbm, np_inp, s_hbm, \"device\")\n \n   def test_device_put_hbm_to_host(self):\n-    # TODO(jieying): remove after 12/26/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 32):\n-      raise unittest.SkipTest(\"CopyToMemorySpace is not supported on PJRT C API version < 0.32.\")\n-\n     mesh = jtu.create_global_mesh((4, 2), (\"x\", \"y\"))\n     s_host = NamedSharding(mesh, P(\"y\"), memory_kind=\"unpinned_host\")\n     inp = jnp.arange(16).reshape(8, 2)\n@@ -718,9 +710,6 @@ class MemoriesComputationTest(jtu.BufferDonationTestCase):\n   def test_device_put_different_device_and_memory_host_to_hbm(self):\n     if jax.device_count() < 3:\n       raise unittest.SkipTest(\"Test requires >=3 devices\")\n-    # TODO(jieying): remove after 12/26/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 32):\n-      raise unittest.SkipTest(\"CopyToMemorySpace is not supported on PJRT C API version < 0.32.\")\n \n     out_host0 = jax.device_put(\n         jnp.arange(8),\n@@ -738,9 +727,6 @@ class MemoriesComputationTest(jtu.BufferDonationTestCase):\n   def test_device_put_different_device_and_memory_hbm_to_host(self):\n     if jax.device_count() < 3:\n       raise unittest.SkipTest(\"Test requires >=3 devices\")\n-    # TODO(jieying): remove after 12/26/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 32):\n-      raise unittest.SkipTest(\"CopyToMemorySpace is not supported on PJRT C API version < 0.32.\")\n \n     out_hbm0 = jnp.arange(8)\n \n@@ -758,9 +744,6 @@ class MemoriesComputationTest(jtu.BufferDonationTestCase):\n   def test_device_put_on_different_device_with_the_same_memory_kind(self):\n     if len(jax.devices()) < 2:\n       raise unittest.SkipTest(\"Test requires >=2 devices.\")\n-    # TODO(jieying): remove after 12/26/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 32):\n-      raise unittest.SkipTest(\"CopyToMemorySpace is not supported on PJRT C API version < 0.32.\")\n \n     np_inp = np.arange(16).reshape(8, 2)\n \n@@ -779,10 +762,6 @@ class MemoriesComputationTest(jtu.BufferDonationTestCase):\n         out_host_dev_1, np_inp, s_host_dev_1, \"unpinned_host\")\n \n   def test_device_put_resharding(self):\n-    # TODO(jieying): remove after 12/26/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 32):\n-      raise unittest.SkipTest(\"CopyToMemorySpace is not supported on PJRT C API version < 0.32.\")\n-\n     mesh = jtu.create_global_mesh((2, 2), (\"x\", \"y\"))\n     s_host = NamedSharding(mesh, P(\"x\", \"y\"), memory_kind=\"unpinned_host\")\n     s_hbm = s_host.with_memory_kind(\"device\")\n@@ -807,10 +786,6 @@ class MemoriesComputationTest(jtu.BufferDonationTestCase):\n         out_sharded_hbm, np_inp, s_hbm, \"device\")\n \n   def test_jit_host_inputs_via_device_put_outside(self):\n-    # TODO(jieying): remove after 12/26/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 32):\n-      raise unittest.SkipTest(\"CopyToMemorySpace is not supported on PJRT C API version < 0.32.\")\n-\n     mesh = jtu.create_global_mesh((4, 2), (\"x\", \"y\"))\n     s_host = NamedSharding(mesh, P(\"x\", \"y\"), memory_kind=\"unpinned_host\")\n     s_hbm = s_host.with_memory_kind(\"device\")\n"
        },
        {
            "name": "pgle_test.py",
            "path": "tests/pgle_test.py",
            "patches": [
                {
                    "old_start": 18,
                    "old_length": 7,
                    "new_start": 18,
                    "new_length": 6,
                    "hunk": "@@ -18,7 +18,6 @@ import logging\n import math\n import os\n import tempfile\n-import unittest\n \n from absl.testing import absltest\n import jax\n"
                },
                {
                    "old_start": 37,
                    "old_length": 11,
                    "new_start": 36,
                    "new_length": 6,
                    "hunk": "@@ -37,11 +36,6 @@ config.parse_flags_with_absl()\n class PgleTest(jtu.JaxTestCase):\n \n   def testPassingFDOProfile(self):\n-    # TODO(jieying): remove after 01/10/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 34):\n-      raise unittest.SkipTest(\n-          'Profiler is not supported on PJRT C API version < 0.34.'\n-      )\n     mesh = jtu.create_global_mesh((2,), ('x',))\n     @partial(\n         jax.jit,\n"
                }
            ],
            "whole_deleted": "-import unittest\n-    # TODO(jieying): remove after 01/10/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 34):\n-      raise unittest.SkipTest(\n-          'Profiler is not supported on PJRT C API version < 0.34.'\n-      )\n",
            "whole_added": "",
            "whole_hunk": "@@ -18,7 +18,6 @@ import logging\n import math\n import os\n import tempfile\n-import unittest\n \n from absl.testing import absltest\n import jax\n@@ -37,11 +36,6 @@ config.parse_flags_with_absl()\n class PgleTest(jtu.JaxTestCase):\n \n   def testPassingFDOProfile(self):\n-    # TODO(jieying): remove after 01/10/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 34):\n-      raise unittest.SkipTest(\n-          'Profiler is not supported on PJRT C API version < 0.34.'\n-      )\n     mesh = jtu.create_global_mesh((2,), ('x',))\n     @partial(\n         jax.jit,\n"
        },
        {
            "name": "profiler_test.py",
            "path": "tests/profiler_test.py",
            "patches": [
                {
                    "old_start": 84,
                    "old_length": 11,
                    "new_start": 84,
                    "new_length": 6,
                    "hunk": "@@ -84,11 +84,6 @@ class ProfilerTest(unittest.TestCase):\n       jax.profiler.stop_server()\n \n   def testProgrammaticProfiling(self):\n-    # TODO(jieying): remove after 01/10/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 34):\n-      raise unittest.SkipTest(\n-          \"Profiler is not supported on PJRT C API version < 0.34.\"\n-      )\n     with tempfile.TemporaryDirectory() as tmpdir:\n       try:\n         jax.profiler.start_trace(tmpdir)\n"
                },
                {
                    "old_start": 110,
                    "old_length": 11,
                    "new_start": 105,
                    "new_length": 6,
                    "hunk": "@@ -110,11 +105,6 @@ class ProfilerTest(unittest.TestCase):\n       self.assertIn(b\"pxla.py\", proto)\n \n   def testProfilerGetFDOProfile(self):\n-    # TODO(jieying): remove after 01/10/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 34):\n-      raise unittest.SkipTest(\n-          \"Profiler is not supported on PJRT C API version < 0.34.\"\n-      )\n     # Tests stop_and_get_fod_profile could run.\n     try:\n       jax.profiler.start_trace(\"test\")\n"
                },
                {
                    "old_start": 127,
                    "old_length": 11,
                    "new_start": 117,
                    "new_length": 6,
                    "hunk": "@@ -127,11 +117,6 @@ class ProfilerTest(unittest.TestCase):\n       self.assertIn(b\"copy\", fdo_profile)\n \n   def testProgrammaticProfilingErrors(self):\n-    # TODO(jieying): remove after 01/10/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 34):\n-      raise unittest.SkipTest(\n-          \"Profiler is not supported on PJRT C API version < 0.34.\"\n-      )\n     with self.assertRaisesRegex(RuntimeError, \"No profile started\"):\n       jax.profiler.stop_trace()\n \n"
                },
                {
                    "old_start": 147,
                    "old_length": 11,
                    "new_start": 132,
                    "new_length": 6,
                    "hunk": "@@ -147,11 +132,6 @@ class ProfilerTest(unittest.TestCase):\n       jax.profiler.stop_trace()\n \n   def testProgrammaticProfilingContextManager(self):\n-    # TODO(jieying): remove after 01/10/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 34):\n-      raise unittest.SkipTest(\n-          \"Profiler is not supported on PJRT C API version < 0.34.\"\n-      )\n     with tempfile.TemporaryDirectory() as tmpdir:\n       with jax.profiler.trace(tmpdir):\n         jax.pmap(lambda x: jax.lax.psum(x + 1, 'i'), axis_name='i')(\n"
                },
                {
                    "old_start": 208,
                    "old_length": 11,
                    "new_start": 188,
                    "new_length": 6,
                    "hunk": "@@ -208,11 +188,6 @@ class ProfilerTest(unittest.TestCase):\n   @unittest.skipIf(not (portpicker and profiler_client and tf_profiler),\n     \"Test requires tensorflow.profiler and portpicker\")\n   def testSingleWorkerSamplingMode(self, delay_ms=None):\n-    # TODO(jieying): remove after 01/10/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 34):\n-      raise unittest.SkipTest(\n-          \"Profiler is not supported on PJRT C API version < 0.34.\"\n-      )\n     def on_worker(port, worker_start):\n       jax.profiler.start_server(port)\n       worker_start.set()\n"
                },
                {
                    "old_start": 258,
                    "old_length": 11,
                    "new_start": 233,
                    "new_length": 6,
                    "hunk": "@@ -258,11 +233,6 @@ class ProfilerTest(unittest.TestCase):\n     \"Test requires tensorflow.profiler, portpicker and \"\n     \"tensorboard_profile_plugin\")\n   def test_remote_profiler(self):\n-    # TODO(jieying): remove after 01/10/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 34):\n-      raise unittest.SkipTest(\n-          \"Profiler is not supported on PJRT C API version < 0.34.\"\n-      )\n     port = portpicker.pick_unused_port()\n     jax.profiler.start_server(port)\n "
                }
            ],
            "whole_deleted": "-    # TODO(jieying): remove after 01/10/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 34):\n-      raise unittest.SkipTest(\n-          \"Profiler is not supported on PJRT C API version < 0.34.\"\n-      )\n-    # TODO(jieying): remove after 01/10/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 34):\n-      raise unittest.SkipTest(\n-          \"Profiler is not supported on PJRT C API version < 0.34.\"\n-      )\n-    # TODO(jieying): remove after 01/10/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 34):\n-      raise unittest.SkipTest(\n-          \"Profiler is not supported on PJRT C API version < 0.34.\"\n-      )\n-    # TODO(jieying): remove after 01/10/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 34):\n-      raise unittest.SkipTest(\n-          \"Profiler is not supported on PJRT C API version < 0.34.\"\n-      )\n-    # TODO(jieying): remove after 01/10/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 34):\n-      raise unittest.SkipTest(\n-          \"Profiler is not supported on PJRT C API version < 0.34.\"\n-      )\n-    # TODO(jieying): remove after 01/10/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 34):\n-      raise unittest.SkipTest(\n-          \"Profiler is not supported on PJRT C API version < 0.34.\"\n-      )\n",
            "whole_added": "",
            "whole_hunk": "@@ -84,11 +84,6 @@ class ProfilerTest(unittest.TestCase):\n       jax.profiler.stop_server()\n \n   def testProgrammaticProfiling(self):\n-    # TODO(jieying): remove after 01/10/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 34):\n-      raise unittest.SkipTest(\n-          \"Profiler is not supported on PJRT C API version < 0.34.\"\n-      )\n     with tempfile.TemporaryDirectory() as tmpdir:\n       try:\n         jax.profiler.start_trace(tmpdir)\n@@ -110,11 +105,6 @@ class ProfilerTest(unittest.TestCase):\n       self.assertIn(b\"pxla.py\", proto)\n \n   def testProfilerGetFDOProfile(self):\n-    # TODO(jieying): remove after 01/10/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 34):\n-      raise unittest.SkipTest(\n-          \"Profiler is not supported on PJRT C API version < 0.34.\"\n-      )\n     # Tests stop_and_get_fod_profile could run.\n     try:\n       jax.profiler.start_trace(\"test\")\n@@ -127,11 +117,6 @@ class ProfilerTest(unittest.TestCase):\n       self.assertIn(b\"copy\", fdo_profile)\n \n   def testProgrammaticProfilingErrors(self):\n-    # TODO(jieying): remove after 01/10/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 34):\n-      raise unittest.SkipTest(\n-          \"Profiler is not supported on PJRT C API version < 0.34.\"\n-      )\n     with self.assertRaisesRegex(RuntimeError, \"No profile started\"):\n       jax.profiler.stop_trace()\n \n@@ -147,11 +132,6 @@ class ProfilerTest(unittest.TestCase):\n       jax.profiler.stop_trace()\n \n   def testProgrammaticProfilingContextManager(self):\n-    # TODO(jieying): remove after 01/10/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 34):\n-      raise unittest.SkipTest(\n-          \"Profiler is not supported on PJRT C API version < 0.34.\"\n-      )\n     with tempfile.TemporaryDirectory() as tmpdir:\n       with jax.profiler.trace(tmpdir):\n         jax.pmap(lambda x: jax.lax.psum(x + 1, 'i'), axis_name='i')(\n@@ -208,11 +188,6 @@ class ProfilerTest(unittest.TestCase):\n   @unittest.skipIf(not (portpicker and profiler_client and tf_profiler),\n     \"Test requires tensorflow.profiler and portpicker\")\n   def testSingleWorkerSamplingMode(self, delay_ms=None):\n-    # TODO(jieying): remove after 01/10/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 34):\n-      raise unittest.SkipTest(\n-          \"Profiler is not supported on PJRT C API version < 0.34.\"\n-      )\n     def on_worker(port, worker_start):\n       jax.profiler.start_server(port)\n       worker_start.set()\n@@ -258,11 +233,6 @@ class ProfilerTest(unittest.TestCase):\n     \"Test requires tensorflow.profiler, portpicker and \"\n     \"tensorboard_profile_plugin\")\n   def test_remote_profiler(self):\n-    # TODO(jieying): remove after 01/10/2023.\n-    if not jtu.pjrt_c_api_version_at_least(0, 34):\n-      raise unittest.SkipTest(\n-          \"Profiler is not supported on PJRT C API version < 0.34.\"\n-      )\n     port = portpicker.pick_unused_port()\n     jax.profiler.start_server(port)\n "
        }
    ]
},
{
    "Id": 88,
    "commit_link": "https://github.com/google/jax/commit/61aa7e89aaf7837befb0e3792c1dd99ca3f847e2",
    "date": "2024-02-23T17:16:53-08:00",
    "message": "[Mosaic] Fix bug in divisibility check in infer_vector_layout load and store rules\n\nPiperOrigin-RevId: 609876232",
    "changes": [
        {
            "name": "infer_vector_layout.cc",
            "path": "jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc",
            "patches": [
                {
                    "old_start": 847,
                    "old_length": 7,
                    "new_start": 847,
                    "new_length": 7,
                    "hunk": "@@ -847,7 +847,7 @@ class VectorLayoutInferer {\n         tile_offsets.push_back(cast<IntegerAttr>(cst_op.getValue()).getInt() %\n                                tiling[i]);\n       } else {\n-        if (failed(verifyDivisibleIndex(tiled_index, tiling[0], dim, op))) {\n+        if (failed(verifyDivisibleIndex(tiled_index, tiling[i], dim, op))) {\n           return failure();\n         }\n         tile_offsets.push_back(0);\n"
                },
                {
                    "old_start": 1188,
                    "old_length": 7,
                    "new_start": 1188,
                    "new_length": 7,
                    "hunk": "@@ -1188,7 +1188,7 @@ class VectorLayoutInferer {\n         tile_offsets.push_back(cast<IntegerAttr>(cst_op.getValue()).getInt() %\n                                tiling[i]);\n       } else {\n-        if (failed(verifyDivisibleIndex(tiled_index, tiling[0], dim, op))) {\n+        if (failed(verifyDivisibleIndex(tiled_index, tiling[i], dim, op))) {\n           return failure();\n         }\n         tile_offsets.push_back(0);"
                }
            ],
            "whole_deleted": "-        if (failed(verifyDivisibleIndex(tiled_index, tiling[0], dim, op))) {\n-        if (failed(verifyDivisibleIndex(tiled_index, tiling[0], dim, op))) {\n",
            "whole_added": "+        if (failed(verifyDivisibleIndex(tiled_index, tiling[i], dim, op))) {\n+        if (failed(verifyDivisibleIndex(tiled_index, tiling[i], dim, op))) {\n",
            "whole_hunk": "@@ -847,7 +847,7 @@ class VectorLayoutInferer {\n         tile_offsets.push_back(cast<IntegerAttr>(cst_op.getValue()).getInt() %\n                                tiling[i]);\n       } else {\n-        if (failed(verifyDivisibleIndex(tiled_index, tiling[0], dim, op))) {\n+        if (failed(verifyDivisibleIndex(tiled_index, tiling[i], dim, op))) {\n           return failure();\n         }\n         tile_offsets.push_back(0);\n@@ -1188,7 +1188,7 @@ class VectorLayoutInferer {\n         tile_offsets.push_back(cast<IntegerAttr>(cst_op.getValue()).getInt() %\n                                tiling[i]);\n       } else {\n-        if (failed(verifyDivisibleIndex(tiled_index, tiling[0], dim, op))) {\n+        if (failed(verifyDivisibleIndex(tiled_index, tiling[i], dim, op))) {\n           return failure();\n         }\n         tile_offsets.push_back(0);"
        }
    ]
},
{
    "Id": 89,
    "commit_link": "https://github.com/google/jax/commit/4e61c8856be2fff91d2c519b8e9ff5f03f141165",
    "date": "2024-02-23T10:03:13-08:00",
    "message": "Add a fast path MPMD device_put test.\n\nThis checks that if the devices in a sharding are different, we still take `xc.copy_array_to_devices_with_sharding` path.\n\nThis is to prevent changes to shard_arg handler of Array that checks for devices instead of indices.\n\nPiperOrigin-RevId: 609760758",
    "changes": [
        {
            "name": "test_util.py",
            "path": "jax/_src/test_util.py",
            "patches": [
                {
                    "old_start": 49,
                    "old_length": 6,
                    "new_start": 49,
                    "new_length": 7,
                    "hunk": "@@ -49,6 +49,7 @@ from jax._src import linear_util as lu\n from jax._src import dtypes as _dtypes\n from jax._src import monitoring\n from jax._src import stages\n+from jax._src.lib import xla_client as xc\n from jax._src.cloud_tpu_init import running_in_cloud_tpu_vm\n from jax._src.interpreters import pxla\n from jax._src.numpy.util import promote_dtypes, promote_dtypes_inexact\n"
                },
                {
                    "old_start": 224,
                    "old_length": 6,
                    "new_start": 225,
                    "new_length": 22,
                    "hunk": "@@ -224,6 +225,22 @@ def count_primitive_compiles():\n     count[0] = dispatch.xla_primitive_callable.cache_info().misses\n \n \n+@contextmanager\n+def count_device_put_fast_path_hit():\n+  original_fn = xc.copy_array_to_devices_with_sharding\n+  count = [0]\n+\n+  def copy_array_to_devices_with_sharding_and_count(*args, **kwargs):\n+    count[0] += 1\n+    return original_fn(*args, **kwargs)\n+\n+  xc.copy_array_to_devices_with_sharding = copy_array_to_devices_with_sharding_and_count\n+  try:\n+    yield count\n+  finally:\n+    xc.copy_array_to_devices_with_sharding = original_fn\n+\n+\n @contextmanager\n def count_pjit_cpp_cache_miss():\n   original_pjit_lower = pjit_lib._pjit_lower\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+from jax._src.lib import xla_client as xc\n+@contextmanager\n+def count_device_put_fast_path_hit():\n+  original_fn = xc.copy_array_to_devices_with_sharding\n+  count = [0]\n+\n+  def copy_array_to_devices_with_sharding_and_count(*args, **kwargs):\n+    count[0] += 1\n+    return original_fn(*args, **kwargs)\n+\n+  xc.copy_array_to_devices_with_sharding = copy_array_to_devices_with_sharding_and_count\n+  try:\n+    yield count\n+  finally:\n+    xc.copy_array_to_devices_with_sharding = original_fn\n+\n+\n",
            "whole_hunk": "@@ -49,6 +49,7 @@ from jax._src import linear_util as lu\n from jax._src import dtypes as _dtypes\n from jax._src import monitoring\n from jax._src import stages\n+from jax._src.lib import xla_client as xc\n from jax._src.cloud_tpu_init import running_in_cloud_tpu_vm\n from jax._src.interpreters import pxla\n from jax._src.numpy.util import promote_dtypes, promote_dtypes_inexact\n@@ -224,6 +225,22 @@ def count_primitive_compiles():\n     count[0] = dispatch.xla_primitive_callable.cache_info().misses\n \n \n+@contextmanager\n+def count_device_put_fast_path_hit():\n+  original_fn = xc.copy_array_to_devices_with_sharding\n+  count = [0]\n+\n+  def copy_array_to_devices_with_sharding_and_count(*args, **kwargs):\n+    count[0] += 1\n+    return original_fn(*args, **kwargs)\n+\n+  xc.copy_array_to_devices_with_sharding = copy_array_to_devices_with_sharding_and_count\n+  try:\n+    yield count\n+  finally:\n+    xc.copy_array_to_devices_with_sharding = original_fn\n+\n+\n @contextmanager\n def count_pjit_cpp_cache_miss():\n   original_pjit_lower = pjit_lib._pjit_lower\n"
        },
        {
            "name": "pjit_test.py",
            "path": "tests/pjit_test.py",
            "patches": [
                {
                    "old_start": 3816,
                    "old_length": 6,
                    "new_start": 3816,
                    "new_length": 27,
                    "hunk": "@@ -3816,6 +3816,27 @@ class ArrayPjitTest(jtu.JaxTestCase):\n     self.assertEqual(out.sharding, s)\n     self.assertArraysEqual(out, np_inp)\n \n+  def test_mpmd_device_put_fast_path(self):\n+    if jax.device_count() < 4:\n+      self.skipTest('Needs >= 4 devices')\n+\n+    dev_count = jax.device_count()\n+    mesh1 = jax.sharding.Mesh(jax.devices()[:dev_count//2], 'x')\n+    mesh2 = jax.sharding.Mesh(jax.devices()[dev_count//2:], 'x')\n+    inp = np.arange(8)\n+    arr1 = jax.device_put(inp, NamedSharding(mesh1, P('x')))\n+\n+    # This is to prevent changes to shard_arg_handler of Array which checks for\n+    # indices to take the fast path for resharding. Changes made to the handler\n+    # to check for shardings instead of indices will cause this test to fail and\n+    # that is expected.\n+    with jtu.count_device_put_fast_path_hit() as count:\n+      out = jax.device_put(arr1, NamedSharding(mesh2, P('x')))\n+    self.assertEqual(count[0], 1)\n+    self.assertTupleEqual(out.sharding._device_assignment,\n+                          mesh2._flat_devices_tuple)\n+    self.assertArraysEqual(out, inp)\n+\n \n class TempSharding(Sharding):\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  def test_mpmd_device_put_fast_path(self):\n+    if jax.device_count() < 4:\n+      self.skipTest('Needs >= 4 devices')\n+\n+    dev_count = jax.device_count()\n+    mesh1 = jax.sharding.Mesh(jax.devices()[:dev_count//2], 'x')\n+    mesh2 = jax.sharding.Mesh(jax.devices()[dev_count//2:], 'x')\n+    inp = np.arange(8)\n+    arr1 = jax.device_put(inp, NamedSharding(mesh1, P('x')))\n+\n+    # This is to prevent changes to shard_arg_handler of Array which checks for\n+    # indices to take the fast path for resharding. Changes made to the handler\n+    # to check for shardings instead of indices will cause this test to fail and\n+    # that is expected.\n+    with jtu.count_device_put_fast_path_hit() as count:\n+      out = jax.device_put(arr1, NamedSharding(mesh2, P('x')))\n+    self.assertEqual(count[0], 1)\n+    self.assertTupleEqual(out.sharding._device_assignment,\n+                          mesh2._flat_devices_tuple)\n+    self.assertArraysEqual(out, inp)\n+\n",
            "whole_hunk": "@@ -3816,6 +3816,27 @@ class ArrayPjitTest(jtu.JaxTestCase):\n     self.assertEqual(out.sharding, s)\n     self.assertArraysEqual(out, np_inp)\n \n+  def test_mpmd_device_put_fast_path(self):\n+    if jax.device_count() < 4:\n+      self.skipTest('Needs >= 4 devices')\n+\n+    dev_count = jax.device_count()\n+    mesh1 = jax.sharding.Mesh(jax.devices()[:dev_count//2], 'x')\n+    mesh2 = jax.sharding.Mesh(jax.devices()[dev_count//2:], 'x')\n+    inp = np.arange(8)\n+    arr1 = jax.device_put(inp, NamedSharding(mesh1, P('x')))\n+\n+    # This is to prevent changes to shard_arg_handler of Array which checks for\n+    # indices to take the fast path for resharding. Changes made to the handler\n+    # to check for shardings instead of indices will cause this test to fail and\n+    # that is expected.\n+    with jtu.count_device_put_fast_path_hit() as count:\n+      out = jax.device_put(arr1, NamedSharding(mesh2, P('x')))\n+    self.assertEqual(count[0], 1)\n+    self.assertTupleEqual(out.sharding._device_assignment,\n+                          mesh2._flat_devices_tuple)\n+    self.assertArraysEqual(out, inp)\n+\n \n class TempSharding(Sharding):\n "
        }
    ]
},
{
    "Id": 90,
    "commit_link": "https://github.com/google/jax/commit/46ec581c5598296935a84535732a8a7147199196",
    "date": "2024-02-19T08:51:03-08:00",
    "message": "Added a few missing compute capability checks to Pallas:GPU tests\n\nPiperOrigin-RevId: 608348004",
    "changes": [
        {
            "name": "BUILD",
            "path": "tests/pallas/BUILD",
            "patches": [
                {
                    "old_start": 48,
                    "old_length": 7,
                    "new_start": 48,
                    "new_length": 6,
                    "hunk": "@@ -48,7 +48,6 @@ jax_test(\n     disable_configs = [\n         \"gpu\",\n         \"gpu_a100\",\n-        \"gpu_p100\",\n     ],\n     enable_configs = [\n         \"gpu_x32\",\n"
                }
            ],
            "whole_deleted": "-        \"gpu_p100\",\n",
            "whole_added": "",
            "whole_hunk": "@@ -48,7 +48,6 @@ jax_test(\n     disable_configs = [\n         \"gpu\",\n         \"gpu_a100\",\n-        \"gpu_p100\",\n     ],\n     enable_configs = [\n         \"gpu_x32\",\n"
        },
        {
            "name": "pallas_test.py",
            "path": "tests/pallas/pallas_test.py",
            "patches": [
                {
                    "old_start": 769,
                    "old_length": 6,
                    "new_start": 769,
                    "new_length": 8,
                    "hunk": "@@ -769,6 +769,8 @@ class PallasCallTest(PallasTest):\n     (2, 1, 1),\n   ])\n   def test_atomic_cas(self, init_value, cmp, new_value):\n+    if not self.check_gpu_capability_at_least(70):\n+      raise unittest.SkipTest(\"requires a GPU with compute capability >= sm70\")\n \n     @functools.partial(\n         self.pallas_call, out_shape=(\n"
                },
                {
                    "old_start": 789,
                    "old_length": 6,
                    "new_start": 791,
                    "new_length": 10,
                    "hunk": "@@ -789,6 +791,10 @@ class PallasCallTest(PallasTest):\n   def test_atomic_counter(self, num_threads):\n     if self.INTERPRET:\n       self.skipTest(\"While loop not supported in interpreter mode.\")\n+\n+    if not self.check_gpu_capability_at_least(70):\n+      raise unittest.SkipTest(\"requires a GPU compute capability >= sm70\")\n+\n     @functools.partial(\n         self.pallas_call, out_shape=(\n           jax.ShapeDtypeStruct((), jnp.int32),"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    if not self.check_gpu_capability_at_least(70):\n+      raise unittest.SkipTest(\"requires a GPU with compute capability >= sm70\")\n+\n+    if not self.check_gpu_capability_at_least(70):\n+      raise unittest.SkipTest(\"requires a GPU compute capability >= sm70\")\n+\n",
            "whole_hunk": "@@ -769,6 +769,8 @@ class PallasCallTest(PallasTest):\n     (2, 1, 1),\n   ])\n   def test_atomic_cas(self, init_value, cmp, new_value):\n+    if not self.check_gpu_capability_at_least(70):\n+      raise unittest.SkipTest(\"requires a GPU with compute capability >= sm70\")\n \n     @functools.partial(\n         self.pallas_call, out_shape=(\n@@ -789,6 +791,10 @@ class PallasCallTest(PallasTest):\n   def test_atomic_counter(self, num_threads):\n     if self.INTERPRET:\n       self.skipTest(\"While loop not supported in interpreter mode.\")\n+\n+    if not self.check_gpu_capability_at_least(70):\n+      raise unittest.SkipTest(\"requires a GPU compute capability >= sm70\")\n+\n     @functools.partial(\n         self.pallas_call, out_shape=(\n           jax.ShapeDtypeStruct((), jnp.int32),"
        }
    ]
},
{
    "Id": 91,
    "commit_link": "https://github.com/google/jax/commit/243e7edc5667564184b6a093bf98774617237afd",
    "date": "2024-02-15T14:51:45-08:00",
    "message": "[Mosaic] In apply_vector_layout.cc, check layout validity when reading the attribute\n\nThis allows us to rely on this throughout the code and replace some checks with TPU_ASSERT_*. They have the semantics of an assert and make it clearer that it is an unexpected internal error (instead of unimplemented or invalid user input that we should handle).\n\nNote: the original error messages for some of these checks were using the wrong input names.\nPiperOrigin-RevId: 607463728",
    "changes": [
        {
            "name": "apply_vector_layout.cc",
            "path": "jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc",
            "patches": [
                {
                    "old_start": 519,
                    "old_length": 23,
                    "new_start": 519,
                    "new_length": 42,
                    "hunk": "@@ -519,23 +519,42 @@ FailureOr<SmallVector<Layout>> getLayoutArrayFromAttr(const Attribute attr) {\n   return SmallVector<Layout>{};\n }\n \n+bool layoutIsValidForValue(const Layout &l, const Value v) {\n+  // l must be non-null iff v is of vector type\n+  if (const auto vty = dyn_cast<VectorType>(v.getType())) {\n+    return l.has_value() && l->layout_rank() <= vty.getRank();\n+  }\n+  return !l.has_value();\n+}\n+\n // TODO(tlongeri): Unify with infer_vector_layout.cc's getOutLayout.\n-FailureOr<SmallVector<Layout>> getOutLayout(Operation &op) {\n-  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> out_layout,\n+FailureOr<SmallVector<Layout>> getOutLayouts(Operation &op) {\n+  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> out_layouts,\n                              getLayoutArrayFromAttr(op.getAttr(\"out_layout\")));\n-  if (out_layout.size() != op.getNumResults()) {\n-    return failure();\n+  if (out_layouts.size() != op.getNumResults()) {\n+    return op.emitOpError(\"out_layout size does not match number of results\");\n   }\n-  return out_layout;\n+  for (const auto [l, res] : llvm::zip_equal(out_layouts, op.getResults())) {\n+    if (!layoutIsValidForValue(l, res)) {\n+      return op.emitOpError(\"Invalid output layout\");\n+    }\n+  }\n+  return out_layouts;\n }\n \n-FailureOr<SmallVector<Layout>> getInLayout(Operation &op) {\n-  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> in_layout,\n+FailureOr<SmallVector<Layout>> getInLayouts(Operation &op) {\n+  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> in_layouts,\n                              getLayoutArrayFromAttr(op.getAttr(\"in_layout\")));\n-  if (in_layout.size() != op.getNumOperands()) {\n-    return failure();\n+  if (in_layouts.size() != op.getNumOperands()) {\n+    return op.emitOpError(\"in_layout size does not match number of operands\");\n   }\n-  return in_layout;\n+  for (const auto [l, operand] :\n+       llvm::zip_equal(in_layouts, op.getOperands())) {\n+    if (!layoutIsValidForValue(l, operand)) {\n+      return op.emitOpError(\"Invalid output layout\");\n+    }\n+  }\n+  return in_layouts;\n }\n \n LogicalResult elementwise_op_rule(RewriteContext &ctx, Operation &op,\n"
                },
                {
                    "old_start": 857,
                    "old_length": 7,
                    "new_start": 876,
                    "new_length": 7,
                    "hunk": "@@ -857,7 +876,7 @@ LogicalResult scf_for_rule(RewriteContext &ctx, Operation &op,\n         \"Expected matched layouts in scf.for's inputs and outputs\");\n   }\n   FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> yield_in_layouts,\n-                             getInLayout(*for_op.getBody()->getTerminator()));\n+                             getInLayouts(*for_op.getBody()->getTerminator()));\n   if (!llvm::equal(ArrayRef<Layout>(yield_in_layouts), layouts_out)) {\n     return op.emitOpError(\n         \"Expected matched layouts in scf.yield operands and scf.for's results\");\n"
                },
                {
                    "old_start": 981,
                    "old_length": 7,
                    "new_start": 1000,
                    "new_length": 7,
                    "hunk": "@@ -981,7 +1000,7 @@ LogicalResult scf_if_rule(RewriteContext &ctx, Operation &op,\n   ImplicitLocOpBuilder builder(op.getLoc(), &op);\n   scf::IfOp if_op = cast<scf::IfOp>(op);\n   FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> then_yield_in_layouts,\n-                             getInLayout(*if_op.thenYield()));\n+                             getInLayouts(*if_op.thenYield()));\n   // TODO(tlongeri): ArrayRef<Layout> conversion should not be necessary, fix\n   //                 after LLVM adds const qualifiers to ==/!= operators. Also\n   //                 applies to else_yield_in_layouts comparison below.\n"
                },
                {
                    "old_start": 1000,
                    "old_length": 7,
                    "new_start": 1019,
                    "new_length": 7,
                    "hunk": "@@ -1000,7 +1019,7 @@ LogicalResult scf_if_rule(RewriteContext &ctx, Operation &op,\n     return success();\n   }\n   FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> else_yield_in_layouts,\n-                             getInLayout(*if_op.elseYield()));\n+                             getInLayouts(*if_op.elseYield()));\n   if (!layouts_out.empty() &&\n       ArrayRef<Layout>(else_yield_in_layouts) != layouts_out) {\n     return op.emitOpError(\n"
                },
                {
                    "old_start": 1095,
                    "old_length": 13,
                    "new_start": 1114,
                    "new_length": 9,
                    "hunk": "@@ -1095,13 +1114,9 @@ LogicalResult tpu_load_rule(RewriteContext &ctx, Operation &op,\n                             const ArrayRef<Layout> layouts_in,\n                             const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  if (llvm::any_of(layouts_in,\n-                   [&](const Layout &l) { return l.has_value(); })) {\n-    return op.emitOpError(\"Expected null input layouts\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(llvm::none_of(layouts_in,\n+                              [&](const Layout &l) { return l.has_value(); }));\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   const VectorLayout &layout_out = *layouts_out.front();\n   // We expect the result is already a native-sized vreg.\n   // TODO(b/300493694): Support other bitwidths\n"
                },
                {
                    "old_start": 1349,
                    "old_length": 14,
                    "new_start": 1364,
                    "new_length": 9,
                    "hunk": "@@ -1349,14 +1364,9 @@ LogicalResult tpu_matmul_rule(RewriteContext &ctx, Operation &op,\n                               const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 3);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  for (const Layout &layout : layouts_in) {\n-    if (!layout.has_value()) {\n-      return op.emitOpError(\"Expected non-null input layouts\");\n-    }\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(\n+      llvm::all_of(layouts_in, [&](const Layout &l) { return l.has_value(); }));\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   auto matmul_op = cast<tpu::MatmulOp>(op);\n   return matmul_rule_impl(ctx, *matmul_op, matmul_op.getTransposeLhs(),\n                           matmul_op.getTransposeRhs(), *layouts_in[0],\n"
                },
                {
                    "old_start": 1367,
                    "old_length": 13,
                    "new_start": 1377,
                    "new_length": 9,
                    "hunk": "@@ -1367,13 +1377,9 @@ LogicalResult tpu_store_rule(RewriteContext &ctx, Operation &op,\n                              const ArrayRef<Layout> layouts_in,\n                              const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 0);\n-  if (llvm::any_of(layouts_in.drop_front(),\n-                   [&](const Layout &l) { return l.has_value(); })) {\n-    return op.emitOpError(\"Expected null layouts for tpu.store indices\");\n-  }\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null layout for tpu.store base\");\n-  }\n+  TPU_ASSERT_OP(layouts_in.front().has_value());  // value to store layout\n+  TPU_ASSERT_OP(llvm::none_of(layouts_in.drop_front(),\n+                              [&](const Layout &l) { return l.has_value(); }));\n   OpBuilder builder(&op);\n   const VectorLayout &to_store_layout = *layouts_in.front();\n   // We expect the value to store is already a native-sized vreg.\n"
                },
                {
                    "old_start": 1406,
                    "old_length": 12,
                    "new_start": 1412,
                    "new_length": 8,
                    "hunk": "@@ -1406,12 +1412,8 @@ LogicalResult tpu_bitcast_rule(RewriteContext &ctx, Operation &op,\n                                const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 1);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null input layout\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   const VectorLayout &layout_in = *layouts_in.front();\n   const VectorLayout &layout_out = *layouts_out.front();\n   if (!layout_in.hasNativeTiling(ctx.target_shape) ||\n"
                },
                {
                    "old_start": 1690,
                    "old_length": 9,
                    "new_start": 1692,
                    "new_length": 9,
                    "hunk": "@@ -1690,9 +1692,9 @@ LogicalResult tpu_concatenate_rule(RewriteContext &ctx, Operation &op,\n                                    const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), op.getNumOperands());\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(\n+      llvm::all_of(layouts_in, [](const Layout &l) { return l.has_value(); }));\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   const VectorLayout &layout = *layouts_out.front();\n   for (const Layout &l : layouts_in) {\n     if (l != layout) {\n"
                },
                {
                    "old_start": 1752,
                    "old_length": 9,
                    "new_start": 1754,
                    "new_length": 7,
                    "hunk": "@@ -1752,9 +1754,7 @@ LogicalResult tpu_iota_rule(RewriteContext &ctx, Operation &op,\n                             const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 0);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   const VectorLayout &layout_out = *layouts_out.front();\n   ImplicitLocOpBuilder builder(op.getLoc(), &op);\n   tpu::IotaOp iota_op = cast<tpu::IotaOp>(op);\n"
                },
                {
                    "old_start": 1840,
                    "old_length": 12,
                    "new_start": 1840,
                    "new_length": 8,
                    "hunk": "@@ -1840,12 +1840,8 @@ LogicalResult tpu_gather_rule(RewriteContext &ctx, Operation &op,\n                               const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 1);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null input layout\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   const VectorLayout &layout_in = *layouts_in.front();\n   const VectorLayout &layout_out = *layouts_out.front();\n   if (layout_in.implicit_dim() != VectorLayout::ImplicitDim::kNone ||\n"
                },
                {
                    "old_start": 1951,
                    "old_length": 12,
                    "new_start": 1947,
                    "new_length": 8,
                    "hunk": "@@ -1951,12 +1947,8 @@ LogicalResult tpu_repeat_rule(RewriteContext &ctx, Operation &op,\n                               const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 1);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null input layout\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   const VectorLayout &layout_in = *layouts_in.front();\n   const VectorLayout &layout_out = *layouts_out.front();\n   if (layout_in.implicit_dim() != VectorLayout::ImplicitDim::kNone) {\n"
                },
                {
                    "old_start": 1997,
                    "old_length": 13,
                    "new_start": 1989,
                    "new_length": 9,
                    "hunk": "@@ -1997,13 +1989,9 @@ LogicalResult vector_load_rule(RewriteContext &ctx, Operation &op,\n                                const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n   MLIRContext *const mlir_ctx = op.getContext();\n-  if (llvm::any_of(layouts_in,\n-                   [&](const Layout &l) { return l.has_value(); })) {\n-    return op.emitOpError(\"Expected null input layouts\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(llvm::none_of(layouts_in,\n+                              [&](const Layout &l) { return l.has_value(); }));\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   const VectorLayout &layout_out = *layouts_out.front();\n   ImplicitLocOpBuilder builder(op.getLoc(), &op);\n   auto load_op = cast<vector::LoadOp>(op);\n"
                },
                {
                    "old_start": 2225,
                    "old_length": 9,
                    "new_start": 2213,
                    "new_length": 7,
                    "hunk": "@@ -2225,9 +2213,7 @@ LogicalResult vector_broadcast_rule(RewriteContext &ctx, Operation &op,\n                                     const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 1);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   const Layout &maybe_layout_in = layouts_in.front();\n   const VectorLayout &layout_out = *layouts_out.front();\n   ImplicitLocOpBuilder builder(op.getLoc(), &op);\n"
                },
                {
                    "old_start": 2443,
                    "old_length": 9,
                    "new_start": 2429,
                    "new_length": 7,
                    "hunk": "@@ -2443,9 +2429,7 @@ LogicalResult vector_extract_rule(RewriteContext &ctx, Operation &op,\n   }\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 1);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n   const VectorLayout &layout_in = *layouts_in.front();\n   if (layouts_out.front().has_value()) {\n     return op.emitOpError(\"Not implemented: Only scalar results supported\");\n"
                },
                {
                    "old_start": 2482,
                    "old_length": 14,
                    "new_start": 2466,
                    "new_length": 9,
                    "hunk": "@@ -2482,14 +2466,9 @@ LogicalResult vector_contract_rule(RewriteContext &ctx, Operation &op,\n                                    const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 3);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  for (const Layout &layout : layouts_in) {\n-    if (!layout.has_value()) {\n-      return op.emitOpError(\"Expected non-null input layouts\");\n-    }\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(\n+      llvm::all_of(layouts_in, [&](const Layout &l) { return l.has_value(); }));\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   MLIRContext *const mlir_ctx = ctx.getMLIRContext();\n   Builder builder(mlir_ctx);\n   auto vector_contract_op = cast<vector::ContractionOp>(op);\n"
                },
                {
                    "old_start": 2535,
                    "old_length": 12,
                    "new_start": 2514,
                    "new_length": 8,
                    "hunk": "@@ -2535,12 +2514,8 @@ LogicalResult vector_extract_strided_slice_rule(\n     const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 1);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null input layout\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   const VectorLayout &layout_in = *layouts_in.front();\n   const VectorLayout &layout_out = *layouts_out.front();\n   if (!layout_in.hasNaturalTopology(ctx.target_shape)) {\n"
                },
                {
                    "old_start": 2606,
                    "old_length": 11,
                    "new_start": 2581,
                    "new_length": 8,
                    "hunk": "@@ -2606,11 +2581,8 @@ LogicalResult vector_multi_reduction_rule(RewriteContext &ctx, Operation &op,\n                                           const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 2);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  for (const Layout &layout : layouts_in) {\n-    if (!layout.has_value()) {\n-      return op.emitOpError(\"Expected non-null input layout\");\n-    }\n-  }\n+  TPU_ASSERT_OP(\n+      llvm::all_of(layouts_in, [&](const Layout &l) { return l.has_value(); }));\n   const VectorLayout &src_layout = *layouts_in[0];\n   const VectorLayout &acc_layout = *layouts_in[1];\n   const VectorLayout &dst_layout = *layouts_out[0];\n"
                },
                {
                    "old_start": 2862,
                    "old_length": 12,
                    "new_start": 2834,
                    "new_length": 8,
                    "hunk": "@@ -2862,12 +2834,8 @@ LogicalResult vector_shape_cast_rule(RewriteContext &ctx, Operation &op,\n                                      const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 1);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null input layout\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   using Tiling = std::array<int64_t, 2>;\n   const VectorLayout &layout_in = *layouts_in.front();\n   const VectorLayout &layout_out = *layouts_out.front();\n"
                },
                {
                    "old_start": 3055,
                    "old_length": 12,
                    "new_start": 3023,
                    "new_length": 9,
                    "hunk": "@@ -3055,12 +3023,9 @@ LogicalResult vector_store_rule(RewriteContext &ctx, Operation &op,\n                                 const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 0);\n   MLIRContext *const mlir_ctx = op.getContext();\n-  if (!layouts_in.front().has_value() ||\n-      llvm::any_of(layouts_in.drop_front(),\n-                   [&](const Layout &l) { return l.has_value(); })) {\n-    return op.emitOpError(\n-        \"Expected null input layouts for vector.store indices\");\n-  }\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_OP(llvm::none_of(layouts_in.drop_front(),\n+                              [&](const Layout &l) { return l.has_value(); }));\n   ImplicitLocOpBuilder builder(op.getLoc(), &op);\n   vector::StoreOp store_op = cast<vector::StoreOp>(op);\n   const VectorType ty = store_op.getValueToStore().getType();\n"
                },
                {
                    "old_start": 3230,
                    "old_length": 12,
                    "new_start": 3195,
                    "new_length": 8,
                    "hunk": "@@ -3230,12 +3195,8 @@ LogicalResult vector_transpose_rule(RewriteContext &ctx, Operation &op,\n                                     const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 1);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null input layout\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   const VectorLayout &layout_in = *layouts_in.front();\n   const VectorLayout &layout_out = *layouts_out.front();\n   if (layout_in.implicit_dim() != VectorLayout::ImplicitDim::kNone ||\n"
                },
                {
                    "old_start": 3456,
                    "old_length": 7,
                    "new_start": 3417,
                    "new_length": 7,
                    "hunk": "@@ -3456,7 +3417,7 @@ FailureOr<xla::Array<Value>> disassemble(\n   Operation *const op = op_result.getOwner();\n   const unsigned res_idx = op_result.getResultNumber();\n   FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> def_layouts,\n-                             getOutLayout(*op));\n+                             getOutLayouts(*op));\n   const Layout def_layout = def_layouts[res_idx];\n   TPU_ASSERT_LOC(val.getLoc(), def_layout.has_value());\n   TPU_ASSERT_LOC(val.getLoc(),\n"
                },
                {
                    "old_start": 4156,
                    "old_length": 21,
                    "new_start": 4117,
                    "new_length": 18,
                    "hunk": "@@ -4156,21 +4117,18 @@ LogicalResult applyLayoutOp(RewriteContext &ctx, Operation &op) {\n   // If one of the operands is not of vector type, the corresponding entry in\n   // the layout_in tuple will be None. The same applies to the results of the\n   // operation and the layout_out tuple.\n-  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> layout_out,\n-                             getOutLayout(op));\n-  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> layout_in,\n-                             getInLayout(op));\n-  if (!layout_in.empty() && !isa<tpu::AssumeLayoutOp>(op)) {\n+  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> layouts_out,\n+                             getOutLayouts(op));\n+  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> layouts_in,\n+                             getInLayouts(op));\n+  if (!layouts_in.empty() && !isa<tpu::AssumeLayoutOp>(op)) {\n     // Relayout the operands, if their requested input layouts don't match the\n     // layouts in which they were produced.\n     for (auto [idx, tup] :\n-         llvm::enumerate(llvm::zip(op.getOperands(), layout_in))) {\n+         llvm::enumerate(llvm::zip(op.getOperands(), layouts_in))) {\n       auto [operand, li] = tup;\n       auto vty = dyn_cast<VectorType>(operand.getType());\n-      if ((vty == nullptr) == li.has_value()) {\n-        return op.emitError(\n-            \"Layout should be none iff operand is not a vector\");\n-      }\n+      TPU_ASSERT_EQ_OP(vty != nullptr, li.has_value());\n       if (vty == nullptr) {\n         continue;\n       }\n"
                },
                {
                    "old_start": 4186,
                    "old_length": 11,
                    "new_start": 4144,
                    "new_length": 9,
                    "hunk": "@@ -4186,11 +4144,9 @@ LogicalResult applyLayoutOp(RewriteContext &ctx, Operation &op) {\n       TPU_ASSERT_OP(def_op);\n       const unsigned res_idx = op_result.getResultNumber();\n       FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> def_layouts,\n-                                 getOutLayout(*def_op));\n+                                 getOutLayouts(*def_op));\n       const Layout lo = def_layouts[res_idx];\n-      if (!lo.has_value()) {\n-        return op.emitError() << \"Vector result should have a defined layout\";\n-      }\n+      TPU_ASSERT_OP(lo.has_value());\n       if (lo->generalizes(*li, vty.getShape(), ctx.target_shape)) {\n         continue;\n       }\n"
                },
                {
                    "old_start": 4203,
                    "old_length": 9,
                    "new_start": 4159,
                    "new_length": 9,
                    "hunk": "@@ -4203,9 +4159,9 @@ LogicalResult applyLayoutOp(RewriteContext &ctx, Operation &op) {\n   }\n \n   const bool no_vector_args =\n-      llvm::none_of(layout_out,\n+      llvm::none_of(layouts_out,\n                     [](Layout layout) { return layout.has_value(); }) &&\n-      llvm::none_of(layout_in,\n+      llvm::none_of(layouts_in,\n                     [](Layout layout) { return layout.has_value(); });\n   if (no_vector_args && op.getRegions().empty()) {\n     // We don't need to do anything for scalar operations.\n"
                },
                {
                    "old_start": 4220,
                    "old_length": 10,
                    "new_start": 4176,
                    "new_length": 10,
                    "hunk": "@@ -4220,10 +4176,10 @@ LogicalResult applyLayoutOp(RewriteContext &ctx, Operation &op) {\n   if (auto rule_it = rules().find(op.getName().getStringRef());\n       rule_it != rules().end()) {\n     const rule_type &rule = rule_it->getValue();\n-    return rule(ctx, op, layout_in, layout_out);\n+    return rule(ctx, op, layouts_in, layouts_out);\n   }\n   if (OpTrait::hasElementwiseMappableTraits(&op)) {\n-    return elementwise_op_rule(ctx, op, layout_in, layout_out);\n+    return elementwise_op_rule(ctx, op, layouts_in, layouts_out);\n   }\n   return op.emitError(\"Not implemented: Unsupported operation: \")\n          << op.getName();"
                }
            ],
            "whole_deleted": "-FailureOr<SmallVector<Layout>> getOutLayout(Operation &op) {\n-  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> out_layout,\n-  if (out_layout.size() != op.getNumResults()) {\n-    return failure();\n-  return out_layout;\n-FailureOr<SmallVector<Layout>> getInLayout(Operation &op) {\n-  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> in_layout,\n-  if (in_layout.size() != op.getNumOperands()) {\n-    return failure();\n-  return in_layout;\n-                             getInLayout(*for_op.getBody()->getTerminator()));\n-                             getInLayout(*if_op.thenYield()));\n-                             getInLayout(*if_op.elseYield()));\n-  if (llvm::any_of(layouts_in,\n-                   [&](const Layout &l) { return l.has_value(); })) {\n-    return op.emitOpError(\"Expected null input layouts\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n-  for (const Layout &layout : layouts_in) {\n-    if (!layout.has_value()) {\n-      return op.emitOpError(\"Expected non-null input layouts\");\n-    }\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n-  if (llvm::any_of(layouts_in.drop_front(),\n-                   [&](const Layout &l) { return l.has_value(); })) {\n-    return op.emitOpError(\"Expected null layouts for tpu.store indices\");\n-  }\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null layout for tpu.store base\");\n-  }\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null input layout\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null input layout\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null input layout\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n-  if (llvm::any_of(layouts_in,\n-                   [&](const Layout &l) { return l.has_value(); })) {\n-    return op.emitOpError(\"Expected null input layouts\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n-  for (const Layout &layout : layouts_in) {\n-    if (!layout.has_value()) {\n-      return op.emitOpError(\"Expected non-null input layouts\");\n-    }\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null input layout\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n-  for (const Layout &layout : layouts_in) {\n-    if (!layout.has_value()) {\n-      return op.emitOpError(\"Expected non-null input layout\");\n-    }\n-  }\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null input layout\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n-  if (!layouts_in.front().has_value() ||\n-      llvm::any_of(layouts_in.drop_front(),\n-                   [&](const Layout &l) { return l.has_value(); })) {\n-    return op.emitOpError(\n-        \"Expected null input layouts for vector.store indices\");\n-  }\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null input layout\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n-                             getOutLayout(*op));\n-  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> layout_out,\n-                             getOutLayout(op));\n-  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> layout_in,\n-                             getInLayout(op));\n-  if (!layout_in.empty() && !isa<tpu::AssumeLayoutOp>(op)) {\n-         llvm::enumerate(llvm::zip(op.getOperands(), layout_in))) {\n-      if ((vty == nullptr) == li.has_value()) {\n-        return op.emitError(\n-            \"Layout should be none iff operand is not a vector\");\n-      }\n-                                 getOutLayout(*def_op));\n-      if (!lo.has_value()) {\n-        return op.emitError() << \"Vector result should have a defined layout\";\n-      }\n-      llvm::none_of(layout_out,\n-      llvm::none_of(layout_in,\n-    return rule(ctx, op, layout_in, layout_out);\n-    return elementwise_op_rule(ctx, op, layout_in, layout_out);\n",
            "whole_added": "+bool layoutIsValidForValue(const Layout &l, const Value v) {\n+  // l must be non-null iff v is of vector type\n+  if (const auto vty = dyn_cast<VectorType>(v.getType())) {\n+    return l.has_value() && l->layout_rank() <= vty.getRank();\n+  }\n+  return !l.has_value();\n+}\n+\n+FailureOr<SmallVector<Layout>> getOutLayouts(Operation &op) {\n+  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> out_layouts,\n+  if (out_layouts.size() != op.getNumResults()) {\n+    return op.emitOpError(\"out_layout size does not match number of results\");\n+  for (const auto [l, res] : llvm::zip_equal(out_layouts, op.getResults())) {\n+    if (!layoutIsValidForValue(l, res)) {\n+      return op.emitOpError(\"Invalid output layout\");\n+    }\n+  }\n+  return out_layouts;\n+FailureOr<SmallVector<Layout>> getInLayouts(Operation &op) {\n+  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> in_layouts,\n+  if (in_layouts.size() != op.getNumOperands()) {\n+    return op.emitOpError(\"in_layout size does not match number of operands\");\n+  for (const auto [l, operand] :\n+       llvm::zip_equal(in_layouts, op.getOperands())) {\n+    if (!layoutIsValidForValue(l, operand)) {\n+      return op.emitOpError(\"Invalid output layout\");\n+    }\n+  }\n+  return in_layouts;\n+                             getInLayouts(*for_op.getBody()->getTerminator()));\n+                             getInLayouts(*if_op.thenYield()));\n+                             getInLayouts(*if_op.elseYield()));\n+  TPU_ASSERT_OP(llvm::none_of(layouts_in,\n+                              [&](const Layout &l) { return l.has_value(); }));\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n+  TPU_ASSERT_OP(\n+      llvm::all_of(layouts_in, [&](const Layout &l) { return l.has_value(); }));\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n+  TPU_ASSERT_OP(layouts_in.front().has_value());  // value to store layout\n+  TPU_ASSERT_OP(llvm::none_of(layouts_in.drop_front(),\n+                              [&](const Layout &l) { return l.has_value(); }));\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n+  TPU_ASSERT_OP(\n+      llvm::all_of(layouts_in, [](const Layout &l) { return l.has_value(); }));\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n+  TPU_ASSERT_OP(llvm::none_of(layouts_in,\n+                              [&](const Layout &l) { return l.has_value(); }));\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_OP(\n+      llvm::all_of(layouts_in, [&](const Layout &l) { return l.has_value(); }));\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n+  TPU_ASSERT_OP(\n+      llvm::all_of(layouts_in, [&](const Layout &l) { return l.has_value(); }));\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_OP(llvm::none_of(layouts_in.drop_front(),\n+                              [&](const Layout &l) { return l.has_value(); }));\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n+                             getOutLayouts(*op));\n+  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> layouts_out,\n+                             getOutLayouts(op));\n+  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> layouts_in,\n+                             getInLayouts(op));\n+  if (!layouts_in.empty() && !isa<tpu::AssumeLayoutOp>(op)) {\n+         llvm::enumerate(llvm::zip(op.getOperands(), layouts_in))) {\n+      TPU_ASSERT_EQ_OP(vty != nullptr, li.has_value());\n+                                 getOutLayouts(*def_op));\n+      TPU_ASSERT_OP(lo.has_value());\n+      llvm::none_of(layouts_out,\n+      llvm::none_of(layouts_in,\n+    return rule(ctx, op, layouts_in, layouts_out);\n+    return elementwise_op_rule(ctx, op, layouts_in, layouts_out);\n",
            "whole_hunk": "@@ -519,23 +519,42 @@ FailureOr<SmallVector<Layout>> getLayoutArrayFromAttr(const Attribute attr) {\n   return SmallVector<Layout>{};\n }\n \n+bool layoutIsValidForValue(const Layout &l, const Value v) {\n+  // l must be non-null iff v is of vector type\n+  if (const auto vty = dyn_cast<VectorType>(v.getType())) {\n+    return l.has_value() && l->layout_rank() <= vty.getRank();\n+  }\n+  return !l.has_value();\n+}\n+\n // TODO(tlongeri): Unify with infer_vector_layout.cc's getOutLayout.\n-FailureOr<SmallVector<Layout>> getOutLayout(Operation &op) {\n-  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> out_layout,\n+FailureOr<SmallVector<Layout>> getOutLayouts(Operation &op) {\n+  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> out_layouts,\n                              getLayoutArrayFromAttr(op.getAttr(\"out_layout\")));\n-  if (out_layout.size() != op.getNumResults()) {\n-    return failure();\n+  if (out_layouts.size() != op.getNumResults()) {\n+    return op.emitOpError(\"out_layout size does not match number of results\");\n   }\n-  return out_layout;\n+  for (const auto [l, res] : llvm::zip_equal(out_layouts, op.getResults())) {\n+    if (!layoutIsValidForValue(l, res)) {\n+      return op.emitOpError(\"Invalid output layout\");\n+    }\n+  }\n+  return out_layouts;\n }\n \n-FailureOr<SmallVector<Layout>> getInLayout(Operation &op) {\n-  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> in_layout,\n+FailureOr<SmallVector<Layout>> getInLayouts(Operation &op) {\n+  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> in_layouts,\n                              getLayoutArrayFromAttr(op.getAttr(\"in_layout\")));\n-  if (in_layout.size() != op.getNumOperands()) {\n-    return failure();\n+  if (in_layouts.size() != op.getNumOperands()) {\n+    return op.emitOpError(\"in_layout size does not match number of operands\");\n   }\n-  return in_layout;\n+  for (const auto [l, operand] :\n+       llvm::zip_equal(in_layouts, op.getOperands())) {\n+    if (!layoutIsValidForValue(l, operand)) {\n+      return op.emitOpError(\"Invalid output layout\");\n+    }\n+  }\n+  return in_layouts;\n }\n \n LogicalResult elementwise_op_rule(RewriteContext &ctx, Operation &op,\n@@ -857,7 +876,7 @@ LogicalResult scf_for_rule(RewriteContext &ctx, Operation &op,\n         \"Expected matched layouts in scf.for's inputs and outputs\");\n   }\n   FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> yield_in_layouts,\n-                             getInLayout(*for_op.getBody()->getTerminator()));\n+                             getInLayouts(*for_op.getBody()->getTerminator()));\n   if (!llvm::equal(ArrayRef<Layout>(yield_in_layouts), layouts_out)) {\n     return op.emitOpError(\n         \"Expected matched layouts in scf.yield operands and scf.for's results\");\n@@ -981,7 +1000,7 @@ LogicalResult scf_if_rule(RewriteContext &ctx, Operation &op,\n   ImplicitLocOpBuilder builder(op.getLoc(), &op);\n   scf::IfOp if_op = cast<scf::IfOp>(op);\n   FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> then_yield_in_layouts,\n-                             getInLayout(*if_op.thenYield()));\n+                             getInLayouts(*if_op.thenYield()));\n   // TODO(tlongeri): ArrayRef<Layout> conversion should not be necessary, fix\n   //                 after LLVM adds const qualifiers to ==/!= operators. Also\n   //                 applies to else_yield_in_layouts comparison below.\n@@ -1000,7 +1019,7 @@ LogicalResult scf_if_rule(RewriteContext &ctx, Operation &op,\n     return success();\n   }\n   FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> else_yield_in_layouts,\n-                             getInLayout(*if_op.elseYield()));\n+                             getInLayouts(*if_op.elseYield()));\n   if (!layouts_out.empty() &&\n       ArrayRef<Layout>(else_yield_in_layouts) != layouts_out) {\n     return op.emitOpError(\n@@ -1095,13 +1114,9 @@ LogicalResult tpu_load_rule(RewriteContext &ctx, Operation &op,\n                             const ArrayRef<Layout> layouts_in,\n                             const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  if (llvm::any_of(layouts_in,\n-                   [&](const Layout &l) { return l.has_value(); })) {\n-    return op.emitOpError(\"Expected null input layouts\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(llvm::none_of(layouts_in,\n+                              [&](const Layout &l) { return l.has_value(); }));\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   const VectorLayout &layout_out = *layouts_out.front();\n   // We expect the result is already a native-sized vreg.\n   // TODO(b/300493694): Support other bitwidths\n@@ -1349,14 +1364,9 @@ LogicalResult tpu_matmul_rule(RewriteContext &ctx, Operation &op,\n                               const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 3);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  for (const Layout &layout : layouts_in) {\n-    if (!layout.has_value()) {\n-      return op.emitOpError(\"Expected non-null input layouts\");\n-    }\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(\n+      llvm::all_of(layouts_in, [&](const Layout &l) { return l.has_value(); }));\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   auto matmul_op = cast<tpu::MatmulOp>(op);\n   return matmul_rule_impl(ctx, *matmul_op, matmul_op.getTransposeLhs(),\n                           matmul_op.getTransposeRhs(), *layouts_in[0],\n@@ -1367,13 +1377,9 @@ LogicalResult tpu_store_rule(RewriteContext &ctx, Operation &op,\n                              const ArrayRef<Layout> layouts_in,\n                              const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 0);\n-  if (llvm::any_of(layouts_in.drop_front(),\n-                   [&](const Layout &l) { return l.has_value(); })) {\n-    return op.emitOpError(\"Expected null layouts for tpu.store indices\");\n-  }\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null layout for tpu.store base\");\n-  }\n+  TPU_ASSERT_OP(layouts_in.front().has_value());  // value to store layout\n+  TPU_ASSERT_OP(llvm::none_of(layouts_in.drop_front(),\n+                              [&](const Layout &l) { return l.has_value(); }));\n   OpBuilder builder(&op);\n   const VectorLayout &to_store_layout = *layouts_in.front();\n   // We expect the value to store is already a native-sized vreg.\n@@ -1406,12 +1412,8 @@ LogicalResult tpu_bitcast_rule(RewriteContext &ctx, Operation &op,\n                                const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 1);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null input layout\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   const VectorLayout &layout_in = *layouts_in.front();\n   const VectorLayout &layout_out = *layouts_out.front();\n   if (!layout_in.hasNativeTiling(ctx.target_shape) ||\n@@ -1690,9 +1692,9 @@ LogicalResult tpu_concatenate_rule(RewriteContext &ctx, Operation &op,\n                                    const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), op.getNumOperands());\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(\n+      llvm::all_of(layouts_in, [](const Layout &l) { return l.has_value(); }));\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   const VectorLayout &layout = *layouts_out.front();\n   for (const Layout &l : layouts_in) {\n     if (l != layout) {\n@@ -1752,9 +1754,7 @@ LogicalResult tpu_iota_rule(RewriteContext &ctx, Operation &op,\n                             const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 0);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   const VectorLayout &layout_out = *layouts_out.front();\n   ImplicitLocOpBuilder builder(op.getLoc(), &op);\n   tpu::IotaOp iota_op = cast<tpu::IotaOp>(op);\n@@ -1840,12 +1840,8 @@ LogicalResult tpu_gather_rule(RewriteContext &ctx, Operation &op,\n                               const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 1);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null input layout\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   const VectorLayout &layout_in = *layouts_in.front();\n   const VectorLayout &layout_out = *layouts_out.front();\n   if (layout_in.implicit_dim() != VectorLayout::ImplicitDim::kNone ||\n@@ -1951,12 +1947,8 @@ LogicalResult tpu_repeat_rule(RewriteContext &ctx, Operation &op,\n                               const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 1);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null input layout\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   const VectorLayout &layout_in = *layouts_in.front();\n   const VectorLayout &layout_out = *layouts_out.front();\n   if (layout_in.implicit_dim() != VectorLayout::ImplicitDim::kNone) {\n@@ -1997,13 +1989,9 @@ LogicalResult vector_load_rule(RewriteContext &ctx, Operation &op,\n                                const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n   MLIRContext *const mlir_ctx = op.getContext();\n-  if (llvm::any_of(layouts_in,\n-                   [&](const Layout &l) { return l.has_value(); })) {\n-    return op.emitOpError(\"Expected null input layouts\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(llvm::none_of(layouts_in,\n+                              [&](const Layout &l) { return l.has_value(); }));\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   const VectorLayout &layout_out = *layouts_out.front();\n   ImplicitLocOpBuilder builder(op.getLoc(), &op);\n   auto load_op = cast<vector::LoadOp>(op);\n@@ -2225,9 +2213,7 @@ LogicalResult vector_broadcast_rule(RewriteContext &ctx, Operation &op,\n                                     const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 1);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   const Layout &maybe_layout_in = layouts_in.front();\n   const VectorLayout &layout_out = *layouts_out.front();\n   ImplicitLocOpBuilder builder(op.getLoc(), &op);\n@@ -2443,9 +2429,7 @@ LogicalResult vector_extract_rule(RewriteContext &ctx, Operation &op,\n   }\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 1);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n   const VectorLayout &layout_in = *layouts_in.front();\n   if (layouts_out.front().has_value()) {\n     return op.emitOpError(\"Not implemented: Only scalar results supported\");\n@@ -2482,14 +2466,9 @@ LogicalResult vector_contract_rule(RewriteContext &ctx, Operation &op,\n                                    const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 3);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  for (const Layout &layout : layouts_in) {\n-    if (!layout.has_value()) {\n-      return op.emitOpError(\"Expected non-null input layouts\");\n-    }\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(\n+      llvm::all_of(layouts_in, [&](const Layout &l) { return l.has_value(); }));\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   MLIRContext *const mlir_ctx = ctx.getMLIRContext();\n   Builder builder(mlir_ctx);\n   auto vector_contract_op = cast<vector::ContractionOp>(op);\n@@ -2535,12 +2514,8 @@ LogicalResult vector_extract_strided_slice_rule(\n     const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 1);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null input layout\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   const VectorLayout &layout_in = *layouts_in.front();\n   const VectorLayout &layout_out = *layouts_out.front();\n   if (!layout_in.hasNaturalTopology(ctx.target_shape)) {\n@@ -2606,11 +2581,8 @@ LogicalResult vector_multi_reduction_rule(RewriteContext &ctx, Operation &op,\n                                           const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 2);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  for (const Layout &layout : layouts_in) {\n-    if (!layout.has_value()) {\n-      return op.emitOpError(\"Expected non-null input layout\");\n-    }\n-  }\n+  TPU_ASSERT_OP(\n+      llvm::all_of(layouts_in, [&](const Layout &l) { return l.has_value(); }));\n   const VectorLayout &src_layout = *layouts_in[0];\n   const VectorLayout &acc_layout = *layouts_in[1];\n   const VectorLayout &dst_layout = *layouts_out[0];\n@@ -2862,12 +2834,8 @@ LogicalResult vector_shape_cast_rule(RewriteContext &ctx, Operation &op,\n                                      const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 1);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null input layout\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   using Tiling = std::array<int64_t, 2>;\n   const VectorLayout &layout_in = *layouts_in.front();\n   const VectorLayout &layout_out = *layouts_out.front();\n@@ -3055,12 +3023,9 @@ LogicalResult vector_store_rule(RewriteContext &ctx, Operation &op,\n                                 const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 0);\n   MLIRContext *const mlir_ctx = op.getContext();\n-  if (!layouts_in.front().has_value() ||\n-      llvm::any_of(layouts_in.drop_front(),\n-                   [&](const Layout &l) { return l.has_value(); })) {\n-    return op.emitOpError(\n-        \"Expected null input layouts for vector.store indices\");\n-  }\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_OP(llvm::none_of(layouts_in.drop_front(),\n+                              [&](const Layout &l) { return l.has_value(); }));\n   ImplicitLocOpBuilder builder(op.getLoc(), &op);\n   vector::StoreOp store_op = cast<vector::StoreOp>(op);\n   const VectorType ty = store_op.getValueToStore().getType();\n@@ -3230,12 +3195,8 @@ LogicalResult vector_transpose_rule(RewriteContext &ctx, Operation &op,\n                                     const ArrayRef<Layout> layouts_out) {\n   TPU_ASSERT_EQ_OP(layouts_in.size(), 1);\n   TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  if (!layouts_in.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null input layout\");\n-  }\n-  if (!layouts_out.front().has_value()) {\n-    return op.emitOpError(\"Expected non-null output layout\");\n-  }\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n   const VectorLayout &layout_in = *layouts_in.front();\n   const VectorLayout &layout_out = *layouts_out.front();\n   if (layout_in.implicit_dim() != VectorLayout::ImplicitDim::kNone ||\n@@ -3456,7 +3417,7 @@ FailureOr<xla::Array<Value>> disassemble(\n   Operation *const op = op_result.getOwner();\n   const unsigned res_idx = op_result.getResultNumber();\n   FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> def_layouts,\n-                             getOutLayout(*op));\n+                             getOutLayouts(*op));\n   const Layout def_layout = def_layouts[res_idx];\n   TPU_ASSERT_LOC(val.getLoc(), def_layout.has_value());\n   TPU_ASSERT_LOC(val.getLoc(),\n@@ -4156,21 +4117,18 @@ LogicalResult applyLayoutOp(RewriteContext &ctx, Operation &op) {\n   // If one of the operands is not of vector type, the corresponding entry in\n   // the layout_in tuple will be None. The same applies to the results of the\n   // operation and the layout_out tuple.\n-  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> layout_out,\n-                             getOutLayout(op));\n-  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> layout_in,\n-                             getInLayout(op));\n-  if (!layout_in.empty() && !isa<tpu::AssumeLayoutOp>(op)) {\n+  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> layouts_out,\n+                             getOutLayouts(op));\n+  FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> layouts_in,\n+                             getInLayouts(op));\n+  if (!layouts_in.empty() && !isa<tpu::AssumeLayoutOp>(op)) {\n     // Relayout the operands, if their requested input layouts don't match the\n     // layouts in which they were produced.\n     for (auto [idx, tup] :\n-         llvm::enumerate(llvm::zip(op.getOperands(), layout_in))) {\n+         llvm::enumerate(llvm::zip(op.getOperands(), layouts_in))) {\n       auto [operand, li] = tup;\n       auto vty = dyn_cast<VectorType>(operand.getType());\n-      if ((vty == nullptr) == li.has_value()) {\n-        return op.emitError(\n-            \"Layout should be none iff operand is not a vector\");\n-      }\n+      TPU_ASSERT_EQ_OP(vty != nullptr, li.has_value());\n       if (vty == nullptr) {\n         continue;\n       }\n@@ -4186,11 +4144,9 @@ LogicalResult applyLayoutOp(RewriteContext &ctx, Operation &op) {\n       TPU_ASSERT_OP(def_op);\n       const unsigned res_idx = op_result.getResultNumber();\n       FAILUREOR_ASSIGN_OR_RETURN(const SmallVector<Layout> def_layouts,\n-                                 getOutLayout(*def_op));\n+                                 getOutLayouts(*def_op));\n       const Layout lo = def_layouts[res_idx];\n-      if (!lo.has_value()) {\n-        return op.emitError() << \"Vector result should have a defined layout\";\n-      }\n+      TPU_ASSERT_OP(lo.has_value());\n       if (lo->generalizes(*li, vty.getShape(), ctx.target_shape)) {\n         continue;\n       }\n@@ -4203,9 +4159,9 @@ LogicalResult applyLayoutOp(RewriteContext &ctx, Operation &op) {\n   }\n \n   const bool no_vector_args =\n-      llvm::none_of(layout_out,\n+      llvm::none_of(layouts_out,\n                     [](Layout layout) { return layout.has_value(); }) &&\n-      llvm::none_of(layout_in,\n+      llvm::none_of(layouts_in,\n                     [](Layout layout) { return layout.has_value(); });\n   if (no_vector_args && op.getRegions().empty()) {\n     // We don't need to do anything for scalar operations.\n@@ -4220,10 +4176,10 @@ LogicalResult applyLayoutOp(RewriteContext &ctx, Operation &op) {\n   if (auto rule_it = rules().find(op.getName().getStringRef());\n       rule_it != rules().end()) {\n     const rule_type &rule = rule_it->getValue();\n-    return rule(ctx, op, layout_in, layout_out);\n+    return rule(ctx, op, layouts_in, layouts_out);\n   }\n   if (OpTrait::hasElementwiseMappableTraits(&op)) {\n-    return elementwise_op_rule(ctx, op, layout_in, layout_out);\n+    return elementwise_op_rule(ctx, op, layouts_in, layouts_out);\n   }\n   return op.emitError(\"Not implemented: Unsupported operation: \")\n          << op.getName();"
        }
    ]
},
{
    "Id": 92,
    "commit_link": "https://github.com/google/jax/commit/273b5e29de25cc1e0777ce3fa21f4bfb995dc13c",
    "date": "2024-02-15T11:53:42-08:00",
    "message": "[XLA:Mosaic] Fix dim check in concatenate helper function.\n\nPiperOrigin-RevId: 607405219",
    "changes": [
        {
            "name": "apply_vector_layout.cc",
            "path": "jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc",
            "patches": [
                {
                    "old_start": 147,
                    "old_length": 7,
                    "new_start": 147,
                    "new_length": 12,
                    "hunk": "@@ -147,7 +147,12 @@ xla::Array<Value> concatenate(const ArrayRef<xla::Array<Value>> arrays,\n   SmallVector<int64_t> dims(toArrayRef(arrays[0].dimensions()));\n   CHECK(0 <= axis && axis < dims.size());\n   for (size_t i = 1; i < arrays.size(); ++i) {\n-    CHECK(arrays[i].dimensions() == arrays[0].dimensions());\n+    CHECK_EQ(arrays[i].num_dimensions(), arrays[0].num_dimensions());\n+    for (size_t j = 0; j < arrays[i].num_dimensions(); ++j) {\n+      if (j != axis) {\n+        CHECK_EQ(arrays[i].dim(j), arrays[0].dim(j));\n+      }\n+    }\n     dims[axis] += arrays[i].dim(axis);\n   }\n   xla::Array<Value> res(dims);"
                }
            ],
            "whole_deleted": "-    CHECK(arrays[i].dimensions() == arrays[0].dimensions());\n",
            "whole_added": "+    CHECK_EQ(arrays[i].num_dimensions(), arrays[0].num_dimensions());\n+    for (size_t j = 0; j < arrays[i].num_dimensions(); ++j) {\n+      if (j != axis) {\n+        CHECK_EQ(arrays[i].dim(j), arrays[0].dim(j));\n+      }\n+    }\n",
            "whole_hunk": "@@ -147,7 +147,12 @@ xla::Array<Value> concatenate(const ArrayRef<xla::Array<Value>> arrays,\n   SmallVector<int64_t> dims(toArrayRef(arrays[0].dimensions()));\n   CHECK(0 <= axis && axis < dims.size());\n   for (size_t i = 1; i < arrays.size(); ++i) {\n-    CHECK(arrays[i].dimensions() == arrays[0].dimensions());\n+    CHECK_EQ(arrays[i].num_dimensions(), arrays[0].num_dimensions());\n+    for (size_t j = 0; j < arrays[i].num_dimensions(); ++j) {\n+      if (j != axis) {\n+        CHECK_EQ(arrays[i].dim(j), arrays[0].dim(j));\n+      }\n+    }\n     dims[axis] += arrays[i].dim(axis);\n   }\n   xla::Array<Value> res(dims);"
        }
    ]
},
{
    "Id": 93,
    "commit_link": "https://github.com/google/jax/commit/b4c8b0e4fbcbe815dcb76c974f62d9512ed2ce36",
    "date": "2024-02-14T20:49:08-08:00",
    "message": "Check if the Triton dialect bindings are available in lib/triton.py\n\nIIRC we used to import these bindings in lib/__init__.py which is imported\nas part of the top-level jax package. So, it did make sense to delay the\ncheck until we actually need the bindings.\n\nHowever, we have since moved the bindings to lib/triton.py and thus we could\nmove the check there.\n\nPiperOrigin-RevId: 607196039",
    "changes": [
        {
            "name": "triton.py",
            "path": "jax/_src/lib/triton.py",
            "patches": [
                {
                    "old_start": 14,
                    "old_length": 12,
                    "new_start": 14,
                    "new_length": 11,
                    "hunk": "@@ -14,12 +14,11 @@\n \n # ruff: noqa\n \n-from typing import Any\n-\n-dialect: Any = None\n try:\n   from jaxlib.triton import dialect  # pytype: disable=import-error\n-except ImportError:\n-  # TODO(slebedev): Switch to a jaxlib version guard, once Triton bindings\n-  # are bundled with jaxlib.\n-  pass\n+except ImportError as e:\n+  raise ModuleNotFoundError(\n+      \"Cannot import the Triton bindings. You may need a newer version of\"\n+      \" jaxlib. Try installing a nightly wheel following instructions in\"\n+      \" https://jax.readthedocs.io/en/latest/installation.html#nightly-installation\"\n+  ) from e\n"
                }
            ],
            "whole_deleted": "-from typing import Any\n-\n-dialect: Any = None\n-except ImportError:\n-  # TODO(slebedev): Switch to a jaxlib version guard, once Triton bindings\n-  # are bundled with jaxlib.\n-  pass\n",
            "whole_added": "+except ImportError as e:\n+  raise ModuleNotFoundError(\n+      \"Cannot import the Triton bindings. You may need a newer version of\"\n+      \" jaxlib. Try installing a nightly wheel following instructions in\"\n+      \" https://jax.readthedocs.io/en/latest/installation.html#nightly-installation\"\n+  ) from e\n",
            "whole_hunk": "@@ -14,12 +14,11 @@\n \n # ruff: noqa\n \n-from typing import Any\n-\n-dialect: Any = None\n try:\n   from jaxlib.triton import dialect  # pytype: disable=import-error\n-except ImportError:\n-  # TODO(slebedev): Switch to a jaxlib version guard, once Triton bindings\n-  # are bundled with jaxlib.\n-  pass\n+except ImportError as e:\n+  raise ModuleNotFoundError(\n+      \"Cannot import the Triton bindings. You may need a newer version of\"\n+      \" jaxlib. Try installing a nightly wheel following instructions in\"\n+      \" https://jax.readthedocs.io/en/latest/installation.html#nightly-installation\"\n+  ) from e\n"
        },
        {
            "name": "lowering.py",
            "path": "jax/_src/pallas/triton/lowering.py",
            "patches": [
                {
                    "old_start": 65,
                    "old_length": 13,
                    "new_start": 65,
                    "new_length": 6,
                    "hunk": "@@ -65,13 +65,6 @@ import triton.backends.nvidia.compiler as cb\n # TODO(sharadmv): Enable type checking.\n # mypy: ignore-errors\n \n-if tt_dialect is None:\n-  raise RuntimeError(\n-      \"Cannot import the Triton bindings. You may need a newer version of\"\n-      \" jaxlib. Try installing a nightly wheel following instructions in\"\n-      \" https://jax.readthedocs.io/en/latest/installation.html#nightly-installation\"\n-  )\n-\n map, unsafe_map = util.safe_map, map\n zip, unsafe_zip = util.safe_zip, zip\n partial = functools.partial"
                }
            ],
            "whole_deleted": "-if tt_dialect is None:\n-  raise RuntimeError(\n-      \"Cannot import the Triton bindings. You may need a newer version of\"\n-      \" jaxlib. Try installing a nightly wheel following instructions in\"\n-      \" https://jax.readthedocs.io/en/latest/installation.html#nightly-installation\"\n-  )\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -65,13 +65,6 @@ import triton.backends.nvidia.compiler as cb\n # TODO(sharadmv): Enable type checking.\n # mypy: ignore-errors\n \n-if tt_dialect is None:\n-  raise RuntimeError(\n-      \"Cannot import the Triton bindings. You may need a newer version of\"\n-      \" jaxlib. Try installing a nightly wheel following instructions in\"\n-      \" https://jax.readthedocs.io/en/latest/installation.html#nightly-installation\"\n-  )\n-\n map, unsafe_map = util.safe_map, map\n zip, unsafe_zip = util.safe_zip, zip\n partial = functools.partial"
        }
    ]
},
{
    "Id": 94,
    "commit_link": "https://github.com/google/jax/commit/66a4dc59c69d0c34693c3c046135298eee1286d1",
    "date": "2024-02-13T10:36:26-08:00",
    "message": "Move test for float0 values out of nary_op_weak_type_rule and into unop_dtype_rule.\n\nWith the current code structure, it appears we check for float0s twice for non-unary ops, both in the dtype rule and the weak type rule. The only reason I can see this code is in nary weak type rule is to make sure it is also called for unary operators. But we can achieve that by just putting the code in the unary dtype rule.\n\nIt's also a bit odd that this dtype check is in the weak type rule at all: it's just a standard dtype check, so it should go in the dtype rule. This allows us to use the standard weak type rule.\n\nPiperOrigin-RevId: 606671661",
    "changes": [
        {
            "name": "lax.py",
            "path": "jax/_src/lax/lax.py",
            "patches": [
                {
                    "old_start": 1545,
                    "old_length": 6,
                    "new_start": 1545,
                    "new_length": 15,
                    "hunk": "@@ -1545,6 +1545,15 @@ _strip_weak_type = lambda *args, **_: False\n \n \n def unop_dtype_rule(result_dtype, accepted_dtypes, name, aval, **kwargs):\n+  if aval.dtype == dtypes.float0:\n+    raise TypeError(\n+        f\"Called {name} with a float0 array. \"\n+        \"float0s do not support any operations by design, because they \"\n+        \"are not compatible with non-trivial vector spaces. No implicit dtype \"\n+        \"conversion is done. You can use np.zeros_like(arr, dtype=np.float) \"\n+        \"to cast a float0 array to a regular zeros array. \\n\"\n+        \"If you didn't expect to get a float0 you might have accidentally \"\n+        \"taken a gradient with respect to an integer argument.\")\n   if not any(dtypes.issubdtype(aval.dtype, t) for t in accepted_dtypes):\n     msg = '{} does not accept dtype {}. Accepted dtypes are subtypes of {}.'\n     typename = dtype_to_string(aval.dtype)\n"
                },
                {
                    "old_start": 1555,
                    "old_length": 9,
                    "new_start": 1564,
                    "new_length": 7,
                    "hunk": "@@ -1555,9 +1564,7 @@ def unop_dtype_rule(result_dtype, accepted_dtypes, name, aval, **kwargs):\n \n def unop(result_dtype, accepted_dtypes, name):\n   dtype_rule = partial(unop_dtype_rule, result_dtype, accepted_dtypes, name)\n-  weak_type_rule = partial(_naryop_weak_type_rule, name)\n-  prim = standard_primitive(_attrgetter('shape'), dtype_rule, name,\n-                            weak_type_rule=weak_type_rule)\n+  prim = standard_primitive(_attrgetter('shape'), dtype_rule, name)\n   batching.defvectorized(prim)\n   pe.def_trivial_padding(prim)\n   return prim\n"
                },
                {
                    "old_start": 1619,
                    "old_length": 18,
                    "new_start": 1626,
                    "new_length": 6,
                    "hunk": "@@ -1619,18 +1626,6 @@ def broadcasting_shape_rule(name, *avals):\n \n   return tuple(result_shape)\n \n-def _naryop_weak_type_rule(name, *avals, **kwargs):\n-  if any(aval.dtype == dtypes.float0 for aval in avals):\n-    pos = next(i for i, aval in enumerate(avals) if aval.dtype == dtypes.float0)\n-    raise TypeError(\n-        f\"Called {name} with a float0 at position {pos}. \"\n-        \"float0s do not support any operations by design, because they \"\n-        \"are not compatible with non-trivial vector spaces. No implicit dtype \"\n-        \"conversion is done. You can use np.zeros_like(arr, dtype=np.float) \"\n-        \"to cast a float0 array to a regular zeros array. \\n\"\n-        \"If you didn't expect to get a float0 you might have accidentally \"\n-        \"taken a gradient with respect to an integer argument.\")\n-  return all(aval.weak_type for aval in avals)\n \n def naryop(result_dtype, accepted_dtypes, name, allow_extended_dtype=False,\n            require_same_dtypes=False):\n"
                },
                {
                    "old_start": 1638,
                    "old_length": 9,
                    "new_start": 1633,
                    "new_length": 7,
                    "hunk": "@@ -1638,9 +1633,7 @@ def naryop(result_dtype, accepted_dtypes, name, allow_extended_dtype=False,\n                        allow_extended_dtype=allow_extended_dtype,\n                        require_same=require_same_dtypes)\n   shape_rule = partial(broadcasting_shape_rule, name)\n-  weak_type_rule = partial(_naryop_weak_type_rule, name)\n-  prim = standard_primitive(shape_rule, dtype_rule, name,\n-                            weak_type_rule=weak_type_rule)\n+  prim = standard_primitive(shape_rule, dtype_rule, name)\n   batching.defbroadcasting(prim)\n   pe.def_trivial_padding(prim)\n   return prim"
                }
            ],
            "whole_deleted": "-  weak_type_rule = partial(_naryop_weak_type_rule, name)\n-  prim = standard_primitive(_attrgetter('shape'), dtype_rule, name,\n-                            weak_type_rule=weak_type_rule)\n-def _naryop_weak_type_rule(name, *avals, **kwargs):\n-  if any(aval.dtype == dtypes.float0 for aval in avals):\n-    pos = next(i for i, aval in enumerate(avals) if aval.dtype == dtypes.float0)\n-    raise TypeError(\n-        f\"Called {name} with a float0 at position {pos}. \"\n-        \"float0s do not support any operations by design, because they \"\n-        \"are not compatible with non-trivial vector spaces. No implicit dtype \"\n-        \"conversion is done. You can use np.zeros_like(arr, dtype=np.float) \"\n-        \"to cast a float0 array to a regular zeros array. \\n\"\n-        \"If you didn't expect to get a float0 you might have accidentally \"\n-        \"taken a gradient with respect to an integer argument.\")\n-  return all(aval.weak_type for aval in avals)\n-  weak_type_rule = partial(_naryop_weak_type_rule, name)\n-  prim = standard_primitive(shape_rule, dtype_rule, name,\n-                            weak_type_rule=weak_type_rule)\n",
            "whole_added": "+  if aval.dtype == dtypes.float0:\n+    raise TypeError(\n+        f\"Called {name} with a float0 array. \"\n+        \"float0s do not support any operations by design, because they \"\n+        \"are not compatible with non-trivial vector spaces. No implicit dtype \"\n+        \"conversion is done. You can use np.zeros_like(arr, dtype=np.float) \"\n+        \"to cast a float0 array to a regular zeros array. \\n\"\n+        \"If you didn't expect to get a float0 you might have accidentally \"\n+        \"taken a gradient with respect to an integer argument.\")\n+  prim = standard_primitive(_attrgetter('shape'), dtype_rule, name)\n+  prim = standard_primitive(shape_rule, dtype_rule, name)\n",
            "whole_hunk": "@@ -1545,6 +1545,15 @@ _strip_weak_type = lambda *args, **_: False\n \n \n def unop_dtype_rule(result_dtype, accepted_dtypes, name, aval, **kwargs):\n+  if aval.dtype == dtypes.float0:\n+    raise TypeError(\n+        f\"Called {name} with a float0 array. \"\n+        \"float0s do not support any operations by design, because they \"\n+        \"are not compatible with non-trivial vector spaces. No implicit dtype \"\n+        \"conversion is done. You can use np.zeros_like(arr, dtype=np.float) \"\n+        \"to cast a float0 array to a regular zeros array. \\n\"\n+        \"If you didn't expect to get a float0 you might have accidentally \"\n+        \"taken a gradient with respect to an integer argument.\")\n   if not any(dtypes.issubdtype(aval.dtype, t) for t in accepted_dtypes):\n     msg = '{} does not accept dtype {}. Accepted dtypes are subtypes of {}.'\n     typename = dtype_to_string(aval.dtype)\n@@ -1555,9 +1564,7 @@ def unop_dtype_rule(result_dtype, accepted_dtypes, name, aval, **kwargs):\n \n def unop(result_dtype, accepted_dtypes, name):\n   dtype_rule = partial(unop_dtype_rule, result_dtype, accepted_dtypes, name)\n-  weak_type_rule = partial(_naryop_weak_type_rule, name)\n-  prim = standard_primitive(_attrgetter('shape'), dtype_rule, name,\n-                            weak_type_rule=weak_type_rule)\n+  prim = standard_primitive(_attrgetter('shape'), dtype_rule, name)\n   batching.defvectorized(prim)\n   pe.def_trivial_padding(prim)\n   return prim\n@@ -1619,18 +1626,6 @@ def broadcasting_shape_rule(name, *avals):\n \n   return tuple(result_shape)\n \n-def _naryop_weak_type_rule(name, *avals, **kwargs):\n-  if any(aval.dtype == dtypes.float0 for aval in avals):\n-    pos = next(i for i, aval in enumerate(avals) if aval.dtype == dtypes.float0)\n-    raise TypeError(\n-        f\"Called {name} with a float0 at position {pos}. \"\n-        \"float0s do not support any operations by design, because they \"\n-        \"are not compatible with non-trivial vector spaces. No implicit dtype \"\n-        \"conversion is done. You can use np.zeros_like(arr, dtype=np.float) \"\n-        \"to cast a float0 array to a regular zeros array. \\n\"\n-        \"If you didn't expect to get a float0 you might have accidentally \"\n-        \"taken a gradient with respect to an integer argument.\")\n-  return all(aval.weak_type for aval in avals)\n \n def naryop(result_dtype, accepted_dtypes, name, allow_extended_dtype=False,\n            require_same_dtypes=False):\n@@ -1638,9 +1633,7 @@ def naryop(result_dtype, accepted_dtypes, name, allow_extended_dtype=False,\n                        allow_extended_dtype=allow_extended_dtype,\n                        require_same=require_same_dtypes)\n   shape_rule = partial(broadcasting_shape_rule, name)\n-  weak_type_rule = partial(_naryop_weak_type_rule, name)\n-  prim = standard_primitive(shape_rule, dtype_rule, name,\n-                            weak_type_rule=weak_type_rule)\n+  prim = standard_primitive(shape_rule, dtype_rule, name)\n   batching.defbroadcasting(prim)\n   pe.def_trivial_padding(prim)\n   return prim"
        }
    ]
},
{
    "Id": 95,
    "commit_link": "https://github.com/google/jax/commit/5833b0767b92417505eb99cde1839839551d4867",
    "date": "2024-02-13T07:38:02-08:00",
    "message": "Use an isinstance check rather than dtypes.issubdtype to check whether the dtype in an aval is an extended dtype.\n\nWe don't need the full generality of issubdtype, and this is slightly faster. This operation is very common (e.g., for every aval construction, even with a non-extended dtype).\n\nOn my laptop:\n\n```\nIn [18]: d = jnp.dtype(jnp.int32)\n\nIn [20]: %timeit jax.dtypes.issubdtype(d, jax.dtypes.extended)\n490 ns \u00b1 2.78 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)\n\nIn [22]: %timeit isinstance(d, jax._src.dtypes.ExtendedDType)\n78.3 ns \u00b1 0.111 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000,000 loops each)\n```\n\nPiperOrigin-RevId: 606616884",
    "changes": [
        {
            "name": "core.py",
            "path": "jax/_src/core.py",
            "patches": [
                {
                    "old_start": 1565,
                    "old_length": 7,
                    "new_start": 1565,
                    "new_length": 7,
                    "hunk": "@@ -1565,7 +1565,7 @@ def physical_aval(aval: AbstractValue) -> AbstractValue: ...\n \n def physical_aval(aval):\n   aval_dtype = getattr(aval, 'dtype', None)\n-  if aval_dtype and dtypes.issubdtype(aval_dtype, dtypes.extended):\n+  if aval_dtype and isinstance(aval_dtype, dtypes.ExtendedDType):\n     ctor = type(aval)\n     aval_shape = getattr(aval, 'shape', None)\n     assert aval_shape is not None, (ctor, aval)\n"
                },
                {
                    "old_start": 1576,
                    "old_length": 14,
                    "new_start": 1576,
                    "new_length": 14,
                    "hunk": "@@ -1576,14 +1576,14 @@ def physical_aval(aval):\n     return aval\n \n def _short_dtype_name(dtype) -> str:\n-  if dtypes.issubdtype(dtype, dtypes.extended):\n+  if isinstance(dtype, dtypes.ExtendedDType):\n     return str(dtype)\n   else:\n     return (dtype.name.replace('float', 'f').replace('uint'   , 'u')\n                       .replace('int'  , 'i').replace('complex', 'c'))\n \n def _dtype_object(dtype):\n-  return dtype if dtypes.issubdtype(dtype, dtypes.extended) else np.dtype(dtype)\n+  return dtype if isinstance(dtype, dtypes.ExtendedDType) else np.dtype(dtype)\n \n class UnshapedArray(AbstractValue):\n   __slots__ = ['dtype', 'weak_type']\n"
                },
                {
                    "old_start": 1792,
                    "old_length": 7,
                    "new_start": 1792,
                    "new_length": 7,
                    "hunk": "@@ -1792,7 +1792,7 @@ class ConcreteArray(ShapedArray):\n   _complex = concretization_function_error(complex, True)\n \n def primal_dtype_to_tangent_dtype(primal_dtype):\n-  if dtypes.issubdtype(primal_dtype, dtypes.extended):\n+  if isinstance(primal_dtype, dtypes.ExtendedDType):\n     return primal_dtype._rules.tangent_dtype(primal_dtype)  # type: ignore\n   elif not dtypes.issubdtype(primal_dtype, np.inexact):\n     return dtypes.float0\n"
                }
            ],
            "whole_deleted": "-  if aval_dtype and dtypes.issubdtype(aval_dtype, dtypes.extended):\n-  if dtypes.issubdtype(dtype, dtypes.extended):\n-  return dtype if dtypes.issubdtype(dtype, dtypes.extended) else np.dtype(dtype)\n-  if dtypes.issubdtype(primal_dtype, dtypes.extended):\n",
            "whole_added": "+  if aval_dtype and isinstance(aval_dtype, dtypes.ExtendedDType):\n+  if isinstance(dtype, dtypes.ExtendedDType):\n+  return dtype if isinstance(dtype, dtypes.ExtendedDType) else np.dtype(dtype)\n+  if isinstance(primal_dtype, dtypes.ExtendedDType):\n",
            "whole_hunk": "@@ -1565,7 +1565,7 @@ def physical_aval(aval: AbstractValue) -> AbstractValue: ...\n \n def physical_aval(aval):\n   aval_dtype = getattr(aval, 'dtype', None)\n-  if aval_dtype and dtypes.issubdtype(aval_dtype, dtypes.extended):\n+  if aval_dtype and isinstance(aval_dtype, dtypes.ExtendedDType):\n     ctor = type(aval)\n     aval_shape = getattr(aval, 'shape', None)\n     assert aval_shape is not None, (ctor, aval)\n@@ -1576,14 +1576,14 @@ def physical_aval(aval):\n     return aval\n \n def _short_dtype_name(dtype) -> str:\n-  if dtypes.issubdtype(dtype, dtypes.extended):\n+  if isinstance(dtype, dtypes.ExtendedDType):\n     return str(dtype)\n   else:\n     return (dtype.name.replace('float', 'f').replace('uint'   , 'u')\n                       .replace('int'  , 'i').replace('complex', 'c'))\n \n def _dtype_object(dtype):\n-  return dtype if dtypes.issubdtype(dtype, dtypes.extended) else np.dtype(dtype)\n+  return dtype if isinstance(dtype, dtypes.ExtendedDType) else np.dtype(dtype)\n \n class UnshapedArray(AbstractValue):\n   __slots__ = ['dtype', 'weak_type']\n@@ -1792,7 +1792,7 @@ class ConcreteArray(ShapedArray):\n   _complex = concretization_function_error(complex, True)\n \n def primal_dtype_to_tangent_dtype(primal_dtype):\n-  if dtypes.issubdtype(primal_dtype, dtypes.extended):\n+  if isinstance(primal_dtype, dtypes.ExtendedDType):\n     return primal_dtype._rules.tangent_dtype(primal_dtype)  # type: ignore\n   elif not dtypes.issubdtype(primal_dtype, np.inexact):\n     return dtypes.float0\n"
        },
        {
            "name": "lax.py",
            "path": "jax/_src/lax/lax.py",
            "patches": [
                {
                    "old_start": 1570,
                    "old_length": 7,
                    "new_start": 1570,
                    "new_length": 7,
                    "hunk": "@@ -1570,7 +1570,7 @@ def naryop_dtype_rule(result_dtype, accepted_dtypes, name, *avals,\n   del kwargs\n   assert len(avals) == len(accepted_dtypes), (avals, accepted_dtypes)\n   for i, aval in enumerate(avals):\n-    if allow_extended_dtype and dtypes.issubdtype(aval.dtype, dtypes.extended):\n+    if allow_extended_dtype and isinstance(aval.dtype, dtypes.ExtendedDType):\n       continue\n     types = accepted_dtypes[i]\n     if not any(dtypes.issubdtype(aval.dtype, t) for t in types):"
                }
            ],
            "whole_deleted": "-    if allow_extended_dtype and dtypes.issubdtype(aval.dtype, dtypes.extended):\n",
            "whole_added": "+    if allow_extended_dtype and isinstance(aval.dtype, dtypes.ExtendedDType):\n",
            "whole_hunk": "@@ -1570,7 +1570,7 @@ def naryop_dtype_rule(result_dtype, accepted_dtypes, name, *avals,\n   del kwargs\n   assert len(avals) == len(accepted_dtypes), (avals, accepted_dtypes)\n   for i, aval in enumerate(avals):\n-    if allow_extended_dtype and dtypes.issubdtype(aval.dtype, dtypes.extended):\n+    if allow_extended_dtype and isinstance(aval.dtype, dtypes.ExtendedDType):\n       continue\n     types = accepted_dtypes[i]\n     if not any(dtypes.issubdtype(aval.dtype, t) for t in types):"
        }
    ]
},
{
    "Id": 96,
    "commit_link": "https://github.com/google/jax/commit/59307e96251cec8ac416db99f4967a5699465480",
    "date": "2024-02-09T09:05:09-08:00",
    "message": "add jax.cudnn & add check for bias/mask sharding",
    "changes": [
        {
            "name": "fused_attention_stablehlo.py",
            "path": "jax/_src/cudnn/fused_attention_stablehlo.py",
            "patches": [
                {
                    "old_start": 27,
                    "old_length": 7,
                    "new_start": 27,
                    "new_length": 6,
                    "hunk": "@@ -27,7 +27,6 @@ from jax._src.lib.mlir.dialects import hlo\n from jax._src.core import ShapedArray\n \n from jax.experimental.custom_partitioning import custom_partitioning\n-from jax.experimental.pjit import pjit\n from jax.sharding import Mesh, PartitionSpec, NamedSharding\n \n from jax._src.interpreters import batching\n"
                },
                {
                    "old_start": 214,
                    "old_length": 7,
                    "new_start": 213,
                    "new_length": 8,
                    "hunk": "@@ -214,7 +213,8 @@ def check_is_flash_attention(query, key):\n     # check if regular fused attention is supported\n     is_flash_attention = False\n   else:\n-    raise NotImplementedError(\"Unsupported sequence length and head dim.\")\n+    raise NotImplementedError(\n+      f\"Unsupported sequence length Q {q_seq_len}, KV {kv_sqe_len} and head dim {head_dim}.\")\n   return is_flash_attention, is_cross_attention\n \n def check_cudnn_version(is_flash_attention, is_cross_attention):\n"
                },
                {
                    "old_start": 533,
                    "old_length": 46,
                    "new_start": 533,
                    "new_length": 71,
                    "hunk": "@@ -533,46 +533,71 @@ def _get_padded_spec(arg_info):\n   assert len(spec) <= ndim\n   return spec + (None,) * (ndim - len(spec))\n \n+def _check_qkv_bias_mask_spec(query_spec, key_spec, value_spec, bias_spec, mask_spec):\n+  # check qkv spec\n+  if not query_spec == key_spec == value_spec:\n+    raise ValueError(\"Query, key and value should have same sharding.\")\n+  *batch_spec, q_seq_spec, num_head_spec, head_spec = query_spec\n+  if q_seq_spec != None:\n+    raise ValueError(\"Sharding on sequence dim is not allowed.\")\n+  if head_spec != None:\n+    raise ValueError(\"Sharding on head dim is not allowed.\")\n+  # check bias and mask spec\n+  if bias_spec:\n+    *bias_batch_spec, bias_num_head_spec, bias_q_seq_spec, bias_kv_seq_spec = bias_spec\n+    if bias_batch_spec != batch_spec or bias_num_head_spec != num_head_spec:\n+      raise ValueError(\"Query and bias should have same sharding on batch and num_head dim.\")\n+    if bias_q_seq_spec != None or bias_kv_seq_spec != None:\n+      raise ValueError(\"Sharding on bias sequence dim is not allowed.\")\n+  if mask_spec:\n+    *mask_batch_spec, mask_num_head_spec, mask_q_seq_spec, mask_kv_seq_spec = mask_spec\n+    if mask_batch_spec != batch_spec or mask_num_head_spec != num_head_spec:\n+      raise ValueError(\"Query and mask should have same sharding on batch and num_head dim.\")\n+    if mask_q_seq_spec != None or mask_kv_seq_spec != None:\n+      raise ValueError(\"Sharding on mask sequence dim is not allowed.\")\n+\n # fwd custom partition\n-def _infer_fwd_output_sharding(mesh, arg_shapes):\n+def _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args):\n   # only sharding on batch and num_head dim is allowed\n   # (*batch, q_seq, num_head, head)\n   query_spec = _get_padded_spec(arg_shapes[0])\n   # (*batch, kv_seq, num_head, head)\n   key_spec = _get_padded_spec(arg_shapes[1])\n   value_spec = _get_padded_spec(arg_shapes[2])\n-  if not query_spec == key_spec == value_spec:\n-    raise ValueError(\"Query, key and value should have same sharding.\")\n-  seq_spec = query_spec[-3]\n-  head_spec = query_spec[-1]\n-  if seq_spec != None:\n-    raise ValueError(\"Sharding on sequence dim is not allowed.\")\n-  if head_spec != None:\n-    raise ValueError(\"Sharding on head dim is not allowed.\")\n+  has_bias, has_mask = variadic_args\n+  bias_spec = _get_padded_spec(arg_shapes[3]) if has_bias else None\n+  mask_spec = _get_padded_spec(arg_shapes[4]) if has_mask else None\n+  _check_qkv_bias_mask_spec(query_spec, key_spec, value_spec, bias_spec, mask_spec)\n   # keep out sharding same as query sharding since they have same shape\n   out_sharding = NamedSharding(mesh, PartitionSpec(*query_spec))\n   # activation sharding\n-  activation_sharding = NamedSharding(mesh, PartitionSpec(*query_spec[:-3], query_spec[-2], query_spec[-3], None))\n+  *batch_spec, q_seq_spec, num_head_spec, head_spec = query_spec\n+  activation_sharding = NamedSharding(mesh, PartitionSpec(*batch_spec, num_head_spec, q_seq_spec, None))\n   return (out_sharding, activation_sharding)\n \n _dot_product_attention_fwd_lower = custom_partitioning(_dot_product_attention_fwd_impl, static_argnums=(5,6,7,8,9,10))\n def _dot_product_attention_fwd_infer_sharding_from_operands(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, mesh, arg_shapes, result_shape):\n-  return _infer_fwd_output_sharding(mesh, arg_shapes)\n+  return _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args)\n \n def _dot_product_attention_fwd_partition(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, mesh, arg_shapes, result_shape):\n   # args sharding\n   arg_shardings = tuple([arg_i.sharding for arg_i in arg_shapes])\n-  out_shardings = _infer_fwd_output_sharding(mesh, arg_shapes)\n+  out_shardings = _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args)\n   impl = partial(_dot_product_attention_fwd_impl, scale=scale, seed=seed, dropout_rate=dropout_rate,\n                 variadic_args=variadic_args, is_flash_attention=is_flash_attention, is_causal_mask=is_causal_mask)\n   return mesh, impl, out_shardings, arg_shardings\n \n # bwd custom partition\n-def _infer_bwd_output_sharding(mesh, arg_shapes):\n+def _infer_bwd_output_sharding(mesh, arg_shapes, variadic_args):\n   # (*batch, q_seq, num_head, head)\n   query_spec = _get_padded_spec(arg_shapes[0])\n   # (*batch, kv_seq, num_head, head)\n   key_spec = _get_padded_spec(arg_shapes[1])\n+  value_spec = _get_padded_spec(arg_shapes[2])\n+  has_bias, has_mask = variadic_args\n+  bias_spec = _get_padded_spec(arg_shapes[3]) if has_bias else None\n+  mask_spec = _get_padded_spec(arg_shapes[4]) if has_mask else None\n+  _check_qkv_bias_mask_spec(query_spec, key_spec, value_spec, bias_spec, mask_spec)\n   # keep grad query sharding same as query sharding\n   grad_query_sharding = NamedSharding(mesh, PartitionSpec(*query_spec))\n   grad_key_sharding = NamedSharding(mesh, PartitionSpec(*key_spec))\n"
                },
                {
                    "old_start": 582,
                    "old_length": 10,
                    "new_start": 607,
                    "new_length": 10,
                    "hunk": "@@ -582,10 +607,10 @@ def _infer_bwd_output_sharding(mesh, arg_shapes):\n \n _dot_product_attention_bwd_lower = custom_partitioning(_dot_product_attention_bwd_impl, static_argnums=(8,9,10,11,12,13))\n def _dot_product_attention_bwd_infer_sharding_from_operands(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, mesh, arg_shapes, result_shape):\n-  return _infer_bwd_output_sharding(mesh, arg_shapes)\n+  return _infer_bwd_output_sharding(mesh, arg_shapes, variadic_args)\n \n def _dot_product_attention_bwd_partition(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, mesh, arg_shapes, result_shape):\n-  out_shardings = _infer_bwd_output_sharding(mesh, arg_shapes)\n+  out_shardings = _infer_bwd_output_sharding(mesh, arg_shapes, variadic_args)\n   # args sharding\n   arg_shardings = tuple([arg_i.sharding for arg_i in arg_shapes])\n   impl = partial(_dot_product_attention_bwd_impl, scale=scale, seed=seed, dropout_rate=dropout_rate,\n"
                },
                {
                    "old_start": 700,
                    "old_length": 7,
                    "new_start": 725,
                    "new_length": 7,
                    "hunk": "@@ -700,7 +725,7 @@ def dot_product_attention(query: Array,\n     mask: mask used mask out logits with shape of `[batch, num_heads,\n     q_length, kv_length]`.\n     scale: scale for the query.\n-    dropout_rate: dropout rate\n+    dropout_rate: dropout rate.\n   Returns:\n     Output of shape `[batch, q_length, num_heads, v_depth_per_head]`.\n   \"\"\"\n"
                }
            ],
            "whole_deleted": "-from jax.experimental.pjit import pjit\n-    raise NotImplementedError(\"Unsupported sequence length and head dim.\")\n-def _infer_fwd_output_sharding(mesh, arg_shapes):\n-  if not query_spec == key_spec == value_spec:\n-    raise ValueError(\"Query, key and value should have same sharding.\")\n-  seq_spec = query_spec[-3]\n-  head_spec = query_spec[-1]\n-  if seq_spec != None:\n-    raise ValueError(\"Sharding on sequence dim is not allowed.\")\n-  if head_spec != None:\n-    raise ValueError(\"Sharding on head dim is not allowed.\")\n-  activation_sharding = NamedSharding(mesh, PartitionSpec(*query_spec[:-3], query_spec[-2], query_spec[-3], None))\n-  return _infer_fwd_output_sharding(mesh, arg_shapes)\n-  out_shardings = _infer_fwd_output_sharding(mesh, arg_shapes)\n-def _infer_bwd_output_sharding(mesh, arg_shapes):\n-  return _infer_bwd_output_sharding(mesh, arg_shapes)\n-  out_shardings = _infer_bwd_output_sharding(mesh, arg_shapes)\n-    dropout_rate: dropout rate\n",
            "whole_added": "+    raise NotImplementedError(\n+      f\"Unsupported sequence length Q {q_seq_len}, KV {kv_sqe_len} and head dim {head_dim}.\")\n+def _check_qkv_bias_mask_spec(query_spec, key_spec, value_spec, bias_spec, mask_spec):\n+  # check qkv spec\n+  if not query_spec == key_spec == value_spec:\n+    raise ValueError(\"Query, key and value should have same sharding.\")\n+  *batch_spec, q_seq_spec, num_head_spec, head_spec = query_spec\n+  if q_seq_spec != None:\n+    raise ValueError(\"Sharding on sequence dim is not allowed.\")\n+  if head_spec != None:\n+    raise ValueError(\"Sharding on head dim is not allowed.\")\n+  # check bias and mask spec\n+  if bias_spec:\n+    *bias_batch_spec, bias_num_head_spec, bias_q_seq_spec, bias_kv_seq_spec = bias_spec\n+    if bias_batch_spec != batch_spec or bias_num_head_spec != num_head_spec:\n+      raise ValueError(\"Query and bias should have same sharding on batch and num_head dim.\")\n+    if bias_q_seq_spec != None or bias_kv_seq_spec != None:\n+      raise ValueError(\"Sharding on bias sequence dim is not allowed.\")\n+  if mask_spec:\n+    *mask_batch_spec, mask_num_head_spec, mask_q_seq_spec, mask_kv_seq_spec = mask_spec\n+    if mask_batch_spec != batch_spec or mask_num_head_spec != num_head_spec:\n+      raise ValueError(\"Query and mask should have same sharding on batch and num_head dim.\")\n+    if mask_q_seq_spec != None or mask_kv_seq_spec != None:\n+      raise ValueError(\"Sharding on mask sequence dim is not allowed.\")\n+\n+def _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args):\n+  has_bias, has_mask = variadic_args\n+  bias_spec = _get_padded_spec(arg_shapes[3]) if has_bias else None\n+  mask_spec = _get_padded_spec(arg_shapes[4]) if has_mask else None\n+  _check_qkv_bias_mask_spec(query_spec, key_spec, value_spec, bias_spec, mask_spec)\n+  *batch_spec, q_seq_spec, num_head_spec, head_spec = query_spec\n+  activation_sharding = NamedSharding(mesh, PartitionSpec(*batch_spec, num_head_spec, q_seq_spec, None))\n+  return _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args)\n+  out_shardings = _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args)\n+def _infer_bwd_output_sharding(mesh, arg_shapes, variadic_args):\n+  value_spec = _get_padded_spec(arg_shapes[2])\n+  has_bias, has_mask = variadic_args\n+  bias_spec = _get_padded_spec(arg_shapes[3]) if has_bias else None\n+  mask_spec = _get_padded_spec(arg_shapes[4]) if has_mask else None\n+  _check_qkv_bias_mask_spec(query_spec, key_spec, value_spec, bias_spec, mask_spec)\n+  return _infer_bwd_output_sharding(mesh, arg_shapes, variadic_args)\n+  out_shardings = _infer_bwd_output_sharding(mesh, arg_shapes, variadic_args)\n+    dropout_rate: dropout rate.\n",
            "whole_hunk": "@@ -27,7 +27,6 @@ from jax._src.lib.mlir.dialects import hlo\n from jax._src.core import ShapedArray\n \n from jax.experimental.custom_partitioning import custom_partitioning\n-from jax.experimental.pjit import pjit\n from jax.sharding import Mesh, PartitionSpec, NamedSharding\n \n from jax._src.interpreters import batching\n@@ -214,7 +213,8 @@ def check_is_flash_attention(query, key):\n     # check if regular fused attention is supported\n     is_flash_attention = False\n   else:\n-    raise NotImplementedError(\"Unsupported sequence length and head dim.\")\n+    raise NotImplementedError(\n+      f\"Unsupported sequence length Q {q_seq_len}, KV {kv_sqe_len} and head dim {head_dim}.\")\n   return is_flash_attention, is_cross_attention\n \n def check_cudnn_version(is_flash_attention, is_cross_attention):\n@@ -533,46 +533,71 @@ def _get_padded_spec(arg_info):\n   assert len(spec) <= ndim\n   return spec + (None,) * (ndim - len(spec))\n \n+def _check_qkv_bias_mask_spec(query_spec, key_spec, value_spec, bias_spec, mask_spec):\n+  # check qkv spec\n+  if not query_spec == key_spec == value_spec:\n+    raise ValueError(\"Query, key and value should have same sharding.\")\n+  *batch_spec, q_seq_spec, num_head_spec, head_spec = query_spec\n+  if q_seq_spec != None:\n+    raise ValueError(\"Sharding on sequence dim is not allowed.\")\n+  if head_spec != None:\n+    raise ValueError(\"Sharding on head dim is not allowed.\")\n+  # check bias and mask spec\n+  if bias_spec:\n+    *bias_batch_spec, bias_num_head_spec, bias_q_seq_spec, bias_kv_seq_spec = bias_spec\n+    if bias_batch_spec != batch_spec or bias_num_head_spec != num_head_spec:\n+      raise ValueError(\"Query and bias should have same sharding on batch and num_head dim.\")\n+    if bias_q_seq_spec != None or bias_kv_seq_spec != None:\n+      raise ValueError(\"Sharding on bias sequence dim is not allowed.\")\n+  if mask_spec:\n+    *mask_batch_spec, mask_num_head_spec, mask_q_seq_spec, mask_kv_seq_spec = mask_spec\n+    if mask_batch_spec != batch_spec or mask_num_head_spec != num_head_spec:\n+      raise ValueError(\"Query and mask should have same sharding on batch and num_head dim.\")\n+    if mask_q_seq_spec != None or mask_kv_seq_spec != None:\n+      raise ValueError(\"Sharding on mask sequence dim is not allowed.\")\n+\n # fwd custom partition\n-def _infer_fwd_output_sharding(mesh, arg_shapes):\n+def _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args):\n   # only sharding on batch and num_head dim is allowed\n   # (*batch, q_seq, num_head, head)\n   query_spec = _get_padded_spec(arg_shapes[0])\n   # (*batch, kv_seq, num_head, head)\n   key_spec = _get_padded_spec(arg_shapes[1])\n   value_spec = _get_padded_spec(arg_shapes[2])\n-  if not query_spec == key_spec == value_spec:\n-    raise ValueError(\"Query, key and value should have same sharding.\")\n-  seq_spec = query_spec[-3]\n-  head_spec = query_spec[-1]\n-  if seq_spec != None:\n-    raise ValueError(\"Sharding on sequence dim is not allowed.\")\n-  if head_spec != None:\n-    raise ValueError(\"Sharding on head dim is not allowed.\")\n+  has_bias, has_mask = variadic_args\n+  bias_spec = _get_padded_spec(arg_shapes[3]) if has_bias else None\n+  mask_spec = _get_padded_spec(arg_shapes[4]) if has_mask else None\n+  _check_qkv_bias_mask_spec(query_spec, key_spec, value_spec, bias_spec, mask_spec)\n   # keep out sharding same as query sharding since they have same shape\n   out_sharding = NamedSharding(mesh, PartitionSpec(*query_spec))\n   # activation sharding\n-  activation_sharding = NamedSharding(mesh, PartitionSpec(*query_spec[:-3], query_spec[-2], query_spec[-3], None))\n+  *batch_spec, q_seq_spec, num_head_spec, head_spec = query_spec\n+  activation_sharding = NamedSharding(mesh, PartitionSpec(*batch_spec, num_head_spec, q_seq_spec, None))\n   return (out_sharding, activation_sharding)\n \n _dot_product_attention_fwd_lower = custom_partitioning(_dot_product_attention_fwd_impl, static_argnums=(5,6,7,8,9,10))\n def _dot_product_attention_fwd_infer_sharding_from_operands(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, mesh, arg_shapes, result_shape):\n-  return _infer_fwd_output_sharding(mesh, arg_shapes)\n+  return _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args)\n \n def _dot_product_attention_fwd_partition(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, mesh, arg_shapes, result_shape):\n   # args sharding\n   arg_shardings = tuple([arg_i.sharding for arg_i in arg_shapes])\n-  out_shardings = _infer_fwd_output_sharding(mesh, arg_shapes)\n+  out_shardings = _infer_fwd_output_sharding(mesh, arg_shapes, variadic_args)\n   impl = partial(_dot_product_attention_fwd_impl, scale=scale, seed=seed, dropout_rate=dropout_rate,\n                 variadic_args=variadic_args, is_flash_attention=is_flash_attention, is_causal_mask=is_causal_mask)\n   return mesh, impl, out_shardings, arg_shardings\n \n # bwd custom partition\n-def _infer_bwd_output_sharding(mesh, arg_shapes):\n+def _infer_bwd_output_sharding(mesh, arg_shapes, variadic_args):\n   # (*batch, q_seq, num_head, head)\n   query_spec = _get_padded_spec(arg_shapes[0])\n   # (*batch, kv_seq, num_head, head)\n   key_spec = _get_padded_spec(arg_shapes[1])\n+  value_spec = _get_padded_spec(arg_shapes[2])\n+  has_bias, has_mask = variadic_args\n+  bias_spec = _get_padded_spec(arg_shapes[3]) if has_bias else None\n+  mask_spec = _get_padded_spec(arg_shapes[4]) if has_mask else None\n+  _check_qkv_bias_mask_spec(query_spec, key_spec, value_spec, bias_spec, mask_spec)\n   # keep grad query sharding same as query sharding\n   grad_query_sharding = NamedSharding(mesh, PartitionSpec(*query_spec))\n   grad_key_sharding = NamedSharding(mesh, PartitionSpec(*key_spec))\n@@ -582,10 +607,10 @@ def _infer_bwd_output_sharding(mesh, arg_shapes):\n \n _dot_product_attention_bwd_lower = custom_partitioning(_dot_product_attention_bwd_impl, static_argnums=(8,9,10,11,12,13))\n def _dot_product_attention_bwd_infer_sharding_from_operands(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, mesh, arg_shapes, result_shape):\n-  return _infer_bwd_output_sharding(mesh, arg_shapes)\n+  return _infer_bwd_output_sharding(mesh, arg_shapes, variadic_args)\n \n def _dot_product_attention_bwd_partition(scale, seed, dropout_rate, variadic_args, is_flash_attention, is_causal_mask, mesh, arg_shapes, result_shape):\n-  out_shardings = _infer_bwd_output_sharding(mesh, arg_shapes)\n+  out_shardings = _infer_bwd_output_sharding(mesh, arg_shapes, variadic_args)\n   # args sharding\n   arg_shardings = tuple([arg_i.sharding for arg_i in arg_shapes])\n   impl = partial(_dot_product_attention_bwd_impl, scale=scale, seed=seed, dropout_rate=dropout_rate,\n@@ -700,7 +725,7 @@ def dot_product_attention(query: Array,\n     mask: mask used mask out logits with shape of `[batch, num_heads,\n     q_length, kv_length]`.\n     scale: scale for the query.\n-    dropout_rate: dropout rate\n+    dropout_rate: dropout rate.\n   Returns:\n     Output of shape `[batch, q_length, num_heads, v_depth_per_head]`.\n   \"\"\"\n"
        },
        {
            "name": "__init__.py",
            "path": "jax/cudnn/__init__.py",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 15,
                    "hunk": "@@ -0,0 +1,15 @@\n+# Copyright 2024 The JAX Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from jax._src.cudnn.fused_attention_stablehlo import dot_product_attention\n\\ No newline at end of file\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+# Copyright 2024 The JAX Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from jax._src.cudnn.fused_attention_stablehlo import dot_product_attention\n",
            "whole_hunk": "@@ -0,0 +1,15 @@\n+# Copyright 2024 The JAX Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from jax._src.cudnn.fused_attention_stablehlo import dot_product_attention\n\\ No newline at end of file\n"
        },
        {
            "name": "fused_attention_stablehlo_test.py",
            "path": "tests/fused_attention_stablehlo_test.py",
            "patches": [
                {
                    "old_start": 25,
                    "old_length": 7,
                    "new_start": 25,
                    "new_length": 7,
                    "hunk": "@@ -25,7 +25,7 @@ from jax.sharding import Mesh\n from jax.sharding import PartitionSpec, NamedSharding\n from jax._src import config\n from jax._src import test_util as jtu\n-from jax._src.cudnn.fused_attention_stablehlo import dot_product_attention\n+from jax.cudnn import dot_product_attention\n \n config.parse_flags_with_absl()\n Array = jnp.ndarray\n"
                },
                {
                    "old_start": 76,
                    "old_length": 6,
                    "new_start": 76,
                    "new_length": 8,
                    "hunk": "@@ -76,6 +76,8 @@ def sdpa_ref(query: Array,\n     bias = get_causal_mask(attn_weights)\n   if bias is not None:\n     attn_weights = attn_weights + bias.astype(attn_weights.dtype)\n+  if mask is not None:\n+    attn_weights = jax.lax.select(mask, attn_weights, large_negative_number)\n   attn_weights = jax.nn.softmax(attn_weights)\n   if dropout_rate > 0.:\n     keep_prob = 1.0 - dropout_rate\n"
                },
                {
                    "old_start": 107,
                    "old_length": 6,
                    "new_start": 109,
                    "new_length": 7,
                    "hunk": "@@ -107,6 +109,7 @@ class DotProductAttentionTest(jtu.JaxTestCase):\n       num_heads=[8],\n       head_dim=[64, 128],\n       use_bias=[True],\n+      use_mask=[True],\n       is_causal_mask=[False],\n       dropout_rate=[0, 0.5],\n       scale=[0.5],\n"
                },
                {
                    "old_start": 114,
                    "old_length": 7,
                    "new_start": 117,
                    "new_length": 7,
                    "hunk": "@@ -114,7 +117,7 @@ class DotProductAttentionTest(jtu.JaxTestCase):\n   )\n   @jtu.run_on_devices(\"cuda\")\n   def test_sdpa(self, batch_size: int, seq_len: int, num_heads: int,\n-                head_dim: int, use_bias: bool, is_causal_mask: bool,\n+                head_dim: int, use_bias: bool, use_mask: bool, is_causal_mask: bool,\n                 dropout_rate: float, scale: float, dtype: jnp.dtype):\n     if seq_len == 256 and is_causal_mask:\n       self.skipTest(\"Fused attention does not support mask generation.\")\n"
                },
                {
                    "old_start": 123,
                    "old_length": 7,
                    "new_start": 126,
                    "new_length": 7,
                    "hunk": "@@ -123,7 +126,7 @@ class DotProductAttentionTest(jtu.JaxTestCase):\n     if len(jax.local_devices()) <= 4:\n       self.skipTest(\"Require at least 4 devices to run sharding tests.\")\n \n-    k1, k2, k3, k4, k5 = jax.random.split(jax.random.key(0), 5)\n+    k1, k2, k3, k4, k5, k6 = jax.random.split(jax.random.key(0), 6)\n     query = jax.random.normal(\n         k1, (batch_size, seq_len, num_heads, head_dim), dtype=dtype)\n     key = jax.random.normal(\n"
                },
                {
                    "old_start": 137,
                    "old_length": 7,
                    "new_start": 140,
                    "new_length": 11,
                    "hunk": "@@ -137,7 +140,11 @@ class DotProductAttentionTest(jtu.JaxTestCase):\n         k5, (batch_size, num_heads, seq_len, seq_len), dtype=dtype)\n     else:\n       bias = None\n-\n+    if use_mask:\n+      mask = jax.random.bernoulli(\n+        k5, 0.5, (batch_size, num_heads, seq_len, seq_len)).astype(dtype)\n+    else:\n+      mask = None\n     devices = np.array(jax.local_devices()[:4])\n     devices = devices.reshape((2, 2))\n     with Mesh(devices, ('dp', 'tp')) as mesh:\n"
                },
                {
                    "old_start": 145,
                    "old_length": 17,
                    "new_start": 152,
                    "new_length": 22,
                    "hunk": "@@ -145,17 +152,22 @@ class DotProductAttentionTest(jtu.JaxTestCase):\n       qkv_sharding = NamedSharding(mesh, qkv_spec)\n       if bias is not None:\n         bias_spec = PartitionSpec('dp', 'tp', None, None)\n+        mask_spec = PartitionSpec('dp', 'tp', None, None)\n       else:\n         bias_spec = PartitionSpec()\n+        mask_spec = PartitionSpec()\n       bias_sharding = NamedSharding(mesh, bias_spec)\n+      mask_sharding = NamedSharding(mesh, mask_spec)\n       replicated = NamedSharding(mesh, PartitionSpec())\n       query = jax.device_put(query, qkv_sharding)\n       key = jax.device_put(key, qkv_sharding)\n       value = jax.device_put(value, qkv_sharding)\n       if bias is not None:\n         bias = jax.device_put(bias, bias_sharding)\n+      if mask is not None:\n+        mask = jax.device_put(mask, mask_sharding)\n       grad = jax.device_put(grad, qkv_sharding)\n-      in_shardings = (qkv_sharding, qkv_sharding, qkv_sharding, qkv_sharding, bias_sharding, replicated)\n+      in_shardings = (qkv_sharding, qkv_sharding, qkv_sharding, qkv_sharding, bias_sharding, mask_sharding)\n       out_shardings = (replicated, (qkv_sharding, qkv_sharding, qkv_sharding))\n       jitted_sdpa_train = jax.jit(\n         partial(sdpa_train, scale=scale, is_causal_mask=is_causal_mask, dropout_rate=dropout_rate),\n"
                },
                {
                    "old_start": 169,
                    "old_length": 8,
                    "new_start": 181,
                    "new_length": 8,
                    "hunk": "@@ -169,8 +181,8 @@ class DotProductAttentionTest(jtu.JaxTestCase):\n         out_shardings=out_shardings\n       )\n \n-      out, (query_grad, key_grad, value_grad) = jitted_sdpa_train(query, key, value, grad, bias, None)\n-      out_ref, (query_grad_ref, key_grad_ref, value_grad_ref) = jitted_sdpa_train_ref(query, key, value, grad, bias, None)\n+      out, (query_grad, key_grad, value_grad) = jitted_sdpa_train(query, key, value, grad, bias, mask)\n+      out_ref, (query_grad_ref, key_grad_ref, value_grad_ref) = jitted_sdpa_train_ref(query, key, value, grad, bias, mask)\n       self.assertArraysAllClose(out_ref, out, rtol=1e-5, atol=1e-5)\n       if seq_len > 512:\n         # query_grad in flash attention is not deterministic"
                }
            ],
            "whole_deleted": "-from jax._src.cudnn.fused_attention_stablehlo import dot_product_attention\n-                head_dim: int, use_bias: bool, is_causal_mask: bool,\n-    k1, k2, k3, k4, k5 = jax.random.split(jax.random.key(0), 5)\n-\n-      in_shardings = (qkv_sharding, qkv_sharding, qkv_sharding, qkv_sharding, bias_sharding, replicated)\n-      out, (query_grad, key_grad, value_grad) = jitted_sdpa_train(query, key, value, grad, bias, None)\n-      out_ref, (query_grad_ref, key_grad_ref, value_grad_ref) = jitted_sdpa_train_ref(query, key, value, grad, bias, None)\n",
            "whole_added": "+from jax.cudnn import dot_product_attention\n+  if mask is not None:\n+    attn_weights = jax.lax.select(mask, attn_weights, large_negative_number)\n+      use_mask=[True],\n+                head_dim: int, use_bias: bool, use_mask: bool, is_causal_mask: bool,\n+    k1, k2, k3, k4, k5, k6 = jax.random.split(jax.random.key(0), 6)\n+    if use_mask:\n+      mask = jax.random.bernoulli(\n+        k5, 0.5, (batch_size, num_heads, seq_len, seq_len)).astype(dtype)\n+    else:\n+      mask = None\n+        mask_spec = PartitionSpec('dp', 'tp', None, None)\n+        mask_spec = PartitionSpec()\n+      mask_sharding = NamedSharding(mesh, mask_spec)\n+      if mask is not None:\n+        mask = jax.device_put(mask, mask_sharding)\n+      in_shardings = (qkv_sharding, qkv_sharding, qkv_sharding, qkv_sharding, bias_sharding, mask_sharding)\n+      out, (query_grad, key_grad, value_grad) = jitted_sdpa_train(query, key, value, grad, bias, mask)\n+      out_ref, (query_grad_ref, key_grad_ref, value_grad_ref) = jitted_sdpa_train_ref(query, key, value, grad, bias, mask)\n",
            "whole_hunk": "@@ -25,7 +25,7 @@ from jax.sharding import Mesh\n from jax.sharding import PartitionSpec, NamedSharding\n from jax._src import config\n from jax._src import test_util as jtu\n-from jax._src.cudnn.fused_attention_stablehlo import dot_product_attention\n+from jax.cudnn import dot_product_attention\n \n config.parse_flags_with_absl()\n Array = jnp.ndarray\n@@ -76,6 +76,8 @@ def sdpa_ref(query: Array,\n     bias = get_causal_mask(attn_weights)\n   if bias is not None:\n     attn_weights = attn_weights + bias.astype(attn_weights.dtype)\n+  if mask is not None:\n+    attn_weights = jax.lax.select(mask, attn_weights, large_negative_number)\n   attn_weights = jax.nn.softmax(attn_weights)\n   if dropout_rate > 0.:\n     keep_prob = 1.0 - dropout_rate\n@@ -107,6 +109,7 @@ class DotProductAttentionTest(jtu.JaxTestCase):\n       num_heads=[8],\n       head_dim=[64, 128],\n       use_bias=[True],\n+      use_mask=[True],\n       is_causal_mask=[False],\n       dropout_rate=[0, 0.5],\n       scale=[0.5],\n@@ -114,7 +117,7 @@ class DotProductAttentionTest(jtu.JaxTestCase):\n   )\n   @jtu.run_on_devices(\"cuda\")\n   def test_sdpa(self, batch_size: int, seq_len: int, num_heads: int,\n-                head_dim: int, use_bias: bool, is_causal_mask: bool,\n+                head_dim: int, use_bias: bool, use_mask: bool, is_causal_mask: bool,\n                 dropout_rate: float, scale: float, dtype: jnp.dtype):\n     if seq_len == 256 and is_causal_mask:\n       self.skipTest(\"Fused attention does not support mask generation.\")\n@@ -123,7 +126,7 @@ class DotProductAttentionTest(jtu.JaxTestCase):\n     if len(jax.local_devices()) <= 4:\n       self.skipTest(\"Require at least 4 devices to run sharding tests.\")\n \n-    k1, k2, k3, k4, k5 = jax.random.split(jax.random.key(0), 5)\n+    k1, k2, k3, k4, k5, k6 = jax.random.split(jax.random.key(0), 6)\n     query = jax.random.normal(\n         k1, (batch_size, seq_len, num_heads, head_dim), dtype=dtype)\n     key = jax.random.normal(\n@@ -137,7 +140,11 @@ class DotProductAttentionTest(jtu.JaxTestCase):\n         k5, (batch_size, num_heads, seq_len, seq_len), dtype=dtype)\n     else:\n       bias = None\n-\n+    if use_mask:\n+      mask = jax.random.bernoulli(\n+        k5, 0.5, (batch_size, num_heads, seq_len, seq_len)).astype(dtype)\n+    else:\n+      mask = None\n     devices = np.array(jax.local_devices()[:4])\n     devices = devices.reshape((2, 2))\n     with Mesh(devices, ('dp', 'tp')) as mesh:\n@@ -145,17 +152,22 @@ class DotProductAttentionTest(jtu.JaxTestCase):\n       qkv_sharding = NamedSharding(mesh, qkv_spec)\n       if bias is not None:\n         bias_spec = PartitionSpec('dp', 'tp', None, None)\n+        mask_spec = PartitionSpec('dp', 'tp', None, None)\n       else:\n         bias_spec = PartitionSpec()\n+        mask_spec = PartitionSpec()\n       bias_sharding = NamedSharding(mesh, bias_spec)\n+      mask_sharding = NamedSharding(mesh, mask_spec)\n       replicated = NamedSharding(mesh, PartitionSpec())\n       query = jax.device_put(query, qkv_sharding)\n       key = jax.device_put(key, qkv_sharding)\n       value = jax.device_put(value, qkv_sharding)\n       if bias is not None:\n         bias = jax.device_put(bias, bias_sharding)\n+      if mask is not None:\n+        mask = jax.device_put(mask, mask_sharding)\n       grad = jax.device_put(grad, qkv_sharding)\n-      in_shardings = (qkv_sharding, qkv_sharding, qkv_sharding, qkv_sharding, bias_sharding, replicated)\n+      in_shardings = (qkv_sharding, qkv_sharding, qkv_sharding, qkv_sharding, bias_sharding, mask_sharding)\n       out_shardings = (replicated, (qkv_sharding, qkv_sharding, qkv_sharding))\n       jitted_sdpa_train = jax.jit(\n         partial(sdpa_train, scale=scale, is_causal_mask=is_causal_mask, dropout_rate=dropout_rate),\n@@ -169,8 +181,8 @@ class DotProductAttentionTest(jtu.JaxTestCase):\n         out_shardings=out_shardings\n       )\n \n-      out, (query_grad, key_grad, value_grad) = jitted_sdpa_train(query, key, value, grad, bias, None)\n-      out_ref, (query_grad_ref, key_grad_ref, value_grad_ref) = jitted_sdpa_train_ref(query, key, value, grad, bias, None)\n+      out, (query_grad, key_grad, value_grad) = jitted_sdpa_train(query, key, value, grad, bias, mask)\n+      out_ref, (query_grad_ref, key_grad_ref, value_grad_ref) = jitted_sdpa_train_ref(query, key, value, grad, bias, mask)\n       self.assertArraysAllClose(out_ref, out, rtol=1e-5, atol=1e-5)\n       if seq_len > 512:\n         # query_grad in flash attention is not deterministic"
        }
    ]
},
{
    "Id": 97,
    "commit_link": "https://github.com/google/jax/commit/983bb32ae61bc8c9837b737ee8039e337968f441",
    "date": "2024-02-08T10:09:47+01:00",
    "message": "[shape_poly] Add limited support for equality explicit constraints.\n\nPreviously, we have added support for inequality constraints. Now\nwe also add equality constraints. These are useful when encountering\nerrors due to inability to check equalities of symbolic expressions,\ne.g., in the broadcasting rules. For example, the current decision\nprocedure cannot decide that `mod(mod(a, 2), 2) == mod(a, 2)`.\nTo work around such limitations, it is now possible to add\nthe above as an equality constraint.\nLike other explicit constraints, this will be used to decide equalities during staging, and will be checked at shape refinement time.\n\nSee more details in the README.md changes.",
    "changes": [
        {
            "name": "core.py",
            "path": "jax/_src/core.py",
            "patches": [
                {
                    "old_start": 2016,
                    "old_length": 7,
                    "new_start": 2016,
                    "new_length": 9,
                    "hunk": "@@ -2016,7 +2016,9 @@ def divide_shape_sizes(s1: Shape, s2: Shape) -> DimSize:\n     return 1\n   q, r = divmod(sz1, sz2)\n   if isinstance(r, Tracer) or r != 0:\n-    raise InconclusiveDimensionOperation(f\"Cannot divide evenly the sizes of shapes {tuple(s1)} and {tuple(s2)}\")\n+    raise InconclusiveDimensionOperation(\n+        f\"Cannot divide evenly the sizes of shapes {tuple(s1)} and {tuple(s2)}. \"\n+        f\"The remainder {r} should be 0.\")\n   return q\n \n def cancel_divide_tracers(num, denom):\n"
                }
            ],
            "whole_deleted": "-    raise InconclusiveDimensionOperation(f\"Cannot divide evenly the sizes of shapes {tuple(s1)} and {tuple(s2)}\")\n",
            "whole_added": "+    raise InconclusiveDimensionOperation(\n+        f\"Cannot divide evenly the sizes of shapes {tuple(s1)} and {tuple(s2)}. \"\n+        f\"The remainder {r} should be 0.\")\n",
            "whole_hunk": "@@ -2016,7 +2016,9 @@ def divide_shape_sizes(s1: Shape, s2: Shape) -> DimSize:\n     return 1\n   q, r = divmod(sz1, sz2)\n   if isinstance(r, Tracer) or r != 0:\n-    raise InconclusiveDimensionOperation(f\"Cannot divide evenly the sizes of shapes {tuple(s1)} and {tuple(s2)}\")\n+    raise InconclusiveDimensionOperation(\n+        f\"Cannot divide evenly the sizes of shapes {tuple(s1)} and {tuple(s2)}. \"\n+        f\"The remainder {r} should be 0.\")\n   return q\n \n def cancel_divide_tracers(num, denom):\n"
        },
        {
            "name": "lax.py",
            "path": "jax/_src/lax/lax.py",
            "patches": [
                {
                    "old_start": 3396,
                    "old_length": 10,
                    "new_start": 3396,
                    "new_length": 14,
                    "hunk": "@@ -3396,10 +3396,14 @@ def _reshape_shape_rule(operand, *, new_sizes, dimensions):\n     msg = 'reshape new_sizes must all be positive, got {}.'\n     raise TypeError(msg.format(new_sizes))\n   # TODO(necula): re-enable this check\n+  operand_size = math.prod(np.shape(operand))\n+  new_size = math.prod(new_sizes)\n   if (not config.dynamic_shapes.value and\n-      not math.prod(np.shape(operand)) == math.prod(new_sizes)):\n-    msg = 'reshape total size must be unchanged, got new_sizes {} for shape {}.'\n-    raise TypeError(msg.format(new_sizes, np.shape(operand)))\n+      not operand_size == new_size):\n+    msg = (f\"reshape total size must be unchanged, got new_sizes {new_sizes} \"\n+           f\"(of total size {new_size}) for shape {np.shape(operand)} \"\n+           f\"(of total size {operand_size}).\")\n+    raise TypeError(msg)\n   if dimensions is not None:\n     if set(dimensions) != set(range(np.ndim(operand))):\n       msg = ('reshape dimensions must be a permutation of operand dimensions, '\n"
                }
            ],
            "whole_deleted": "-      not math.prod(np.shape(operand)) == math.prod(new_sizes)):\n-    msg = 'reshape total size must be unchanged, got new_sizes {} for shape {}.'\n-    raise TypeError(msg.format(new_sizes, np.shape(operand)))\n",
            "whole_added": "+  operand_size = math.prod(np.shape(operand))\n+  new_size = math.prod(new_sizes)\n+      not operand_size == new_size):\n+    msg = (f\"reshape total size must be unchanged, got new_sizes {new_sizes} \"\n+           f\"(of total size {new_size}) for shape {np.shape(operand)} \"\n+           f\"(of total size {operand_size}).\")\n+    raise TypeError(msg)\n",
            "whole_hunk": "@@ -3396,10 +3396,14 @@ def _reshape_shape_rule(operand, *, new_sizes, dimensions):\n     msg = 'reshape new_sizes must all be positive, got {}.'\n     raise TypeError(msg.format(new_sizes))\n   # TODO(necula): re-enable this check\n+  operand_size = math.prod(np.shape(operand))\n+  new_size = math.prod(new_sizes)\n   if (not config.dynamic_shapes.value and\n-      not math.prod(np.shape(operand)) == math.prod(new_sizes)):\n-    msg = 'reshape total size must be unchanged, got new_sizes {} for shape {}.'\n-    raise TypeError(msg.format(new_sizes, np.shape(operand)))\n+      not operand_size == new_size):\n+    msg = (f\"reshape total size must be unchanged, got new_sizes {new_sizes} \"\n+           f\"(of total size {new_size}) for shape {np.shape(operand)} \"\n+           f\"(of total size {operand_size}).\")\n+    raise TypeError(msg)\n   if dimensions is not None:\n     if set(dimensions) != set(range(np.ndim(operand))):\n       msg = ('reshape dimensions must be a permutation of operand dimensions, '\n"
        },
        {
            "name": "_shape_poly.py",
            "path": "jax/experimental/export/_shape_poly.py",
            "patches": [
                {
                    "old_start": 40,
                    "old_length": 7,
                    "new_start": 40,
                    "new_length": 6,
                    "hunk": "@@ -40,7 +40,6 @@ from enum import Enum\n import functools\n import itertools\n import io\n-import math\n import operator as op\n import threading\n import tokenize\n"
                },
                {
                    "old_start": 73,
                    "old_length": 6,
                    "new_start": 72,
                    "new_length": 10,
                    "hunk": "@@ -73,6 +72,10 @@ DType = Any\n # Tuples of terms and their coefficients, sorted with the largest term first.\n SortedTerms = Sequence[tuple[\"_DimMon\", int]]\n \n+# Normalization rules represent the explicit constraint `t*tk == e` as\n+# a mapping of `t` to `(e, tk)`.\n+NormalizationRules = dict[\"_DimMon\", tuple[\"_DimExpr\", int]]\n+\n class InconclusiveDimensionOperation(core.InconclusiveDimensionOperation):\n   \"\"\"Raised when we cannot conclusively compute with symbolic dimensions.\"\"\"\n \n"
                },
                {
                    "old_start": 99,
                    "old_length": 6,
                    "new_start": 102,
                    "new_length": 21,
                    "hunk": "@@ -99,6 +102,21 @@ class _ShapePolyThreadLocalState(threading.local):\n \n thread_local_state = _ShapePolyThreadLocalState()\n \n+\n+class Comparator(Enum):\n+  EQ = 1\n+  GEQ = 2\n+\n+@dataclasses.dataclass(frozen=True)\n+class _SymbolicConstraint:\n+  cmp: Comparator\n+  debug_str: str  # The form in which the user expressed it, for error messages\n+  diff: _DimExpr  # For GEQ: diff >= 0, and for EQ: diff == 0\n+\n+  def __repr__(self):\n+    return f\"Constraint({self.debug_str}: {self.diff})\"\n+\n+\n class _DimAtom:\n   \"\"\"Represents an atom in a symbolic dimension expression.\n \n"
                },
                {
                    "old_start": 268,
                    "old_length": 6,
                    "new_start": 287,
                    "new_length": 8,
                    "hunk": "@@ -268,6 +287,8 @@ class _DimMon(dict):\n     return \"*\".join(f\"{key}^{exponent}\" if exponent != 1 else str(key)\n                     for key, exponent in sorted(self.items()))\n \n+  __repr__ = __str__\n+\n   @classmethod\n   def from_var(cls, v: str) -> _DimMon:\n     return _DimMon({_DimAtom.from_var(v): 1})\n"
                },
                {
                    "old_start": 412,
                    "old_length": 7,
                    "new_start": 433,
                    "new_length": 7,
                    "hunk": "@@ -412,7 +433,7 @@ class _DimExpr:\n \n   @classmethod\n   def from_monomial(cls, mon: _DimMon, exp: int, scope: SymbolicScope):\n-    return _DimExpr.normalize(((mon, exp),), scope)\n+    return _DimExpr.normalize_coeffs({mon: exp}, scope)\n \n   @classmethod\n   def from_constant(cls, c: int, scope: SymbolicScope):\n"
                },
                {
                    "old_start": 465,
                    "old_length": 19,
                    "new_start": 486,
                    "new_length": 44,
                    "hunk": "@@ -465,19 +486,44 @@ class _DimExpr:\n     return (n1, n2, mon)\n \n   @classmethod\n-  def normalize(cls, coeffs: SortedTerms,\n-                scope: SymbolicScope) -> DimSize:\n-    \"\"\"The main constructor for _DimExpr.\n+  def add_coeff(cls, coeffs: dict[_DimMon, int], t: _DimMon, coeff: int):\n+    \"\"\"coeffs[t] += coeff, with squashing 0 coefficients.\"\"\"\n+    if coeff == 0: return\n+    n_coeff = coeffs.get(t, 0) + coeff\n+    if n_coeff == 0:\n+      del coeffs[t]\n+    else:\n+      coeffs[t] = n_coeff\n+\n+  @classmethod\n+  def normalize_coeffs(cls,\n+                       coeffs: dict[_DimMon, int],\n+                       scope: SymbolicScope) -> DimSize:\n+    \"\"\"Constructs a _DimExpr in normal form.\n \n     Ensures that the symbolic dimension is normalized, e.g., does not\n-    have terms with coefficient 0, and it is represented as a Python int\n-    if it is known to be a constant.\n+    have terms with coefficient 0, it reflects all the scope\n+    normalization_rules, and it is represented as a Python integer if it is\n+    known to be a constant.\n+\n+    Does not attempt to normalize the keys (terms) inside `coeffs`.\n     \"\"\"\n-    non_zero_coeffs = tuple(c for c in coeffs if c[1] != 0)\n-    if not non_zero_coeffs: return 0\n-    if non_zero_coeffs[0][0].degree == 0:\n-      return int(non_zero_coeffs[0][1])\n-    return _DimExpr(non_zero_coeffs, scope)\n+    new_coeffs = coeffs.copy()\n+    for t, t_k in coeffs.items():\n+      if t_k == 0:\n+        del new_coeffs[t]\n+        continue\n+      after, t_k_after = scope._normalization_rules.get(t, (None, 0))\n+      if after is not None and t_k % t_k_after == 0:\n+        # We have t*t_k_after -> after.\n+        # We subtract `t*t_k` and add `c * (- (t_k // t_k_after))`.\n+        _DimExpr.add_coeff(new_coeffs, t, - t_k)\n+        for t2, tc2 in after._monomials_sorted:\n+          _DimExpr.add_coeff(new_coeffs, t2, tc2 * (t_k // t_k_after))\n+    new_terms = _DimExpr._coeff_dict_to_terms(new_coeffs)\n+    if not new_terms: return 0\n+    if new_terms[0][0].degree == 0: return new_terms[0][1]\n+    return _DimExpr(new_terms, scope)\n \n   def to_monomial(self) -> _DimMon | None:\n     \"\"\"Extract the single monomial from a symbolic expression.\n"
                },
                {
                    "old_start": 611,
                    "old_length": 8,
                    "new_start": 657,
                    "new_length": 8,
                    "hunk": "@@ -611,8 +657,8 @@ class _DimExpr:\n     other = _ensure_poly(other, \"add\", self.scope)\n     coeffs = dict(self._monomials_sorted)\n     for mon, coeff in other.monomials():\n-      coeffs[mon] = coeff + coeffs.get(mon, 0)\n-    return _DimExpr.normalize(_DimExpr._coeff_dict_to_terms(coeffs), self.scope)\n+      _DimExpr.add_coeff(coeffs, mon, coeff)\n+    return _DimExpr.normalize_coeffs(coeffs, self.scope)\n \n   def __radd__(self, other):\n     if isinstance(other, core.Tracer) or not _convertible_to_poly(other):\n"
                },
                {
                    "old_start": 645,
                    "old_length": 8,
                    "new_start": 691,
                    "new_length": 8,
                    "hunk": "@@ -645,8 +691,8 @@ class _DimExpr:\n     for mon1, coeff1 in self.monomials():\n       for mon2, coeff2 in other.monomials():\n         mon = mon1.mul(mon2)\n-        coeffs[mon] = coeff1 * coeff2 + coeffs.get(mon, 0)\n-    return _DimExpr.normalize(_DimExpr._coeff_dict_to_terms(coeffs), self.scope)\n+        _DimExpr.add_coeff(coeffs, mon, coeff1 * coeff2)\n+    return _DimExpr.normalize_coeffs(coeffs, self.scope)\n \n   def __rmul__(self, other):\n     if isinstance(other, core.Tracer) or not _convertible_to_poly(other):\n"
                },
                {
                    "old_start": 805,
                    "old_length": 25,
                    "new_start": 851,
                    "new_length": 25,
                    "hunk": "@@ -805,25 +851,25 @@ class _DimExpr:\n     return functools.reduce(_evaluate_add, terms) if len(terms) > 1 else terms[0]\n \n   def max(self, other: DimSize) -> DimSize:\n-    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_leq0)\n+    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_or_leq0)\n     if 0 <= lb: return self\n     if ub <= 0: return other\n     return _DimExpr.from_operation(_DimAtom.MAX, self, other, scope=self.scope)\n \n   def rmax(self, other: DimSize) -> DimSize:\n-    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_leq0)\n+    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_or_leq0)\n     if 0 <= lb: return self\n     if ub <= 0: return other\n     return _DimExpr.from_operation(_DimAtom.MAX, other, self, scope=self.scope)\n \n   def min(self, other: DimSize) -> DimSize:\n-    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_leq0)\n+    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_or_leq0)\n     if 0 <= lb: return other\n     if ub <= 0: return self\n     return _DimExpr.from_operation(_DimAtom.MIN, self, other, scope=self.scope)\n \n   def rmin(self, other: DimSize) -> DimSize:\n-    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_leq0)\n+    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_or_leq0)\n     if 0 <= lb: return other\n     if ub <= 0: return self\n     return _DimExpr.from_operation(_DimAtom.MIN, other, self, scope=self.scope)\n"
                },
                {
                    "old_start": 854,
                    "old_length": 7,
                    "new_start": 900,
                    "new_length": 7,
                    "hunk": "@@ -854,7 +900,7 @@ def cmp_sequence(s1, s2, elem_cmp) -> int:\n   if len(s1) < l2: return -1\n   return 0\n \n-def _stop_early_for_geq0_leq0(lb, ub):\n+def _stop_early_for_geq0_or_leq0(lb, ub):\n   return 0 <= lb or ub <= 0\n \n class SymbolicScope:\n"
                },
                {
                    "old_start": 873,
                    "old_length": 59,
                    "new_start": 919,
                    "new_length": 88,
                    "hunk": "@@ -873,59 +919,88 @@ class SymbolicScope:\n     constraints_str: A sequence of constraints on symbolic dimension expressions,\n       of the form `e1 >= e2` or `e1 <= e2`.\n   \"\"\"\n+\n   def __init__(self,\n                constraints_str: Sequence[str] = ()):\n-    self._location_frame = source_info_util.user_frame(source_info_util.current())\n-    self._explicit_constraints: list[tuple[_DimExpr, str]] = []\n+    if isinstance(constraints_str, str):\n+      raise ValueError(\n+          \"The symbolic constraints should be a sequence of strings. \"\n+          f\"Got {repr(constraints_str)}\")\n \n-    constraints = self._parse_constraints(constraints_str)\n-    for c, c_str in zip(constraints, constraints_str):\n-      if (const := _DimExpr.to_constant(c)) is not None:\n-        if const < 0:\n-          raise ValueError(f\"Unsatisfiable explicit constraint: {c_str}\")\n-        continue\n-      self._explicit_constraints.append((c, c_str))\n+    self._location_frame = source_info_util.user_frame(source_info_util.current())\n+    # Keep the explicit constraints in the order in which they were added\n+    self._explicit_constraints: list[_SymbolicConstraint] = []\n \n     # We cache the _DimExpr.bounds calls. The result depends only on the\n     # explicit and implicit constraints, so it is safe to keep it in the\n-    # scope.\n+    # scope. Set the cache before we parse constraints.\n     self._bounds_cache: dict[tuple[_DimExpr,\n                                    Callable[[float, float], bool] | None],\n                              tuple[float, float]] = {}\n+    # We turn the equality constraints into normalization rules.\n+    # For an explicit constraint `t*tk == e`, we keep\n+    # `_normalization_rules[t] = (e, tk)`.\n+    # During building of expressions, if we encounter the term\n+    # `t*tk1` and `tk1 % tk == 0`, we replace it with `e*(tk1 // tk)`.\n+    self._normalization_rules: NormalizationRules = {}\n+\n+    for c_str in constraints_str:\n+      self._parse_and_process_explicit_constraint(c_str)\n+      self._bounds_cache.clear()\n \n   def __str__(self) -> str:\n     extras = []\n     if self._explicit_constraints:\n       extras.append(\" with constraints:\")\n-      for _, c_str in self._explicit_constraints:\n-        extras.append(f\"  {c_str}\")\n+      for constr in self._explicit_constraints:\n+        extras.append(f\"  {constr.debug_str}\")\n     loc = source_info_util._summarize_frame(self._location_frame) if self._location_frame else \"unknown\"\n     return (\n         f\"{id(self)} created at {loc}\" +\n         \"\\n\".join(extras))\n   __repr__ = __str__\n \n-  def _parse_constraints(self, constraints_str: Sequence[str]) -> Sequence[_DimExpr]:\n-    # Parse some contraints into the current scope.\n-    def parse_one(cs: str) -> _DimExpr:\n-      if not isinstance(cs, str):\n-        raise ValueError(\n-            f\"symbolic_scope must be invoked with a string: got {repr(cs)}\")\n-      eq_pos = cs.find(\"=\")\n-      if eq_pos <= 0 or cs[eq_pos - 1] not in [\">\", \"<\"]:\n-        raise ValueError(\"Constraint parsing error: must contain one of '>=' or '<='\")\n-      e1_str = cs[:eq_pos - 1]\n-      e1, = _Parser(e1_str, None, repr(e1_str), self).parse()\n-      e2_str = cs[eq_pos + 1:]\n-      e2, = _Parser(e2_str, None, repr(e2_str), self).parse()\n-      diff = e1 - e2 if cs[eq_pos - 1] == \">\" else e2 - e1\n-      return _ensure_poly(diff, \"symbolic_scope\", self)\n-\n-    if isinstance(constraints_str, str):\n+  def _parse_and_process_explicit_constraint(self, c_str: str):\n+    if not isinstance(c_str, str):\n       raise ValueError(\n-          \"The symbolic constraints should be a sequence of strings. \"\n-          f\"Got {repr(constraints_str)}\")\n-    return tuple(parse_one(cs) for cs in constraints_str)\n+          f\"SymbolicScope constraint must be a string: got {repr(c_str)}\")\n+    cmp_pos, cmp, is_geq = c_str.find(\"==\"), Comparator.EQ, True\n+    if cmp_pos < 0:\n+      cmp_pos, cmp, is_geq = c_str.find(\">=\"), Comparator.GEQ, True\n+      if cmp_pos < 0:\n+        cmp_pos, cmp, is_geq = c_str.find(\"<=\"), Comparator.GEQ, False\n+      if cmp_pos < 0:\n+        raise ValueError(\"Constraint parsing error: must contain one of '==' or '>=' or '<='\")\n+    e1_str = c_str[:cmp_pos]\n+    e1, = _Parser(e1_str, None, repr(e1_str), self).parse()\n+    e2_str = c_str[cmp_pos + 2:]\n+    e2, = _Parser(e2_str, None, repr(e2_str), self).parse()\n+    if cmp == Comparator.GEQ and not is_geq:\n+      e1, e2 = e2, e1\n+\n+    diff = e1 - e2\n+    if (diff_const := _DimExpr.to_constant(diff)) is not None:\n+      if ((cmp == Comparator.EQ and diff_const != 0) or\n+          (cmp == Comparator.GEQ and diff_const < 0)):\n+        raise ValueError(f\"Unsatisfiable explicit constraint: {c_str}\")\n+      return\n+\n+    constr = _SymbolicConstraint(debug_str=c_str, cmp=cmp, diff=diff)  # type: ignore[arg-type]\n+    self._explicit_constraints.append(constr)\n+    if cmp == Comparator.EQ:\n+      if not isinstance(e1, _DimExpr):\n+        raise ValueError(\"Invalid equality constraint: {e1} == {e2}. \"\n+                         \"The left-hand-side must be of the form `term * coefficient`.\")\n+      (before, before_k), *rest = e1._monomials_sorted\n+      if rest:\n+        raise ValueError(\"Invalid equality constraint: {e1} == {e2}. \"\n+                         \"The left-hand-side must be of the form `term * coefficient`.\")\n+\n+      after = _ensure_poly(e2, \"parse_constraint\", e1.scope)\n+      if before in self._normalization_rules:\n+        raise NotImplementedError(\n+            f\"Found multiple equality constraints with the same left-hand-side: {before}\")\n+      self._normalization_rules[before] = (after, before_k)\n \n   def _check_same_scope(self, other: _DimExpr,\n                         when: str = \"\",\n"
                },
                {
                    "old_start": 1474,
                    "old_length": 7,
                    "new_start": 1549,
                    "new_length": 7,
                    "hunk": "@@ -1474,7 +1549,7 @@ class _Parser:\n         power, tok = self.integer(tok)\n         a = a ** power\n \n-      acc = acc * a if acc is not None else a  # type:ignore [operator]\n+      acc = acc * a if acc is not None else a  # type: ignore[operator]\n       if tok.exact_type in self.FOLLOW_MON:\n         return acc, tok\n       tok = self.consume_token(tok, tokenize.STAR)\n"
                },
                {
                    "old_start": 1510,
                    "old_length": 6,
                    "new_start": 1585,
                    "new_length": 10,
                    "hunk": "@@ -1510,6 +1585,10 @@ class _Parser:\n     tok = self.consume_token(tok, tokenize.COMMA)\n     e2, tok = self.expr(tok)\n     tok = self.consume_token(tok, tokenize.RPAR)\n+    if op == _DimAtom.MAX:\n+      return core.max_dim(e1, e2), tok\n+    if op == _DimAtom.MIN:\n+      return core.min_dim(e1, e2), tok\n     return _DimExpr.from_operation(op, e1, e2,\n                                    scope=self.scope), tok  # type: ignore\n \n"
                },
                {
                    "old_start": 1586,
                    "old_length": 9,
                    "new_start": 1665,
                    "new_length": 6,
                    "hunk": "@@ -1586,9 +1665,6 @@ class CachingShapeEvaluator:\n \n @dataclasses.dataclass(frozen=True)\n class ShapeConstraint:\n-  class Comparator(Enum):\n-    EQ = 1\n-    GEQ = 2\n \n   comp: Comparator\n   left: DimSize\n"
                },
                {
                    "old_start": 1601,
                    "old_length": 9,
                    "new_start": 1677,
                    "new_length": 9,
                    "hunk": "@@ -1601,9 +1677,9 @@ class ShapeConstraint:\n     \"\"\"Evaluates a constraint statically.\"\"\"\n     left, right = eval.evaluate(self.left), eval.evaluate(self.right)\n     try:\n-      if self.comp == ShapeConstraint.Comparator.EQ:\n+      if self.comp == Comparator.EQ:\n         ok = (left == right)\n-      elif self.comp == ShapeConstraint.Comparator.GEQ:\n+      elif self.comp == Comparator.GEQ:\n         ok = (left >= right)\n       else:\n         assert False  # We are in a context where we know we can evaluate\n"
                },
                {
                    "old_start": 1625,
                    "old_length": 24,
                    "new_start": 1701,
                    "new_length": 24,
                    "hunk": "@@ -1625,24 +1701,24 @@ class ShapeConstraint:\n     # Try to evaluate the constraint statically.\n     if core.is_constant_shape((left, right)):\n       left_int, right_int = op.index(left), op.index(right)\n-      if self.comp == ShapeConstraint.Comparator.EQ:\n+      if self.comp == Comparator.EQ:\n         if not (left_int == right_int):\n           raise self.make_error(eval)\n-      elif self.comp == ShapeConstraint.Comparator.GEQ:\n+      elif self.comp == Comparator.GEQ:\n         if not (left_int >= right_int):\n           raise self.make_error(eval)\n       else: assert False\n       return None\n \n-    if self.comp == ShapeConstraint.Comparator.EQ:\n+    if self.comp == Comparator.EQ:\n       is_ok = lax.eq(left, right)\n-    elif self.comp == ShapeConstraint.Comparator.GEQ:\n+    elif self.comp == Comparator.GEQ:\n       is_ok = lax.ge(left, right)\n     else: assert False\n     return is_ok\n \n   def __str__(self):\n-    return (f\"{self.left} {'==' if self.comp == ShapeConstraint.Comparator.EQ else '>='} {self.right}\"\n+    return (f\"{self.left} {'==' if self.comp == Comparator.EQ else '>='} {self.right}\"\n             f\" ({self.error_message_pieces})\")\n   __repr__ = __str__\n \n"
                },
                {
                    "old_start": 1687,
                    "old_length": 7,
                    "new_start": 1763,
                    "new_length": 7,
                    "hunk": "@@ -1687,7 +1763,7 @@ class ShapeConstraints:\n     self.constraints: list[ShapeConstraint] = []\n \n   def add_constraint(self,\n-                     comp: ShapeConstraint.Comparator,\n+                     comp: Comparator,\n                      left: DimSize, right: DimSize,\n                      error_message_pieces: Sequence[str | DimSize]):\n     c = ShapeConstraint(comp, left, right, error_message_pieces)\n"
                },
                {
                    "old_start": 1910,
                    "old_length": 7,
                    "new_start": 1986,
                    "new_length": 7,
                    "hunk": "@@ -1910,7 +1986,7 @@ def _solve_dim_equations(\n       else:\n         var_value, var_remainder = divmod(dim_value, core.dim_constant(factor_var))  # type: ignore\n         shape_constraints.add_constraint(\n-            ShapeConstraint.Comparator.EQ, var_remainder, 0,\n+            Comparator.EQ, var_remainder, 0,\n             error_message_pieces=([\n                 \"Input shapes do not match the polymorphic shapes specification. \"\n                 \"Division had remainder \", var_remainder,\n"
                },
                {
                    "old_start": 1929,
                    "old_length": 7,
                    "new_start": 2005,
                    "new_length": 7,
                    "hunk": "@@ -1929,7 +2005,7 @@ def _solve_dim_equations(\n         \"), \"])\n \n       shape_constraints.add_constraint(\n-          ShapeConstraint.Comparator.GEQ, var_value, 1,\n+          Comparator.GEQ, var_value, 1,\n           error_message_pieces=[\n                 \"Input shapes do not match the polymorphic shapes specification. \"\n                 f\"Expected value >= 1 for dimension variable '{var}'.\" +\n"
                },
                {
                    "old_start": 1941,
                    "old_length": 7,
                    "new_start": 2017,
                    "new_length": 7,
                    "hunk": "@@ -1941,7 +2017,7 @@ def _solve_dim_equations(\n     else:\n       # All variables are resolved for this equation, we emit an assertion\n       shape_constraints.add_constraint(\n-          ShapeConstraint.Comparator.EQ,\n+          Comparator.EQ,\n           _DimExpr.from_var(eqn.dim_name, eqn.aval_dim_expr.scope),\n           eqn.aval_dim_expr.evaluate(shape_env),\n           error_message_pieces=([\n"
                },
                {
                    "old_start": 1958,
                    "old_length": 14,
                    "new_start": 2034,
                    "new_length": 15,
                    "hunk": "@@ -1958,14 +2034,15 @@ def _solve_dim_equations(\n   def add_explicit_symbolic_constraints(shape_env: DimVarEnv):\n     if not shape_env: return\n     assert scope is not None\n-    for c, c_str in scope._explicit_constraints:\n-      c_value = c.evaluate(shape_env)\n+    for constr in scope._explicit_constraints:\n+      c_value = constr.diff.evaluate(shape_env)\n       shape_constraints.add_constraint(\n-          ShapeConstraint.Comparator.GEQ, c_value, 0,\n+          constr.cmp, c_value, 0,\n           error_message_pieces=[\n-                f\"Input shapes do not match the symbolic shape constraint {c_str}. \"\n-                f\"Expected '{c}' to be greater or equal to 0, but found \",\n-                c_value,\n+                f\"Input shapes do not match the symbolic shape constraint {constr.debug_str}. \"\n+                f\"Expected '{constr.diff}' to be \"\n+                f\"{'greater or equal' if constr.cmp == Comparator.GEQ else 'equal'} to 0, \"\n+                \"but found \", c_value,\n                 \". \" + poly_specs_err_msg\n               ] + solution_error_message_pieces + [\n               solution_err_msg_trailer_errors])\n"
                }
            ],
            "whole_deleted": "-import math\n-    return _DimExpr.normalize(((mon, exp),), scope)\n-  def normalize(cls, coeffs: SortedTerms,\n-                scope: SymbolicScope) -> DimSize:\n-    \"\"\"The main constructor for _DimExpr.\n-    have terms with coefficient 0, and it is represented as a Python int\n-    if it is known to be a constant.\n-    non_zero_coeffs = tuple(c for c in coeffs if c[1] != 0)\n-    if not non_zero_coeffs: return 0\n-    if non_zero_coeffs[0][0].degree == 0:\n-      return int(non_zero_coeffs[0][1])\n-    return _DimExpr(non_zero_coeffs, scope)\n-      coeffs[mon] = coeff + coeffs.get(mon, 0)\n-    return _DimExpr.normalize(_DimExpr._coeff_dict_to_terms(coeffs), self.scope)\n-        coeffs[mon] = coeff1 * coeff2 + coeffs.get(mon, 0)\n-    return _DimExpr.normalize(_DimExpr._coeff_dict_to_terms(coeffs), self.scope)\n-    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_leq0)\n-    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_leq0)\n-    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_leq0)\n-    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_leq0)\n-def _stop_early_for_geq0_leq0(lb, ub):\n-    self._location_frame = source_info_util.user_frame(source_info_util.current())\n-    self._explicit_constraints: list[tuple[_DimExpr, str]] = []\n-    constraints = self._parse_constraints(constraints_str)\n-    for c, c_str in zip(constraints, constraints_str):\n-      if (const := _DimExpr.to_constant(c)) is not None:\n-        if const < 0:\n-          raise ValueError(f\"Unsatisfiable explicit constraint: {c_str}\")\n-        continue\n-      self._explicit_constraints.append((c, c_str))\n-    # scope.\n-      for _, c_str in self._explicit_constraints:\n-        extras.append(f\"  {c_str}\")\n-  def _parse_constraints(self, constraints_str: Sequence[str]) -> Sequence[_DimExpr]:\n-    # Parse some contraints into the current scope.\n-    def parse_one(cs: str) -> _DimExpr:\n-      if not isinstance(cs, str):\n-        raise ValueError(\n-            f\"symbolic_scope must be invoked with a string: got {repr(cs)}\")\n-      eq_pos = cs.find(\"=\")\n-      if eq_pos <= 0 or cs[eq_pos - 1] not in [\">\", \"<\"]:\n-        raise ValueError(\"Constraint parsing error: must contain one of '>=' or '<='\")\n-      e1_str = cs[:eq_pos - 1]\n-      e1, = _Parser(e1_str, None, repr(e1_str), self).parse()\n-      e2_str = cs[eq_pos + 1:]\n-      e2, = _Parser(e2_str, None, repr(e2_str), self).parse()\n-      diff = e1 - e2 if cs[eq_pos - 1] == \">\" else e2 - e1\n-      return _ensure_poly(diff, \"symbolic_scope\", self)\n-\n-    if isinstance(constraints_str, str):\n-          \"The symbolic constraints should be a sequence of strings. \"\n-          f\"Got {repr(constraints_str)}\")\n-    return tuple(parse_one(cs) for cs in constraints_str)\n-      acc = acc * a if acc is not None else a  # type:ignore [operator]\n-  class Comparator(Enum):\n-    EQ = 1\n-    GEQ = 2\n-      if self.comp == ShapeConstraint.Comparator.EQ:\n-      elif self.comp == ShapeConstraint.Comparator.GEQ:\n-      if self.comp == ShapeConstraint.Comparator.EQ:\n-      elif self.comp == ShapeConstraint.Comparator.GEQ:\n-    if self.comp == ShapeConstraint.Comparator.EQ:\n-    elif self.comp == ShapeConstraint.Comparator.GEQ:\n-    return (f\"{self.left} {'==' if self.comp == ShapeConstraint.Comparator.EQ else '>='} {self.right}\"\n-                     comp: ShapeConstraint.Comparator,\n-            ShapeConstraint.Comparator.EQ, var_remainder, 0,\n-          ShapeConstraint.Comparator.GEQ, var_value, 1,\n-          ShapeConstraint.Comparator.EQ,\n-    for c, c_str in scope._explicit_constraints:\n-      c_value = c.evaluate(shape_env)\n-          ShapeConstraint.Comparator.GEQ, c_value, 0,\n-                f\"Input shapes do not match the symbolic shape constraint {c_str}. \"\n-                f\"Expected '{c}' to be greater or equal to 0, but found \",\n-                c_value,\n",
            "whole_added": "+# Normalization rules represent the explicit constraint `t*tk == e` as\n+# a mapping of `t` to `(e, tk)`.\n+NormalizationRules = dict[\"_DimMon\", tuple[\"_DimExpr\", int]]\n+\n+\n+class Comparator(Enum):\n+  EQ = 1\n+  GEQ = 2\n+\n+@dataclasses.dataclass(frozen=True)\n+class _SymbolicConstraint:\n+  cmp: Comparator\n+  debug_str: str  # The form in which the user expressed it, for error messages\n+  diff: _DimExpr  # For GEQ: diff >= 0, and for EQ: diff == 0\n+\n+  def __repr__(self):\n+    return f\"Constraint({self.debug_str}: {self.diff})\"\n+\n+\n+  __repr__ = __str__\n+\n+    return _DimExpr.normalize_coeffs({mon: exp}, scope)\n+  def add_coeff(cls, coeffs: dict[_DimMon, int], t: _DimMon, coeff: int):\n+    \"\"\"coeffs[t] += coeff, with squashing 0 coefficients.\"\"\"\n+    if coeff == 0: return\n+    n_coeff = coeffs.get(t, 0) + coeff\n+    if n_coeff == 0:\n+      del coeffs[t]\n+    else:\n+      coeffs[t] = n_coeff\n+\n+  @classmethod\n+  def normalize_coeffs(cls,\n+                       coeffs: dict[_DimMon, int],\n+                       scope: SymbolicScope) -> DimSize:\n+    \"\"\"Constructs a _DimExpr in normal form.\n+    have terms with coefficient 0, it reflects all the scope\n+    normalization_rules, and it is represented as a Python integer if it is\n+    known to be a constant.\n+\n+    Does not attempt to normalize the keys (terms) inside `coeffs`.\n+    new_coeffs = coeffs.copy()\n+    for t, t_k in coeffs.items():\n+      if t_k == 0:\n+        del new_coeffs[t]\n+        continue\n+      after, t_k_after = scope._normalization_rules.get(t, (None, 0))\n+      if after is not None and t_k % t_k_after == 0:\n+        # We have t*t_k_after -> after.\n+        # We subtract `t*t_k` and add `c * (- (t_k // t_k_after))`.\n+        _DimExpr.add_coeff(new_coeffs, t, - t_k)\n+        for t2, tc2 in after._monomials_sorted:\n+          _DimExpr.add_coeff(new_coeffs, t2, tc2 * (t_k // t_k_after))\n+    new_terms = _DimExpr._coeff_dict_to_terms(new_coeffs)\n+    if not new_terms: return 0\n+    if new_terms[0][0].degree == 0: return new_terms[0][1]\n+    return _DimExpr(new_terms, scope)\n+      _DimExpr.add_coeff(coeffs, mon, coeff)\n+    return _DimExpr.normalize_coeffs(coeffs, self.scope)\n+        _DimExpr.add_coeff(coeffs, mon, coeff1 * coeff2)\n+    return _DimExpr.normalize_coeffs(coeffs, self.scope)\n+    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_or_leq0)\n+    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_or_leq0)\n+    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_or_leq0)\n+    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_or_leq0)\n+def _stop_early_for_geq0_or_leq0(lb, ub):\n+\n+    if isinstance(constraints_str, str):\n+      raise ValueError(\n+          \"The symbolic constraints should be a sequence of strings. \"\n+          f\"Got {repr(constraints_str)}\")\n+    self._location_frame = source_info_util.user_frame(source_info_util.current())\n+    # Keep the explicit constraints in the order in which they were added\n+    self._explicit_constraints: list[_SymbolicConstraint] = []\n+    # scope. Set the cache before we parse constraints.\n+    # We turn the equality constraints into normalization rules.\n+    # For an explicit constraint `t*tk == e`, we keep\n+    # `_normalization_rules[t] = (e, tk)`.\n+    # During building of expressions, if we encounter the term\n+    # `t*tk1` and `tk1 % tk == 0`, we replace it with `e*(tk1 // tk)`.\n+    self._normalization_rules: NormalizationRules = {}\n+\n+    for c_str in constraints_str:\n+      self._parse_and_process_explicit_constraint(c_str)\n+      self._bounds_cache.clear()\n+      for constr in self._explicit_constraints:\n+        extras.append(f\"  {constr.debug_str}\")\n+  def _parse_and_process_explicit_constraint(self, c_str: str):\n+    if not isinstance(c_str, str):\n+          f\"SymbolicScope constraint must be a string: got {repr(c_str)}\")\n+    cmp_pos, cmp, is_geq = c_str.find(\"==\"), Comparator.EQ, True\n+    if cmp_pos < 0:\n+      cmp_pos, cmp, is_geq = c_str.find(\">=\"), Comparator.GEQ, True\n+      if cmp_pos < 0:\n+        cmp_pos, cmp, is_geq = c_str.find(\"<=\"), Comparator.GEQ, False\n+      if cmp_pos < 0:\n+        raise ValueError(\"Constraint parsing error: must contain one of '==' or '>=' or '<='\")\n+    e1_str = c_str[:cmp_pos]\n+    e1, = _Parser(e1_str, None, repr(e1_str), self).parse()\n+    e2_str = c_str[cmp_pos + 2:]\n+    e2, = _Parser(e2_str, None, repr(e2_str), self).parse()\n+    if cmp == Comparator.GEQ and not is_geq:\n+      e1, e2 = e2, e1\n+\n+    diff = e1 - e2\n+    if (diff_const := _DimExpr.to_constant(diff)) is not None:\n+      if ((cmp == Comparator.EQ and diff_const != 0) or\n+          (cmp == Comparator.GEQ and diff_const < 0)):\n+        raise ValueError(f\"Unsatisfiable explicit constraint: {c_str}\")\n+      return\n+\n+    constr = _SymbolicConstraint(debug_str=c_str, cmp=cmp, diff=diff)  # type: ignore[arg-type]\n+    self._explicit_constraints.append(constr)\n+    if cmp == Comparator.EQ:\n+      if not isinstance(e1, _DimExpr):\n+        raise ValueError(\"Invalid equality constraint: {e1} == {e2}. \"\n+                         \"The left-hand-side must be of the form `term * coefficient`.\")\n+      (before, before_k), *rest = e1._monomials_sorted\n+      if rest:\n+        raise ValueError(\"Invalid equality constraint: {e1} == {e2}. \"\n+                         \"The left-hand-side must be of the form `term * coefficient`.\")\n+\n+      after = _ensure_poly(e2, \"parse_constraint\", e1.scope)\n+      if before in self._normalization_rules:\n+        raise NotImplementedError(\n+            f\"Found multiple equality constraints with the same left-hand-side: {before}\")\n+      self._normalization_rules[before] = (after, before_k)\n+      acc = acc * a if acc is not None else a  # type: ignore[operator]\n+    if op == _DimAtom.MAX:\n+      return core.max_dim(e1, e2), tok\n+    if op == _DimAtom.MIN:\n+      return core.min_dim(e1, e2), tok\n+      if self.comp == Comparator.EQ:\n+      elif self.comp == Comparator.GEQ:\n+      if self.comp == Comparator.EQ:\n+      elif self.comp == Comparator.GEQ:\n+    if self.comp == Comparator.EQ:\n+    elif self.comp == Comparator.GEQ:\n+    return (f\"{self.left} {'==' if self.comp == Comparator.EQ else '>='} {self.right}\"\n+                     comp: Comparator,\n+            Comparator.EQ, var_remainder, 0,\n+          Comparator.GEQ, var_value, 1,\n+          Comparator.EQ,\n+    for constr in scope._explicit_constraints:\n+      c_value = constr.diff.evaluate(shape_env)\n+          constr.cmp, c_value, 0,\n+                f\"Input shapes do not match the symbolic shape constraint {constr.debug_str}. \"\n+                f\"Expected '{constr.diff}' to be \"\n+                f\"{'greater or equal' if constr.cmp == Comparator.GEQ else 'equal'} to 0, \"\n+                \"but found \", c_value,\n",
            "whole_hunk": "@@ -40,7 +40,6 @@ from enum import Enum\n import functools\n import itertools\n import io\n-import math\n import operator as op\n import threading\n import tokenize\n@@ -73,6 +72,10 @@ DType = Any\n # Tuples of terms and their coefficients, sorted with the largest term first.\n SortedTerms = Sequence[tuple[\"_DimMon\", int]]\n \n+# Normalization rules represent the explicit constraint `t*tk == e` as\n+# a mapping of `t` to `(e, tk)`.\n+NormalizationRules = dict[\"_DimMon\", tuple[\"_DimExpr\", int]]\n+\n class InconclusiveDimensionOperation(core.InconclusiveDimensionOperation):\n   \"\"\"Raised when we cannot conclusively compute with symbolic dimensions.\"\"\"\n \n@@ -99,6 +102,21 @@ class _ShapePolyThreadLocalState(threading.local):\n \n thread_local_state = _ShapePolyThreadLocalState()\n \n+\n+class Comparator(Enum):\n+  EQ = 1\n+  GEQ = 2\n+\n+@dataclasses.dataclass(frozen=True)\n+class _SymbolicConstraint:\n+  cmp: Comparator\n+  debug_str: str  # The form in which the user expressed it, for error messages\n+  diff: _DimExpr  # For GEQ: diff >= 0, and for EQ: diff == 0\n+\n+  def __repr__(self):\n+    return f\"Constraint({self.debug_str}: {self.diff})\"\n+\n+\n class _DimAtom:\n   \"\"\"Represents an atom in a symbolic dimension expression.\n \n@@ -268,6 +287,8 @@ class _DimMon(dict):\n     return \"*\".join(f\"{key}^{exponent}\" if exponent != 1 else str(key)\n                     for key, exponent in sorted(self.items()))\n \n+  __repr__ = __str__\n+\n   @classmethod\n   def from_var(cls, v: str) -> _DimMon:\n     return _DimMon({_DimAtom.from_var(v): 1})\n@@ -412,7 +433,7 @@ class _DimExpr:\n \n   @classmethod\n   def from_monomial(cls, mon: _DimMon, exp: int, scope: SymbolicScope):\n-    return _DimExpr.normalize(((mon, exp),), scope)\n+    return _DimExpr.normalize_coeffs({mon: exp}, scope)\n \n   @classmethod\n   def from_constant(cls, c: int, scope: SymbolicScope):\n@@ -465,19 +486,44 @@ class _DimExpr:\n     return (n1, n2, mon)\n \n   @classmethod\n-  def normalize(cls, coeffs: SortedTerms,\n-                scope: SymbolicScope) -> DimSize:\n-    \"\"\"The main constructor for _DimExpr.\n+  def add_coeff(cls, coeffs: dict[_DimMon, int], t: _DimMon, coeff: int):\n+    \"\"\"coeffs[t] += coeff, with squashing 0 coefficients.\"\"\"\n+    if coeff == 0: return\n+    n_coeff = coeffs.get(t, 0) + coeff\n+    if n_coeff == 0:\n+      del coeffs[t]\n+    else:\n+      coeffs[t] = n_coeff\n+\n+  @classmethod\n+  def normalize_coeffs(cls,\n+                       coeffs: dict[_DimMon, int],\n+                       scope: SymbolicScope) -> DimSize:\n+    \"\"\"Constructs a _DimExpr in normal form.\n \n     Ensures that the symbolic dimension is normalized, e.g., does not\n-    have terms with coefficient 0, and it is represented as a Python int\n-    if it is known to be a constant.\n+    have terms with coefficient 0, it reflects all the scope\n+    normalization_rules, and it is represented as a Python integer if it is\n+    known to be a constant.\n+\n+    Does not attempt to normalize the keys (terms) inside `coeffs`.\n     \"\"\"\n-    non_zero_coeffs = tuple(c for c in coeffs if c[1] != 0)\n-    if not non_zero_coeffs: return 0\n-    if non_zero_coeffs[0][0].degree == 0:\n-      return int(non_zero_coeffs[0][1])\n-    return _DimExpr(non_zero_coeffs, scope)\n+    new_coeffs = coeffs.copy()\n+    for t, t_k in coeffs.items():\n+      if t_k == 0:\n+        del new_coeffs[t]\n+        continue\n+      after, t_k_after = scope._normalization_rules.get(t, (None, 0))\n+      if after is not None and t_k % t_k_after == 0:\n+        # We have t*t_k_after -> after.\n+        # We subtract `t*t_k` and add `c * (- (t_k // t_k_after))`.\n+        _DimExpr.add_coeff(new_coeffs, t, - t_k)\n+        for t2, tc2 in after._monomials_sorted:\n+          _DimExpr.add_coeff(new_coeffs, t2, tc2 * (t_k // t_k_after))\n+    new_terms = _DimExpr._coeff_dict_to_terms(new_coeffs)\n+    if not new_terms: return 0\n+    if new_terms[0][0].degree == 0: return new_terms[0][1]\n+    return _DimExpr(new_terms, scope)\n \n   def to_monomial(self) -> _DimMon | None:\n     \"\"\"Extract the single monomial from a symbolic expression.\n@@ -611,8 +657,8 @@ class _DimExpr:\n     other = _ensure_poly(other, \"add\", self.scope)\n     coeffs = dict(self._monomials_sorted)\n     for mon, coeff in other.monomials():\n-      coeffs[mon] = coeff + coeffs.get(mon, 0)\n-    return _DimExpr.normalize(_DimExpr._coeff_dict_to_terms(coeffs), self.scope)\n+      _DimExpr.add_coeff(coeffs, mon, coeff)\n+    return _DimExpr.normalize_coeffs(coeffs, self.scope)\n \n   def __radd__(self, other):\n     if isinstance(other, core.Tracer) or not _convertible_to_poly(other):\n@@ -645,8 +691,8 @@ class _DimExpr:\n     for mon1, coeff1 in self.monomials():\n       for mon2, coeff2 in other.monomials():\n         mon = mon1.mul(mon2)\n-        coeffs[mon] = coeff1 * coeff2 + coeffs.get(mon, 0)\n-    return _DimExpr.normalize(_DimExpr._coeff_dict_to_terms(coeffs), self.scope)\n+        _DimExpr.add_coeff(coeffs, mon, coeff1 * coeff2)\n+    return _DimExpr.normalize_coeffs(coeffs, self.scope)\n \n   def __rmul__(self, other):\n     if isinstance(other, core.Tracer) or not _convertible_to_poly(other):\n@@ -805,25 +851,25 @@ class _DimExpr:\n     return functools.reduce(_evaluate_add, terms) if len(terms) > 1 else terms[0]\n \n   def max(self, other: DimSize) -> DimSize:\n-    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_leq0)\n+    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_or_leq0)\n     if 0 <= lb: return self\n     if ub <= 0: return other\n     return _DimExpr.from_operation(_DimAtom.MAX, self, other, scope=self.scope)\n \n   def rmax(self, other: DimSize) -> DimSize:\n-    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_leq0)\n+    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_or_leq0)\n     if 0 <= lb: return self\n     if ub <= 0: return other\n     return _DimExpr.from_operation(_DimAtom.MAX, other, self, scope=self.scope)\n \n   def min(self, other: DimSize) -> DimSize:\n-    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_leq0)\n+    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_or_leq0)\n     if 0 <= lb: return other\n     if ub <= 0: return self\n     return _DimExpr.from_operation(_DimAtom.MIN, self, other, scope=self.scope)\n \n   def rmin(self, other: DimSize) -> DimSize:\n-    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_leq0)\n+    lb, ub = _bounds_decision(self - other, _stop_early_for_geq0_or_leq0)\n     if 0 <= lb: return other\n     if ub <= 0: return self\n     return _DimExpr.from_operation(_DimAtom.MIN, other, self, scope=self.scope)\n@@ -854,7 +900,7 @@ def cmp_sequence(s1, s2, elem_cmp) -> int:\n   if len(s1) < l2: return -1\n   return 0\n \n-def _stop_early_for_geq0_leq0(lb, ub):\n+def _stop_early_for_geq0_or_leq0(lb, ub):\n   return 0 <= lb or ub <= 0\n \n class SymbolicScope:\n@@ -873,59 +919,88 @@ class SymbolicScope:\n     constraints_str: A sequence of constraints on symbolic dimension expressions,\n       of the form `e1 >= e2` or `e1 <= e2`.\n   \"\"\"\n+\n   def __init__(self,\n                constraints_str: Sequence[str] = ()):\n-    self._location_frame = source_info_util.user_frame(source_info_util.current())\n-    self._explicit_constraints: list[tuple[_DimExpr, str]] = []\n+    if isinstance(constraints_str, str):\n+      raise ValueError(\n+          \"The symbolic constraints should be a sequence of strings. \"\n+          f\"Got {repr(constraints_str)}\")\n \n-    constraints = self._parse_constraints(constraints_str)\n-    for c, c_str in zip(constraints, constraints_str):\n-      if (const := _DimExpr.to_constant(c)) is not None:\n-        if const < 0:\n-          raise ValueError(f\"Unsatisfiable explicit constraint: {c_str}\")\n-        continue\n-      self._explicit_constraints.append((c, c_str))\n+    self._location_frame = source_info_util.user_frame(source_info_util.current())\n+    # Keep the explicit constraints in the order in which they were added\n+    self._explicit_constraints: list[_SymbolicConstraint] = []\n \n     # We cache the _DimExpr.bounds calls. The result depends only on the\n     # explicit and implicit constraints, so it is safe to keep it in the\n-    # scope.\n+    # scope. Set the cache before we parse constraints.\n     self._bounds_cache: dict[tuple[_DimExpr,\n                                    Callable[[float, float], bool] | None],\n                              tuple[float, float]] = {}\n+    # We turn the equality constraints into normalization rules.\n+    # For an explicit constraint `t*tk == e`, we keep\n+    # `_normalization_rules[t] = (e, tk)`.\n+    # During building of expressions, if we encounter the term\n+    # `t*tk1` and `tk1 % tk == 0`, we replace it with `e*(tk1 // tk)`.\n+    self._normalization_rules: NormalizationRules = {}\n+\n+    for c_str in constraints_str:\n+      self._parse_and_process_explicit_constraint(c_str)\n+      self._bounds_cache.clear()\n \n   def __str__(self) -> str:\n     extras = []\n     if self._explicit_constraints:\n       extras.append(\" with constraints:\")\n-      for _, c_str in self._explicit_constraints:\n-        extras.append(f\"  {c_str}\")\n+      for constr in self._explicit_constraints:\n+        extras.append(f\"  {constr.debug_str}\")\n     loc = source_info_util._summarize_frame(self._location_frame) if self._location_frame else \"unknown\"\n     return (\n         f\"{id(self)} created at {loc}\" +\n         \"\\n\".join(extras))\n   __repr__ = __str__\n \n-  def _parse_constraints(self, constraints_str: Sequence[str]) -> Sequence[_DimExpr]:\n-    # Parse some contraints into the current scope.\n-    def parse_one(cs: str) -> _DimExpr:\n-      if not isinstance(cs, str):\n-        raise ValueError(\n-            f\"symbolic_scope must be invoked with a string: got {repr(cs)}\")\n-      eq_pos = cs.find(\"=\")\n-      if eq_pos <= 0 or cs[eq_pos - 1] not in [\">\", \"<\"]:\n-        raise ValueError(\"Constraint parsing error: must contain one of '>=' or '<='\")\n-      e1_str = cs[:eq_pos - 1]\n-      e1, = _Parser(e1_str, None, repr(e1_str), self).parse()\n-      e2_str = cs[eq_pos + 1:]\n-      e2, = _Parser(e2_str, None, repr(e2_str), self).parse()\n-      diff = e1 - e2 if cs[eq_pos - 1] == \">\" else e2 - e1\n-      return _ensure_poly(diff, \"symbolic_scope\", self)\n-\n-    if isinstance(constraints_str, str):\n+  def _parse_and_process_explicit_constraint(self, c_str: str):\n+    if not isinstance(c_str, str):\n       raise ValueError(\n-          \"The symbolic constraints should be a sequence of strings. \"\n-          f\"Got {repr(constraints_str)}\")\n-    return tuple(parse_one(cs) for cs in constraints_str)\n+          f\"SymbolicScope constraint must be a string: got {repr(c_str)}\")\n+    cmp_pos, cmp, is_geq = c_str.find(\"==\"), Comparator.EQ, True\n+    if cmp_pos < 0:\n+      cmp_pos, cmp, is_geq = c_str.find(\">=\"), Comparator.GEQ, True\n+      if cmp_pos < 0:\n+        cmp_pos, cmp, is_geq = c_str.find(\"<=\"), Comparator.GEQ, False\n+      if cmp_pos < 0:\n+        raise ValueError(\"Constraint parsing error: must contain one of '==' or '>=' or '<='\")\n+    e1_str = c_str[:cmp_pos]\n+    e1, = _Parser(e1_str, None, repr(e1_str), self).parse()\n+    e2_str = c_str[cmp_pos + 2:]\n+    e2, = _Parser(e2_str, None, repr(e2_str), self).parse()\n+    if cmp == Comparator.GEQ and not is_geq:\n+      e1, e2 = e2, e1\n+\n+    diff = e1 - e2\n+    if (diff_const := _DimExpr.to_constant(diff)) is not None:\n+      if ((cmp == Comparator.EQ and diff_const != 0) or\n+          (cmp == Comparator.GEQ and diff_const < 0)):\n+        raise ValueError(f\"Unsatisfiable explicit constraint: {c_str}\")\n+      return\n+\n+    constr = _SymbolicConstraint(debug_str=c_str, cmp=cmp, diff=diff)  # type: ignore[arg-type]\n+    self._explicit_constraints.append(constr)\n+    if cmp == Comparator.EQ:\n+      if not isinstance(e1, _DimExpr):\n+        raise ValueError(\"Invalid equality constraint: {e1} == {e2}. \"\n+                         \"The left-hand-side must be of the form `term * coefficient`.\")\n+      (before, before_k), *rest = e1._monomials_sorted\n+      if rest:\n+        raise ValueError(\"Invalid equality constraint: {e1} == {e2}. \"\n+                         \"The left-hand-side must be of the form `term * coefficient`.\")\n+\n+      after = _ensure_poly(e2, \"parse_constraint\", e1.scope)\n+      if before in self._normalization_rules:\n+        raise NotImplementedError(\n+            f\"Found multiple equality constraints with the same left-hand-side: {before}\")\n+      self._normalization_rules[before] = (after, before_k)\n \n   def _check_same_scope(self, other: _DimExpr,\n                         when: str = \"\",\n@@ -1474,7 +1549,7 @@ class _Parser:\n         power, tok = self.integer(tok)\n         a = a ** power\n \n-      acc = acc * a if acc is not None else a  # type:ignore [operator]\n+      acc = acc * a if acc is not None else a  # type: ignore[operator]\n       if tok.exact_type in self.FOLLOW_MON:\n         return acc, tok\n       tok = self.consume_token(tok, tokenize.STAR)\n@@ -1510,6 +1585,10 @@ class _Parser:\n     tok = self.consume_token(tok, tokenize.COMMA)\n     e2, tok = self.expr(tok)\n     tok = self.consume_token(tok, tokenize.RPAR)\n+    if op == _DimAtom.MAX:\n+      return core.max_dim(e1, e2), tok\n+    if op == _DimAtom.MIN:\n+      return core.min_dim(e1, e2), tok\n     return _DimExpr.from_operation(op, e1, e2,\n                                    scope=self.scope), tok  # type: ignore\n \n@@ -1586,9 +1665,6 @@ class CachingShapeEvaluator:\n \n @dataclasses.dataclass(frozen=True)\n class ShapeConstraint:\n-  class Comparator(Enum):\n-    EQ = 1\n-    GEQ = 2\n \n   comp: Comparator\n   left: DimSize\n@@ -1601,9 +1677,9 @@ class ShapeConstraint:\n     \"\"\"Evaluates a constraint statically.\"\"\"\n     left, right = eval.evaluate(self.left), eval.evaluate(self.right)\n     try:\n-      if self.comp == ShapeConstraint.Comparator.EQ:\n+      if self.comp == Comparator.EQ:\n         ok = (left == right)\n-      elif self.comp == ShapeConstraint.Comparator.GEQ:\n+      elif self.comp == Comparator.GEQ:\n         ok = (left >= right)\n       else:\n         assert False  # We are in a context where we know we can evaluate\n@@ -1625,24 +1701,24 @@ class ShapeConstraint:\n     # Try to evaluate the constraint statically.\n     if core.is_constant_shape((left, right)):\n       left_int, right_int = op.index(left), op.index(right)\n-      if self.comp == ShapeConstraint.Comparator.EQ:\n+      if self.comp == Comparator.EQ:\n         if not (left_int == right_int):\n           raise self.make_error(eval)\n-      elif self.comp == ShapeConstraint.Comparator.GEQ:\n+      elif self.comp == Comparator.GEQ:\n         if not (left_int >= right_int):\n           raise self.make_error(eval)\n       else: assert False\n       return None\n \n-    if self.comp == ShapeConstraint.Comparator.EQ:\n+    if self.comp == Comparator.EQ:\n       is_ok = lax.eq(left, right)\n-    elif self.comp == ShapeConstraint.Comparator.GEQ:\n+    elif self.comp == Comparator.GEQ:\n       is_ok = lax.ge(left, right)\n     else: assert False\n     return is_ok\n \n   def __str__(self):\n-    return (f\"{self.left} {'==' if self.comp == ShapeConstraint.Comparator.EQ else '>='} {self.right}\"\n+    return (f\"{self.left} {'==' if self.comp == Comparator.EQ else '>='} {self.right}\"\n             f\" ({self.error_message_pieces})\")\n   __repr__ = __str__\n \n@@ -1687,7 +1763,7 @@ class ShapeConstraints:\n     self.constraints: list[ShapeConstraint] = []\n \n   def add_constraint(self,\n-                     comp: ShapeConstraint.Comparator,\n+                     comp: Comparator,\n                      left: DimSize, right: DimSize,\n                      error_message_pieces: Sequence[str | DimSize]):\n     c = ShapeConstraint(comp, left, right, error_message_pieces)\n@@ -1910,7 +1986,7 @@ def _solve_dim_equations(\n       else:\n         var_value, var_remainder = divmod(dim_value, core.dim_constant(factor_var))  # type: ignore\n         shape_constraints.add_constraint(\n-            ShapeConstraint.Comparator.EQ, var_remainder, 0,\n+            Comparator.EQ, var_remainder, 0,\n             error_message_pieces=([\n                 \"Input shapes do not match the polymorphic shapes specification. \"\n                 \"Division had remainder \", var_remainder,\n@@ -1929,7 +2005,7 @@ def _solve_dim_equations(\n         \"), \"])\n \n       shape_constraints.add_constraint(\n-          ShapeConstraint.Comparator.GEQ, var_value, 1,\n+          Comparator.GEQ, var_value, 1,\n           error_message_pieces=[\n                 \"Input shapes do not match the polymorphic shapes specification. \"\n                 f\"Expected value >= 1 for dimension variable '{var}'.\" +\n@@ -1941,7 +2017,7 @@ def _solve_dim_equations(\n     else:\n       # All variables are resolved for this equation, we emit an assertion\n       shape_constraints.add_constraint(\n-          ShapeConstraint.Comparator.EQ,\n+          Comparator.EQ,\n           _DimExpr.from_var(eqn.dim_name, eqn.aval_dim_expr.scope),\n           eqn.aval_dim_expr.evaluate(shape_env),\n           error_message_pieces=([\n@@ -1958,14 +2034,15 @@ def _solve_dim_equations(\n   def add_explicit_symbolic_constraints(shape_env: DimVarEnv):\n     if not shape_env: return\n     assert scope is not None\n-    for c, c_str in scope._explicit_constraints:\n-      c_value = c.evaluate(shape_env)\n+    for constr in scope._explicit_constraints:\n+      c_value = constr.diff.evaluate(shape_env)\n       shape_constraints.add_constraint(\n-          ShapeConstraint.Comparator.GEQ, c_value, 0,\n+          constr.cmp, c_value, 0,\n           error_message_pieces=[\n-                f\"Input shapes do not match the symbolic shape constraint {c_str}. \"\n-                f\"Expected '{c}' to be greater or equal to 0, but found \",\n-                c_value,\n+                f\"Input shapes do not match the symbolic shape constraint {constr.debug_str}. \"\n+                f\"Expected '{constr.diff}' to be \"\n+                f\"{'greater or equal' if constr.cmp == Comparator.GEQ else 'equal'} to 0, \"\n+                \"but found \", c_value,\n                 \". \" + poly_specs_err_msg\n               ] + solution_error_message_pieces + [\n               solution_err_msg_trailer_errors])\n"
        },
        {
            "name": "_shape_poly_decision.py",
            "path": "jax/experimental/export/_shape_poly_decision.py",
            "patches": [
                {
                    "old_start": 31,
                    "old_length": 8,
                    "new_start": 31,
                    "new_length": 10,
                    "hunk": "@@ -31,8 +31,10 @@ from jax.experimental.export._shape_poly import (\n   SymbolicScope,\n   DimSize,\n   InconclusiveDimensionOperation,\n+  Comparator,\n )\n \n+def sgn(x): return 1 if x >= 0 else -1\n \n def geq_decision(e1: DimSize, e2: DimSize,\n                  cmp_str: Callable[[], str]) -> bool:\n"
                },
                {
                    "old_start": 55,
                    "old_length": 7,
                    "new_start": 57,
                    "new_length": 7,
                    "hunk": "@@ -55,7 +57,7 @@ def geq_decision(e1: DimSize, e2: DimSize,\n   else:\n     return int(e1) >= int(e2)\n   decision = _DecisionByElimination(scope)\n-  lb, ub = decision.bounds(e1 - e2, _stop_early_for_geq0)\n+  lb, ub = decision.bounds(e1 - e2, _stop_early_for_geq0_or_lt0)\n   if lb >= 0:\n     return True\n   if ub < 0:\n"
                },
                {
                    "old_start": 70,
                    "old_length": 7,
                    "new_start": 72,
                    "new_length": 7,
                    "hunk": "@@ -70,7 +72,7 @@ def geq_decision(e1: DimSize, e2: DimSize,\n \n _shape_poly._geq_decision = geq_decision\n \n-def _stop_early_for_geq0(lb, ub):\n+def _stop_early_for_geq0_or_lt0(lb, ub):\n   return lb >= 0 or ub < 0\n \n def bounds_decision(e: DimSize,\n"
                },
                {
                    "old_start": 98,
                    "old_length": 6,
                    "new_start": 100,
                    "new_length": 8,
                    "hunk": "@@ -98,6 +100,8 @@ class _DecisionByElimination:\n \n   if `sgn(m_c)*m_k < 0`\n     then `abs(m_c)*e <= e0`, hence, `UB(e) <= floor(UB(e0) / abs(m_c))`,\n+\n+  See the implementation in self.combine_term_with_existing.\n   \"\"\"\n   def __init__(self, scope: SymbolicScope):\n     self.scope = scope\n"
                },
                {
                    "old_start": 106,
                    "old_length": 122,
                    "new_start": 110,
                    "new_length": 161,
                    "hunk": "@@ -106,122 +110,161 @@ class _DecisionByElimination:\n     # the explicit constraints.\n     self._term_bounds: dict[_DimMon, tuple[float, float]] = {}\n     # The _expr_constraints represents a set of constraints that are not\n-    # just simple monomials. The set is represented as a mapping from a\n-    # monomial \"m\" to tuples (k, c) where \"c >= 0\" represents a constraint that\n-    # has \"m\" as the leading monomial with coefficient \"k\".\n-    self._expr_constraints: dict[_DimMon, set[tuple[int, _DimExpr]]] = collections.defaultdict(set)\n-\n-    # TODO: find a way to reuse the state reflecting the explicit constraints\n-    # We sort the constraints, so that the results of the heuristics do not\n-    # depend on the order in which the user writes the constraints.\n-    for c, c_str in sorted(scope._explicit_constraints,\n-                           key=lambda c: c[0]._monomials_sorted):\n-      self.add_constraint(c, 0, c_str)\n-\n-  def add_constraint(self,\n-                     e1: _DimExpr | int | float,\n-                     e2: _DimExpr | int | float,\n-                     constraint_str: str | None = None):\n+    # just simple terms. The set is represented as a mapping from a\n+    # term \"t\" to tuples (cmp, k, c) where \"c >= 0\" (if cmp is GEQ else \"c == 0\")\n+    # represents a constraint that has \"t\" as the leading term with coefficient \"k\".\n+    self._expr_constraints: dict[_DimMon, set[tuple[Comparator, int, _DimExpr]]] = collections.defaultdict(set)\n+\n+    # Process the explicit constraints in the order in which the user specifies\n+    # them. This is because the heuristics depend on the order in which the\n+    # constraints are processed, and this way we give the user a way to control\n+    # the result (albeit, for now, without a good feedback loop to understand\n+    # how the order matters for inequalities).\n+    for constr in scope._explicit_constraints:\n+      if constr.cmp == Comparator.GEQ:\n+        self.combine_and_add_constraint(constr.cmp, constr.diff, 0, constr.debug_str)\n+      else:\n+        # The equality constraints are not needed for inequality decisions,\n+        # because the LHS should always be rewritten in terms of the RHS.\n+        # In fact, adding them may break the assumption that if we eliminate\n+        # the leading term we end up with only smaller terms, because the LHS\n+        # may appear in the rest and may be rewritten to something larger.\n+        # However, we want to add the implicit constraints within.\n+        for m, _ in constr.diff.monomials():\n+          if m.degree == 0: continue\n+          self.add_implicit_constraints(m)\n+\n+  def combine_and_add_constraint(self,\n+                                 cmp: Comparator,\n+                                 e1: _DimExpr | int | float,\n+                                 e2: _DimExpr | int | float,\n+                                 debug_str: str | None = None):\n     \"\"\"Adds a constraint \"e1 >= e2\" to the internal state.\"\"\"\n     if isinstance(e1, float):\n-      if np.isinf(e1) and e1 >= 0: return\n+      if np.isinf(e1) and e1 >= 0 and cmp == Comparator.GEQ: return\n       assert e1 == np.floor(e1)\n       e1 = int(e1)\n     if isinstance(e2, float):\n-      if np.isinf(e2) and e2 <= 0: return\n+      if np.isinf(e2) and e2 <= 0 and cmp == Comparator.GEQ: return\n+      assert e2 == np.floor(e2)\n       e2 = int(e2)\n-    e = e1 if isinstance(e2, (int, float)) and e2 == 0 else e1 - e2\n-    if constraint_str is None:\n-      constraint_str = f\"{e1} >= {e2}\"\n+    e = e1 - e2\n+    if debug_str is None:\n+      debug_str = f\"{e1} >= {e2}\"\n     if (const := _DimExpr.to_constant(e)) is not None:\n       if const < 0:\n-        raise ValueError(f\"Unsatisfiable constraint: {constraint_str}\")\n+        raise ValueError(f\"Unsatisfiable constraint: {debug_str}\")\n       return\n     assert isinstance(e, _DimExpr)\n-    self._add_to_state(e, constraint_str)\n-    combinations = self._combine_with_existing_constraints(e, constraint_str)\n-    for a in combinations:\n-      self._add_to_state(a, f\"{a} >= 0\")\n-\n-\n-  def _combine_with_existing_constraints(self,\n-                                         e: _DimExpr,\n-                                         debug_str: str) -> set[_DimExpr]:\n-    # This combines `e` with those constraints already present. The resulting\n-    # constraints are not scanned for new internal constraints (because there\n-    # are no new monomials), but they are also not combined further.\n-    # TODO: this results in incompleteness, but it is probably a good\n-    # compromise.\n-    combinations: set[_DimExpr] = set()\n-    def acc_combination(e: _DimExpr | int):\n-      if (const := _DimExpr.to_constant(e)) is not None:\n-        if const < 0:\n-          raise ValueError(f\"Unsatisfiable constraints: {debug_str}\")\n-      else:\n-        combinations.add(e)  # type: ignore\n-\n-    # First combine with the existing monomial constraints\n-    for e_m, e_c in e.monomials():\n-      if e_m.degree == 0: continue\n-      m_lb, m_ub = self._term_bounds.get(e_m, (-np.inf, np.inf))\n-      if e_c > 0:\n-        if m_ub < np.inf:\n-          e_minus_m = _DimExpr._linear_combination(e._monomials_sorted, 0, 1,\n-                                                   [(e_m, e_c)], 0, -1)\n-          e_minus_m_ub = _DimExpr._linear_combination(e_minus_m, 0, 1,\n-                                                      [(_DimMon(), 1)], 0, e_c * int(m_ub))\n-          acc_combination(_DimExpr(e_minus_m_ub, e.scope))\n-      else:\n-        if m_lb > -np.inf:\n-          e_minus_m = _DimExpr._linear_combination(e._monomials_sorted, 0, 1,\n-                                                   [(e_m, e_c)], 0, -1)\n-          e_minus_m_lb = _DimExpr._linear_combination(e_minus_m, 0, 1,\n-                                                      [(_DimMon(), 1)], 0, e_c * int(m_lb))\n-          acc_combination(_DimExpr(e_minus_m_lb, e.scope))\n-\n-    for prev_constraints in self._expr_constraints.values():\n-      for _, prev in prev_constraints:\n-        # Compose \"e\" with \"prev\" if they have one monomial with different\n-        # signs\n-        prev_coeffs = dict(prev._monomials_sorted)\n-        for e_m, e_c in e.monomials():\n-          if e_m.degree == 0: continue\n-          prev_c = prev_coeffs.get(e_m)\n-          if prev_c is not None and prev_c * e_c < 0:\n-            new_constraint = _DimExpr(\n-                _DimExpr._linear_combination(e._monomials_sorted, 0, abs(prev_c),\n-                                             prev._monomials_sorted, 0, abs(e_c)),\n-                e.scope)\n-            acc_combination(new_constraint)\n-            break\n-\n-    return combinations\n-\n-  def _add_to_state(self, e: _DimExpr,\n-                    constraint_str: str):\n+    # TODO: we only really need to add the implicit constraints now, else\n+    # we may have to consider combining e with itself below (not harmful, just\n+    # wasteful.\n+    self.add_to_state(cmp, e, debug_str)\n+    geq_combinations = self.combine_constraint_with_existing(cmp, e, debug_str)\n+    for cmp, a in geq_combinations:\n+      self.add_to_state(cmp, a, f\"{a} >= 0\")\n+\n+  def add_to_state(self,\n+                   cmp: Comparator,\n+                   e: _DimExpr,\n+                   debug_str: str):\n     \"\"\"Updates the internal state to reflect \"e >= 0\". \"\"\"\n     assert _DimExpr.to_constant(e) is None\n     for m, m_c in e.monomials():\n       if m.degree == 0: continue\n-      _add_internal_constraints(self, m, e.scope)\n+      self.add_implicit_constraints(m)\n \n     if (mon_factors := e.to_single_term()) is not None:\n-      n, mon_c, mon = mon_factors\n-      bounds = self._term_bounds.get(mon, (- np.inf, np.inf))\n-      if mon_c > 0:\n-        mon_ge = int(np.ceil(- n / mon_c))\n-        new_bounds = (max(mon_ge, bounds[0]), bounds[1])\n-      else:\n-        le = int(np.floor(-n / mon_c))\n-        new_bounds = (bounds[0], min(le, bounds[1]))\n-      if new_bounds[0] > new_bounds[1]:\n-        raise ValueError(f\"Unsatisfiable constraint: {constraint_str}\")\n+      n, mon_c, mon = mon_factors  # n + mon * mon_c [== | >=] 0\n+      lb, ub = self._term_bounds.get(mon, (- np.inf, np.inf))\n+      if cmp == Comparator.EQ:\n+        # n + mon_c * mon == 0  ->  mon == - n // mon_c\n+        if n % mon_c:\n+          raise ValueError(f\"Unsatisfiable constraint: {debug_str}\")\n+        mon_val = - (n // mon_c)\n+        lb = max(lb, mon_val)\n+        ub = min(ub, mon_val)\n+      else:  # GEQ\n+        if mon_c > 0:\n+          lb = max(lb, int(np.ceil(- n / mon_c)))\n+        else:\n+          ub = min(ub, int(np.floor(- n / mon_c)))\n+      if lb > ub:\n+        raise ValueError(f\"Unsatisfiable constraint: {debug_str}\")\n \n-      self._term_bounds[mon] = new_bounds\n+      self._term_bounds[mon] = (lb, ub)\n       return\n \n-    lead_m, lead_m_c = e.leading_term\n-    self._expr_constraints[lead_m].add((lead_m_c, e))\n+    lead_t, lead_t_k = e.leading_term\n+    self._expr_constraints[lead_t].add((cmp, lead_t_k, e))\n+\n+  def combine_term_with_existing(self, t: _DimMon, t_k: int, *,\n+                                 scope: _shape_poly.SymbolicScope,\n+                                 only_smaller_than_t=True,\n+                                 ) -> Sequence[tuple[Comparator,\n+                                                     _DimExpr,\n+                                                     int,\n+                                                     int]]:\n+    \"\"\"\n+    Combine a term with existing constraints.\n+    For input (t, t_k) the tuple (c_eq, c, c_s, t_s) is among the returned\n+    tuples if there exists a constraint `c =[c_eq] 0` that can be combined\n+    with `t*t_k` to eliminate `t`.\n+\n+      * `c =[c_eq] 0`\n+      * The term `comb = t*t_k*t_s + c*c_s` does not contain `t`, and if\n+        `only_smaller_than_t` then `comb` contains only terms structurally\n+         smaller than `t`.\n+      * `c_s > 0`\n+    \"\"\"\n+    # TODO: maybe a generator is useful here instead of materializing the list\n+    acc: list[tuple[Comparator, _DimExpr, int, int]] = []\n+    # First combine with the existing term constraints\n+    t_lb, t_ub = self._term_bounds.get(t, (-np.inf, np.inf))\n+    if t_lb == t_ub:\n+      acc.append((Comparator.EQ, _DimExpr(((t, 1),), scope) - int(t_lb),\n+                  abs(t_k), - sgn(t_k)))\n+    else:\n+      if t_lb > -np.inf:\n+        acc.append((Comparator.GEQ, _DimExpr(((t, 1),), scope) - int(t_lb),\n+                    abs(t_k), - sgn(t_k)))\n+      if t_ub < np.inf:\n+        acc.append((Comparator.GEQ, _DimExpr(((t, -1),), scope) + int(t_ub),\n+                    abs(t_k), sgn(t_k)))\n+\n+    for prev_constraint in ([self._expr_constraints[t]] if only_smaller_than_t\n+                            else self._expr_constraints.values()):\n+      for c_eq, _, c in prev_constraint:\n+        # TODO: optimize this dict()\n+        tc_k = dict(c._monomials_sorted).get(t)\n+        if tc_k is not None:\n+          # c =[c_eq] 0 AND t*tc_k appears in c.\n+          c_s = abs(t_k)\n+          c_t = - tc_k * sgn(t_k)\n+          acc.append((c_eq, c, c_s, c_t))\n+    return acc\n+\n+  def combine_constraint_with_existing(self,\n+                                       eq: Comparator,\n+                                       e: _DimExpr,\n+                                       debug_str: str) -> set[tuple[Comparator, _DimExpr]]:\n+    combinations: set[tuple[Comparator, _DimExpr]] = set()\n+    for t, t_k in e._monomials_sorted:\n+      if t.degree == 0: continue\n+      for (c_eq, c, c_s, t_s) in self.combine_term_with_existing(t, t_k,\n+                                                                 only_smaller_than_t=False,\n+                                                                 scope=e.scope):\n+        # c =[c_eq] 0 AND c_s > 0 AND t*t_k*t_s + c*c_s does not contain t\n+        if t_s > 0 or eq == Comparator.EQ:\n+          new_eq = Comparator.EQ if (eq == c_eq == Comparator.EQ) else Comparator.GEQ\n+          new_e = e * t_s + c * c_s\n+          if (const := _DimExpr.to_constant(new_e)) is not None:\n+            if ((new_eq == Comparator.GEQ and const < 0) or\n+                (new_eq == Comparator.EQ and const != 0)):\n+              raise ValueError(f\"Unsatisfiable constraints: {debug_str}\")\n+          else:\n+            combinations.add((new_eq, new_e))  # type: ignore\n+    return combinations\n \n   def bounds(self, e: DimSize,\n              stop_early: Callable[[float, float], bool] | None\n"
                },
                {
                    "old_start": 233,
                    "old_length": 10,
                    "new_start": 276,
                    "new_length": 17,
                    "hunk": "@@ -233,10 +276,17 @@ class _DecisionByElimination:\n     if (const := _DimExpr.to_constant(e)) is not None:\n       return (const, const)\n     assert isinstance(e, _DimExpr)\n-    cache_key = (e, stop_early)\n-    if (res := self.scope._bounds_cache.get(cache_key)) is not None: return res\n-    res = self._bounds_for_sorted_terms(e.scope, e._monomials_sorted, 0, stop_early)\n-    self.scope._bounds_cache[cache_key] = res\n+    # TODO: turn off the caching for now. Sometimes it leads to incompleteness.\n+    # It is also too weak because if the stop_early is None, then the result\n+    # should apply for any other stop_early. Similarly, if stop_early is\n+    # _stop_early_for_geq0_or_leq0 and the result spans 0, then this should\n+    # apply even if we don't have a stop_early.\n+    #cache_key = (e, stop_early)\n+    #if (res := self.scope._bounds_cache.get(cache_key)) is not None: return res\n+    lb, ub = self._bounds_for_sorted_terms(e.scope, e._monomials_sorted, 0, stop_early)\n+    res = (int(lb) if lb > -np.inf else lb,\n+           int(ub) if ub < np.inf else ub)\n+    #self.scope._bounds_cache[cache_key] = res\n     return res\n \n   def _bounds_for_sorted_terms(self,\n"
                },
                {
                    "old_start": 250,
                    "old_length": 59,
                    "new_start": 300,
                    "new_length": 51,
                    "hunk": "@@ -250,59 +300,51 @@ class _DecisionByElimination:\n     \"\"\"\n     if i >= len(e): return (0, 0)\n \n-    m, m_c = e[i]\n-    if len(m) == 0:  # A constant\n+    t, t_k = e[i]\n+    if len(t) == 0:  # A constant\n       assert i == len(e) - 1  # Must be last\n-      return (m_c, m_c)\n+      return (t_k, t_k)\n \n-    _add_internal_constraints(self, m, scope)\n+    self.add_implicit_constraints(t)\n     lb = -np.inf\n     ub = np.inf\n \n-    # Look among the term bounds\n-    if m in self._term_bounds:\n-      m_lb, m_ub = self._term_bounds.get(m, (- np.inf, np.inf))\n-      rest_lb, rest_ub = self._bounds_for_sorted_terms(scope, e, i + 1, None)\n-      if m_c > 0:\n-        lb = max(lb, m_c * m_lb + rest_lb)\n-        ub = min(ub, m_c * m_ub + rest_ub)\n-      else:\n-        lb = max(lb, m_c * m_ub + rest_lb)\n-        ub = min(ub, m_c * m_lb + rest_ub)\n-\n-      if stop_early is not None and stop_early(lb, ub): return (lb, ub)\n+    for (c_eq, c, c_s, t_s) in self.combine_term_with_existing(t, t_k,\n+                                                               only_smaller_than_t=True,\n+                                                               scope=scope):\n+      # `c =[eq] 0` AND `t*t_k*t_s + c*c_s` contains only terms smaller than t\n+      # AND c_s > 0.\n+      # rest = e[i:]*t_s + c*c_s` AND `rest_ub >= rest >= rest_lb`\n+      rest = _DimExpr._linear_combination(e, i, t_s,\n+                                          c._monomials_sorted, 0, c_s)\n+      rest_lb, rest_ub = self._bounds_for_sorted_terms(scope, rest, 0, None)\n+      if rest_ub < np.inf:\n+        if t_s > 0:\n+          ub = min(ub, int(np.floor(rest_ub / t_s)))\n+        else:\n+          lb = max(lb, int(np.ceil(rest_ub / t_s)))\n \n-    # Now look through the _expr_constraints\n-    if m in self._expr_constraints:\n-      for m_k, c in self._expr_constraints[m]:\n-        # A complex expression. See comments from top of class.\n-        sgn_m_k = 1 if m_k > 0 else -1\n-        abs_m_k = m_k * sgn_m_k\n-        # The recursive call has a smaller leading monomial, because we are only\n-        # looking at the tail of e, and in c the largest monomial is m, and the\n-        # merging will cancel the m.\n-        rest = _DimExpr._linear_combination(e, i, abs_m_k,\n-                                            c._monomials_sorted, 0, - sgn_m_k * m_c)\n-        rest_lb, rest_ub = self._bounds_for_sorted_terms(scope, rest, 0, None)\n-        if m_c / m_k > 0:\n-          lb = max(lb, np.ceil(rest_lb / abs_m_k))\n+      if rest_lb > - np.inf and c_eq == Comparator.EQ:\n+        if t_s > 0:\n+          lb = max(lb, int(np.ceil(rest_lb / t_s)))\n         else:\n-          ub = min(ub, np.floor(rest_ub / abs_m_k))\n-        if stop_early is not None and stop_early(lb, ub): return (lb, ub)\n+          ub = min(ub, int(np.floor(rest_lb / t_s)))\n+\n+      if stop_early is not None and stop_early(lb, ub): return (lb, ub)\n \n     # Now look for special rules for atoms\n-    if (m_a := m.to_atom()) is not None:\n+    if (m_a := t.to_atom()) is not None:\n       if m_a.operation in [_DimAtom.MAX, _DimAtom.MIN]:\n         # m_c*MAX(op1, op2) + rest_e >= max(m_c * op1 + rest_e, m_c * op2 + rest_e)\n         #   if m_c > 0. Similar rules for when m_c < 0 and for MIN.\n         op1, op2 = m_a.operands\n         rest1 = _DimExpr._linear_combination(e, i + 1, 1,\n-                                             op1._monomials_sorted, 0, m_c)\n+                                             op1._monomials_sorted, 0, t_k)\n         rest2 = _DimExpr._linear_combination(e, i + 1, 1,\n-                                             op2._monomials_sorted, 0, m_c)\n+                                             op2._monomials_sorted, 0, t_k)\n         rest1_lb, rest1_ub = self._bounds_for_sorted_terms(scope, rest1, 0, None)\n         rest2_lb, rest2_ub = self._bounds_for_sorted_terms(scope, rest2, 0, None)\n-        like_max = (m_c > 0 if m_a.operation == _DimAtom.MAX else m_c < 0)\n+        like_max = (t_k > 0 if m_a.operation == _DimAtom.MAX else t_k < 0)\n         if like_max:\n           lb = max(lb, max(rest1_lb, rest2_lb))\n           ub = min(ub, max(rest1_ub, rest2_ub))\n"
                },
                {
                    "old_start": 313,
                    "old_length": 101,
                    "new_start": 355,
                    "new_length": 101,
                    "hunk": "@@ -313,101 +355,101 @@ class _DecisionByElimination:\n \n     return lb, ub\n \n-def _add_internal_constraints(decision: _DecisionByElimination, m: _DimMon, scope: SymbolicScope):\n-  \"\"\"Adds the internal constraints for the monomial `m`.\"\"\"\n-  if m in decision._processed_for_internal_constraints: return\n-  decision._processed_for_internal_constraints.add(m)\n-  m_e = _DimExpr.from_monomial(m, 1, scope)  # m as a _DimExpr\n-  a = m.to_atom()\n-  if a is None:\n-    # This is a multiplication of atoms. Try to compute bounds based on\n-    # the bounds of the atoms.\n-    bounds = []\n-    for a, exp in m.items():\n-      a_l, a_u = decision.bounds(_DimExpr.from_monomial(_DimMon.from_atom(a, 1),\n-                                                        1, scope), None)\n-      assert a_l <= a_u\n-      bounds.append((a_l ** exp, a_u ** exp))\n-\n-    candidate_bounds = [math.prod(atom_bounds)\n-                        for atom_bounds in itertools.product(*bounds)]\n-    m_l = min(*candidate_bounds)\n-    m_u = max(*candidate_bounds)\n-    decision.add_constraint(m_e, m_l)\n-    decision.add_constraint(m_u, m_e)\n-    return\n-\n-  # It is an atom, is it a variable?\n-  if (v := a.to_var()) is not None:\n-    decision.add_constraint(m_e, 1)  # v >= 1\n-    return\n-\n-  if a.operation == _DimAtom.MOD:\n-    op1, op2 = a.operands\n-    op2_b_l, op2_b_u = decision.bounds(op2, _stop_early_for_geq0)\n-    if op2_b_l > 0:  # positive divisor\n-      decision.add_constraint(m_e, 0)  # m >= 0\n-      decision.add_constraint(op2 - 1, m_e)  # m <= op2 - 1\n-      decision.add_constraint(op2_b_u - 1, m_e)\n-    elif op2_b_u < 0:  # negative divisor\n-      decision.add_constraint(m_e, op2 + 1)  # m >= op2 + 1\n-      decision.add_constraint(m_e, op2_b_l + 1)\n-      decision.add_constraint(0, m_e)  # m <= 0\n-    return\n-\n-  if a.operation == _DimAtom.FLOORDIV:\n-    op1, op2 = a.operands\n-    (op1_l, op1_u) = decision.bounds(op1, None)\n-    (op2_l, op2_u) = decision.bounds(op2, None)\n-\n-    def math_floor_with_inf(a: float, b: float):  # math.floor, but aware of inf\n-      # When either a or b are infinite, the results represent the limit\n-      # of \"a // b\".\n-      assert b != 0\n-      if not np.isinf(b):  # divisor b is finite\n-        if not np.isinf(a):\n-          return math.floor(a / b)\n-        # a is infinite, b is finite\n-        return -np.inf if (a >= 0) != (b >= 0) else np.inf\n-      elif not np.isinf(a):  # dividend a is finite and divisor b is infinite\n-        return -1 if (a >= 0) != (b >= 0) else 0\n-      else:  # both dividend and divisor are infinite\n-        return -np.inf if (a >= 0) != (b >= 0) else np.inf\n-\n-    # Same reasoning as for multiplication: the bounds are among the cross-product\n-    # of the bounds.\n-    candidate_bounds = [math_floor_with_inf(op1_l, op2_l),\n-                        math_floor_with_inf(op1_l, op2_u),\n-                        math_floor_with_inf(op1_u, op2_l),\n-                        math_floor_with_inf(op1_u, op2_u)]\n-    m_l = min(*candidate_bounds)\n-    m_u = max(*candidate_bounds)\n-    decision.add_constraint(m_e, m_l)\n-    decision.add_constraint(m_u, m_e)\n-    if op2_l >= 0:\n-      if decision.bounds(op1, _stop_early_for_geq0)[0] >= 0:\n-        decision.add_constraint(m_e, 0)\n-      mod_e = _DimExpr.from_operation(_DimAtom.MOD, op1, op2,\n-                                      scope=scope)\n-      combined = op2 * m_e + mod_e\n-      decision.add_constraint(op1, combined)\n-      decision.add_constraint(combined, op1)\n-    return\n-\n-  if a.operation == _DimAtom.MAX:\n-    op1, op2 = a.operands\n-    op1_b_l, op1_b_u = decision.bounds(op1, None)\n-    op2_b_l, op2_b_u = decision.bounds(op2, None)\n-    decision.add_constraint(m_e, max(op1_b_l, op2_b_l))\n-    decision.add_constraint(max(op1_b_u, op2_b_u), m_e)\n-    decision.add_constraint(m_e, op1)\n-    decision.add_constraint(m_e, op2)\n-\n-  if a.operation == _DimAtom.MIN:\n-    op1, op2 = a.operands\n-    op1_b_l, op1_b_u = decision.bounds(op1, None)\n-    op2_b_l, op2_b_u = decision.bounds(op2, None)\n-    decision.add_constraint(m_e, min(op1_b_l, op2_b_l))\n-    decision.add_constraint(min(op1_b_u, op2_b_u), m_e)\n-    decision.add_constraint(op1, m_e)\n-    decision.add_constraint(op2, m_e)\n+  def add_implicit_constraints(self: _DecisionByElimination, m: _DimMon):\n+    \"\"\"Adds the internal constraints for the monomial `m`.\"\"\"\n+    if m in self._processed_for_internal_constraints: return\n+    self._processed_for_internal_constraints.add(m)\n+    m_e = _DimExpr.from_monomial(m, 1, self.scope)  # m as a _DimExpr\n+    a = m.to_atom()\n+    if a is None:\n+      # This is a multiplication of atoms. Try to compute bounds based on\n+      # the bounds of the atoms.\n+      bounds = []\n+      for a, exp in m.items():\n+        a_l, a_u = self.bounds(_DimExpr.from_monomial(_DimMon.from_atom(a, 1),\n+                                                      1, self.scope), None)\n+        assert a_l <= a_u\n+        bounds.append((a_l ** exp, a_u ** exp))\n+\n+      candidate_bounds = [math.prod(atom_bounds)\n+                          for atom_bounds in itertools.product(*bounds)]\n+      m_l = min(*candidate_bounds)\n+      m_u = max(*candidate_bounds)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_e, m_l)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_u, m_e)\n+      return\n+\n+    # It is an atom, is it a variable?\n+    if (v := a.to_var()) is not None:\n+      self.combine_and_add_constraint(Comparator.GEQ, m_e, 1,\n+                                      debug_str=f\"{v} >= 1\")  # v >= 1\n+      return\n+\n+    if a.operation == _DimAtom.MOD:\n+      op1, op2 = a.operands\n+      op2_b_l, op2_b_u = self.bounds(op2, _stop_early_for_geq0_or_lt0)\n+      if op2_b_l > 0:  # positive divisor\n+        self.combine_and_add_constraint(Comparator.GEQ, m_e, 0)  # m >= 0\n+        self.combine_and_add_constraint(Comparator.GEQ, op2 - 1, m_e)  # m <= op2 - 1\n+        self.combine_and_add_constraint(Comparator.GEQ, op2_b_u - 1, m_e)\n+      elif op2_b_u < 0:  # negative divisor\n+        self.combine_and_add_constraint(Comparator.GEQ, m_e, op2 + 1)  # m >= op2 + 1\n+        self.combine_and_add_constraint(Comparator.GEQ, m_e, op2_b_l + 1)\n+        self.combine_and_add_constraint(Comparator.GEQ, 0, m_e)  # m <= 0\n+      return\n+\n+    if a.operation == _DimAtom.FLOORDIV:\n+      op1, op2 = a.operands\n+      (op1_l, op1_u) = self.bounds(op1, None)\n+      (op2_l, op2_u) = self.bounds(op2, None)\n+\n+      def math_floor_with_inf(a: float, b: float):  # math.floor, but aware of inf\n+        # When either a or b are infinite, the results represent the limit\n+        # of \"a // b\".\n+        assert b != 0\n+        if not np.isinf(b):  # divisor b is finite\n+          if not np.isinf(a):\n+            return math.floor(a / b)\n+          # a is infinite, b is finite\n+          return -np.inf if (a >= 0) != (b >= 0) else np.inf\n+        elif not np.isinf(a):  # dividend a is finite and divisor b is infinite\n+          return -1 if (a >= 0) != (b >= 0) else 0\n+        else:  # both dividend and divisor are infinite\n+          return -np.inf if (a >= 0) != (b >= 0) else np.inf\n+\n+      # Same reasoning as for multiplication: the bounds are among the cross-product\n+      # of the bounds.\n+      candidate_bounds = [math_floor_with_inf(op1_l, op2_l),\n+                          math_floor_with_inf(op1_l, op2_u),\n+                          math_floor_with_inf(op1_u, op2_l),\n+                          math_floor_with_inf(op1_u, op2_u)]\n+      m_l = min(*candidate_bounds)\n+      m_u = max(*candidate_bounds)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_e, m_l)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_u, m_e)\n+      if op2_l >= 0:\n+        if self.bounds(op1, _stop_early_for_geq0_or_lt0)[0] >= 0:\n+          self.combine_and_add_constraint(Comparator.GEQ, m_e, 0)\n+        mod_e = _DimExpr.from_operation(_DimAtom.MOD, op1, op2,\n+                                        scope=self.scope)\n+        combined = op2 * m_e + mod_e\n+        self.combine_and_add_constraint(Comparator.EQ, op1, combined)\n+      return\n+\n+    if a.operation == _DimAtom.MAX:\n+      op1, op2 = a.operands\n+      op1_b_l, op1_b_u = self.bounds(op1, None)\n+      op2_b_l, op2_b_u = self.bounds(op2, None)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_e, max(op1_b_l, op2_b_l))\n+      self.combine_and_add_constraint(Comparator.GEQ, max(op1_b_u, op2_b_u), m_e)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_e, op1)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_e, op2)\n+\n+    if a.operation == _DimAtom.MIN:\n+      op1, op2 = a.operands\n+      op1_b_l, op1_b_u = self.bounds(op1, None)\n+      op2_b_l, op2_b_u = self.bounds(op2, None)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_e, min(op1_b_l, op2_b_l))\n+      self.combine_and_add_constraint(Comparator.GEQ, min(op1_b_u, op2_b_u), m_e)\n+      self.combine_and_add_constraint(Comparator.GEQ, op1, m_e)\n+      self.combine_and_add_constraint(Comparator.GEQ, op2, m_e)\n"
                }
            ],
            "whole_deleted": "-  lb, ub = decision.bounds(e1 - e2, _stop_early_for_geq0)\n-def _stop_early_for_geq0(lb, ub):\n-    # just simple monomials. The set is represented as a mapping from a\n-    # monomial \"m\" to tuples (k, c) where \"c >= 0\" represents a constraint that\n-    # has \"m\" as the leading monomial with coefficient \"k\".\n-    self._expr_constraints: dict[_DimMon, set[tuple[int, _DimExpr]]] = collections.defaultdict(set)\n-\n-    # TODO: find a way to reuse the state reflecting the explicit constraints\n-    # We sort the constraints, so that the results of the heuristics do not\n-    # depend on the order in which the user writes the constraints.\n-    for c, c_str in sorted(scope._explicit_constraints,\n-                           key=lambda c: c[0]._monomials_sorted):\n-      self.add_constraint(c, 0, c_str)\n-\n-  def add_constraint(self,\n-                     e1: _DimExpr | int | float,\n-                     e2: _DimExpr | int | float,\n-                     constraint_str: str | None = None):\n-      if np.isinf(e1) and e1 >= 0: return\n-      if np.isinf(e2) and e2 <= 0: return\n-    e = e1 if isinstance(e2, (int, float)) and e2 == 0 else e1 - e2\n-    if constraint_str is None:\n-      constraint_str = f\"{e1} >= {e2}\"\n-        raise ValueError(f\"Unsatisfiable constraint: {constraint_str}\")\n-    self._add_to_state(e, constraint_str)\n-    combinations = self._combine_with_existing_constraints(e, constraint_str)\n-    for a in combinations:\n-      self._add_to_state(a, f\"{a} >= 0\")\n-\n-\n-  def _combine_with_existing_constraints(self,\n-                                         e: _DimExpr,\n-                                         debug_str: str) -> set[_DimExpr]:\n-    # This combines `e` with those constraints already present. The resulting\n-    # constraints are not scanned for new internal constraints (because there\n-    # are no new monomials), but they are also not combined further.\n-    # TODO: this results in incompleteness, but it is probably a good\n-    # compromise.\n-    combinations: set[_DimExpr] = set()\n-    def acc_combination(e: _DimExpr | int):\n-      if (const := _DimExpr.to_constant(e)) is not None:\n-        if const < 0:\n-          raise ValueError(f\"Unsatisfiable constraints: {debug_str}\")\n-      else:\n-        combinations.add(e)  # type: ignore\n-\n-    # First combine with the existing monomial constraints\n-    for e_m, e_c in e.monomials():\n-      if e_m.degree == 0: continue\n-      m_lb, m_ub = self._term_bounds.get(e_m, (-np.inf, np.inf))\n-      if e_c > 0:\n-        if m_ub < np.inf:\n-          e_minus_m = _DimExpr._linear_combination(e._monomials_sorted, 0, 1,\n-                                                   [(e_m, e_c)], 0, -1)\n-          e_minus_m_ub = _DimExpr._linear_combination(e_minus_m, 0, 1,\n-                                                      [(_DimMon(), 1)], 0, e_c * int(m_ub))\n-          acc_combination(_DimExpr(e_minus_m_ub, e.scope))\n-      else:\n-        if m_lb > -np.inf:\n-          e_minus_m = _DimExpr._linear_combination(e._monomials_sorted, 0, 1,\n-                                                   [(e_m, e_c)], 0, -1)\n-          e_minus_m_lb = _DimExpr._linear_combination(e_minus_m, 0, 1,\n-                                                      [(_DimMon(), 1)], 0, e_c * int(m_lb))\n-          acc_combination(_DimExpr(e_minus_m_lb, e.scope))\n-\n-    for prev_constraints in self._expr_constraints.values():\n-      for _, prev in prev_constraints:\n-        # Compose \"e\" with \"prev\" if they have one monomial with different\n-        # signs\n-        prev_coeffs = dict(prev._monomials_sorted)\n-        for e_m, e_c in e.monomials():\n-          if e_m.degree == 0: continue\n-          prev_c = prev_coeffs.get(e_m)\n-          if prev_c is not None and prev_c * e_c < 0:\n-            new_constraint = _DimExpr(\n-                _DimExpr._linear_combination(e._monomials_sorted, 0, abs(prev_c),\n-                                             prev._monomials_sorted, 0, abs(e_c)),\n-                e.scope)\n-            acc_combination(new_constraint)\n-            break\n-\n-    return combinations\n-\n-  def _add_to_state(self, e: _DimExpr,\n-                    constraint_str: str):\n-      _add_internal_constraints(self, m, e.scope)\n-      n, mon_c, mon = mon_factors\n-      bounds = self._term_bounds.get(mon, (- np.inf, np.inf))\n-      if mon_c > 0:\n-        mon_ge = int(np.ceil(- n / mon_c))\n-        new_bounds = (max(mon_ge, bounds[0]), bounds[1])\n-      else:\n-        le = int(np.floor(-n / mon_c))\n-        new_bounds = (bounds[0], min(le, bounds[1]))\n-      if new_bounds[0] > new_bounds[1]:\n-        raise ValueError(f\"Unsatisfiable constraint: {constraint_str}\")\n-      self._term_bounds[mon] = new_bounds\n-    lead_m, lead_m_c = e.leading_term\n-    self._expr_constraints[lead_m].add((lead_m_c, e))\n-    cache_key = (e, stop_early)\n-    if (res := self.scope._bounds_cache.get(cache_key)) is not None: return res\n-    res = self._bounds_for_sorted_terms(e.scope, e._monomials_sorted, 0, stop_early)\n-    self.scope._bounds_cache[cache_key] = res\n-    m, m_c = e[i]\n-    if len(m) == 0:  # A constant\n-      return (m_c, m_c)\n-    _add_internal_constraints(self, m, scope)\n-    # Look among the term bounds\n-    if m in self._term_bounds:\n-      m_lb, m_ub = self._term_bounds.get(m, (- np.inf, np.inf))\n-      rest_lb, rest_ub = self._bounds_for_sorted_terms(scope, e, i + 1, None)\n-      if m_c > 0:\n-        lb = max(lb, m_c * m_lb + rest_lb)\n-        ub = min(ub, m_c * m_ub + rest_ub)\n-      else:\n-        lb = max(lb, m_c * m_ub + rest_lb)\n-        ub = min(ub, m_c * m_lb + rest_ub)\n-\n-      if stop_early is not None and stop_early(lb, ub): return (lb, ub)\n-    # Now look through the _expr_constraints\n-    if m in self._expr_constraints:\n-      for m_k, c in self._expr_constraints[m]:\n-        # A complex expression. See comments from top of class.\n-        sgn_m_k = 1 if m_k > 0 else -1\n-        abs_m_k = m_k * sgn_m_k\n-        # The recursive call has a smaller leading monomial, because we are only\n-        # looking at the tail of e, and in c the largest monomial is m, and the\n-        # merging will cancel the m.\n-        rest = _DimExpr._linear_combination(e, i, abs_m_k,\n-                                            c._monomials_sorted, 0, - sgn_m_k * m_c)\n-        rest_lb, rest_ub = self._bounds_for_sorted_terms(scope, rest, 0, None)\n-        if m_c / m_k > 0:\n-          lb = max(lb, np.ceil(rest_lb / abs_m_k))\n-          ub = min(ub, np.floor(rest_ub / abs_m_k))\n-        if stop_early is not None and stop_early(lb, ub): return (lb, ub)\n-    if (m_a := m.to_atom()) is not None:\n-                                             op1._monomials_sorted, 0, m_c)\n-                                             op2._monomials_sorted, 0, m_c)\n-        like_max = (m_c > 0 if m_a.operation == _DimAtom.MAX else m_c < 0)\n-def _add_internal_constraints(decision: _DecisionByElimination, m: _DimMon, scope: SymbolicScope):\n-  \"\"\"Adds the internal constraints for the monomial `m`.\"\"\"\n-  if m in decision._processed_for_internal_constraints: return\n-  decision._processed_for_internal_constraints.add(m)\n-  m_e = _DimExpr.from_monomial(m, 1, scope)  # m as a _DimExpr\n-  a = m.to_atom()\n-  if a is None:\n-    # This is a multiplication of atoms. Try to compute bounds based on\n-    # the bounds of the atoms.\n-    bounds = []\n-    for a, exp in m.items():\n-      a_l, a_u = decision.bounds(_DimExpr.from_monomial(_DimMon.from_atom(a, 1),\n-                                                        1, scope), None)\n-      assert a_l <= a_u\n-      bounds.append((a_l ** exp, a_u ** exp))\n-\n-    candidate_bounds = [math.prod(atom_bounds)\n-                        for atom_bounds in itertools.product(*bounds)]\n-    m_l = min(*candidate_bounds)\n-    m_u = max(*candidate_bounds)\n-    decision.add_constraint(m_e, m_l)\n-    decision.add_constraint(m_u, m_e)\n-    return\n-\n-  # It is an atom, is it a variable?\n-  if (v := a.to_var()) is not None:\n-    decision.add_constraint(m_e, 1)  # v >= 1\n-    return\n-\n-  if a.operation == _DimAtom.MOD:\n-    op1, op2 = a.operands\n-    op2_b_l, op2_b_u = decision.bounds(op2, _stop_early_for_geq0)\n-    if op2_b_l > 0:  # positive divisor\n-      decision.add_constraint(m_e, 0)  # m >= 0\n-      decision.add_constraint(op2 - 1, m_e)  # m <= op2 - 1\n-      decision.add_constraint(op2_b_u - 1, m_e)\n-    elif op2_b_u < 0:  # negative divisor\n-      decision.add_constraint(m_e, op2 + 1)  # m >= op2 + 1\n-      decision.add_constraint(m_e, op2_b_l + 1)\n-      decision.add_constraint(0, m_e)  # m <= 0\n-    return\n-\n-  if a.operation == _DimAtom.FLOORDIV:\n-    op1, op2 = a.operands\n-    (op1_l, op1_u) = decision.bounds(op1, None)\n-    (op2_l, op2_u) = decision.bounds(op2, None)\n-\n-    def math_floor_with_inf(a: float, b: float):  # math.floor, but aware of inf\n-      # When either a or b are infinite, the results represent the limit\n-      # of \"a // b\".\n-      assert b != 0\n-      if not np.isinf(b):  # divisor b is finite\n-        if not np.isinf(a):\n-          return math.floor(a / b)\n-        # a is infinite, b is finite\n-        return -np.inf if (a >= 0) != (b >= 0) else np.inf\n-      elif not np.isinf(a):  # dividend a is finite and divisor b is infinite\n-        return -1 if (a >= 0) != (b >= 0) else 0\n-      else:  # both dividend and divisor are infinite\n-        return -np.inf if (a >= 0) != (b >= 0) else np.inf\n-\n-    # Same reasoning as for multiplication: the bounds are among the cross-product\n-    # of the bounds.\n-    candidate_bounds = [math_floor_with_inf(op1_l, op2_l),\n-                        math_floor_with_inf(op1_l, op2_u),\n-                        math_floor_with_inf(op1_u, op2_l),\n-                        math_floor_with_inf(op1_u, op2_u)]\n-    m_l = min(*candidate_bounds)\n-    m_u = max(*candidate_bounds)\n-    decision.add_constraint(m_e, m_l)\n-    decision.add_constraint(m_u, m_e)\n-    if op2_l >= 0:\n-      if decision.bounds(op1, _stop_early_for_geq0)[0] >= 0:\n-        decision.add_constraint(m_e, 0)\n-      mod_e = _DimExpr.from_operation(_DimAtom.MOD, op1, op2,\n-                                      scope=scope)\n-      combined = op2 * m_e + mod_e\n-      decision.add_constraint(op1, combined)\n-      decision.add_constraint(combined, op1)\n-    return\n-\n-  if a.operation == _DimAtom.MAX:\n-    op1, op2 = a.operands\n-    op1_b_l, op1_b_u = decision.bounds(op1, None)\n-    op2_b_l, op2_b_u = decision.bounds(op2, None)\n-    decision.add_constraint(m_e, max(op1_b_l, op2_b_l))\n-    decision.add_constraint(max(op1_b_u, op2_b_u), m_e)\n-    decision.add_constraint(m_e, op1)\n-    decision.add_constraint(m_e, op2)\n-\n-  if a.operation == _DimAtom.MIN:\n-    op1, op2 = a.operands\n-    op1_b_l, op1_b_u = decision.bounds(op1, None)\n-    op2_b_l, op2_b_u = decision.bounds(op2, None)\n-    decision.add_constraint(m_e, min(op1_b_l, op2_b_l))\n-    decision.add_constraint(min(op1_b_u, op2_b_u), m_e)\n-    decision.add_constraint(op1, m_e)\n-    decision.add_constraint(op2, m_e)\n",
            "whole_added": "+  Comparator,\n+def sgn(x): return 1 if x >= 0 else -1\n+  lb, ub = decision.bounds(e1 - e2, _stop_early_for_geq0_or_lt0)\n+def _stop_early_for_geq0_or_lt0(lb, ub):\n+\n+  See the implementation in self.combine_term_with_existing.\n+    # just simple terms. The set is represented as a mapping from a\n+    # term \"t\" to tuples (cmp, k, c) where \"c >= 0\" (if cmp is GEQ else \"c == 0\")\n+    # represents a constraint that has \"t\" as the leading term with coefficient \"k\".\n+    self._expr_constraints: dict[_DimMon, set[tuple[Comparator, int, _DimExpr]]] = collections.defaultdict(set)\n+\n+    # Process the explicit constraints in the order in which the user specifies\n+    # them. This is because the heuristics depend on the order in which the\n+    # constraints are processed, and this way we give the user a way to control\n+    # the result (albeit, for now, without a good feedback loop to understand\n+    # how the order matters for inequalities).\n+    for constr in scope._explicit_constraints:\n+      if constr.cmp == Comparator.GEQ:\n+        self.combine_and_add_constraint(constr.cmp, constr.diff, 0, constr.debug_str)\n+      else:\n+        # The equality constraints are not needed for inequality decisions,\n+        # because the LHS should always be rewritten in terms of the RHS.\n+        # In fact, adding them may break the assumption that if we eliminate\n+        # the leading term we end up with only smaller terms, because the LHS\n+        # may appear in the rest and may be rewritten to something larger.\n+        # However, we want to add the implicit constraints within.\n+        for m, _ in constr.diff.monomials():\n+          if m.degree == 0: continue\n+          self.add_implicit_constraints(m)\n+\n+  def combine_and_add_constraint(self,\n+                                 cmp: Comparator,\n+                                 e1: _DimExpr | int | float,\n+                                 e2: _DimExpr | int | float,\n+                                 debug_str: str | None = None):\n+      if np.isinf(e1) and e1 >= 0 and cmp == Comparator.GEQ: return\n+      if np.isinf(e2) and e2 <= 0 and cmp == Comparator.GEQ: return\n+      assert e2 == np.floor(e2)\n+    e = e1 - e2\n+    if debug_str is None:\n+      debug_str = f\"{e1} >= {e2}\"\n+        raise ValueError(f\"Unsatisfiable constraint: {debug_str}\")\n+    # TODO: we only really need to add the implicit constraints now, else\n+    # we may have to consider combining e with itself below (not harmful, just\n+    # wasteful.\n+    self.add_to_state(cmp, e, debug_str)\n+    geq_combinations = self.combine_constraint_with_existing(cmp, e, debug_str)\n+    for cmp, a in geq_combinations:\n+      self.add_to_state(cmp, a, f\"{a} >= 0\")\n+\n+  def add_to_state(self,\n+                   cmp: Comparator,\n+                   e: _DimExpr,\n+                   debug_str: str):\n+      self.add_implicit_constraints(m)\n+      n, mon_c, mon = mon_factors  # n + mon * mon_c [== | >=] 0\n+      lb, ub = self._term_bounds.get(mon, (- np.inf, np.inf))\n+      if cmp == Comparator.EQ:\n+        # n + mon_c * mon == 0  ->  mon == - n // mon_c\n+        if n % mon_c:\n+          raise ValueError(f\"Unsatisfiable constraint: {debug_str}\")\n+        mon_val = - (n // mon_c)\n+        lb = max(lb, mon_val)\n+        ub = min(ub, mon_val)\n+      else:  # GEQ\n+        if mon_c > 0:\n+          lb = max(lb, int(np.ceil(- n / mon_c)))\n+        else:\n+          ub = min(ub, int(np.floor(- n / mon_c)))\n+      if lb > ub:\n+        raise ValueError(f\"Unsatisfiable constraint: {debug_str}\")\n+      self._term_bounds[mon] = (lb, ub)\n+    lead_t, lead_t_k = e.leading_term\n+    self._expr_constraints[lead_t].add((cmp, lead_t_k, e))\n+\n+  def combine_term_with_existing(self, t: _DimMon, t_k: int, *,\n+                                 scope: _shape_poly.SymbolicScope,\n+                                 only_smaller_than_t=True,\n+                                 ) -> Sequence[tuple[Comparator,\n+                                                     _DimExpr,\n+                                                     int,\n+                                                     int]]:\n+    \"\"\"\n+    Combine a term with existing constraints.\n+    For input (t, t_k) the tuple (c_eq, c, c_s, t_s) is among the returned\n+    tuples if there exists a constraint `c =[c_eq] 0` that can be combined\n+    with `t*t_k` to eliminate `t`.\n+\n+      * `c =[c_eq] 0`\n+      * The term `comb = t*t_k*t_s + c*c_s` does not contain `t`, and if\n+        `only_smaller_than_t` then `comb` contains only terms structurally\n+         smaller than `t`.\n+      * `c_s > 0`\n+    \"\"\"\n+    # TODO: maybe a generator is useful here instead of materializing the list\n+    acc: list[tuple[Comparator, _DimExpr, int, int]] = []\n+    # First combine with the existing term constraints\n+    t_lb, t_ub = self._term_bounds.get(t, (-np.inf, np.inf))\n+    if t_lb == t_ub:\n+      acc.append((Comparator.EQ, _DimExpr(((t, 1),), scope) - int(t_lb),\n+                  abs(t_k), - sgn(t_k)))\n+    else:\n+      if t_lb > -np.inf:\n+        acc.append((Comparator.GEQ, _DimExpr(((t, 1),), scope) - int(t_lb),\n+                    abs(t_k), - sgn(t_k)))\n+      if t_ub < np.inf:\n+        acc.append((Comparator.GEQ, _DimExpr(((t, -1),), scope) + int(t_ub),\n+                    abs(t_k), sgn(t_k)))\n+\n+    for prev_constraint in ([self._expr_constraints[t]] if only_smaller_than_t\n+                            else self._expr_constraints.values()):\n+      for c_eq, _, c in prev_constraint:\n+        # TODO: optimize this dict()\n+        tc_k = dict(c._monomials_sorted).get(t)\n+        if tc_k is not None:\n+          # c =[c_eq] 0 AND t*tc_k appears in c.\n+          c_s = abs(t_k)\n+          c_t = - tc_k * sgn(t_k)\n+          acc.append((c_eq, c, c_s, c_t))\n+    return acc\n+\n+  def combine_constraint_with_existing(self,\n+                                       eq: Comparator,\n+                                       e: _DimExpr,\n+                                       debug_str: str) -> set[tuple[Comparator, _DimExpr]]:\n+    combinations: set[tuple[Comparator, _DimExpr]] = set()\n+    for t, t_k in e._monomials_sorted:\n+      if t.degree == 0: continue\n+      for (c_eq, c, c_s, t_s) in self.combine_term_with_existing(t, t_k,\n+                                                                 only_smaller_than_t=False,\n+                                                                 scope=e.scope):\n+        # c =[c_eq] 0 AND c_s > 0 AND t*t_k*t_s + c*c_s does not contain t\n+        if t_s > 0 or eq == Comparator.EQ:\n+          new_eq = Comparator.EQ if (eq == c_eq == Comparator.EQ) else Comparator.GEQ\n+          new_e = e * t_s + c * c_s\n+          if (const := _DimExpr.to_constant(new_e)) is not None:\n+            if ((new_eq == Comparator.GEQ and const < 0) or\n+                (new_eq == Comparator.EQ and const != 0)):\n+              raise ValueError(f\"Unsatisfiable constraints: {debug_str}\")\n+          else:\n+            combinations.add((new_eq, new_e))  # type: ignore\n+    return combinations\n+    # TODO: turn off the caching for now. Sometimes it leads to incompleteness.\n+    # It is also too weak because if the stop_early is None, then the result\n+    # should apply for any other stop_early. Similarly, if stop_early is\n+    # _stop_early_for_geq0_or_leq0 and the result spans 0, then this should\n+    # apply even if we don't have a stop_early.\n+    #cache_key = (e, stop_early)\n+    #if (res := self.scope._bounds_cache.get(cache_key)) is not None: return res\n+    lb, ub = self._bounds_for_sorted_terms(e.scope, e._monomials_sorted, 0, stop_early)\n+    res = (int(lb) if lb > -np.inf else lb,\n+           int(ub) if ub < np.inf else ub)\n+    #self.scope._bounds_cache[cache_key] = res\n+    t, t_k = e[i]\n+    if len(t) == 0:  # A constant\n+      return (t_k, t_k)\n+    self.add_implicit_constraints(t)\n+    for (c_eq, c, c_s, t_s) in self.combine_term_with_existing(t, t_k,\n+                                                               only_smaller_than_t=True,\n+                                                               scope=scope):\n+      # `c =[eq] 0` AND `t*t_k*t_s + c*c_s` contains only terms smaller than t\n+      # AND c_s > 0.\n+      # rest = e[i:]*t_s + c*c_s` AND `rest_ub >= rest >= rest_lb`\n+      rest = _DimExpr._linear_combination(e, i, t_s,\n+                                          c._monomials_sorted, 0, c_s)\n+      rest_lb, rest_ub = self._bounds_for_sorted_terms(scope, rest, 0, None)\n+      if rest_ub < np.inf:\n+        if t_s > 0:\n+          ub = min(ub, int(np.floor(rest_ub / t_s)))\n+        else:\n+          lb = max(lb, int(np.ceil(rest_ub / t_s)))\n+      if rest_lb > - np.inf and c_eq == Comparator.EQ:\n+        if t_s > 0:\n+          lb = max(lb, int(np.ceil(rest_lb / t_s)))\n+          ub = min(ub, int(np.floor(rest_lb / t_s)))\n+\n+      if stop_early is not None and stop_early(lb, ub): return (lb, ub)\n+    if (m_a := t.to_atom()) is not None:\n+                                             op1._monomials_sorted, 0, t_k)\n+                                             op2._monomials_sorted, 0, t_k)\n+        like_max = (t_k > 0 if m_a.operation == _DimAtom.MAX else t_k < 0)\n+  def add_implicit_constraints(self: _DecisionByElimination, m: _DimMon):\n+    \"\"\"Adds the internal constraints for the monomial `m`.\"\"\"\n+    if m in self._processed_for_internal_constraints: return\n+    self._processed_for_internal_constraints.add(m)\n+    m_e = _DimExpr.from_monomial(m, 1, self.scope)  # m as a _DimExpr\n+    a = m.to_atom()\n+    if a is None:\n+      # This is a multiplication of atoms. Try to compute bounds based on\n+      # the bounds of the atoms.\n+      bounds = []\n+      for a, exp in m.items():\n+        a_l, a_u = self.bounds(_DimExpr.from_monomial(_DimMon.from_atom(a, 1),\n+                                                      1, self.scope), None)\n+        assert a_l <= a_u\n+        bounds.append((a_l ** exp, a_u ** exp))\n+\n+      candidate_bounds = [math.prod(atom_bounds)\n+                          for atom_bounds in itertools.product(*bounds)]\n+      m_l = min(*candidate_bounds)\n+      m_u = max(*candidate_bounds)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_e, m_l)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_u, m_e)\n+      return\n+\n+    # It is an atom, is it a variable?\n+    if (v := a.to_var()) is not None:\n+      self.combine_and_add_constraint(Comparator.GEQ, m_e, 1,\n+                                      debug_str=f\"{v} >= 1\")  # v >= 1\n+      return\n+\n+    if a.operation == _DimAtom.MOD:\n+      op1, op2 = a.operands\n+      op2_b_l, op2_b_u = self.bounds(op2, _stop_early_for_geq0_or_lt0)\n+      if op2_b_l > 0:  # positive divisor\n+        self.combine_and_add_constraint(Comparator.GEQ, m_e, 0)  # m >= 0\n+        self.combine_and_add_constraint(Comparator.GEQ, op2 - 1, m_e)  # m <= op2 - 1\n+        self.combine_and_add_constraint(Comparator.GEQ, op2_b_u - 1, m_e)\n+      elif op2_b_u < 0:  # negative divisor\n+        self.combine_and_add_constraint(Comparator.GEQ, m_e, op2 + 1)  # m >= op2 + 1\n+        self.combine_and_add_constraint(Comparator.GEQ, m_e, op2_b_l + 1)\n+        self.combine_and_add_constraint(Comparator.GEQ, 0, m_e)  # m <= 0\n+      return\n+\n+    if a.operation == _DimAtom.FLOORDIV:\n+      op1, op2 = a.operands\n+      (op1_l, op1_u) = self.bounds(op1, None)\n+      (op2_l, op2_u) = self.bounds(op2, None)\n+\n+      def math_floor_with_inf(a: float, b: float):  # math.floor, but aware of inf\n+        # When either a or b are infinite, the results represent the limit\n+        # of \"a // b\".\n+        assert b != 0\n+        if not np.isinf(b):  # divisor b is finite\n+          if not np.isinf(a):\n+            return math.floor(a / b)\n+          # a is infinite, b is finite\n+          return -np.inf if (a >= 0) != (b >= 0) else np.inf\n+        elif not np.isinf(a):  # dividend a is finite and divisor b is infinite\n+          return -1 if (a >= 0) != (b >= 0) else 0\n+        else:  # both dividend and divisor are infinite\n+          return -np.inf if (a >= 0) != (b >= 0) else np.inf\n+\n+      # Same reasoning as for multiplication: the bounds are among the cross-product\n+      # of the bounds.\n+      candidate_bounds = [math_floor_with_inf(op1_l, op2_l),\n+                          math_floor_with_inf(op1_l, op2_u),\n+                          math_floor_with_inf(op1_u, op2_l),\n+                          math_floor_with_inf(op1_u, op2_u)]\n+      m_l = min(*candidate_bounds)\n+      m_u = max(*candidate_bounds)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_e, m_l)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_u, m_e)\n+      if op2_l >= 0:\n+        if self.bounds(op1, _stop_early_for_geq0_or_lt0)[0] >= 0:\n+          self.combine_and_add_constraint(Comparator.GEQ, m_e, 0)\n+        mod_e = _DimExpr.from_operation(_DimAtom.MOD, op1, op2,\n+                                        scope=self.scope)\n+        combined = op2 * m_e + mod_e\n+        self.combine_and_add_constraint(Comparator.EQ, op1, combined)\n+      return\n+\n+    if a.operation == _DimAtom.MAX:\n+      op1, op2 = a.operands\n+      op1_b_l, op1_b_u = self.bounds(op1, None)\n+      op2_b_l, op2_b_u = self.bounds(op2, None)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_e, max(op1_b_l, op2_b_l))\n+      self.combine_and_add_constraint(Comparator.GEQ, max(op1_b_u, op2_b_u), m_e)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_e, op1)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_e, op2)\n+\n+    if a.operation == _DimAtom.MIN:\n+      op1, op2 = a.operands\n+      op1_b_l, op1_b_u = self.bounds(op1, None)\n+      op2_b_l, op2_b_u = self.bounds(op2, None)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_e, min(op1_b_l, op2_b_l))\n+      self.combine_and_add_constraint(Comparator.GEQ, min(op1_b_u, op2_b_u), m_e)\n+      self.combine_and_add_constraint(Comparator.GEQ, op1, m_e)\n+      self.combine_and_add_constraint(Comparator.GEQ, op2, m_e)\n",
            "whole_hunk": "@@ -31,8 +31,10 @@ from jax.experimental.export._shape_poly import (\n   SymbolicScope,\n   DimSize,\n   InconclusiveDimensionOperation,\n+  Comparator,\n )\n \n+def sgn(x): return 1 if x >= 0 else -1\n \n def geq_decision(e1: DimSize, e2: DimSize,\n                  cmp_str: Callable[[], str]) -> bool:\n@@ -55,7 +57,7 @@ def geq_decision(e1: DimSize, e2: DimSize,\n   else:\n     return int(e1) >= int(e2)\n   decision = _DecisionByElimination(scope)\n-  lb, ub = decision.bounds(e1 - e2, _stop_early_for_geq0)\n+  lb, ub = decision.bounds(e1 - e2, _stop_early_for_geq0_or_lt0)\n   if lb >= 0:\n     return True\n   if ub < 0:\n@@ -70,7 +72,7 @@ def geq_decision(e1: DimSize, e2: DimSize,\n \n _shape_poly._geq_decision = geq_decision\n \n-def _stop_early_for_geq0(lb, ub):\n+def _stop_early_for_geq0_or_lt0(lb, ub):\n   return lb >= 0 or ub < 0\n \n def bounds_decision(e: DimSize,\n@@ -98,6 +100,8 @@ class _DecisionByElimination:\n \n   if `sgn(m_c)*m_k < 0`\n     then `abs(m_c)*e <= e0`, hence, `UB(e) <= floor(UB(e0) / abs(m_c))`,\n+\n+  See the implementation in self.combine_term_with_existing.\n   \"\"\"\n   def __init__(self, scope: SymbolicScope):\n     self.scope = scope\n@@ -106,122 +110,161 @@ class _DecisionByElimination:\n     # the explicit constraints.\n     self._term_bounds: dict[_DimMon, tuple[float, float]] = {}\n     # The _expr_constraints represents a set of constraints that are not\n-    # just simple monomials. The set is represented as a mapping from a\n-    # monomial \"m\" to tuples (k, c) where \"c >= 0\" represents a constraint that\n-    # has \"m\" as the leading monomial with coefficient \"k\".\n-    self._expr_constraints: dict[_DimMon, set[tuple[int, _DimExpr]]] = collections.defaultdict(set)\n-\n-    # TODO: find a way to reuse the state reflecting the explicit constraints\n-    # We sort the constraints, so that the results of the heuristics do not\n-    # depend on the order in which the user writes the constraints.\n-    for c, c_str in sorted(scope._explicit_constraints,\n-                           key=lambda c: c[0]._monomials_sorted):\n-      self.add_constraint(c, 0, c_str)\n-\n-  def add_constraint(self,\n-                     e1: _DimExpr | int | float,\n-                     e2: _DimExpr | int | float,\n-                     constraint_str: str | None = None):\n+    # just simple terms. The set is represented as a mapping from a\n+    # term \"t\" to tuples (cmp, k, c) where \"c >= 0\" (if cmp is GEQ else \"c == 0\")\n+    # represents a constraint that has \"t\" as the leading term with coefficient \"k\".\n+    self._expr_constraints: dict[_DimMon, set[tuple[Comparator, int, _DimExpr]]] = collections.defaultdict(set)\n+\n+    # Process the explicit constraints in the order in which the user specifies\n+    # them. This is because the heuristics depend on the order in which the\n+    # constraints are processed, and this way we give the user a way to control\n+    # the result (albeit, for now, without a good feedback loop to understand\n+    # how the order matters for inequalities).\n+    for constr in scope._explicit_constraints:\n+      if constr.cmp == Comparator.GEQ:\n+        self.combine_and_add_constraint(constr.cmp, constr.diff, 0, constr.debug_str)\n+      else:\n+        # The equality constraints are not needed for inequality decisions,\n+        # because the LHS should always be rewritten in terms of the RHS.\n+        # In fact, adding them may break the assumption that if we eliminate\n+        # the leading term we end up with only smaller terms, because the LHS\n+        # may appear in the rest and may be rewritten to something larger.\n+        # However, we want to add the implicit constraints within.\n+        for m, _ in constr.diff.monomials():\n+          if m.degree == 0: continue\n+          self.add_implicit_constraints(m)\n+\n+  def combine_and_add_constraint(self,\n+                                 cmp: Comparator,\n+                                 e1: _DimExpr | int | float,\n+                                 e2: _DimExpr | int | float,\n+                                 debug_str: str | None = None):\n     \"\"\"Adds a constraint \"e1 >= e2\" to the internal state.\"\"\"\n     if isinstance(e1, float):\n-      if np.isinf(e1) and e1 >= 0: return\n+      if np.isinf(e1) and e1 >= 0 and cmp == Comparator.GEQ: return\n       assert e1 == np.floor(e1)\n       e1 = int(e1)\n     if isinstance(e2, float):\n-      if np.isinf(e2) and e2 <= 0: return\n+      if np.isinf(e2) and e2 <= 0 and cmp == Comparator.GEQ: return\n+      assert e2 == np.floor(e2)\n       e2 = int(e2)\n-    e = e1 if isinstance(e2, (int, float)) and e2 == 0 else e1 - e2\n-    if constraint_str is None:\n-      constraint_str = f\"{e1} >= {e2}\"\n+    e = e1 - e2\n+    if debug_str is None:\n+      debug_str = f\"{e1} >= {e2}\"\n     if (const := _DimExpr.to_constant(e)) is not None:\n       if const < 0:\n-        raise ValueError(f\"Unsatisfiable constraint: {constraint_str}\")\n+        raise ValueError(f\"Unsatisfiable constraint: {debug_str}\")\n       return\n     assert isinstance(e, _DimExpr)\n-    self._add_to_state(e, constraint_str)\n-    combinations = self._combine_with_existing_constraints(e, constraint_str)\n-    for a in combinations:\n-      self._add_to_state(a, f\"{a} >= 0\")\n-\n-\n-  def _combine_with_existing_constraints(self,\n-                                         e: _DimExpr,\n-                                         debug_str: str) -> set[_DimExpr]:\n-    # This combines `e` with those constraints already present. The resulting\n-    # constraints are not scanned for new internal constraints (because there\n-    # are no new monomials), but they are also not combined further.\n-    # TODO: this results in incompleteness, but it is probably a good\n-    # compromise.\n-    combinations: set[_DimExpr] = set()\n-    def acc_combination(e: _DimExpr | int):\n-      if (const := _DimExpr.to_constant(e)) is not None:\n-        if const < 0:\n-          raise ValueError(f\"Unsatisfiable constraints: {debug_str}\")\n-      else:\n-        combinations.add(e)  # type: ignore\n-\n-    # First combine with the existing monomial constraints\n-    for e_m, e_c in e.monomials():\n-      if e_m.degree == 0: continue\n-      m_lb, m_ub = self._term_bounds.get(e_m, (-np.inf, np.inf))\n-      if e_c > 0:\n-        if m_ub < np.inf:\n-          e_minus_m = _DimExpr._linear_combination(e._monomials_sorted, 0, 1,\n-                                                   [(e_m, e_c)], 0, -1)\n-          e_minus_m_ub = _DimExpr._linear_combination(e_minus_m, 0, 1,\n-                                                      [(_DimMon(), 1)], 0, e_c * int(m_ub))\n-          acc_combination(_DimExpr(e_minus_m_ub, e.scope))\n-      else:\n-        if m_lb > -np.inf:\n-          e_minus_m = _DimExpr._linear_combination(e._monomials_sorted, 0, 1,\n-                                                   [(e_m, e_c)], 0, -1)\n-          e_minus_m_lb = _DimExpr._linear_combination(e_minus_m, 0, 1,\n-                                                      [(_DimMon(), 1)], 0, e_c * int(m_lb))\n-          acc_combination(_DimExpr(e_minus_m_lb, e.scope))\n-\n-    for prev_constraints in self._expr_constraints.values():\n-      for _, prev in prev_constraints:\n-        # Compose \"e\" with \"prev\" if they have one monomial with different\n-        # signs\n-        prev_coeffs = dict(prev._monomials_sorted)\n-        for e_m, e_c in e.monomials():\n-          if e_m.degree == 0: continue\n-          prev_c = prev_coeffs.get(e_m)\n-          if prev_c is not None and prev_c * e_c < 0:\n-            new_constraint = _DimExpr(\n-                _DimExpr._linear_combination(e._monomials_sorted, 0, abs(prev_c),\n-                                             prev._monomials_sorted, 0, abs(e_c)),\n-                e.scope)\n-            acc_combination(new_constraint)\n-            break\n-\n-    return combinations\n-\n-  def _add_to_state(self, e: _DimExpr,\n-                    constraint_str: str):\n+    # TODO: we only really need to add the implicit constraints now, else\n+    # we may have to consider combining e with itself below (not harmful, just\n+    # wasteful.\n+    self.add_to_state(cmp, e, debug_str)\n+    geq_combinations = self.combine_constraint_with_existing(cmp, e, debug_str)\n+    for cmp, a in geq_combinations:\n+      self.add_to_state(cmp, a, f\"{a} >= 0\")\n+\n+  def add_to_state(self,\n+                   cmp: Comparator,\n+                   e: _DimExpr,\n+                   debug_str: str):\n     \"\"\"Updates the internal state to reflect \"e >= 0\". \"\"\"\n     assert _DimExpr.to_constant(e) is None\n     for m, m_c in e.monomials():\n       if m.degree == 0: continue\n-      _add_internal_constraints(self, m, e.scope)\n+      self.add_implicit_constraints(m)\n \n     if (mon_factors := e.to_single_term()) is not None:\n-      n, mon_c, mon = mon_factors\n-      bounds = self._term_bounds.get(mon, (- np.inf, np.inf))\n-      if mon_c > 0:\n-        mon_ge = int(np.ceil(- n / mon_c))\n-        new_bounds = (max(mon_ge, bounds[0]), bounds[1])\n-      else:\n-        le = int(np.floor(-n / mon_c))\n-        new_bounds = (bounds[0], min(le, bounds[1]))\n-      if new_bounds[0] > new_bounds[1]:\n-        raise ValueError(f\"Unsatisfiable constraint: {constraint_str}\")\n+      n, mon_c, mon = mon_factors  # n + mon * mon_c [== | >=] 0\n+      lb, ub = self._term_bounds.get(mon, (- np.inf, np.inf))\n+      if cmp == Comparator.EQ:\n+        # n + mon_c * mon == 0  ->  mon == - n // mon_c\n+        if n % mon_c:\n+          raise ValueError(f\"Unsatisfiable constraint: {debug_str}\")\n+        mon_val = - (n // mon_c)\n+        lb = max(lb, mon_val)\n+        ub = min(ub, mon_val)\n+      else:  # GEQ\n+        if mon_c > 0:\n+          lb = max(lb, int(np.ceil(- n / mon_c)))\n+        else:\n+          ub = min(ub, int(np.floor(- n / mon_c)))\n+      if lb > ub:\n+        raise ValueError(f\"Unsatisfiable constraint: {debug_str}\")\n \n-      self._term_bounds[mon] = new_bounds\n+      self._term_bounds[mon] = (lb, ub)\n       return\n \n-    lead_m, lead_m_c = e.leading_term\n-    self._expr_constraints[lead_m].add((lead_m_c, e))\n+    lead_t, lead_t_k = e.leading_term\n+    self._expr_constraints[lead_t].add((cmp, lead_t_k, e))\n+\n+  def combine_term_with_existing(self, t: _DimMon, t_k: int, *,\n+                                 scope: _shape_poly.SymbolicScope,\n+                                 only_smaller_than_t=True,\n+                                 ) -> Sequence[tuple[Comparator,\n+                                                     _DimExpr,\n+                                                     int,\n+                                                     int]]:\n+    \"\"\"\n+    Combine a term with existing constraints.\n+    For input (t, t_k) the tuple (c_eq, c, c_s, t_s) is among the returned\n+    tuples if there exists a constraint `c =[c_eq] 0` that can be combined\n+    with `t*t_k` to eliminate `t`.\n+\n+      * `c =[c_eq] 0`\n+      * The term `comb = t*t_k*t_s + c*c_s` does not contain `t`, and if\n+        `only_smaller_than_t` then `comb` contains only terms structurally\n+         smaller than `t`.\n+      * `c_s > 0`\n+    \"\"\"\n+    # TODO: maybe a generator is useful here instead of materializing the list\n+    acc: list[tuple[Comparator, _DimExpr, int, int]] = []\n+    # First combine with the existing term constraints\n+    t_lb, t_ub = self._term_bounds.get(t, (-np.inf, np.inf))\n+    if t_lb == t_ub:\n+      acc.append((Comparator.EQ, _DimExpr(((t, 1),), scope) - int(t_lb),\n+                  abs(t_k), - sgn(t_k)))\n+    else:\n+      if t_lb > -np.inf:\n+        acc.append((Comparator.GEQ, _DimExpr(((t, 1),), scope) - int(t_lb),\n+                    abs(t_k), - sgn(t_k)))\n+      if t_ub < np.inf:\n+        acc.append((Comparator.GEQ, _DimExpr(((t, -1),), scope) + int(t_ub),\n+                    abs(t_k), sgn(t_k)))\n+\n+    for prev_constraint in ([self._expr_constraints[t]] if only_smaller_than_t\n+                            else self._expr_constraints.values()):\n+      for c_eq, _, c in prev_constraint:\n+        # TODO: optimize this dict()\n+        tc_k = dict(c._monomials_sorted).get(t)\n+        if tc_k is not None:\n+          # c =[c_eq] 0 AND t*tc_k appears in c.\n+          c_s = abs(t_k)\n+          c_t = - tc_k * sgn(t_k)\n+          acc.append((c_eq, c, c_s, c_t))\n+    return acc\n+\n+  def combine_constraint_with_existing(self,\n+                                       eq: Comparator,\n+                                       e: _DimExpr,\n+                                       debug_str: str) -> set[tuple[Comparator, _DimExpr]]:\n+    combinations: set[tuple[Comparator, _DimExpr]] = set()\n+    for t, t_k in e._monomials_sorted:\n+      if t.degree == 0: continue\n+      for (c_eq, c, c_s, t_s) in self.combine_term_with_existing(t, t_k,\n+                                                                 only_smaller_than_t=False,\n+                                                                 scope=e.scope):\n+        # c =[c_eq] 0 AND c_s > 0 AND t*t_k*t_s + c*c_s does not contain t\n+        if t_s > 0 or eq == Comparator.EQ:\n+          new_eq = Comparator.EQ if (eq == c_eq == Comparator.EQ) else Comparator.GEQ\n+          new_e = e * t_s + c * c_s\n+          if (const := _DimExpr.to_constant(new_e)) is not None:\n+            if ((new_eq == Comparator.GEQ and const < 0) or\n+                (new_eq == Comparator.EQ and const != 0)):\n+              raise ValueError(f\"Unsatisfiable constraints: {debug_str}\")\n+          else:\n+            combinations.add((new_eq, new_e))  # type: ignore\n+    return combinations\n \n   def bounds(self, e: DimSize,\n              stop_early: Callable[[float, float], bool] | None\n@@ -233,10 +276,17 @@ class _DecisionByElimination:\n     if (const := _DimExpr.to_constant(e)) is not None:\n       return (const, const)\n     assert isinstance(e, _DimExpr)\n-    cache_key = (e, stop_early)\n-    if (res := self.scope._bounds_cache.get(cache_key)) is not None: return res\n-    res = self._bounds_for_sorted_terms(e.scope, e._monomials_sorted, 0, stop_early)\n-    self.scope._bounds_cache[cache_key] = res\n+    # TODO: turn off the caching for now. Sometimes it leads to incompleteness.\n+    # It is also too weak because if the stop_early is None, then the result\n+    # should apply for any other stop_early. Similarly, if stop_early is\n+    # _stop_early_for_geq0_or_leq0 and the result spans 0, then this should\n+    # apply even if we don't have a stop_early.\n+    #cache_key = (e, stop_early)\n+    #if (res := self.scope._bounds_cache.get(cache_key)) is not None: return res\n+    lb, ub = self._bounds_for_sorted_terms(e.scope, e._monomials_sorted, 0, stop_early)\n+    res = (int(lb) if lb > -np.inf else lb,\n+           int(ub) if ub < np.inf else ub)\n+    #self.scope._bounds_cache[cache_key] = res\n     return res\n \n   def _bounds_for_sorted_terms(self,\n@@ -250,59 +300,51 @@ class _DecisionByElimination:\n     \"\"\"\n     if i >= len(e): return (0, 0)\n \n-    m, m_c = e[i]\n-    if len(m) == 0:  # A constant\n+    t, t_k = e[i]\n+    if len(t) == 0:  # A constant\n       assert i == len(e) - 1  # Must be last\n-      return (m_c, m_c)\n+      return (t_k, t_k)\n \n-    _add_internal_constraints(self, m, scope)\n+    self.add_implicit_constraints(t)\n     lb = -np.inf\n     ub = np.inf\n \n-    # Look among the term bounds\n-    if m in self._term_bounds:\n-      m_lb, m_ub = self._term_bounds.get(m, (- np.inf, np.inf))\n-      rest_lb, rest_ub = self._bounds_for_sorted_terms(scope, e, i + 1, None)\n-      if m_c > 0:\n-        lb = max(lb, m_c * m_lb + rest_lb)\n-        ub = min(ub, m_c * m_ub + rest_ub)\n-      else:\n-        lb = max(lb, m_c * m_ub + rest_lb)\n-        ub = min(ub, m_c * m_lb + rest_ub)\n-\n-      if stop_early is not None and stop_early(lb, ub): return (lb, ub)\n+    for (c_eq, c, c_s, t_s) in self.combine_term_with_existing(t, t_k,\n+                                                               only_smaller_than_t=True,\n+                                                               scope=scope):\n+      # `c =[eq] 0` AND `t*t_k*t_s + c*c_s` contains only terms smaller than t\n+      # AND c_s > 0.\n+      # rest = e[i:]*t_s + c*c_s` AND `rest_ub >= rest >= rest_lb`\n+      rest = _DimExpr._linear_combination(e, i, t_s,\n+                                          c._monomials_sorted, 0, c_s)\n+      rest_lb, rest_ub = self._bounds_for_sorted_terms(scope, rest, 0, None)\n+      if rest_ub < np.inf:\n+        if t_s > 0:\n+          ub = min(ub, int(np.floor(rest_ub / t_s)))\n+        else:\n+          lb = max(lb, int(np.ceil(rest_ub / t_s)))\n \n-    # Now look through the _expr_constraints\n-    if m in self._expr_constraints:\n-      for m_k, c in self._expr_constraints[m]:\n-        # A complex expression. See comments from top of class.\n-        sgn_m_k = 1 if m_k > 0 else -1\n-        abs_m_k = m_k * sgn_m_k\n-        # The recursive call has a smaller leading monomial, because we are only\n-        # looking at the tail of e, and in c the largest monomial is m, and the\n-        # merging will cancel the m.\n-        rest = _DimExpr._linear_combination(e, i, abs_m_k,\n-                                            c._monomials_sorted, 0, - sgn_m_k * m_c)\n-        rest_lb, rest_ub = self._bounds_for_sorted_terms(scope, rest, 0, None)\n-        if m_c / m_k > 0:\n-          lb = max(lb, np.ceil(rest_lb / abs_m_k))\n+      if rest_lb > - np.inf and c_eq == Comparator.EQ:\n+        if t_s > 0:\n+          lb = max(lb, int(np.ceil(rest_lb / t_s)))\n         else:\n-          ub = min(ub, np.floor(rest_ub / abs_m_k))\n-        if stop_early is not None and stop_early(lb, ub): return (lb, ub)\n+          ub = min(ub, int(np.floor(rest_lb / t_s)))\n+\n+      if stop_early is not None and stop_early(lb, ub): return (lb, ub)\n \n     # Now look for special rules for atoms\n-    if (m_a := m.to_atom()) is not None:\n+    if (m_a := t.to_atom()) is not None:\n       if m_a.operation in [_DimAtom.MAX, _DimAtom.MIN]:\n         # m_c*MAX(op1, op2) + rest_e >= max(m_c * op1 + rest_e, m_c * op2 + rest_e)\n         #   if m_c > 0. Similar rules for when m_c < 0 and for MIN.\n         op1, op2 = m_a.operands\n         rest1 = _DimExpr._linear_combination(e, i + 1, 1,\n-                                             op1._monomials_sorted, 0, m_c)\n+                                             op1._monomials_sorted, 0, t_k)\n         rest2 = _DimExpr._linear_combination(e, i + 1, 1,\n-                                             op2._monomials_sorted, 0, m_c)\n+                                             op2._monomials_sorted, 0, t_k)\n         rest1_lb, rest1_ub = self._bounds_for_sorted_terms(scope, rest1, 0, None)\n         rest2_lb, rest2_ub = self._bounds_for_sorted_terms(scope, rest2, 0, None)\n-        like_max = (m_c > 0 if m_a.operation == _DimAtom.MAX else m_c < 0)\n+        like_max = (t_k > 0 if m_a.operation == _DimAtom.MAX else t_k < 0)\n         if like_max:\n           lb = max(lb, max(rest1_lb, rest2_lb))\n           ub = min(ub, max(rest1_ub, rest2_ub))\n@@ -313,101 +355,101 @@ class _DecisionByElimination:\n \n     return lb, ub\n \n-def _add_internal_constraints(decision: _DecisionByElimination, m: _DimMon, scope: SymbolicScope):\n-  \"\"\"Adds the internal constraints for the monomial `m`.\"\"\"\n-  if m in decision._processed_for_internal_constraints: return\n-  decision._processed_for_internal_constraints.add(m)\n-  m_e = _DimExpr.from_monomial(m, 1, scope)  # m as a _DimExpr\n-  a = m.to_atom()\n-  if a is None:\n-    # This is a multiplication of atoms. Try to compute bounds based on\n-    # the bounds of the atoms.\n-    bounds = []\n-    for a, exp in m.items():\n-      a_l, a_u = decision.bounds(_DimExpr.from_monomial(_DimMon.from_atom(a, 1),\n-                                                        1, scope), None)\n-      assert a_l <= a_u\n-      bounds.append((a_l ** exp, a_u ** exp))\n-\n-    candidate_bounds = [math.prod(atom_bounds)\n-                        for atom_bounds in itertools.product(*bounds)]\n-    m_l = min(*candidate_bounds)\n-    m_u = max(*candidate_bounds)\n-    decision.add_constraint(m_e, m_l)\n-    decision.add_constraint(m_u, m_e)\n-    return\n-\n-  # It is an atom, is it a variable?\n-  if (v := a.to_var()) is not None:\n-    decision.add_constraint(m_e, 1)  # v >= 1\n-    return\n-\n-  if a.operation == _DimAtom.MOD:\n-    op1, op2 = a.operands\n-    op2_b_l, op2_b_u = decision.bounds(op2, _stop_early_for_geq0)\n-    if op2_b_l > 0:  # positive divisor\n-      decision.add_constraint(m_e, 0)  # m >= 0\n-      decision.add_constraint(op2 - 1, m_e)  # m <= op2 - 1\n-      decision.add_constraint(op2_b_u - 1, m_e)\n-    elif op2_b_u < 0:  # negative divisor\n-      decision.add_constraint(m_e, op2 + 1)  # m >= op2 + 1\n-      decision.add_constraint(m_e, op2_b_l + 1)\n-      decision.add_constraint(0, m_e)  # m <= 0\n-    return\n-\n-  if a.operation == _DimAtom.FLOORDIV:\n-    op1, op2 = a.operands\n-    (op1_l, op1_u) = decision.bounds(op1, None)\n-    (op2_l, op2_u) = decision.bounds(op2, None)\n-\n-    def math_floor_with_inf(a: float, b: float):  # math.floor, but aware of inf\n-      # When either a or b are infinite, the results represent the limit\n-      # of \"a // b\".\n-      assert b != 0\n-      if not np.isinf(b):  # divisor b is finite\n-        if not np.isinf(a):\n-          return math.floor(a / b)\n-        # a is infinite, b is finite\n-        return -np.inf if (a >= 0) != (b >= 0) else np.inf\n-      elif not np.isinf(a):  # dividend a is finite and divisor b is infinite\n-        return -1 if (a >= 0) != (b >= 0) else 0\n-      else:  # both dividend and divisor are infinite\n-        return -np.inf if (a >= 0) != (b >= 0) else np.inf\n-\n-    # Same reasoning as for multiplication: the bounds are among the cross-product\n-    # of the bounds.\n-    candidate_bounds = [math_floor_with_inf(op1_l, op2_l),\n-                        math_floor_with_inf(op1_l, op2_u),\n-                        math_floor_with_inf(op1_u, op2_l),\n-                        math_floor_with_inf(op1_u, op2_u)]\n-    m_l = min(*candidate_bounds)\n-    m_u = max(*candidate_bounds)\n-    decision.add_constraint(m_e, m_l)\n-    decision.add_constraint(m_u, m_e)\n-    if op2_l >= 0:\n-      if decision.bounds(op1, _stop_early_for_geq0)[0] >= 0:\n-        decision.add_constraint(m_e, 0)\n-      mod_e = _DimExpr.from_operation(_DimAtom.MOD, op1, op2,\n-                                      scope=scope)\n-      combined = op2 * m_e + mod_e\n-      decision.add_constraint(op1, combined)\n-      decision.add_constraint(combined, op1)\n-    return\n-\n-  if a.operation == _DimAtom.MAX:\n-    op1, op2 = a.operands\n-    op1_b_l, op1_b_u = decision.bounds(op1, None)\n-    op2_b_l, op2_b_u = decision.bounds(op2, None)\n-    decision.add_constraint(m_e, max(op1_b_l, op2_b_l))\n-    decision.add_constraint(max(op1_b_u, op2_b_u), m_e)\n-    decision.add_constraint(m_e, op1)\n-    decision.add_constraint(m_e, op2)\n-\n-  if a.operation == _DimAtom.MIN:\n-    op1, op2 = a.operands\n-    op1_b_l, op1_b_u = decision.bounds(op1, None)\n-    op2_b_l, op2_b_u = decision.bounds(op2, None)\n-    decision.add_constraint(m_e, min(op1_b_l, op2_b_l))\n-    decision.add_constraint(min(op1_b_u, op2_b_u), m_e)\n-    decision.add_constraint(op1, m_e)\n-    decision.add_constraint(op2, m_e)\n+  def add_implicit_constraints(self: _DecisionByElimination, m: _DimMon):\n+    \"\"\"Adds the internal constraints for the monomial `m`.\"\"\"\n+    if m in self._processed_for_internal_constraints: return\n+    self._processed_for_internal_constraints.add(m)\n+    m_e = _DimExpr.from_monomial(m, 1, self.scope)  # m as a _DimExpr\n+    a = m.to_atom()\n+    if a is None:\n+      # This is a multiplication of atoms. Try to compute bounds based on\n+      # the bounds of the atoms.\n+      bounds = []\n+      for a, exp in m.items():\n+        a_l, a_u = self.bounds(_DimExpr.from_monomial(_DimMon.from_atom(a, 1),\n+                                                      1, self.scope), None)\n+        assert a_l <= a_u\n+        bounds.append((a_l ** exp, a_u ** exp))\n+\n+      candidate_bounds = [math.prod(atom_bounds)\n+                          for atom_bounds in itertools.product(*bounds)]\n+      m_l = min(*candidate_bounds)\n+      m_u = max(*candidate_bounds)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_e, m_l)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_u, m_e)\n+      return\n+\n+    # It is an atom, is it a variable?\n+    if (v := a.to_var()) is not None:\n+      self.combine_and_add_constraint(Comparator.GEQ, m_e, 1,\n+                                      debug_str=f\"{v} >= 1\")  # v >= 1\n+      return\n+\n+    if a.operation == _DimAtom.MOD:\n+      op1, op2 = a.operands\n+      op2_b_l, op2_b_u = self.bounds(op2, _stop_early_for_geq0_or_lt0)\n+      if op2_b_l > 0:  # positive divisor\n+        self.combine_and_add_constraint(Comparator.GEQ, m_e, 0)  # m >= 0\n+        self.combine_and_add_constraint(Comparator.GEQ, op2 - 1, m_e)  # m <= op2 - 1\n+        self.combine_and_add_constraint(Comparator.GEQ, op2_b_u - 1, m_e)\n+      elif op2_b_u < 0:  # negative divisor\n+        self.combine_and_add_constraint(Comparator.GEQ, m_e, op2 + 1)  # m >= op2 + 1\n+        self.combine_and_add_constraint(Comparator.GEQ, m_e, op2_b_l + 1)\n+        self.combine_and_add_constraint(Comparator.GEQ, 0, m_e)  # m <= 0\n+      return\n+\n+    if a.operation == _DimAtom.FLOORDIV:\n+      op1, op2 = a.operands\n+      (op1_l, op1_u) = self.bounds(op1, None)\n+      (op2_l, op2_u) = self.bounds(op2, None)\n+\n+      def math_floor_with_inf(a: float, b: float):  # math.floor, but aware of inf\n+        # When either a or b are infinite, the results represent the limit\n+        # of \"a // b\".\n+        assert b != 0\n+        if not np.isinf(b):  # divisor b is finite\n+          if not np.isinf(a):\n+            return math.floor(a / b)\n+          # a is infinite, b is finite\n+          return -np.inf if (a >= 0) != (b >= 0) else np.inf\n+        elif not np.isinf(a):  # dividend a is finite and divisor b is infinite\n+          return -1 if (a >= 0) != (b >= 0) else 0\n+        else:  # both dividend and divisor are infinite\n+          return -np.inf if (a >= 0) != (b >= 0) else np.inf\n+\n+      # Same reasoning as for multiplication: the bounds are among the cross-product\n+      # of the bounds.\n+      candidate_bounds = [math_floor_with_inf(op1_l, op2_l),\n+                          math_floor_with_inf(op1_l, op2_u),\n+                          math_floor_with_inf(op1_u, op2_l),\n+                          math_floor_with_inf(op1_u, op2_u)]\n+      m_l = min(*candidate_bounds)\n+      m_u = max(*candidate_bounds)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_e, m_l)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_u, m_e)\n+      if op2_l >= 0:\n+        if self.bounds(op1, _stop_early_for_geq0_or_lt0)[0] >= 0:\n+          self.combine_and_add_constraint(Comparator.GEQ, m_e, 0)\n+        mod_e = _DimExpr.from_operation(_DimAtom.MOD, op1, op2,\n+                                        scope=self.scope)\n+        combined = op2 * m_e + mod_e\n+        self.combine_and_add_constraint(Comparator.EQ, op1, combined)\n+      return\n+\n+    if a.operation == _DimAtom.MAX:\n+      op1, op2 = a.operands\n+      op1_b_l, op1_b_u = self.bounds(op1, None)\n+      op2_b_l, op2_b_u = self.bounds(op2, None)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_e, max(op1_b_l, op2_b_l))\n+      self.combine_and_add_constraint(Comparator.GEQ, max(op1_b_u, op2_b_u), m_e)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_e, op1)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_e, op2)\n+\n+    if a.operation == _DimAtom.MIN:\n+      op1, op2 = a.operands\n+      op1_b_l, op1_b_u = self.bounds(op1, None)\n+      op2_b_l, op2_b_u = self.bounds(op2, None)\n+      self.combine_and_add_constraint(Comparator.GEQ, m_e, min(op1_b_l, op2_b_l))\n+      self.combine_and_add_constraint(Comparator.GEQ, min(op1_b_u, op2_b_u), m_e)\n+      self.combine_and_add_constraint(Comparator.GEQ, op1, m_e)\n+      self.combine_and_add_constraint(Comparator.GEQ, op2, m_e)\n"
        },
        {
            "name": "shape_poly_test.py",
            "path": "tests/shape_poly_test.py",
            "patches": [
                {
                    "old_start": 685,
                    "old_length": 6,
                    "new_start": 685,
                    "new_length": 75,
                    "hunk": "@@ -685,6 +685,75 @@ class DimExprTest(jtu.JaxTestCase):\n       self.sampled_assertion(remainder, lambda *args: divmod(*args)[1],\n                              dividend, divisor)\n \n+\n+  def test_unit_combine_term_with_constraints(self):\n+    a, b, c, d, e = shape_poly.symbolic_shape(\"a, b, c, d, e\",\n+        constraints=[\n+           \"a >= 4\",\n+           \"b <= 5\",\n+           \"c + 3*d >= 10\", \"c - 2*d >= -4\",  # -> 5c >= 8 -> c >= 2\n+        ])\n+    scope = a.scope\n+    def _m(e: shape_poly._DimExpr) -> shape_poly._DimMon:\n+      return e.to_monomial()\n+    Comparator = shape_poly.Comparator\n+    decision = shape_poly_decision._DecisionByElimination(scope)\n+\n+    self.assertSetEqual(\n+        set(),\n+        set(decision.combine_term_with_existing(_m(e), 2, scope=scope)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, a - 4, 2, -1)},\n+        set(decision.combine_term_with_existing(_m(a), 2, scope=scope)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, a - 4, 2, 1)},\n+        set(decision.combine_term_with_existing(_m(a), -2, scope=scope)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, b - 1, 2, -1),\n+         (Comparator.GEQ, -b + 5, 2, 1)},\n+        set(decision.combine_term_with_existing(_m(b), 2, scope=scope)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, b - 1, 2, 1),\n+         (Comparator.GEQ, -b + 5, 2, -1)},\n+        set(decision.combine_term_with_existing(_m(b), -2, scope=scope)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, c - 2, 2, -1),\n+         (Comparator.GEQ, -2*d + c + 4, 2, -1),\n+         (Comparator.GEQ, 3*d + c - 10, 2, -1)},\n+        set(decision.combine_term_with_existing(_m(c), 2, scope=scope,\n+                                                only_smaller_than_t=False)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, c - 2, 2, 1),\n+         (Comparator.GEQ, -2*d + c + 4, 2, 1),\n+         (Comparator.GEQ, 3*d + c - 10, 2, 1)},\n+        set(decision.combine_term_with_existing(_m(c), -2, scope=scope,\n+                                                only_smaller_than_t=False)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, c - 2, 2, -1)},\n+        set(decision.combine_term_with_existing(_m(c), 2, scope=scope,\n+                                                only_smaller_than_t=True)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, d - 1, 2, -1),\n+         (Comparator.GEQ, -2*d + c + 4, 2, 2),\n+         (Comparator.GEQ, 3*d + c - 10, 2, -3)},\n+        set(decision.combine_term_with_existing(_m(d), 2, scope=scope,\n+                                                only_smaller_than_t=False)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, d - 1, 2, -1),\n+         (Comparator.GEQ, -2*d + c + 4, 2, 2),\n+         (Comparator.GEQ, 3*d + c - 10, 2, -3)},\n+        set(decision.combine_term_with_existing(_m(d), 2, scope=scope,\n+                                                only_smaller_than_t=True)))\n+\n   def test_non_negative_dim(self):\n     a, = shape_poly.symbolic_shape(\"a,\")\n \n"
                },
                {
                    "old_start": 725,
                    "old_length": 13,
                    "new_start": 794,
                    "new_length": 13,
                    "hunk": "@@ -725,13 +794,13 @@ class DimExprTest(jtu.JaxTestCase):\n     self.sampled_assertion(core.max_dim((a - 4) // 2 + 1, 0),\n                            core.stride_dim, a, 4, 2)\n \n-  def test_constraints_basic(self):\n+  def test_constraints_ge_basic(self):\n     a, b = shape_poly.symbolic_shape(\"a, b\",\n                                      constraints=(\"a >= 5\", \"b <= 16\"))\n     self.assertEqual(_bounds(a), (5, np.inf))\n     self.assertEqual(_bounds(b), (1, 16))\n \n-  def test_constraints_trivial(self):\n+  def test_constraints_ge_trivial(self):\n     a, = shape_poly.symbolic_shape(\"a\",\n                                    # Trivial, is dropped\n                                    constraints=(\"a <= a + 1\",))\n"
                },
                {
                    "old_start": 748,
                    "old_length": 7,
                    "new_start": 817,
                    "new_length": 34,
                    "hunk": "@@ -748,7 +817,34 @@ class DimExprTest(jtu.JaxTestCase):\n                                        constraints=(\"a <= 0\",))\n       a >= b\n \n-  def test_constraints_a_minus_4d(self):\n+    with self.assertRaisesRegex(ValueError,\n+                                \"Unsatisfiable.*a <= 0\"):\n+      a, b = shape_poly.symbolic_shape(\"a, b\",\n+                                       # Contradicts the default a >= 1\n+                                       constraints=(\"a <= 0\",))\n+      a >= b\n+\n+  def test_constraints_ge_smallest_first(self):\n+    # Since we process the smaller constraints first, they take effect for the\n+    # processing of the remaining constraints.\n+    a, = shape_poly.symbolic_shape(\"a\",\n+                                   # max(a, 2) = a\n+                                   constraints=(\"a >= 4\", \"max(a, 2) >= 6\"))\n+    self.assertEqual(core.max_dim(a, 2), a)\n+    self.assertEqual(_bounds(a), (6, np.inf))\n+\n+  def test_constraints_ge_smallest_first_caching(self):\n+    # When we process the constraint with max(a, 5) we will compute the bounds\n+    # of (a, 5) and at that time we will cache that \"-1 <= a - 5\". We should\n+    # not cache that, because later we learn that a >= 5\n+    a, = shape_poly.symbolic_shape(\"a\",\n+                                   # max(a, 2) = a\n+                                   constraints=(\"a >= 4\", \"max(a, 5) >= 5\",\n+                                                \"max(a, 2) >= 5\"))\n+    self.assertEqual(core.max_dim(a, 2), a)\n+    self.assertEqual(core.max_dim(a, 5), a)\n+\n+  def test_constraints_ge_a_minus_4d(self):\n     # simulates d = div(a, 4) and m = mod(a, 4)\n     assumptions = [\"a >= 4*d + m \",\n                    \"a <= 4*d + m\",\n"
                },
                {
                    "old_start": 764,
                    "old_length": 16,
                    "new_start": 860,
                    "new_length": 29,
                    "hunk": "@@ -764,16 +860,29 @@ class DimExprTest(jtu.JaxTestCase):\n   def test_constraints_errors(self):\n     with self.assertRaisesRegex(ValueError,\n                                 \"The symbolic constraints should be a sequence of strings\"):\n-      _ = shape_poly.symbolic_shape(\"a, b\",\n+      _ = shape_poly.symbolic_shape(\"a\",\n                                     constraints=\"a <= a - 1\")\n \n     with self.assertRaisesRegex(ValueError,\n-                                \"Constraint parsing error: must contain one of '>=' or '<='\"):\n-      _ = shape_poly.symbolic_shape(\"a, b\",\n+                                \"Constraint parsing error: must contain one of '==' or '>=' or '<='\"):\n+      _ = shape_poly.symbolic_shape(\"a\",\n+                                    constraints=(\"a != 0\",))\n+\n+    with self.assertRaisesRegex(ValueError,\n+                                re.escape(\"Unsatisfiable explicit constraint: a == a + 1\")):\n+      _ = shape_poly.symbolic_shape(\"a\",\n                                     # Contradicts the default a >= 1\n-                                    constraints=(\"a == 0\",))\n+                                    constraints=(\"a == a + 1\",))\n \n-  def test_constraints_monomial(self):\n+    with self.assertRaisesRegex(ValueError,\n+                                re.escape(\"Unsatisfiable constraint: a >= 1\")):\n+      a, = shape_poly.symbolic_shape(\"a\",\n+                                     # Contradicts the default a >= 1\n+                                     constraints=(\"a == 0\",))\n+      # We detect unsatisfiablity once we try to decide inequalities\n+      _ = a >= -1\n+\n+  def test_constraints_ge_monomial(self):\n     a, b = shape_poly.symbolic_shape(\n         \"a, b\",\n         constraints=(\"max(a, b) >= 8\", \"min(a, b) <= 2\",\n"
                },
                {
                    "old_start": 786,
                    "old_length": 7,
                    "new_start": 895,
                    "new_length": 7,
                    "hunk": "@@ -786,7 +895,7 @@ class DimExprTest(jtu.JaxTestCase):\n     self.assertGreaterEqual(a % b, 1)\n     self.assertGreaterEqual(core.max_dim(a, b) % 4, 2)\n \n-  def test_constraints_monomial_derived(self):\n+  def test_constraints_ge_monomial_derived(self):\n     a, b = shape_poly.symbolic_shape(\n         \"a, b\",\n         constraints=(\"a >= 4\", \"3 >= b\"))\n"
                },
                {
                    "old_start": 795,
                    "old_length": 12,
                    "new_start": 904,
                    "new_length": 12,
                    "hunk": "@@ -795,12 +904,12 @@ class DimExprTest(jtu.JaxTestCase):\n     self.assertEqual(core.min_dim(a, b), b)\n     self.assertGreaterEqual(a // 3, 1)\n \n-  def test_constraints_not_monomial(self):\n+  def test_constraints_ge_not_monomial(self):\n     a, b = shape_poly.symbolic_shape(\"a, b\",\n-                                        constraints=(\"a >= b\",))\n+                                     constraints=(\"a >= b\",))\n     self.assertGreaterEqual(a, b)\n \n-  def test_constraints_complex(self):\n+  def test_constraints_ge_complex(self):\n     a, b, c = shape_poly.symbolic_shape(\n         \"a, b, c\",\n         constraints=(\"a + 2 <= b\", \"b <= a + 5\", \"a + b >= c\"))\n"
                },
                {
                    "old_start": 809,
                    "old_length": 7,
                    "new_start": 918,
                    "new_length": 7,
                    "hunk": "@@ -809,7 +918,7 @@ class DimExprTest(jtu.JaxTestCase):\n     self.assertEqual(_bounds(b - a - 7), (-5, -2))\n     self.assertEqual(_bounds(c - 2*a - 5), (-np.inf, 0))\n \n-  def test_constraints_fractional(self):\n+  def test_constraints_ge_fractional(self):\n     a, = shape_poly.symbolic_shape(\"a\",\n                                    constraints=(\"2 * a >=  5\", \"3 * a <= 10\",))\n     self.assertEqual(_bounds(5*a - 2), (13, 13))\n"
                },
                {
                    "old_start": 825,
                    "old_length": 7,
                    "new_start": 934,
                    "new_length": 7,
                    "hunk": "@@ -825,7 +934,7 @@ class DimExprTest(jtu.JaxTestCase):\n           dict(constraint=\"2*a + -3*b >= 10\", exp=\"-1*a + 2*b\", bounds=(-np.inf, np.inf)),\n       ]\n   )\n-  def test_constraints_complex_gen(self,\n+  def test_constraints_ge_complex_gen(self,\n                                    constraint: str, exp: str,\n                                    bounds: tuple[float, float]):\n     a, b, exp = shape_poly.symbolic_shape(\n"
                },
                {
                    "old_start": 833,
                    "old_length": 7,
                    "new_start": 942,
                    "new_length": 7,
                    "hunk": "@@ -833,7 +942,7 @@ class DimExprTest(jtu.JaxTestCase):\n         constraints=(constraint,))\n     self.assertEqual(bounds, _bounds(exp))\n \n-  def test_constraints_override(self):\n+  def test_constraints_ge_override(self):\n     # Some constaints override other\n     a, b = shape_poly.symbolic_shape(\"a, b\",\n                                      constraints=(\"a >= 5\", \"b <= 16\",\n"
                },
                {
                    "old_start": 841,
                    "old_length": 6,
                    "new_start": 950,
                    "new_length": 108,
                    "hunk": "@@ -841,6 +950,108 @@ class DimExprTest(jtu.JaxTestCase):\n     self.assertEqual(_bounds(a), (10, np.inf))\n     self.assertEqual(_bounds(b), (1, 10))\n \n+  def test_constraints_eq_1(self):\n+    # Some constaints override other\n+    a, b, c = shape_poly.symbolic_shape(\"a, b, c\",\n+                                        constraints=(\"max(a, b) == c\",))\n+    self.assertEqual(_bounds(core.max_dim(a, b) - c + 3), (3, 3))\n+\n+  def test_constraints_eq_2(self):\n+    a, b = shape_poly.symbolic_shape(\"a, b\",\n+                                     constraints=(\"max(a, b) == 5\",\n+                                                  \"min(a, b) == 3\"))\n+    self.assertEqual(_bounds(core.max_dim(a, b)), (5, 5))\n+    self.assertEqual(_bounds(core.min_dim(a, b)), (3, 3))\n+    self.assertEqual(_bounds(a), (3, 5))\n+\n+  def test_constraints_eq_3(self):\n+    a, b = shape_poly.symbolic_shape(\n+        \"a, b\",\n+        # The constraint implies `b >= 2` thus `min(b, 2)` gets normalized\n+        # to `2`\n+        constraints=(\"min(a, b) == b - min(b, 2)\",))\n+    self.assertEqual(core.min_dim(b, 2), 2)\n+    self.assertEqual(_bounds(b), (2, np.inf))\n+    # TODO: the following ought to work, but the way we wrote the equality\n+    # constraint, `min(b, 2)` gets rewritten to `2`.\n+    #self.assertEqual(core.min_dim(a, b), b - core.min_dim(b, 2))\n+\n+  def test_constraints_eq_4(self):\n+    # Equalities of a variable with an expression\n+    a, b, c, d = shape_poly.symbolic_shape(\n+        \"a, b, c, d\",\n+        constraints=(\"floordiv(d, 2) == a + 5\",\n+                     \"floordiv(c, 3) == a + 3\",\n+                     \"b == a + 1\"))\n+    self.assertEqual(b, a + 1)\n+    self.assertEqual(d // 2, a + 5)\n+    self.assertEqual(c // 3, b + 2)\n+\n+  def test_constraints_eq_5(self):\n+    # Constraints involving both inequalities and equalities\n+    a, b, c, d = shape_poly.symbolic_shape(\n+        \"a, b, c, d\",\n+        constraints=(# max(c, a) == c because c >= b + 3 = a + 3\n+                     \"b == a\",\n+                     \"c >= b + 3\",\n+                     \"floordiv(d, 2) == floordiv(max(c, a), 3) + 2\",\n+        ))\n+    self.assertEqual(core.max_dim(c, a), c)\n+    # d // 2 = c // 3 + 2 = b + 5\n+    self.assertEqual(_bounds(d // 2 - c // 3), (2, 2))\n+\n+  def test_constraints_eq_6(self):\n+    # Constraints involving both inequalities and equalities\n+    a, b, c, d = shape_poly.symbolic_shape(\n+        \"a, b, c, d\",\n+        constraints=(\"b == a\",\n+                     \"c >= b + 3\",\n+                     \"max(d, a) == b + 3\",))\n+    self.assertEqual(core.max_dim(c, a), c)\n+    # d <= max(d, a) == a + 3, thus d - a <= 3\n+    self.assertEqual(_bounds(d - b), (-np.inf, 3))\n+\n+  def test_constraints_eq_threefry(self):\n+    # Test equalities that arise out of the threefree lowering\n+    # x : i32[a]  # a may be even or odd\n+    # x_padded: i32[a + a % 2] = jnp.concat([x, jnp.zeros((a % 2,))])\n+    # x_reshaped: i32[2, (a + a % 2) // 2] = x_padded.reshape((-1, 2))\n+    # x_1 = x_reshaped.reshape((-1,))\n+    a, = shape_poly.symbolic_shape(\n+        \"a\",\n+        constraints=(\"mod(a + mod(a, 2), -2) == 0\",\n+                     \"2*floordiv(mod(a, 2) + a, -2) == a\"))\n+\n+    x_reshaped, r = divmod(a + a % 2, -2)\n+    self.assertEqual(r, 0)\n+    self.assertEqual(x_reshaped, (a + a % 2) // -2)\n+    self.assertEqual(2 * x_reshaped, a)\n+\n+  def test_constraints_a_minus_4d_eq(self):\n+    # simulates d = div(a, 4) and m = mod(a, 4)\n+    assumptions = [\"4*d == a - m\", \"m >= 0\", \"m <= 3\"]\n+    scope = shape_poly.SymbolicScope(assumptions)\n+    a, d = shape_poly.symbolic_shape(\"a, d\", scope=scope)\n+    self.assertEqual(_bounds(a - 4*d), (1, 3))  # a - 4d = m >= 1\n+    # TODO: The incompleteness is due to the way we combine external constraints\n+    self.assertEqual(_bounds(a - 2*d),\n+                     _expect(best=(3, np.inf), current=(-np.inf, np.inf)))  # a - 2d = m + 2d >= 3\n+    # TODO: The incompleteness is due to the way we combine external constraints\n+    self.assertEqual(_bounds(a),\n+                     _expect(best=(5, np.inf), current=(1, np.inf)))  # a >= 4d + m >= 5\n+\n+    # Now with a different order of constraints\n+    assumptions1 = [\"m1 >= 0\", \"m1 <= 3\", \"a1 == 4*d1 + m1\"]\n+    scope1 = shape_poly.SymbolicScope(assumptions1)\n+    a1, d1, m1 = shape_poly.symbolic_shape(\"a1, d1, m1\", scope=scope1)\n+    # TODO: The incompleteness is due to the way we combine external constraints\n+    self.assertEqual(_bounds(a1 - 4*d1),\n+                     _expect(best=(1, 3), current=(1, 3)))  # a - 4d = m >= 1\n+    self.assertEqual(_bounds(a1 - 2*d1), (3, np.inf))  # a - 2d = m + 2d >= 3\n+    # TODO: The incompleteness is due to the way we combine external constraints\n+    self.assertEqual(_bounds(a1),\n+                     _expect(best=(5, np.inf), current=(-np.inf, np.inf)))  # a >= 4d + m >= 5\n+\n   def test_constraints_error_msg(self):\n     a, b = shape_poly.symbolic_shape(\"a, b\",\n                                      constraints=(\"a >= 5\",))\n"
                },
                {
                    "old_start": 1320,
                    "old_length": 6,
                    "new_start": 1531,
                    "new_length": 41,
                    "hunk": "@@ -1320,6 +1531,41 @@ class ShapePolyTest(jtu.JaxTestCase):\n                      polymorphic_shapes=[\"a\"],\n                      symbolic_constraints=[\"a >= 8\"])\n \n+  def test_constraints_slice_in_dim_eq(self):\n+    def f(x, y):  # x: i32[a], y: i32[b]\n+      v1 = x[:y.shape[0]]  # i32[min(a, b)]\n+      v2 = y[2:]  # i32[b - min(b, 2)]\n+      return v1 + v2\n+    check_shape_poly(self, f,\n+                     arg_descriptors=[RandArg((16,), _i32),  # a == 16\n+                                      RandArg((18,), _i32)],  # b == 18\n+                     polymorphic_shapes=[\"a\", \"b\"],\n+                     # This is the exact failure we are getting.\n+                     symbolic_constraints=[\n+                         \"b >= 2\",\n+                         \"min(a, b) == b - 2\"\n+                     ])\n+\n+  def test_constraints_eq_threefry(self):\n+    # This pattern arises in the lowering of threefry\n+    def f(x):  # x: i32[a]\n+      a, = x.shape\n+      x_padded = jnp.concatenate([x, jnp.zeros((a % 2,), dtype=x.dtype)])\n+      x_reshaped = x_padded.reshape((2, -1))\n+      x_reshaped += 2\n+      x_padded_1 = x_reshaped.reshape((-1,))\n+      x_1 = x_padded_1[:a]\n+      # Ensure x and x_1 have the same shape\n+      return x + x_1\n+\n+    check_shape_poly(self, f,\n+                     arg_descriptors=[RandArg((16,), _i32)],  # a == 16\n+                     polymorphic_shapes=[\"a\"],\n+                     # These are the exact failure we are getting.\n+                     symbolic_constraints=[\n+                         \"mod(a + mod(a, 2), -2) == 0\",\n+                         \"-2*floordiv(a + mod(a, 2), -2) == a + mod(a, 2)\"])\n+\n   def test_constraints_for_profile(self):\n     # A somewhat more involved tests to stress test the correctness and\n     # performance\n"
                },
                {
                    "old_start": 1333,
                    "old_length": 9,
                    "new_start": 1579,
                    "new_length": 8,
                    "hunk": "@@ -1333,9 +1579,8 @@ class ShapePolyTest(jtu.JaxTestCase):\n         acc += jnp.sum(slice, axis=0)\n       return acc\n \n-    exp = export.export(f)(jax.ShapeDtypeStruct(export.symbolic_shape(\"a, b\"),\n-                                                np.int32))\n-\n+    _ = export.export(f)(jax.ShapeDtypeStruct(export.symbolic_shape(\"a, b\"),\n+                                              np.int32))\n \n \n   def test_constraints_compile_time_check(self):"
                }
            ],
            "whole_deleted": "-  def test_constraints_basic(self):\n-  def test_constraints_trivial(self):\n-  def test_constraints_a_minus_4d(self):\n-      _ = shape_poly.symbolic_shape(\"a, b\",\n-                                \"Constraint parsing error: must contain one of '>=' or '<='\"):\n-      _ = shape_poly.symbolic_shape(\"a, b\",\n-                                    constraints=(\"a == 0\",))\n-  def test_constraints_monomial(self):\n-  def test_constraints_monomial_derived(self):\n-  def test_constraints_not_monomial(self):\n-                                        constraints=(\"a >= b\",))\n-  def test_constraints_complex(self):\n-  def test_constraints_fractional(self):\n-  def test_constraints_complex_gen(self,\n-  def test_constraints_override(self):\n-    exp = export.export(f)(jax.ShapeDtypeStruct(export.symbolic_shape(\"a, b\"),\n-                                                np.int32))\n-\n",
            "whole_added": "+\n+  def test_unit_combine_term_with_constraints(self):\n+    a, b, c, d, e = shape_poly.symbolic_shape(\"a, b, c, d, e\",\n+        constraints=[\n+           \"a >= 4\",\n+           \"b <= 5\",\n+           \"c + 3*d >= 10\", \"c - 2*d >= -4\",  # -> 5c >= 8 -> c >= 2\n+        ])\n+    scope = a.scope\n+    def _m(e: shape_poly._DimExpr) -> shape_poly._DimMon:\n+      return e.to_monomial()\n+    Comparator = shape_poly.Comparator\n+    decision = shape_poly_decision._DecisionByElimination(scope)\n+\n+    self.assertSetEqual(\n+        set(),\n+        set(decision.combine_term_with_existing(_m(e), 2, scope=scope)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, a - 4, 2, -1)},\n+        set(decision.combine_term_with_existing(_m(a), 2, scope=scope)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, a - 4, 2, 1)},\n+        set(decision.combine_term_with_existing(_m(a), -2, scope=scope)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, b - 1, 2, -1),\n+         (Comparator.GEQ, -b + 5, 2, 1)},\n+        set(decision.combine_term_with_existing(_m(b), 2, scope=scope)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, b - 1, 2, 1),\n+         (Comparator.GEQ, -b + 5, 2, -1)},\n+        set(decision.combine_term_with_existing(_m(b), -2, scope=scope)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, c - 2, 2, -1),\n+         (Comparator.GEQ, -2*d + c + 4, 2, -1),\n+         (Comparator.GEQ, 3*d + c - 10, 2, -1)},\n+        set(decision.combine_term_with_existing(_m(c), 2, scope=scope,\n+                                                only_smaller_than_t=False)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, c - 2, 2, 1),\n+         (Comparator.GEQ, -2*d + c + 4, 2, 1),\n+         (Comparator.GEQ, 3*d + c - 10, 2, 1)},\n+        set(decision.combine_term_with_existing(_m(c), -2, scope=scope,\n+                                                only_smaller_than_t=False)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, c - 2, 2, -1)},\n+        set(decision.combine_term_with_existing(_m(c), 2, scope=scope,\n+                                                only_smaller_than_t=True)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, d - 1, 2, -1),\n+         (Comparator.GEQ, -2*d + c + 4, 2, 2),\n+         (Comparator.GEQ, 3*d + c - 10, 2, -3)},\n+        set(decision.combine_term_with_existing(_m(d), 2, scope=scope,\n+                                                only_smaller_than_t=False)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, d - 1, 2, -1),\n+         (Comparator.GEQ, -2*d + c + 4, 2, 2),\n+         (Comparator.GEQ, 3*d + c - 10, 2, -3)},\n+        set(decision.combine_term_with_existing(_m(d), 2, scope=scope,\n+                                                only_smaller_than_t=True)))\n+\n+  def test_constraints_ge_basic(self):\n+  def test_constraints_ge_trivial(self):\n+    with self.assertRaisesRegex(ValueError,\n+                                \"Unsatisfiable.*a <= 0\"):\n+      a, b = shape_poly.symbolic_shape(\"a, b\",\n+                                       # Contradicts the default a >= 1\n+                                       constraints=(\"a <= 0\",))\n+      a >= b\n+\n+  def test_constraints_ge_smallest_first(self):\n+    # Since we process the smaller constraints first, they take effect for the\n+    # processing of the remaining constraints.\n+    a, = shape_poly.symbolic_shape(\"a\",\n+                                   # max(a, 2) = a\n+                                   constraints=(\"a >= 4\", \"max(a, 2) >= 6\"))\n+    self.assertEqual(core.max_dim(a, 2), a)\n+    self.assertEqual(_bounds(a), (6, np.inf))\n+\n+  def test_constraints_ge_smallest_first_caching(self):\n+    # When we process the constraint with max(a, 5) we will compute the bounds\n+    # of (a, 5) and at that time we will cache that \"-1 <= a - 5\". We should\n+    # not cache that, because later we learn that a >= 5\n+    a, = shape_poly.symbolic_shape(\"a\",\n+                                   # max(a, 2) = a\n+                                   constraints=(\"a >= 4\", \"max(a, 5) >= 5\",\n+                                                \"max(a, 2) >= 5\"))\n+    self.assertEqual(core.max_dim(a, 2), a)\n+    self.assertEqual(core.max_dim(a, 5), a)\n+\n+  def test_constraints_ge_a_minus_4d(self):\n+      _ = shape_poly.symbolic_shape(\"a\",\n+                                \"Constraint parsing error: must contain one of '==' or '>=' or '<='\"):\n+      _ = shape_poly.symbolic_shape(\"a\",\n+                                    constraints=(\"a != 0\",))\n+\n+    with self.assertRaisesRegex(ValueError,\n+                                re.escape(\"Unsatisfiable explicit constraint: a == a + 1\")):\n+      _ = shape_poly.symbolic_shape(\"a\",\n+                                    constraints=(\"a == a + 1\",))\n+    with self.assertRaisesRegex(ValueError,\n+                                re.escape(\"Unsatisfiable constraint: a >= 1\")):\n+      a, = shape_poly.symbolic_shape(\"a\",\n+                                     # Contradicts the default a >= 1\n+                                     constraints=(\"a == 0\",))\n+      # We detect unsatisfiablity once we try to decide inequalities\n+      _ = a >= -1\n+\n+  def test_constraints_ge_monomial(self):\n+  def test_constraints_ge_monomial_derived(self):\n+  def test_constraints_ge_not_monomial(self):\n+                                     constraints=(\"a >= b\",))\n+  def test_constraints_ge_complex(self):\n+  def test_constraints_ge_fractional(self):\n+  def test_constraints_ge_complex_gen(self,\n+  def test_constraints_ge_override(self):\n+  def test_constraints_eq_1(self):\n+    # Some constaints override other\n+    a, b, c = shape_poly.symbolic_shape(\"a, b, c\",\n+                                        constraints=(\"max(a, b) == c\",))\n+    self.assertEqual(_bounds(core.max_dim(a, b) - c + 3), (3, 3))\n+\n+  def test_constraints_eq_2(self):\n+    a, b = shape_poly.symbolic_shape(\"a, b\",\n+                                     constraints=(\"max(a, b) == 5\",\n+                                                  \"min(a, b) == 3\"))\n+    self.assertEqual(_bounds(core.max_dim(a, b)), (5, 5))\n+    self.assertEqual(_bounds(core.min_dim(a, b)), (3, 3))\n+    self.assertEqual(_bounds(a), (3, 5))\n+\n+  def test_constraints_eq_3(self):\n+    a, b = shape_poly.symbolic_shape(\n+        \"a, b\",\n+        # The constraint implies `b >= 2` thus `min(b, 2)` gets normalized\n+        # to `2`\n+        constraints=(\"min(a, b) == b - min(b, 2)\",))\n+    self.assertEqual(core.min_dim(b, 2), 2)\n+    self.assertEqual(_bounds(b), (2, np.inf))\n+    # TODO: the following ought to work, but the way we wrote the equality\n+    # constraint, `min(b, 2)` gets rewritten to `2`.\n+    #self.assertEqual(core.min_dim(a, b), b - core.min_dim(b, 2))\n+\n+  def test_constraints_eq_4(self):\n+    # Equalities of a variable with an expression\n+    a, b, c, d = shape_poly.symbolic_shape(\n+        \"a, b, c, d\",\n+        constraints=(\"floordiv(d, 2) == a + 5\",\n+                     \"floordiv(c, 3) == a + 3\",\n+                     \"b == a + 1\"))\n+    self.assertEqual(b, a + 1)\n+    self.assertEqual(d // 2, a + 5)\n+    self.assertEqual(c // 3, b + 2)\n+\n+  def test_constraints_eq_5(self):\n+    # Constraints involving both inequalities and equalities\n+    a, b, c, d = shape_poly.symbolic_shape(\n+        \"a, b, c, d\",\n+        constraints=(# max(c, a) == c because c >= b + 3 = a + 3\n+                     \"b == a\",\n+                     \"c >= b + 3\",\n+                     \"floordiv(d, 2) == floordiv(max(c, a), 3) + 2\",\n+        ))\n+    self.assertEqual(core.max_dim(c, a), c)\n+    # d // 2 = c // 3 + 2 = b + 5\n+    self.assertEqual(_bounds(d // 2 - c // 3), (2, 2))\n+\n+  def test_constraints_eq_6(self):\n+    # Constraints involving both inequalities and equalities\n+    a, b, c, d = shape_poly.symbolic_shape(\n+        \"a, b, c, d\",\n+        constraints=(\"b == a\",\n+                     \"c >= b + 3\",\n+                     \"max(d, a) == b + 3\",))\n+    self.assertEqual(core.max_dim(c, a), c)\n+    # d <= max(d, a) == a + 3, thus d - a <= 3\n+    self.assertEqual(_bounds(d - b), (-np.inf, 3))\n+\n+  def test_constraints_eq_threefry(self):\n+    # Test equalities that arise out of the threefree lowering\n+    # x : i32[a]  # a may be even or odd\n+    # x_padded: i32[a + a % 2] = jnp.concat([x, jnp.zeros((a % 2,))])\n+    # x_reshaped: i32[2, (a + a % 2) // 2] = x_padded.reshape((-1, 2))\n+    # x_1 = x_reshaped.reshape((-1,))\n+    a, = shape_poly.symbolic_shape(\n+        \"a\",\n+        constraints=(\"mod(a + mod(a, 2), -2) == 0\",\n+                     \"2*floordiv(mod(a, 2) + a, -2) == a\"))\n+\n+    x_reshaped, r = divmod(a + a % 2, -2)\n+    self.assertEqual(r, 0)\n+    self.assertEqual(x_reshaped, (a + a % 2) // -2)\n+    self.assertEqual(2 * x_reshaped, a)\n+\n+  def test_constraints_a_minus_4d_eq(self):\n+    # simulates d = div(a, 4) and m = mod(a, 4)\n+    assumptions = [\"4*d == a - m\", \"m >= 0\", \"m <= 3\"]\n+    scope = shape_poly.SymbolicScope(assumptions)\n+    a, d = shape_poly.symbolic_shape(\"a, d\", scope=scope)\n+    self.assertEqual(_bounds(a - 4*d), (1, 3))  # a - 4d = m >= 1\n+    # TODO: The incompleteness is due to the way we combine external constraints\n+    self.assertEqual(_bounds(a - 2*d),\n+                     _expect(best=(3, np.inf), current=(-np.inf, np.inf)))  # a - 2d = m + 2d >= 3\n+    # TODO: The incompleteness is due to the way we combine external constraints\n+    self.assertEqual(_bounds(a),\n+                     _expect(best=(5, np.inf), current=(1, np.inf)))  # a >= 4d + m >= 5\n+\n+    # Now with a different order of constraints\n+    assumptions1 = [\"m1 >= 0\", \"m1 <= 3\", \"a1 == 4*d1 + m1\"]\n+    scope1 = shape_poly.SymbolicScope(assumptions1)\n+    a1, d1, m1 = shape_poly.symbolic_shape(\"a1, d1, m1\", scope=scope1)\n+    # TODO: The incompleteness is due to the way we combine external constraints\n+    self.assertEqual(_bounds(a1 - 4*d1),\n+                     _expect(best=(1, 3), current=(1, 3)))  # a - 4d = m >= 1\n+    self.assertEqual(_bounds(a1 - 2*d1), (3, np.inf))  # a - 2d = m + 2d >= 3\n+    # TODO: The incompleteness is due to the way we combine external constraints\n+    self.assertEqual(_bounds(a1),\n+                     _expect(best=(5, np.inf), current=(-np.inf, np.inf)))  # a >= 4d + m >= 5\n+\n+  def test_constraints_slice_in_dim_eq(self):\n+    def f(x, y):  # x: i32[a], y: i32[b]\n+      v1 = x[:y.shape[0]]  # i32[min(a, b)]\n+      v2 = y[2:]  # i32[b - min(b, 2)]\n+      return v1 + v2\n+    check_shape_poly(self, f,\n+                     arg_descriptors=[RandArg((16,), _i32),  # a == 16\n+                                      RandArg((18,), _i32)],  # b == 18\n+                     polymorphic_shapes=[\"a\", \"b\"],\n+                     # This is the exact failure we are getting.\n+                     symbolic_constraints=[\n+                         \"b >= 2\",\n+                         \"min(a, b) == b - 2\"\n+                     ])\n+\n+  def test_constraints_eq_threefry(self):\n+    # This pattern arises in the lowering of threefry\n+    def f(x):  # x: i32[a]\n+      a, = x.shape\n+      x_padded = jnp.concatenate([x, jnp.zeros((a % 2,), dtype=x.dtype)])\n+      x_reshaped = x_padded.reshape((2, -1))\n+      x_reshaped += 2\n+      x_padded_1 = x_reshaped.reshape((-1,))\n+      x_1 = x_padded_1[:a]\n+      # Ensure x and x_1 have the same shape\n+      return x + x_1\n+\n+    check_shape_poly(self, f,\n+                     arg_descriptors=[RandArg((16,), _i32)],  # a == 16\n+                     polymorphic_shapes=[\"a\"],\n+                     # These are the exact failure we are getting.\n+                     symbolic_constraints=[\n+                         \"mod(a + mod(a, 2), -2) == 0\",\n+                         \"-2*floordiv(a + mod(a, 2), -2) == a + mod(a, 2)\"])\n+\n+    _ = export.export(f)(jax.ShapeDtypeStruct(export.symbolic_shape(\"a, b\"),\n+                                              np.int32))\n",
            "whole_hunk": "@@ -685,6 +685,75 @@ class DimExprTest(jtu.JaxTestCase):\n       self.sampled_assertion(remainder, lambda *args: divmod(*args)[1],\n                              dividend, divisor)\n \n+\n+  def test_unit_combine_term_with_constraints(self):\n+    a, b, c, d, e = shape_poly.symbolic_shape(\"a, b, c, d, e\",\n+        constraints=[\n+           \"a >= 4\",\n+           \"b <= 5\",\n+           \"c + 3*d >= 10\", \"c - 2*d >= -4\",  # -> 5c >= 8 -> c >= 2\n+        ])\n+    scope = a.scope\n+    def _m(e: shape_poly._DimExpr) -> shape_poly._DimMon:\n+      return e.to_monomial()\n+    Comparator = shape_poly.Comparator\n+    decision = shape_poly_decision._DecisionByElimination(scope)\n+\n+    self.assertSetEqual(\n+        set(),\n+        set(decision.combine_term_with_existing(_m(e), 2, scope=scope)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, a - 4, 2, -1)},\n+        set(decision.combine_term_with_existing(_m(a), 2, scope=scope)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, a - 4, 2, 1)},\n+        set(decision.combine_term_with_existing(_m(a), -2, scope=scope)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, b - 1, 2, -1),\n+         (Comparator.GEQ, -b + 5, 2, 1)},\n+        set(decision.combine_term_with_existing(_m(b), 2, scope=scope)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, b - 1, 2, 1),\n+         (Comparator.GEQ, -b + 5, 2, -1)},\n+        set(decision.combine_term_with_existing(_m(b), -2, scope=scope)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, c - 2, 2, -1),\n+         (Comparator.GEQ, -2*d + c + 4, 2, -1),\n+         (Comparator.GEQ, 3*d + c - 10, 2, -1)},\n+        set(decision.combine_term_with_existing(_m(c), 2, scope=scope,\n+                                                only_smaller_than_t=False)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, c - 2, 2, 1),\n+         (Comparator.GEQ, -2*d + c + 4, 2, 1),\n+         (Comparator.GEQ, 3*d + c - 10, 2, 1)},\n+        set(decision.combine_term_with_existing(_m(c), -2, scope=scope,\n+                                                only_smaller_than_t=False)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, c - 2, 2, -1)},\n+        set(decision.combine_term_with_existing(_m(c), 2, scope=scope,\n+                                                only_smaller_than_t=True)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, d - 1, 2, -1),\n+         (Comparator.GEQ, -2*d + c + 4, 2, 2),\n+         (Comparator.GEQ, 3*d + c - 10, 2, -3)},\n+        set(decision.combine_term_with_existing(_m(d), 2, scope=scope,\n+                                                only_smaller_than_t=False)))\n+\n+    self.assertSetEqual(\n+        {(Comparator.GEQ, d - 1, 2, -1),\n+         (Comparator.GEQ, -2*d + c + 4, 2, 2),\n+         (Comparator.GEQ, 3*d + c - 10, 2, -3)},\n+        set(decision.combine_term_with_existing(_m(d), 2, scope=scope,\n+                                                only_smaller_than_t=True)))\n+\n   def test_non_negative_dim(self):\n     a, = shape_poly.symbolic_shape(\"a,\")\n \n@@ -725,13 +794,13 @@ class DimExprTest(jtu.JaxTestCase):\n     self.sampled_assertion(core.max_dim((a - 4) // 2 + 1, 0),\n                            core.stride_dim, a, 4, 2)\n \n-  def test_constraints_basic(self):\n+  def test_constraints_ge_basic(self):\n     a, b = shape_poly.symbolic_shape(\"a, b\",\n                                      constraints=(\"a >= 5\", \"b <= 16\"))\n     self.assertEqual(_bounds(a), (5, np.inf))\n     self.assertEqual(_bounds(b), (1, 16))\n \n-  def test_constraints_trivial(self):\n+  def test_constraints_ge_trivial(self):\n     a, = shape_poly.symbolic_shape(\"a\",\n                                    # Trivial, is dropped\n                                    constraints=(\"a <= a + 1\",))\n@@ -748,7 +817,34 @@ class DimExprTest(jtu.JaxTestCase):\n                                        constraints=(\"a <= 0\",))\n       a >= b\n \n-  def test_constraints_a_minus_4d(self):\n+    with self.assertRaisesRegex(ValueError,\n+                                \"Unsatisfiable.*a <= 0\"):\n+      a, b = shape_poly.symbolic_shape(\"a, b\",\n+                                       # Contradicts the default a >= 1\n+                                       constraints=(\"a <= 0\",))\n+      a >= b\n+\n+  def test_constraints_ge_smallest_first(self):\n+    # Since we process the smaller constraints first, they take effect for the\n+    # processing of the remaining constraints.\n+    a, = shape_poly.symbolic_shape(\"a\",\n+                                   # max(a, 2) = a\n+                                   constraints=(\"a >= 4\", \"max(a, 2) >= 6\"))\n+    self.assertEqual(core.max_dim(a, 2), a)\n+    self.assertEqual(_bounds(a), (6, np.inf))\n+\n+  def test_constraints_ge_smallest_first_caching(self):\n+    # When we process the constraint with max(a, 5) we will compute the bounds\n+    # of (a, 5) and at that time we will cache that \"-1 <= a - 5\". We should\n+    # not cache that, because later we learn that a >= 5\n+    a, = shape_poly.symbolic_shape(\"a\",\n+                                   # max(a, 2) = a\n+                                   constraints=(\"a >= 4\", \"max(a, 5) >= 5\",\n+                                                \"max(a, 2) >= 5\"))\n+    self.assertEqual(core.max_dim(a, 2), a)\n+    self.assertEqual(core.max_dim(a, 5), a)\n+\n+  def test_constraints_ge_a_minus_4d(self):\n     # simulates d = div(a, 4) and m = mod(a, 4)\n     assumptions = [\"a >= 4*d + m \",\n                    \"a <= 4*d + m\",\n@@ -764,16 +860,29 @@ class DimExprTest(jtu.JaxTestCase):\n   def test_constraints_errors(self):\n     with self.assertRaisesRegex(ValueError,\n                                 \"The symbolic constraints should be a sequence of strings\"):\n-      _ = shape_poly.symbolic_shape(\"a, b\",\n+      _ = shape_poly.symbolic_shape(\"a\",\n                                     constraints=\"a <= a - 1\")\n \n     with self.assertRaisesRegex(ValueError,\n-                                \"Constraint parsing error: must contain one of '>=' or '<='\"):\n-      _ = shape_poly.symbolic_shape(\"a, b\",\n+                                \"Constraint parsing error: must contain one of '==' or '>=' or '<='\"):\n+      _ = shape_poly.symbolic_shape(\"a\",\n+                                    constraints=(\"a != 0\",))\n+\n+    with self.assertRaisesRegex(ValueError,\n+                                re.escape(\"Unsatisfiable explicit constraint: a == a + 1\")):\n+      _ = shape_poly.symbolic_shape(\"a\",\n                                     # Contradicts the default a >= 1\n-                                    constraints=(\"a == 0\",))\n+                                    constraints=(\"a == a + 1\",))\n \n-  def test_constraints_monomial(self):\n+    with self.assertRaisesRegex(ValueError,\n+                                re.escape(\"Unsatisfiable constraint: a >= 1\")):\n+      a, = shape_poly.symbolic_shape(\"a\",\n+                                     # Contradicts the default a >= 1\n+                                     constraints=(\"a == 0\",))\n+      # We detect unsatisfiablity once we try to decide inequalities\n+      _ = a >= -1\n+\n+  def test_constraints_ge_monomial(self):\n     a, b = shape_poly.symbolic_shape(\n         \"a, b\",\n         constraints=(\"max(a, b) >= 8\", \"min(a, b) <= 2\",\n@@ -786,7 +895,7 @@ class DimExprTest(jtu.JaxTestCase):\n     self.assertGreaterEqual(a % b, 1)\n     self.assertGreaterEqual(core.max_dim(a, b) % 4, 2)\n \n-  def test_constraints_monomial_derived(self):\n+  def test_constraints_ge_monomial_derived(self):\n     a, b = shape_poly.symbolic_shape(\n         \"a, b\",\n         constraints=(\"a >= 4\", \"3 >= b\"))\n@@ -795,12 +904,12 @@ class DimExprTest(jtu.JaxTestCase):\n     self.assertEqual(core.min_dim(a, b), b)\n     self.assertGreaterEqual(a // 3, 1)\n \n-  def test_constraints_not_monomial(self):\n+  def test_constraints_ge_not_monomial(self):\n     a, b = shape_poly.symbolic_shape(\"a, b\",\n-                                        constraints=(\"a >= b\",))\n+                                     constraints=(\"a >= b\",))\n     self.assertGreaterEqual(a, b)\n \n-  def test_constraints_complex(self):\n+  def test_constraints_ge_complex(self):\n     a, b, c = shape_poly.symbolic_shape(\n         \"a, b, c\",\n         constraints=(\"a + 2 <= b\", \"b <= a + 5\", \"a + b >= c\"))\n@@ -809,7 +918,7 @@ class DimExprTest(jtu.JaxTestCase):\n     self.assertEqual(_bounds(b - a - 7), (-5, -2))\n     self.assertEqual(_bounds(c - 2*a - 5), (-np.inf, 0))\n \n-  def test_constraints_fractional(self):\n+  def test_constraints_ge_fractional(self):\n     a, = shape_poly.symbolic_shape(\"a\",\n                                    constraints=(\"2 * a >=  5\", \"3 * a <= 10\",))\n     self.assertEqual(_bounds(5*a - 2), (13, 13))\n@@ -825,7 +934,7 @@ class DimExprTest(jtu.JaxTestCase):\n           dict(constraint=\"2*a + -3*b >= 10\", exp=\"-1*a + 2*b\", bounds=(-np.inf, np.inf)),\n       ]\n   )\n-  def test_constraints_complex_gen(self,\n+  def test_constraints_ge_complex_gen(self,\n                                    constraint: str, exp: str,\n                                    bounds: tuple[float, float]):\n     a, b, exp = shape_poly.symbolic_shape(\n@@ -833,7 +942,7 @@ class DimExprTest(jtu.JaxTestCase):\n         constraints=(constraint,))\n     self.assertEqual(bounds, _bounds(exp))\n \n-  def test_constraints_override(self):\n+  def test_constraints_ge_override(self):\n     # Some constaints override other\n     a, b = shape_poly.symbolic_shape(\"a, b\",\n                                      constraints=(\"a >= 5\", \"b <= 16\",\n@@ -841,6 +950,108 @@ class DimExprTest(jtu.JaxTestCase):\n     self.assertEqual(_bounds(a), (10, np.inf))\n     self.assertEqual(_bounds(b), (1, 10))\n \n+  def test_constraints_eq_1(self):\n+    # Some constaints override other\n+    a, b, c = shape_poly.symbolic_shape(\"a, b, c\",\n+                                        constraints=(\"max(a, b) == c\",))\n+    self.assertEqual(_bounds(core.max_dim(a, b) - c + 3), (3, 3))\n+\n+  def test_constraints_eq_2(self):\n+    a, b = shape_poly.symbolic_shape(\"a, b\",\n+                                     constraints=(\"max(a, b) == 5\",\n+                                                  \"min(a, b) == 3\"))\n+    self.assertEqual(_bounds(core.max_dim(a, b)), (5, 5))\n+    self.assertEqual(_bounds(core.min_dim(a, b)), (3, 3))\n+    self.assertEqual(_bounds(a), (3, 5))\n+\n+  def test_constraints_eq_3(self):\n+    a, b = shape_poly.symbolic_shape(\n+        \"a, b\",\n+        # The constraint implies `b >= 2` thus `min(b, 2)` gets normalized\n+        # to `2`\n+        constraints=(\"min(a, b) == b - min(b, 2)\",))\n+    self.assertEqual(core.min_dim(b, 2), 2)\n+    self.assertEqual(_bounds(b), (2, np.inf))\n+    # TODO: the following ought to work, but the way we wrote the equality\n+    # constraint, `min(b, 2)` gets rewritten to `2`.\n+    #self.assertEqual(core.min_dim(a, b), b - core.min_dim(b, 2))\n+\n+  def test_constraints_eq_4(self):\n+    # Equalities of a variable with an expression\n+    a, b, c, d = shape_poly.symbolic_shape(\n+        \"a, b, c, d\",\n+        constraints=(\"floordiv(d, 2) == a + 5\",\n+                     \"floordiv(c, 3) == a + 3\",\n+                     \"b == a + 1\"))\n+    self.assertEqual(b, a + 1)\n+    self.assertEqual(d // 2, a + 5)\n+    self.assertEqual(c // 3, b + 2)\n+\n+  def test_constraints_eq_5(self):\n+    # Constraints involving both inequalities and equalities\n+    a, b, c, d = shape_poly.symbolic_shape(\n+        \"a, b, c, d\",\n+        constraints=(# max(c, a) == c because c >= b + 3 = a + 3\n+                     \"b == a\",\n+                     \"c >= b + 3\",\n+                     \"floordiv(d, 2) == floordiv(max(c, a), 3) + 2\",\n+        ))\n+    self.assertEqual(core.max_dim(c, a), c)\n+    # d // 2 = c // 3 + 2 = b + 5\n+    self.assertEqual(_bounds(d // 2 - c // 3), (2, 2))\n+\n+  def test_constraints_eq_6(self):\n+    # Constraints involving both inequalities and equalities\n+    a, b, c, d = shape_poly.symbolic_shape(\n+        \"a, b, c, d\",\n+        constraints=(\"b == a\",\n+                     \"c >= b + 3\",\n+                     \"max(d, a) == b + 3\",))\n+    self.assertEqual(core.max_dim(c, a), c)\n+    # d <= max(d, a) == a + 3, thus d - a <= 3\n+    self.assertEqual(_bounds(d - b), (-np.inf, 3))\n+\n+  def test_constraints_eq_threefry(self):\n+    # Test equalities that arise out of the threefree lowering\n+    # x : i32[a]  # a may be even or odd\n+    # x_padded: i32[a + a % 2] = jnp.concat([x, jnp.zeros((a % 2,))])\n+    # x_reshaped: i32[2, (a + a % 2) // 2] = x_padded.reshape((-1, 2))\n+    # x_1 = x_reshaped.reshape((-1,))\n+    a, = shape_poly.symbolic_shape(\n+        \"a\",\n+        constraints=(\"mod(a + mod(a, 2), -2) == 0\",\n+                     \"2*floordiv(mod(a, 2) + a, -2) == a\"))\n+\n+    x_reshaped, r = divmod(a + a % 2, -2)\n+    self.assertEqual(r, 0)\n+    self.assertEqual(x_reshaped, (a + a % 2) // -2)\n+    self.assertEqual(2 * x_reshaped, a)\n+\n+  def test_constraints_a_minus_4d_eq(self):\n+    # simulates d = div(a, 4) and m = mod(a, 4)\n+    assumptions = [\"4*d == a - m\", \"m >= 0\", \"m <= 3\"]\n+    scope = shape_poly.SymbolicScope(assumptions)\n+    a, d = shape_poly.symbolic_shape(\"a, d\", scope=scope)\n+    self.assertEqual(_bounds(a - 4*d), (1, 3))  # a - 4d = m >= 1\n+    # TODO: The incompleteness is due to the way we combine external constraints\n+    self.assertEqual(_bounds(a - 2*d),\n+                     _expect(best=(3, np.inf), current=(-np.inf, np.inf)))  # a - 2d = m + 2d >= 3\n+    # TODO: The incompleteness is due to the way we combine external constraints\n+    self.assertEqual(_bounds(a),\n+                     _expect(best=(5, np.inf), current=(1, np.inf)))  # a >= 4d + m >= 5\n+\n+    # Now with a different order of constraints\n+    assumptions1 = [\"m1 >= 0\", \"m1 <= 3\", \"a1 == 4*d1 + m1\"]\n+    scope1 = shape_poly.SymbolicScope(assumptions1)\n+    a1, d1, m1 = shape_poly.symbolic_shape(\"a1, d1, m1\", scope=scope1)\n+    # TODO: The incompleteness is due to the way we combine external constraints\n+    self.assertEqual(_bounds(a1 - 4*d1),\n+                     _expect(best=(1, 3), current=(1, 3)))  # a - 4d = m >= 1\n+    self.assertEqual(_bounds(a1 - 2*d1), (3, np.inf))  # a - 2d = m + 2d >= 3\n+    # TODO: The incompleteness is due to the way we combine external constraints\n+    self.assertEqual(_bounds(a1),\n+                     _expect(best=(5, np.inf), current=(-np.inf, np.inf)))  # a >= 4d + m >= 5\n+\n   def test_constraints_error_msg(self):\n     a, b = shape_poly.symbolic_shape(\"a, b\",\n                                      constraints=(\"a >= 5\",))\n@@ -1320,6 +1531,41 @@ class ShapePolyTest(jtu.JaxTestCase):\n                      polymorphic_shapes=[\"a\"],\n                      symbolic_constraints=[\"a >= 8\"])\n \n+  def test_constraints_slice_in_dim_eq(self):\n+    def f(x, y):  # x: i32[a], y: i32[b]\n+      v1 = x[:y.shape[0]]  # i32[min(a, b)]\n+      v2 = y[2:]  # i32[b - min(b, 2)]\n+      return v1 + v2\n+    check_shape_poly(self, f,\n+                     arg_descriptors=[RandArg((16,), _i32),  # a == 16\n+                                      RandArg((18,), _i32)],  # b == 18\n+                     polymorphic_shapes=[\"a\", \"b\"],\n+                     # This is the exact failure we are getting.\n+                     symbolic_constraints=[\n+                         \"b >= 2\",\n+                         \"min(a, b) == b - 2\"\n+                     ])\n+\n+  def test_constraints_eq_threefry(self):\n+    # This pattern arises in the lowering of threefry\n+    def f(x):  # x: i32[a]\n+      a, = x.shape\n+      x_padded = jnp.concatenate([x, jnp.zeros((a % 2,), dtype=x.dtype)])\n+      x_reshaped = x_padded.reshape((2, -1))\n+      x_reshaped += 2\n+      x_padded_1 = x_reshaped.reshape((-1,))\n+      x_1 = x_padded_1[:a]\n+      # Ensure x and x_1 have the same shape\n+      return x + x_1\n+\n+    check_shape_poly(self, f,\n+                     arg_descriptors=[RandArg((16,), _i32)],  # a == 16\n+                     polymorphic_shapes=[\"a\"],\n+                     # These are the exact failure we are getting.\n+                     symbolic_constraints=[\n+                         \"mod(a + mod(a, 2), -2) == 0\",\n+                         \"-2*floordiv(a + mod(a, 2), -2) == a + mod(a, 2)\"])\n+\n   def test_constraints_for_profile(self):\n     # A somewhat more involved tests to stress test the correctness and\n     # performance\n@@ -1333,9 +1579,8 @@ class ShapePolyTest(jtu.JaxTestCase):\n         acc += jnp.sum(slice, axis=0)\n       return acc\n \n-    exp = export.export(f)(jax.ShapeDtypeStruct(export.symbolic_shape(\"a, b\"),\n-                                                np.int32))\n-\n+    _ = export.export(f)(jax.ShapeDtypeStruct(export.symbolic_shape(\"a, b\"),\n+                                              np.int32))\n \n \n   def test_constraints_compile_time_check(self):"
        }
    ]
},
{
    "Id": 98,
    "commit_link": "https://github.com/google/jax/commit/cd1332eec6de8e99be70c066b636cb5c4e1d5a4c",
    "date": "2024-01-29T13:23:19-08:00",
    "message": "Removes extra parens from sharding comparison check (the original code always produced 'false').\n\nPiperOrigin-RevId: 602482106",
    "changes": [
        {
            "name": "pjit_test.py",
            "path": "tests/pjit_test.py",
            "patches": [
                {
                    "old_start": 1568,
                    "old_length": 9,
                    "new_start": 1568,
                    "new_length": 13,
                    "hunk": "@@ -1568,9 +1568,13 @@ class AutoShardingPjitTest(jtu.JaxTestCase):\n       inp = core.ShapedArray(input_data.shape, input_data.dtype)\n       compiled = f.lower(inp).compile()\n \n-      different_pspec = (P('y', 'x')\n-                          if compiled.input_shardings[0][0].spec == P(('x',), ('y',))\n-                          else P('x', 'y'))\n+      different_pspec = (\n+          P('y', 'x')\n+          if compiled.input_shardings[0][0].is_equivalent_to(\n+              NamedSharding(global_mesh, P('x', 'y')), len(global_input_shape)\n+          )\n+          else P('x', 'y')\n+      )\n       arr, _ = create_array(global_input_shape, global_mesh, different_pspec,\n                             input_data)\n       with self.assertRaisesRegex("
                }
            ],
            "whole_deleted": "-      different_pspec = (P('y', 'x')\n-                          if compiled.input_shardings[0][0].spec == P(('x',), ('y',))\n-                          else P('x', 'y'))\n",
            "whole_added": "+      different_pspec = (\n+          P('y', 'x')\n+          if compiled.input_shardings[0][0].is_equivalent_to(\n+              NamedSharding(global_mesh, P('x', 'y')), len(global_input_shape)\n+          )\n+          else P('x', 'y')\n+      )\n",
            "whole_hunk": "@@ -1568,9 +1568,13 @@ class AutoShardingPjitTest(jtu.JaxTestCase):\n       inp = core.ShapedArray(input_data.shape, input_data.dtype)\n       compiled = f.lower(inp).compile()\n \n-      different_pspec = (P('y', 'x')\n-                          if compiled.input_shardings[0][0].spec == P(('x',), ('y',))\n-                          else P('x', 'y'))\n+      different_pspec = (\n+          P('y', 'x')\n+          if compiled.input_shardings[0][0].is_equivalent_to(\n+              NamedSharding(global_mesh, P('x', 'y')), len(global_input_shape)\n+          )\n+          else P('x', 'y')\n+      )\n       arr, _ = create_array(global_input_shape, global_mesh, different_pspec,\n                             input_data)\n       with self.assertRaisesRegex("
        }
    ]
},
{
    "Id": 99,
    "commit_link": "https://github.com/google/jax/commit/ebfce197ea0972c693bd31c546243a83205a6c9d",
    "date": "2024-01-24T16:41:37-08:00",
    "message": "Emit dense arrays for StableHLO ops migrating to dense arrays\n\nWe are migrating some attrs on some StableHLO ops to use DenseI64ArrayAttr instead of DenseIntElementsAttr. Using DenseI64ArrayAttr enforces that the attr values are 1-dimensional and provides nicer APIs. (see https://github.com/openxla/stablehlo/issues/1578 for additional context)\n\nUnfortunately, we have to duplicate the `dense_int_array` function because we migrated the ops in batches. We can't use the existing `dense_int_array` function because it would produce arrays for ops that hadn't yet been migrated. This PR makes the final batch of changes, so no additional methods should be added going forward.\n\nWe also have to introduce a new `dense_bool_array` function, with a similar version check.\n\nWhen the minimum supported jaxlib version uses a recent enough version of StableHLO  (v6 or above), it will be possible to remove the version checks and remove the duplicated `dense_int_array_v6` function.\n\nPiperOrigin-RevId: 601271749",
    "changes": [
        {
            "name": "mlir.py",
            "path": "jax/_src/interpreters/mlir.py",
            "patches": [
                {
                    "old_start": 82,
                    "old_length": 10,
                    "new_start": 82,
                    "new_length": 17,
                    "hunk": "@@ -82,10 +82,17 @@ def dense_int_elements(xs) -> ir.DenseIntElementsAttr:\n   return ir.DenseIntElementsAttr.get(np.asarray(xs, np.int64))\n \n def dense_int_array(xs) -> ir.DenseIntElementsAttr | ir.DenseI64ArrayAttr:\n+  # TODO: b/321794305 - remove this check when jaxlib is on StableHLO API v5 or higher\n   if hlo.get_api_version() < 5:\n     return dense_int_elements(xs)\n   return ir.DenseI64ArrayAttr.get(np.asarray(xs, np.int64))\n \n+# TODO: b/321794305 - delete this when jaxlib is on StableHLO API v6 or higher\n+def dense_int_array_v6(xs) -> ir.DenseIntElementsAttr | ir.DenseI64ArrayAttr:\n+  if hlo.get_api_version() < 6 or xc.mlir_api_version < 55:\n+    return dense_int_elements(xs)\n+  return ir.DenseI64ArrayAttr.get(np.asarray(xs, np.int64))\n+\n def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr:\n   a = np.packbits(np.array(xs, np.bool_), bitorder='little')\n   # TODO(b/209005197): Work around for MLIR crash for non-splat single element\n"
                },
                {
                    "old_start": 95,
                    "old_length": 6,
                    "new_start": 102,
                    "new_length": 12,
                    "hunk": "@@ -95,6 +102,12 @@ def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr:\n   return ir.DenseElementsAttr.get(\n       a, type=ir.IntegerType.get_signless(1), shape=[len(xs)])\n \n+def dense_bool_array(xs: Sequence[bool]) -> ir.DenseElementsAttr | ir.DenseBoolArrayAttr:\n+  # TODO: b/321794305 - remove this check when jaxlib is on StableHLO API v6 or higher\n+  if hlo.get_api_version() < 6 or xc.mlir_api_version < 55:\n+    return dense_bool_elements(xs)\n+  return ir.DenseBoolArrayAttr.get(xs)\n+\n def i32_attr(i): return ir.IntegerAttr.get(ir.IntegerType.get_signless(32), i)\n def i64_attr(i): return ir.IntegerAttr.get(ir.IntegerType.get_signless(64), i)\n \n"
                },
                {
                    "old_start": 304,
                    "old_length": 7,
                    "new_start": 317,
                    "new_length": 7,
                    "hunk": "@@ -304,7 +317,7 @@ def _ndarray_constant_handler(val: np.ndarray) -> Sequence[ir.Value]:\n         ir.RankedTensorType.get(\n             val.shape, dtype_to_ir_type(collapsed_val.dtype)),\n         _numpy_array_constant(collapsed_val)[0],\n-        dense_int_elements(other_axes))\n+        dense_int_array_v6(other_axes))\n     return (out,)\n   else:\n     return _numpy_array_constant(val)\n"
                },
                {
                    "old_start": 1890,
                    "old_length": 14,
                    "new_start": 1903,
                    "new_length": 14,
                    "hunk": "@@ -1890,14 +1903,14 @@ def broadcast_in_dim(ctx: LoweringRuleContext, op, aval_out: core.AbstractValue,\n       return hlo.dynamic_broadcast_in_dim(\n           aval_to_ir_type(aval_out), op,\n           shape,\n-          dense_int_elements(broadcast_dimensions),\n+          dense_int_array_v6(broadcast_dimensions),\n       )\n     else:\n       assert all(d != ir.ShapedType.get_dynamic_size()\n                  for d in aval_out.shape), aval_out  # type: ignore\n       return hlo.broadcast_in_dim(\n           aval_to_ir_type(aval_out), op,\n-          dense_int_elements(broadcast_dimensions))\n+          dense_int_array_v6(broadcast_dimensions))\n \n def multi_broadcast_in_dim(ctx: LoweringRuleContext,\n                            ops: Sequence[ir.Value],\n"
                },
                {
                    "old_start": 2716,
                    "old_length": 10,
                    "new_start": 2729,
                    "new_length": 10,
                    "hunk": "@@ -2716,10 +2729,10 @@ def reduce_window(\n     rw = hlo.ReduceWindowOp(\n         list(map(aval_to_ir_type, out_avals)),\n         operands, init_values,\n-        dense_int_elements(window_dimensions),\n-        window_strides=dense_int_elements(window_strides),\n-        base_dilations=dense_int_elements(base_dilation),\n-        window_dilations=dense_int_elements(window_dilation),\n+        dense_int_array_v6(window_dimensions),\n+        window_strides=dense_int_array_v6(window_strides),\n+        base_dilations=dense_int_array_v6(base_dilation),\n+        window_dilations=dense_int_array_v6(window_dilation),\n         padding=ir.DenseIntElementsAttr.get(np.asarray(padding, np.int64),\n                                             shape=(len(padding), 2)))\n     reducer = rw.regions[0].blocks.append(*(scalar_types + scalar_types))\n"
                }
            ],
            "whole_deleted": "-        dense_int_elements(other_axes))\n-          dense_int_elements(broadcast_dimensions),\n-          dense_int_elements(broadcast_dimensions))\n-        dense_int_elements(window_dimensions),\n-        window_strides=dense_int_elements(window_strides),\n-        base_dilations=dense_int_elements(base_dilation),\n-        window_dilations=dense_int_elements(window_dilation),\n",
            "whole_added": "+  # TODO: b/321794305 - remove this check when jaxlib is on StableHLO API v5 or higher\n+# TODO: b/321794305 - delete this when jaxlib is on StableHLO API v6 or higher\n+def dense_int_array_v6(xs) -> ir.DenseIntElementsAttr | ir.DenseI64ArrayAttr:\n+  if hlo.get_api_version() < 6 or xc.mlir_api_version < 55:\n+    return dense_int_elements(xs)\n+  return ir.DenseI64ArrayAttr.get(np.asarray(xs, np.int64))\n+\n+def dense_bool_array(xs: Sequence[bool]) -> ir.DenseElementsAttr | ir.DenseBoolArrayAttr:\n+  # TODO: b/321794305 - remove this check when jaxlib is on StableHLO API v6 or higher\n+  if hlo.get_api_version() < 6 or xc.mlir_api_version < 55:\n+    return dense_bool_elements(xs)\n+  return ir.DenseBoolArrayAttr.get(xs)\n+\n+        dense_int_array_v6(other_axes))\n+          dense_int_array_v6(broadcast_dimensions),\n+          dense_int_array_v6(broadcast_dimensions))\n+        dense_int_array_v6(window_dimensions),\n+        window_strides=dense_int_array_v6(window_strides),\n+        base_dilations=dense_int_array_v6(base_dilation),\n+        window_dilations=dense_int_array_v6(window_dilation),\n",
            "whole_hunk": "@@ -82,10 +82,17 @@ def dense_int_elements(xs) -> ir.DenseIntElementsAttr:\n   return ir.DenseIntElementsAttr.get(np.asarray(xs, np.int64))\n \n def dense_int_array(xs) -> ir.DenseIntElementsAttr | ir.DenseI64ArrayAttr:\n+  # TODO: b/321794305 - remove this check when jaxlib is on StableHLO API v5 or higher\n   if hlo.get_api_version() < 5:\n     return dense_int_elements(xs)\n   return ir.DenseI64ArrayAttr.get(np.asarray(xs, np.int64))\n \n+# TODO: b/321794305 - delete this when jaxlib is on StableHLO API v6 or higher\n+def dense_int_array_v6(xs) -> ir.DenseIntElementsAttr | ir.DenseI64ArrayAttr:\n+  if hlo.get_api_version() < 6 or xc.mlir_api_version < 55:\n+    return dense_int_elements(xs)\n+  return ir.DenseI64ArrayAttr.get(np.asarray(xs, np.int64))\n+\n def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr:\n   a = np.packbits(np.array(xs, np.bool_), bitorder='little')\n   # TODO(b/209005197): Work around for MLIR crash for non-splat single element\n@@ -95,6 +102,12 @@ def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr:\n   return ir.DenseElementsAttr.get(\n       a, type=ir.IntegerType.get_signless(1), shape=[len(xs)])\n \n+def dense_bool_array(xs: Sequence[bool]) -> ir.DenseElementsAttr | ir.DenseBoolArrayAttr:\n+  # TODO: b/321794305 - remove this check when jaxlib is on StableHLO API v6 or higher\n+  if hlo.get_api_version() < 6 or xc.mlir_api_version < 55:\n+    return dense_bool_elements(xs)\n+  return ir.DenseBoolArrayAttr.get(xs)\n+\n def i32_attr(i): return ir.IntegerAttr.get(ir.IntegerType.get_signless(32), i)\n def i64_attr(i): return ir.IntegerAttr.get(ir.IntegerType.get_signless(64), i)\n \n@@ -304,7 +317,7 @@ def _ndarray_constant_handler(val: np.ndarray) -> Sequence[ir.Value]:\n         ir.RankedTensorType.get(\n             val.shape, dtype_to_ir_type(collapsed_val.dtype)),\n         _numpy_array_constant(collapsed_val)[0],\n-        dense_int_elements(other_axes))\n+        dense_int_array_v6(other_axes))\n     return (out,)\n   else:\n     return _numpy_array_constant(val)\n@@ -1890,14 +1903,14 @@ def broadcast_in_dim(ctx: LoweringRuleContext, op, aval_out: core.AbstractValue,\n       return hlo.dynamic_broadcast_in_dim(\n           aval_to_ir_type(aval_out), op,\n           shape,\n-          dense_int_elements(broadcast_dimensions),\n+          dense_int_array_v6(broadcast_dimensions),\n       )\n     else:\n       assert all(d != ir.ShapedType.get_dynamic_size()\n                  for d in aval_out.shape), aval_out  # type: ignore\n       return hlo.broadcast_in_dim(\n           aval_to_ir_type(aval_out), op,\n-          dense_int_elements(broadcast_dimensions))\n+          dense_int_array_v6(broadcast_dimensions))\n \n def multi_broadcast_in_dim(ctx: LoweringRuleContext,\n                            ops: Sequence[ir.Value],\n@@ -2716,10 +2729,10 @@ def reduce_window(\n     rw = hlo.ReduceWindowOp(\n         list(map(aval_to_ir_type, out_avals)),\n         operands, init_values,\n-        dense_int_elements(window_dimensions),\n-        window_strides=dense_int_elements(window_strides),\n-        base_dilations=dense_int_elements(base_dilation),\n-        window_dilations=dense_int_elements(window_dilation),\n+        dense_int_array_v6(window_dimensions),\n+        window_strides=dense_int_array_v6(window_strides),\n+        base_dilations=dense_int_array_v6(base_dilation),\n+        window_dilations=dense_int_array_v6(window_dilation),\n         padding=ir.DenseIntElementsAttr.get(np.asarray(padding, np.int64),\n                                             shape=(len(padding), 2)))\n     reducer = rw.regions[0].blocks.append(*(scalar_types + scalar_types))\n"
        },
        {
            "name": "convolution.py",
            "path": "jax/_src/lax/convolution.py",
            "patches": [
                {
                    "old_start": 702,
                    "old_length": 7,
                    "new_start": 702,
                    "new_length": 7,
                    "hunk": "@@ -702,7 +702,7 @@ def _conv_general_dilated_lower(\n   num_spatial_dims = len(rhs_spec) - 2\n   if len(padding) == 0:\n     padding = np.zeros((0, 2), dtype=np.int64)\n-  window_reversal = mlir.dense_bool_elements([False] * num_spatial_dims)\n+  window_reversal = mlir.dense_bool_array([False] * num_spatial_dims)\n   if (not core.is_constant_shape(window_strides) or\n       not core.is_constant_shape(lhs_dilation) or\n       not core.is_constant_shape(rhs_dilation) or\n"
                },
                {
                    "old_start": 719,
                    "old_length": 10,
                    "new_start": 719,
                    "new_length": 10,
                    "hunk": "@@ -719,10 +719,10 @@ def _conv_general_dilated_lower(\n           dimension_numbers=dnums,\n           feature_group_count=mlir.i64_attr(feature_group_count),\n           batch_group_count=mlir.i64_attr(batch_group_count),\n-          window_strides=mlir.dense_int_elements(window_strides),\n+          window_strides=mlir.dense_int_array_v6(window_strides),\n           padding=mlir.dense_int_elements(padding),\n-          lhs_dilation=mlir.dense_int_elements(lhs_dilation),\n-          rhs_dilation=mlir.dense_int_elements(rhs_dilation),\n+          lhs_dilation=mlir.dense_int_array_v6(lhs_dilation),\n+          rhs_dilation=mlir.dense_int_array_v6(rhs_dilation),\n           window_reversal=window_reversal,\n           precision_config=lax.precision_attr(precision))\n     ]\n"
                },
                {
                    "old_start": 744,
                    "old_length": 9,
                    "new_start": 744,
                    "new_length": 9,
                    "hunk": "@@ -744,9 +744,9 @@ def _conv_general_dilated_lower(\n           dimension_numbers=dnums,\n           feature_group_count=mlir.i64_attr(feature_group_count),\n           batch_group_count=mlir.i64_attr(batch_group_count),\n-          window_strides=mlir.dense_int_elements(window_strides),\n-          lhs_dilation=mlir.dense_int_elements(lhs_dilation),\n-          rhs_dilation=mlir.dense_int_elements(rhs_dilation),\n+          window_strides=mlir.dense_int_array_v6(window_strides),\n+          lhs_dilation=mlir.dense_int_array_v6(lhs_dilation),\n+          rhs_dilation=mlir.dense_int_array_v6(rhs_dilation),\n           window_reversal=window_reversal,\n           precision_config=lax.precision_attr(precision))\n     ]\n"
                }
            ],
            "whole_deleted": "-  window_reversal = mlir.dense_bool_elements([False] * num_spatial_dims)\n-          window_strides=mlir.dense_int_elements(window_strides),\n-          lhs_dilation=mlir.dense_int_elements(lhs_dilation),\n-          rhs_dilation=mlir.dense_int_elements(rhs_dilation),\n-          window_strides=mlir.dense_int_elements(window_strides),\n-          lhs_dilation=mlir.dense_int_elements(lhs_dilation),\n-          rhs_dilation=mlir.dense_int_elements(rhs_dilation),\n",
            "whole_added": "+  window_reversal = mlir.dense_bool_array([False] * num_spatial_dims)\n+          window_strides=mlir.dense_int_array_v6(window_strides),\n+          lhs_dilation=mlir.dense_int_array_v6(lhs_dilation),\n+          rhs_dilation=mlir.dense_int_array_v6(rhs_dilation),\n+          window_strides=mlir.dense_int_array_v6(window_strides),\n+          lhs_dilation=mlir.dense_int_array_v6(lhs_dilation),\n+          rhs_dilation=mlir.dense_int_array_v6(rhs_dilation),\n",
            "whole_hunk": "@@ -702,7 +702,7 @@ def _conv_general_dilated_lower(\n   num_spatial_dims = len(rhs_spec) - 2\n   if len(padding) == 0:\n     padding = np.zeros((0, 2), dtype=np.int64)\n-  window_reversal = mlir.dense_bool_elements([False] * num_spatial_dims)\n+  window_reversal = mlir.dense_bool_array([False] * num_spatial_dims)\n   if (not core.is_constant_shape(window_strides) or\n       not core.is_constant_shape(lhs_dilation) or\n       not core.is_constant_shape(rhs_dilation) or\n@@ -719,10 +719,10 @@ def _conv_general_dilated_lower(\n           dimension_numbers=dnums,\n           feature_group_count=mlir.i64_attr(feature_group_count),\n           batch_group_count=mlir.i64_attr(batch_group_count),\n-          window_strides=mlir.dense_int_elements(window_strides),\n+          window_strides=mlir.dense_int_array_v6(window_strides),\n           padding=mlir.dense_int_elements(padding),\n-          lhs_dilation=mlir.dense_int_elements(lhs_dilation),\n-          rhs_dilation=mlir.dense_int_elements(rhs_dilation),\n+          lhs_dilation=mlir.dense_int_array_v6(lhs_dilation),\n+          rhs_dilation=mlir.dense_int_array_v6(rhs_dilation),\n           window_reversal=window_reversal,\n           precision_config=lax.precision_attr(precision))\n     ]\n@@ -744,9 +744,9 @@ def _conv_general_dilated_lower(\n           dimension_numbers=dnums,\n           feature_group_count=mlir.i64_attr(feature_group_count),\n           batch_group_count=mlir.i64_attr(batch_group_count),\n-          window_strides=mlir.dense_int_elements(window_strides),\n-          lhs_dilation=mlir.dense_int_elements(lhs_dilation),\n-          rhs_dilation=mlir.dense_int_elements(rhs_dilation),\n+          window_strides=mlir.dense_int_array_v6(window_strides),\n+          lhs_dilation=mlir.dense_int_array_v6(lhs_dilation),\n+          rhs_dilation=mlir.dense_int_array_v6(rhs_dilation),\n           window_reversal=window_reversal,\n           precision_config=lax.precision_attr(precision))\n     ]\n"
        },
        {
            "name": "lax.py",
            "path": "jax/_src/lax/lax.py",
            "patches": [
                {
                    "old_start": 1700,
                    "old_length": 7,
                    "new_start": 1700,
                    "new_length": 7,
                    "hunk": "@@ -1700,7 +1700,7 @@ def broadcast_hlo(\n   for aval, arg in zip(avals, args):\n     if aval.shape != aval_out.shape:\n       assert len(aval.shape) <= len(aval_out.shape), (aval, aval_out)\n-      dims = mlir.dense_int_elements(\n+      dims = mlir.dense_int_array_v6(\n           range(len(aval_out.shape) - len(aval.shape), len(aval_out.shape)))\n       if any(isinstance(d, ir.Value) for d in aval_out.shape):\n         arg = hlo.dynamic_broadcast_in_dim(\n"
                },
                {
                    "old_start": 3801,
                    "old_length": 7,
                    "new_start": 3801,
                    "new_length": 7,
                    "hunk": "@@ -3801,7 +3801,7 @@ def _reduce_lower(ctx, *values, computation, jaxpr, dimensions):\n   operands, init_values = util.split_list(values, [len(values) // 2])\n   init_value_avals = ctx.avals_in[len(values) // 2:]\n   op = hlo.ReduceOp([mlir.aval_to_ir_type(aval) for aval in ctx.avals_out],\n-                    operands, init_values, mlir.dense_int_elements(dimensions))\n+                    operands, init_values, mlir.dense_int_array_v6(dimensions))\n   ir_types = [mlir.aval_to_ir_type(aval) for aval in init_value_avals]\n   reducer = op.regions[0].blocks.append(*(ir_types + ir_types))\n   with ir.InsertionPoint(reducer):\n"
                },
                {
                    "old_start": 4000,
                    "old_length": 7,
                    "new_start": 4000,
                    "new_length": 7,
                    "hunk": "@@ -4000,7 +4000,7 @@ def _unary_reduce_lower(reducer, unit_factory, ctx, x, *, axes):\n   dtype = aval_out.dtype\n   op = hlo.ReduceOp([mlir.aval_to_ir_type(aval_out)], [x],\n                     mlir.ir_constants(unit_factory(aval_out.dtype)),\n-                    mlir.dense_int_elements(axes))\n+                    mlir.dense_int_array_v6(axes))\n   scalar_type = mlir.aval_to_ir_type(core.ShapedArray((), dtype))\n   reducer_region = op.regions[0].blocks.append(scalar_type, scalar_type)\n   with ir.InsertionPoint(reducer_region):\n"
                }
            ],
            "whole_deleted": "-      dims = mlir.dense_int_elements(\n-                    operands, init_values, mlir.dense_int_elements(dimensions))\n-                    mlir.dense_int_elements(axes))\n",
            "whole_added": "+      dims = mlir.dense_int_array_v6(\n+                    operands, init_values, mlir.dense_int_array_v6(dimensions))\n+                    mlir.dense_int_array_v6(axes))\n",
            "whole_hunk": "@@ -1700,7 +1700,7 @@ def broadcast_hlo(\n   for aval, arg in zip(avals, args):\n     if aval.shape != aval_out.shape:\n       assert len(aval.shape) <= len(aval_out.shape), (aval, aval_out)\n-      dims = mlir.dense_int_elements(\n+      dims = mlir.dense_int_array_v6(\n           range(len(aval_out.shape) - len(aval.shape), len(aval_out.shape)))\n       if any(isinstance(d, ir.Value) for d in aval_out.shape):\n         arg = hlo.dynamic_broadcast_in_dim(\n@@ -3801,7 +3801,7 @@ def _reduce_lower(ctx, *values, computation, jaxpr, dimensions):\n   operands, init_values = util.split_list(values, [len(values) // 2])\n   init_value_avals = ctx.avals_in[len(values) // 2:]\n   op = hlo.ReduceOp([mlir.aval_to_ir_type(aval) for aval in ctx.avals_out],\n-                    operands, init_values, mlir.dense_int_elements(dimensions))\n+                    operands, init_values, mlir.dense_int_array_v6(dimensions))\n   ir_types = [mlir.aval_to_ir_type(aval) for aval in init_value_avals]\n   reducer = op.regions[0].blocks.append(*(ir_types + ir_types))\n   with ir.InsertionPoint(reducer):\n@@ -4000,7 +4000,7 @@ def _unary_reduce_lower(reducer, unit_factory, ctx, x, *, axes):\n   dtype = aval_out.dtype\n   op = hlo.ReduceOp([mlir.aval_to_ir_type(aval_out)], [x],\n                     mlir.ir_constants(unit_factory(aval_out.dtype)),\n-                    mlir.dense_int_elements(axes))\n+                    mlir.dense_int_array_v6(axes))\n   scalar_type = mlir.aval_to_ir_type(core.ShapedArray((), dtype))\n   reducer_region = op.regions[0].blocks.append(scalar_type, scalar_type)\n   with ir.InsertionPoint(reducer_region):\n"
        },
        {
            "name": "parallel.py",
            "path": "jax/_src/lax/parallel.py",
            "patches": [
                {
                    "old_start": 1184,
                    "old_length": 7,
                    "new_start": 1184,
                    "new_length": 7,
                    "hunk": "@@ -1184,7 +1184,7 @@ def _all_gather_lowering(ctx, x, *, all_gather_dimension, axis_name,\n     broadcast_dimensions = [i for i in range(len(new_shape)) if i != all_gather_dimension]\n     x = hlo.broadcast_in_dim(\n         mlir.aval_to_ir_type(x_aval.update(shape=new_shape)), x,\n-        mlir.dense_int_elements(broadcast_dimensions))\n+        mlir.dense_int_array_v6(broadcast_dimensions))\n   replica_groups = _replica_groups(ctx.module_context.axis_env, axis_name,\n                                     axis_index_groups)\n   if is_spmd:\n"
                }
            ],
            "whole_deleted": "-        mlir.dense_int_elements(broadcast_dimensions))\n",
            "whole_added": "+        mlir.dense_int_array_v6(broadcast_dimensions))\n",
            "whole_hunk": "@@ -1184,7 +1184,7 @@ def _all_gather_lowering(ctx, x, *, all_gather_dimension, axis_name,\n     broadcast_dimensions = [i for i in range(len(new_shape)) if i != all_gather_dimension]\n     x = hlo.broadcast_in_dim(\n         mlir.aval_to_ir_type(x_aval.update(shape=new_shape)), x,\n-        mlir.dense_int_elements(broadcast_dimensions))\n+        mlir.dense_int_array_v6(broadcast_dimensions))\n   replica_groups = _replica_groups(ctx.module_context.axis_env, axis_name,\n                                     axis_index_groups)\n   if is_spmd:\n"
        },
        {
            "name": "slicing.py",
            "path": "jax/_src/lax/slicing.py",
            "patches": [
                {
                    "old_start": 1845,
                    "old_length": 7,
                    "new_start": 1845,
                    "new_length": 7,
                    "hunk": "@@ -1845,7 +1845,7 @@ def _gather_lower(ctx, operand, indices, *,\n         operand,\n         indices,\n         dnums,\n-        mlir.dense_int_elements(slice_sizes),\n+        mlir.dense_int_array_v6(slice_sizes),\n         indices_are_sorted=ir.BoolAttr.get(indices_are_sorted))]\n \n mlir.register_lowering(gather_p, _gather_lower)\n"
                }
            ],
            "whole_deleted": "-        mlir.dense_int_elements(slice_sizes),\n",
            "whole_added": "+        mlir.dense_int_array_v6(slice_sizes),\n",
            "whole_hunk": "@@ -1845,7 +1845,7 @@ def _gather_lower(ctx, operand, indices, *,\n         operand,\n         indices,\n         dnums,\n-        mlir.dense_int_elements(slice_sizes),\n+        mlir.dense_int_array_v6(slice_sizes),\n         indices_are_sorted=ir.BoolAttr.get(indices_are_sorted))]\n \n mlir.register_lowering(gather_p, _gather_lower)\n"
        },
        {
            "name": "windowed_reductions.py",
            "path": "jax/_src/lax/windowed_reductions.py",
            "patches": [
                {
                    "old_start": 520,
                    "old_length": 8,
                    "new_start": 520,
                    "new_length": 8,
                    "hunk": "@@ -520,8 +520,8 @@ def _select_and_scatter_lower(\n       operand,\n       source,\n       init_value,\n-      window_dimensions=mlir.dense_int_elements(window_dimensions),\n-      window_strides=mlir.dense_int_elements(window_strides),\n+      window_dimensions=mlir.dense_int_array_v6(window_dimensions),\n+      window_strides=mlir.dense_int_array_v6(window_strides),\n       padding=ir.DenseIntElementsAttr.get(np.asarray(padding, np.int64),\n                                           shape=(len(padding), 2)))\n   select = op.select.blocks.append(scalar_type, scalar_type)\n"
                }
            ],
            "whole_deleted": "-      window_dimensions=mlir.dense_int_elements(window_dimensions),\n-      window_strides=mlir.dense_int_elements(window_strides),\n",
            "whole_added": "+      window_dimensions=mlir.dense_int_array_v6(window_dimensions),\n+      window_strides=mlir.dense_int_array_v6(window_strides),\n",
            "whole_hunk": "@@ -520,8 +520,8 @@ def _select_and_scatter_lower(\n       operand,\n       source,\n       init_value,\n-      window_dimensions=mlir.dense_int_elements(window_dimensions),\n-      window_strides=mlir.dense_int_elements(window_strides),\n+      window_dimensions=mlir.dense_int_array_v6(window_dimensions),\n+      window_strides=mlir.dense_int_array_v6(window_strides),\n       padding=ir.DenseIntElementsAttr.get(np.asarray(padding, np.int64),\n                                           shape=(len(padding), 2)))\n   select = op.select.blocks.append(scalar_type, scalar_type)\n"
        },
        {
            "name": "mlir.py",
            "path": "jax/interpreters/mlir.py",
            "patches": [
                {
                    "old_start": 35,
                    "old_length": 7,
                    "new_start": 35,
                    "new_length": 9,
                    "hunk": "@@ -35,7 +35,9 @@ from jax._src.interpreters.mlir import (\n   core_call_lowering as core_call_lowering,\n   custom_call as custom_call,\n   dense_bool_elements as dense_bool_elements,\n+  dense_bool_array as dense_bool_array,\n   dense_int_array as dense_int_array,\n+  dense_int_array_v6 as dense_int_array_v6,\n   dense_int_elements as dense_int_elements,\n   dtype_to_ir_type as dtype_to_ir_type,\n   emit_python_callback as emit_python_callback,\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  dense_bool_array as dense_bool_array,\n+  dense_int_array_v6 as dense_int_array_v6,\n",
            "whole_hunk": "@@ -35,7 +35,9 @@ from jax._src.interpreters.mlir import (\n   core_call_lowering as core_call_lowering,\n   custom_call as custom_call,\n   dense_bool_elements as dense_bool_elements,\n+  dense_bool_array as dense_bool_array,\n   dense_int_array as dense_int_array,\n+  dense_int_array_v6 as dense_int_array_v6,\n   dense_int_elements as dense_int_elements,\n   dtype_to_ir_type as dtype_to_ir_type,\n   emit_python_callback as emit_python_callback,\n"
        },
        {
            "name": "gpu_solver.py",
            "path": "jaxlib/gpu_solver.py",
            "patches": [
                {
                    "old_start": 28,
                    "old_length": 7,
                    "new_start": 28,
                    "new_length": 7,
                    "hunk": "@@ -28,7 +28,7 @@ from jaxlib import xla_client\n \n from .hlo_helpers import (\n     DimensionSize, ShapeTypePair, mk_result_types_and_shapes,\n-    custom_call, ensure_hlo_s32, hlo_s32, dense_int_array)\n+    custom_call, ensure_hlo_s32, hlo_s32, dense_int_array, dense_int_array_v6)\n \n try:\n   from .cuda import _blas as _cublas  # pytype: disable=import-error\n"
                },
                {
                    "old_start": 536,
                    "old_length": 13,
                    "new_start": 536,
                    "new_length": 14,
                    "hunk": "@@ -536,13 +536,14 @@ def _sytrd_hlo(platform, gpu_solver, dtype, a, *, lower):\n   # simply copy it back to where it needs to be:\n   intattr = lambda xs: ir.DenseIntElementsAttr.get(np.asarray(xs, np.int64))\n   intarrattr = lambda xs: dense_int_array(np.asarray(xs, np.int64))\n+  intarrattr_v6 = lambda xs: dense_int_array_v6(np.asarray(xs, np.int64))\n   if not lower and platform == \"cu\" and m > 1:\n     start = (0,) * len(batch_dims) + (0,)\n     end = batch_dims + (1,)\n     s = hlo.slice(\n-        e, intarrattr(start), intarrattr(end),intarrattr([1] * len(start)))\n+        e, intarrattr(start), intarrattr(end), intarrattr([1] * len(start)))\n     s_type = ir.RankedTensorType.get(batch_dims + (1, 1), diag_type)\n-    s = hlo.broadcast_in_dim(s_type, s, intattr(range(len(dims) - 1)))\n+    s = hlo.broadcast_in_dim(s_type, s, intarrattr_v6(range(len(dims) - 1)))\n     # The diagonals are always real; convert to complex if needed.\n     s = hlo.convert(\n         ir.RankedTensorType.get(s_type.shape, a_type.element_type), s)\n"
                }
            ],
            "whole_deleted": "-    custom_call, ensure_hlo_s32, hlo_s32, dense_int_array)\n-        e, intarrattr(start), intarrattr(end),intarrattr([1] * len(start)))\n-    s = hlo.broadcast_in_dim(s_type, s, intattr(range(len(dims) - 1)))\n",
            "whole_added": "+    custom_call, ensure_hlo_s32, hlo_s32, dense_int_array, dense_int_array_v6)\n+  intarrattr_v6 = lambda xs: dense_int_array_v6(np.asarray(xs, np.int64))\n+        e, intarrattr(start), intarrattr(end), intarrattr([1] * len(start)))\n+    s = hlo.broadcast_in_dim(s_type, s, intarrattr_v6(range(len(dims) - 1)))\n",
            "whole_hunk": "@@ -28,7 +28,7 @@ from jaxlib import xla_client\n \n from .hlo_helpers import (\n     DimensionSize, ShapeTypePair, mk_result_types_and_shapes,\n-    custom_call, ensure_hlo_s32, hlo_s32, dense_int_array)\n+    custom_call, ensure_hlo_s32, hlo_s32, dense_int_array, dense_int_array_v6)\n \n try:\n   from .cuda import _blas as _cublas  # pytype: disable=import-error\n@@ -536,13 +536,14 @@ def _sytrd_hlo(platform, gpu_solver, dtype, a, *, lower):\n   # simply copy it back to where it needs to be:\n   intattr = lambda xs: ir.DenseIntElementsAttr.get(np.asarray(xs, np.int64))\n   intarrattr = lambda xs: dense_int_array(np.asarray(xs, np.int64))\n+  intarrattr_v6 = lambda xs: dense_int_array_v6(np.asarray(xs, np.int64))\n   if not lower and platform == \"cu\" and m > 1:\n     start = (0,) * len(batch_dims) + (0,)\n     end = batch_dims + (1,)\n     s = hlo.slice(\n-        e, intarrattr(start), intarrattr(end),intarrattr([1] * len(start)))\n+        e, intarrattr(start), intarrattr(end), intarrattr([1] * len(start)))\n     s_type = ir.RankedTensorType.get(batch_dims + (1, 1), diag_type)\n-    s = hlo.broadcast_in_dim(s_type, s, intattr(range(len(dims) - 1)))\n+    s = hlo.broadcast_in_dim(s_type, s, intarrattr_v6(range(len(dims) - 1)))\n     # The diagonals are always real; convert to complex if needed.\n     s = hlo.convert(\n         ir.RankedTensorType.get(s_type.shape, a_type.element_type), s)\n"
        },
        {
            "name": "hlo_helpers.py",
            "path": "jaxlib/hlo_helpers.py",
            "patches": [
                {
                    "old_start": 111,
                    "old_length": 10,
                    "new_start": 111,
                    "new_length": 17,
                    "hunk": "@@ -111,10 +111,17 @@ def ensure_hlo_s32(x: DimensionSize):\n   return hlo_s32(x) if isinstance(x, int) else x\n \n def dense_int_array(xs) -> ir.DenseIntElementsAttr | ir.DenseI64ArrayAttr:\n+  # TODO: b/321794305 - remove this check when jaxlib is on StableHLO API v5 or higher\n   if hlo.get_api_version() < 5:\n     return ir.DenseIntElementsAttr.get(np.asarray(xs, np.int64))\n   return ir.DenseI64ArrayAttr.get(np.asarray(xs, np.int64))\n \n+# TODO: b/321794305 - delete this when jaxlib is on StableHLO API v6 or higher\n+def dense_int_array_v6(xs) -> ir.DenseIntElementsAttr | ir.DenseI64ArrayAttr:\n+  if hlo.get_api_version() < 6:\n+    return ir.DenseIntElementsAttr.get(np.asarray(xs, np.int64))\n+  return ir.DenseI64ArrayAttr.get(np.asarray(xs, np.int64))\n+\n def hlo_min(x: DimensionSize, y: DimensionSize) -> DimensionSize:\n   if type(x) is int:\n     if type(y) is int:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  # TODO: b/321794305 - remove this check when jaxlib is on StableHLO API v5 or higher\n+# TODO: b/321794305 - delete this when jaxlib is on StableHLO API v6 or higher\n+def dense_int_array_v6(xs) -> ir.DenseIntElementsAttr | ir.DenseI64ArrayAttr:\n+  if hlo.get_api_version() < 6:\n+    return ir.DenseIntElementsAttr.get(np.asarray(xs, np.int64))\n+  return ir.DenseI64ArrayAttr.get(np.asarray(xs, np.int64))\n+\n",
            "whole_hunk": "@@ -111,10 +111,17 @@ def ensure_hlo_s32(x: DimensionSize):\n   return hlo_s32(x) if isinstance(x, int) else x\n \n def dense_int_array(xs) -> ir.DenseIntElementsAttr | ir.DenseI64ArrayAttr:\n+  # TODO: b/321794305 - remove this check when jaxlib is on StableHLO API v5 or higher\n   if hlo.get_api_version() < 5:\n     return ir.DenseIntElementsAttr.get(np.asarray(xs, np.int64))\n   return ir.DenseI64ArrayAttr.get(np.asarray(xs, np.int64))\n \n+# TODO: b/321794305 - delete this when jaxlib is on StableHLO API v6 or higher\n+def dense_int_array_v6(xs) -> ir.DenseIntElementsAttr | ir.DenseI64ArrayAttr:\n+  if hlo.get_api_version() < 6:\n+    return ir.DenseIntElementsAttr.get(np.asarray(xs, np.int64))\n+  return ir.DenseI64ArrayAttr.get(np.asarray(xs, np.int64))\n+\n def hlo_min(x: DimensionSize, y: DimensionSize) -> DimensionSize:\n   if type(x) is int:\n     if type(y) is int:"
        }
    ]
},
{
    "Id": 100,
    "commit_link": "https://github.com/google/jax/commit/55c9ed614cb3228895935a609d9b21f3e77e7838",
    "date": "2024-01-15T14:41:32-08:00",
    "message": "[splash_attention] Fix empty mask corner cases in mask_info construction\n\n* When a mask is completely zero'd out, then return trivial data_next and mask_next tensors\n* When shrinking the launch grid, handle the case in which all the blocks in a row are masked-out. These rows will be completely padded.\n\nPiperOrigin-RevId: 598665385",
    "changes": [
        {
            "name": "splash_attention_mask_info.py",
            "path": "jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_mask_info.py",
            "patches": [
                {
                    "old_start": 244,
                    "old_length": 6,
                    "new_start": 244,
                    "new_length": 10,
                    "hunk": "@@ -244,6 +244,10 @@ def _get_mask_info_for_shard(\n     mask_next = np.zeros(output_shape, dtype=np.int32)\n   data_next = np.zeros(output_shape, dtype=np.int32)\n \n+  # If the mask is completelly zero'd out return freshly initialized outputs.\n+  if not data_coords:\n+    return data_next, mask_next\n+\n   data_coords_iter = iter(data_coords)\n   first_j = coord_j = next(data_coords_iter)\n   if mask_next is not None and mask_coords:\n"
                },
                {
                    "old_start": 777,
                    "old_length": 12,
                    "new_start": 781,
                    "new_length": 13,
                    "hunk": "@@ -777,12 +781,13 @@ def _shrink_mask_info(\n   assert mask_next is None or mask_next.ndim == 3\n \n   head_block_mask = block_mask[0]\n-  non_zero_rows, non_zero_cols = np.nonzero(head_block_mask)\n-  # Group non-zero columns based on which row they belong to.\n-  unique_row_indices = np.unique(non_zero_rows, return_index=True)[1]\n-  grouped_non_zero_cols = np.split(non_zero_cols, unique_row_indices)[1:]\n \n-  assert all(len(x) != 0 for x in grouped_non_zero_cols)\n+  grouped_non_zero_cols = []\n+  # Group non-zero columns based on which row they belong to.\n+  for row_index in range(head_block_mask.shape[0]):\n+    head_block_mask_row = head_block_mask[row_index, :]\n+    non_zero_cols = np.nonzero(head_block_mask_row)[0]\n+    grouped_non_zero_cols.append(non_zero_cols)\n \n   # Pad each row in the non-zero indices to match the width of the longest\n   # row. This avoids having jagged rows."
                }
            ],
            "whole_deleted": "-  non_zero_rows, non_zero_cols = np.nonzero(head_block_mask)\n-  # Group non-zero columns based on which row they belong to.\n-  unique_row_indices = np.unique(non_zero_rows, return_index=True)[1]\n-  grouped_non_zero_cols = np.split(non_zero_cols, unique_row_indices)[1:]\n-  assert all(len(x) != 0 for x in grouped_non_zero_cols)\n",
            "whole_added": "+  # If the mask is completelly zero'd out return freshly initialized outputs.\n+  if not data_coords:\n+    return data_next, mask_next\n+\n+  grouped_non_zero_cols = []\n+  # Group non-zero columns based on which row they belong to.\n+  for row_index in range(head_block_mask.shape[0]):\n+    head_block_mask_row = head_block_mask[row_index, :]\n+    non_zero_cols = np.nonzero(head_block_mask_row)[0]\n+    grouped_non_zero_cols.append(non_zero_cols)\n",
            "whole_hunk": "@@ -244,6 +244,10 @@ def _get_mask_info_for_shard(\n     mask_next = np.zeros(output_shape, dtype=np.int32)\n   data_next = np.zeros(output_shape, dtype=np.int32)\n \n+  # If the mask is completelly zero'd out return freshly initialized outputs.\n+  if not data_coords:\n+    return data_next, mask_next\n+\n   data_coords_iter = iter(data_coords)\n   first_j = coord_j = next(data_coords_iter)\n   if mask_next is not None and mask_coords:\n@@ -777,12 +781,13 @@ def _shrink_mask_info(\n   assert mask_next is None or mask_next.ndim == 3\n \n   head_block_mask = block_mask[0]\n-  non_zero_rows, non_zero_cols = np.nonzero(head_block_mask)\n-  # Group non-zero columns based on which row they belong to.\n-  unique_row_indices = np.unique(non_zero_rows, return_index=True)[1]\n-  grouped_non_zero_cols = np.split(non_zero_cols, unique_row_indices)[1:]\n \n-  assert all(len(x) != 0 for x in grouped_non_zero_cols)\n+  grouped_non_zero_cols = []\n+  # Group non-zero columns based on which row they belong to.\n+  for row_index in range(head_block_mask.shape[0]):\n+    head_block_mask_row = head_block_mask[row_index, :]\n+    non_zero_cols = np.nonzero(head_block_mask_row)[0]\n+    grouped_non_zero_cols.append(non_zero_cols)\n \n   # Pad each row in the non-zero indices to match the width of the longest\n   # row. This avoids having jagged rows."
        }
    ]
},
{
    "Id": 101,
    "commit_link": "https://github.com/google/jax/commit/78b46043b08f6ee8a8e7ba810ad10e7e19841806",
    "date": "2024-01-15T05:44:18-08:00",
    "message": "Decorate jax.nn.initializers.Initializer as @typing.runtime_checkable\n\nWithout this decorator, we get a warning from typeguard:\n\n```\n.../typeguard/_checkers.py:474: UserWarning: Typeguard cannot check the Initializer protocol because it is a non-runtime protocol. If you would like to type check this protocol, please use @typing.runtime_checkable\n```\n\nPiperOrigin-RevId: 598588778",
    "changes": [
        {
            "name": "initializers.py",
            "path": "jax/_src/nn/initializers.py",
            "patches": [
                {
                    "old_start": 21,
                    "old_length": 6,
                    "new_start": 21,
                    "new_length": 7,
                    "hunk": "@@ -21,6 +21,7 @@ from __future__ import annotations\n \n from collections.abc import Sequence\n import math\n+import typing\n from typing import Any, Literal, Protocol\n \n import numpy as np\n"
                },
                {
                    "old_start": 44,
                    "old_length": 6,
                    "new_start": 45,
                    "new_length": 7,
                    "hunk": "@@ -44,6 +45,7 @@ DTypeLikeInexact = Any  # DTypeLikeFloat | DTypeLikeComplex\n RealNumeric = Any  # Scalar jnp array or float\n \n @export\n+@typing.runtime_checkable\n class Initializer(Protocol):\n   @staticmethod\n   def __call__(key: KeyArray,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+import typing\n+@typing.runtime_checkable\n",
            "whole_hunk": "@@ -21,6 +21,7 @@ from __future__ import annotations\n \n from collections.abc import Sequence\n import math\n+import typing\n from typing import Any, Literal, Protocol\n \n import numpy as np\n@@ -44,6 +45,7 @@ DTypeLikeInexact = Any  # DTypeLikeFloat | DTypeLikeComplex\n RealNumeric = Any  # Scalar jnp array or float\n \n @export\n+@typing.runtime_checkable\n class Initializer(Protocol):\n   @staticmethod\n   def __call__(key: KeyArray,"
        }
    ]
},
{
    "Id": 102,
    "commit_link": "https://github.com/google/jax/commit/da96633f1137b1c663a3b95be996ad38566098d9",
    "date": "2024-01-08T14:03:33-08:00",
    "message": "Correct the cache miss metric instrumentation due to the new min cache entry size flag\n\nSince introduction of the min cache entry size check for compilation cache, the cache miss metric overcounts the skipped caches whose sizes are smaller than the min cache entry size. After moving the metric instrumentation to compilation_cache.put_executable_and_time, the cache miss metric will be incremented if both compile time and cache entry size are greater than the minimum thresholds.\n\nPiperOrigin-RevId: 596696013",
    "changes": [
        {
            "name": "BUILD",
            "path": "jax/BUILD",
            "patches": [
                {
                    "old_start": 365,
                    "old_length": 6,
                    "new_start": 365,
                    "new_length": 7,
                    "hunk": "@@ -365,6 +365,7 @@ pytype_strict_library(\n         \":compilation_cache_interface\",\n         \":config\",\n         \":gfile_cache\",\n+        \":monitoring\",\n         \":path\",\n         \"//jax/_src/lib\",\n     ] + py_deps(\"numpy\") + py_deps(\"zstandard\"),\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \":monitoring\",\n",
            "whole_hunk": "@@ -365,6 +365,7 @@ pytype_strict_library(\n         \":compilation_cache_interface\",\n         \":config\",\n         \":gfile_cache\",\n+        \":monitoring\",\n         \":path\",\n         \"//jax/_src/lib\",\n     ] + py_deps(\"numpy\") + py_deps(\"zstandard\"),\n"
        },
        {
            "name": "compilation_cache.py",
            "path": "jax/_src/compilation_cache.py",
            "patches": [
                {
                    "old_start": 29,
                    "old_length": 6,
                    "new_start": 29,
                    "new_length": 7,
                    "hunk": "@@ -29,6 +29,7 @@ except ImportError:\n from jax._src import cache_key\n from jax._src.compilation_cache_interface import CacheInterface\n from jax._src import config\n+from jax._src import monitoring\n from jax._src.gfile_cache import GFileCache\n from jax._src.lib import xla_client\n from jax._src.lib.mlir import ir\n"
                },
                {
                    "old_start": 161,
                    "old_length": 11,
                    "new_start": 162,
                    "new_length": 7,
                    "hunk": "@@ -161,11 +162,7 @@ def put_executable_and_time(\n   if cache is None:\n     logger.debug(\"put_executable_and_time: cache is disabled/not initialized\")\n     return\n-  logger.debug(\n-      \"Writing %s to persistent compilation cache with key %s.\",\n-      module_name,\n-      cache_key,\n-  )\n+\n   serialized_executable = backend.serialize_executable(executable)\n   executable_and_time = combine_executable_and_time(\n       serialized_executable, compile_time)\n"
                },
                {
                    "old_start": 186,
                    "old_length": 6,
                    "new_start": 183,
                    "new_length": 12,
                    "hunk": "@@ -186,6 +183,12 @@ def put_executable_and_time(\n         min_entry_size,\n     )\n   else:\n+    logger.debug(\n+        \"Writing %s to persistent compilation cache with key %s.\",\n+        module_name,\n+        cache_key\n+    )\n+    monitoring.record_event('/jax/compilation_cache/cache_misses')\n     cache.put(cache_key, executable_and_time)\n \n \n"
                }
            ],
            "whole_deleted": "-  logger.debug(\n-      \"Writing %s to persistent compilation cache with key %s.\",\n-      module_name,\n-      cache_key,\n-  )\n",
            "whole_added": "+from jax._src import monitoring\n+\n+    logger.debug(\n+        \"Writing %s to persistent compilation cache with key %s.\",\n+        module_name,\n+        cache_key\n+    )\n+    monitoring.record_event('/jax/compilation_cache/cache_misses')\n",
            "whole_hunk": "@@ -29,6 +29,7 @@ except ImportError:\n from jax._src import cache_key\n from jax._src.compilation_cache_interface import CacheInterface\n from jax._src import config\n+from jax._src import monitoring\n from jax._src.gfile_cache import GFileCache\n from jax._src.lib import xla_client\n from jax._src.lib.mlir import ir\n@@ -161,11 +162,7 @@ def put_executable_and_time(\n   if cache is None:\n     logger.debug(\"put_executable_and_time: cache is disabled/not initialized\")\n     return\n-  logger.debug(\n-      \"Writing %s to persistent compilation cache with key %s.\",\n-      module_name,\n-      cache_key,\n-  )\n+\n   serialized_executable = backend.serialize_executable(executable)\n   executable_and_time = combine_executable_and_time(\n       serialized_executable, compile_time)\n@@ -186,6 +183,12 @@ def put_executable_and_time(\n         min_entry_size,\n     )\n   else:\n+    logger.debug(\n+        \"Writing %s to persistent compilation cache with key %s.\",\n+        module_name,\n+        cache_key\n+    )\n+    monitoring.record_event('/jax/compilation_cache/cache_misses')\n     cache.put(cache_key, executable_and_time)\n \n \n"
        },
        {
            "name": "compiler.py",
            "path": "jax/_src/compiler.py",
            "patches": [
                {
                    "old_start": 346,
                    "old_length": 9,
                    "new_start": 346,
                    "new_length": 8,
                    "hunk": "@@ -346,9 +346,8 @@ def _cache_write(cache_key: str,\n     return\n   else:\n     logger.debug(\n-        \"'%s' took at least %.2f seconds to compile (%.2fs), writing persistent\"\n-        \" cache entry\", module_name, min_compile_time, compile_time_secs)\n-    monitoring.record_event('/jax/compilation_cache/cache_misses')\n+        \"'%s' took at least %.2f seconds to compile (%.2fs)\",\n+        module_name, min_compile_time, compile_time_secs)\n \n   try:\n     compilation_cache.put_executable_and_time(\n"
                }
            ],
            "whole_deleted": "-        \"'%s' took at least %.2f seconds to compile (%.2fs), writing persistent\"\n-        \" cache entry\", module_name, min_compile_time, compile_time_secs)\n-    monitoring.record_event('/jax/compilation_cache/cache_misses')\n",
            "whole_added": "+        \"'%s' took at least %.2f seconds to compile (%.2fs)\",\n+        module_name, min_compile_time, compile_time_secs)\n",
            "whole_hunk": "@@ -346,9 +346,8 @@ def _cache_write(cache_key: str,\n     return\n   else:\n     logger.debug(\n-        \"'%s' took at least %.2f seconds to compile (%.2fs), writing persistent\"\n-        \" cache entry\", module_name, min_compile_time, compile_time_secs)\n-    monitoring.record_event('/jax/compilation_cache/cache_misses')\n+        \"'%s' took at least %.2f seconds to compile (%.2fs)\",\n+        module_name, min_compile_time, compile_time_secs)\n \n   try:\n     compilation_cache.put_executable_and_time(\n"
        },
        {
            "name": "compilation_cache_test.py",
            "path": "tests/compilation_cache_test.py",
            "patches": [
                {
                    "old_start": 379,
                    "old_length": 12,
                    "new_start": 379,
                    "new_length": 13,
                    "hunk": "@@ -379,12 +379,13 @@ class CompilationCacheTest(jtu.JaxTestCase):\n         - previous_counts[\"/jax/compilation_cache/compile_requests_use_cache\"],\n         3)\n \n-  @parameterized.parameters(0, 2)\n-  def test_cache_misses_metric(self, min_compile_time_secs):\n+  @parameterized.parameters(0, 1048576)  # 0 byte, 1 MiB\n+  def test_cache_misses_metric(self, min_entry_size):\n     previous_counts = Counter(_counts)\n     with (\n       tempfile.TemporaryDirectory() as tmpdir,\n-      config.persistent_cache_min_compile_time_secs(min_compile_time_secs),\n+      config.persistent_cache_min_compile_time_secs(2),\n+      config.persistent_cache_min_entry_size_bytes(min_entry_size),\n     ):\n       cc.initialize_cache(tmpdir)\n \n"
                },
                {
                    "old_start": 393,
                    "old_length": 10,
                    "new_start": 394,
                    "new_length": 16,
                    "hunk": "@@ -393,10 +394,16 @@ class CompilationCacheTest(jtu.JaxTestCase):\n         jit(lambda x: x + 1)(1)\n         jit(lambda x: x + 2)(1)\n \n-    self.assertEqual(\n-        _counts[\"/jax/compilation_cache/cache_misses\"]\n-        - previous_counts[\"/jax/compilation_cache/cache_misses\"],\n-        2)\n+    if min_entry_size <= 0:\n+      self.assertEqual(\n+          _counts[\"/jax/compilation_cache/cache_misses\"]\n+          - previous_counts[\"/jax/compilation_cache/cache_misses\"],\n+          2)\n+    else:\n+      self.assertEqual(\n+          _counts[\"/jax/compilation_cache/cache_misses\"]\n+          - previous_counts[\"/jax/compilation_cache/cache_misses\"],\n+          0)\n \n   def test_cache_hits_metric(self):\n     previous_counts = Counter(_counts)"
                }
            ],
            "whole_deleted": "-  @parameterized.parameters(0, 2)\n-  def test_cache_misses_metric(self, min_compile_time_secs):\n-      config.persistent_cache_min_compile_time_secs(min_compile_time_secs),\n-    self.assertEqual(\n-        _counts[\"/jax/compilation_cache/cache_misses\"]\n-        - previous_counts[\"/jax/compilation_cache/cache_misses\"],\n-        2)\n",
            "whole_added": "+  @parameterized.parameters(0, 1048576)  # 0 byte, 1 MiB\n+  def test_cache_misses_metric(self, min_entry_size):\n+      config.persistent_cache_min_compile_time_secs(2),\n+      config.persistent_cache_min_entry_size_bytes(min_entry_size),\n+    if min_entry_size <= 0:\n+      self.assertEqual(\n+          _counts[\"/jax/compilation_cache/cache_misses\"]\n+          - previous_counts[\"/jax/compilation_cache/cache_misses\"],\n+          2)\n+    else:\n+      self.assertEqual(\n+          _counts[\"/jax/compilation_cache/cache_misses\"]\n+          - previous_counts[\"/jax/compilation_cache/cache_misses\"],\n+          0)\n",
            "whole_hunk": "@@ -379,12 +379,13 @@ class CompilationCacheTest(jtu.JaxTestCase):\n         - previous_counts[\"/jax/compilation_cache/compile_requests_use_cache\"],\n         3)\n \n-  @parameterized.parameters(0, 2)\n-  def test_cache_misses_metric(self, min_compile_time_secs):\n+  @parameterized.parameters(0, 1048576)  # 0 byte, 1 MiB\n+  def test_cache_misses_metric(self, min_entry_size):\n     previous_counts = Counter(_counts)\n     with (\n       tempfile.TemporaryDirectory() as tmpdir,\n-      config.persistent_cache_min_compile_time_secs(min_compile_time_secs),\n+      config.persistent_cache_min_compile_time_secs(2),\n+      config.persistent_cache_min_entry_size_bytes(min_entry_size),\n     ):\n       cc.initialize_cache(tmpdir)\n \n@@ -393,10 +394,16 @@ class CompilationCacheTest(jtu.JaxTestCase):\n         jit(lambda x: x + 1)(1)\n         jit(lambda x: x + 2)(1)\n \n-    self.assertEqual(\n-        _counts[\"/jax/compilation_cache/cache_misses\"]\n-        - previous_counts[\"/jax/compilation_cache/cache_misses\"],\n-        2)\n+    if min_entry_size <= 0:\n+      self.assertEqual(\n+          _counts[\"/jax/compilation_cache/cache_misses\"]\n+          - previous_counts[\"/jax/compilation_cache/cache_misses\"],\n+          2)\n+    else:\n+      self.assertEqual(\n+          _counts[\"/jax/compilation_cache/cache_misses\"]\n+          - previous_counts[\"/jax/compilation_cache/cache_misses\"],\n+          0)\n \n   def test_cache_hits_metric(self):\n     previous_counts = Counter(_counts)"
        }
    ]
},
{
    "Id": 104,
    "commit_link": "https://github.com/google/jax/commit/cd0e10f29b9983080c9e1a5a4e39a7b16726dda6",
    "date": "2024-01-07T13:18:18+02:00",
    "message": "[shape_poly] Simplify and speed-up the __eq__ functions for symbolic expressions\n\nEquality is used heavily for symbolic expressions because we use them\nas dictionary keys or in sets. Previously, we used a more complete\nand more expensive form of equality where we attempted to prove that\n\"e1 - e2 >= 0\" and \"e1 - e2 <= 0\". This is an overkill and none\nof the tests we have so far rely on this power. Now we just\nnormalize \"e1 - e2\" and if it reduces syntactically to an integer\nwe check if the integer is 0. If the difference does not reduce\nto an integer we say that the expressions are disequal.\n\nThis may possibly change user-visible behavior when it depends\non the outcome of equality comparisons of symbolic dimensions\nin presence of shape polymorphism.",
    "changes": [
        {
            "name": "shape_poly.py",
            "path": "jax/experimental/export/shape_poly.py",
            "patches": [
                {
                    "old_start": 527,
                    "old_length": 14,
                    "new_start": 527,
                    "new_length": 24,
                    "hunk": "@@ -527,14 +527,24 @@ class _DimExpr():\n       return cmp_comparable(s_mon[1], o_mon[1])\n     return cmp_sequence(s_mons, o_mons, cmp_mon)\n \n-  def eq(self, other) -> bool:\n-    lb, ub = _ensure_poly(self - other, \"eq\").bounds()\n-    if lb == ub == 0:\n-      return True\n-    if lb > 0 or ub < 0:\n+  def eq(self, other: _DimExpr) -> bool:\n+    # Equality is used very frequently because expressions are cached. We could\n+    # implement a more precise version based on `(self - other).bounds() = (0, 0)`\n+    # but that would be too expensive. It would also have the unfortunate drawback\n+    # that we cannot then cache `e.bounds()` because hashing invokes equality\n+    # which would lead to infinite recursion.\n+    diff = self - other\n+\n+    # We look for `self - other == k`, and we rely on the fact that when we\n+    # normalize _DimExpr that represent integers as ints.\n+    if is_poly_dim(diff):\n+      # Here we really ought to raise InconclusiveDimensionOperation, but __eq__\n+      # cannot raise exceptions, because it is used indirectly when hashing.\n+      # So, we say that the expressions are disequal, which is really unsound.\n+      # See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#comparison-of-symbolic-dimensions-is-partially-supported\n       return False\n-    # See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#comparison-of-symbolic-dimensions-is-partially-supported\n-    return False\n+\n+    return diff == 0\n \n   def inconclusive_comparison(self, operation: str, op: Any) -> Exception:\n     return InconclusiveDimensionOperation(\n"
                },
                {
                    "old_start": 698,
                    "old_length": 9,
                    "new_start": 708,
                    "new_length": 15,
                    "hunk": "@@ -698,9 +708,15 @@ class _DimExpr():\n       raise InconclusiveDimensionOperation(f\"Symbolic dimension '{self}' used in a context that requires a constant\")\n \n   # We must overload __eq__ and __ne__, or else we get unsound defaults.\n-  __eq__ = eq\n-  def __ne__(self, other) -> bool:\n-    return not self.eq(other)\n+  def __eq__(self, other: Any) -> bool:\n+    if not isinstance(other, _DimExpr) and not core.is_constant_dim(other):\n+      return False\n+    else:\n+      other = _ensure_poly(other, \"eq\")\n+    return self.eq(other)\n+\n+  def __ne__(self, other: Any) -> bool:\n+    return not self.__eq__(other)\n \n   __ge__ = ge\n "
                }
            ],
            "whole_deleted": "-  def eq(self, other) -> bool:\n-    lb, ub = _ensure_poly(self - other, \"eq\").bounds()\n-    if lb == ub == 0:\n-      return True\n-    if lb > 0 or ub < 0:\n-    # See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#comparison-of-symbolic-dimensions-is-partially-supported\n-    return False\n-  __eq__ = eq\n-  def __ne__(self, other) -> bool:\n-    return not self.eq(other)\n",
            "whole_added": "+  def eq(self, other: _DimExpr) -> bool:\n+    # Equality is used very frequently because expressions are cached. We could\n+    # implement a more precise version based on `(self - other).bounds() = (0, 0)`\n+    # but that would be too expensive. It would also have the unfortunate drawback\n+    # that we cannot then cache `e.bounds()` because hashing invokes equality\n+    # which would lead to infinite recursion.\n+    diff = self - other\n+\n+    # We look for `self - other == k`, and we rely on the fact that when we\n+    # normalize _DimExpr that represent integers as ints.\n+    if is_poly_dim(diff):\n+      # Here we really ought to raise InconclusiveDimensionOperation, but __eq__\n+      # cannot raise exceptions, because it is used indirectly when hashing.\n+      # So, we say that the expressions are disequal, which is really unsound.\n+      # See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#comparison-of-symbolic-dimensions-is-partially-supported\n+\n+    return diff == 0\n+  def __eq__(self, other: Any) -> bool:\n+    if not isinstance(other, _DimExpr) and not core.is_constant_dim(other):\n+      return False\n+    else:\n+      other = _ensure_poly(other, \"eq\")\n+    return self.eq(other)\n+\n+  def __ne__(self, other: Any) -> bool:\n+    return not self.__eq__(other)\n",
            "whole_hunk": "@@ -527,14 +527,24 @@ class _DimExpr():\n       return cmp_comparable(s_mon[1], o_mon[1])\n     return cmp_sequence(s_mons, o_mons, cmp_mon)\n \n-  def eq(self, other) -> bool:\n-    lb, ub = _ensure_poly(self - other, \"eq\").bounds()\n-    if lb == ub == 0:\n-      return True\n-    if lb > 0 or ub < 0:\n+  def eq(self, other: _DimExpr) -> bool:\n+    # Equality is used very frequently because expressions are cached. We could\n+    # implement a more precise version based on `(self - other).bounds() = (0, 0)`\n+    # but that would be too expensive. It would also have the unfortunate drawback\n+    # that we cannot then cache `e.bounds()` because hashing invokes equality\n+    # which would lead to infinite recursion.\n+    diff = self - other\n+\n+    # We look for `self - other == k`, and we rely on the fact that when we\n+    # normalize _DimExpr that represent integers as ints.\n+    if is_poly_dim(diff):\n+      # Here we really ought to raise InconclusiveDimensionOperation, but __eq__\n+      # cannot raise exceptions, because it is used indirectly when hashing.\n+      # So, we say that the expressions are disequal, which is really unsound.\n+      # See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#comparison-of-symbolic-dimensions-is-partially-supported\n       return False\n-    # See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#comparison-of-symbolic-dimensions-is-partially-supported\n-    return False\n+\n+    return diff == 0\n \n   def inconclusive_comparison(self, operation: str, op: Any) -> Exception:\n     return InconclusiveDimensionOperation(\n@@ -698,9 +708,15 @@ class _DimExpr():\n       raise InconclusiveDimensionOperation(f\"Symbolic dimension '{self}' used in a context that requires a constant\")\n \n   # We must overload __eq__ and __ne__, or else we get unsound defaults.\n-  __eq__ = eq\n-  def __ne__(self, other) -> bool:\n-    return not self.eq(other)\n+  def __eq__(self, other: Any) -> bool:\n+    if not isinstance(other, _DimExpr) and not core.is_constant_dim(other):\n+      return False\n+    else:\n+      other = _ensure_poly(other, \"eq\")\n+    return self.eq(other)\n+\n+  def __ne__(self, other: Any) -> bool:\n+    return not self.__eq__(other)\n \n   __ge__ = ge\n "
        }
    ]
},
{
    "Id": 105,
    "commit_link": "https://github.com/google/jax/commit/085b23d10db60da138cd881ec08c2b5291abc42f",
    "date": "2024-01-05T08:50:42-08:00",
    "message": "Remove the check for existence of _npy_value before taking the fast path for `__getitem__`. This must have been a remnant of SDA era.\n\nPiperOrigin-RevId: 596005983",
    "changes": [
        {
            "name": "array.py",
            "path": "jax/_src/array.py",
            "patches": [
                {
                    "old_start": 318,
                    "old_length": 17,
                    "new_start": 318,
                    "new_length": 16,
                    "hunk": "@@ -318,17 +318,16 @@ class ArrayImpl(basearray.Array):\n         cidx = (idx,) + (slice(None),) * (len(self.shape) - 1)\n       else:\n         cidx = idx + (slice(None),) * (len(self.shape) - len(idx))\n-      if self._npy_value is None:\n-        indices = tuple(self.sharding.devices_indices_map(self.shape).values())\n-        try:\n-          arr_idx = indices.index(cidx)\n-        except ValueError:\n-          arr_idx = None\n-        if arr_idx is not None:\n-          a = self._arrays[arr_idx]\n-          return ArrayImpl(\n-              a.aval, SingleDeviceSharding(_get_device(a)), [a], committed=False,\n-              _skip_checks=True)\n+      indices = tuple(self.sharding.devices_indices_map(self.shape).values())\n+      try:\n+        arr_idx = indices.index(cidx)\n+      except ValueError:\n+        arr_idx = None\n+      if arr_idx is not None:\n+        a = self._arrays[arr_idx]\n+        return ArrayImpl(\n+            a.aval, SingleDeviceSharding(_get_device(a)), [a], committed=False,\n+            _skip_checks=True)\n       return lax_numpy._rewriting_take(self, idx)\n     else:\n       return lax_numpy._rewriting_take(self, idx)"
                }
            ],
            "whole_deleted": "-      if self._npy_value is None:\n-        indices = tuple(self.sharding.devices_indices_map(self.shape).values())\n-        try:\n-          arr_idx = indices.index(cidx)\n-        except ValueError:\n-          arr_idx = None\n-        if arr_idx is not None:\n-          a = self._arrays[arr_idx]\n-          return ArrayImpl(\n-              a.aval, SingleDeviceSharding(_get_device(a)), [a], committed=False,\n-              _skip_checks=True)\n",
            "whole_added": "+      indices = tuple(self.sharding.devices_indices_map(self.shape).values())\n+      try:\n+        arr_idx = indices.index(cidx)\n+      except ValueError:\n+        arr_idx = None\n+      if arr_idx is not None:\n+        a = self._arrays[arr_idx]\n+        return ArrayImpl(\n+            a.aval, SingleDeviceSharding(_get_device(a)), [a], committed=False,\n+            _skip_checks=True)\n",
            "whole_hunk": "@@ -318,17 +318,16 @@ class ArrayImpl(basearray.Array):\n         cidx = (idx,) + (slice(None),) * (len(self.shape) - 1)\n       else:\n         cidx = idx + (slice(None),) * (len(self.shape) - len(idx))\n-      if self._npy_value is None:\n-        indices = tuple(self.sharding.devices_indices_map(self.shape).values())\n-        try:\n-          arr_idx = indices.index(cidx)\n-        except ValueError:\n-          arr_idx = None\n-        if arr_idx is not None:\n-          a = self._arrays[arr_idx]\n-          return ArrayImpl(\n-              a.aval, SingleDeviceSharding(_get_device(a)), [a], committed=False,\n-              _skip_checks=True)\n+      indices = tuple(self.sharding.devices_indices_map(self.shape).values())\n+      try:\n+        arr_idx = indices.index(cidx)\n+      except ValueError:\n+        arr_idx = None\n+      if arr_idx is not None:\n+        a = self._arrays[arr_idx]\n+        return ArrayImpl(\n+            a.aval, SingleDeviceSharding(_get_device(a)), [a], committed=False,\n+            _skip_checks=True)\n       return lax_numpy._rewriting_take(self, idx)\n     else:\n       return lax_numpy._rewriting_take(self, idx)"
        }
    ]
},
{
    "Id": 106,
    "commit_link": "https://github.com/google/jax/commit/ea66029731413a30f57a15e942b794288c1253a3",
    "date": "2024-01-04T15:17:05-08:00",
    "message": "Introduce min entry size check for compilation cache.\n\nCurrently, the persistent compilation cache has a time\nthreshold: the entry is cached only if the compilation\ntime is less than the threshold. If compilation happens\nto take a while, but the resulting executable is small,\nthere is nothing that prevents caching. This can result\nin a large number of small files in the cache.\n\nIntroduce a size threshold. If the resulting executable's\nsize (after serialization and compression) is less than\nthis threshold, don't cache. This check is in addition to\nthe compilation time check described above.\n\nTesting: new unit test, test workload.\nPiperOrigin-RevId: 595815611",
    "changes": [
        {
            "name": "compilation_cache.py",
            "path": "jax/_src/compilation_cache.py",
            "patches": [
                {
                    "old_start": 28,
                    "old_length": 7,
                    "new_start": 28,
                    "new_length": 7,
                    "hunk": "@@ -28,7 +28,7 @@ except ImportError:\n \n from jax._src import cache_key\n from jax._src.compilation_cache_interface import CacheInterface\n-from jax._src.config import config\n+from jax._src import config\n from jax._src.gfile_cache import GFileCache\n from jax._src.lib import xla_client\n from jax._src.lib.mlir import ir\n"
                },
                {
                    "old_start": 71,
                    "old_length": 11,
                    "new_start": 70,
                    "new_length": 16,
                    "hunk": "@@ -71,11 +70,16 @@ def initialize_cache(path) -> None:\n   Set the path. To take effect, should be called prior to any calls to\n   get_executable_and_time() and put_executable_and_time().\n   \"\"\"\n-  config.update(\"jax_compilation_cache_dir\", path)\n+  config.config.update(\"jax_compilation_cache_dir\", path)\n+\n+\n+def default_min_cache_entry_size() -> int:\n+  \"\"\"Returns the minimum size below which the entry should not be cached.\"\"\"\n+  return 0\n \n \n def _is_cache_enabled() -> bool:\n-  return config.jax_enable_compilation_cache\n+  return config.enable_compilation_cache.value\n \n \n def _initialize_cache() -> None:\n"
                },
                {
                    "old_start": 92,
                    "old_length": 9,
                    "new_start": 96,
                    "new_length": 15,
                    "hunk": "@@ -92,9 +96,15 @@ def _initialize_cache() -> None:\n       logger.debug(\"_initialize_cache: cache is disabled!\")\n       return\n \n+    # Set the minimum cache size entry only if the flag\n+    # --jax_persistent_cache_min_entry_size_bytes has not been set.\n+    if config.persistent_cache_min_entry_size_bytes.value == 0:\n+      config.config.update(\"jax_persistent_cache_min_entry_size_bytes\",\n+                           default_min_cache_entry_size())\n+\n     global _cache\n     assert _cache is None, \"The cache has already been initialized!\"\n-    path: str = config.jax_compilation_cache_dir\n+    path: str = config.compilation_cache_dir.value\n     # If the path is not set, the cache will not be enabled.\n     if not path:\n       return\n"
                },
                {
                    "old_start": 164,
                    "old_length": 7,
                    "new_start": 174,
                    "new_length": 19,
                    "hunk": "@@ -164,7 +174,19 @@ def put_executable_and_time(\n     executable_and_time = compressor.compress(executable_and_time)\n   else:\n     executable_and_time = zlib.compress(executable_and_time)\n-  cache.put(cache_key, executable_and_time)\n+\n+  min_entry_size = config.persistent_cache_min_entry_size_bytes.value\n+  entry_size = len(executable_and_time)\n+  if entry_size < min_entry_size:\n+    logger.info(\n+        \"Not writing cache entry with key %s since its size (%d bytes) \"\n+        \"is less than threshold (%d bytes)\",\n+        cache_key,\n+        entry_size,\n+        min_entry_size,\n+    )\n+  else:\n+    cache.put(cache_key, executable_and_time)\n \n \n def get_cache_key(module: ir.Module, devices: np.ndarray, compile_options,\n"
                }
            ],
            "whole_deleted": "-from jax._src.config import config\n-  config.update(\"jax_compilation_cache_dir\", path)\n-  return config.jax_enable_compilation_cache\n-    path: str = config.jax_compilation_cache_dir\n-  cache.put(cache_key, executable_and_time)\n",
            "whole_added": "+from jax._src import config\n+  config.config.update(\"jax_compilation_cache_dir\", path)\n+\n+\n+def default_min_cache_entry_size() -> int:\n+  \"\"\"Returns the minimum size below which the entry should not be cached.\"\"\"\n+  return 0\n+  return config.enable_compilation_cache.value\n+    # Set the minimum cache size entry only if the flag\n+    # --jax_persistent_cache_min_entry_size_bytes has not been set.\n+    if config.persistent_cache_min_entry_size_bytes.value == 0:\n+      config.config.update(\"jax_persistent_cache_min_entry_size_bytes\",\n+                           default_min_cache_entry_size())\n+\n+    path: str = config.compilation_cache_dir.value\n+\n+  min_entry_size = config.persistent_cache_min_entry_size_bytes.value\n+  entry_size = len(executable_and_time)\n+  if entry_size < min_entry_size:\n+    logger.info(\n+        \"Not writing cache entry with key %s since its size (%d bytes) \"\n+        \"is less than threshold (%d bytes)\",\n+        cache_key,\n+        entry_size,\n+        min_entry_size,\n+    )\n+  else:\n+    cache.put(cache_key, executable_and_time)\n",
            "whole_hunk": "@@ -28,7 +28,7 @@ except ImportError:\n \n from jax._src import cache_key\n from jax._src.compilation_cache_interface import CacheInterface\n-from jax._src.config import config\n+from jax._src import config\n from jax._src.gfile_cache import GFileCache\n from jax._src.lib import xla_client\n from jax._src.lib.mlir import ir\n@@ -71,11 +70,16 @@ def initialize_cache(path) -> None:\n   Set the path. To take effect, should be called prior to any calls to\n   get_executable_and_time() and put_executable_and_time().\n   \"\"\"\n-  config.update(\"jax_compilation_cache_dir\", path)\n+  config.config.update(\"jax_compilation_cache_dir\", path)\n+\n+\n+def default_min_cache_entry_size() -> int:\n+  \"\"\"Returns the minimum size below which the entry should not be cached.\"\"\"\n+  return 0\n \n \n def _is_cache_enabled() -> bool:\n-  return config.jax_enable_compilation_cache\n+  return config.enable_compilation_cache.value\n \n \n def _initialize_cache() -> None:\n@@ -92,9 +96,15 @@ def _initialize_cache() -> None:\n       logger.debug(\"_initialize_cache: cache is disabled!\")\n       return\n \n+    # Set the minimum cache size entry only if the flag\n+    # --jax_persistent_cache_min_entry_size_bytes has not been set.\n+    if config.persistent_cache_min_entry_size_bytes.value == 0:\n+      config.config.update(\"jax_persistent_cache_min_entry_size_bytes\",\n+                           default_min_cache_entry_size())\n+\n     global _cache\n     assert _cache is None, \"The cache has already been initialized!\"\n-    path: str = config.jax_compilation_cache_dir\n+    path: str = config.compilation_cache_dir.value\n     # If the path is not set, the cache will not be enabled.\n     if not path:\n       return\n@@ -164,7 +174,19 @@ def put_executable_and_time(\n     executable_and_time = compressor.compress(executable_and_time)\n   else:\n     executable_and_time = zlib.compress(executable_and_time)\n-  cache.put(cache_key, executable_and_time)\n+\n+  min_entry_size = config.persistent_cache_min_entry_size_bytes.value\n+  entry_size = len(executable_and_time)\n+  if entry_size < min_entry_size:\n+    logger.info(\n+        \"Not writing cache entry with key %s since its size (%d bytes) \"\n+        \"is less than threshold (%d bytes)\",\n+        cache_key,\n+        entry_size,\n+        min_entry_size,\n+    )\n+  else:\n+    cache.put(cache_key, executable_and_time)\n \n \n def get_cache_key(module: ir.Module, devices: np.ndarray, compile_options,\n"
        },
        {
            "name": "config.py",
            "path": "jax/_src/config.py",
            "patches": [
                {
                    "old_start": 964,
                    "old_length": 6,
                    "new_start": 964,
                    "new_length": 17,
                    "hunk": "@@ -964,6 +964,17 @@ persistent_cache_min_compile_time_secs = define_float_state(\n           'persistent compilation cache. This threshold can be raised to '\n           'decrease the number of entries written to the cache.'))\n \n+persistent_cache_min_entry_size_bytes = define_int_state(\n+    name='jax_persistent_cache_min_entry_size_bytes',\n+    default=0,\n+    help=('The minimum size (in bytes) of an entry that will be cached in the '\n+          'persistent compilation cache: '\n+          '* -1: disable the size restriction and prevent overrides. '\n+          '* Leave at default (0) to allow for overrides. The override will '\n+          '  typically ensure that the minimum size is optimal for the '\n+          '  filesystem being used for the cache. '\n+          '* > 0: the actual minimum size desired; no overrides.'))\n+\n compilation_cache_include_metadata_in_key = define_bool_state(\n     name='jax_compilation_cache_include_metadata_in_key',\n     default=False,\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+persistent_cache_min_entry_size_bytes = define_int_state(\n+    name='jax_persistent_cache_min_entry_size_bytes',\n+    default=0,\n+    help=('The minimum size (in bytes) of an entry that will be cached in the '\n+          'persistent compilation cache: '\n+          '* -1: disable the size restriction and prevent overrides. '\n+          '* Leave at default (0) to allow for overrides. The override will '\n+          '  typically ensure that the minimum size is optimal for the '\n+          '  filesystem being used for the cache. '\n+          '* > 0: the actual minimum size desired; no overrides.'))\n+\n",
            "whole_hunk": "@@ -964,6 +964,17 @@ persistent_cache_min_compile_time_secs = define_float_state(\n           'persistent compilation cache. This threshold can be raised to '\n           'decrease the number of entries written to the cache.'))\n \n+persistent_cache_min_entry_size_bytes = define_int_state(\n+    name='jax_persistent_cache_min_entry_size_bytes',\n+    default=0,\n+    help=('The minimum size (in bytes) of an entry that will be cached in the '\n+          'persistent compilation cache: '\n+          '* -1: disable the size restriction and prevent overrides. '\n+          '* Leave at default (0) to allow for overrides. The override will '\n+          '  typically ensure that the minimum size is optimal for the '\n+          '  filesystem being used for the cache. '\n+          '* > 0: the actual minimum size desired; no overrides.'))\n+\n compilation_cache_include_metadata_in_key = define_bool_state(\n     name='jax_compilation_cache_include_metadata_in_key',\n     default=False,\n"
        },
        {
            "name": "test_util.py",
            "path": "jax/_src/test_util.py",
            "patches": [
                {
                    "old_start": 952,
                    "old_length": 6,
                    "new_start": 952,
                    "new_length": 7,
                    "hunk": "@@ -952,6 +952,7 @@ class JaxTestCase(parameterized.TestCase):\n       stack.enter_context(config.enable_compilation_cache(True))\n       stack.enter_context(config.raise_persistent_cache_errors(True))\n       stack.enter_context(config.persistent_cache_min_compile_time_secs(0))\n+      stack.enter_context(config.persistent_cache_min_entry_size_bytes(0))\n \n       tmp_dir = stack.enter_context(tempfile.TemporaryDirectory())\n       compilation_cache.initialize_cache(tmp_dir)\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+      stack.enter_context(config.persistent_cache_min_entry_size_bytes(0))\n",
            "whole_hunk": "@@ -952,6 +952,7 @@ class JaxTestCase(parameterized.TestCase):\n       stack.enter_context(config.enable_compilation_cache(True))\n       stack.enter_context(config.raise_persistent_cache_errors(True))\n       stack.enter_context(config.persistent_cache_min_compile_time_secs(0))\n+      stack.enter_context(config.persistent_cache_min_entry_size_bytes(0))\n \n       tmp_dir = stack.enter_context(tempfile.TemporaryDirectory())\n       compilation_cache.initialize_cache(tmp_dir)\n"
        },
        {
            "name": "compilation_cache_test.py",
            "path": "tests/compilation_cache_test.py",
            "patches": [
                {
                    "old_start": 62,
                    "old_length": 6,
                    "new_start": 62,
                    "new_length": 7,
                    "hunk": "@@ -62,6 +62,7 @@ def increment_event_count(event):\n     jax_enable_compilation_cache=True,\n     jax_raise_persistent_cache_errors=True,\n     jax_persistent_cache_min_compile_time_secs=0,\n+    jax_persistent_cache_min_entry_size_bytes=0,\n )\n class CompilationCacheTest(jtu.JaxTestCase):\n \n"
                },
                {
                    "old_start": 280,
                    "old_length": 10,
                    "new_start": 281,
                    "new_length": 23,
                    "hunk": "@@ -280,10 +281,23 @@ class CompilationCacheTest(jtu.JaxTestCase):\n             str(w[0].message),\n         )\n \n+  def test_min_entry_size(self):\n+    with (\n+      tempfile.TemporaryDirectory() as tmpdir,\n+      config.persistent_cache_min_compile_time_secs(0),\n+      config.persistent_cache_min_entry_size_bytes(1048576),  # 1MiB\n+    ):\n+      cc.initialize_cache(tmpdir)\n+\n+      jit(lambda x: x + 1)(1)\n+      files_in_cache = len(os.listdir(tmpdir))\n+      self.assertEqual(files_in_cache, 0)\n+\n   def test_min_compile_time(self):\n     with (\n       tempfile.TemporaryDirectory() as tmpdir,\n       config.persistent_cache_min_compile_time_secs(2),\n+      config.persistent_cache_min_entry_size_bytes(0),\n     ):\n       cc.initialize_cache(tmpdir)\n \n"
                },
                {
                    "old_start": 303,
                    "old_length": 6,
                    "new_start": 317,
                    "new_length": 7,
                    "hunk": "@@ -303,6 +317,7 @@ class CompilationCacheTest(jtu.JaxTestCase):\n     with (\n       tempfile.TemporaryDirectory() as tmpdir,\n       config.persistent_cache_min_compile_time_secs(2),\n+      config.persistent_cache_min_entry_size_bytes(0),\n     ):\n       cc.initialize_cache(tmpdir)\n \n"
                },
                {
                    "old_start": 388,
                    "old_length": 6,
                    "new_start": 403,
                    "new_length": 7,
                    "hunk": "@@ -388,6 +403,7 @@ class CompilationCacheTest(jtu.JaxTestCase):\n     with (\n       tempfile.TemporaryDirectory() as tmpdir,\n       config.persistent_cache_min_compile_time_secs(2),\n+      config.persistent_cache_min_entry_size_bytes(0),\n     ):\n       cc.initialize_cache(tmpdir)\n \n"
                },
                {
                    "old_start": 405,
                    "old_length": 6,
                    "new_start": 421,
                    "new_length": 7,
                    "hunk": "@@ -405,6 +421,7 @@ class CompilationCacheTest(jtu.JaxTestCase):\n @jtu.with_config(\n     jax_enable_compilation_cache=False,\n     jax_persistent_cache_min_compile_time_secs=0,\n+    jax_persistent_cache_min_entry_size_bytes=0,\n )\n class CompilationCacheDisabledTest(jtu.JaxTestCase):\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    jax_persistent_cache_min_entry_size_bytes=0,\n+  def test_min_entry_size(self):\n+    with (\n+      tempfile.TemporaryDirectory() as tmpdir,\n+      config.persistent_cache_min_compile_time_secs(0),\n+      config.persistent_cache_min_entry_size_bytes(1048576),  # 1MiB\n+    ):\n+      cc.initialize_cache(tmpdir)\n+\n+      jit(lambda x: x + 1)(1)\n+      files_in_cache = len(os.listdir(tmpdir))\n+      self.assertEqual(files_in_cache, 0)\n+\n+      config.persistent_cache_min_entry_size_bytes(0),\n+      config.persistent_cache_min_entry_size_bytes(0),\n+      config.persistent_cache_min_entry_size_bytes(0),\n+    jax_persistent_cache_min_entry_size_bytes=0,\n",
            "whole_hunk": "@@ -62,6 +62,7 @@ def increment_event_count(event):\n     jax_enable_compilation_cache=True,\n     jax_raise_persistent_cache_errors=True,\n     jax_persistent_cache_min_compile_time_secs=0,\n+    jax_persistent_cache_min_entry_size_bytes=0,\n )\n class CompilationCacheTest(jtu.JaxTestCase):\n \n@@ -280,10 +281,23 @@ class CompilationCacheTest(jtu.JaxTestCase):\n             str(w[0].message),\n         )\n \n+  def test_min_entry_size(self):\n+    with (\n+      tempfile.TemporaryDirectory() as tmpdir,\n+      config.persistent_cache_min_compile_time_secs(0),\n+      config.persistent_cache_min_entry_size_bytes(1048576),  # 1MiB\n+    ):\n+      cc.initialize_cache(tmpdir)\n+\n+      jit(lambda x: x + 1)(1)\n+      files_in_cache = len(os.listdir(tmpdir))\n+      self.assertEqual(files_in_cache, 0)\n+\n   def test_min_compile_time(self):\n     with (\n       tempfile.TemporaryDirectory() as tmpdir,\n       config.persistent_cache_min_compile_time_secs(2),\n+      config.persistent_cache_min_entry_size_bytes(0),\n     ):\n       cc.initialize_cache(tmpdir)\n \n@@ -303,6 +317,7 @@ class CompilationCacheTest(jtu.JaxTestCase):\n     with (\n       tempfile.TemporaryDirectory() as tmpdir,\n       config.persistent_cache_min_compile_time_secs(2),\n+      config.persistent_cache_min_entry_size_bytes(0),\n     ):\n       cc.initialize_cache(tmpdir)\n \n@@ -388,6 +403,7 @@ class CompilationCacheTest(jtu.JaxTestCase):\n     with (\n       tempfile.TemporaryDirectory() as tmpdir,\n       config.persistent_cache_min_compile_time_secs(2),\n+      config.persistent_cache_min_entry_size_bytes(0),\n     ):\n       cc.initialize_cache(tmpdir)\n \n@@ -405,6 +421,7 @@ class CompilationCacheTest(jtu.JaxTestCase):\n @jtu.with_config(\n     jax_enable_compilation_cache=False,\n     jax_persistent_cache_min_compile_time_secs=0,\n+    jax_persistent_cache_min_entry_size_bytes=0,\n )\n class CompilationCacheDisabledTest(jtu.JaxTestCase):\n "
        }
    ]
},
{
    "Id": 107,
    "commit_link": "https://github.com/google/jax/commit/a678015c7428722a5edf3427b99d00f2049fce6c",
    "date": "2024-01-02T08:51:56-08:00",
    "message": "Implement a stable serialization API for Mosaic\n\nThis lets us break a dependency on standard MLIR dialects while serializing\nthe program into HLO. The scheme is simple: we make a lightweight lazy fork\nof existing dialects by mangling the dialect name and otherwise keeping the\nstructure of the ops identical. This keeps serialization and deserialization\nsimple, for as long as the upstream dialects don't change much. If they do,\nwe have to increment our version counter and write rules that update the IR\nstructure.\n\nNote that this scheme only protects us from changes such as changing the\nattributes annotating the ops (renaming, etc.). However, it doesn't protect\nus from the attributes defined by a dialect from changing. Still, as far as\nI can tell, the only attributes we depend on are enums (which are simply\nplain integer attributes, so we can remap their values) and affine maps\n(that are unlikely to change much, I hope).\n\nThis does not actually wire up the pass yet, as we are currently reorganizing\nthe Python/C++ boundary significantly. The integration should be completed\nonce that works is done.\n\nPiperOrigin-RevId: 595128374",
    "changes": [
        {
            "name": "tpu.td",
            "path": "jaxlib/mosaic/dialect/tpu/tpu.td",
            "patches": [
                {
                    "old_start": 487,
                    "old_length": 6,
                    "new_start": 487,
                    "new_length": 10,
                    "hunk": "@@ -487,6 +487,10 @@ def DebugAssertInsertionPass : Pass<\"debug-assert-insertion\", \"::mlir::func::Fun\n   let constructor = \"::mlir::tpu::createDebugAssertInsertionPass()\";\n }\n \n+def MosaicSerdePass : Pass<\"mosaic-serde\", \"::mlir::ModuleOp\"> {\n+  let options = [Option<\"serialize\", \"serialize\", \"bool\", \"\", \"\">];\n+}\n+\n def LogicalToPhysicalDeviceIdPass : Pass<\"logical-to-physical-device-id\", \"::mlir::func::FuncOp\"> {\n   let dependentDialects = [\n     \"::mlir::func::FuncDialect\",\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+def MosaicSerdePass : Pass<\"mosaic-serde\", \"::mlir::ModuleOp\"> {\n+  let options = [Option<\"serialize\", \"serialize\", \"bool\", \"\", \"\">];\n+}\n+\n",
            "whole_hunk": "@@ -487,6 +487,10 @@ def DebugAssertInsertionPass : Pass<\"debug-assert-insertion\", \"::mlir::func::Fun\n   let constructor = \"::mlir::tpu::createDebugAssertInsertionPass()\";\n }\n \n+def MosaicSerdePass : Pass<\"mosaic-serde\", \"::mlir::ModuleOp\"> {\n+  let options = [Option<\"serialize\", \"serialize\", \"bool\", \"\", \"\">];\n+}\n+\n def LogicalToPhysicalDeviceIdPass : Pass<\"logical-to-physical-device-id\", \"::mlir::func::FuncOp\"> {\n   let dependentDialects = [\n     \"::mlir::func::FuncDialect\",\n"
        },
        {
            "name": "tpu_dialect.h",
            "path": "jaxlib/mosaic/dialect/tpu/tpu_dialect.h",
            "patches": [
                {
                    "old_start": 23,
                    "old_length": 6,
                    "new_start": 23,
                    "new_length": 7,
                    "hunk": "@@ -23,6 +23,7 @@ limitations under the License.\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/Pass/Pass.h\"\n+#include \"mlir/include/mlir/IR/BuiltinOps.h\"\n #include \"mlir/include/mlir/IR/BuiltinTypes.h\"\n #include \"mlir/include/mlir/IR/Value.h\"\n #include \"mlir/include/mlir/Support/LogicalResult.h\"\n"
                },
                {
                    "old_start": 65,
                    "old_length": 6,
                    "new_start": 66,
                    "new_length": 9,
                    "hunk": "@@ -65,6 +66,9 @@ std::unique_ptr<OperationPass<func::FuncOp>> createLinalgVectorizationPass();\n \n std::unique_ptr<OperationPass<func::FuncOp>> createDebugAssertInsertionPass();\n \n+#define GEN_PASS_DECL_MOSAICSERDEPASS\n+#include \"jaxlib/mosaic/dialect/tpu/tpu_passes.h.inc\"\n+\n // Changes the memory space of the value and propagates it through the program.\n LogicalResult specializeMemorySpace(TypedValue<MemRefType> value,\n                                     MemorySpace memory_space);\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include \"mlir/include/mlir/IR/BuiltinOps.h\"\n+#define GEN_PASS_DECL_MOSAICSERDEPASS\n+#include \"jaxlib/mosaic/dialect/tpu/tpu_passes.h.inc\"\n+\n",
            "whole_hunk": "@@ -23,6 +23,7 @@ limitations under the License.\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/Pass/Pass.h\"\n+#include \"mlir/include/mlir/IR/BuiltinOps.h\"\n #include \"mlir/include/mlir/IR/BuiltinTypes.h\"\n #include \"mlir/include/mlir/IR/Value.h\"\n #include \"mlir/include/mlir/Support/LogicalResult.h\"\n@@ -65,6 +66,9 @@ std::unique_ptr<OperationPass<func::FuncOp>> createLinalgVectorizationPass();\n \n std::unique_ptr<OperationPass<func::FuncOp>> createDebugAssertInsertionPass();\n \n+#define GEN_PASS_DECL_MOSAICSERDEPASS\n+#include \"jaxlib/mosaic/dialect/tpu/tpu_passes.h.inc\"\n+\n // Changes the memory space of the value and propagates it through the program.\n LogicalResult specializeMemorySpace(TypedValue<MemRefType> value,\n                                     MemorySpace memory_space);\n"
        },
        {
            "name": "serde.cc",
            "path": "jaxlib/mosaic/dialect/tpu/transforms/serde.cc",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 132,
                    "hunk": "@@ -0,0 +1,132 @@\n+/* Copyright 2023 The JAX Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// We need to keep some extra headers for the code in tpu_passes.h.inc.\n+\n+#include <memory>  // IWYU pragma: keep\n+#include <optional>\n+#include <string>\n+#include <string_view>\n+\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/OperationSupport.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/IR/Visitors.h\"\n+#include \"mlir/Pass/Pass.h\"  // IWYU pragma: keep\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/include/mlir/IR/OperationSupport.h\"\n+\n+namespace mlir::tpu {\n+\n+#define GEN_PASS_DECL_MOSAICSERDEPASS\n+#define GEN_PASS_DEF_MOSAICSERDEPASS\n+#include \"jaxlib/mosaic/dialect/tpu/tpu_passes.h.inc\"\n+\n+namespace {\n+\n+constexpr std::string_view kMangledDialect = \"stable_mosaic.\";\n+constexpr StringRef kVersionAttrName = \"stable_mosaic.version\";\n+constexpr int kVersion = 1;\n+\n+StringRef mangle(StringRef name, std::string* storage) {\n+  storage->clear();\n+  storage->reserve(kMangledDialect.size() + name.size());\n+  storage->insert(storage->end(), kMangledDialect.begin(),\n+                  kMangledDialect.end());\n+  storage->insert(storage->end(), name.begin(), name.end());\n+  return *storage;\n+}\n+\n+std::optional<StringRef> demangle(StringRef name) {\n+  if (!name.starts_with(kMangledDialect)) {\n+    return std::nullopt;\n+  }\n+  return name.drop_front(kMangledDialect.size());\n+}\n+\n+struct MosaicSerdePass : public impl::MosaicSerdePassBase<MosaicSerdePass> {\n+  using Base::Base;\n+\n+  void runOnOperation() override {\n+    ModuleOp module = getOperation();\n+    if (serialize && !module->getContext()->allowsUnregisteredDialects()) {\n+      module.emitError() << \"Cannot serialize within a context that does not \"\n+                            \"allow unregistered dialects.\";\n+      signalPassFailure();\n+      return;\n+    }\n+    if (serialize) {\n+      module->setAttr(\n+          kVersionAttrName,\n+          IntegerAttr::get(IntegerType::get(module->getContext(), 64),\n+                           kVersion));\n+    } else {\n+      IntegerAttr version_attr =\n+          module->getAttrOfType<IntegerAttr>(kVersionAttrName);\n+      if (!version_attr) {\n+        module->emitError(\"Missing or invalid Mosaic version attribute\");\n+        signalPassFailure();\n+        return;\n+      }\n+      if (version_attr.getValue() != kVersion) {\n+        module->emitError(\"Unsupported Mosaic version: \")\n+            << version_attr.getValue().getSExtValue();\n+        signalPassFailure();\n+        return;\n+      }\n+      module->removeAttr(kVersionAttrName);\n+    }\n+    std::string name_storage;\n+    auto result = module.walk([this, &name_storage](Operation* op) {\n+      if (isa<ModuleOp>(op)) {  // Don't mangle the ModuleOp itself.\n+        return WalkResult::advance();\n+      }\n+      std::optional<OperationName> new_name;\n+      if (serialize) {\n+        auto new_name_str = mangle(op->getName().getStringRef(), &name_storage);\n+        new_name = OperationName(new_name_str, op->getContext());\n+      } else {\n+        if (auto demangled = demangle(op->getName().getStringRef())) {\n+          auto new_name_str = *demangled;\n+          if (auto registered = RegisteredOperationName::lookup(\n+                  new_name_str, op->getContext())) {\n+            new_name = *registered;\n+          } else {\n+            new_name = OperationName(new_name_str, op->getContext());\n+          }\n+        } else {\n+          op->emitError(\"Operation not in a serialized form\");\n+          return WalkResult::interrupt();\n+        }\n+      }\n+      auto new_op = Operation::create(\n+          op->getLoc(), *new_name, op->getResultTypes(), op->getOperands(),\n+          op->getAttrs(), nullptr, op->getSuccessors(), op->getRegions());\n+      op->getBlock()->getOperations().insertAfter(Block::iterator(op), new_op);\n+      op->replaceAllUsesWith(new_op->getResults());\n+      op->erase();\n+      return WalkResult::advance();\n+    });\n+    if (result.wasInterrupted()) {\n+      signalPassFailure();\n+      return;\n+    }\n+  }\n+};\n+\n+}  // namespace\n+\n+}  // namespace mlir::tpu"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+/* Copyright 2023 The JAX Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// We need to keep some extra headers for the code in tpu_passes.h.inc.\n+\n+#include <memory>  // IWYU pragma: keep\n+#include <optional>\n+#include <string>\n+#include <string_view>\n+\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/OperationSupport.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/IR/Visitors.h\"\n+#include \"mlir/Pass/Pass.h\"  // IWYU pragma: keep\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/include/mlir/IR/OperationSupport.h\"\n+\n+namespace mlir::tpu {\n+\n+#define GEN_PASS_DECL_MOSAICSERDEPASS\n+#define GEN_PASS_DEF_MOSAICSERDEPASS\n+#include \"jaxlib/mosaic/dialect/tpu/tpu_passes.h.inc\"\n+\n+namespace {\n+\n+constexpr std::string_view kMangledDialect = \"stable_mosaic.\";\n+constexpr StringRef kVersionAttrName = \"stable_mosaic.version\";\n+constexpr int kVersion = 1;\n+\n+StringRef mangle(StringRef name, std::string* storage) {\n+  storage->clear();\n+  storage->reserve(kMangledDialect.size() + name.size());\n+  storage->insert(storage->end(), kMangledDialect.begin(),\n+                  kMangledDialect.end());\n+  storage->insert(storage->end(), name.begin(), name.end());\n+  return *storage;\n+}\n+\n+std::optional<StringRef> demangle(StringRef name) {\n+  if (!name.starts_with(kMangledDialect)) {\n+    return std::nullopt;\n+  }\n+  return name.drop_front(kMangledDialect.size());\n+}\n+\n+struct MosaicSerdePass : public impl::MosaicSerdePassBase<MosaicSerdePass> {\n+  using Base::Base;\n+\n+  void runOnOperation() override {\n+    ModuleOp module = getOperation();\n+    if (serialize && !module->getContext()->allowsUnregisteredDialects()) {\n+      module.emitError() << \"Cannot serialize within a context that does not \"\n+                            \"allow unregistered dialects.\";\n+      signalPassFailure();\n+      return;\n+    }\n+    if (serialize) {\n+      module->setAttr(\n+          kVersionAttrName,\n+          IntegerAttr::get(IntegerType::get(module->getContext(), 64),\n+                           kVersion));\n+    } else {\n+      IntegerAttr version_attr =\n+          module->getAttrOfType<IntegerAttr>(kVersionAttrName);\n+      if (!version_attr) {\n+        module->emitError(\"Missing or invalid Mosaic version attribute\");\n+        signalPassFailure();\n+        return;\n+      }\n+      if (version_attr.getValue() != kVersion) {\n+        module->emitError(\"Unsupported Mosaic version: \")\n+            << version_attr.getValue().getSExtValue();\n+        signalPassFailure();\n+        return;\n+      }\n+      module->removeAttr(kVersionAttrName);\n+    }\n+    std::string name_storage;\n+    auto result = module.walk([this, &name_storage](Operation* op) {\n+      if (isa<ModuleOp>(op)) {  // Don't mangle the ModuleOp itself.\n+        return WalkResult::advance();\n+      }\n+      std::optional<OperationName> new_name;\n+      if (serialize) {\n+        auto new_name_str = mangle(op->getName().getStringRef(), &name_storage);\n+        new_name = OperationName(new_name_str, op->getContext());\n+      } else {\n+        if (auto demangled = demangle(op->getName().getStringRef())) {\n+          auto new_name_str = *demangled;\n+          if (auto registered = RegisteredOperationName::lookup(\n+                  new_name_str, op->getContext())) {\n+            new_name = *registered;\n+          } else {\n+            new_name = OperationName(new_name_str, op->getContext());\n+          }\n+        } else {\n+          op->emitError(\"Operation not in a serialized form\");\n+          return WalkResult::interrupt();\n+        }\n+      }\n+      auto new_op = Operation::create(\n+          op->getLoc(), *new_name, op->getResultTypes(), op->getOperands(),\n+          op->getAttrs(), nullptr, op->getSuccessors(), op->getRegions());\n+      op->getBlock()->getOperations().insertAfter(Block::iterator(op), new_op);\n+      op->replaceAllUsesWith(new_op->getResults());\n+      op->erase();\n+      return WalkResult::advance();\n+    });\n+    if (result.wasInterrupted()) {\n+      signalPassFailure();\n+      return;\n+    }\n+  }\n+};\n+\n+}  // namespace\n+\n+}  // namespace mlir::tpu\n",
            "whole_hunk": "@@ -0,0 +1,132 @@\n+/* Copyright 2023 The JAX Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// We need to keep some extra headers for the code in tpu_passes.h.inc.\n+\n+#include <memory>  // IWYU pragma: keep\n+#include <optional>\n+#include <string>\n+#include <string_view>\n+\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/OperationSupport.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/IR/Visitors.h\"\n+#include \"mlir/Pass/Pass.h\"  // IWYU pragma: keep\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/include/mlir/IR/OperationSupport.h\"\n+\n+namespace mlir::tpu {\n+\n+#define GEN_PASS_DECL_MOSAICSERDEPASS\n+#define GEN_PASS_DEF_MOSAICSERDEPASS\n+#include \"jaxlib/mosaic/dialect/tpu/tpu_passes.h.inc\"\n+\n+namespace {\n+\n+constexpr std::string_view kMangledDialect = \"stable_mosaic.\";\n+constexpr StringRef kVersionAttrName = \"stable_mosaic.version\";\n+constexpr int kVersion = 1;\n+\n+StringRef mangle(StringRef name, std::string* storage) {\n+  storage->clear();\n+  storage->reserve(kMangledDialect.size() + name.size());\n+  storage->insert(storage->end(), kMangledDialect.begin(),\n+                  kMangledDialect.end());\n+  storage->insert(storage->end(), name.begin(), name.end());\n+  return *storage;\n+}\n+\n+std::optional<StringRef> demangle(StringRef name) {\n+  if (!name.starts_with(kMangledDialect)) {\n+    return std::nullopt;\n+  }\n+  return name.drop_front(kMangledDialect.size());\n+}\n+\n+struct MosaicSerdePass : public impl::MosaicSerdePassBase<MosaicSerdePass> {\n+  using Base::Base;\n+\n+  void runOnOperation() override {\n+    ModuleOp module = getOperation();\n+    if (serialize && !module->getContext()->allowsUnregisteredDialects()) {\n+      module.emitError() << \"Cannot serialize within a context that does not \"\n+                            \"allow unregistered dialects.\";\n+      signalPassFailure();\n+      return;\n+    }\n+    if (serialize) {\n+      module->setAttr(\n+          kVersionAttrName,\n+          IntegerAttr::get(IntegerType::get(module->getContext(), 64),\n+                           kVersion));\n+    } else {\n+      IntegerAttr version_attr =\n+          module->getAttrOfType<IntegerAttr>(kVersionAttrName);\n+      if (!version_attr) {\n+        module->emitError(\"Missing or invalid Mosaic version attribute\");\n+        signalPassFailure();\n+        return;\n+      }\n+      if (version_attr.getValue() != kVersion) {\n+        module->emitError(\"Unsupported Mosaic version: \")\n+            << version_attr.getValue().getSExtValue();\n+        signalPassFailure();\n+        return;\n+      }\n+      module->removeAttr(kVersionAttrName);\n+    }\n+    std::string name_storage;\n+    auto result = module.walk([this, &name_storage](Operation* op) {\n+      if (isa<ModuleOp>(op)) {  // Don't mangle the ModuleOp itself.\n+        return WalkResult::advance();\n+      }\n+      std::optional<OperationName> new_name;\n+      if (serialize) {\n+        auto new_name_str = mangle(op->getName().getStringRef(), &name_storage);\n+        new_name = OperationName(new_name_str, op->getContext());\n+      } else {\n+        if (auto demangled = demangle(op->getName().getStringRef())) {\n+          auto new_name_str = *demangled;\n+          if (auto registered = RegisteredOperationName::lookup(\n+                  new_name_str, op->getContext())) {\n+            new_name = *registered;\n+          } else {\n+            new_name = OperationName(new_name_str, op->getContext());\n+          }\n+        } else {\n+          op->emitError(\"Operation not in a serialized form\");\n+          return WalkResult::interrupt();\n+        }\n+      }\n+      auto new_op = Operation::create(\n+          op->getLoc(), *new_name, op->getResultTypes(), op->getOperands(),\n+          op->getAttrs(), nullptr, op->getSuccessors(), op->getRegions());\n+      op->getBlock()->getOperations().insertAfter(Block::iterator(op), new_op);\n+      op->replaceAllUsesWith(new_op->getResults());\n+      op->erase();\n+      return WalkResult::advance();\n+    });\n+    if (result.wasInterrupted()) {\n+      signalPassFailure();\n+      return;\n+    }\n+  }\n+};\n+\n+}  // namespace\n+\n+}  // namespace mlir::tpu"
        }
    ]
},
{
    "Id": 108,
    "commit_link": "https://github.com/google/jax/commit/0419e014f1e0a7970f3afbb9bef10fc477c84496",
    "date": "2024-01-02T03:19:35-08:00",
    "message": "[Mosaic] Add a pass to check operation invariants on-device\n\nThis lets us easily catch things such as out-of-bounds loads\nor reference slices (leading to OOB DMAs or loads downstream).\n\nPiperOrigin-RevId: 595072511",
    "changes": [
        {
            "name": "tpu_custom_call.py",
            "path": "jax/_src/tpu_custom_call.py",
            "patches": [
                {
                    "old_start": 51,
                    "old_length": 6,
                    "new_start": 51,
                    "new_length": 14,
                    "hunk": "@@ -51,6 +51,14 @@ _MOSAIC_USE_CPP_PASSES = config.define_bool_state(\n         \" passes (still a WIP)\"\n     ),\n )\n+_MOSAIC_ON_DEVICE_CHECKS = config.define_string_state(\n+    name=\"mosaic_on_device_checks\",\n+    default=\"\",\n+    help=(\n+        \"If True, additional on-device asserts are inserted into the program,\"\n+        \" to verify operation invariants (accesses in-bounds, etc.)\"\n+    ),\n+)\n \n tpu = tpu_mosaic.tpu\n apply_vector_layout = tpu_mosaic.apply_vector_layout\n"
                },
                {
                    "old_start": 327,
                    "old_length": 6,
                    "new_start": 335,
                    "new_length": 24,
                    "hunk": "@@ -327,6 +335,24 @@ def _lower_tpu_kernel(\n       pipeline = [\n           \"canonicalize\",\n           \"cse\",\n+      ]\n+      pipeline = PassManager.parse(f\"builtin.module({','.join(pipeline)})\")\n+      _run_pass_pipeline(pipeline, module, \"post-simplify\")\n+\n+      if checks := _MOSAIC_ON_DEVICE_CHECKS.value:\n+        checks = set(checks.split(\",\"))\n+        if checks == {\"bounds\"}:  # We only support one kind of checks now.\n+          pipeline = PassManager.parse(\n+              \"builtin.module(func.func(debug-assert-insertion))\"\n+          )\n+          _run_pass_pipeline(pipeline, module, \"post-assert-insertion\")\n+        elif checks:\n+          checks.discard(\"bounds\")\n+          raise ValueError(\n+              f\"Unrecognized on-device check categories: {', '.join(checks)}\"\n+          )\n+\n+      pipeline = [\n           \"func.func(tpu-infer-vector-layout{sublane-count=8 lane-count=128})\",\n       ]\n       pipeline = PassManager.parse(f\"builtin.module({','.join(pipeline)})\")\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+_MOSAIC_ON_DEVICE_CHECKS = config.define_string_state(\n+    name=\"mosaic_on_device_checks\",\n+    default=\"\",\n+    help=(\n+        \"If True, additional on-device asserts are inserted into the program,\"\n+        \" to verify operation invariants (accesses in-bounds, etc.)\"\n+    ),\n+)\n+      ]\n+      pipeline = PassManager.parse(f\"builtin.module({','.join(pipeline)})\")\n+      _run_pass_pipeline(pipeline, module, \"post-simplify\")\n+\n+      if checks := _MOSAIC_ON_DEVICE_CHECKS.value:\n+        checks = set(checks.split(\",\"))\n+        if checks == {\"bounds\"}:  # We only support one kind of checks now.\n+          pipeline = PassManager.parse(\n+              \"builtin.module(func.func(debug-assert-insertion))\"\n+          )\n+          _run_pass_pipeline(pipeline, module, \"post-assert-insertion\")\n+        elif checks:\n+          checks.discard(\"bounds\")\n+          raise ValueError(\n+              f\"Unrecognized on-device check categories: {', '.join(checks)}\"\n+          )\n+\n+      pipeline = [\n",
            "whole_hunk": "@@ -51,6 +51,14 @@ _MOSAIC_USE_CPP_PASSES = config.define_bool_state(\n         \" passes (still a WIP)\"\n     ),\n )\n+_MOSAIC_ON_DEVICE_CHECKS = config.define_string_state(\n+    name=\"mosaic_on_device_checks\",\n+    default=\"\",\n+    help=(\n+        \"If True, additional on-device asserts are inserted into the program,\"\n+        \" to verify operation invariants (accesses in-bounds, etc.)\"\n+    ),\n+)\n \n tpu = tpu_mosaic.tpu\n apply_vector_layout = tpu_mosaic.apply_vector_layout\n@@ -327,6 +335,24 @@ def _lower_tpu_kernel(\n       pipeline = [\n           \"canonicalize\",\n           \"cse\",\n+      ]\n+      pipeline = PassManager.parse(f\"builtin.module({','.join(pipeline)})\")\n+      _run_pass_pipeline(pipeline, module, \"post-simplify\")\n+\n+      if checks := _MOSAIC_ON_DEVICE_CHECKS.value:\n+        checks = set(checks.split(\",\"))\n+        if checks == {\"bounds\"}:  # We only support one kind of checks now.\n+          pipeline = PassManager.parse(\n+              \"builtin.module(func.func(debug-assert-insertion))\"\n+          )\n+          _run_pass_pipeline(pipeline, module, \"post-assert-insertion\")\n+        elif checks:\n+          checks.discard(\"bounds\")\n+          raise ValueError(\n+              f\"Unrecognized on-device check categories: {', '.join(checks)}\"\n+          )\n+\n+      pipeline = [\n           \"func.func(tpu-infer-vector-layout{sublane-count=8 lane-count=128})\",\n       ]\n       pipeline = PassManager.parse(f\"builtin.module({','.join(pipeline)})\")\n"
        },
        {
            "name": "tpu.td",
            "path": "jaxlib/mosaic/dialect/tpu/tpu.td",
            "patches": [
                {
                    "old_start": 476,
                    "old_length": 6,
                    "new_start": 476,
                    "new_length": 16,
                    "hunk": "@@ -476,6 +476,16 @@ def TPU_MaskCastOp : TPU_Op<\"mask_cast\", [Pure]> {\n   let hasVerifier = 1;\n }\n \n+def DebugAssertInsertionPass : Pass<\"debug-assert-insertion\", \"::mlir::func::FuncOp\"> {\n+  let dependentDialects = [\n+    \"::mlir::func::FuncDialect\",\n+    \"::mlir::arith::ArithDialect\",\n+    \"::mlir::cf::ControlFlowDialect\",\n+    \"::mlir::vector::VectorDialect\",\n+    \"::mlir::tpu::TPUDialect\",\n+  ];\n+  let constructor = \"::mlir::tpu::createDebugAssertInsertionPass()\";\n+}\n \n def LogicalToPhysicalDeviceIdPass : Pass<\"logical-to-physical-device-id\", \"::mlir::func::FuncOp\"> {\n   let dependentDialects = [\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+def DebugAssertInsertionPass : Pass<\"debug-assert-insertion\", \"::mlir::func::FuncOp\"> {\n+  let dependentDialects = [\n+    \"::mlir::func::FuncDialect\",\n+    \"::mlir::arith::ArithDialect\",\n+    \"::mlir::cf::ControlFlowDialect\",\n+    \"::mlir::vector::VectorDialect\",\n+    \"::mlir::tpu::TPUDialect\",\n+  ];\n+  let constructor = \"::mlir::tpu::createDebugAssertInsertionPass()\";\n+}\n",
            "whole_hunk": "@@ -476,6 +476,16 @@ def TPU_MaskCastOp : TPU_Op<\"mask_cast\", [Pure]> {\n   let hasVerifier = 1;\n }\n \n+def DebugAssertInsertionPass : Pass<\"debug-assert-insertion\", \"::mlir::func::FuncOp\"> {\n+  let dependentDialects = [\n+    \"::mlir::func::FuncDialect\",\n+    \"::mlir::arith::ArithDialect\",\n+    \"::mlir::cf::ControlFlowDialect\",\n+    \"::mlir::vector::VectorDialect\",\n+    \"::mlir::tpu::TPUDialect\",\n+  ];\n+  let constructor = \"::mlir::tpu::createDebugAssertInsertionPass()\";\n+}\n \n def LogicalToPhysicalDeviceIdPass : Pass<\"logical-to-physical-device-id\", \"::mlir::func::FuncOp\"> {\n   let dependentDialects = [\n"
        },
        {
            "name": "tpu_dialect.h",
            "path": "jaxlib/mosaic/dialect/tpu/tpu_dialect.h",
            "patches": [
                {
                    "old_start": 63,
                    "old_length": 6,
                    "new_start": 63,
                    "new_length": 8,
                    "hunk": "@@ -63,6 +63,8 @@ createLogicalToPhysicalDeviceIdPass(int64_t total_devices);\n \n std::unique_ptr<OperationPass<func::FuncOp>> createLinalgVectorizationPass();\n \n+std::unique_ptr<OperationPass<func::FuncOp>> createDebugAssertInsertionPass();\n+\n // Changes the memory space of the value and propagates it through the program.\n LogicalResult specializeMemorySpace(TypedValue<MemRefType> value,\n                                     MemorySpace memory_space);\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+std::unique_ptr<OperationPass<func::FuncOp>> createDebugAssertInsertionPass();\n+\n",
            "whole_hunk": "@@ -63,6 +63,8 @@ createLogicalToPhysicalDeviceIdPass(int64_t total_devices);\n \n std::unique_ptr<OperationPass<func::FuncOp>> createLinalgVectorizationPass();\n \n+std::unique_ptr<OperationPass<func::FuncOp>> createDebugAssertInsertionPass();\n+\n // Changes the memory space of the value and propagates it through the program.\n LogicalResult specializeMemorySpace(TypedValue<MemRefType> value,\n                                     MemorySpace memory_space);\n"
        },
        {
            "name": "debug_assert_insertion.cc",
            "path": "jaxlib/mosaic/dialect/tpu/transforms/debug_assert_insertion.cc",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 142,
                    "hunk": "@@ -0,0 +1,142 @@\n+/* Copyright 2023 The JAX Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cstdint>\n+#include <functional>\n+#include <memory>\n+#include <string>\n+\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/StringMap.h\"\n+#include \"llvm/Support/raw_ostream.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n+#include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/ImplicitLocOpBuilder.h\"\n+#include \"mlir/IR/Types.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/IR/ValueRange.h\"\n+#include \"mlir/IR/Visitors.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"jaxlib/mosaic/dialect/tpu/tpu_dialect.h\"\n+\n+namespace mlir::tpu {\n+\n+#define GEN_PASS_DECL_DEBUGASSERTINSERTIONPASS\n+#define GEN_PASS_DEF_DEBUGASSERTINSERTIONPASS\n+#include \"jaxlib/mosaic/dialect/tpu/tpu_passes.h.inc\"\n+\n+namespace {\n+\n+using rule_type = std::function<void(Operation *)>;\n+\n+template <typename Op>\n+rule_type as_generic_rule(void (*rule)(Op)) {\n+  return [rule](const Operation *op) { return rule(cast<Op>(op)); };\n+}\n+\n+void assertIsValidSubwindow(Operation *op, mlir::ValueRange base_indices,\n+                            ArrayRef<int64_t> window_shape,\n+                            ArrayRef<int64_t> full_shape) {\n+  if (base_indices.size() != window_shape.size() ||\n+      base_indices.size() != full_shape.size()) {\n+    return;  // Malformed op.\n+  }\n+  if (base_indices.empty()) {\n+    return;\n+  }\n+  Type idx_type = base_indices.front().getType();\n+  ImplicitLocOpBuilder builder(op->getLoc(), op);\n+  for (auto [dim, access] :\n+       llvm::enumerate(llvm::zip(base_indices, window_shape, full_shape))) {\n+    auto [idx, size, bound] = access;\n+    Value positive = builder.create<arith::CmpIOp>(\n+        arith::CmpIPredicate::sge, idx,\n+        builder.create<arith::ConstantOp>(builder.getIntegerAttr(idx_type, 0)));\n+    Value in_bounds = builder.create<arith::CmpIOp>(\n+        arith::CmpIPredicate::sle,\n+        builder.create<arith::AddIOp>(\n+            idx, builder.create<arith::ConstantOp>(\n+                     builder.getIntegerAttr(idx_type, size))),\n+        builder.create<arith::ConstantOp>(\n+            builder.getIntegerAttr(idx_type, bound)));\n+    std::string msg;\n+    llvm::raw_string_ostream msg_builder(msg);\n+    msg_builder << \"Operation \" << op->getName().getStringRef().str()\n+                << \" references out-of-bounds elements in dimension \"\n+                << std::to_string(dim) << \" (source location: \" << op->getLoc()\n+                << \")\";\n+    builder.create<cf::AssertOp>(\n+        builder.create<arith::AndIOp>(positive, in_bounds), msg);\n+  }\n+}\n+\n+void vector_load_rule(vector::LoadOp op) {\n+  assertIsValidSubwindow(op, op.getIndices(),\n+                         /*window_shape=*/op.getVectorType().getShape(),\n+                         /*full_shape=*/op.getBase().getType().getShape());\n+}\n+\n+void vector_store_rule(vector::StoreOp op) {\n+  assertIsValidSubwindow(op, op.getIndices(),\n+                         /*window_shape=*/op.getVectorType().getShape(),\n+                         /*full_shape=*/op.getBase().getType().getShape());\n+}\n+\n+void tpu_memref_slice_rule(tpu::MemRefSliceOp op) {\n+  assertIsValidSubwindow(op, op.getBaseIdx(),\n+                         /*window_shape=*/op.getResult().getType().getShape(),\n+                         /*full_shape=*/op.getMemRef().getType().getShape());\n+}\n+\n+const llvm::StringMap<rule_type> &rules() {\n+  static auto rules = new llvm::StringMap<rule_type>{\n+      // TODO: tpu::LoadOp, tpu::StoreOp\n+      {vector::LoadOp::getOperationName(), as_generic_rule(vector_load_rule)},\n+      {vector::StoreOp::getOperationName(), as_generic_rule(vector_store_rule)},\n+      {tpu::MemRefSliceOp::getOperationName(),\n+       as_generic_rule(tpu_memref_slice_rule)},\n+  };\n+  return *rules;\n+}\n+\n+struct DebugAssertInsertionPass\n+    : public impl::DebugAssertInsertionPassBase<DebugAssertInsertionPass> {\n+  void runOnOperation() override {\n+    func::FuncOp func = getOperation();\n+    func.walk([](Operation *op) {\n+      if (auto rule_it = rules().find(op->getName().getStringRef());\n+          rule_it != rules().end()) {\n+        const rule_type &rule = rule_it->getValue();\n+        rule(op);\n+      }\n+      return WalkResult::advance();\n+    });\n+  }\n+};\n+\n+}  // namespace\n+\n+std::unique_ptr<OperationPass<func::FuncOp>> createDebugAssertInsertionPass() {\n+  return std::make_unique<DebugAssertInsertionPass>();\n+}\n+\n+}  // namespace mlir::tpu"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+/* Copyright 2023 The JAX Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cstdint>\n+#include <functional>\n+#include <memory>\n+#include <string>\n+\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/StringMap.h\"\n+#include \"llvm/Support/raw_ostream.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n+#include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/ImplicitLocOpBuilder.h\"\n+#include \"mlir/IR/Types.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/IR/ValueRange.h\"\n+#include \"mlir/IR/Visitors.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"jaxlib/mosaic/dialect/tpu/tpu_dialect.h\"\n+\n+namespace mlir::tpu {\n+\n+#define GEN_PASS_DECL_DEBUGASSERTINSERTIONPASS\n+#define GEN_PASS_DEF_DEBUGASSERTINSERTIONPASS\n+#include \"jaxlib/mosaic/dialect/tpu/tpu_passes.h.inc\"\n+\n+namespace {\n+\n+using rule_type = std::function<void(Operation *)>;\n+\n+template <typename Op>\n+rule_type as_generic_rule(void (*rule)(Op)) {\n+  return [rule](const Operation *op) { return rule(cast<Op>(op)); };\n+}\n+\n+void assertIsValidSubwindow(Operation *op, mlir::ValueRange base_indices,\n+                            ArrayRef<int64_t> window_shape,\n+                            ArrayRef<int64_t> full_shape) {\n+  if (base_indices.size() != window_shape.size() ||\n+      base_indices.size() != full_shape.size()) {\n+    return;  // Malformed op.\n+  }\n+  if (base_indices.empty()) {\n+    return;\n+  }\n+  Type idx_type = base_indices.front().getType();\n+  ImplicitLocOpBuilder builder(op->getLoc(), op);\n+  for (auto [dim, access] :\n+       llvm::enumerate(llvm::zip(base_indices, window_shape, full_shape))) {\n+    auto [idx, size, bound] = access;\n+    Value positive = builder.create<arith::CmpIOp>(\n+        arith::CmpIPredicate::sge, idx,\n+        builder.create<arith::ConstantOp>(builder.getIntegerAttr(idx_type, 0)));\n+    Value in_bounds = builder.create<arith::CmpIOp>(\n+        arith::CmpIPredicate::sle,\n+        builder.create<arith::AddIOp>(\n+            idx, builder.create<arith::ConstantOp>(\n+                     builder.getIntegerAttr(idx_type, size))),\n+        builder.create<arith::ConstantOp>(\n+            builder.getIntegerAttr(idx_type, bound)));\n+    std::string msg;\n+    llvm::raw_string_ostream msg_builder(msg);\n+    msg_builder << \"Operation \" << op->getName().getStringRef().str()\n+                << \" references out-of-bounds elements in dimension \"\n+                << std::to_string(dim) << \" (source location: \" << op->getLoc()\n+                << \")\";\n+    builder.create<cf::AssertOp>(\n+        builder.create<arith::AndIOp>(positive, in_bounds), msg);\n+  }\n+}\n+\n+void vector_load_rule(vector::LoadOp op) {\n+  assertIsValidSubwindow(op, op.getIndices(),\n+                         /*window_shape=*/op.getVectorType().getShape(),\n+                         /*full_shape=*/op.getBase().getType().getShape());\n+}\n+\n+void vector_store_rule(vector::StoreOp op) {\n+  assertIsValidSubwindow(op, op.getIndices(),\n+                         /*window_shape=*/op.getVectorType().getShape(),\n+                         /*full_shape=*/op.getBase().getType().getShape());\n+}\n+\n+void tpu_memref_slice_rule(tpu::MemRefSliceOp op) {\n+  assertIsValidSubwindow(op, op.getBaseIdx(),\n+                         /*window_shape=*/op.getResult().getType().getShape(),\n+                         /*full_shape=*/op.getMemRef().getType().getShape());\n+}\n+\n+const llvm::StringMap<rule_type> &rules() {\n+  static auto rules = new llvm::StringMap<rule_type>{\n+      // TODO: tpu::LoadOp, tpu::StoreOp\n+      {vector::LoadOp::getOperationName(), as_generic_rule(vector_load_rule)},\n+      {vector::StoreOp::getOperationName(), as_generic_rule(vector_store_rule)},\n+      {tpu::MemRefSliceOp::getOperationName(),\n+       as_generic_rule(tpu_memref_slice_rule)},\n+  };\n+  return *rules;\n+}\n+\n+struct DebugAssertInsertionPass\n+    : public impl::DebugAssertInsertionPassBase<DebugAssertInsertionPass> {\n+  void runOnOperation() override {\n+    func::FuncOp func = getOperation();\n+    func.walk([](Operation *op) {\n+      if (auto rule_it = rules().find(op->getName().getStringRef());\n+          rule_it != rules().end()) {\n+        const rule_type &rule = rule_it->getValue();\n+        rule(op);\n+      }\n+      return WalkResult::advance();\n+    });\n+  }\n+};\n+\n+}  // namespace\n+\n+std::unique_ptr<OperationPass<func::FuncOp>> createDebugAssertInsertionPass() {\n+  return std::make_unique<DebugAssertInsertionPass>();\n+}\n+\n+}  // namespace mlir::tpu\n",
            "whole_hunk": "@@ -0,0 +1,142 @@\n+/* Copyright 2023 The JAX Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cstdint>\n+#include <functional>\n+#include <memory>\n+#include <string>\n+\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/StringMap.h\"\n+#include \"llvm/Support/raw_ostream.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n+#include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/ImplicitLocOpBuilder.h\"\n+#include \"mlir/IR/Types.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/IR/ValueRange.h\"\n+#include \"mlir/IR/Visitors.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"jaxlib/mosaic/dialect/tpu/tpu_dialect.h\"\n+\n+namespace mlir::tpu {\n+\n+#define GEN_PASS_DECL_DEBUGASSERTINSERTIONPASS\n+#define GEN_PASS_DEF_DEBUGASSERTINSERTIONPASS\n+#include \"jaxlib/mosaic/dialect/tpu/tpu_passes.h.inc\"\n+\n+namespace {\n+\n+using rule_type = std::function<void(Operation *)>;\n+\n+template <typename Op>\n+rule_type as_generic_rule(void (*rule)(Op)) {\n+  return [rule](const Operation *op) { return rule(cast<Op>(op)); };\n+}\n+\n+void assertIsValidSubwindow(Operation *op, mlir::ValueRange base_indices,\n+                            ArrayRef<int64_t> window_shape,\n+                            ArrayRef<int64_t> full_shape) {\n+  if (base_indices.size() != window_shape.size() ||\n+      base_indices.size() != full_shape.size()) {\n+    return;  // Malformed op.\n+  }\n+  if (base_indices.empty()) {\n+    return;\n+  }\n+  Type idx_type = base_indices.front().getType();\n+  ImplicitLocOpBuilder builder(op->getLoc(), op);\n+  for (auto [dim, access] :\n+       llvm::enumerate(llvm::zip(base_indices, window_shape, full_shape))) {\n+    auto [idx, size, bound] = access;\n+    Value positive = builder.create<arith::CmpIOp>(\n+        arith::CmpIPredicate::sge, idx,\n+        builder.create<arith::ConstantOp>(builder.getIntegerAttr(idx_type, 0)));\n+    Value in_bounds = builder.create<arith::CmpIOp>(\n+        arith::CmpIPredicate::sle,\n+        builder.create<arith::AddIOp>(\n+            idx, builder.create<arith::ConstantOp>(\n+                     builder.getIntegerAttr(idx_type, size))),\n+        builder.create<arith::ConstantOp>(\n+            builder.getIntegerAttr(idx_type, bound)));\n+    std::string msg;\n+    llvm::raw_string_ostream msg_builder(msg);\n+    msg_builder << \"Operation \" << op->getName().getStringRef().str()\n+                << \" references out-of-bounds elements in dimension \"\n+                << std::to_string(dim) << \" (source location: \" << op->getLoc()\n+                << \")\";\n+    builder.create<cf::AssertOp>(\n+        builder.create<arith::AndIOp>(positive, in_bounds), msg);\n+  }\n+}\n+\n+void vector_load_rule(vector::LoadOp op) {\n+  assertIsValidSubwindow(op, op.getIndices(),\n+                         /*window_shape=*/op.getVectorType().getShape(),\n+                         /*full_shape=*/op.getBase().getType().getShape());\n+}\n+\n+void vector_store_rule(vector::StoreOp op) {\n+  assertIsValidSubwindow(op, op.getIndices(),\n+                         /*window_shape=*/op.getVectorType().getShape(),\n+                         /*full_shape=*/op.getBase().getType().getShape());\n+}\n+\n+void tpu_memref_slice_rule(tpu::MemRefSliceOp op) {\n+  assertIsValidSubwindow(op, op.getBaseIdx(),\n+                         /*window_shape=*/op.getResult().getType().getShape(),\n+                         /*full_shape=*/op.getMemRef().getType().getShape());\n+}\n+\n+const llvm::StringMap<rule_type> &rules() {\n+  static auto rules = new llvm::StringMap<rule_type>{\n+      // TODO: tpu::LoadOp, tpu::StoreOp\n+      {vector::LoadOp::getOperationName(), as_generic_rule(vector_load_rule)},\n+      {vector::StoreOp::getOperationName(), as_generic_rule(vector_store_rule)},\n+      {tpu::MemRefSliceOp::getOperationName(),\n+       as_generic_rule(tpu_memref_slice_rule)},\n+  };\n+  return *rules;\n+}\n+\n+struct DebugAssertInsertionPass\n+    : public impl::DebugAssertInsertionPassBase<DebugAssertInsertionPass> {\n+  void runOnOperation() override {\n+    func::FuncOp func = getOperation();\n+    func.walk([](Operation *op) {\n+      if (auto rule_it = rules().find(op->getName().getStringRef());\n+          rule_it != rules().end()) {\n+        const rule_type &rule = rule_it->getValue();\n+        rule(op);\n+      }\n+      return WalkResult::advance();\n+    });\n+  }\n+};\n+\n+}  // namespace\n+\n+std::unique_ptr<OperationPass<func::FuncOp>> createDebugAssertInsertionPass() {\n+  return std::make_unique<DebugAssertInsertionPass>();\n+}\n+\n+}  // namespace mlir::tpu"
        }
    ]
},
{
    "Id": 109,
    "commit_link": "https://github.com/google/jax/commit/1044c4f9e3927f77d9e64ae2c7ac2163f86b9825",
    "date": "2024-01-02T02:37:25-08:00",
    "message": "[Pallas] Add missing shape checks in the Pallas FlashAttention kernel for TPUs\n\nMissing shape checks can cause hard to understand runtime errors caused by\nOOB checks inserted by XLA. We weren't verifying that the attention bias and\nthe segment ids have the shapes we were expecting.\n\nPiperOrigin-RevId: 595065315",
    "changes": [
        {
            "name": "flash_attention.py",
            "path": "jax/experimental/pallas/ops/tpu/flash_attention.py",
            "patches": [
                {
                    "old_start": 43,
                    "old_length": 8,
                    "new_start": 43,
                    "new_length": 8,
                    "hunk": "@@ -43,8 +43,8 @@ class SegmentIds(NamedTuple):\n     kv: segment ids along the KV sequence.\n   \"\"\"\n \n-  q: jax.Array  # [q_seq_len]\n-  kv: jax.Array  # [kv_seq_len]\n+  q: jax.Array  # [batch_size, q_seq_len]\n+  kv: jax.Array  # [batch_size, kv_seq_len]\n \n \n @dataclasses.dataclass(frozen=True)\n"
                },
                {
                    "old_start": 174,
                    "old_length": 6,
                    "new_start": 174,
                    "new_length": 23,
                    "hunk": "@@ -174,6 +174,23 @@ def flash_attention(\n     raise ValueError(\n         f\"KV sequence length mismatch: got {kv_seq_len} and {kv_seq_len_v}\"\n     )\n+  if ab is not None:\n+    if ab.shape != (batch_size, num_heads, q_seq_len, kv_seq_len):\n+      raise ValueError(\n+          f\"Attention bias shape mismatch: expected ({batch_size=},\"\n+          f\" {num_heads=}, {q_seq_len=}, {kv_seq_len=}), got {ab.shape}\"\n+      )\n+  if segment_ids is not None:\n+    if segment_ids.q.shape != (batch_size, q_seq_len):\n+      raise ValueError(\n+          f\"Q segment ids shape mismatch: expected ({batch_size=},\"\n+          f\" {q_seq_len=},), got {segment_ids.q.shape}\"\n+      )\n+    if segment_ids.kv.shape != (batch_size, kv_seq_len):\n+      raise ValueError(\n+          f\"KV segment ids shape mismatch: expected ({batch_size=},\"\n+          f\" {kv_seq_len=},), got {segment_ids.kv.shape}\"\n+      )\n   if block_sizes is None:\n     block_sizes = BlockSizes.get_default(\n         batch_size, num_heads, q_seq_len, kv_seq_len, d_model"
                }
            ],
            "whole_deleted": "-  q: jax.Array  # [q_seq_len]\n-  kv: jax.Array  # [kv_seq_len]\n",
            "whole_added": "+  q: jax.Array  # [batch_size, q_seq_len]\n+  kv: jax.Array  # [batch_size, kv_seq_len]\n+  if ab is not None:\n+    if ab.shape != (batch_size, num_heads, q_seq_len, kv_seq_len):\n+      raise ValueError(\n+          f\"Attention bias shape mismatch: expected ({batch_size=},\"\n+          f\" {num_heads=}, {q_seq_len=}, {kv_seq_len=}), got {ab.shape}\"\n+      )\n+  if segment_ids is not None:\n+    if segment_ids.q.shape != (batch_size, q_seq_len):\n+      raise ValueError(\n+          f\"Q segment ids shape mismatch: expected ({batch_size=},\"\n+          f\" {q_seq_len=},), got {segment_ids.q.shape}\"\n+      )\n+    if segment_ids.kv.shape != (batch_size, kv_seq_len):\n+      raise ValueError(\n+          f\"KV segment ids shape mismatch: expected ({batch_size=},\"\n+          f\" {kv_seq_len=},), got {segment_ids.kv.shape}\"\n+      )\n",
            "whole_hunk": "@@ -43,8 +43,8 @@ class SegmentIds(NamedTuple):\n     kv: segment ids along the KV sequence.\n   \"\"\"\n \n-  q: jax.Array  # [q_seq_len]\n-  kv: jax.Array  # [kv_seq_len]\n+  q: jax.Array  # [batch_size, q_seq_len]\n+  kv: jax.Array  # [batch_size, kv_seq_len]\n \n \n @dataclasses.dataclass(frozen=True)\n@@ -174,6 +174,23 @@ def flash_attention(\n     raise ValueError(\n         f\"KV sequence length mismatch: got {kv_seq_len} and {kv_seq_len_v}\"\n     )\n+  if ab is not None:\n+    if ab.shape != (batch_size, num_heads, q_seq_len, kv_seq_len):\n+      raise ValueError(\n+          f\"Attention bias shape mismatch: expected ({batch_size=},\"\n+          f\" {num_heads=}, {q_seq_len=}, {kv_seq_len=}), got {ab.shape}\"\n+      )\n+  if segment_ids is not None:\n+    if segment_ids.q.shape != (batch_size, q_seq_len):\n+      raise ValueError(\n+          f\"Q segment ids shape mismatch: expected ({batch_size=},\"\n+          f\" {q_seq_len=},), got {segment_ids.q.shape}\"\n+      )\n+    if segment_ids.kv.shape != (batch_size, kv_seq_len):\n+      raise ValueError(\n+          f\"KV segment ids shape mismatch: expected ({batch_size=},\"\n+          f\" {kv_seq_len=},), got {segment_ids.kv.shape}\"\n+      )\n   if block_sizes is None:\n     block_sizes = BlockSizes.get_default(\n         batch_size, num_heads, q_seq_len, kv_seq_len, d_model"
        }
    ]
}]