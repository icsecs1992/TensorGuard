[{
    "Id": 0,
    "commit_link": "https://github.com/keras-team/keras/commit/f5d308752bc98017c47e6eb4cd29f1e61bc43796",
    "date": "2024-06-24T21:07:48-07:00",
    "message": "`keras.utils.split_dataset` now supports nested structures in dataset. (#19911)\n\nFixes https://github.com/keras-team/keras/issues/19797\r\n\r\nAlso:\r\n- remove conversion to `np.array` since ealier check verifies inputs are `np.array`.\r\n- improved verification in the `np.array` case to check the type of all the elements, not just the first one.\r\n- removed code related to `dict`s, which are not supported as top-level structure.\r\n- refactored tests as parameterized tests.",
    "changes": [
        {
            "name": "dataset_utils.py",
            "path": "keras/src/utils/dataset_utils.py",
            "patches": [
                {
                    "old_start": 6,
                    "old_length": 6,
                    "new_start": 6,
                    "new_length": 7,
                    "hunk": "@@ -6,6 +6,7 @@ from multiprocessing.pool import ThreadPool\n \n import numpy as np\n \n+from keras.src import tree\n from keras.src.api_export import keras_export\n from keras.src.utils import io_utils\n from keras.src.utils.module_utils import tensorflow as tf\n"
                },
                {
                    "old_start": 137,
                    "old_length": 16,
                    "new_start": 138,
                    "new_length": 7,
                    "hunk": "@@ -137,16 +138,7 @@ def _convert_dataset_to_list(\n         data_size_warning_flag,\n         start_time,\n     ):\n-        if dataset_type_spec in [tuple, list]:\n-            # The try-except here is for NumPy 1.24 compatibility, see:\n-            # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n-            try:\n-                arr = np.array(sample)\n-            except ValueError:\n-                arr = np.array(sample, dtype=object)\n-            dataset_as_list.append(arr)\n-        else:\n-            dataset_as_list.append(sample)\n+        dataset_as_list.append(sample)\n \n     return dataset_as_list\n \n"
                },
                {
                    "old_start": 169,
                    "old_length": 23,
                    "new_start": 161,
                    "new_length": 23,
                    "hunk": "@@ -169,23 +161,23 @@ def _get_data_iterator_from_dataset(dataset, dataset_type_spec):\n                 \"Please provide a non-empty list of arrays.\"\n             )\n \n-        if _get_type_spec(dataset[0]) is np.ndarray:\n-            expected_shape = dataset[0].shape\n-            for i, element in enumerate(dataset):\n-                if np.array(element).shape[0] != expected_shape[0]:\n-                    raise ValueError(\n-                        \"Received a list of NumPy arrays with different \"\n-                        f\"lengths. Mismatch found at index {i}, \"\n-                        f\"Expected shape={expected_shape} \"\n-                        f\"Received shape={np.array(element).shape}.\"\n-                        \"Please provide a list of NumPy arrays with \"\n-                        \"the same length.\"\n-                    )\n-        else:\n-            raise ValueError(\n-                \"Expected a list of `numpy.ndarray` objects,\"\n-                f\"Received: {type(dataset[0])}\"\n-            )\n+        expected_shape = None\n+        for i, element in enumerate(dataset):\n+            if not isinstance(element, np.ndarray):\n+                raise ValueError(\n+                    \"Expected a list of `numpy.ndarray` objects,\"\n+                    f\"Received: {type(element)} at index {i}.\"\n+                )\n+            if expected_shape is None:\n+                expected_shape = element.shape\n+            elif element.shape[0] != expected_shape[0]:\n+                raise ValueError(\n+                    \"Received a list of NumPy arrays with different lengths.\"\n+                    f\"Mismatch found at index {i}, \"\n+                    f\"Expected shape={expected_shape} \"\n+                    f\"Received shape={np.array(element).shape}.\"\n+                    \"Please provide a list of NumPy arrays of the same length.\"\n+                )\n \n         return iter(zip(*dataset))\n     elif dataset_type_spec == tuple:\n"
                },
                {
                    "old_start": 195,
                    "old_length": 23,
                    "new_start": 187,
                    "new_length": 23,
                    "hunk": "@@ -195,23 +187,23 @@ def _get_data_iterator_from_dataset(dataset, dataset_type_spec):\n                 \"Please provide a non-empty tuple of arrays.\"\n             )\n \n-        if _get_type_spec(dataset[0]) is np.ndarray:\n-            expected_shape = dataset[0].shape\n-            for i, element in enumerate(dataset):\n-                if np.array(element).shape[0] != expected_shape[0]:\n-                    raise ValueError(\n-                        \"Received a tuple of NumPy arrays with different \"\n-                        f\"lengths. Mismatch found at index {i}, \"\n-                        f\"Expected shape={expected_shape} \"\n-                        f\"Received shape={np.array(element).shape}.\"\n-                        \"Please provide a tuple of NumPy arrays with \"\n-                        \"the same length.\"\n-                    )\n-        else:\n-            raise ValueError(\n-                \"Expected a tuple of `numpy.ndarray` objects, \"\n-                f\"Received: {type(dataset[0])}\"\n-            )\n+        expected_shape = None\n+        for i, element in enumerate(dataset):\n+            if not isinstance(element, np.ndarray):\n+                raise ValueError(\n+                    \"Expected a tuple of `numpy.ndarray` objects,\"\n+                    f\"Received: {type(element)} at index {i}.\"\n+                )\n+            if expected_shape is None:\n+                expected_shape = element.shape\n+            elif element.shape[0] != expected_shape[0]:\n+                raise ValueError(\n+                    \"Received a tuple of NumPy arrays with different lengths.\"\n+                    f\"Mismatch found at index {i}, \"\n+                    f\"Expected shape={expected_shape} \"\n+                    f\"Received shape={np.array(element).shape}.\"\n+                    \"Please provide a tuple of NumPy arrays of the same length.\"\n+                )\n \n         return iter(zip(*dataset))\n     elif dataset_type_spec == tf.data.Dataset:\n"
                },
                {
                    "old_start": 436,
                    "old_length": 23,
                    "new_start": 428,
                    "new_length": 24,
                    "hunk": "@@ -436,23 +428,24 @@ def _restore_dataset_from_list(\n     dataset_as_list, dataset_type_spec, original_dataset\n ):\n     \"\"\"Restore the dataset from the list of arrays.\"\"\"\n-    if dataset_type_spec in [tuple, list]:\n-        return tuple(np.array(sample) for sample in zip(*dataset_as_list))\n-    elif dataset_type_spec == tf.data.Dataset:\n-        if isinstance(original_dataset.element_spec, dict):\n-            restored_dataset = {}\n-            for d in dataset_as_list:\n-                for k, v in d.items():\n-                    if k not in restored_dataset:\n-                        restored_dataset[k] = [v]\n-                    else:\n-                        restored_dataset[k].append(v)\n-            return restored_dataset\n-        else:\n-            return tuple(np.array(sample) for sample in zip(*dataset_as_list))\n+    if dataset_type_spec in [tuple, list, tf.data.Dataset] or is_torch_dataset(\n+        original_dataset\n+    ):\n+        # Save structure by taking the first element.\n+        element_spec = dataset_as_list[0]\n+        # Flatten each element.\n+        dataset_as_list = [tree.flatten(sample) for sample in dataset_as_list]\n+        # Combine respective elements at all indices.\n+        dataset_as_list = [np.array(sample) for sample in zip(*dataset_as_list)]\n+        # Recreate the original structure of elements.\n+        dataset_as_list = tree.pack_sequence_as(element_spec, dataset_as_list)\n+        # Turn lists to tuples as tf.data will fail on lists.\n+        return tree.traverse(\n+            lambda x: tuple(x) if isinstance(x, list) else x,\n+            dataset_as_list,\n+            top_down=False,\n+        )\n \n-    elif is_torch_dataset(original_dataset):\n-        return tuple(np.array(sample) for sample in zip(*dataset_as_list))\n     return dataset_as_list\n \n \n"
                },
                {
                    "old_start": 477,
                    "old_length": 14,
                    "new_start": 470,
                    "new_length": 12,
                    "hunk": "@@ -477,14 +470,12 @@ def _get_type_spec(dataset):\n         return list\n     elif isinstance(dataset, np.ndarray):\n         return np.ndarray\n-    elif isinstance(dataset, dict):\n-        return dict\n     elif isinstance(dataset, tf.data.Dataset):\n         return tf.data.Dataset\n     elif is_torch_dataset(dataset):\n-        from torch.utils.data import Dataset as torchDataset\n+        from torch.utils.data import Dataset as TorchDataset\n \n-        return torchDataset\n+        return TorchDataset\n     else:\n         return None\n \n"
                }
            ],
            "whole_deleted": "-        if dataset_type_spec in [tuple, list]:\n-            # The try-except here is for NumPy 1.24 compatibility, see:\n-            # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n-            try:\n-                arr = np.array(sample)\n-            except ValueError:\n-                arr = np.array(sample, dtype=object)\n-            dataset_as_list.append(arr)\n-        else:\n-            dataset_as_list.append(sample)\n-        if _get_type_spec(dataset[0]) is np.ndarray:\n-            expected_shape = dataset[0].shape\n-            for i, element in enumerate(dataset):\n-                if np.array(element).shape[0] != expected_shape[0]:\n-                    raise ValueError(\n-                        \"Received a list of NumPy arrays with different \"\n-                        f\"lengths. Mismatch found at index {i}, \"\n-                        f\"Expected shape={expected_shape} \"\n-                        f\"Received shape={np.array(element).shape}.\"\n-                        \"Please provide a list of NumPy arrays with \"\n-                        \"the same length.\"\n-                    )\n-        else:\n-            raise ValueError(\n-                \"Expected a list of `numpy.ndarray` objects,\"\n-                f\"Received: {type(dataset[0])}\"\n-            )\n-        if _get_type_spec(dataset[0]) is np.ndarray:\n-            expected_shape = dataset[0].shape\n-            for i, element in enumerate(dataset):\n-                if np.array(element).shape[0] != expected_shape[0]:\n-                    raise ValueError(\n-                        \"Received a tuple of NumPy arrays with different \"\n-                        f\"lengths. Mismatch found at index {i}, \"\n-                        f\"Expected shape={expected_shape} \"\n-                        f\"Received shape={np.array(element).shape}.\"\n-                        \"Please provide a tuple of NumPy arrays with \"\n-                        \"the same length.\"\n-                    )\n-        else:\n-            raise ValueError(\n-                \"Expected a tuple of `numpy.ndarray` objects, \"\n-                f\"Received: {type(dataset[0])}\"\n-            )\n-    if dataset_type_spec in [tuple, list]:\n-        return tuple(np.array(sample) for sample in zip(*dataset_as_list))\n-    elif dataset_type_spec == tf.data.Dataset:\n-        if isinstance(original_dataset.element_spec, dict):\n-            restored_dataset = {}\n-            for d in dataset_as_list:\n-                for k, v in d.items():\n-                    if k not in restored_dataset:\n-                        restored_dataset[k] = [v]\n-                    else:\n-                        restored_dataset[k].append(v)\n-            return restored_dataset\n-        else:\n-            return tuple(np.array(sample) for sample in zip(*dataset_as_list))\n-    elif is_torch_dataset(original_dataset):\n-        return tuple(np.array(sample) for sample in zip(*dataset_as_list))\n-    elif isinstance(dataset, dict):\n-        return dict\n-        from torch.utils.data import Dataset as torchDataset\n-        return torchDataset\n",
            "whole_added": "+from keras.src import tree\n+        dataset_as_list.append(sample)\n+        expected_shape = None\n+        for i, element in enumerate(dataset):\n+            if not isinstance(element, np.ndarray):\n+                raise ValueError(\n+                    \"Expected a list of `numpy.ndarray` objects,\"\n+                    f\"Received: {type(element)} at index {i}.\"\n+                )\n+            if expected_shape is None:\n+                expected_shape = element.shape\n+            elif element.shape[0] != expected_shape[0]:\n+                raise ValueError(\n+                    \"Received a list of NumPy arrays with different lengths.\"\n+                    f\"Mismatch found at index {i}, \"\n+                    f\"Expected shape={expected_shape} \"\n+                    f\"Received shape={np.array(element).shape}.\"\n+                    \"Please provide a list of NumPy arrays of the same length.\"\n+                )\n+        expected_shape = None\n+        for i, element in enumerate(dataset):\n+            if not isinstance(element, np.ndarray):\n+                raise ValueError(\n+                    \"Expected a tuple of `numpy.ndarray` objects,\"\n+                    f\"Received: {type(element)} at index {i}.\"\n+                )\n+            if expected_shape is None:\n+                expected_shape = element.shape\n+            elif element.shape[0] != expected_shape[0]:\n+                raise ValueError(\n+                    \"Received a tuple of NumPy arrays with different lengths.\"\n+                    f\"Mismatch found at index {i}, \"\n+                    f\"Expected shape={expected_shape} \"\n+                    f\"Received shape={np.array(element).shape}.\"\n+                    \"Please provide a tuple of NumPy arrays of the same length.\"\n+                )\n+    if dataset_type_spec in [tuple, list, tf.data.Dataset] or is_torch_dataset(\n+        original_dataset\n+    ):\n+        # Save structure by taking the first element.\n+        element_spec = dataset_as_list[0]\n+        # Flatten each element.\n+        dataset_as_list = [tree.flatten(sample) for sample in dataset_as_list]\n+        # Combine respective elements at all indices.\n+        dataset_as_list = [np.array(sample) for sample in zip(*dataset_as_list)]\n+        # Recreate the original structure of elements.\n+        dataset_as_list = tree.pack_sequence_as(element_spec, dataset_as_list)\n+        # Turn lists to tuples as tf.data will fail on lists.\n+        return tree.traverse(\n+            lambda x: tuple(x) if isinstance(x, list) else x,\n+            dataset_as_list,\n+            top_down=False,\n+        )\n+        from torch.utils.data import Dataset as TorchDataset\n+        return TorchDataset\n",
            "whole_hunk": "@@ -6,6 +6,7 @@ from multiprocessing.pool import ThreadPool\n \n import numpy as np\n \n+from keras.src import tree\n from keras.src.api_export import keras_export\n from keras.src.utils import io_utils\n from keras.src.utils.module_utils import tensorflow as tf\n@@ -137,16 +138,7 @@ def _convert_dataset_to_list(\n         data_size_warning_flag,\n         start_time,\n     ):\n-        if dataset_type_spec in [tuple, list]:\n-            # The try-except here is for NumPy 1.24 compatibility, see:\n-            # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n-            try:\n-                arr = np.array(sample)\n-            except ValueError:\n-                arr = np.array(sample, dtype=object)\n-            dataset_as_list.append(arr)\n-        else:\n-            dataset_as_list.append(sample)\n+        dataset_as_list.append(sample)\n \n     return dataset_as_list\n \n@@ -169,23 +161,23 @@ def _get_data_iterator_from_dataset(dataset, dataset_type_spec):\n                 \"Please provide a non-empty list of arrays.\"\n             )\n \n-        if _get_type_spec(dataset[0]) is np.ndarray:\n-            expected_shape = dataset[0].shape\n-            for i, element in enumerate(dataset):\n-                if np.array(element).shape[0] != expected_shape[0]:\n-                    raise ValueError(\n-                        \"Received a list of NumPy arrays with different \"\n-                        f\"lengths. Mismatch found at index {i}, \"\n-                        f\"Expected shape={expected_shape} \"\n-                        f\"Received shape={np.array(element).shape}.\"\n-                        \"Please provide a list of NumPy arrays with \"\n-                        \"the same length.\"\n-                    )\n-        else:\n-            raise ValueError(\n-                \"Expected a list of `numpy.ndarray` objects,\"\n-                f\"Received: {type(dataset[0])}\"\n-            )\n+        expected_shape = None\n+        for i, element in enumerate(dataset):\n+            if not isinstance(element, np.ndarray):\n+                raise ValueError(\n+                    \"Expected a list of `numpy.ndarray` objects,\"\n+                    f\"Received: {type(element)} at index {i}.\"\n+                )\n+            if expected_shape is None:\n+                expected_shape = element.shape\n+            elif element.shape[0] != expected_shape[0]:\n+                raise ValueError(\n+                    \"Received a list of NumPy arrays with different lengths.\"\n+                    f\"Mismatch found at index {i}, \"\n+                    f\"Expected shape={expected_shape} \"\n+                    f\"Received shape={np.array(element).shape}.\"\n+                    \"Please provide a list of NumPy arrays of the same length.\"\n+                )\n \n         return iter(zip(*dataset))\n     elif dataset_type_spec == tuple:\n@@ -195,23 +187,23 @@ def _get_data_iterator_from_dataset(dataset, dataset_type_spec):\n                 \"Please provide a non-empty tuple of arrays.\"\n             )\n \n-        if _get_type_spec(dataset[0]) is np.ndarray:\n-            expected_shape = dataset[0].shape\n-            for i, element in enumerate(dataset):\n-                if np.array(element).shape[0] != expected_shape[0]:\n-                    raise ValueError(\n-                        \"Received a tuple of NumPy arrays with different \"\n-                        f\"lengths. Mismatch found at index {i}, \"\n-                        f\"Expected shape={expected_shape} \"\n-                        f\"Received shape={np.array(element).shape}.\"\n-                        \"Please provide a tuple of NumPy arrays with \"\n-                        \"the same length.\"\n-                    )\n-        else:\n-            raise ValueError(\n-                \"Expected a tuple of `numpy.ndarray` objects, \"\n-                f\"Received: {type(dataset[0])}\"\n-            )\n+        expected_shape = None\n+        for i, element in enumerate(dataset):\n+            if not isinstance(element, np.ndarray):\n+                raise ValueError(\n+                    \"Expected a tuple of `numpy.ndarray` objects,\"\n+                    f\"Received: {type(element)} at index {i}.\"\n+                )\n+            if expected_shape is None:\n+                expected_shape = element.shape\n+            elif element.shape[0] != expected_shape[0]:\n+                raise ValueError(\n+                    \"Received a tuple of NumPy arrays with different lengths.\"\n+                    f\"Mismatch found at index {i}, \"\n+                    f\"Expected shape={expected_shape} \"\n+                    f\"Received shape={np.array(element).shape}.\"\n+                    \"Please provide a tuple of NumPy arrays of the same length.\"\n+                )\n \n         return iter(zip(*dataset))\n     elif dataset_type_spec == tf.data.Dataset:\n@@ -436,23 +428,24 @@ def _restore_dataset_from_list(\n     dataset_as_list, dataset_type_spec, original_dataset\n ):\n     \"\"\"Restore the dataset from the list of arrays.\"\"\"\n-    if dataset_type_spec in [tuple, list]:\n-        return tuple(np.array(sample) for sample in zip(*dataset_as_list))\n-    elif dataset_type_spec == tf.data.Dataset:\n-        if isinstance(original_dataset.element_spec, dict):\n-            restored_dataset = {}\n-            for d in dataset_as_list:\n-                for k, v in d.items():\n-                    if k not in restored_dataset:\n-                        restored_dataset[k] = [v]\n-                    else:\n-                        restored_dataset[k].append(v)\n-            return restored_dataset\n-        else:\n-            return tuple(np.array(sample) for sample in zip(*dataset_as_list))\n+    if dataset_type_spec in [tuple, list, tf.data.Dataset] or is_torch_dataset(\n+        original_dataset\n+    ):\n+        # Save structure by taking the first element.\n+        element_spec = dataset_as_list[0]\n+        # Flatten each element.\n+        dataset_as_list = [tree.flatten(sample) for sample in dataset_as_list]\n+        # Combine respective elements at all indices.\n+        dataset_as_list = [np.array(sample) for sample in zip(*dataset_as_list)]\n+        # Recreate the original structure of elements.\n+        dataset_as_list = tree.pack_sequence_as(element_spec, dataset_as_list)\n+        # Turn lists to tuples as tf.data will fail on lists.\n+        return tree.traverse(\n+            lambda x: tuple(x) if isinstance(x, list) else x,\n+            dataset_as_list,\n+            top_down=False,\n+        )\n \n-    elif is_torch_dataset(original_dataset):\n-        return tuple(np.array(sample) for sample in zip(*dataset_as_list))\n     return dataset_as_list\n \n \n@@ -477,14 +470,12 @@ def _get_type_spec(dataset):\n         return list\n     elif isinstance(dataset, np.ndarray):\n         return np.ndarray\n-    elif isinstance(dataset, dict):\n-        return dict\n     elif isinstance(dataset, tf.data.Dataset):\n         return tf.data.Dataset\n     elif is_torch_dataset(dataset):\n-        from torch.utils.data import Dataset as torchDataset\n+        from torch.utils.data import Dataset as TorchDataset\n \n-        return torchDataset\n+        return TorchDataset\n     else:\n         return None\n \n"
        },
        {
            "name": "dataset_utils_test.py",
            "path": "keras/src/utils/dataset_utils_test.py",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 145,
                    "new_start": 1,
                    "new_length": 49,
                    "hunk": "@@ -1,145 +1,49 @@\n+import itertools\n+\n import numpy as np\n+from absl.testing import parameterized\n+from torch.utils.data import Dataset as TorchDataset\n \n from keras.src.testing import test_case\n+from keras.src.testing.test_utils import named_product\n from keras.src.utils.dataset_utils import split_dataset\n from keras.src.utils.module_utils import tensorflow as tf\n \n \n-class DatasetUtilsTest(test_case.TestCase):\n-    def test_split_dataset_list(self):\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = [\n-            np.random.sample((n_sample, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        ]\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (n_cols)\n-        )\n+class MyTorchDataset(TorchDataset):\n \n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = [\n-            np.random.sample((n_sample, 100, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        ]\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (100, n_cols)\n-        )\n+    def __init__(self, x, y):\n+        self.x = x\n+        self.y = y\n \n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = [\n-            np.random.sample((n_sample, 10, 10, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        ]\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (10, 10, n_cols)\n-        )\n+    def __len__(self):\n+        return len(self.x)\n \n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = [\n-            np.random.sample((n_sample, 100, 10, 30, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        ]\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape,\n-            (100, 10, 30, n_cols),\n-        )\n+    def __getitem__(self, index):\n+        return self.x[index], self.y[index]\n \n-    def test_split_dataset_tuple(self):\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = (\n-            np.random.sample((n_sample, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (n_cols)\n-        )\n \n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = (\n-            np.random.sample((n_sample, 100, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (100, n_cols)\n+class DatasetUtilsTest(test_case.TestCase, parameterized.TestCase):\n+    @parameterized.named_parameters(\n+        named_product(\n+            dataset_type=[\"list\", \"tuple\", \"tensorflow\", \"torch\"],\n+            features_shape=[(2,), (100, 2), (10, 10, 2)],\n         )\n+    )\n+    def test_split_dataset(self, dataset_type, features_shape):\n+        n_sample, left_size, right_size = 100, 0.2, 0.8\n+        features = np.random.sample((n_sample,) + features_shape)\n+        labels = np.random.sample((n_sample, 1))\n \n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = (\n-            np.random.sample((n_sample, 10, 10, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (10, 10, n_cols)\n-        )\n+        if dataset_type == \"list\":\n+            dataset = [features, labels]\n+        elif dataset_type == \"tuple\":\n+            dataset = (features, labels)\n+        elif dataset_type == \"tensorflow\":\n+            dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n+        elif dataset_type == \"torch\":\n+            dataset = MyTorchDataset(features, labels)\n \n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = (\n-            np.random.sample((n_sample, 100, 10, 30, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n         dataset_left, dataset_right = split_dataset(\n             dataset, left_size=left_size, right_size=right_size\n         )\n"
                },
                {
                    "old_start": 149,
                    "old_length": 77,
                    "new_start": 53,
                    "new_length": 30,
                    "hunk": "@@ -149,77 +53,30 @@ class DatasetUtilsTest(test_case.TestCase):\n         self.assertEqual(\n             int(dataset_right.cardinality()), int(n_sample * right_size)\n         )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape,\n-            (100, 10, 30, n_cols),\n-        )\n-\n-    def test_split_dataset_tensorflow(self):\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n-        dataset_left, dataset_right = split_dataset(\n-            tf_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 100, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n-        dataset_left, dataset_right = split_dataset(\n-            tf_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (100, n_cols)\n-        )\n+        for sample in itertools.chain(dataset_left, dataset_right):\n+            self.assertEqual(sample[0].shape, features_shape)\n+            self.assertEqual(sample[1].shape, (1,))\n+\n+    @parameterized.named_parameters(\n+        named_product(structure_type=[\"dict\", \"tuple\"])\n+    )\n+    def test_split_dataset_nested_structures(self, structure_type):\n+        n_sample, left_size, right_size = 100, 0.2, 0.8\n+        features1 = np.random.sample((n_sample, 2))\n+        features2 = np.random.sample((n_sample, 10, 2))\n+        labels = np.random.sample((n_sample, 1))\n+\n+        if structure_type == \"dict\":\n+            dataset = tf.data.Dataset.from_tensor_slices(\n+                {\"x1\": features1, \"x2\": features2, \"labels\": labels}\n+            )\n+        elif structure_type == \"tuple\":\n+            dataset = tf.data.Dataset.from_tensor_slices(\n+                ((features1, features2), labels)\n+            )\n \n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 10, 10, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n         dataset_left, dataset_right = split_dataset(\n-            tf_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (10, 10, n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 100, 10, 30, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n-        dataset_left, dataset_right = split_dataset(\n-            tf_dataset, left_size=left_size, right_size=right_size\n+            dataset, left_size=left_size, right_size=right_size\n         )\n         self.assertEqual(\n             int(dataset_left.cardinality()), int(n_sample * left_size)\n"
                },
                {
                    "old_start": 227,
                    "old_length": 108,
                    "new_start": 84,
                    "new_length": 11,
                    "hunk": "@@ -227,108 +84,11 @@ class DatasetUtilsTest(test_case.TestCase):\n         self.assertEqual(\n             int(dataset_right.cardinality()), int(n_sample * right_size)\n         )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape,\n-            (100, 10, 30, n_cols),\n-        )\n-\n-    def test_split_dataset_torch(self):\n-        # sample torch dataset class\n-        from torch.utils.data import Dataset as torchDataset\n-\n-        class Dataset(torchDataset):\n-            \"Characterizes a dataset for PyTorch\"\n-\n-            def __init__(self, x, y):\n-                \"Initialization\"\n-                self.x = x\n-                self.y = y\n-\n-            def __len__(self):\n-                \"Denotes the total number of samples\"\n-                return len(self.x)\n-\n-            def __getitem__(self, index):\n-                \"Generates one sample of data\"\n-                return self.x[index], self.y[index]\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        torch_dataset = Dataset(features, labels)\n-        dataset_left, dataset_right = split_dataset(\n-            torch_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_left]), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_right]),\n-            int(n_sample * right_size),\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (n_cols,)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 100, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        torch_dataset = Dataset(features, labels)\n-        dataset_left, dataset_right = split_dataset(\n-            torch_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_left]), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_right]),\n-            int(n_sample * right_size),\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (100, n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 10, 10, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        torch_dataset = Dataset(features, labels)\n-        dataset_left, dataset_right = split_dataset(\n-            torch_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_left]), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_right]),\n-            int(n_sample * right_size),\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (10, 10, n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 100, 10, 30, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        torch_dataset = Dataset(features, labels)\n-        dataset_left, dataset_right = split_dataset(\n-            torch_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_left]), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_right]),\n-            int(n_sample * right_size),\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape,\n-            (100, 10, 30, n_cols),\n-        )\n+        for sample in itertools.chain(dataset_left, dataset_right):\n+            if structure_type == \"dict\":\n+                x1, x2, labels = sample[\"x1\"], sample[\"x2\"], sample[\"labels\"]\n+            elif structure_type == \"tuple\":\n+                (x1, x2), labels = sample\n+            self.assertEqual(x1.shape, (2,))\n+            self.assertEqual(x2.shape, (10, 2))\n+            self.assertEqual(labels.shape, (1,))"
                }
            ],
            "whole_deleted": "-class DatasetUtilsTest(test_case.TestCase):\n-    def test_split_dataset_list(self):\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = [\n-            np.random.sample((n_sample, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        ]\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (n_cols)\n-        )\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = [\n-            np.random.sample((n_sample, 100, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        ]\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (100, n_cols)\n-        )\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = [\n-            np.random.sample((n_sample, 10, 10, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        ]\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (10, 10, n_cols)\n-        )\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = [\n-            np.random.sample((n_sample, 100, 10, 30, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        ]\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape,\n-            (100, 10, 30, n_cols),\n-        )\n-    def test_split_dataset_tuple(self):\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = (\n-            np.random.sample((n_sample, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (n_cols)\n-        )\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = (\n-            np.random.sample((n_sample, 100, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (100, n_cols)\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = (\n-            np.random.sample((n_sample, 10, 10, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (10, 10, n_cols)\n-        )\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = (\n-            np.random.sample((n_sample, 100, 10, 30, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape,\n-            (100, 10, 30, n_cols),\n-        )\n-\n-    def test_split_dataset_tensorflow(self):\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n-        dataset_left, dataset_right = split_dataset(\n-            tf_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 100, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n-        dataset_left, dataset_right = split_dataset(\n-            tf_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (100, n_cols)\n-        )\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 10, 10, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n-            tf_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (10, 10, n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 100, 10, 30, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n-        dataset_left, dataset_right = split_dataset(\n-            tf_dataset, left_size=left_size, right_size=right_size\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape,\n-            (100, 10, 30, n_cols),\n-        )\n-\n-    def test_split_dataset_torch(self):\n-        # sample torch dataset class\n-        from torch.utils.data import Dataset as torchDataset\n-\n-        class Dataset(torchDataset):\n-            \"Characterizes a dataset for PyTorch\"\n-\n-            def __init__(self, x, y):\n-                \"Initialization\"\n-                self.x = x\n-                self.y = y\n-\n-            def __len__(self):\n-                \"Denotes the total number of samples\"\n-                return len(self.x)\n-\n-            def __getitem__(self, index):\n-                \"Generates one sample of data\"\n-                return self.x[index], self.y[index]\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        torch_dataset = Dataset(features, labels)\n-        dataset_left, dataset_right = split_dataset(\n-            torch_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_left]), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_right]),\n-            int(n_sample * right_size),\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (n_cols,)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 100, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        torch_dataset = Dataset(features, labels)\n-        dataset_left, dataset_right = split_dataset(\n-            torch_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_left]), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_right]),\n-            int(n_sample * right_size),\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (100, n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 10, 10, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        torch_dataset = Dataset(features, labels)\n-        dataset_left, dataset_right = split_dataset(\n-            torch_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_left]), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_right]),\n-            int(n_sample * right_size),\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (10, 10, n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 100, 10, 30, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        torch_dataset = Dataset(features, labels)\n-        dataset_left, dataset_right = split_dataset(\n-            torch_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_left]), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_right]),\n-            int(n_sample * right_size),\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape,\n-            (100, 10, 30, n_cols),\n-        )\n",
            "whole_added": "+import itertools\n+\n+from absl.testing import parameterized\n+from torch.utils.data import Dataset as TorchDataset\n+from keras.src.testing.test_utils import named_product\n+class MyTorchDataset(TorchDataset):\n+    def __init__(self, x, y):\n+        self.x = x\n+        self.y = y\n+    def __len__(self):\n+        return len(self.x)\n+    def __getitem__(self, index):\n+        return self.x[index], self.y[index]\n+class DatasetUtilsTest(test_case.TestCase, parameterized.TestCase):\n+    @parameterized.named_parameters(\n+        named_product(\n+            dataset_type=[\"list\", \"tuple\", \"tensorflow\", \"torch\"],\n+            features_shape=[(2,), (100, 2), (10, 10, 2)],\n+    )\n+    def test_split_dataset(self, dataset_type, features_shape):\n+        n_sample, left_size, right_size = 100, 0.2, 0.8\n+        features = np.random.sample((n_sample,) + features_shape)\n+        labels = np.random.sample((n_sample, 1))\n+        if dataset_type == \"list\":\n+            dataset = [features, labels]\n+        elif dataset_type == \"tuple\":\n+            dataset = (features, labels)\n+        elif dataset_type == \"tensorflow\":\n+            dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n+        elif dataset_type == \"torch\":\n+            dataset = MyTorchDataset(features, labels)\n+        for sample in itertools.chain(dataset_left, dataset_right):\n+            self.assertEqual(sample[0].shape, features_shape)\n+            self.assertEqual(sample[1].shape, (1,))\n+\n+    @parameterized.named_parameters(\n+        named_product(structure_type=[\"dict\", \"tuple\"])\n+    )\n+    def test_split_dataset_nested_structures(self, structure_type):\n+        n_sample, left_size, right_size = 100, 0.2, 0.8\n+        features1 = np.random.sample((n_sample, 2))\n+        features2 = np.random.sample((n_sample, 10, 2))\n+        labels = np.random.sample((n_sample, 1))\n+\n+        if structure_type == \"dict\":\n+            dataset = tf.data.Dataset.from_tensor_slices(\n+                {\"x1\": features1, \"x2\": features2, \"labels\": labels}\n+            )\n+        elif structure_type == \"tuple\":\n+            dataset = tf.data.Dataset.from_tensor_slices(\n+                ((features1, features2), labels)\n+            )\n+            dataset, left_size=left_size, right_size=right_size\n+        for sample in itertools.chain(dataset_left, dataset_right):\n+            if structure_type == \"dict\":\n+                x1, x2, labels = sample[\"x1\"], sample[\"x2\"], sample[\"labels\"]\n+            elif structure_type == \"tuple\":\n+                (x1, x2), labels = sample\n+            self.assertEqual(x1.shape, (2,))\n+            self.assertEqual(x2.shape, (10, 2))\n+            self.assertEqual(labels.shape, (1,))\n",
            "whole_hunk": "@@ -1,145 +1,49 @@\n+import itertools\n+\n import numpy as np\n+from absl.testing import parameterized\n+from torch.utils.data import Dataset as TorchDataset\n \n from keras.src.testing import test_case\n+from keras.src.testing.test_utils import named_product\n from keras.src.utils.dataset_utils import split_dataset\n from keras.src.utils.module_utils import tensorflow as tf\n \n \n-class DatasetUtilsTest(test_case.TestCase):\n-    def test_split_dataset_list(self):\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = [\n-            np.random.sample((n_sample, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        ]\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (n_cols)\n-        )\n+class MyTorchDataset(TorchDataset):\n \n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = [\n-            np.random.sample((n_sample, 100, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        ]\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (100, n_cols)\n-        )\n+    def __init__(self, x, y):\n+        self.x = x\n+        self.y = y\n \n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = [\n-            np.random.sample((n_sample, 10, 10, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        ]\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (10, 10, n_cols)\n-        )\n+    def __len__(self):\n+        return len(self.x)\n \n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = [\n-            np.random.sample((n_sample, 100, 10, 30, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        ]\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape,\n-            (100, 10, 30, n_cols),\n-        )\n+    def __getitem__(self, index):\n+        return self.x[index], self.y[index]\n \n-    def test_split_dataset_tuple(self):\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = (\n-            np.random.sample((n_sample, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (n_cols)\n-        )\n \n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = (\n-            np.random.sample((n_sample, 100, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (100, n_cols)\n+class DatasetUtilsTest(test_case.TestCase, parameterized.TestCase):\n+    @parameterized.named_parameters(\n+        named_product(\n+            dataset_type=[\"list\", \"tuple\", \"tensorflow\", \"torch\"],\n+            features_shape=[(2,), (100, 2), (10, 10, 2)],\n         )\n+    )\n+    def test_split_dataset(self, dataset_type, features_shape):\n+        n_sample, left_size, right_size = 100, 0.2, 0.8\n+        features = np.random.sample((n_sample,) + features_shape)\n+        labels = np.random.sample((n_sample, 1))\n \n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = (\n-            np.random.sample((n_sample, 10, 10, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (10, 10, n_cols)\n-        )\n+        if dataset_type == \"list\":\n+            dataset = [features, labels]\n+        elif dataset_type == \"tuple\":\n+            dataset = (features, labels)\n+        elif dataset_type == \"tensorflow\":\n+            dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n+        elif dataset_type == \"torch\":\n+            dataset = MyTorchDataset(features, labels)\n \n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = (\n-            np.random.sample((n_sample, 100, 10, 30, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n         dataset_left, dataset_right = split_dataset(\n             dataset, left_size=left_size, right_size=right_size\n         )\n@@ -149,77 +53,30 @@ class DatasetUtilsTest(test_case.TestCase):\n         self.assertEqual(\n             int(dataset_right.cardinality()), int(n_sample * right_size)\n         )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape,\n-            (100, 10, 30, n_cols),\n-        )\n-\n-    def test_split_dataset_tensorflow(self):\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n-        dataset_left, dataset_right = split_dataset(\n-            tf_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 100, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n-        dataset_left, dataset_right = split_dataset(\n-            tf_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (100, n_cols)\n-        )\n+        for sample in itertools.chain(dataset_left, dataset_right):\n+            self.assertEqual(sample[0].shape, features_shape)\n+            self.assertEqual(sample[1].shape, (1,))\n+\n+    @parameterized.named_parameters(\n+        named_product(structure_type=[\"dict\", \"tuple\"])\n+    )\n+    def test_split_dataset_nested_structures(self, structure_type):\n+        n_sample, left_size, right_size = 100, 0.2, 0.8\n+        features1 = np.random.sample((n_sample, 2))\n+        features2 = np.random.sample((n_sample, 10, 2))\n+        labels = np.random.sample((n_sample, 1))\n+\n+        if structure_type == \"dict\":\n+            dataset = tf.data.Dataset.from_tensor_slices(\n+                {\"x1\": features1, \"x2\": features2, \"labels\": labels}\n+            )\n+        elif structure_type == \"tuple\":\n+            dataset = tf.data.Dataset.from_tensor_slices(\n+                ((features1, features2), labels)\n+            )\n \n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 10, 10, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n         dataset_left, dataset_right = split_dataset(\n-            tf_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (10, 10, n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 100, 10, 30, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n-        dataset_left, dataset_right = split_dataset(\n-            tf_dataset, left_size=left_size, right_size=right_size\n+            dataset, left_size=left_size, right_size=right_size\n         )\n         self.assertEqual(\n             int(dataset_left.cardinality()), int(n_sample * left_size)\n@@ -227,108 +84,11 @@ class DatasetUtilsTest(test_case.TestCase):\n         self.assertEqual(\n             int(dataset_right.cardinality()), int(n_sample * right_size)\n         )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape,\n-            (100, 10, 30, n_cols),\n-        )\n-\n-    def test_split_dataset_torch(self):\n-        # sample torch dataset class\n-        from torch.utils.data import Dataset as torchDataset\n-\n-        class Dataset(torchDataset):\n-            \"Characterizes a dataset for PyTorch\"\n-\n-            def __init__(self, x, y):\n-                \"Initialization\"\n-                self.x = x\n-                self.y = y\n-\n-            def __len__(self):\n-                \"Denotes the total number of samples\"\n-                return len(self.x)\n-\n-            def __getitem__(self, index):\n-                \"Generates one sample of data\"\n-                return self.x[index], self.y[index]\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        torch_dataset = Dataset(features, labels)\n-        dataset_left, dataset_right = split_dataset(\n-            torch_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_left]), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_right]),\n-            int(n_sample * right_size),\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (n_cols,)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 100, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        torch_dataset = Dataset(features, labels)\n-        dataset_left, dataset_right = split_dataset(\n-            torch_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_left]), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_right]),\n-            int(n_sample * right_size),\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (100, n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 10, 10, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        torch_dataset = Dataset(features, labels)\n-        dataset_left, dataset_right = split_dataset(\n-            torch_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_left]), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_right]),\n-            int(n_sample * right_size),\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (10, 10, n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 100, 10, 30, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        torch_dataset = Dataset(features, labels)\n-        dataset_left, dataset_right = split_dataset(\n-            torch_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_left]), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_right]),\n-            int(n_sample * right_size),\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape,\n-            (100, 10, 30, n_cols),\n-        )\n+        for sample in itertools.chain(dataset_left, dataset_right):\n+            if structure_type == \"dict\":\n+                x1, x2, labels = sample[\"x1\"], sample[\"x2\"], sample[\"labels\"]\n+            elif structure_type == \"tuple\":\n+                (x1, x2), labels = sample\n+            self.assertEqual(x1.shape, (2,))\n+            self.assertEqual(x2.shape, (10, 2))\n+            self.assertEqual(labels.shape, (1,))"
        }
    ]
},
{
    "Id": 1,
    "commit_link": "https://github.com/keras-team/keras/commit/4e0a920ea106afc36a03158b4f1a8f5a70a8e39c",
    "date": "2024-04-23T20:53:10-07:00",
    "message": "Clean up duplicated `inputs_quantizer` (#19604)\n\n* Cleanup duplicated `inputs_quantizer` and add type check for `input_spec` and `supports_masking`\r\n\r\n* Revert setter",
    "changes": [
        {
            "name": "dense.py",
            "path": "keras/src/layers/core/dense.py",
            "patches": [
                {
                    "old_start": 557,
                    "old_length": 8,
                    "new_start": 557,
                    "new_length": 6,
                    "hunk": "@@ -557,8 +557,6 @@ class Dense(Layer):\n \n         self._tracker.unlock()\n         if mode == \"int8\":\n-            # Configure `self.inputs_quantizer`\n-            self.inputs_quantizer = quantizers.AbsMaxQuantizer(axis=-1)\n             # Quantize `self._kernel` to int8 and compute corresponding scale\n             kernel_value, kernel_scale = quantizers.abs_max_quantize(\n                 self._kernel, axis=0\n"
                }
            ],
            "whole_deleted": "-            # Configure `self.inputs_quantizer`\n-            self.inputs_quantizer = quantizers.AbsMaxQuantizer(axis=-1)\n",
            "whole_added": "",
            "whole_hunk": "@@ -557,8 +557,6 @@ class Dense(Layer):\n \n         self._tracker.unlock()\n         if mode == \"int8\":\n-            # Configure `self.inputs_quantizer`\n-            self.inputs_quantizer = quantizers.AbsMaxQuantizer(axis=-1)\n             # Quantize `self._kernel` to int8 and compute corresponding scale\n             kernel_value, kernel_scale = quantizers.abs_max_quantize(\n                 self._kernel, axis=0\n"
        },
        {
            "name": "einsum_dense.py",
            "path": "keras/src/layers/core/einsum_dense.py",
            "patches": [
                {
                    "old_start": 684,
                    "old_length": 10,
                    "new_start": 684,
                    "new_length": 6,
                    "hunk": "@@ -684,10 +684,6 @@ class EinsumDense(Layer):\n                 self._custom_gradient_equation,\n                 self._kernel_reverse_transpose_axes,\n             ) = _analyze_quantization_info(self.equation, self.input_spec.ndim)\n-            # Configure `self.inputs_quantizer`\n-            self.inputs_quantizer = quantizers.AbsMaxQuantizer(\n-                axis=self._input_reduced_axes\n-            )\n             # Quantize `self._kernel` to int8 and compute corresponding scale\n             kernel_value, kernel_scale = quantizers.abs_max_quantize(\n                 self._kernel, axis=self._kernel_reduced_axes"
                }
            ],
            "whole_deleted": "-            # Configure `self.inputs_quantizer`\n-            self.inputs_quantizer = quantizers.AbsMaxQuantizer(\n-                axis=self._input_reduced_axes\n-            )\n",
            "whole_added": "",
            "whole_hunk": "@@ -684,10 +684,6 @@ class EinsumDense(Layer):\n                 self._custom_gradient_equation,\n                 self._kernel_reverse_transpose_axes,\n             ) = _analyze_quantization_info(self.equation, self.input_spec.ndim)\n-            # Configure `self.inputs_quantizer`\n-            self.inputs_quantizer = quantizers.AbsMaxQuantizer(\n-                axis=self._input_reduced_axes\n-            )\n             # Quantize `self._kernel` to int8 and compute corresponding scale\n             kernel_value, kernel_scale = quantizers.abs_max_quantize(\n                 self._kernel, axis=self._kernel_reduced_axes"
        }
    ]
},
{
    "Id": 2,
    "commit_link": "https://github.com/keras-team/keras/commit/5b9992297398a7d7f5ed113747f074685081a2a4",
    "date": "2024-04-16T12:29:39-07:00",
    "message": "Fix `RandomBrightness`, Enhance `IndexLookup` Initialization and Expand Test Coverage for `Preprocessing Layers` (#19513)\n\n* Add tests for CategoryEncoding class in category_encoding_test.py\r\n\r\n* fix\r\n\r\n* Fix IndexLookup class initialization and add test cases\r\n\r\n* Add test case for IndexLookupLayerTest without vocabulary\r\n\r\n* Fix IndexLookup class initialization\r\n\r\n* Add normalization test cases\r\n\r\n* Add test cases for Hashing class\r\n\r\n* Fix value range validation error in RandomBrightness class\r\n\r\n* Refactor IndexLookup class initialization and add test cases\r\n\r\n* Reffix ndexLookup class initialization and afix est cases",
    "changes": [
        {
            "name": "category_encoding_test.py",
            "path": "keras/layers/preprocessing/category_encoding_test.py",
            "patches": [
                {
                    "old_start": 260,
                    "old_length": 3,
                    "new_start": 260,
                    "new_length": 71,
                    "hunk": "@@ -260,3 +260,71 @@ class CategoryEncodingTest(testing.TestCase, parameterized.TestCase):\n         for output in ds.take(1):\n             output = output.numpy()\n         self.assertAllClose(output, expected_output)\n+\n+    def test_category_encoding_without_num_tokens(self):\n+        with self.assertRaisesRegex(\n+            ValueError, r\"num_tokens must be set to use this layer\"\n+        ):\n+            layers.CategoryEncoding(output_mode=\"multi_hot\")\n+\n+    def test_category_encoding_with_invalid_num_tokens(self):\n+        with self.assertRaisesRegex(ValueError, r\"`num_tokens` must be >= 1\"):\n+            layers.CategoryEncoding(num_tokens=0, output_mode=\"multi_hot\")\n+\n+        with self.assertRaisesRegex(ValueError, r\"`num_tokens` must be >= 1\"):\n+            layers.CategoryEncoding(num_tokens=-1, output_mode=\"multi_hot\")\n+\n+    def test_category_encoding_with_unnecessary_count_weights(self):\n+        layer = layers.CategoryEncoding(num_tokens=4, output_mode=\"multi_hot\")\n+        input_data = np.array([0, 1, 2, 3])\n+        count_weights = np.array([0.1, 0.2, 0.3, 0.4])\n+        with self.assertRaisesRegex(\n+            ValueError, r\"`count_weights` is not used when `output_mode`\"\n+        ):\n+            layer(input_data, count_weights=count_weights)\n+\n+    def test_invalid_output_mode_raises_error(self):\n+        with self.assertRaisesRegex(\n+            ValueError, r\"Unknown arg for output_mode: invalid_mode\"\n+        ):\n+            layers.CategoryEncoding(num_tokens=4, output_mode=\"invalid_mode\")\n+\n+    def test_encode_one_hot_single_sample(self):\n+        layer = layers.CategoryEncoding(num_tokens=4, output_mode=\"one_hot\")\n+        input_array = np.array([1, 2, 3, 1])\n+        expected_output = np.array(\n+            [\n+                [0, 1, 0, 0],\n+                [0, 0, 1, 0],\n+                [0, 0, 0, 1],\n+                [0, 1, 0, 0],\n+            ]\n+        )\n+        output = layer._encode(input_array)\n+        self.assertAllClose(expected_output, output)\n+\n+    def test_encode_one_hot_batched_samples(self):\n+        layer = layers.CategoryEncoding(num_tokens=4, output_mode=\"one_hot\")\n+        input_array = np.array([[3, 2, 0, 1], [3, 2, 0, 1]])\n+        expected_output = np.array(\n+            [\n+                [[0, 0, 0, 1], [0, 0, 1, 0], [1, 0, 0, 0], [0, 1, 0, 0]],\n+                [[0, 0, 0, 1], [0, 0, 1, 0], [1, 0, 0, 0], [0, 1, 0, 0]],\n+            ]\n+        )\n+        output = layer._encode(input_array)\n+        self.assertAllClose(expected_output, output)\n+\n+    def test_count_single_sample(self):\n+        layer = layers.CategoryEncoding(num_tokens=4, output_mode=\"count\")\n+        input_array = np.array([1, 2, 3, 1])\n+        expected_output = np.array([0, 2, 1, 1])\n+        output = layer._count(input_array)\n+        self.assertAllClose(expected_output, output)\n+\n+    def test_count_batched_samples(self):\n+        layer = layers.CategoryEncoding(num_tokens=4, output_mode=\"count\")\n+        input_array = np.array([[1, 2, 3, 1], [0, 3, 1, 0]])\n+        expected_output = np.array([[0, 2, 1, 1], [2, 1, 0, 1]])\n+        output = layer._count(input_array)\n+        self.assertAllClose(expected_output, output)\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+\n+    def test_category_encoding_without_num_tokens(self):\n+        with self.assertRaisesRegex(\n+            ValueError, r\"num_tokens must be set to use this layer\"\n+        ):\n+            layers.CategoryEncoding(output_mode=\"multi_hot\")\n+\n+    def test_category_encoding_with_invalid_num_tokens(self):\n+        with self.assertRaisesRegex(ValueError, r\"`num_tokens` must be >= 1\"):\n+            layers.CategoryEncoding(num_tokens=0, output_mode=\"multi_hot\")\n+\n+        with self.assertRaisesRegex(ValueError, r\"`num_tokens` must be >= 1\"):\n+            layers.CategoryEncoding(num_tokens=-1, output_mode=\"multi_hot\")\n+\n+    def test_category_encoding_with_unnecessary_count_weights(self):\n+        layer = layers.CategoryEncoding(num_tokens=4, output_mode=\"multi_hot\")\n+        input_data = np.array([0, 1, 2, 3])\n+        count_weights = np.array([0.1, 0.2, 0.3, 0.4])\n+        with self.assertRaisesRegex(\n+            ValueError, r\"`count_weights` is not used when `output_mode`\"\n+        ):\n+            layer(input_data, count_weights=count_weights)\n+\n+    def test_invalid_output_mode_raises_error(self):\n+        with self.assertRaisesRegex(\n+            ValueError, r\"Unknown arg for output_mode: invalid_mode\"\n+        ):\n+            layers.CategoryEncoding(num_tokens=4, output_mode=\"invalid_mode\")\n+\n+    def test_encode_one_hot_single_sample(self):\n+        layer = layers.CategoryEncoding(num_tokens=4, output_mode=\"one_hot\")\n+        input_array = np.array([1, 2, 3, 1])\n+        expected_output = np.array(\n+            [\n+                [0, 1, 0, 0],\n+                [0, 0, 1, 0],\n+                [0, 0, 0, 1],\n+                [0, 1, 0, 0],\n+            ]\n+        )\n+        output = layer._encode(input_array)\n+        self.assertAllClose(expected_output, output)\n+\n+    def test_encode_one_hot_batched_samples(self):\n+        layer = layers.CategoryEncoding(num_tokens=4, output_mode=\"one_hot\")\n+        input_array = np.array([[3, 2, 0, 1], [3, 2, 0, 1]])\n+        expected_output = np.array(\n+            [\n+                [[0, 0, 0, 1], [0, 0, 1, 0], [1, 0, 0, 0], [0, 1, 0, 0]],\n+                [[0, 0, 0, 1], [0, 0, 1, 0], [1, 0, 0, 0], [0, 1, 0, 0]],\n+            ]\n+        )\n+        output = layer._encode(input_array)\n+        self.assertAllClose(expected_output, output)\n+\n+    def test_count_single_sample(self):\n+        layer = layers.CategoryEncoding(num_tokens=4, output_mode=\"count\")\n+        input_array = np.array([1, 2, 3, 1])\n+        expected_output = np.array([0, 2, 1, 1])\n+        output = layer._count(input_array)\n+        self.assertAllClose(expected_output, output)\n+\n+    def test_count_batched_samples(self):\n+        layer = layers.CategoryEncoding(num_tokens=4, output_mode=\"count\")\n+        input_array = np.array([[1, 2, 3, 1], [0, 3, 1, 0]])\n+        expected_output = np.array([[0, 2, 1, 1], [2, 1, 0, 1]])\n+        output = layer._count(input_array)\n+        self.assertAllClose(expected_output, output)\n",
            "whole_hunk": "@@ -260,3 +260,71 @@ class CategoryEncodingTest(testing.TestCase, parameterized.TestCase):\n         for output in ds.take(1):\n             output = output.numpy()\n         self.assertAllClose(output, expected_output)\n+\n+    def test_category_encoding_without_num_tokens(self):\n+        with self.assertRaisesRegex(\n+            ValueError, r\"num_tokens must be set to use this layer\"\n+        ):\n+            layers.CategoryEncoding(output_mode=\"multi_hot\")\n+\n+    def test_category_encoding_with_invalid_num_tokens(self):\n+        with self.assertRaisesRegex(ValueError, r\"`num_tokens` must be >= 1\"):\n+            layers.CategoryEncoding(num_tokens=0, output_mode=\"multi_hot\")\n+\n+        with self.assertRaisesRegex(ValueError, r\"`num_tokens` must be >= 1\"):\n+            layers.CategoryEncoding(num_tokens=-1, output_mode=\"multi_hot\")\n+\n+    def test_category_encoding_with_unnecessary_count_weights(self):\n+        layer = layers.CategoryEncoding(num_tokens=4, output_mode=\"multi_hot\")\n+        input_data = np.array([0, 1, 2, 3])\n+        count_weights = np.array([0.1, 0.2, 0.3, 0.4])\n+        with self.assertRaisesRegex(\n+            ValueError, r\"`count_weights` is not used when `output_mode`\"\n+        ):\n+            layer(input_data, count_weights=count_weights)\n+\n+    def test_invalid_output_mode_raises_error(self):\n+        with self.assertRaisesRegex(\n+            ValueError, r\"Unknown arg for output_mode: invalid_mode\"\n+        ):\n+            layers.CategoryEncoding(num_tokens=4, output_mode=\"invalid_mode\")\n+\n+    def test_encode_one_hot_single_sample(self):\n+        layer = layers.CategoryEncoding(num_tokens=4, output_mode=\"one_hot\")\n+        input_array = np.array([1, 2, 3, 1])\n+        expected_output = np.array(\n+            [\n+                [0, 1, 0, 0],\n+                [0, 0, 1, 0],\n+                [0, 0, 0, 1],\n+                [0, 1, 0, 0],\n+            ]\n+        )\n+        output = layer._encode(input_array)\n+        self.assertAllClose(expected_output, output)\n+\n+    def test_encode_one_hot_batched_samples(self):\n+        layer = layers.CategoryEncoding(num_tokens=4, output_mode=\"one_hot\")\n+        input_array = np.array([[3, 2, 0, 1], [3, 2, 0, 1]])\n+        expected_output = np.array(\n+            [\n+                [[0, 0, 0, 1], [0, 0, 1, 0], [1, 0, 0, 0], [0, 1, 0, 0]],\n+                [[0, 0, 0, 1], [0, 0, 1, 0], [1, 0, 0, 0], [0, 1, 0, 0]],\n+            ]\n+        )\n+        output = layer._encode(input_array)\n+        self.assertAllClose(expected_output, output)\n+\n+    def test_count_single_sample(self):\n+        layer = layers.CategoryEncoding(num_tokens=4, output_mode=\"count\")\n+        input_array = np.array([1, 2, 3, 1])\n+        expected_output = np.array([0, 2, 1, 1])\n+        output = layer._count(input_array)\n+        self.assertAllClose(expected_output, output)\n+\n+    def test_count_batched_samples(self):\n+        layer = layers.CategoryEncoding(num_tokens=4, output_mode=\"count\")\n+        input_array = np.array([[1, 2, 3, 1], [0, 3, 1, 0]])\n+        expected_output = np.array([[0, 2, 1, 1], [2, 1, 0, 1]])\n+        output = layer._count(input_array)\n+        self.assertAllClose(expected_output, output)\n"
        },
        {
            "name": "hashing_test.py",
            "path": "keras/layers/preprocessing/hashing_test.py",
            "patches": [
                {
                    "old_start": 393,
                    "old_length": 6,
                    "new_start": 393,
                    "new_length": 43,
                    "hunk": "@@ -393,6 +393,43 @@ class HashingTest(testing.TestCase, parameterized.TestCase):\n             expected, backend.convert_to_numpy(out_data).tolist()\n         )\n \n+    def test_hashing_invalid_num_bins(self):\n+        # Test with `num_bins` set to None\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"The `num_bins` for `Hashing` cannot be `None` or non-positive\",\n+        ):\n+            layers.Hashing(num_bins=None)\n+\n+        # Test with `num_bins` set to 0\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"The `num_bins` for `Hashing` cannot be `None` or non-positive\",\n+        ):\n+            layers.Hashing(num_bins=0)\n+\n+    def test_hashing_invalid_output_mode(self):\n+        # Test with an unsupported `output_mode`\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `output_mode`. Expected one of\",\n+        ):\n+            layers.Hashing(num_bins=3, output_mode=\"unsupported_mode\")\n+\n+    def test_hashing_invalid_dtype_for_int_mode(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            'When `output_mode=\"int\"`, `dtype` should be an integer type,',\n+        ):\n+            layers.Hashing(num_bins=3, output_mode=\"int\", dtype=\"float32\")\n+\n+    def test_hashing_sparse_with_int_mode(self):\n+        # Test setting `sparse=True` with `output_mode='int'`\n+        with self.assertRaisesRegex(\n+            ValueError, \"`sparse` may only be true if `output_mode` is\"\n+        ):\n+            layers.Hashing(num_bins=3, output_mode=\"int\", sparse=True)\n+\n \n # TODO: support tf.RaggedTensor.\n # def test_hash_ragged_string_input_farmhash(self):\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    def test_hashing_invalid_num_bins(self):\n+        # Test with `num_bins` set to None\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"The `num_bins` for `Hashing` cannot be `None` or non-positive\",\n+        ):\n+            layers.Hashing(num_bins=None)\n+\n+        # Test with `num_bins` set to 0\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"The `num_bins` for `Hashing` cannot be `None` or non-positive\",\n+        ):\n+            layers.Hashing(num_bins=0)\n+\n+    def test_hashing_invalid_output_mode(self):\n+        # Test with an unsupported `output_mode`\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `output_mode`. Expected one of\",\n+        ):\n+            layers.Hashing(num_bins=3, output_mode=\"unsupported_mode\")\n+\n+    def test_hashing_invalid_dtype_for_int_mode(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            'When `output_mode=\"int\"`, `dtype` should be an integer type,',\n+        ):\n+            layers.Hashing(num_bins=3, output_mode=\"int\", dtype=\"float32\")\n+\n+    def test_hashing_sparse_with_int_mode(self):\n+        # Test setting `sparse=True` with `output_mode='int'`\n+        with self.assertRaisesRegex(\n+            ValueError, \"`sparse` may only be true if `output_mode` is\"\n+        ):\n+            layers.Hashing(num_bins=3, output_mode=\"int\", sparse=True)\n+\n",
            "whole_hunk": "@@ -393,6 +393,43 @@ class HashingTest(testing.TestCase, parameterized.TestCase):\n             expected, backend.convert_to_numpy(out_data).tolist()\n         )\n \n+    def test_hashing_invalid_num_bins(self):\n+        # Test with `num_bins` set to None\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"The `num_bins` for `Hashing` cannot be `None` or non-positive\",\n+        ):\n+            layers.Hashing(num_bins=None)\n+\n+        # Test with `num_bins` set to 0\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"The `num_bins` for `Hashing` cannot be `None` or non-positive\",\n+        ):\n+            layers.Hashing(num_bins=0)\n+\n+    def test_hashing_invalid_output_mode(self):\n+        # Test with an unsupported `output_mode`\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `output_mode`. Expected one of\",\n+        ):\n+            layers.Hashing(num_bins=3, output_mode=\"unsupported_mode\")\n+\n+    def test_hashing_invalid_dtype_for_int_mode(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            'When `output_mode=\"int\"`, `dtype` should be an integer type,',\n+        ):\n+            layers.Hashing(num_bins=3, output_mode=\"int\", dtype=\"float32\")\n+\n+    def test_hashing_sparse_with_int_mode(self):\n+        # Test setting `sparse=True` with `output_mode='int'`\n+        with self.assertRaisesRegex(\n+            ValueError, \"`sparse` may only be true if `output_mode` is\"\n+        ):\n+            layers.Hashing(num_bins=3, output_mode=\"int\", sparse=True)\n+\n \n # TODO: support tf.RaggedTensor.\n # def test_hash_ragged_string_input_farmhash(self):\n"
        },
        {
            "name": "index_lookup_test.py",
            "path": "keras/layers/preprocessing/index_lookup_test.py",
            "patches": [
                {
                    "old_start": 427,
                    "old_length": 3,
                    "new_start": 427,
                    "new_length": 193,
                    "hunk": "@@ -427,3 +427,193 @@ class IndexLookupLayerTest(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(list(output), [2, 3, 1])\n         if backend.backend() != \"torch\":\n             self.run_class_serialization_test(layer)\n+\n+    def test_max_tokens_less_than_two(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"If set, `max_tokens` must be greater than 1.\",\n+        ):\n+            layers.IndexLookup(\n+                max_tokens=1,\n+                num_oov_indices=1,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"int64\",\n+            )\n+\n+    def test_max_tokens_none_with_pad_to_max_tokens(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"If pad_to_max_tokens is True, must set `max_tokens`.\",\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"int64\",\n+                pad_to_max_tokens=True,\n+            )\n+\n+    def test_negative_num_oov_indices(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"`num_oov_indices` must be greater than or equal to 0.\",\n+        ):\n+            layers.IndexLookup(\n+                max_tokens=10,\n+                num_oov_indices=-1,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"int64\",\n+            )\n+\n+    def test_invert_with_non_int_output_mode(self):\n+        with self.assertRaisesRegex(\n+            ValueError, r\"`output_mode` must be `'int'` when `invert` is true.\"\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                invert=True,\n+                output_mode=\"one_hot\",  # Invalid combination\n+            )\n+\n+    def test_sparse_true_with_int_output_mode(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`sparse` may only be true if `output_mode` is `'one_hot'`\",\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                sparse=True,\n+                output_mode=\"int\",  # Invalid combination\n+            )\n+\n+    def test_idf_weights_set_with_non_tfidf_output_mode(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`idf_weights` should only be set if `output_mode` is `'tf_idf'`\",\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                idf_weights=[\n+                    0.5,\n+                    0.1,\n+                    0.3,\n+                ],  # Should not be set for non-TF-IDF modes\n+                output_mode=\"int\",\n+            )\n+\n+    def test_unrecognized_kwargs(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"Unrecognized keyword argument\"\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                output_mode=\"int\",\n+                # This is an unrecognized argument\n+                extra_arg=True,\n+            )\n+\n+    def test_non_tf_idf_with_idf_weights(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"`idf_weights` should only be set if `output_mode` is\",\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                output_mode=\"multi_hot\",\n+                idf_weights=[\n+                    0.5,\n+                    0.1,\n+                    0.3,\n+                ],  # idf_weights not valid for multi_hot mode\n+            )\n+\n+    def test_vocabulary_file_does_not_exist(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Vocabulary file path/to/missing_vocab.txt does not exist\",\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                output_mode=\"int\",\n+                # Nonexistent file path\n+                vocabulary=\"path/to/missing_vocab.txt\",\n+            )\n+\n+    def test_repeated_tokens_in_vocabulary(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"The passed vocabulary has at least one repeated term.\"\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                vocabulary=[\"token\", \"token\", \"unique\"],\n+            )\n+\n+    def test_mask_token_in_wrong_position(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Found reserved mask token at unexpected location in `vocabulary`.\",\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=\"mask\",\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                vocabulary=[\n+                    \"token\",\n+                    \"mask\",\n+                    \"unique\",\n+                ],  # 'mask' should be at the start if included explicitly\n+            )\n+\n+    def test_ensure_known_vocab_size_without_vocabulary(self):\n+        kwargs = {\n+            \"num_oov_indices\": 1,\n+            # Assume empty string or some default token is valid.\n+            \"mask_token\": \"\",\n+            # Assume [OOV] or some default token is valid.\n+            \"oov_token\": \"[OOV]\",\n+            \"output_mode\": \"multi_hot\",\n+            \"pad_to_max_tokens\": False,\n+            \"vocabulary_dtype\": \"string\",\n+            \"max_tokens\": None,\n+        }\n+        layer = layers.IndexLookup(**kwargs)\n+\n+        # Try calling the layer without setting the vocabulary.\n+        with self.assertRaisesRegex(\n+            RuntimeError, \"When using `output_mode=multi_hot` and\"\n+        ):\n+            input_data = [\"sample\", \"data\"]\n+            layer(input_data)\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+\n+    def test_max_tokens_less_than_two(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"If set, `max_tokens` must be greater than 1.\",\n+        ):\n+            layers.IndexLookup(\n+                max_tokens=1,\n+                num_oov_indices=1,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"int64\",\n+            )\n+\n+    def test_max_tokens_none_with_pad_to_max_tokens(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"If pad_to_max_tokens is True, must set `max_tokens`.\",\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"int64\",\n+                pad_to_max_tokens=True,\n+            )\n+\n+    def test_negative_num_oov_indices(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"`num_oov_indices` must be greater than or equal to 0.\",\n+        ):\n+            layers.IndexLookup(\n+                max_tokens=10,\n+                num_oov_indices=-1,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"int64\",\n+            )\n+\n+    def test_invert_with_non_int_output_mode(self):\n+        with self.assertRaisesRegex(\n+            ValueError, r\"`output_mode` must be `'int'` when `invert` is true.\"\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                invert=True,\n+                output_mode=\"one_hot\",  # Invalid combination\n+            )\n+\n+    def test_sparse_true_with_int_output_mode(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`sparse` may only be true if `output_mode` is `'one_hot'`\",\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                sparse=True,\n+                output_mode=\"int\",  # Invalid combination\n+            )\n+\n+    def test_idf_weights_set_with_non_tfidf_output_mode(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`idf_weights` should only be set if `output_mode` is `'tf_idf'`\",\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                idf_weights=[\n+                    0.5,\n+                    0.1,\n+                    0.3,\n+                ],  # Should not be set for non-TF-IDF modes\n+                output_mode=\"int\",\n+            )\n+\n+    def test_unrecognized_kwargs(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"Unrecognized keyword argument\"\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                output_mode=\"int\",\n+                # This is an unrecognized argument\n+                extra_arg=True,\n+            )\n+\n+    def test_non_tf_idf_with_idf_weights(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"`idf_weights` should only be set if `output_mode` is\",\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                output_mode=\"multi_hot\",\n+                idf_weights=[\n+                    0.5,\n+                    0.1,\n+                    0.3,\n+                ],  # idf_weights not valid for multi_hot mode\n+            )\n+\n+    def test_vocabulary_file_does_not_exist(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Vocabulary file path/to/missing_vocab.txt does not exist\",\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                output_mode=\"int\",\n+                # Nonexistent file path\n+                vocabulary=\"path/to/missing_vocab.txt\",\n+            )\n+\n+    def test_repeated_tokens_in_vocabulary(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"The passed vocabulary has at least one repeated term.\"\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                vocabulary=[\"token\", \"token\", \"unique\"],\n+            )\n+\n+    def test_mask_token_in_wrong_position(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Found reserved mask token at unexpected location in `vocabulary`.\",\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=\"mask\",\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                vocabulary=[\n+                    \"token\",\n+                    \"mask\",\n+                    \"unique\",\n+                ],  # 'mask' should be at the start if included explicitly\n+            )\n+\n+    def test_ensure_known_vocab_size_without_vocabulary(self):\n+        kwargs = {\n+            \"num_oov_indices\": 1,\n+            # Assume empty string or some default token is valid.\n+            \"mask_token\": \"\",\n+            # Assume [OOV] or some default token is valid.\n+            \"oov_token\": \"[OOV]\",\n+            \"output_mode\": \"multi_hot\",\n+            \"pad_to_max_tokens\": False,\n+            \"vocabulary_dtype\": \"string\",\n+            \"max_tokens\": None,\n+        }\n+        layer = layers.IndexLookup(**kwargs)\n+\n+        # Try calling the layer without setting the vocabulary.\n+        with self.assertRaisesRegex(\n+            RuntimeError, \"When using `output_mode=multi_hot` and\"\n+        ):\n+            input_data = [\"sample\", \"data\"]\n+            layer(input_data)\n",
            "whole_hunk": "@@ -427,3 +427,193 @@ class IndexLookupLayerTest(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(list(output), [2, 3, 1])\n         if backend.backend() != \"torch\":\n             self.run_class_serialization_test(layer)\n+\n+    def test_max_tokens_less_than_two(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"If set, `max_tokens` must be greater than 1.\",\n+        ):\n+            layers.IndexLookup(\n+                max_tokens=1,\n+                num_oov_indices=1,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"int64\",\n+            )\n+\n+    def test_max_tokens_none_with_pad_to_max_tokens(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"If pad_to_max_tokens is True, must set `max_tokens`.\",\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"int64\",\n+                pad_to_max_tokens=True,\n+            )\n+\n+    def test_negative_num_oov_indices(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"`num_oov_indices` must be greater than or equal to 0.\",\n+        ):\n+            layers.IndexLookup(\n+                max_tokens=10,\n+                num_oov_indices=-1,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"int64\",\n+            )\n+\n+    def test_invert_with_non_int_output_mode(self):\n+        with self.assertRaisesRegex(\n+            ValueError, r\"`output_mode` must be `'int'` when `invert` is true.\"\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                invert=True,\n+                output_mode=\"one_hot\",  # Invalid combination\n+            )\n+\n+    def test_sparse_true_with_int_output_mode(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`sparse` may only be true if `output_mode` is `'one_hot'`\",\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                sparse=True,\n+                output_mode=\"int\",  # Invalid combination\n+            )\n+\n+    def test_idf_weights_set_with_non_tfidf_output_mode(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`idf_weights` should only be set if `output_mode` is `'tf_idf'`\",\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                idf_weights=[\n+                    0.5,\n+                    0.1,\n+                    0.3,\n+                ],  # Should not be set for non-TF-IDF modes\n+                output_mode=\"int\",\n+            )\n+\n+    def test_unrecognized_kwargs(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"Unrecognized keyword argument\"\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                output_mode=\"int\",\n+                # This is an unrecognized argument\n+                extra_arg=True,\n+            )\n+\n+    def test_non_tf_idf_with_idf_weights(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"`idf_weights` should only be set if `output_mode` is\",\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                output_mode=\"multi_hot\",\n+                idf_weights=[\n+                    0.5,\n+                    0.1,\n+                    0.3,\n+                ],  # idf_weights not valid for multi_hot mode\n+            )\n+\n+    def test_vocabulary_file_does_not_exist(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Vocabulary file path/to/missing_vocab.txt does not exist\",\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                output_mode=\"int\",\n+                # Nonexistent file path\n+                vocabulary=\"path/to/missing_vocab.txt\",\n+            )\n+\n+    def test_repeated_tokens_in_vocabulary(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"The passed vocabulary has at least one repeated term.\"\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=None,\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                vocabulary=[\"token\", \"token\", \"unique\"],\n+            )\n+\n+    def test_mask_token_in_wrong_position(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Found reserved mask token at unexpected location in `vocabulary`.\",\n+        ):\n+            layers.IndexLookup(\n+                num_oov_indices=1,\n+                max_tokens=None,\n+                mask_token=\"mask\",\n+                oov_token=None,\n+                vocabulary_dtype=\"string\",\n+                vocabulary=[\n+                    \"token\",\n+                    \"mask\",\n+                    \"unique\",\n+                ],  # 'mask' should be at the start if included explicitly\n+            )\n+\n+    def test_ensure_known_vocab_size_without_vocabulary(self):\n+        kwargs = {\n+            \"num_oov_indices\": 1,\n+            # Assume empty string or some default token is valid.\n+            \"mask_token\": \"\",\n+            # Assume [OOV] or some default token is valid.\n+            \"oov_token\": \"[OOV]\",\n+            \"output_mode\": \"multi_hot\",\n+            \"pad_to_max_tokens\": False,\n+            \"vocabulary_dtype\": \"string\",\n+            \"max_tokens\": None,\n+        }\n+        layer = layers.IndexLookup(**kwargs)\n+\n+        # Try calling the layer without setting the vocabulary.\n+        with self.assertRaisesRegex(\n+            RuntimeError, \"When using `output_mode=multi_hot` and\"\n+        ):\n+            input_data = [\"sample\", \"data\"]\n+            layer(input_data)\n"
        },
        {
            "name": "normalization_test.py",
            "path": "keras/layers/preprocessing/normalization_test.py",
            "patches": [
                {
                    "old_start": 91,
                    "old_length": 10,
                    "new_start": 91,
                    "new_length": 6,
                    "hunk": "@@ -91,10 +91,6 @@ class NormalizationTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(np.var(output, axis=(0, 3)), 1.0, atol=1e-5)\n         self.assertAllClose(np.mean(output, axis=(0, 3)), 0.0, atol=1e-5)\n \n-    def test_normalization_errors(self):\n-        # TODO\n-        pass\n-\n     @pytest.mark.skipif(\n         backend.backend() != \"torch\",\n         reason=\"Test symbolic call for torch meta device.\",\n"
                },
                {
                    "old_start": 107,
                    "old_length": 3,
                    "new_start": 103,
                    "new_length": 44,
                    "hunk": "@@ -107,3 +103,44 @@ class NormalizationTest(testing.TestCase, parameterized.TestCase):\n         layer.adapt(data)\n         with core.device_scope(\"meta\"):\n             layer(data)\n+\n+    def test_normalization_with_mean_only_raises_error(self):\n+        # Test error when only `mean` is provided\n+        with self.assertRaisesRegex(\n+            ValueError, \"both `mean` and `variance` must be set\"\n+        ):\n+            layers.Normalization(mean=0.5)\n+\n+    def test_normalization_with_variance_only_raises_error(self):\n+        # Test error when only `variance` is provided\n+        with self.assertRaisesRegex(\n+            ValueError, \"both `mean` and `variance` must be set\"\n+        ):\n+            layers.Normalization(variance=0.1)\n+\n+    def test_normalization_axis_too_high(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"All `axis` values must be in the range\"\n+        ):\n+            layer = layers.Normalization(axis=3)\n+            layer.build((2, 2))\n+\n+    def test_normalization_axis_too_low(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"All `axis` values must be in the range\"\n+        ):\n+            layer = layers.Normalization(axis=-4)\n+            layer.build((2, 3, 4))\n+\n+    def test_normalization_unknown_axis_shape(self):\n+        with self.assertRaisesRegex(ValueError, \"All `axis` values to be kept\"):\n+            layer = layers.Normalization(axis=1)\n+            layer.build((None, None))\n+\n+    def test_normalization_adapt_with_incompatible_shape(self):\n+        layer = layers.Normalization(axis=-1)\n+        initial_shape = (10, 5)\n+        layer.build(initial_shape)\n+        new_shape_data = np.random.random((10, 3))\n+        with self.assertRaisesRegex(ValueError, \"an incompatible shape\"):\n+            layer.adapt(new_shape_data)\n"
                }
            ],
            "whole_deleted": "-    def test_normalization_errors(self):\n-        # TODO\n-        pass\n-\n",
            "whole_added": "+\n+    def test_normalization_with_mean_only_raises_error(self):\n+        # Test error when only `mean` is provided\n+        with self.assertRaisesRegex(\n+            ValueError, \"both `mean` and `variance` must be set\"\n+        ):\n+            layers.Normalization(mean=0.5)\n+\n+    def test_normalization_with_variance_only_raises_error(self):\n+        # Test error when only `variance` is provided\n+        with self.assertRaisesRegex(\n+            ValueError, \"both `mean` and `variance` must be set\"\n+        ):\n+            layers.Normalization(variance=0.1)\n+\n+    def test_normalization_axis_too_high(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"All `axis` values must be in the range\"\n+        ):\n+            layer = layers.Normalization(axis=3)\n+            layer.build((2, 2))\n+\n+    def test_normalization_axis_too_low(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"All `axis` values must be in the range\"\n+        ):\n+            layer = layers.Normalization(axis=-4)\n+            layer.build((2, 3, 4))\n+\n+    def test_normalization_unknown_axis_shape(self):\n+        with self.assertRaisesRegex(ValueError, \"All `axis` values to be kept\"):\n+            layer = layers.Normalization(axis=1)\n+            layer.build((None, None))\n+\n+    def test_normalization_adapt_with_incompatible_shape(self):\n+        layer = layers.Normalization(axis=-1)\n+        initial_shape = (10, 5)\n+        layer.build(initial_shape)\n+        new_shape_data = np.random.random((10, 3))\n+        with self.assertRaisesRegex(ValueError, \"an incompatible shape\"):\n+            layer.adapt(new_shape_data)\n",
            "whole_hunk": "@@ -91,10 +91,6 @@ class NormalizationTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(np.var(output, axis=(0, 3)), 1.0, atol=1e-5)\n         self.assertAllClose(np.mean(output, axis=(0, 3)), 0.0, atol=1e-5)\n \n-    def test_normalization_errors(self):\n-        # TODO\n-        pass\n-\n     @pytest.mark.skipif(\n         backend.backend() != \"torch\",\n         reason=\"Test symbolic call for torch meta device.\",\n@@ -107,3 +103,44 @@ class NormalizationTest(testing.TestCase, parameterized.TestCase):\n         layer.adapt(data)\n         with core.device_scope(\"meta\"):\n             layer(data)\n+\n+    def test_normalization_with_mean_only_raises_error(self):\n+        # Test error when only `mean` is provided\n+        with self.assertRaisesRegex(\n+            ValueError, \"both `mean` and `variance` must be set\"\n+        ):\n+            layers.Normalization(mean=0.5)\n+\n+    def test_normalization_with_variance_only_raises_error(self):\n+        # Test error when only `variance` is provided\n+        with self.assertRaisesRegex(\n+            ValueError, \"both `mean` and `variance` must be set\"\n+        ):\n+            layers.Normalization(variance=0.1)\n+\n+    def test_normalization_axis_too_high(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"All `axis` values must be in the range\"\n+        ):\n+            layer = layers.Normalization(axis=3)\n+            layer.build((2, 2))\n+\n+    def test_normalization_axis_too_low(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"All `axis` values must be in the range\"\n+        ):\n+            layer = layers.Normalization(axis=-4)\n+            layer.build((2, 3, 4))\n+\n+    def test_normalization_unknown_axis_shape(self):\n+        with self.assertRaisesRegex(ValueError, \"All `axis` values to be kept\"):\n+            layer = layers.Normalization(axis=1)\n+            layer.build((None, None))\n+\n+    def test_normalization_adapt_with_incompatible_shape(self):\n+        layer = layers.Normalization(axis=-1)\n+        initial_shape = (10, 5)\n+        layer.build(initial_shape)\n+        new_shape_data = np.random.random((10, 3))\n+        with self.assertRaisesRegex(ValueError, \"an incompatible shape\"):\n+            layer.adapt(new_shape_data)\n"
        },
        {
            "name": "random_brightness.py",
            "path": "keras/layers/preprocessing/random_brightness.py",
            "patches": [
                {
                    "old_start": 80,
                    "old_length": 12,
                    "new_start": 80,
                    "new_length": 12,
                    "hunk": "@@ -80,12 +80,12 @@ class RandomBrightness(TFDataLayer):\n     def _set_value_range(self, value_range):\n         if not isinstance(value_range, (tuple, list)):\n             raise ValueError(\n-                self.value_range_VALIDATION_ERROR\n+                self._VALUE_RANGE_VALIDATION_ERROR\n                 + f\"Received: value_range={value_range}\"\n             )\n         if len(value_range) != 2:\n             raise ValueError(\n-                self.value_range_VALIDATION_ERROR\n+                self._VALUE_RANGE_VALIDATION_ERROR\n                 + f\"Received: value_range={value_range}\"\n             )\n         self.value_range = sorted(value_range)\n"
                }
            ],
            "whole_deleted": "-                self.value_range_VALIDATION_ERROR\n-                self.value_range_VALIDATION_ERROR\n",
            "whole_added": "+                self._VALUE_RANGE_VALIDATION_ERROR\n+                self._VALUE_RANGE_VALIDATION_ERROR\n",
            "whole_hunk": "@@ -80,12 +80,12 @@ class RandomBrightness(TFDataLayer):\n     def _set_value_range(self, value_range):\n         if not isinstance(value_range, (tuple, list)):\n             raise ValueError(\n-                self.value_range_VALIDATION_ERROR\n+                self._VALUE_RANGE_VALIDATION_ERROR\n                 + f\"Received: value_range={value_range}\"\n             )\n         if len(value_range) != 2:\n             raise ValueError(\n-                self.value_range_VALIDATION_ERROR\n+                self._VALUE_RANGE_VALIDATION_ERROR\n                 + f\"Received: value_range={value_range}\"\n             )\n         self.value_range = sorted(value_range)\n"
        },
        {
            "name": "random_brightness_test.py",
            "path": "keras/layers/preprocessing/random_brightness_test.py",
            "patches": [
                {
                    "old_start": 58,
                    "old_length": 3,
                    "new_start": 58,
                    "new_length": 59,
                    "hunk": "@@ -58,3 +58,59 @@ class RandomBrightnessTest(testing.TestCase):\n         ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n         for output in ds.take(1):\n             output.numpy()\n+\n+    def test_value_range_incorrect_type(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"The `value_range` argument should be a list of two numbers.*\",\n+        ):\n+            layers.RandomBrightness(factor=0.1, value_range=\"incorrect_type\")\n+\n+    def test_value_range_incorrect_length(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"The `value_range` argument should be a list of two numbers.*\",\n+        ):\n+            layers.RandomBrightness(factor=0.1, value_range=[10])\n+\n+    def test_set_factor_incorrect_length(self):\n+        layer = layers.RandomBrightness(factor=0.5)\n+        with self.assertRaisesRegex(\n+            ValueError, \"The `factor` argument should be a number.*\"\n+        ):\n+            layer._set_factor([0.1])  # Only one element in list\n+\n+    def test_set_factor_incorrect_type(self):\n+        layer = layers.RandomBrightness(factor=0.5)\n+        with self.assertRaisesRegex(\n+            ValueError, \"The `factor` argument should be a number.*\"\n+        ):\n+            layer._set_factor(\n+                \"invalid_type\"\n+            )  # Passing a string instead of a number or a list/tuple of numbers\n+\n+    def test_factor_range_below_lower_bound(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"The `factor` argument should be a number.*\"\n+        ):\n+            # Passing a value less than -1.0\n+            layers.RandomBrightness(factor=-1.1)\n+\n+    def test_factor_range_above_upper_bound(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"The `factor` argument should be a number.*\"\n+        ):\n+            # Passing a value more than 1.0\n+            layers.RandomBrightness(factor=1.1)\n+\n+    def test_randomly_adjust_brightness_input_incorrect_rank(self):\n+        layer = layers.RandomBrightness(factor=0.1)\n+        wrong_rank_input = np.random.rand(10, 10)\n+\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Expected the input image to be rank 3 or 4.\",\n+        ):\n+            layer(\n+                wrong_rank_input, training=True\n+            )  # Call the method that triggers the error"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+\n+    def test_value_range_incorrect_type(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"The `value_range` argument should be a list of two numbers.*\",\n+        ):\n+            layers.RandomBrightness(factor=0.1, value_range=\"incorrect_type\")\n+\n+    def test_value_range_incorrect_length(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"The `value_range` argument should be a list of two numbers.*\",\n+        ):\n+            layers.RandomBrightness(factor=0.1, value_range=[10])\n+\n+    def test_set_factor_incorrect_length(self):\n+        layer = layers.RandomBrightness(factor=0.5)\n+        with self.assertRaisesRegex(\n+            ValueError, \"The `factor` argument should be a number.*\"\n+        ):\n+            layer._set_factor([0.1])  # Only one element in list\n+\n+    def test_set_factor_incorrect_type(self):\n+        layer = layers.RandomBrightness(factor=0.5)\n+        with self.assertRaisesRegex(\n+            ValueError, \"The `factor` argument should be a number.*\"\n+        ):\n+            layer._set_factor(\n+                \"invalid_type\"\n+            )  # Passing a string instead of a number or a list/tuple of numbers\n+\n+    def test_factor_range_below_lower_bound(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"The `factor` argument should be a number.*\"\n+        ):\n+            # Passing a value less than -1.0\n+            layers.RandomBrightness(factor=-1.1)\n+\n+    def test_factor_range_above_upper_bound(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"The `factor` argument should be a number.*\"\n+        ):\n+            # Passing a value more than 1.0\n+            layers.RandomBrightness(factor=1.1)\n+\n+    def test_randomly_adjust_brightness_input_incorrect_rank(self):\n+        layer = layers.RandomBrightness(factor=0.1)\n+        wrong_rank_input = np.random.rand(10, 10)\n+\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Expected the input image to be rank 3 or 4.\",\n+        ):\n+            layer(\n+                wrong_rank_input, training=True\n+            )  # Call the method that triggers the error\n",
            "whole_hunk": "@@ -58,3 +58,59 @@ class RandomBrightnessTest(testing.TestCase):\n         ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n         for output in ds.take(1):\n             output.numpy()\n+\n+    def test_value_range_incorrect_type(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"The `value_range` argument should be a list of two numbers.*\",\n+        ):\n+            layers.RandomBrightness(factor=0.1, value_range=\"incorrect_type\")\n+\n+    def test_value_range_incorrect_length(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"The `value_range` argument should be a list of two numbers.*\",\n+        ):\n+            layers.RandomBrightness(factor=0.1, value_range=[10])\n+\n+    def test_set_factor_incorrect_length(self):\n+        layer = layers.RandomBrightness(factor=0.5)\n+        with self.assertRaisesRegex(\n+            ValueError, \"The `factor` argument should be a number.*\"\n+        ):\n+            layer._set_factor([0.1])  # Only one element in list\n+\n+    def test_set_factor_incorrect_type(self):\n+        layer = layers.RandomBrightness(factor=0.5)\n+        with self.assertRaisesRegex(\n+            ValueError, \"The `factor` argument should be a number.*\"\n+        ):\n+            layer._set_factor(\n+                \"invalid_type\"\n+            )  # Passing a string instead of a number or a list/tuple of numbers\n+\n+    def test_factor_range_below_lower_bound(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"The `factor` argument should be a number.*\"\n+        ):\n+            # Passing a value less than -1.0\n+            layers.RandomBrightness(factor=-1.1)\n+\n+    def test_factor_range_above_upper_bound(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"The `factor` argument should be a number.*\"\n+        ):\n+            # Passing a value more than 1.0\n+            layers.RandomBrightness(factor=1.1)\n+\n+    def test_randomly_adjust_brightness_input_incorrect_rank(self):\n+        layer = layers.RandomBrightness(factor=0.1)\n+        wrong_rank_input = np.random.rand(10, 10)\n+\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Expected the input image to be rank 3 or 4.\",\n+        ):\n+            layer(\n+                wrong_rank_input, training=True\n+            )  # Call the method that triggers the error"
        }
    ]
},
{
    "Id": 3,
    "commit_link": "https://github.com/keras-team/keras/commit/aa3a61bccca5770279ed83efd4737c0857212a9d",
    "date": "2024-03-22T21:44:13-07:00",
    "message": "Fix type check for input (#19362)\n\n* Fix type check for input\r\n\r\n* format",
    "changes": [
        {
            "name": "dot.py",
            "path": "keras/layers/merging/dot.py",
            "patches": [
                {
                    "old_start": 262,
                    "old_length": 7,
                    "new_start": 262,
                    "new_length": 10,
                    "hunk": "@@ -262,7 +262,10 @@ class Dot(Merge):\n \n     def build(self, input_shape):\n         # Used purely for shape validation.\n-        if not isinstance(input_shape[0], tuple) or len(input_shape) != 2:\n+        if (\n+            not isinstance(input_shape[0], (tuple, list))\n+            or len(input_shape) != 2\n+        ):\n             raise ValueError(\n                 f\"A `Dot` layer should be called on a list of 2 inputs. \"\n                 f\"Received: input_shape={input_shape}\""
                }
            ],
            "whole_deleted": "-        if not isinstance(input_shape[0], tuple) or len(input_shape) != 2:\n",
            "whole_added": "+        if (\n+            not isinstance(input_shape[0], (tuple, list))\n+            or len(input_shape) != 2\n+        ):\n",
            "whole_hunk": "@@ -262,7 +262,10 @@ class Dot(Merge):\n \n     def build(self, input_shape):\n         # Used purely for shape validation.\n-        if not isinstance(input_shape[0], tuple) or len(input_shape) != 2:\n+        if (\n+            not isinstance(input_shape[0], (tuple, list))\n+            or len(input_shape) != 2\n+        ):\n             raise ValueError(\n                 f\"A `Dot` layer should be called on a list of 2 inputs. \"\n                 f\"Received: input_shape={input_shape}\""
        }
    ]
},
{
    "Id": 4,
    "commit_link": "https://github.com/keras-team/keras/commit/b2ef949cceb01c53d231a4da9cbfbaa12cea981d",
    "date": "2024-03-18T15:25:43-07:00",
    "message": "Use Value dim shape for Attention compute_output_shape (#19284)\n\n* Use Value dim shape for Attention compute_output_shape\r\n\r\n* Fix attention layer compute output shape\r\n\r\n* fix format\r\n\r\n* check compute_output_shape with output",
    "changes": [
        {
            "name": "dtype_policy.py",
            "path": "keras/dtype_policies/dtype_policy.py",
            "patches": [
                {
                    "old_start": 173,
                    "old_length": 9,
                    "new_start": 173,
                    "new_length": 6,
                    "hunk": "@@ -173,9 +173,6 @@ class FloatDTypePolicy(DTypePolicy):\n             return \"float16\", \"float32\"\n         elif name == \"mixed_bfloat16\":\n             return \"bfloat16\", \"float32\"\n-        elif name == \"uint8\":\n-            dtype = backend.standardize_dtype(name)\n-            return dtype, dtype\n         try:\n             dtype = backend.standardize_dtype(name)\n             return dtype, dtype\n"
                }
            ],
            "whole_deleted": "-        elif name == \"uint8\":\n-            dtype = backend.standardize_dtype(name)\n-            return dtype, dtype\n",
            "whole_added": "",
            "whole_hunk": "@@ -173,9 +173,6 @@ class FloatDTypePolicy(DTypePolicy):\n             return \"float16\", \"float32\"\n         elif name == \"mixed_bfloat16\":\n             return \"bfloat16\", \"float32\"\n-        elif name == \"uint8\":\n-            dtype = backend.standardize_dtype(name)\n-            return dtype, dtype\n         try:\n             dtype = backend.standardize_dtype(name)\n             return dtype, dtype\n"
        },
        {
            "name": "attention.py",
            "path": "keras/layers/attention/attention.py",
            "patches": [
                {
                    "old_start": 242,
                    "old_length": 7,
                    "new_start": 242,
                    "new_length": 8,
                    "hunk": "@@ -242,7 +242,8 @@ class Attention(Layer):\n         return ops.convert_to_tensor(mask[0])\n \n     def compute_output_shape(self, input_shape):\n-        return input_shape[0]\n+        \"\"\"Returns shape of value tensor dim, but for query tensor length\"\"\"\n+        return (*input_shape[0][:-1], input_shape[1][-1])\n \n     def _validate_inputs(self, inputs, mask=None):\n         \"\"\"Validates arguments of the call method.\"\"\"\n"
                }
            ],
            "whole_deleted": "-        return input_shape[0]\n",
            "whole_added": "+        \"\"\"Returns shape of value tensor dim, but for query tensor length\"\"\"\n+        return (*input_shape[0][:-1], input_shape[1][-1])\n",
            "whole_hunk": "@@ -242,7 +242,8 @@ class Attention(Layer):\n         return ops.convert_to_tensor(mask[0])\n \n     def compute_output_shape(self, input_shape):\n-        return input_shape[0]\n+        \"\"\"Returns shape of value tensor dim, but for query tensor length\"\"\"\n+        return (*input_shape[0][:-1], input_shape[1][-1])\n \n     def _validate_inputs(self, inputs, mask=None):\n         \"\"\"Validates arguments of the call method.\"\"\"\n"
        },
        {
            "name": "attention_test.py",
            "path": "keras/layers/attention/attention_test.py",
            "patches": [
                {
                    "old_start": 342,
                    "old_length": 3,
                    "new_start": 342,
                    "new_length": 19,
                    "hunk": "@@ -342,3 +342,19 @@ class AttentionTest(testing.TestCase):\n             computed_mask = layer.compute_mask(inputs=dummy_inputs, mask=mask)\n             computed_mask = ops.convert_to_numpy(computed_mask)\n             self.assertTrue(np.array_equal(computed_mask, valid_mask))\n+\n+    def test_attention_compute_output_shape(self):\n+        layer = layers.Attention()\n+\n+        query = np.random.random((2, 3, 4))\n+        value = np.random.random((2, 3, 5))\n+        key = np.random.random((2, 3, 4))\n+        layer = layers.Attention()\n+        output = layer([query, value, key])\n+        self.assertAllEqual(output.shape, value.shape)\n+        self.assertAllEqual(\n+            layer.compute_output_shape(\n+                input_shape=[query.shape, value.shape, key.shape]\n+            ),\n+            output.shape,\n+        )"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+\n+    def test_attention_compute_output_shape(self):\n+        layer = layers.Attention()\n+\n+        query = np.random.random((2, 3, 4))\n+        value = np.random.random((2, 3, 5))\n+        key = np.random.random((2, 3, 4))\n+        layer = layers.Attention()\n+        output = layer([query, value, key])\n+        self.assertAllEqual(output.shape, value.shape)\n+        self.assertAllEqual(\n+            layer.compute_output_shape(\n+                input_shape=[query.shape, value.shape, key.shape]\n+            ),\n+            output.shape,\n+        )\n",
            "whole_hunk": "@@ -342,3 +342,19 @@ class AttentionTest(testing.TestCase):\n             computed_mask = layer.compute_mask(inputs=dummy_inputs, mask=mask)\n             computed_mask = ops.convert_to_numpy(computed_mask)\n             self.assertTrue(np.array_equal(computed_mask, valid_mask))\n+\n+    def test_attention_compute_output_shape(self):\n+        layer = layers.Attention()\n+\n+        query = np.random.random((2, 3, 4))\n+        value = np.random.random((2, 3, 5))\n+        key = np.random.random((2, 3, 4))\n+        layer = layers.Attention()\n+        output = layer([query, value, key])\n+        self.assertAllEqual(output.shape, value.shape)\n+        self.assertAllEqual(\n+            layer.compute_output_shape(\n+                input_shape=[query.shape, value.shape, key.shape]\n+            ),\n+            output.shape,\n+        )"
        }
    ]
},
{
    "Id": 5,
    "commit_link": "https://github.com/keras-team/keras/commit/c591329161302e9a413d2bde065f8fefffa7b921",
    "date": "2024-03-14T09:11:39-07:00",
    "message": "Fix mixed precision check in TestCase.run_layer_test: compare with output_spec dtype instead of hardcoded float16 (#19297)\n\n* Fix mixed precision check: compare with output spec dtype instead of hardcoded float16\r\n\r\n* Revert \"Fix mixed precision check: compare with output spec dtype instead of hardcoded float16\"\r\n\r\nThis reverts commit 94d55841610f3a79c8488032140c381942134524.\r\n\r\n* Restore changes\r\n\r\n* Trying to reformat code\r\n\r\n* Restore formatted code\r\n\r\n* Fix formatting\r\n\r\n* Fix black latest wrong formatting",
    "changes": [
        {
            "name": "test_case.py",
            "path": "keras/testing/test_case.py",
            "patches": [
                {
                    "old_start": 519,
                    "old_length": 14,
                    "new_start": 519,
                    "new_length": 29,
                    "hunk": "@@ -519,14 +519,29 @@ class TestCase(unittest.TestCase):\n \n             if run_mixed_precision_check:\n                 layer = layer_cls(**{**init_kwargs, \"dtype\": \"mixed_float16\"})\n+                input_spec = tree.map_structure(\n+                    lambda spec: KerasTensor(\n+                        spec.shape,\n+                        dtype=(\n+                            layer.compute_dtype\n+                            if layer.autocast\n+                            and backend.is_float_dtype(spec.dtype)\n+                            else spec.dtype\n+                        ),\n+                    ),\n+                    keras_tensor_inputs,\n+                )\n                 if isinstance(input_data, dict):\n                     output_data = layer(**input_data, **call_kwargs)\n+                    output_spec = layer.compute_output_spec(**input_spec)\n                 else:\n                     output_data = layer(input_data, **call_kwargs)\n-                for tensor in tree.flatten(output_data):\n+                    output_spec = layer.compute_output_spec(input_spec)\n+                for tensor, spec in zip(\n+                    tree.flatten(output_data), tree.flatten(output_spec)\n+                ):\n                     dtype = standardize_dtype(tensor.dtype)\n-                    if is_float_dtype(dtype):\n-                        self.assertEqual(dtype, \"float16\")\n+                    self.assertEqual(dtype, spec.dtype)\n                 for weight in layer.weights:\n                     dtype = standardize_dtype(weight.dtype)\n                     if is_float_dtype(dtype):"
                }
            ],
            "whole_deleted": "-                for tensor in tree.flatten(output_data):\n-                    if is_float_dtype(dtype):\n-                        self.assertEqual(dtype, \"float16\")\n",
            "whole_added": "+                input_spec = tree.map_structure(\n+                    lambda spec: KerasTensor(\n+                        spec.shape,\n+                        dtype=(\n+                            layer.compute_dtype\n+                            if layer.autocast\n+                            and backend.is_float_dtype(spec.dtype)\n+                            else spec.dtype\n+                        ),\n+                    ),\n+                    keras_tensor_inputs,\n+                )\n+                    output_spec = layer.compute_output_spec(**input_spec)\n+                    output_spec = layer.compute_output_spec(input_spec)\n+                for tensor, spec in zip(\n+                    tree.flatten(output_data), tree.flatten(output_spec)\n+                ):\n+                    self.assertEqual(dtype, spec.dtype)\n",
            "whole_hunk": "@@ -519,14 +519,29 @@ class TestCase(unittest.TestCase):\n \n             if run_mixed_precision_check:\n                 layer = layer_cls(**{**init_kwargs, \"dtype\": \"mixed_float16\"})\n+                input_spec = tree.map_structure(\n+                    lambda spec: KerasTensor(\n+                        spec.shape,\n+                        dtype=(\n+                            layer.compute_dtype\n+                            if layer.autocast\n+                            and backend.is_float_dtype(spec.dtype)\n+                            else spec.dtype\n+                        ),\n+                    ),\n+                    keras_tensor_inputs,\n+                )\n                 if isinstance(input_data, dict):\n                     output_data = layer(**input_data, **call_kwargs)\n+                    output_spec = layer.compute_output_spec(**input_spec)\n                 else:\n                     output_data = layer(input_data, **call_kwargs)\n-                for tensor in tree.flatten(output_data):\n+                    output_spec = layer.compute_output_spec(input_spec)\n+                for tensor, spec in zip(\n+                    tree.flatten(output_data), tree.flatten(output_spec)\n+                ):\n                     dtype = standardize_dtype(tensor.dtype)\n-                    if is_float_dtype(dtype):\n-                        self.assertEqual(dtype, \"float16\")\n+                    self.assertEqual(dtype, spec.dtype)\n                 for weight in layer.weights:\n                     dtype = standardize_dtype(weight.dtype)\n                     if is_float_dtype(dtype):"
        }
    ]
},
{
    "Id": 6,
    "commit_link": "https://github.com/keras-team/keras/commit/4f63678a72a2e7c16c32506189e5428d93d7b945",
    "date": "2024-03-08T08:29:11-08:00",
    "message": "fix isinstance check for tuple input in pack_x_y_sample_weight (#19269)",
    "changes": [
        {
            "name": "data_adapter_utils.py",
            "path": "keras/trainers/data_adapters/data_adapter_utils.py",
            "patches": [
                {
                    "old_start": 100,
                    "old_length": 7,
                    "new_start": 100,
                    "new_length": 7,
                    "hunk": "@@ -100,7 +100,7 @@ def pack_x_y_sample_weight(x, y=None, sample_weight=None):\n         # there is no ambiguity. This also makes NumPy and Dataset\n         # consistent in that the user does not have to wrap their Dataset\n         # data in an unnecessary tuple.\n-        if not isinstance(x, tuple or list):\n+        if not isinstance(x, (tuple, list)):\n             return x\n         else:\n             return (x,)"
                }
            ],
            "whole_deleted": "-        if not isinstance(x, tuple or list):\n",
            "whole_added": "+        if not isinstance(x, (tuple, list)):\n",
            "whole_hunk": "@@ -100,7 +100,7 @@ def pack_x_y_sample_weight(x, y=None, sample_weight=None):\n         # there is no ambiguity. This also makes NumPy and Dataset\n         # consistent in that the user does not have to wrap their Dataset\n         # data in an unnecessary tuple.\n-        if not isinstance(x, tuple or list):\n+        if not isinstance(x, (tuple, list)):\n             return x\n         else:\n             return (x,)"
        }
    ]
},
{
    "Id": 7,
    "commit_link": "https://github.com/keras-team/keras/commit/22a4ea757b247d9c090e6b30cec41ab5975a73f6",
    "date": "2024-02-19T19:14:11-08:00",
    "message": "Check raised regex in convolutional tests (#19202)\n\n* Strengthen the tests by checking regex\r\n\r\n* Check regex in conv_transpose tests\r\n\r\n* Check regex in depthwise conv tests\r\n\r\n* Check regex in separable conv tests\r\n\r\n* Reformatting\r\n\r\n* Fix linting\r\n\r\n* Fix formatting",
    "changes": [
        {
            "name": "base_depthwise_conv.py",
            "path": "keras/layers/convolutional/base_depthwise_conv.py",
            "patches": [
                {
                    "old_start": 132,
                    "old_length": 7,
                    "new_start": 132,
                    "new_length": 7,
                    "hunk": "@@ -132,7 +132,7 @@ class BaseDepthwiseConv(Layer):\n         if self.depth_multiplier is not None and self.depth_multiplier <= 0:\n             raise ValueError(\n                 \"Invalid value for argument `depth_multiplier`. Expected a \"\n-                \"strictly  positive value. Received \"\n+                \"strictly positive value. Received \"\n                 f\"depth_multiplier={self.depth_multiplier}.\"\n             )\n \n"
                }
            ],
            "whole_deleted": "-                \"strictly  positive value. Received \"\n",
            "whole_added": "+                \"strictly positive value. Received \"\n",
            "whole_hunk": "@@ -132,7 +132,7 @@ class BaseDepthwiseConv(Layer):\n         if self.depth_multiplier is not None and self.depth_multiplier <= 0:\n             raise ValueError(\n                 \"Invalid value for argument `depth_multiplier`. Expected a \"\n-                \"strictly  positive value. Received \"\n+                \"strictly positive value. Received \"\n                 f\"depth_multiplier={self.depth_multiplier}.\"\n             )\n \n"
        },
        {
            "name": "base_separable_conv.py",
            "path": "keras/layers/convolutional/base_separable_conv.py",
            "patches": [
                {
                    "old_start": 135,
                    "old_length": 7,
                    "new_start": 135,
                    "new_length": 7,
                    "hunk": "@@ -135,7 +135,7 @@ class BaseSeparableConv(Layer):\n         if self.depth_multiplier is not None and self.depth_multiplier <= 0:\n             raise ValueError(\n                 \"Invalid value for argument `depth_multiplier`. Expected a \"\n-                \"strictly  positive value. Received \"\n+                \"strictly positive value. Received \"\n                 f\"depth_multiplier={self.depth_multiplier}.\"\n             )\n \n"
                }
            ],
            "whole_deleted": "-                \"strictly  positive value. Received \"\n",
            "whole_added": "+                \"strictly positive value. Received \"\n",
            "whole_hunk": "@@ -135,7 +135,7 @@ class BaseSeparableConv(Layer):\n         if self.depth_multiplier is not None and self.depth_multiplier <= 0:\n             raise ValueError(\n                 \"Invalid value for argument `depth_multiplier`. Expected a \"\n-                \"strictly  positive value. Received \"\n+                \"strictly positive value. Received \"\n                 f\"depth_multiplier={self.depth_multiplier}.\"\n             )\n \n"
        },
        {
            "name": "conv_test.py",
            "path": "keras/layers/convolutional/conv_test.py",
            "patches": [
                {
                    "old_start": 486,
                    "old_length": 19,
                    "new_start": 486,
                    "new_length": 38,
                    "hunk": "@@ -486,19 +486,38 @@ class ConvBasicTest(testing.TestCase, parameterized.TestCase):\n \n     def test_bad_init_args(self):\n         # `filters` is not positive.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `filters`. Expected a \"\n+            \"strictly positive value. Received filters=0.\",\n+        ):\n             layers.Conv1D(filters=0, kernel_size=1)\n \n         # `kernel_size` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `kernel_size` argument must be a tuple of \\d+ \"\n+            r\"integers. Received kernel_size=\\(1, 0\\), including values \\{0\\} \"\n+            r\"that do not satisfy `value > 0`\",\n+        ):\n             layers.Conv2D(filters=2, kernel_size=(1, 0))\n \n         # `strides` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `strides` argument must be a tuple of \\d+ \"\n+            r\"integers. Received strides=\\(1, 0\\), including values \\{0\\} that \"\n+            r\"do not satisfy `value > 0`\",\n+        ):\n             layers.Conv2D(filters=2, kernel_size=(2, 2), strides=(1, 0))\n \n         # `dilation_rate > 1` while `strides > 1`.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`strides > 1` not supported in conjunction with \"\n+            r\"`dilation_rate > 1`. Received: strides=\\(2, 2\\) and \"\n+            r\"dilation_rate=\\(2, 1\\)\",\n+        ):\n             layers.Conv2D(\n                 filters=2, kernel_size=(2, 2), strides=2, dilation_rate=(2, 1)\n             )\n"
                },
                {
                    "old_start": 512,
                    "old_length": 7,
                    "new_start": 531,
                    "new_length": 11,
                    "hunk": "@@ -512,7 +531,11 @@ class ConvBasicTest(testing.TestCase, parameterized.TestCase):\n             layers.Conv2D(filters=5, kernel_size=(2, 2), groups=0)\n \n         # `filters` cannot be divided by `groups`.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"The number of filters must be evenly divisible by the\"\n+            \" number of groups. Received: groups=2, filters=5.\",\n+        ):\n             layers.Conv2D(filters=5, kernel_size=(2, 2), groups=2)\n \n \n"
                }
            ],
            "whole_deleted": "-        with self.assertRaises(ValueError):\n-        with self.assertRaises(ValueError):\n-        with self.assertRaises(ValueError):\n-        with self.assertRaises(ValueError):\n-        with self.assertRaises(ValueError):\n",
            "whole_added": "+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `filters`. Expected a \"\n+            \"strictly positive value. Received filters=0.\",\n+        ):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `kernel_size` argument must be a tuple of \\d+ \"\n+            r\"integers. Received kernel_size=\\(1, 0\\), including values \\{0\\} \"\n+            r\"that do not satisfy `value > 0`\",\n+        ):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `strides` argument must be a tuple of \\d+ \"\n+            r\"integers. Received strides=\\(1, 0\\), including values \\{0\\} that \"\n+            r\"do not satisfy `value > 0`\",\n+        ):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`strides > 1` not supported in conjunction with \"\n+            r\"`dilation_rate > 1`. Received: strides=\\(2, 2\\) and \"\n+            r\"dilation_rate=\\(2, 1\\)\",\n+        ):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"The number of filters must be evenly divisible by the\"\n+            \" number of groups. Received: groups=2, filters=5.\",\n+        ):\n",
            "whole_hunk": "@@ -486,19 +486,38 @@ class ConvBasicTest(testing.TestCase, parameterized.TestCase):\n \n     def test_bad_init_args(self):\n         # `filters` is not positive.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `filters`. Expected a \"\n+            \"strictly positive value. Received filters=0.\",\n+        ):\n             layers.Conv1D(filters=0, kernel_size=1)\n \n         # `kernel_size` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `kernel_size` argument must be a tuple of \\d+ \"\n+            r\"integers. Received kernel_size=\\(1, 0\\), including values \\{0\\} \"\n+            r\"that do not satisfy `value > 0`\",\n+        ):\n             layers.Conv2D(filters=2, kernel_size=(1, 0))\n \n         # `strides` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `strides` argument must be a tuple of \\d+ \"\n+            r\"integers. Received strides=\\(1, 0\\), including values \\{0\\} that \"\n+            r\"do not satisfy `value > 0`\",\n+        ):\n             layers.Conv2D(filters=2, kernel_size=(2, 2), strides=(1, 0))\n \n         # `dilation_rate > 1` while `strides > 1`.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`strides > 1` not supported in conjunction with \"\n+            r\"`dilation_rate > 1`. Received: strides=\\(2, 2\\) and \"\n+            r\"dilation_rate=\\(2, 1\\)\",\n+        ):\n             layers.Conv2D(\n                 filters=2, kernel_size=(2, 2), strides=2, dilation_rate=(2, 1)\n             )\n@@ -512,7 +531,11 @@ class ConvBasicTest(testing.TestCase, parameterized.TestCase):\n             layers.Conv2D(filters=5, kernel_size=(2, 2), groups=0)\n \n         # `filters` cannot be divided by `groups`.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"The number of filters must be evenly divisible by the\"\n+            \" number of groups. Received: groups=2, filters=5.\",\n+        ):\n             layers.Conv2D(filters=5, kernel_size=(2, 2), groups=2)\n \n \n"
        },
        {
            "name": "conv_transpose_test.py",
            "path": "keras/layers/convolutional/conv_transpose_test.py",
            "patches": [
                {
                    "old_start": 507,
                    "old_length": 21,
                    "new_start": 507,
                    "new_length": 40,
                    "hunk": "@@ -507,21 +507,40 @@ class ConvTransposeBasicTest(testing.TestCase, parameterized.TestCase):\n \n     def test_bad_init_args(self):\n         # `filters` is not positive.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `filters`. Expected a \"\n+            \"strictly positive value. Received filters=0.\",\n+        ):\n             layers.Conv1DTranspose(filters=0, kernel_size=1)\n \n         # `kernel_size` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `kernel_size` argument must be a tuple of \"\n+            r\"\\d+ integers. Received kernel_size=\\(1, 0\\), including values\"\n+            r\" \\{0\\} that do not satisfy `value > 0`\",\n+        ):\n             layers.Conv2DTranspose(filters=2, kernel_size=(1, 0))\n \n         # `strides` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `strides` argument must be a tuple of \\d+ \"\n+            r\"integers. Received strides=\\(1, 0\\), including values \\{0\\} \"\n+            r\"that do not satisfy `value > 0`\",\n+        ):\n             layers.Conv2DTranspose(\n                 filters=2, kernel_size=(2, 2), strides=(1, 0)\n             )\n \n         # `dilation_rate > 1` while `strides > 1`.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`strides > 1` not supported in conjunction with \"\n+            r\"`dilation_rate > 1`. Received: strides=\\(2, 2\\) and \"\n+            r\"dilation_rate=\\(2, 1\\)\",\n+        ):\n             layers.Conv2DTranspose(\n                 filters=2, kernel_size=(2, 2), strides=2, dilation_rate=(2, 1)\n             )\n"
                }
            ],
            "whole_deleted": "-        with self.assertRaises(ValueError):\n-        with self.assertRaises(ValueError):\n-        with self.assertRaises(ValueError):\n-        with self.assertRaises(ValueError):\n",
            "whole_added": "+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `filters`. Expected a \"\n+            \"strictly positive value. Received filters=0.\",\n+        ):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `kernel_size` argument must be a tuple of \"\n+            r\"\\d+ integers. Received kernel_size=\\(1, 0\\), including values\"\n+            r\" \\{0\\} that do not satisfy `value > 0`\",\n+        ):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `strides` argument must be a tuple of \\d+ \"\n+            r\"integers. Received strides=\\(1, 0\\), including values \\{0\\} \"\n+            r\"that do not satisfy `value > 0`\",\n+        ):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`strides > 1` not supported in conjunction with \"\n+            r\"`dilation_rate > 1`. Received: strides=\\(2, 2\\) and \"\n+            r\"dilation_rate=\\(2, 1\\)\",\n+        ):\n",
            "whole_hunk": "@@ -507,21 +507,40 @@ class ConvTransposeBasicTest(testing.TestCase, parameterized.TestCase):\n \n     def test_bad_init_args(self):\n         # `filters` is not positive.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `filters`. Expected a \"\n+            \"strictly positive value. Received filters=0.\",\n+        ):\n             layers.Conv1DTranspose(filters=0, kernel_size=1)\n \n         # `kernel_size` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `kernel_size` argument must be a tuple of \"\n+            r\"\\d+ integers. Received kernel_size=\\(1, 0\\), including values\"\n+            r\" \\{0\\} that do not satisfy `value > 0`\",\n+        ):\n             layers.Conv2DTranspose(filters=2, kernel_size=(1, 0))\n \n         # `strides` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `strides` argument must be a tuple of \\d+ \"\n+            r\"integers. Received strides=\\(1, 0\\), including values \\{0\\} \"\n+            r\"that do not satisfy `value > 0`\",\n+        ):\n             layers.Conv2DTranspose(\n                 filters=2, kernel_size=(2, 2), strides=(1, 0)\n             )\n \n         # `dilation_rate > 1` while `strides > 1`.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`strides > 1` not supported in conjunction with \"\n+            r\"`dilation_rate > 1`. Received: strides=\\(2, 2\\) and \"\n+            r\"dilation_rate=\\(2, 1\\)\",\n+        ):\n             layers.Conv2DTranspose(\n                 filters=2, kernel_size=(2, 2), strides=2, dilation_rate=(2, 1)\n             )\n"
        },
        {
            "name": "depthwise_conv_test.py",
            "path": "keras/layers/convolutional/depthwise_conv_test.py",
            "patches": [
                {
                    "old_start": 293,
                    "old_length": 21,
                    "new_start": 293,
                    "new_length": 41,
                    "hunk": "@@ -293,21 +293,41 @@ class DepthwiseConvBasicTest(testing.TestCase, parameterized.TestCase):\n \n     def test_bad_init_args(self):\n         # `depth_multiplier` is not positive.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `depth_multiplier`. \"\n+            \"Expected a strictly positive value. Received \"\n+            \"depth_multiplier=0.\",\n+        ):\n             layers.DepthwiseConv1D(depth_multiplier=0, kernel_size=1)\n \n         # `kernel_size` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `kernel_size` argument must be a tuple of 2 \"\n+            r\"integers. Received kernel_size=\\(1, 0\\), including values \"\n+            r\"\\{0\\} that do not satisfy `value > 0`\",\n+        ):\n             layers.DepthwiseConv2D(depth_multiplier=2, kernel_size=(1, 0))\n \n         # `strides` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `strides` argument must be a tuple of \\d+ \"\n+            r\"integers. Received strides=\\(1, 0\\), including values \\{0\\} \"\n+            r\"that do not satisfy `value > 0`\",\n+        ):\n             layers.DepthwiseConv2D(\n                 depth_multiplier=2, kernel_size=(2, 2), strides=(1, 0)\n             )\n \n         # `dilation_rate > 1` while `strides > 1`.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`strides > 1` not supported in conjunction with \"\n+            r\"`dilation_rate > 1`. Received: strides=\\(2, 2\\) and \"\n+            r\"dilation_rate=\\(2, 1\\)\",\n+        ):\n             layers.DepthwiseConv2D(\n                 depth_multiplier=2,\n                 kernel_size=(2, 2),\n"
                }
            ],
            "whole_deleted": "-        with self.assertRaises(ValueError):\n-        with self.assertRaises(ValueError):\n-        with self.assertRaises(ValueError):\n-        with self.assertRaises(ValueError):\n",
            "whole_added": "+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `depth_multiplier`. \"\n+            \"Expected a strictly positive value. Received \"\n+            \"depth_multiplier=0.\",\n+        ):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `kernel_size` argument must be a tuple of 2 \"\n+            r\"integers. Received kernel_size=\\(1, 0\\), including values \"\n+            r\"\\{0\\} that do not satisfy `value > 0`\",\n+        ):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `strides` argument must be a tuple of \\d+ \"\n+            r\"integers. Received strides=\\(1, 0\\), including values \\{0\\} \"\n+            r\"that do not satisfy `value > 0`\",\n+        ):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`strides > 1` not supported in conjunction with \"\n+            r\"`dilation_rate > 1`. Received: strides=\\(2, 2\\) and \"\n+            r\"dilation_rate=\\(2, 1\\)\",\n+        ):\n",
            "whole_hunk": "@@ -293,21 +293,41 @@ class DepthwiseConvBasicTest(testing.TestCase, parameterized.TestCase):\n \n     def test_bad_init_args(self):\n         # `depth_multiplier` is not positive.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `depth_multiplier`. \"\n+            \"Expected a strictly positive value. Received \"\n+            \"depth_multiplier=0.\",\n+        ):\n             layers.DepthwiseConv1D(depth_multiplier=0, kernel_size=1)\n \n         # `kernel_size` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `kernel_size` argument must be a tuple of 2 \"\n+            r\"integers. Received kernel_size=\\(1, 0\\), including values \"\n+            r\"\\{0\\} that do not satisfy `value > 0`\",\n+        ):\n             layers.DepthwiseConv2D(depth_multiplier=2, kernel_size=(1, 0))\n \n         # `strides` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `strides` argument must be a tuple of \\d+ \"\n+            r\"integers. Received strides=\\(1, 0\\), including values \\{0\\} \"\n+            r\"that do not satisfy `value > 0`\",\n+        ):\n             layers.DepthwiseConv2D(\n                 depth_multiplier=2, kernel_size=(2, 2), strides=(1, 0)\n             )\n \n         # `dilation_rate > 1` while `strides > 1`.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`strides > 1` not supported in conjunction with \"\n+            r\"`dilation_rate > 1`. Received: strides=\\(2, 2\\) and \"\n+            r\"dilation_rate=\\(2, 1\\)\",\n+        ):\n             layers.DepthwiseConv2D(\n                 depth_multiplier=2,\n                 kernel_size=(2, 2),\n"
        },
        {
            "name": "separable_conv_test.py",
            "path": "keras/layers/convolutional/separable_conv_test.py",
            "patches": [
                {
                    "old_start": 147,
                    "old_length": 21,
                    "new_start": 147,
                    "new_length": 40,
                    "hunk": "@@ -147,21 +147,40 @@ class SeparableConvBasicTest(testing.TestCase, parameterized.TestCase):\n \n     def test_bad_init_args(self):\n         # `depth_multiplier` is not positive.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `depth_multiplier`. \"\n+            \"Expected a strictly positive value. Received \"\n+            \"depth_multiplier=0.\",\n+        ):\n             layers.SeparableConv1D(depth_multiplier=0, filters=1, kernel_size=1)\n \n         # `filters` is not positive.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `filters`. Expected a \"\n+            \"strictly positive value. Received filters=0.\",\n+        ):\n             layers.SeparableConv1D(depth_multiplier=1, filters=0, kernel_size=1)\n \n         # `kernel_size` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `kernel_size` argument must be a tuple of \"\n+            r\"\\d+ integers. Received kernel_size=\\(1, 0\\), including values\"\n+            r\" \\{0\\} that do not satisfy `value > 0`\",\n+        ):\n             layers.SeparableConv2D(\n                 depth_multiplier=2, filters=2, kernel_size=(1, 0)\n             )\n \n         # `strides` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `strides` argument must be a tuple of \\d+ \"\n+            r\"integers. Received strides=\\(1, 0\\), including values \\{0\\} \"\n+            r\"that do not satisfy `value > 0`\",\n+        ):\n             layers.SeparableConv2D(\n                 depth_multiplier=2,\n                 filters=2,\n"
                },
                {
                    "old_start": 170,
                    "old_length": 7,
                    "new_start": 189,
                    "new_length": 12,
                    "hunk": "@@ -170,7 +189,12 @@ class SeparableConvBasicTest(testing.TestCase, parameterized.TestCase):\n             )\n \n         # `dilation_rate > 1` while `strides > 1`.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`strides > 1` not supported in conjunction with \"\n+            r\"`dilation_rate > 1`. Received: strides=\\(2, 2\\) and \"\n+            r\"dilation_rate=\\(2, 1\\)\",\n+        ):\n             layers.SeparableConv2D(\n                 depth_multiplier=2,\n                 filters=2,"
                }
            ],
            "whole_deleted": "-        with self.assertRaises(ValueError):\n-        with self.assertRaises(ValueError):\n-        with self.assertRaises(ValueError):\n-        with self.assertRaises(ValueError):\n-        with self.assertRaises(ValueError):\n",
            "whole_added": "+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `depth_multiplier`. \"\n+            \"Expected a strictly positive value. Received \"\n+            \"depth_multiplier=0.\",\n+        ):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `filters`. Expected a \"\n+            \"strictly positive value. Received filters=0.\",\n+        ):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `kernel_size` argument must be a tuple of \"\n+            r\"\\d+ integers. Received kernel_size=\\(1, 0\\), including values\"\n+            r\" \\{0\\} that do not satisfy `value > 0`\",\n+        ):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `strides` argument must be a tuple of \\d+ \"\n+            r\"integers. Received strides=\\(1, 0\\), including values \\{0\\} \"\n+            r\"that do not satisfy `value > 0`\",\n+        ):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`strides > 1` not supported in conjunction with \"\n+            r\"`dilation_rate > 1`. Received: strides=\\(2, 2\\) and \"\n+            r\"dilation_rate=\\(2, 1\\)\",\n+        ):\n",
            "whole_hunk": "@@ -147,21 +147,40 @@ class SeparableConvBasicTest(testing.TestCase, parameterized.TestCase):\n \n     def test_bad_init_args(self):\n         # `depth_multiplier` is not positive.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `depth_multiplier`. \"\n+            \"Expected a strictly positive value. Received \"\n+            \"depth_multiplier=0.\",\n+        ):\n             layers.SeparableConv1D(depth_multiplier=0, filters=1, kernel_size=1)\n \n         # `filters` is not positive.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `filters`. Expected a \"\n+            \"strictly positive value. Received filters=0.\",\n+        ):\n             layers.SeparableConv1D(depth_multiplier=1, filters=0, kernel_size=1)\n \n         # `kernel_size` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `kernel_size` argument must be a tuple of \"\n+            r\"\\d+ integers. Received kernel_size=\\(1, 0\\), including values\"\n+            r\" \\{0\\} that do not satisfy `value > 0`\",\n+        ):\n             layers.SeparableConv2D(\n                 depth_multiplier=2, filters=2, kernel_size=(1, 0)\n             )\n \n         # `strides` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `strides` argument must be a tuple of \\d+ \"\n+            r\"integers. Received strides=\\(1, 0\\), including values \\{0\\} \"\n+            r\"that do not satisfy `value > 0`\",\n+        ):\n             layers.SeparableConv2D(\n                 depth_multiplier=2,\n                 filters=2,\n@@ -170,7 +189,12 @@ class SeparableConvBasicTest(testing.TestCase, parameterized.TestCase):\n             )\n \n         # `dilation_rate > 1` while `strides > 1`.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`strides > 1` not supported in conjunction with \"\n+            r\"`dilation_rate > 1`. Received: strides=\\(2, 2\\) and \"\n+            r\"dilation_rate=\\(2, 1\\)\",\n+        ):\n             layers.SeparableConv2D(\n                 depth_multiplier=2,\n                 filters=2,"
        }
    ]
},
{
    "Id": 8,
    "commit_link": "https://github.com/keras-team/keras/commit/92dcd8fd137455976d8a3880339030a894626aea",
    "date": "2024-02-13T16:15:36-08:00",
    "message": "Add `normalize` ops (#19154)\n\n* add: `normalize` ops\r\n\r\nLp normalization\r\n\r\n* fix: typos\r\n\r\n* update: move `normalize` to `ops.nn`\r\n\r\n* update: `p` -> `order`\r\n\r\n* update: remove `eps` arg  and change arg order\r\n\r\n* fix: order of args\r\n\r\n* update: docstring for eq\r\n\r\n* add: dynamic shape test\r\n\r\n* add: static shape test\r\n\r\n* add: correctness check",
    "changes": [
        {
            "name": "nn.py",
            "path": "keras/ops/nn.py",
            "patches": [
                {
                    "old_start": 1840,
                    "old_length": 3,
                    "new_start": 1840,
                    "new_length": 57,
                    "hunk": "@@ -1840,3 +1840,57 @@ def ctc_loss(target, output, target_length, output_length, mask_index=0):\n     return backend.nn.ctc_loss(\n         target, output, target_length, output_length, mask_index\n     )\n+\n+\n+class Normalize(Operation):\n+    def __init__(self, axis=-1, order=2):\n+        super().__init__()\n+        self.axis = axis\n+        self.order = order\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(shape=x.shape)\n+\n+    def call(self, x):\n+        return _normalize(x, self.order, self.axis)\n+\n+\n+@keras_export(\n+    [\n+        \"keras.ops.normalize\",\n+        \"keras.ops.nn.normalize\",\n+    ]\n+)\n+def normalize(x, axis=-1, order=2):\n+    \"\"\"Perform Lp normalization of a tensor over the specified axis.\n+\n+    It is defined as: `normalize(x) = x / max(norm(x), epsilon)`.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: The axis or axes along which to perform normalization. Default: 1.\n+        order: The exponent value in the norm formulation. Default: 2.\n+\n+    Returns:\n+        The normalized array.\n+\n+    Example:\n+\n+    >>> x = keras.ops.convert_to_tensor([[1, 2, 3], [4, 5, 6]])\n+    >>> x_norm = keras.ops.math.normalize(x)\n+    >>> print(x_norm)\n+    array([[0.26726124 0.5345225  0.8017837 ]\n+           [0.45584232 0.5698029  0.68376344]], shape=(2, 3), dtype=float32)\n+\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Normalize(axis, order).symbolic_call(x)\n+    return _normalize(x, axis, order)\n+\n+\n+def _normalize(x, axis=-1, order=2):\n+    x = backend.convert_to_tensor(x)\n+    epsilon = backend.config.epsilon()\n+    norm = backend.linalg.norm(x, ord=order, axis=axis, keepdims=True)\n+    denom = backend.numpy.maximum(norm, epsilon)\n+    return backend.numpy.divide(x, denom)\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+\n+\n+class Normalize(Operation):\n+    def __init__(self, axis=-1, order=2):\n+        super().__init__()\n+        self.axis = axis\n+        self.order = order\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(shape=x.shape)\n+\n+    def call(self, x):\n+        return _normalize(x, self.order, self.axis)\n+\n+\n+@keras_export(\n+    [\n+        \"keras.ops.normalize\",\n+        \"keras.ops.nn.normalize\",\n+    ]\n+)\n+def normalize(x, axis=-1, order=2):\n+    \"\"\"Perform Lp normalization of a tensor over the specified axis.\n+\n+    It is defined as: `normalize(x) = x / max(norm(x), epsilon)`.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: The axis or axes along which to perform normalization. Default: 1.\n+        order: The exponent value in the norm formulation. Default: 2.\n+\n+    Returns:\n+        The normalized array.\n+\n+    Example:\n+\n+    >>> x = keras.ops.convert_to_tensor([[1, 2, 3], [4, 5, 6]])\n+    >>> x_norm = keras.ops.math.normalize(x)\n+    >>> print(x_norm)\n+    array([[0.26726124 0.5345225  0.8017837 ]\n+           [0.45584232 0.5698029  0.68376344]], shape=(2, 3), dtype=float32)\n+\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Normalize(axis, order).symbolic_call(x)\n+    return _normalize(x, axis, order)\n+\n+\n+def _normalize(x, axis=-1, order=2):\n+    x = backend.convert_to_tensor(x)\n+    epsilon = backend.config.epsilon()\n+    norm = backend.linalg.norm(x, ord=order, axis=axis, keepdims=True)\n+    denom = backend.numpy.maximum(norm, epsilon)\n+    return backend.numpy.divide(x, denom)\n",
            "whole_hunk": "@@ -1840,3 +1840,57 @@ def ctc_loss(target, output, target_length, output_length, mask_index=0):\n     return backend.nn.ctc_loss(\n         target, output, target_length, output_length, mask_index\n     )\n+\n+\n+class Normalize(Operation):\n+    def __init__(self, axis=-1, order=2):\n+        super().__init__()\n+        self.axis = axis\n+        self.order = order\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(shape=x.shape)\n+\n+    def call(self, x):\n+        return _normalize(x, self.order, self.axis)\n+\n+\n+@keras_export(\n+    [\n+        \"keras.ops.normalize\",\n+        \"keras.ops.nn.normalize\",\n+    ]\n+)\n+def normalize(x, axis=-1, order=2):\n+    \"\"\"Perform Lp normalization of a tensor over the specified axis.\n+\n+    It is defined as: `normalize(x) = x / max(norm(x), epsilon)`.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: The axis or axes along which to perform normalization. Default: 1.\n+        order: The exponent value in the norm formulation. Default: 2.\n+\n+    Returns:\n+        The normalized array.\n+\n+    Example:\n+\n+    >>> x = keras.ops.convert_to_tensor([[1, 2, 3], [4, 5, 6]])\n+    >>> x_norm = keras.ops.math.normalize(x)\n+    >>> print(x_norm)\n+    array([[0.26726124 0.5345225  0.8017837 ]\n+           [0.45584232 0.5698029  0.68376344]], shape=(2, 3), dtype=float32)\n+\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Normalize(axis, order).symbolic_call(x)\n+    return _normalize(x, axis, order)\n+\n+\n+def _normalize(x, axis=-1, order=2):\n+    x = backend.convert_to_tensor(x)\n+    epsilon = backend.config.epsilon()\n+    norm = backend.linalg.norm(x, ord=order, axis=axis, keepdims=True)\n+    denom = backend.numpy.maximum(norm, epsilon)\n+    return backend.numpy.divide(x, denom)\n"
        },
        {
            "name": "nn_test.py",
            "path": "keras/ops/nn_test.py",
            "patches": [
                {
                    "old_start": 607,
                    "old_length": 6,
                    "new_start": 607,
                    "new_length": 10,
                    "hunk": "@@ -607,6 +607,10 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n             scale=KerasTensor([3]),\n         )\n \n+    def test_normalize(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.normalize(x).shape, (None, 2, 3))\n+\n \n class NNOpsStaticShapeTest(testing.TestCase):\n     def test_relu(self):\n"
                },
                {
                    "old_start": 1049,
                    "old_length": 6,
                    "new_start": 1053,
                    "new_length": 10,
                    "hunk": "@@ -1049,6 +1053,10 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         y_lengths = KerasTensor([10], dtype=\"int32\")\n         self.assertEqual(knn.ctc_loss(x, y, x_lengths, y_lengths).shape, (10,))\n \n+    def test_normalize(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.normalize(x).shape, (1, 2, 3))\n+\n \n class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n     def test_relu(self):\n"
                },
                {
                    "old_start": 1844,
                    "old_length": 6,
                    "new_start": 1852,
                    "new_length": 37,
                    "hunk": "@@ -1844,6 +1852,37 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         result = knn.ctc_loss(labels, outputs, label_length, output_length)\n         self.assertAllClose(result, np.array([3.4411672, 1.91680186]))\n \n+    def test_normalize(self):\n+        x = np.array([[1, 2, 3], [1, 2, 3]], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.normalize(x, axis=None),\n+            [\n+                [0.18898225, 0.3779645, 0.56694674],\n+                [0.18898225, 0.3779645, 0.56694674],\n+            ],\n+        )\n+        self.assertAllClose(\n+            knn.normalize(x, axis=0),\n+            [\n+                [0.70710677, 0.70710677, 0.70710677],\n+                [0.70710677, 0.70710677, 0.70710677],\n+            ],\n+        )\n+        self.assertAllClose(\n+            knn.normalize(x, axis=-1),\n+            [\n+                [0.26726124, 0.53452247, 0.8017837],\n+                [0.26726124, 0.53452247, 0.8017837],\n+            ],\n+        )\n+        self.assertAllClose(\n+            knn.normalize(x, order=3),\n+            [\n+                [0.30285344, 0.6057069, 0.9085603],\n+                [0.30285344, 0.6057069, 0.9085603],\n+            ],\n+        )\n+\n \n class TestLogitRecovery(testing.TestCase):\n     def test_logit_recovery_binary_crossentropy(self):"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    def test_normalize(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.normalize(x).shape, (None, 2, 3))\n+\n+    def test_normalize(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.normalize(x).shape, (1, 2, 3))\n+\n+    def test_normalize(self):\n+        x = np.array([[1, 2, 3], [1, 2, 3]], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.normalize(x, axis=None),\n+            [\n+                [0.18898225, 0.3779645, 0.56694674],\n+                [0.18898225, 0.3779645, 0.56694674],\n+            ],\n+        )\n+        self.assertAllClose(\n+            knn.normalize(x, axis=0),\n+            [\n+                [0.70710677, 0.70710677, 0.70710677],\n+                [0.70710677, 0.70710677, 0.70710677],\n+            ],\n+        )\n+        self.assertAllClose(\n+            knn.normalize(x, axis=-1),\n+            [\n+                [0.26726124, 0.53452247, 0.8017837],\n+                [0.26726124, 0.53452247, 0.8017837],\n+            ],\n+        )\n+        self.assertAllClose(\n+            knn.normalize(x, order=3),\n+            [\n+                [0.30285344, 0.6057069, 0.9085603],\n+                [0.30285344, 0.6057069, 0.9085603],\n+            ],\n+        )\n+\n",
            "whole_hunk": "@@ -607,6 +607,10 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n             scale=KerasTensor([3]),\n         )\n \n+    def test_normalize(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.normalize(x).shape, (None, 2, 3))\n+\n \n class NNOpsStaticShapeTest(testing.TestCase):\n     def test_relu(self):\n@@ -1049,6 +1053,10 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         y_lengths = KerasTensor([10], dtype=\"int32\")\n         self.assertEqual(knn.ctc_loss(x, y, x_lengths, y_lengths).shape, (10,))\n \n+    def test_normalize(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.normalize(x).shape, (1, 2, 3))\n+\n \n class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n     def test_relu(self):\n@@ -1844,6 +1852,37 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         result = knn.ctc_loss(labels, outputs, label_length, output_length)\n         self.assertAllClose(result, np.array([3.4411672, 1.91680186]))\n \n+    def test_normalize(self):\n+        x = np.array([[1, 2, 3], [1, 2, 3]], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.normalize(x, axis=None),\n+            [\n+                [0.18898225, 0.3779645, 0.56694674],\n+                [0.18898225, 0.3779645, 0.56694674],\n+            ],\n+        )\n+        self.assertAllClose(\n+            knn.normalize(x, axis=0),\n+            [\n+                [0.70710677, 0.70710677, 0.70710677],\n+                [0.70710677, 0.70710677, 0.70710677],\n+            ],\n+        )\n+        self.assertAllClose(\n+            knn.normalize(x, axis=-1),\n+            [\n+                [0.26726124, 0.53452247, 0.8017837],\n+                [0.26726124, 0.53452247, 0.8017837],\n+            ],\n+        )\n+        self.assertAllClose(\n+            knn.normalize(x, order=3),\n+            [\n+                [0.30285344, 0.6057069, 0.9085603],\n+                [0.30285344, 0.6057069, 0.9085603],\n+            ],\n+        )\n+\n \n class TestLogitRecovery(testing.TestCase):\n     def test_logit_recovery_binary_crossentropy(self):"
        }
    ]
},
{
    "Id": 9,
    "commit_link": "https://github.com/keras-team/keras/commit/f33c02c6e47e9291d127792b13412b9ac94ae7f6",
    "date": "2024-02-09T20:42:52-08:00",
    "message": "Fixes JAX-native serialization bug (#19166)\n\n* Wrap JAX export variables in TF variable, add test for JAX with TF function endpoint\r\n\r\n* Remove unnecessary instance check\r\n\r\n* Fix JAX native serialization in test",
    "changes": [
        {
            "name": "export_lib_test.py",
            "path": "keras/export/export_lib_test.py",
            "patches": [
                {
                    "old_start": 127,
                    "old_length": 12,
                    "new_start": 127,
                    "new_length": 17,
                    "hunk": "@@ -127,12 +127,17 @@ class ExportArchiveTest(testing.TestCase):\n         def model_call(x):\n             return model(x)\n \n+        from jax import default_backend as jax_device\n         from jax.experimental import jax2tf\n \n+        native_jax_compatible = not (\n+            jax_device() == \"gpu\"\n+            and len(tf.config.list_physical_devices(\"GPU\")) == 0\n+        )\n         # now, convert JAX function\n         converted_model_call = jax2tf.convert(\n             model_call,\n-            native_serialization=True,\n+            native_serialization=native_jax_compatible,\n             polymorphic_shapes=[\"(b, 10)\"],\n         )\n "
                }
            ],
            "whole_deleted": "-            native_serialization=True,\n",
            "whole_added": "+        from jax import default_backend as jax_device\n+        native_jax_compatible = not (\n+            jax_device() == \"gpu\"\n+            and len(tf.config.list_physical_devices(\"GPU\")) == 0\n+        )\n+            native_serialization=native_jax_compatible,\n",
            "whole_hunk": "@@ -127,12 +127,17 @@ class ExportArchiveTest(testing.TestCase):\n         def model_call(x):\n             return model(x)\n \n+        from jax import default_backend as jax_device\n         from jax.experimental import jax2tf\n \n+        native_jax_compatible = not (\n+            jax_device() == \"gpu\"\n+            and len(tf.config.list_physical_devices(\"GPU\")) == 0\n+        )\n         # now, convert JAX function\n         converted_model_call = jax2tf.convert(\n             model_call,\n-            native_serialization=True,\n+            native_serialization=native_jax_compatible,\n             polymorphic_shapes=[\"(b, 10)\"],\n         )\n "
        }
    ]
},
{
    "Id": 10,
    "commit_link": "https://github.com/keras-team/keras/commit/b246242a25d29af3561a1be8796596d67638c0ab",
    "date": "2024-02-09T14:38:51-08:00",
    "message": "Wrap JAX export variables in TF variable, add test for JAX with TF function endpoint (#19165)\n\n* Wrap JAX export variables in TF variable, add test for JAX with TF function endpoint\r\n\r\n* Remove unnecessary instance check",
    "changes": [
        {
            "name": "export_lib.py",
            "path": "keras/export/export_lib.py",
            "patches": [
                {
                    "old_start": 142,
                    "old_length": 13,
                    "new_start": 142,
                    "new_length": 28,
                    "hunk": "@@ -142,13 +142,28 @@ class ExportArchive:\n         if isinstance(resource, Layer):\n             # Variables in the lists below are actually part of the trackables\n             # that get saved, because the lists are created in __init__.\n-            self._tf_trackable.variables += resource.variables\n-            self._tf_trackable.trainable_variables += (\n-                resource.trainable_variables\n-            )\n-            self._tf_trackable.non_trainable_variables += (\n-                resource.non_trainable_variables\n-            )\n+            if backend.backend() == \"jax\":\n+                self._tf_trackable.variables += tf.nest.flatten(\n+                    tf.nest.map_structure(tf.Variable, resource.variables)\n+                )\n+                self._tf_trackable.trainable_variables += tf.nest.flatten(\n+                    tf.nest.map_structure(\n+                        tf.Variable, resource.trainable_variables\n+                    )\n+                )\n+                self._tf_trackable.non_trainable_variables += tf.nest.flatten(\n+                    tf.nest.map_structure(\n+                        tf.Variable, resource.non_trainable_variables\n+                    )\n+                )\n+            else:\n+                self._tf_trackable.variables += resource.variables\n+                self._tf_trackable.trainable_variables += (\n+                    resource.trainable_variables\n+                )\n+                self._tf_trackable.non_trainable_variables += (\n+                    resource.non_trainable_variables\n+                )\n \n     def add_endpoint(self, name, fn, input_signature=None):\n         \"\"\"Register a new serving endpoint.\n"
                }
            ],
            "whole_deleted": "-            self._tf_trackable.variables += resource.variables\n-            self._tf_trackable.trainable_variables += (\n-                resource.trainable_variables\n-            )\n-            self._tf_trackable.non_trainable_variables += (\n-                resource.non_trainable_variables\n-            )\n",
            "whole_added": "+            if backend.backend() == \"jax\":\n+                self._tf_trackable.variables += tf.nest.flatten(\n+                    tf.nest.map_structure(tf.Variable, resource.variables)\n+                )\n+                self._tf_trackable.trainable_variables += tf.nest.flatten(\n+                    tf.nest.map_structure(\n+                        tf.Variable, resource.trainable_variables\n+                    )\n+                )\n+                self._tf_trackable.non_trainable_variables += tf.nest.flatten(\n+                    tf.nest.map_structure(\n+                        tf.Variable, resource.non_trainable_variables\n+                    )\n+                )\n+            else:\n+                self._tf_trackable.variables += resource.variables\n+                self._tf_trackable.trainable_variables += (\n+                    resource.trainable_variables\n+                )\n+                self._tf_trackable.non_trainable_variables += (\n+                    resource.non_trainable_variables\n+                )\n",
            "whole_hunk": "@@ -142,13 +142,28 @@ class ExportArchive:\n         if isinstance(resource, Layer):\n             # Variables in the lists below are actually part of the trackables\n             # that get saved, because the lists are created in __init__.\n-            self._tf_trackable.variables += resource.variables\n-            self._tf_trackable.trainable_variables += (\n-                resource.trainable_variables\n-            )\n-            self._tf_trackable.non_trainable_variables += (\n-                resource.non_trainable_variables\n-            )\n+            if backend.backend() == \"jax\":\n+                self._tf_trackable.variables += tf.nest.flatten(\n+                    tf.nest.map_structure(tf.Variable, resource.variables)\n+                )\n+                self._tf_trackable.trainable_variables += tf.nest.flatten(\n+                    tf.nest.map_structure(\n+                        tf.Variable, resource.trainable_variables\n+                    )\n+                )\n+                self._tf_trackable.non_trainable_variables += tf.nest.flatten(\n+                    tf.nest.map_structure(\n+                        tf.Variable, resource.non_trainable_variables\n+                    )\n+                )\n+            else:\n+                self._tf_trackable.variables += resource.variables\n+                self._tf_trackable.trainable_variables += (\n+                    resource.trainable_variables\n+                )\n+                self._tf_trackable.non_trainable_variables += (\n+                    resource.non_trainable_variables\n+                )\n \n     def add_endpoint(self, name, fn, input_signature=None):\n         \"\"\"Register a new serving endpoint.\n"
        },
        {
            "name": "export_lib_test.py",
            "path": "keras/export/export_lib_test.py",
            "patches": [
                {
                    "old_start": 114,
                    "old_length": 6,
                    "new_start": 114,
                    "new_length": 62,
                    "hunk": "@@ -114,6 +114,62 @@ class ExportArchiveTest(testing.TestCase):\n         self.assertLen(revived_model.trainable_variables, 6)\n         self.assertLen(revived_model.non_trainable_variables, 2)\n \n+    @pytest.mark.skipif(\n+        backend.backend() != \"jax\",\n+        reason=\"This test is native to the JAX backend.\",\n+    )\n+    def test_jax_endpoint_registration_tf_function(self):\n+        model = get_model()\n+        ref_input = np.random.normal(size=(3, 10))\n+        model(ref_input)\n+\n+        # build a JAX function\n+        def model_call(x):\n+            return model(x)\n+\n+        from jax.experimental import jax2tf\n+\n+        # now, convert JAX function\n+        converted_model_call = jax2tf.convert(\n+            model_call,\n+            native_serialization=True,\n+            polymorphic_shapes=[\"(b, 10)\"],\n+        )\n+\n+        # you can now build a TF inference function\n+        @tf.function(\n+            input_signature=[tf.TensorSpec(shape=(None, 10), dtype=tf.float32)],\n+            autograph=False,\n+        )\n+        def infer_fn(x):\n+            return converted_model_call(x)\n+\n+        ref_output = infer_fn(ref_input)\n+\n+        # Export with TF inference function as endpoint\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"my_model\")\n+        export_archive = export_lib.ExportArchive()\n+        export_archive.track(model)\n+        export_archive.add_endpoint(\"serve\", infer_fn)\n+        export_archive.write_out(temp_filepath)\n+\n+        # Reload and verify outputs\n+        revived_model = tf.saved_model.load(temp_filepath)\n+        self.assertFalse(hasattr(revived_model, \"_tracked\"))\n+        self.assertAllClose(\n+            ref_output, revived_model.serve(ref_input), atol=1e-6\n+        )\n+        self.assertLen(revived_model.variables, 8)\n+        self.assertLen(revived_model.trainable_variables, 6)\n+        self.assertLen(revived_model.non_trainable_variables, 2)\n+\n+        # Assert all variables wrapped as `tf.Variable`\n+        assert isinstance(export_archive.variables[0], tf.Variable)\n+        assert isinstance(export_archive.trainable_variables[0], tf.Variable)\n+        assert isinstance(\n+            export_archive.non_trainable_variables[0], tf.Variable\n+        )\n+\n     def test_layer_export(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_layer\")\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    @pytest.mark.skipif(\n+        backend.backend() != \"jax\",\n+        reason=\"This test is native to the JAX backend.\",\n+    )\n+    def test_jax_endpoint_registration_tf_function(self):\n+        model = get_model()\n+        ref_input = np.random.normal(size=(3, 10))\n+        model(ref_input)\n+\n+        # build a JAX function\n+        def model_call(x):\n+            return model(x)\n+\n+        from jax.experimental import jax2tf\n+\n+        # now, convert JAX function\n+        converted_model_call = jax2tf.convert(\n+            model_call,\n+            native_serialization=True,\n+            polymorphic_shapes=[\"(b, 10)\"],\n+        )\n+\n+        # you can now build a TF inference function\n+        @tf.function(\n+            input_signature=[tf.TensorSpec(shape=(None, 10), dtype=tf.float32)],\n+            autograph=False,\n+        )\n+        def infer_fn(x):\n+            return converted_model_call(x)\n+\n+        ref_output = infer_fn(ref_input)\n+\n+        # Export with TF inference function as endpoint\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"my_model\")\n+        export_archive = export_lib.ExportArchive()\n+        export_archive.track(model)\n+        export_archive.add_endpoint(\"serve\", infer_fn)\n+        export_archive.write_out(temp_filepath)\n+\n+        # Reload and verify outputs\n+        revived_model = tf.saved_model.load(temp_filepath)\n+        self.assertFalse(hasattr(revived_model, \"_tracked\"))\n+        self.assertAllClose(\n+            ref_output, revived_model.serve(ref_input), atol=1e-6\n+        )\n+        self.assertLen(revived_model.variables, 8)\n+        self.assertLen(revived_model.trainable_variables, 6)\n+        self.assertLen(revived_model.non_trainable_variables, 2)\n+\n+        # Assert all variables wrapped as `tf.Variable`\n+        assert isinstance(export_archive.variables[0], tf.Variable)\n+        assert isinstance(export_archive.trainable_variables[0], tf.Variable)\n+        assert isinstance(\n+            export_archive.non_trainable_variables[0], tf.Variable\n+        )\n+\n",
            "whole_hunk": "@@ -114,6 +114,62 @@ class ExportArchiveTest(testing.TestCase):\n         self.assertLen(revived_model.trainable_variables, 6)\n         self.assertLen(revived_model.non_trainable_variables, 2)\n \n+    @pytest.mark.skipif(\n+        backend.backend() != \"jax\",\n+        reason=\"This test is native to the JAX backend.\",\n+    )\n+    def test_jax_endpoint_registration_tf_function(self):\n+        model = get_model()\n+        ref_input = np.random.normal(size=(3, 10))\n+        model(ref_input)\n+\n+        # build a JAX function\n+        def model_call(x):\n+            return model(x)\n+\n+        from jax.experimental import jax2tf\n+\n+        # now, convert JAX function\n+        converted_model_call = jax2tf.convert(\n+            model_call,\n+            native_serialization=True,\n+            polymorphic_shapes=[\"(b, 10)\"],\n+        )\n+\n+        # you can now build a TF inference function\n+        @tf.function(\n+            input_signature=[tf.TensorSpec(shape=(None, 10), dtype=tf.float32)],\n+            autograph=False,\n+        )\n+        def infer_fn(x):\n+            return converted_model_call(x)\n+\n+        ref_output = infer_fn(ref_input)\n+\n+        # Export with TF inference function as endpoint\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"my_model\")\n+        export_archive = export_lib.ExportArchive()\n+        export_archive.track(model)\n+        export_archive.add_endpoint(\"serve\", infer_fn)\n+        export_archive.write_out(temp_filepath)\n+\n+        # Reload and verify outputs\n+        revived_model = tf.saved_model.load(temp_filepath)\n+        self.assertFalse(hasattr(revived_model, \"_tracked\"))\n+        self.assertAllClose(\n+            ref_output, revived_model.serve(ref_input), atol=1e-6\n+        )\n+        self.assertLen(revived_model.variables, 8)\n+        self.assertLen(revived_model.trainable_variables, 6)\n+        self.assertLen(revived_model.non_trainable_variables, 2)\n+\n+        # Assert all variables wrapped as `tf.Variable`\n+        assert isinstance(export_archive.variables[0], tf.Variable)\n+        assert isinstance(export_archive.trainable_variables[0], tf.Variable)\n+        assert isinstance(\n+            export_archive.non_trainable_variables[0], tf.Variable\n+        )\n+\n     def test_layer_export(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_layer\")\n "
        }
    ]
},
{
    "Id": 11,
    "commit_link": "https://github.com/keras-team/keras/commit/488f33301fa579328dfbdcaa14284bd1a6a07d6c",
    "date": "2024-01-06T18:54:51-08:00",
    "message": "Fix TensorShape bug in tensorflow backend (#18997)\n\n* Fix TensorShape bug in tensorflow backend\r\n\r\nThis prevents downstream issues whenever TensorShape contains Dimension objects (which sometimes happen for legacy reasons).\r\n\r\n* Add check for TensorShape type",
    "changes": [
        {
            "name": "variables.py",
            "path": "keras/backend/common/variables.py",
            "patches": [
                {
                    "old_start": 6,
                    "old_length": 6,
                    "new_start": 6,
                    "new_length": 7,
                    "hunk": "@@ -6,6 +6,7 @@ from keras.backend.common import global_state\n from keras.backend.common.name_scope import current_path\n from keras.backend.common.stateless_scope import get_stateless_scope\n from keras.backend.common.stateless_scope import in_stateless_scope\n+from keras.utils.module_utils import tensorflow as tf\n from keras.utils.naming import auto_name\n \n \n"
                },
                {
                    "old_start": 403,
                    "old_length": 6,
                    "new_start": 404,
                    "new_length": 11,
                    "hunk": "@@ -403,6 +404,11 @@ def standardize_shape(shape):\n             raise ValueError(\"Undefined shapes are not supported.\")\n         if not hasattr(shape, \"__iter__\"):\n             raise ValueError(f\"Cannot convert '{shape}' to a shape.\")\n+        if config.backend() == \"tensorflow\":\n+            if isinstance(shape, tf.TensorShape):\n+                # `tf.TensorShape` may contain `Dimension` objects.\n+                # We need to convert the items in it to either int or `None`\n+                shape = shape.as_list()\n         shape = tuple(shape)\n \n     if config.backend() == \"torch\":"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+from keras.utils.module_utils import tensorflow as tf\n+        if config.backend() == \"tensorflow\":\n+            if isinstance(shape, tf.TensorShape):\n+                # `tf.TensorShape` may contain `Dimension` objects.\n+                # We need to convert the items in it to either int or `None`\n+                shape = shape.as_list()\n",
            "whole_hunk": "@@ -6,6 +6,7 @@ from keras.backend.common import global_state\n from keras.backend.common.name_scope import current_path\n from keras.backend.common.stateless_scope import get_stateless_scope\n from keras.backend.common.stateless_scope import in_stateless_scope\n+from keras.utils.module_utils import tensorflow as tf\n from keras.utils.naming import auto_name\n \n \n@@ -403,6 +404,11 @@ def standardize_shape(shape):\n             raise ValueError(\"Undefined shapes are not supported.\")\n         if not hasattr(shape, \"__iter__\"):\n             raise ValueError(f\"Cannot convert '{shape}' to a shape.\")\n+        if config.backend() == \"tensorflow\":\n+            if isinstance(shape, tf.TensorShape):\n+                # `tf.TensorShape` may contain `Dimension` objects.\n+                # We need to convert the items in it to either int or `None`\n+                shape = shape.as_list()\n         shape = tuple(shape)\n \n     if config.backend() == \"torch\":"
        }
    ]
},
{
    "Id": 12,
    "commit_link": "https://github.com/keras-team/keras/commit/6f35f2efcb14599cc0c982cf39d51df3d56dceb9",
    "date": "2024-01-05T09:08:28-08:00",
    "message": "Allow rank > 2 for input shapes in numerical_utils (#19020)\n\n* Allow rank > 2 for input shapes in numerical_utils\r\n\r\n* Update `utils/numerical_utils.py` to support input shapes with a rank greater than two.\r\n* when the output_mode is  `multi_hot`, `one_hot`, or `int`\r\n\r\nhttps://github.com/keras-team/keras/issues/18995\r\n\r\n* Add more test cases for one_hot and multi_hot\r\n\r\n* Refactor binary_output check for input rank validation\r\n\r\nMoved binary_output evaluation to start.",
    "changes": [
        {
            "name": "discretization_test.py",
            "path": "keras/layers/preprocessing/discretization_test.py",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 6,
                    "new_start": 1,
                    "new_length": 7,
                    "hunk": "@@ -1,6 +1,7 @@\n import os\n \n import numpy as np\n+from absl.testing import parameterized\n from tensorflow import data as tf_data\n \n from keras import backend\n"
                },
                {
                    "old_start": 10,
                    "old_length": 7,
                    "new_start": 11,
                    "new_length": 7,
                    "hunk": "@@ -10,7 +11,7 @@ from keras import testing\n from keras.saving import saving_api\n \n \n-class DicretizationTest(testing.TestCase):\n+class DicretizationTest(testing.TestCase, parameterized.TestCase):\n     def test_discretization_basics(self):\n         self.run_layer_test(\n             layers.Discretization,\n"
                },
                {
                    "old_start": 35,
                    "old_length": 38,
                    "new_start": 36,
                    "new_length": 39,
                    "hunk": "@@ -35,38 +36,39 @@ class DicretizationTest(testing.TestCase):\n         output = layer(np.array([[0.0, 0.1, 0.3]]))\n         self.assertTrue(output.dtype, \"int32\")\n \n-    def test_correctness(self):\n-        # int mode\n-        layer = layers.Discretization(\n-            bin_boundaries=[0.0, 0.5, 1.0], output_mode=\"int\"\n-        )\n-        output = layer(np.array([[-1.0, 0.0, 0.1, 0.8, 1.2]]))\n-        self.assertTrue(backend.is_tensor(output))\n-        self.assertAllClose(output, np.array([[0, 1, 1, 2, 3]]))\n-\n-        # one_hot mode\n-        layer = layers.Discretization(\n-            bin_boundaries=[0.0, 0.5, 1.0], output_mode=\"one_hot\"\n-        )\n-        output = layer(np.array([0.1, 0.8]))\n-        self.assertTrue(backend.is_tensor(output))\n-        self.assertAllClose(output, np.array([[0, 1, 0, 0], [0, 0, 1, 0]]))\n-\n-        # multi_hot mode\n-        layer = layers.Discretization(\n-            bin_boundaries=[0.0, 0.5, 1.0], output_mode=\"multi_hot\"\n-        )\n-        output = layer(np.array([[0.1, 0.8]]))\n-        self.assertTrue(backend.is_tensor(output))\n-        self.assertAllClose(output, np.array([[0, 1, 1, 0]]))\n+    @parameterized.parameters(\n+        [\n+            (\"int\", [[-1.0, 0.0, 0.1, 0.8, 1.2]], [[0, 1, 1, 2, 3]]),\n+            (\"one_hot\", [0.1, 0.8], [[0, 1, 0, 0], [0, 0, 1, 0]]),\n+            (\"multi_hot\", [[0.1, 0.8]], [[0, 1, 1, 0]]),\n+            (\n+                \"one_hot\",\n+                [[[0.15, 0.75], [0.85, 0.45]]],\n+                [\n+                    [\n+                        [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]],\n+                        [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0]],\n+                    ]\n+                ],\n+            ),\n+            (\n+                \"multi_hot\",\n+                [[[0.15, 0.75], [0.85, 0.45]]],\n+                [[[0.0, 1.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0]]],\n+            ),\n+            (\"count\", [[0.1, 0.8, 0.9]], [[0, 1, 2, 0]]),\n+        ]\n+    )\n+    def test_correctness(self, output_mode, input_array, expected_output):\n+        input_array = np.array(input_array)\n+        expected_output = np.array(expected_output)\n \n-        # count mode\n         layer = layers.Discretization(\n-            bin_boundaries=[0.0, 0.5, 1.0], output_mode=\"count\"\n+            bin_boundaries=[0.0, 0.5, 1.0], output_mode=output_mode\n         )\n-        output = layer(np.array([[0.1, 0.8, 0.9]]))\n+        output = layer(input_array)\n         self.assertTrue(backend.is_tensor(output))\n-        self.assertAllClose(output, np.array([[0, 1, 2, 0]]))\n+        self.assertAllClose(output, expected_output)\n \n     def test_tf_data_compatibility(self):\n         # With fixed bins\n"
                }
            ],
            "whole_deleted": "-class DicretizationTest(testing.TestCase):\n-    def test_correctness(self):\n-        # int mode\n-        layer = layers.Discretization(\n-            bin_boundaries=[0.0, 0.5, 1.0], output_mode=\"int\"\n-        )\n-        output = layer(np.array([[-1.0, 0.0, 0.1, 0.8, 1.2]]))\n-        self.assertTrue(backend.is_tensor(output))\n-        self.assertAllClose(output, np.array([[0, 1, 1, 2, 3]]))\n-\n-        # one_hot mode\n-        layer = layers.Discretization(\n-            bin_boundaries=[0.0, 0.5, 1.0], output_mode=\"one_hot\"\n-        )\n-        output = layer(np.array([0.1, 0.8]))\n-        self.assertTrue(backend.is_tensor(output))\n-        self.assertAllClose(output, np.array([[0, 1, 0, 0], [0, 0, 1, 0]]))\n-\n-        # multi_hot mode\n-        layer = layers.Discretization(\n-            bin_boundaries=[0.0, 0.5, 1.0], output_mode=\"multi_hot\"\n-        )\n-        output = layer(np.array([[0.1, 0.8]]))\n-        self.assertTrue(backend.is_tensor(output))\n-        self.assertAllClose(output, np.array([[0, 1, 1, 0]]))\n-        # count mode\n-            bin_boundaries=[0.0, 0.5, 1.0], output_mode=\"count\"\n-        output = layer(np.array([[0.1, 0.8, 0.9]]))\n-        self.assertAllClose(output, np.array([[0, 1, 2, 0]]))\n",
            "whole_added": "+from absl.testing import parameterized\n+class DicretizationTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.parameters(\n+        [\n+            (\"int\", [[-1.0, 0.0, 0.1, 0.8, 1.2]], [[0, 1, 1, 2, 3]]),\n+            (\"one_hot\", [0.1, 0.8], [[0, 1, 0, 0], [0, 0, 1, 0]]),\n+            (\"multi_hot\", [[0.1, 0.8]], [[0, 1, 1, 0]]),\n+            (\n+                \"one_hot\",\n+                [[[0.15, 0.75], [0.85, 0.45]]],\n+                [\n+                    [\n+                        [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]],\n+                        [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0]],\n+                    ]\n+                ],\n+            ),\n+            (\n+                \"multi_hot\",\n+                [[[0.15, 0.75], [0.85, 0.45]]],\n+                [[[0.0, 1.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0]]],\n+            ),\n+            (\"count\", [[0.1, 0.8, 0.9]], [[0, 1, 2, 0]]),\n+        ]\n+    )\n+    def test_correctness(self, output_mode, input_array, expected_output):\n+        input_array = np.array(input_array)\n+        expected_output = np.array(expected_output)\n+            bin_boundaries=[0.0, 0.5, 1.0], output_mode=output_mode\n+        output = layer(input_array)\n+        self.assertAllClose(output, expected_output)\n",
            "whole_hunk": "@@ -1,6 +1,7 @@\n import os\n \n import numpy as np\n+from absl.testing import parameterized\n from tensorflow import data as tf_data\n \n from keras import backend\n@@ -10,7 +11,7 @@ from keras import testing\n from keras.saving import saving_api\n \n \n-class DicretizationTest(testing.TestCase):\n+class DicretizationTest(testing.TestCase, parameterized.TestCase):\n     def test_discretization_basics(self):\n         self.run_layer_test(\n             layers.Discretization,\n@@ -35,38 +36,39 @@ class DicretizationTest(testing.TestCase):\n         output = layer(np.array([[0.0, 0.1, 0.3]]))\n         self.assertTrue(output.dtype, \"int32\")\n \n-    def test_correctness(self):\n-        # int mode\n-        layer = layers.Discretization(\n-            bin_boundaries=[0.0, 0.5, 1.0], output_mode=\"int\"\n-        )\n-        output = layer(np.array([[-1.0, 0.0, 0.1, 0.8, 1.2]]))\n-        self.assertTrue(backend.is_tensor(output))\n-        self.assertAllClose(output, np.array([[0, 1, 1, 2, 3]]))\n-\n-        # one_hot mode\n-        layer = layers.Discretization(\n-            bin_boundaries=[0.0, 0.5, 1.0], output_mode=\"one_hot\"\n-        )\n-        output = layer(np.array([0.1, 0.8]))\n-        self.assertTrue(backend.is_tensor(output))\n-        self.assertAllClose(output, np.array([[0, 1, 0, 0], [0, 0, 1, 0]]))\n-\n-        # multi_hot mode\n-        layer = layers.Discretization(\n-            bin_boundaries=[0.0, 0.5, 1.0], output_mode=\"multi_hot\"\n-        )\n-        output = layer(np.array([[0.1, 0.8]]))\n-        self.assertTrue(backend.is_tensor(output))\n-        self.assertAllClose(output, np.array([[0, 1, 1, 0]]))\n+    @parameterized.parameters(\n+        [\n+            (\"int\", [[-1.0, 0.0, 0.1, 0.8, 1.2]], [[0, 1, 1, 2, 3]]),\n+            (\"one_hot\", [0.1, 0.8], [[0, 1, 0, 0], [0, 0, 1, 0]]),\n+            (\"multi_hot\", [[0.1, 0.8]], [[0, 1, 1, 0]]),\n+            (\n+                \"one_hot\",\n+                [[[0.15, 0.75], [0.85, 0.45]]],\n+                [\n+                    [\n+                        [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]],\n+                        [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0]],\n+                    ]\n+                ],\n+            ),\n+            (\n+                \"multi_hot\",\n+                [[[0.15, 0.75], [0.85, 0.45]]],\n+                [[[0.0, 1.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0]]],\n+            ),\n+            (\"count\", [[0.1, 0.8, 0.9]], [[0, 1, 2, 0]]),\n+        ]\n+    )\n+    def test_correctness(self, output_mode, input_array, expected_output):\n+        input_array = np.array(input_array)\n+        expected_output = np.array(expected_output)\n \n-        # count mode\n         layer = layers.Discretization(\n-            bin_boundaries=[0.0, 0.5, 1.0], output_mode=\"count\"\n+            bin_boundaries=[0.0, 0.5, 1.0], output_mode=output_mode\n         )\n-        output = layer(np.array([[0.1, 0.8, 0.9]]))\n+        output = layer(input_array)\n         self.assertTrue(backend.is_tensor(output))\n-        self.assertAllClose(output, np.array([[0, 1, 2, 0]]))\n+        self.assertAllClose(output, expected_output)\n \n     def test_tf_data_compatibility(self):\n         # With fixed bins\n"
        },
        {
            "name": "numerical_utils.py",
            "path": "keras/utils/numerical_utils.py",
            "patches": [
                {
                    "old_start": 120,
                    "old_length": 20,
                    "new_start": 120,
                    "new_length": 27,
                    "hunk": "@@ -120,20 +120,27 @@ def encode_categorical_inputs(\n     if output_mode == \"int\":\n         return backend_module.cast(inputs, dtype=dtype)\n \n-    original_shape = inputs.shape\n+    binary_output = output_mode in (\"multi_hot\", \"one_hot\")\n+    original_shape = backend_module.shape(inputs)\n+    rank_of_inputs = len(original_shape)\n+\n     # In all cases, we should uprank scalar input to a single sample.\n-    if len(backend_module.shape(inputs)) == 0:\n+    if rank_of_inputs == 0:\n+        # We need to update `rank_of_inputs`\n+        # If necessary.\n         inputs = backend_module.numpy.expand_dims(inputs, -1)\n+    elif rank_of_inputs > 2:\n+        # The `count` mode does not support inputs with a rank greater than 2.\n+        if not binary_output:\n+            raise ValueError(\n+                \"When output_mode is anything other than \"\n+                \"`'multi_hot', 'one_hot', or 'int'`, \"\n+                \"the rank must be 2 or less. \"\n+                f\"Received output_mode: {output_mode} \"\n+                f\"and input shape: {original_shape}, \"\n+                f\"which would result in output rank {rank_of_inputs}.\"\n+            )\n \n-    if len(backend_module.shape(inputs)) > 2:\n-        raise ValueError(\n-            \"When output_mode is not `'int'`, maximum supported output rank \"\n-            f\"is 2. Received output_mode {output_mode} and input shape \"\n-            f\"{original_shape}, \"\n-            f\"which would result in output rank {inputs.shape.rank}.\"\n-        )\n-\n-    binary_output = output_mode in (\"multi_hot\", \"one_hot\")\n     if binary_output:\n         if output_mode == \"one_hot\":\n             bincounts = backend_module.nn.one_hot(inputs, depth)"
                }
            ],
            "whole_deleted": "-    original_shape = inputs.shape\n-    if len(backend_module.shape(inputs)) == 0:\n-    if len(backend_module.shape(inputs)) > 2:\n-        raise ValueError(\n-            \"When output_mode is not `'int'`, maximum supported output rank \"\n-            f\"is 2. Received output_mode {output_mode} and input shape \"\n-            f\"{original_shape}, \"\n-            f\"which would result in output rank {inputs.shape.rank}.\"\n-        )\n-\n-    binary_output = output_mode in (\"multi_hot\", \"one_hot\")\n",
            "whole_added": "+    binary_output = output_mode in (\"multi_hot\", \"one_hot\")\n+    original_shape = backend_module.shape(inputs)\n+    rank_of_inputs = len(original_shape)\n+\n+    if rank_of_inputs == 0:\n+        # We need to update `rank_of_inputs`\n+        # If necessary.\n+    elif rank_of_inputs > 2:\n+        # The `count` mode does not support inputs with a rank greater than 2.\n+        if not binary_output:\n+            raise ValueError(\n+                \"When output_mode is anything other than \"\n+                \"`'multi_hot', 'one_hot', or 'int'`, \"\n+                \"the rank must be 2 or less. \"\n+                f\"Received output_mode: {output_mode} \"\n+                f\"and input shape: {original_shape}, \"\n+                f\"which would result in output rank {rank_of_inputs}.\"\n+            )\n",
            "whole_hunk": "@@ -120,20 +120,27 @@ def encode_categorical_inputs(\n     if output_mode == \"int\":\n         return backend_module.cast(inputs, dtype=dtype)\n \n-    original_shape = inputs.shape\n+    binary_output = output_mode in (\"multi_hot\", \"one_hot\")\n+    original_shape = backend_module.shape(inputs)\n+    rank_of_inputs = len(original_shape)\n+\n     # In all cases, we should uprank scalar input to a single sample.\n-    if len(backend_module.shape(inputs)) == 0:\n+    if rank_of_inputs == 0:\n+        # We need to update `rank_of_inputs`\n+        # If necessary.\n         inputs = backend_module.numpy.expand_dims(inputs, -1)\n+    elif rank_of_inputs > 2:\n+        # The `count` mode does not support inputs with a rank greater than 2.\n+        if not binary_output:\n+            raise ValueError(\n+                \"When output_mode is anything other than \"\n+                \"`'multi_hot', 'one_hot', or 'int'`, \"\n+                \"the rank must be 2 or less. \"\n+                f\"Received output_mode: {output_mode} \"\n+                f\"and input shape: {original_shape}, \"\n+                f\"which would result in output rank {rank_of_inputs}.\"\n+            )\n \n-    if len(backend_module.shape(inputs)) > 2:\n-        raise ValueError(\n-            \"When output_mode is not `'int'`, maximum supported output rank \"\n-            f\"is 2. Received output_mode {output_mode} and input shape \"\n-            f\"{original_shape}, \"\n-            f\"which would result in output rank {inputs.shape.rank}.\"\n-        )\n-\n-    binary_output = output_mode in (\"multi_hot\", \"one_hot\")\n     if binary_output:\n         if output_mode == \"one_hot\":\n             bincounts = backend_module.nn.one_hot(inputs, depth)"
        }
    ]
}]