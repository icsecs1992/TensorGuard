[{
    "Id": 195,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/5b98d43488bed0836b4da5996a50bafd0dd2c11c",
    "date": "2024-04-23T14:18:35+00:00",
    "message": "Verify types in custom op schemas (#124520)\n\nBefore this PR, we didn't check that types in a schema were valid. This\nis because TorchScript treats unknown types as type variables.\n\nThis PR checks types in a schema for the TORCH_LIBRARY APIs. To do this,\nwe add an `allow_typevars` flag to parseSchema so that TorchScript can\nuse allow_typevars=True. We also add some error messages for common\nmistakes (e.g. using int64_t or double in schema).\n\nTest Plan:\n- new tests\n\nDifferential Revision: [D56432690](https://our.internmc.facebook.com/intern/diff/D56432690)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/124520\nApproved by: https://github.com/albanD",
    "label": "YES",
    "changes": [
        {
            "name": "test_custom_ops.py",
            "path": "test/test_custom_ops.py",
            "patches": [
                {
                    "old_start": 1740,
                    "old_length": 6,
                    "new_start": 1740,
                    "new_length": 17,
                    "hunk": "@@ -1740,6 +1740,17 @@ dynamic shape operator: _torch_testing.numpy_nonzero.default\n             res = torch._library.utils.is_functional_schema(schema)\n             self.assertEqual(res, expected)\n \n+    def test_incorrect_schema_types(self):\n+        with torch.library._scoped_library(\"mylib\", \"FRAGMENT\") as lib:\n+            with self.assertRaisesRegex(RuntimeError, \"unknown type specifier\"):\n+                lib.define(\"foo12(Tensor a) -> asdfasdf\")\n+            with self.assertRaisesRegex(RuntimeError, \"unknown type specifier\"):\n+                lib.define(\"foo12(asdf a) -> Tensor\")\n+            with self.assertRaisesRegex(RuntimeError, \"Use `SymInt` or `int`\"):\n+                lib.define(\"foo12(int64_t a) -> Tensor\")\n+            with self.assertRaisesRegex(RuntimeError, \"Use `float`\"):\n+                lib.define(\"foo12(double a) -> Tensor\")\n+\n     def test_is_tensorlist_like_type(self):\n         tensorlists = [\n             # Tensor[]\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    def test_incorrect_schema_types(self):\n+        with torch.library._scoped_library(\"mylib\", \"FRAGMENT\") as lib:\n+            with self.assertRaisesRegex(RuntimeError, \"unknown type specifier\"):\n+                lib.define(\"foo12(Tensor a) -> asdfasdf\")\n+            with self.assertRaisesRegex(RuntimeError, \"unknown type specifier\"):\n+                lib.define(\"foo12(asdf a) -> Tensor\")\n+            with self.assertRaisesRegex(RuntimeError, \"Use `SymInt` or `int`\"):\n+                lib.define(\"foo12(int64_t a) -> Tensor\")\n+            with self.assertRaisesRegex(RuntimeError, \"Use `float`\"):\n+                lib.define(\"foo12(double a) -> Tensor\")\n+\n",
            "whole_hunk": "@@ -1740,6 +1740,17 @@ dynamic shape operator: _torch_testing.numpy_nonzero.default\n             res = torch._library.utils.is_functional_schema(schema)\n             self.assertEqual(res, expected)\n \n+    def test_incorrect_schema_types(self):\n+        with torch.library._scoped_library(\"mylib\", \"FRAGMENT\") as lib:\n+            with self.assertRaisesRegex(RuntimeError, \"unknown type specifier\"):\n+                lib.define(\"foo12(Tensor a) -> asdfasdf\")\n+            with self.assertRaisesRegex(RuntimeError, \"unknown type specifier\"):\n+                lib.define(\"foo12(asdf a) -> Tensor\")\n+            with self.assertRaisesRegex(RuntimeError, \"Use `SymInt` or `int`\"):\n+                lib.define(\"foo12(int64_t a) -> Tensor\")\n+            with self.assertRaisesRegex(RuntimeError, \"Use `float`\"):\n+                lib.define(\"foo12(double a) -> Tensor\")\n+\n     def test_is_tensorlist_like_type(self):\n         tensorlists = [\n             # Tensor[]\n"
        },
        {
            "name": "function_schema_parser.cpp",
            "path": "torch/csrc/jit/frontend/function_schema_parser.cpp",
            "patches": [
                {
                    "old_start": 23,
                    "old_length": 14,
                    "new_start": 23,
                    "new_length": 14,
                    "hunk": "@@ -23,14 +23,14 @@ namespace torch::jit {\n \n namespace {\n struct SchemaParser {\n-  explicit SchemaParser(const std::string& str)\n+  explicit SchemaParser(const std::string& str, bool allow_typevars)\n       : L(std::make_shared<Source>(\n             c10::string_view(str),\n             c10::nullopt,\n             0,\n             nullptr,\n             Source::DONT_COPY)),\n-        type_parser(L, /*parse_complete_tensor_types*/ false) {}\n+        type_parser(L, /*parse_complete_tensor_types*/ false, allow_typevars) {}\n \n   std::variant<OperatorName, FunctionSchema> parseDeclaration() {\n     OperatorName name = parseName();\n"
                },
                {
                    "old_start": 361,
                    "old_length": 16,
                    "new_start": 361,
                    "new_length": 19,
                    "hunk": "@@ -361,16 +361,19 @@ struct SchemaParser {\n   }\n   Lexer L;\n   SchemaTypeParser type_parser;\n+  bool allow_typevars_;\n };\n } // namespace\n \n std::variant<OperatorName, FunctionSchema> parseSchemaOrName(\n-    const std::string& schemaOrName) {\n-  return SchemaParser(schemaOrName).parseExactlyOneDeclaration();\n+    const std::string& schemaOrName,\n+    bool allow_typevars) {\n+  return SchemaParser(schemaOrName, allow_typevars)\n+      .parseExactlyOneDeclaration();\n }\n \n-FunctionSchema parseSchema(const std::string& schema) {\n-  auto parsed = parseSchemaOrName(schema);\n+FunctionSchema parseSchema(const std::string& schema, bool allow_typevars) {\n+  auto parsed = parseSchemaOrName(schema, allow_typevars);\n   TORCH_CHECK(\n       std::holds_alternative<FunctionSchema>(parsed),\n       \"Tried to parse a function schema but only the operator name was given\");\n"
                }
            ],
            "whole_deleted": "-  explicit SchemaParser(const std::string& str)\n-        type_parser(L, /*parse_complete_tensor_types*/ false) {}\n-    const std::string& schemaOrName) {\n-  return SchemaParser(schemaOrName).parseExactlyOneDeclaration();\n-FunctionSchema parseSchema(const std::string& schema) {\n-  auto parsed = parseSchemaOrName(schema);\n",
            "whole_added": "+  explicit SchemaParser(const std::string& str, bool allow_typevars)\n+        type_parser(L, /*parse_complete_tensor_types*/ false, allow_typevars) {}\n+  bool allow_typevars_;\n+    const std::string& schemaOrName,\n+    bool allow_typevars) {\n+  return SchemaParser(schemaOrName, allow_typevars)\n+      .parseExactlyOneDeclaration();\n+FunctionSchema parseSchema(const std::string& schema, bool allow_typevars) {\n+  auto parsed = parseSchemaOrName(schema, allow_typevars);\n",
            "whole_hunk": "@@ -23,14 +23,14 @@ namespace torch::jit {\n \n namespace {\n struct SchemaParser {\n-  explicit SchemaParser(const std::string& str)\n+  explicit SchemaParser(const std::string& str, bool allow_typevars)\n       : L(std::make_shared<Source>(\n             c10::string_view(str),\n             c10::nullopt,\n             0,\n             nullptr,\n             Source::DONT_COPY)),\n-        type_parser(L, /*parse_complete_tensor_types*/ false) {}\n+        type_parser(L, /*parse_complete_tensor_types*/ false, allow_typevars) {}\n \n   std::variant<OperatorName, FunctionSchema> parseDeclaration() {\n     OperatorName name = parseName();\n@@ -361,16 +361,19 @@ struct SchemaParser {\n   }\n   Lexer L;\n   SchemaTypeParser type_parser;\n+  bool allow_typevars_;\n };\n } // namespace\n \n std::variant<OperatorName, FunctionSchema> parseSchemaOrName(\n-    const std::string& schemaOrName) {\n-  return SchemaParser(schemaOrName).parseExactlyOneDeclaration();\n+    const std::string& schemaOrName,\n+    bool allow_typevars) {\n+  return SchemaParser(schemaOrName, allow_typevars)\n+      .parseExactlyOneDeclaration();\n }\n \n-FunctionSchema parseSchema(const std::string& schema) {\n-  auto parsed = parseSchemaOrName(schema);\n+FunctionSchema parseSchema(const std::string& schema, bool allow_typevars) {\n+  auto parsed = parseSchemaOrName(schema, allow_typevars);\n   TORCH_CHECK(\n       std::holds_alternative<FunctionSchema>(parsed),\n       \"Tried to parse a function schema but only the operator name was given\");\n"
        },
        {
            "name": "function_schema_parser.h",
            "path": "torch/csrc/jit/frontend/function_schema_parser.h",
            "patches": [
                {
                    "old_start": 8,
                    "old_length": 9,
                    "new_start": 8,
                    "new_length": 15,
                    "hunk": "@@ -8,9 +8,15 @@\n namespace torch {\n namespace jit {\n \n+// allow_typevars: If true, we assume that lowercase types that we don't\n+// understand are type variables. This is only needed for TorchScript (and not\n+// not needed for custom ops).\n TORCH_API std::variant<c10::OperatorName, c10::FunctionSchema> parseSchemaOrName(\n-    const std::string& schemaOrName);\n-TORCH_API c10::FunctionSchema parseSchema(const std::string& schema);\n+    const std::string& schemaOrName,\n+    bool allow_typevars = true);\n+TORCH_API c10::FunctionSchema parseSchema(\n+    const std::string& schema,\n+    bool allow_typevars = true);\n TORCH_API c10::OperatorName parseName(const std::string& name);\n \n } // namespace jit\n"
                }
            ],
            "whole_deleted": "-    const std::string& schemaOrName);\n-TORCH_API c10::FunctionSchema parseSchema(const std::string& schema);\n",
            "whole_added": "+// allow_typevars: If true, we assume that lowercase types that we don't\n+// understand are type variables. This is only needed for TorchScript (and not\n+// not needed for custom ops).\n+    const std::string& schemaOrName,\n+    bool allow_typevars = true);\n+TORCH_API c10::FunctionSchema parseSchema(\n+    const std::string& schema,\n+    bool allow_typevars = true);\n",
            "whole_hunk": "@@ -8,9 +8,15 @@\n namespace torch {\n namespace jit {\n \n+// allow_typevars: If true, we assume that lowercase types that we don't\n+// understand are type variables. This is only needed for TorchScript (and not\n+// not needed for custom ops).\n TORCH_API std::variant<c10::OperatorName, c10::FunctionSchema> parseSchemaOrName(\n-    const std::string& schemaOrName);\n-TORCH_API c10::FunctionSchema parseSchema(const std::string& schema);\n+    const std::string& schemaOrName,\n+    bool allow_typevars = true);\n+TORCH_API c10::FunctionSchema parseSchema(\n+    const std::string& schema,\n+    bool allow_typevars = true);\n TORCH_API c10::OperatorName parseName(const std::string& name);\n \n } // namespace jit\n"
        },
        {
            "name": "schema_type_parser.cpp",
            "path": "torch/csrc/jit/frontend/schema_type_parser.cpp",
            "patches": [
                {
                    "old_start": 82,
                    "old_length": 12,
                    "new_start": 82,
                    "new_length": 27,
                    "hunk": "@@ -82,12 +82,27 @@ TypePtr SchemaTypeParser::parseBaseType() {\n \n   auto it = type_map.find(text);\n   if (it == type_map.end()) {\n-    if (!text.empty() && islower(text[0])) {\n+    if (allow_typevars_ && !text.empty() && islower(text[0])) {\n       // lower case identifiers that are not otherwise valid types\n       // are treated as type variables\n       return c10::TypeFactory::createNamed<VarType>(text);\n     }\n-    throw ErrorReport(tok.range) << \"unknown type specifier\";\n+    if (text == \"double\") {\n+      throw ErrorReport(tok.range)\n+          << \"Use `float` instead of `double` in an operator's schema string. \"\n+             \"`float` in schema corresponds to the double type in C++\";\n+    }\n+    if (text == \"int64_t\") {\n+      throw ErrorReport(tok.range)\n+          << \"Use `SymInt` or `int` instead of `int64_t` in an operator's schema string. \"\n+             \"`SymInt` corresponds to c10::SymInt in C++ while `int` in schema corresponds \"\n+             \"to the int64_t type in C++.\";\n+    }\n+    throw ErrorReport(tok.range)\n+        << \"unknown type specifier. Common valid schema types include \"\n+           \"Tensor, SymInt, int, float, bool, Scalar; \"\n+           \"for a full list, please see \"\n+           \"https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#func \";\n   }\n   return it->second;\n }\n"
                }
            ],
            "whole_deleted": "-    if (!text.empty() && islower(text[0])) {\n-    throw ErrorReport(tok.range) << \"unknown type specifier\";\n",
            "whole_added": "+    if (allow_typevars_ && !text.empty() && islower(text[0])) {\n+    if (text == \"double\") {\n+      throw ErrorReport(tok.range)\n+          << \"Use `float` instead of `double` in an operator's schema string. \"\n+             \"`float` in schema corresponds to the double type in C++\";\n+    }\n+    if (text == \"int64_t\") {\n+      throw ErrorReport(tok.range)\n+          << \"Use `SymInt` or `int` instead of `int64_t` in an operator's schema string. \"\n+             \"`SymInt` corresponds to c10::SymInt in C++ while `int` in schema corresponds \"\n+             \"to the int64_t type in C++.\";\n+    }\n+    throw ErrorReport(tok.range)\n+        << \"unknown type specifier. Common valid schema types include \"\n+           \"Tensor, SymInt, int, float, bool, Scalar; \"\n+           \"for a full list, please see \"\n+           \"https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#func \";\n",
            "whole_hunk": "@@ -82,12 +82,27 @@ TypePtr SchemaTypeParser::parseBaseType() {\n \n   auto it = type_map.find(text);\n   if (it == type_map.end()) {\n-    if (!text.empty() && islower(text[0])) {\n+    if (allow_typevars_ && !text.empty() && islower(text[0])) {\n       // lower case identifiers that are not otherwise valid types\n       // are treated as type variables\n       return c10::TypeFactory::createNamed<VarType>(text);\n     }\n-    throw ErrorReport(tok.range) << \"unknown type specifier\";\n+    if (text == \"double\") {\n+      throw ErrorReport(tok.range)\n+          << \"Use `float` instead of `double` in an operator's schema string. \"\n+             \"`float` in schema corresponds to the double type in C++\";\n+    }\n+    if (text == \"int64_t\") {\n+      throw ErrorReport(tok.range)\n+          << \"Use `SymInt` or `int` instead of `int64_t` in an operator's schema string. \"\n+             \"`SymInt` corresponds to c10::SymInt in C++ while `int` in schema corresponds \"\n+             \"to the int64_t type in C++.\";\n+    }\n+    throw ErrorReport(tok.range)\n+        << \"unknown type specifier. Common valid schema types include \"\n+           \"Tensor, SymInt, int, float, bool, Scalar; \"\n+           \"for a full list, please see \"\n+           \"https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#func \";\n   }\n   return it->second;\n }\n"
        },
        {
            "name": "schema_type_parser.h",
            "path": "torch/csrc/jit/frontend/schema_type_parser.h",
            "patches": [
                {
                    "old_start": 20,
                    "old_length": 8,
                    "new_start": 20,
                    "new_length": 13,
                    "hunk": "@@ -20,8 +20,13 @@ struct TORCH_API SchemaTypeParser {\n   c10::optional<at::ScalarType> parseTensorDType(const std::string& dtype);\n   TypePtr parseRefinedTensor();\n \n-  SchemaTypeParser(Lexer& L, bool parse_complete_tensor_types)\n-      : complete_tensor_types(parse_complete_tensor_types), L(L) {}\n+  SchemaTypeParser(\n+      Lexer& L,\n+      bool parse_complete_tensor_types,\n+      bool allow_typevars)\n+      : complete_tensor_types(parse_complete_tensor_types),\n+        L(L),\n+        allow_typevars_(allow_typevars) {}\n \n  private:\n   c10::optional<bool> tryToParseRequiresGrad();\n"
                },
                {
                    "old_start": 35,
                    "old_length": 6,
                    "new_start": 40,
                    "new_length": 7,
                    "hunk": "@@ -35,6 +40,7 @@ struct TORCH_API SchemaTypeParser {\n   bool complete_tensor_types;\n   Lexer& L;\n   size_t next_id = 0;\n+  bool allow_typevars_;\n };\n } // namespace jit\n } // namespace torch\n"
                }
            ],
            "whole_deleted": "-  SchemaTypeParser(Lexer& L, bool parse_complete_tensor_types)\n-      : complete_tensor_types(parse_complete_tensor_types), L(L) {}\n",
            "whole_added": "+  SchemaTypeParser(\n+      Lexer& L,\n+      bool parse_complete_tensor_types,\n+      bool allow_typevars)\n+      : complete_tensor_types(parse_complete_tensor_types),\n+        L(L),\n+        allow_typevars_(allow_typevars) {}\n+  bool allow_typevars_;\n",
            "whole_hunk": "@@ -20,8 +20,13 @@ struct TORCH_API SchemaTypeParser {\n   c10::optional<at::ScalarType> parseTensorDType(const std::string& dtype);\n   TypePtr parseRefinedTensor();\n \n-  SchemaTypeParser(Lexer& L, bool parse_complete_tensor_types)\n-      : complete_tensor_types(parse_complete_tensor_types), L(L) {}\n+  SchemaTypeParser(\n+      Lexer& L,\n+      bool parse_complete_tensor_types,\n+      bool allow_typevars)\n+      : complete_tensor_types(parse_complete_tensor_types),\n+        L(L),\n+        allow_typevars_(allow_typevars) {}\n \n  private:\n   c10::optional<bool> tryToParseRequiresGrad();\n@@ -35,6 +40,7 @@ struct TORCH_API SchemaTypeParser {\n   bool complete_tensor_types;\n   Lexer& L;\n   size_t next_id = 0;\n+  bool allow_typevars_;\n };\n } // namespace jit\n } // namespace torch\n"
        },
        {
            "name": "irparser.cpp",
            "path": "torch/csrc/jit/ir/irparser.cpp",
            "patches": [
                {
                    "old_start": 35,
                    "old_length": 7,
                    "new_start": 35,
                    "new_length": 10,
                    "hunk": "@@ -35,7 +35,10 @@ class IRParser {\n       : L(std::make_shared<Source>(str)),\n         g(graph),\n         vmap(vmap),\n-        type_parser(L, /*parse_complete_tensor_types*/ true),\n+        type_parser(\n+            L,\n+            /*parse_complete_tensor_types*/ true,\n+            /*allow_type_vars*/ true),\n         parse_tensor_constants_(parse_tensor_constants) {}\n \n   std::string parseVar();\n"
                }
            ],
            "whole_deleted": "-        type_parser(L, /*parse_complete_tensor_types*/ true),\n",
            "whole_added": "+        type_parser(\n+            L,\n+            /*parse_complete_tensor_types*/ true,\n+            /*allow_type_vars*/ true),\n",
            "whole_hunk": "@@ -35,7 +35,10 @@ class IRParser {\n       : L(std::make_shared<Source>(str)),\n         g(graph),\n         vmap(vmap),\n-        type_parser(L, /*parse_complete_tensor_types*/ true),\n+        type_parser(\n+            L,\n+            /*parse_complete_tensor_types*/ true,\n+            /*allow_type_vars*/ true),\n         parse_tensor_constants_(parse_tensor_constants) {}\n \n   std::string parseVar();\n"
        },
        {
            "name": "init.cpp",
            "path": "torch/csrc/jit/python/init.cpp",
            "patches": [
                {
                    "old_start": 1765,
                    "old_length": 7,
                    "new_start": 1765,
                    "new_length": 11,
                    "hunk": "@@ -1765,7 +1765,11 @@ void initJITBindings(PyObject* module) {\n       },\n       py::arg(\"input\"),\n       py::arg(\"parse_tensor_constants\") = false);\n-  m.def(\"parse_schema\", parseSchema);\n+  m.def(\n+      \"parse_schema\",\n+      &parseSchema,\n+      py::arg(\"schema\"),\n+      py::arg(\"allow_typevars\") = true);\n   m.def(\"unify_type_list\", [](const std::vector<TypePtr>& types) {\n     std::ostringstream s;\n     auto type = unifyTypeList(types, s);\n"
                }
            ],
            "whole_deleted": "-  m.def(\"parse_schema\", parseSchema);\n",
            "whole_added": "+  m.def(\n+      \"parse_schema\",\n+      &parseSchema,\n+      py::arg(\"schema\"),\n+      py::arg(\"allow_typevars\") = true);\n",
            "whole_hunk": "@@ -1765,7 +1765,11 @@ void initJITBindings(PyObject* module) {\n       },\n       py::arg(\"input\"),\n       py::arg(\"parse_tensor_constants\") = false);\n-  m.def(\"parse_schema\", parseSchema);\n+  m.def(\n+      \"parse_schema\",\n+      &parseSchema,\n+      py::arg(\"schema\"),\n+      py::arg(\"allow_typevars\") = true);\n   m.def(\"unify_type_list\", [](const std::vector<TypePtr>& types) {\n     std::ostringstream s;\n     auto type = unifyTypeList(types, s);\n"
        },
        {
            "name": "passes.cpp",
            "path": "torch/csrc/jit/runtime/static/passes.cpp",
            "patches": [
                {
                    "old_start": 1347,
                    "old_length": 7,
                    "new_start": 1347,
                    "new_length": 8,
                    "hunk": "@@ -1347,7 +1347,8 @@ bool isNoOpSlice(Node* node) {\n void EliminateNoOpSlice(std::shared_ptr<Graph>& graph) {\n   DepthFirstGraphNodeIterator it(graph);\n   auto schema = torch::schema(\n-      \"aten::slice.t(t[] l, int? start=None, int? end=None, int step=1) -> t[]\");\n+      \"aten::slice.t(t[] l, int? start=None, int? end=None, int step=1) -> t[]\",\n+      /*allow_typevars*/ true);\n   Node* node = nullptr;\n   std::vector<Node*> to_delete;\n   while ((node = it.next()) != nullptr) {\n"
                }
            ],
            "whole_deleted": "-      \"aten::slice.t(t[] l, int? start=None, int? end=None, int step=1) -> t[]\");\n",
            "whole_added": "+      \"aten::slice.t(t[] l, int? start=None, int? end=None, int step=1) -> t[]\",\n+      /*allow_typevars*/ true);\n",
            "whole_hunk": "@@ -1347,7 +1347,8 @@ bool isNoOpSlice(Node* node) {\n void EliminateNoOpSlice(std::shared_ptr<Graph>& graph) {\n   DepthFirstGraphNodeIterator it(graph);\n   auto schema = torch::schema(\n-      \"aten::slice.t(t[] l, int? start=None, int? end=None, int step=1) -> t[]\");\n+      \"aten::slice.t(t[] l, int? start=None, int? end=None, int step=1) -> t[]\",\n+      /*allow_typevars*/ true);\n   Node* node = nullptr;\n   std::vector<Node*> to_delete;\n   while ((node = it.next()) != nullptr) {\n"
        },
        {
            "name": "library.h",
            "path": "torch/library.h",
            "patches": [
                {
                    "old_start": 406,
                    "old_length": 8,
                    "new_start": 406,
                    "new_length": 8,
                    "hunk": "@@ -406,8 +406,8 @@ inline CppFunction dispatch(c10::DeviceType type, Func&& raw_f) {\n /// ```\n ///\n /// \\ingroup torch-schema-overloads\n-inline c10::FunctionSchema schema(const char* str, c10::AliasAnalysisKind k) {\n-  c10::FunctionSchema s = torch::jit::parseSchema(str);\n+inline c10::FunctionSchema schema(const char* str, c10::AliasAnalysisKind k, bool allow_typevars=false) {\n+  c10::FunctionSchema s = torch::jit::parseSchema(str, /*allow_typevars*/allow_typevars);\n   s.setAliasAnalysis(k);\n   return s;\n }\n"
                },
                {
                    "old_start": 415,
                    "old_length": 8,
                    "new_start": 415,
                    "new_length": 8,
                    "hunk": "@@ -415,8 +415,8 @@ inline c10::FunctionSchema schema(const char* str, c10::AliasAnalysisKind k) {\n /// Function schemas can be directly constructed from string literals.\n ///\n /// \\ingroup torch-schema-overloads\n-inline c10::FunctionSchema schema(const char* s) {\n-  return schema(s, c10::AliasAnalysisKind::FROM_SCHEMA);\n+inline c10::FunctionSchema schema(const char* s, bool allow_typevars=false) {\n+  return schema(s, c10::AliasAnalysisKind::FROM_SCHEMA, allow_typevars);\n }\n \n /// \\private"
                }
            ],
            "whole_deleted": "-inline c10::FunctionSchema schema(const char* str, c10::AliasAnalysisKind k) {\n-  c10::FunctionSchema s = torch::jit::parseSchema(str);\n-inline c10::FunctionSchema schema(const char* s) {\n-  return schema(s, c10::AliasAnalysisKind::FROM_SCHEMA);\n",
            "whole_added": "+inline c10::FunctionSchema schema(const char* str, c10::AliasAnalysisKind k, bool allow_typevars=false) {\n+  c10::FunctionSchema s = torch::jit::parseSchema(str, /*allow_typevars*/allow_typevars);\n+inline c10::FunctionSchema schema(const char* s, bool allow_typevars=false) {\n+  return schema(s, c10::AliasAnalysisKind::FROM_SCHEMA, allow_typevars);\n",
            "whole_hunk": "@@ -406,8 +406,8 @@ inline CppFunction dispatch(c10::DeviceType type, Func&& raw_f) {\n /// ```\n ///\n /// \\ingroup torch-schema-overloads\n-inline c10::FunctionSchema schema(const char* str, c10::AliasAnalysisKind k) {\n-  c10::FunctionSchema s = torch::jit::parseSchema(str);\n+inline c10::FunctionSchema schema(const char* str, c10::AliasAnalysisKind k, bool allow_typevars=false) {\n+  c10::FunctionSchema s = torch::jit::parseSchema(str, /*allow_typevars*/allow_typevars);\n   s.setAliasAnalysis(k);\n   return s;\n }\n@@ -415,8 +415,8 @@ inline c10::FunctionSchema schema(const char* str, c10::AliasAnalysisKind k) {\n /// Function schemas can be directly constructed from string literals.\n ///\n /// \\ingroup torch-schema-overloads\n-inline c10::FunctionSchema schema(const char* s) {\n-  return schema(s, c10::AliasAnalysisKind::FROM_SCHEMA);\n+inline c10::FunctionSchema schema(const char* s, bool allow_typevars=false) {\n+  return schema(s, c10::AliasAnalysisKind::FROM_SCHEMA, allow_typevars);\n }\n \n /// \\private"
        }
    ]
},
{
    "Id": 79,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/fbc7559ceb372d88b55c96ef6984accbaa0ec3ec",
    "date": "2024-06-18T00:55:50+00:00",
    "message": "[custom ops] convert string type annotation to real type (#128809)\n\nFixes #105157\n\nBug source: `from __future__ import annotations` converts type annotation to strings to make forwards references easier. However, existing custom ops do not consider strings to be valid types.\n\nFix: We check if the argument and return type annotation is string type. If so, we try to use `eval` to convert it to a type.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128809\nApproved by: https://github.com/zou3519",
    "label": "YES",
    "changes": [
        {
            "name": "test_infer_schema_annotation.py",
            "path": "test/custom_operator/test_infer_schema_annotation.py",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 207,
                    "hunk": "@@ -0,0 +1,207 @@\n+# Owner(s): [\"module: pt2-dispatcher\"]\n+from __future__ import annotations\n+\n+import typing\n+from typing import List, Optional, Sequence, Union  # noqa: F401\n+\n+import torch\n+import torch._custom_op.impl\n+from torch import Tensor, types\n+from torch.testing._internal.common_utils import run_tests, TestCase\n+\n+\n+mutates_args = {}\n+\n+\n+class TestInferSchemaWithAnnotation(TestCase):\n+    def test_tensor(self):\n+        def foo_op(x: torch.Tensor) -> torch.Tensor:\n+            return x.clone()\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op, mutates_args)\n+        self.assertEqual(result, \"(Tensor x) -> Tensor\")\n+\n+        def foo_op_2(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n+            return x.clone() + y\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_2, mutates_args)\n+        self.assertEqual(result, \"(Tensor x, Tensor y) -> Tensor\")\n+\n+    def test_native_types(self):\n+        def foo_op(x: int) -> int:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op, mutates_args)\n+        self.assertEqual(result, \"(SymInt x) -> SymInt\")\n+\n+        def foo_op_2(x: bool) -> bool:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_2, mutates_args)\n+        self.assertEqual(result, \"(bool x) -> bool\")\n+\n+        def foo_op_3(x: str) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_3, mutates_args)\n+        self.assertEqual(result, \"(str x) -> SymInt\")\n+\n+        def foo_op_4(x: float) -> float:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_4, mutates_args)\n+        self.assertEqual(result, \"(float x) -> float\")\n+\n+    def test_torch_types(self):\n+        def foo_op_1(x: torch.types.Number) -> torch.types.Number:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_1, mutates_args)\n+        self.assertEqual(result, \"(Scalar x) -> Scalar\")\n+\n+        def foo_op_2(x: torch.dtype) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_2, mutates_args)\n+        self.assertEqual(result, \"(ScalarType x) -> SymInt\")\n+\n+        def foo_op_3(x: torch.device) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_3, mutates_args)\n+        self.assertEqual(result, \"(Device x) -> SymInt\")\n+\n+    def test_type_variants(self):\n+        def foo_op_1(x: typing.Optional[int]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_1, mutates_args)\n+        self.assertEqual(result, \"(SymInt? x) -> SymInt\")\n+\n+        def foo_op_2(x: typing.Sequence[int]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_2, mutates_args)\n+        self.assertEqual(result, \"(SymInt[] x) -> SymInt\")\n+\n+        def foo_op_3(x: typing.List[int]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_3, mutates_args)\n+        self.assertEqual(result, \"(SymInt[] x) -> SymInt\")\n+\n+        def foo_op_4(x: typing.Optional[typing.Sequence[int]]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_4, mutates_args)\n+        self.assertEqual(result, \"(SymInt[]? x) -> SymInt\")\n+\n+        def foo_op_5(x: typing.Optional[typing.List[int]]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_5, mutates_args)\n+        self.assertEqual(result, \"(SymInt[]? x) -> SymInt\")\n+\n+        def foo_op_6(x: typing.Union[int, float, bool]) -> types.Number:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_6, mutates_args)\n+        self.assertEqual(result, \"(Scalar x) -> Scalar\")\n+\n+        def foo_op_7(x: typing.Union[int, bool, float]) -> types.Number:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_7, mutates_args)\n+        self.assertEqual(result, \"(Scalar x) -> Scalar\")\n+\n+    def test_no_library_prefix(self):\n+        def foo_op(x: Tensor) -> Tensor:\n+            return x.clone()\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op, mutates_args)\n+        self.assertEqual(result, \"(Tensor x) -> Tensor\")\n+\n+        def foo_op_2(x: Tensor) -> torch.Tensor:\n+            return x.clone()\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_2, mutates_args)\n+        self.assertEqual(result, \"(Tensor x) -> Tensor\")\n+\n+        def foo_op_3(x: torch.Tensor) -> Tensor:\n+            return x.clone()\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_3, mutates_args)\n+        self.assertEqual(result, \"(Tensor x) -> Tensor\")\n+\n+        def foo_op_4(x: List[int]) -> types.Number:\n+            return x[0]\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_4, mutates_args)\n+        self.assertEqual(result, \"(SymInt[] x) -> Scalar\")\n+\n+        def foo_op_5(x: Optional[int]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_5, mutates_args)\n+        self.assertEqual(result, \"(SymInt? x) -> SymInt\")\n+\n+        def foo_op_6(x: Sequence[int]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_6, mutates_args)\n+        self.assertEqual(result, \"(SymInt[] x) -> SymInt\")\n+\n+        def foo_op_7(x: List[int]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_7, mutates_args)\n+        self.assertEqual(result, \"(SymInt[] x) -> SymInt\")\n+\n+        def foo_op_8(x: Optional[Sequence[int]]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_8, mutates_args)\n+        self.assertEqual(result, \"(SymInt[]? x) -> SymInt\")\n+\n+        def foo_op_9(x: Optional[List[int]]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_9, mutates_args)\n+        self.assertEqual(result, \"(SymInt[]? x) -> SymInt\")\n+\n+        def foo_op_10(x: Union[int, float, bool]) -> types.Number:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_10, mutates_args)\n+        self.assertEqual(result, \"(Scalar x) -> Scalar\")\n+\n+        def foo_op_11(x: Union[int, bool, float]) -> types.Number:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_11, mutates_args)\n+        self.assertEqual(result, \"(Scalar x) -> Scalar\")\n+\n+    def test_unsupported_annotation(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"Unsupported type annotation D. It is not a type.\",\n+        ):\n+\n+            def foo_op(x: D) -> Tensor:  # noqa: F821\n+                return torch.Tensor(x)\n+\n+            torch._custom_op.impl.infer_schema(foo_op, mutates_args)\n+\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"Unsupported type annotation E. It is not a type.\",\n+        ):\n+\n+            def foo_op_2(x: Tensor) -> E:  # noqa: F821\n+                return x\n+\n+            torch._custom_op.impl.infer_schema(foo_op_2, mutates_args)\n+\n+\n+if __name__ == \"__main__\":\n+    run_tests()\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+# Owner(s): [\"module: pt2-dispatcher\"]\n+from __future__ import annotations\n+\n+import typing\n+from typing import List, Optional, Sequence, Union  # noqa: F401\n+\n+import torch\n+import torch._custom_op.impl\n+from torch import Tensor, types\n+from torch.testing._internal.common_utils import run_tests, TestCase\n+\n+\n+mutates_args = {}\n+\n+\n+class TestInferSchemaWithAnnotation(TestCase):\n+    def test_tensor(self):\n+        def foo_op(x: torch.Tensor) -> torch.Tensor:\n+            return x.clone()\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op, mutates_args)\n+        self.assertEqual(result, \"(Tensor x) -> Tensor\")\n+\n+        def foo_op_2(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n+            return x.clone() + y\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_2, mutates_args)\n+        self.assertEqual(result, \"(Tensor x, Tensor y) -> Tensor\")\n+\n+    def test_native_types(self):\n+        def foo_op(x: int) -> int:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op, mutates_args)\n+        self.assertEqual(result, \"(SymInt x) -> SymInt\")\n+\n+        def foo_op_2(x: bool) -> bool:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_2, mutates_args)\n+        self.assertEqual(result, \"(bool x) -> bool\")\n+\n+        def foo_op_3(x: str) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_3, mutates_args)\n+        self.assertEqual(result, \"(str x) -> SymInt\")\n+\n+        def foo_op_4(x: float) -> float:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_4, mutates_args)\n+        self.assertEqual(result, \"(float x) -> float\")\n+\n+    def test_torch_types(self):\n+        def foo_op_1(x: torch.types.Number) -> torch.types.Number:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_1, mutates_args)\n+        self.assertEqual(result, \"(Scalar x) -> Scalar\")\n+\n+        def foo_op_2(x: torch.dtype) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_2, mutates_args)\n+        self.assertEqual(result, \"(ScalarType x) -> SymInt\")\n+\n+        def foo_op_3(x: torch.device) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_3, mutates_args)\n+        self.assertEqual(result, \"(Device x) -> SymInt\")\n+\n+    def test_type_variants(self):\n+        def foo_op_1(x: typing.Optional[int]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_1, mutates_args)\n+        self.assertEqual(result, \"(SymInt? x) -> SymInt\")\n+\n+        def foo_op_2(x: typing.Sequence[int]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_2, mutates_args)\n+        self.assertEqual(result, \"(SymInt[] x) -> SymInt\")\n+\n+        def foo_op_3(x: typing.List[int]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_3, mutates_args)\n+        self.assertEqual(result, \"(SymInt[] x) -> SymInt\")\n+\n+        def foo_op_4(x: typing.Optional[typing.Sequence[int]]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_4, mutates_args)\n+        self.assertEqual(result, \"(SymInt[]? x) -> SymInt\")\n+\n+        def foo_op_5(x: typing.Optional[typing.List[int]]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_5, mutates_args)\n+        self.assertEqual(result, \"(SymInt[]? x) -> SymInt\")\n+\n+        def foo_op_6(x: typing.Union[int, float, bool]) -> types.Number:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_6, mutates_args)\n+        self.assertEqual(result, \"(Scalar x) -> Scalar\")\n+\n+        def foo_op_7(x: typing.Union[int, bool, float]) -> types.Number:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_7, mutates_args)\n+        self.assertEqual(result, \"(Scalar x) -> Scalar\")\n+\n+    def test_no_library_prefix(self):\n+        def foo_op(x: Tensor) -> Tensor:\n+            return x.clone()\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op, mutates_args)\n+        self.assertEqual(result, \"(Tensor x) -> Tensor\")\n+\n+        def foo_op_2(x: Tensor) -> torch.Tensor:\n+            return x.clone()\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_2, mutates_args)\n+        self.assertEqual(result, \"(Tensor x) -> Tensor\")\n+\n+        def foo_op_3(x: torch.Tensor) -> Tensor:\n+            return x.clone()\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_3, mutates_args)\n+        self.assertEqual(result, \"(Tensor x) -> Tensor\")\n+\n+        def foo_op_4(x: List[int]) -> types.Number:\n+            return x[0]\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_4, mutates_args)\n+        self.assertEqual(result, \"(SymInt[] x) -> Scalar\")\n+\n+        def foo_op_5(x: Optional[int]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_5, mutates_args)\n+        self.assertEqual(result, \"(SymInt? x) -> SymInt\")\n+\n+        def foo_op_6(x: Sequence[int]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_6, mutates_args)\n+        self.assertEqual(result, \"(SymInt[] x) -> SymInt\")\n+\n+        def foo_op_7(x: List[int]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_7, mutates_args)\n+        self.assertEqual(result, \"(SymInt[] x) -> SymInt\")\n+\n+        def foo_op_8(x: Optional[Sequence[int]]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_8, mutates_args)\n+        self.assertEqual(result, \"(SymInt[]? x) -> SymInt\")\n+\n+        def foo_op_9(x: Optional[List[int]]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_9, mutates_args)\n+        self.assertEqual(result, \"(SymInt[]? x) -> SymInt\")\n+\n+        def foo_op_10(x: Union[int, float, bool]) -> types.Number:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_10, mutates_args)\n+        self.assertEqual(result, \"(Scalar x) -> Scalar\")\n+\n+        def foo_op_11(x: Union[int, bool, float]) -> types.Number:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_11, mutates_args)\n+        self.assertEqual(result, \"(Scalar x) -> Scalar\")\n+\n+    def test_unsupported_annotation(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"Unsupported type annotation D. It is not a type.\",\n+        ):\n+\n+            def foo_op(x: D) -> Tensor:  # noqa: F821\n+                return torch.Tensor(x)\n+\n+            torch._custom_op.impl.infer_schema(foo_op, mutates_args)\n+\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"Unsupported type annotation E. It is not a type.\",\n+        ):\n+\n+            def foo_op_2(x: Tensor) -> E:  # noqa: F821\n+                return x\n+\n+            torch._custom_op.impl.infer_schema(foo_op_2, mutates_args)\n+\n+\n+if __name__ == \"__main__\":\n+    run_tests()\n",
            "whole_hunk": "@@ -0,0 +1,207 @@\n+# Owner(s): [\"module: pt2-dispatcher\"]\n+from __future__ import annotations\n+\n+import typing\n+from typing import List, Optional, Sequence, Union  # noqa: F401\n+\n+import torch\n+import torch._custom_op.impl\n+from torch import Tensor, types\n+from torch.testing._internal.common_utils import run_tests, TestCase\n+\n+\n+mutates_args = {}\n+\n+\n+class TestInferSchemaWithAnnotation(TestCase):\n+    def test_tensor(self):\n+        def foo_op(x: torch.Tensor) -> torch.Tensor:\n+            return x.clone()\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op, mutates_args)\n+        self.assertEqual(result, \"(Tensor x) -> Tensor\")\n+\n+        def foo_op_2(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n+            return x.clone() + y\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_2, mutates_args)\n+        self.assertEqual(result, \"(Tensor x, Tensor y) -> Tensor\")\n+\n+    def test_native_types(self):\n+        def foo_op(x: int) -> int:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op, mutates_args)\n+        self.assertEqual(result, \"(SymInt x) -> SymInt\")\n+\n+        def foo_op_2(x: bool) -> bool:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_2, mutates_args)\n+        self.assertEqual(result, \"(bool x) -> bool\")\n+\n+        def foo_op_3(x: str) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_3, mutates_args)\n+        self.assertEqual(result, \"(str x) -> SymInt\")\n+\n+        def foo_op_4(x: float) -> float:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_4, mutates_args)\n+        self.assertEqual(result, \"(float x) -> float\")\n+\n+    def test_torch_types(self):\n+        def foo_op_1(x: torch.types.Number) -> torch.types.Number:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_1, mutates_args)\n+        self.assertEqual(result, \"(Scalar x) -> Scalar\")\n+\n+        def foo_op_2(x: torch.dtype) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_2, mutates_args)\n+        self.assertEqual(result, \"(ScalarType x) -> SymInt\")\n+\n+        def foo_op_3(x: torch.device) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_3, mutates_args)\n+        self.assertEqual(result, \"(Device x) -> SymInt\")\n+\n+    def test_type_variants(self):\n+        def foo_op_1(x: typing.Optional[int]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_1, mutates_args)\n+        self.assertEqual(result, \"(SymInt? x) -> SymInt\")\n+\n+        def foo_op_2(x: typing.Sequence[int]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_2, mutates_args)\n+        self.assertEqual(result, \"(SymInt[] x) -> SymInt\")\n+\n+        def foo_op_3(x: typing.List[int]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_3, mutates_args)\n+        self.assertEqual(result, \"(SymInt[] x) -> SymInt\")\n+\n+        def foo_op_4(x: typing.Optional[typing.Sequence[int]]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_4, mutates_args)\n+        self.assertEqual(result, \"(SymInt[]? x) -> SymInt\")\n+\n+        def foo_op_5(x: typing.Optional[typing.List[int]]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_5, mutates_args)\n+        self.assertEqual(result, \"(SymInt[]? x) -> SymInt\")\n+\n+        def foo_op_6(x: typing.Union[int, float, bool]) -> types.Number:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_6, mutates_args)\n+        self.assertEqual(result, \"(Scalar x) -> Scalar\")\n+\n+        def foo_op_7(x: typing.Union[int, bool, float]) -> types.Number:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_7, mutates_args)\n+        self.assertEqual(result, \"(Scalar x) -> Scalar\")\n+\n+    def test_no_library_prefix(self):\n+        def foo_op(x: Tensor) -> Tensor:\n+            return x.clone()\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op, mutates_args)\n+        self.assertEqual(result, \"(Tensor x) -> Tensor\")\n+\n+        def foo_op_2(x: Tensor) -> torch.Tensor:\n+            return x.clone()\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_2, mutates_args)\n+        self.assertEqual(result, \"(Tensor x) -> Tensor\")\n+\n+        def foo_op_3(x: torch.Tensor) -> Tensor:\n+            return x.clone()\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_3, mutates_args)\n+        self.assertEqual(result, \"(Tensor x) -> Tensor\")\n+\n+        def foo_op_4(x: List[int]) -> types.Number:\n+            return x[0]\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_4, mutates_args)\n+        self.assertEqual(result, \"(SymInt[] x) -> Scalar\")\n+\n+        def foo_op_5(x: Optional[int]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_5, mutates_args)\n+        self.assertEqual(result, \"(SymInt? x) -> SymInt\")\n+\n+        def foo_op_6(x: Sequence[int]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_6, mutates_args)\n+        self.assertEqual(result, \"(SymInt[] x) -> SymInt\")\n+\n+        def foo_op_7(x: List[int]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_7, mutates_args)\n+        self.assertEqual(result, \"(SymInt[] x) -> SymInt\")\n+\n+        def foo_op_8(x: Optional[Sequence[int]]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_8, mutates_args)\n+        self.assertEqual(result, \"(SymInt[]? x) -> SymInt\")\n+\n+        def foo_op_9(x: Optional[List[int]]) -> int:\n+            return 1\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_9, mutates_args)\n+        self.assertEqual(result, \"(SymInt[]? x) -> SymInt\")\n+\n+        def foo_op_10(x: Union[int, float, bool]) -> types.Number:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_10, mutates_args)\n+        self.assertEqual(result, \"(Scalar x) -> Scalar\")\n+\n+        def foo_op_11(x: Union[int, bool, float]) -> types.Number:\n+            return x\n+\n+        result = torch._custom_op.impl.infer_schema(foo_op_11, mutates_args)\n+        self.assertEqual(result, \"(Scalar x) -> Scalar\")\n+\n+    def test_unsupported_annotation(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"Unsupported type annotation D. It is not a type.\",\n+        ):\n+\n+            def foo_op(x: D) -> Tensor:  # noqa: F821\n+                return torch.Tensor(x)\n+\n+            torch._custom_op.impl.infer_schema(foo_op, mutates_args)\n+\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"Unsupported type annotation E. It is not a type.\",\n+        ):\n+\n+            def foo_op_2(x: Tensor) -> E:  # noqa: F821\n+                return x\n+\n+            torch._custom_op.impl.infer_schema(foo_op_2, mutates_args)\n+\n+\n+if __name__ == \"__main__\":\n+    run_tests()\n"
        },
        {
            "name": "infer_schema.py",
            "path": "torch/_library/infer_schema.py",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 7,
                    "new_start": 1,
                    "new_length": 9,
                    "hunk": "@@ -1,7 +1,9 @@\n # mypy: allow-untyped-defs\n import inspect\n import typing\n+from typing import List, Optional, Sequence, Union  # noqa: F401\n \n+import torch  # noqa: F401\n from .. import device, dtype, Tensor, types\n \n \n"
                },
                {
                    "old_start": 12,
                    "old_length": 6,
                    "new_start": 14,
                    "new_length": 9,
                    "hunk": "@@ -12,6 +14,9 @@ def infer_schema(prototype_function: typing.Callable, mutates_args=()) -> str:\n     write custom ops in real life:\n     - none of the outputs alias any of the inputs or each other.\n     - only the args listed in mutates_args are being mutated.\n+    - string type annotations \"device, dtype, Tensor, types\" without library specification\n+      are assumed to be torch.*. Similarly, string type annotations \"Optional, List, Sequence, Union\"\n+      without library specification are assumed to be typing.*.\n \n     Callers (e.g. the custom ops API) are responsible for checking these assumptions.\n     \"\"\"\n"
                },
                {
                    "old_start": 22,
                    "old_length": 6,
                    "new_start": 27,
                    "new_length": 14,
                    "hunk": "@@ -22,6 +27,14 @@ def infer_schema(prototype_function: typing.Callable, mutates_args=()) -> str:\n             f\"infer_schema(func): {what} \" f\"Got func with signature {sig})\"\n         )\n \n+    def convert_type_string(annotation_type: str):\n+        try:\n+            return eval(annotation_type)\n+        except Exception as e:\n+            error_fn(\n+                f\"Unsupported type annotation {annotation_type}. It is not a type.\"\n+            )\n+\n     params = []\n     seen_args = set()\n     saw_kwarg_only_arg = False\n"
                },
                {
                    "old_start": 38,
                    "old_length": 13,
                    "new_start": 51,
                    "new_length": 19,
                    "hunk": "@@ -38,13 +51,19 @@ def infer_schema(prototype_function: typing.Callable, mutates_args=()) -> str:\n         if param.annotation is inspect.Parameter.empty:\n             error_fn(f\"Parameter {name} must have a type annotation.\")\n \n-        if param.annotation not in SUPPORTED_PARAM_TYPES.keys():\n+        # The annotation might be converted to a string by annotation,\n+        # we convert it to the actual type.\n+        annotation_type = param.annotation\n+        if type(annotation_type) == str:\n+            annotation_type = convert_type_string(annotation_type)\n+\n+        if annotation_type not in SUPPORTED_PARAM_TYPES.keys():\n             error_fn(\n                 f\"Parameter {name} has unsupported type {param.annotation}. \"\n                 f\"The valid types are: {SUPPORTED_PARAM_TYPES.keys()}.\"\n             )\n \n-        schema_type = SUPPORTED_PARAM_TYPES[param.annotation]\n+        schema_type = SUPPORTED_PARAM_TYPES[annotation_type]\n         if name in mutates_args:\n             if not schema_type.startswith(\"Tensor\"):\n                 error_fn(\n"
                },
                {
                    "old_start": 72,
                    "old_length": 7,
                    "new_start": 91,
                    "new_length": 10,
                    "hunk": "@@ -72,7 +91,10 @@ def infer_schema(prototype_function: typing.Callable, mutates_args=()) -> str:\n             f\"mutates_args should contain the names of all args that the \"\n             f\"custom op mutates.\"\n         )\n-    ret = parse_return(sig.return_annotation, error_fn)\n+    return_annotation = sig.return_annotation\n+    if type(return_annotation) == str:\n+        return_annotation = convert_type_string(return_annotation)\n+    ret = parse_return(return_annotation, error_fn)\n     return f\"({', '.join(params)}) -> {ret}\"\n \n "
                }
            ],
            "whole_deleted": "-        if param.annotation not in SUPPORTED_PARAM_TYPES.keys():\n-        schema_type = SUPPORTED_PARAM_TYPES[param.annotation]\n-    ret = parse_return(sig.return_annotation, error_fn)\n",
            "whole_added": "+from typing import List, Optional, Sequence, Union  # noqa: F401\n+import torch  # noqa: F401\n+    - string type annotations \"device, dtype, Tensor, types\" without library specification\n+      are assumed to be torch.*. Similarly, string type annotations \"Optional, List, Sequence, Union\"\n+      without library specification are assumed to be typing.*.\n+    def convert_type_string(annotation_type: str):\n+        try:\n+            return eval(annotation_type)\n+        except Exception as e:\n+            error_fn(\n+                f\"Unsupported type annotation {annotation_type}. It is not a type.\"\n+            )\n+\n+        # The annotation might be converted to a string by annotation,\n+        # we convert it to the actual type.\n+        annotation_type = param.annotation\n+        if type(annotation_type) == str:\n+            annotation_type = convert_type_string(annotation_type)\n+\n+        if annotation_type not in SUPPORTED_PARAM_TYPES.keys():\n+        schema_type = SUPPORTED_PARAM_TYPES[annotation_type]\n+    return_annotation = sig.return_annotation\n+    if type(return_annotation) == str:\n+        return_annotation = convert_type_string(return_annotation)\n+    ret = parse_return(return_annotation, error_fn)\n",
            "whole_hunk": "@@ -1,7 +1,9 @@\n # mypy: allow-untyped-defs\n import inspect\n import typing\n+from typing import List, Optional, Sequence, Union  # noqa: F401\n \n+import torch  # noqa: F401\n from .. import device, dtype, Tensor, types\n \n \n@@ -12,6 +14,9 @@ def infer_schema(prototype_function: typing.Callable, mutates_args=()) -> str:\n     write custom ops in real life:\n     - none of the outputs alias any of the inputs or each other.\n     - only the args listed in mutates_args are being mutated.\n+    - string type annotations \"device, dtype, Tensor, types\" without library specification\n+      are assumed to be torch.*. Similarly, string type annotations \"Optional, List, Sequence, Union\"\n+      without library specification are assumed to be typing.*.\n \n     Callers (e.g. the custom ops API) are responsible for checking these assumptions.\n     \"\"\"\n@@ -22,6 +27,14 @@ def infer_schema(prototype_function: typing.Callable, mutates_args=()) -> str:\n             f\"infer_schema(func): {what} \" f\"Got func with signature {sig})\"\n         )\n \n+    def convert_type_string(annotation_type: str):\n+        try:\n+            return eval(annotation_type)\n+        except Exception as e:\n+            error_fn(\n+                f\"Unsupported type annotation {annotation_type}. It is not a type.\"\n+            )\n+\n     params = []\n     seen_args = set()\n     saw_kwarg_only_arg = False\n@@ -38,13 +51,19 @@ def infer_schema(prototype_function: typing.Callable, mutates_args=()) -> str:\n         if param.annotation is inspect.Parameter.empty:\n             error_fn(f\"Parameter {name} must have a type annotation.\")\n \n-        if param.annotation not in SUPPORTED_PARAM_TYPES.keys():\n+        # The annotation might be converted to a string by annotation,\n+        # we convert it to the actual type.\n+        annotation_type = param.annotation\n+        if type(annotation_type) == str:\n+            annotation_type = convert_type_string(annotation_type)\n+\n+        if annotation_type not in SUPPORTED_PARAM_TYPES.keys():\n             error_fn(\n                 f\"Parameter {name} has unsupported type {param.annotation}. \"\n                 f\"The valid types are: {SUPPORTED_PARAM_TYPES.keys()}.\"\n             )\n \n-        schema_type = SUPPORTED_PARAM_TYPES[param.annotation]\n+        schema_type = SUPPORTED_PARAM_TYPES[annotation_type]\n         if name in mutates_args:\n             if not schema_type.startswith(\"Tensor\"):\n                 error_fn(\n@@ -72,7 +91,10 @@ def infer_schema(prototype_function: typing.Callable, mutates_args=()) -> str:\n             f\"mutates_args should contain the names of all args that the \"\n             f\"custom op mutates.\"\n         )\n-    ret = parse_return(sig.return_annotation, error_fn)\n+    return_annotation = sig.return_annotation\n+    if type(return_annotation) == str:\n+        return_annotation = convert_type_string(return_annotation)\n+    ret = parse_return(return_annotation, error_fn)\n     return f\"({', '.join(params)}) -> {ret}\"\n \n "
        }
    ]
},
{
    "Id": 479,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/ac768333be2ad7cd3435c1ae257bdaf297595714",
    "date": "2023-10-13T18:36:13+00:00",
    "message": "[dynamo] fix prim lowering validation logic for dynamic shape args (#111208)\n\nFixes https://github.com/pytorch/pytorch/issues/111199\n\nFixes https://github.com/pytorch/pytorch/issues/111203\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111208\nApproved by: https://github.com/ezyang",
    "label": "YES",
    "changes": [
        {
            "name": "test_repros.py",
            "path": "test/dynamo/test_repros.py",
            "patches": [
                {
                    "old_start": 3516,
                    "old_length": 6,
                    "new_start": 3516,
                    "new_length": 33,
                    "hunk": "@@ -3516,6 +3516,33 @@ class ReproTests(torch._dynamo.test_case.TestCase):\n         x = torch.rand(4)\n         self.assertTrue(same(fn(x), opt_fn(x)))\n \n+    def test_add_sub_alpha_out(self):\n+        inp = torch.randn(2, 3, 4)\n+        other = 1\n+        alpha = 2\n+        for op in [torch.add, torch.sub]:\n+            out = torch.zeros(2, 3, 4)\n+            compile_out = torch.zeros(2, 3, 4)\n+            op(inp, other, alpha=alpha, out=out)\n+            compiled_fn = torch.compile(op, dynamic=True)\n+            compiled_fn(inp, other, alpha=alpha, out=compile_out)\n+            self.assertTrue(same(out, compile_out))\n+\n+    def test_addr_alpha_beta_out(self):\n+        inp = torch.randn(2, 3)\n+        vec1 = torch.randn(2)\n+        vec2 = torch.randn(3)\n+        alpha = 2\n+        beta = 5\n+\n+        out = torch.zeros(2, 3)\n+        compile_out = torch.zeros(2, 3)\n+\n+        torch.addr(inp, vec1, vec2, alpha=alpha, beta=beta, out=out)\n+        compiled_fn = torch.compile(torch.addr, dynamic=True)\n+        compiled_fn(inp, vec1, vec2, alpha=alpha, beta=beta, out=compile_out)\n+        self.assertTrue(same(out, compile_out))\n+\n \n if __name__ == \"__main__\":\n     from torch._dynamo.test_case import run_tests\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    def test_add_sub_alpha_out(self):\n+        inp = torch.randn(2, 3, 4)\n+        other = 1\n+        alpha = 2\n+        for op in [torch.add, torch.sub]:\n+            out = torch.zeros(2, 3, 4)\n+            compile_out = torch.zeros(2, 3, 4)\n+            op(inp, other, alpha=alpha, out=out)\n+            compiled_fn = torch.compile(op, dynamic=True)\n+            compiled_fn(inp, other, alpha=alpha, out=compile_out)\n+            self.assertTrue(same(out, compile_out))\n+\n+    def test_addr_alpha_beta_out(self):\n+        inp = torch.randn(2, 3)\n+        vec1 = torch.randn(2)\n+        vec2 = torch.randn(3)\n+        alpha = 2\n+        beta = 5\n+\n+        out = torch.zeros(2, 3)\n+        compile_out = torch.zeros(2, 3)\n+\n+        torch.addr(inp, vec1, vec2, alpha=alpha, beta=beta, out=out)\n+        compiled_fn = torch.compile(torch.addr, dynamic=True)\n+        compiled_fn(inp, vec1, vec2, alpha=alpha, beta=beta, out=compile_out)\n+        self.assertTrue(same(out, compile_out))\n+\n",
            "whole_hunk": "@@ -3516,6 +3516,33 @@ class ReproTests(torch._dynamo.test_case.TestCase):\n         x = torch.rand(4)\n         self.assertTrue(same(fn(x), opt_fn(x)))\n \n+    def test_add_sub_alpha_out(self):\n+        inp = torch.randn(2, 3, 4)\n+        other = 1\n+        alpha = 2\n+        for op in [torch.add, torch.sub]:\n+            out = torch.zeros(2, 3, 4)\n+            compile_out = torch.zeros(2, 3, 4)\n+            op(inp, other, alpha=alpha, out=out)\n+            compiled_fn = torch.compile(op, dynamic=True)\n+            compiled_fn(inp, other, alpha=alpha, out=compile_out)\n+            self.assertTrue(same(out, compile_out))\n+\n+    def test_addr_alpha_beta_out(self):\n+        inp = torch.randn(2, 3)\n+        vec1 = torch.randn(2)\n+        vec2 = torch.randn(3)\n+        alpha = 2\n+        beta = 5\n+\n+        out = torch.zeros(2, 3)\n+        compile_out = torch.zeros(2, 3)\n+\n+        torch.addr(inp, vec1, vec2, alpha=alpha, beta=beta, out=out)\n+        compiled_fn = torch.compile(torch.addr, dynamic=True)\n+        compiled_fn(inp, vec1, vec2, alpha=alpha, beta=beta, out=compile_out)\n+        self.assertTrue(same(out, compile_out))\n+\n \n if __name__ == \"__main__\":\n     from torch._dynamo.test_case import run_tests\n"
        },
        {
            "name": "__init__.py",
            "path": "torch/_prims_common/__init__.py",
            "patches": [
                {
                    "old_start": 95,
                    "old_length": 6,
                    "new_start": 95,
                    "new_length": 17,
                    "hunk": "@@ -95,6 +95,17 @@ def same_shape(a: ShapeType, b: ShapeType, *, allow_rhs_unbacked=False) -> bool:\n     return True\n \n \n+def _maybe_get_pytype(t):\n+    if t is torch.SymFloat:\n+        return float\n+    elif t is torch.SymInt:\n+        return int\n+    elif t is torch.SymBool:\n+        return bool\n+    else:\n+        return t\n+\n+\n # TODO: look at using torch.testing.assert_close instead with an option\n #   to just compare metadata\n def compare_tensor_meta(\n"
                },
                {
                    "old_start": 1003,
                    "old_length": 9,
                    "new_start": 1014,
                    "new_length": 10,
                    "hunk": "@@ -1003,9 +1014,10 @@ def get_higher_type(a: type, b: type) -> type:\n \n     The types are ordered bool -> int -> float -> complex.\n     \"\"\"\n+    a, b = _maybe_get_pytype(a), _maybe_get_pytype(b)\n     # Type checking\n-    assert a in _ordered_types\n-    assert b in _ordered_types\n+    if a not in _ordered_types or b not in _ordered_types:\n+        raise RuntimeError(f\"Expected builtin numeric types, found {a}, {b}\")\n \n     if a is b:\n         return a\n"
                },
                {
                    "old_start": 1104,
                    "old_length": 17,
                    "new_start": 1116,
                    "new_length": 13,
                    "hunk": "@@ -1104,17 +1116,13 @@ def is_weakly_lesser_type(a: type, b: type) -> bool:\n \n     The comparison is determined by the following type ordering: bool, int, float, complex.\n     \"\"\"\n-    ordered_types = (\n-        bool,\n-        int,\n-        float,\n-        complex,\n-    )\n \n-    assert a in ordered_types\n-    assert b in ordered_types\n+    a, b = _maybe_get_pytype(a), _maybe_get_pytype(b)\n \n-    for typ in ordered_types:\n+    if a not in _ordered_types or b not in _ordered_types:\n+        raise RuntimeError(f\"Expected builtin numeric types, found {a}, {b}\")\n+\n+    for typ in _ordered_types:\n         if a == typ:\n             return True\n         if b == typ:"
                }
            ],
            "whole_deleted": "-    assert a in _ordered_types\n-    assert b in _ordered_types\n-    ordered_types = (\n-        bool,\n-        int,\n-        float,\n-        complex,\n-    )\n-    assert a in ordered_types\n-    assert b in ordered_types\n-    for typ in ordered_types:\n",
            "whole_added": "+def _maybe_get_pytype(t):\n+    if t is torch.SymFloat:\n+        return float\n+    elif t is torch.SymInt:\n+        return int\n+    elif t is torch.SymBool:\n+        return bool\n+    else:\n+        return t\n+\n+\n+    a, b = _maybe_get_pytype(a), _maybe_get_pytype(b)\n+    if a not in _ordered_types or b not in _ordered_types:\n+        raise RuntimeError(f\"Expected builtin numeric types, found {a}, {b}\")\n+    a, b = _maybe_get_pytype(a), _maybe_get_pytype(b)\n+    if a not in _ordered_types or b not in _ordered_types:\n+        raise RuntimeError(f\"Expected builtin numeric types, found {a}, {b}\")\n+\n+    for typ in _ordered_types:\n",
            "whole_hunk": "@@ -95,6 +95,17 @@ def same_shape(a: ShapeType, b: ShapeType, *, allow_rhs_unbacked=False) -> bool:\n     return True\n \n \n+def _maybe_get_pytype(t):\n+    if t is torch.SymFloat:\n+        return float\n+    elif t is torch.SymInt:\n+        return int\n+    elif t is torch.SymBool:\n+        return bool\n+    else:\n+        return t\n+\n+\n # TODO: look at using torch.testing.assert_close instead with an option\n #   to just compare metadata\n def compare_tensor_meta(\n@@ -1003,9 +1014,10 @@ def get_higher_type(a: type, b: type) -> type:\n \n     The types are ordered bool -> int -> float -> complex.\n     \"\"\"\n+    a, b = _maybe_get_pytype(a), _maybe_get_pytype(b)\n     # Type checking\n-    assert a in _ordered_types\n-    assert b in _ordered_types\n+    if a not in _ordered_types or b not in _ordered_types:\n+        raise RuntimeError(f\"Expected builtin numeric types, found {a}, {b}\")\n \n     if a is b:\n         return a\n@@ -1104,17 +1116,13 @@ def is_weakly_lesser_type(a: type, b: type) -> bool:\n \n     The comparison is determined by the following type ordering: bool, int, float, complex.\n     \"\"\"\n-    ordered_types = (\n-        bool,\n-        int,\n-        float,\n-        complex,\n-    )\n \n-    assert a in ordered_types\n-    assert b in ordered_types\n+    a, b = _maybe_get_pytype(a), _maybe_get_pytype(b)\n \n-    for typ in ordered_types:\n+    if a not in _ordered_types or b not in _ordered_types:\n+        raise RuntimeError(f\"Expected builtin numeric types, found {a}, {b}\")\n+\n+    for typ in _ordered_types:\n         if a == typ:\n             return True\n         if b == typ:"
        }
    ]
},
{
    "Id": 109,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/6d4ec9b2ecba7d26e885bdbb0faeeaa1e148cfd6",
    "date": "2024-06-03T21:21:55+00:00",
    "message": "[RFC] Introduce Checkpointable for DCP (#127540) (#127628)\n\nSummary:\n# Introduce Checkpointable interface for DCP to support arbitrary tensor subclasses for checkpointing\n\n**Authors:**\n* zainhuda\n\n## **Summary**\nThis diff adds a CheckpointableTensor interface to allow for future compatibility for any tensor subclass with DCP in a clean and maintainable way.\n\n## **Motivation**\nFor TorchRec sharding migration from ShardedTensor to DTensor, we create a tensor subclass that is stored by DTensor to support TorchRec's sharding schemes (ex, empty shards, multiple shards on a rank).\n\n## **Proposed Implementation**\nView the CheckpointableTensor interface implementation, in which, we introduce the minimal set of methods needed to be compatible with DCP. These methods are expected to implemented by any tensor subclasses and as such are then checkpointable by DCP.\n\n## **Drawbacks**\nNo drawbacks, it extends functionality in a clean and maintainable way.\n\n## **Alternatives**\nAlternative design was creating paths for checking for certain attributes in tensor subclasses which can get messy and hard to maintain/understand why it was there in the first place.\n\nTest Plan:\nSandcastle\n\ncc mrshenli pritamdamania87 zhaojuanmao satgera gqchen aazzolini osalpekar jiayisuse H-Huang kwen2501 awgu penguinwu fegin XilunWu wanchaol fduwjj wz337 tianyu-l wconstab yf225 chauhang d4l3k LucasLLC\n\nDifferential Revision: D57970603\n\nPulled By: iamzainhuda\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/127628\nApproved by: https://github.com/wz337, https://github.com/XilunWu, https://github.com/fegin",
    "label": "NO",
    "changes": [
        {
            "name": "planner.py",
            "path": "torch/distributed/checkpoint/planner.py",
            "patches": [
                {
                    "old_start": 426,
                    "old_length": 3,
                    "new_start": 426,
                    "new_length": 39,
                    "hunk": "@@ -426,3 +426,39 @@ class LoadPlanner:\n         The contents of tensor will follow its device synchronization model.\n         \"\"\"\n         pass\n+\n+\n+class _Checkpointable:\n+    \"\"\"\n+    Interface for checkpointable objects.\n+    This is to allow arbitrary objects/tensor subclasses to hook into DCP seamlessly through implementing the interface.\n+    \"\"\"\n+\n+    @abc.abstractmethod\n+    def _create_write_items(self, fqn: str, object: Any) -> List[WriteItem]:\n+        \"\"\"\n+        Return a list of WriteItems based on object's contents.\n+        \"\"\"\n+        raise NotImplementedError(\n+            \"_Checkpointable._create_write_items is not implemented\"\n+        )\n+\n+    @abc.abstractmethod\n+    def _create_chunk_list(self, tensor: torch.Tensor) -> List[ChunkStorageMetadata]:\n+        \"\"\"\n+        Return a list of `ChunkStorageMetadata` based on object's contents.\n+        \"\"\"\n+        raise NotImplementedError(\n+            \"_Checkpointable._create_chunk_list is not implemented\"\n+        )\n+\n+    @abc.abstractmethod\n+    def _get_tensor_shard(\n+        self, tensor: torch.Tensor, index: MetadataIndex\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Return a 'torch.Tensor' shard based on 'MetadataIndex'.\n+        \"\"\"\n+        raise NotImplementedError(\n+            \"_Checkpointable._get_tensor_shard is not implemented\"\n+        )\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+\n+\n+class _Checkpointable:\n+    \"\"\"\n+    Interface for checkpointable objects.\n+    This is to allow arbitrary objects/tensor subclasses to hook into DCP seamlessly through implementing the interface.\n+    \"\"\"\n+\n+    @abc.abstractmethod\n+    def _create_write_items(self, fqn: str, object: Any) -> List[WriteItem]:\n+        \"\"\"\n+        Return a list of WriteItems based on object's contents.\n+        \"\"\"\n+        raise NotImplementedError(\n+            \"_Checkpointable._create_write_items is not implemented\"\n+        )\n+\n+    @abc.abstractmethod\n+    def _create_chunk_list(self, tensor: torch.Tensor) -> List[ChunkStorageMetadata]:\n+        \"\"\"\n+        Return a list of `ChunkStorageMetadata` based on object's contents.\n+        \"\"\"\n+        raise NotImplementedError(\n+            \"_Checkpointable._create_chunk_list is not implemented\"\n+        )\n+\n+    @abc.abstractmethod\n+    def _get_tensor_shard(\n+        self, tensor: torch.Tensor, index: MetadataIndex\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Return a 'torch.Tensor' shard based on 'MetadataIndex'.\n+        \"\"\"\n+        raise NotImplementedError(\n+            \"_Checkpointable._get_tensor_shard is not implemented\"\n+        )\n",
            "whole_hunk": "@@ -426,3 +426,39 @@ class LoadPlanner:\n         The contents of tensor will follow its device synchronization model.\n         \"\"\"\n         pass\n+\n+\n+class _Checkpointable:\n+    \"\"\"\n+    Interface for checkpointable objects.\n+    This is to allow arbitrary objects/tensor subclasses to hook into DCP seamlessly through implementing the interface.\n+    \"\"\"\n+\n+    @abc.abstractmethod\n+    def _create_write_items(self, fqn: str, object: Any) -> List[WriteItem]:\n+        \"\"\"\n+        Return a list of WriteItems based on object's contents.\n+        \"\"\"\n+        raise NotImplementedError(\n+            \"_Checkpointable._create_write_items is not implemented\"\n+        )\n+\n+    @abc.abstractmethod\n+    def _create_chunk_list(self, tensor: torch.Tensor) -> List[ChunkStorageMetadata]:\n+        \"\"\"\n+        Return a list of `ChunkStorageMetadata` based on object's contents.\n+        \"\"\"\n+        raise NotImplementedError(\n+            \"_Checkpointable._create_chunk_list is not implemented\"\n+        )\n+\n+    @abc.abstractmethod\n+    def _get_tensor_shard(\n+        self, tensor: torch.Tensor, index: MetadataIndex\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Return a 'torch.Tensor' shard based on 'MetadataIndex'.\n+        \"\"\"\n+        raise NotImplementedError(\n+            \"_Checkpointable._get_tensor_shard is not implemented\"\n+        )\n"
        },
        {
            "name": "planner_helpers.py",
            "path": "torch/distributed/checkpoint/planner_helpers.py",
            "patches": [
                {
                    "old_start": 8,
                    "old_length": 6,
                    "new_start": 8,
                    "new_length": 7,
                    "hunk": "@@ -8,6 +8,7 @@ from torch.distributed._shard.metadata import ShardMetadata\n from torch.distributed._shard.sharded_tensor import ShardedTensor\n from torch.distributed._tensor import DTensor\n from torch.distributed._tensor._utils import compute_local_shape_and_global_offset\n+from torch.distributed.checkpoint.planner import _Checkpointable\n \n from torch.utils._pytree import tree_map_only\n \n"
                },
                {
                    "old_start": 217,
                    "old_length": 7,
                    "new_start": 218,
                    "new_length": 12,
                    "hunk": "@@ -217,7 +218,12 @@ def _create_default_metadata_only_plan(state_dict: STATE_DICT_TYPE) -> SavePlan:\n \n \n def _create_write_items(fqn: str, object: Any) -> List[WriteItem]:\n-    if isinstance(object, DTensor):\n+    if isinstance(object, _Checkpointable):\n+        return object._create_write_items(fqn, object)\n+    elif isinstance(object, DTensor):\n+        # DTensor can contain a local tensor that is a tensor subclass\n+        if isinstance(object.to_local(), _Checkpointable):\n+            return object.to_local()._create_write_items(fqn, object)  # type: ignore[arg-type]\n         return [_create_write_items_for_dtensor(fqn, object)]\n     elif isinstance(object, ShardedTensor):\n         return [\n"
                },
                {
                    "old_start": 242,
                    "old_length": 7,
                    "new_start": 248,
                    "new_length": 12,
                    "hunk": "@@ -242,7 +248,12 @@ def _create_chunk_from_dtensor(tensor: DTensor) -> ChunkStorageMetadata:\n \n \n def _create_chunk_list(tensor: torch.Tensor) -> List[ChunkStorageMetadata]:\n-    if isinstance(tensor, DTensor):\n+    if isinstance(tensor, _Checkpointable):\n+        local_chunks = tensor._create_chunk_list(tensor)\n+    elif isinstance(tensor, DTensor):\n+        # DTensor can contain a local tensor that is a tensor subclass\n+        if isinstance(tensor.to_local(), _Checkpointable):\n+            return tensor.to_local()._create_chunk_list(tensor)  # type: ignore[arg-type]\n         local_chunks = [_create_chunk_from_dtensor(tensor)]\n     elif isinstance(tensor, ShardedTensor):\n         local_chunks = [\n"
                }
            ],
            "whole_deleted": "-    if isinstance(object, DTensor):\n-    if isinstance(tensor, DTensor):\n",
            "whole_added": "+from torch.distributed.checkpoint.planner import _Checkpointable\n+    if isinstance(object, _Checkpointable):\n+        return object._create_write_items(fqn, object)\n+    elif isinstance(object, DTensor):\n+        # DTensor can contain a local tensor that is a tensor subclass\n+        if isinstance(object.to_local(), _Checkpointable):\n+            return object.to_local()._create_write_items(fqn, object)  # type: ignore[arg-type]\n+    if isinstance(tensor, _Checkpointable):\n+        local_chunks = tensor._create_chunk_list(tensor)\n+    elif isinstance(tensor, DTensor):\n+        # DTensor can contain a local tensor that is a tensor subclass\n+        if isinstance(tensor.to_local(), _Checkpointable):\n+            return tensor.to_local()._create_chunk_list(tensor)  # type: ignore[arg-type]\n",
            "whole_hunk": "@@ -8,6 +8,7 @@ from torch.distributed._shard.metadata import ShardMetadata\n from torch.distributed._shard.sharded_tensor import ShardedTensor\n from torch.distributed._tensor import DTensor\n from torch.distributed._tensor._utils import compute_local_shape_and_global_offset\n+from torch.distributed.checkpoint.planner import _Checkpointable\n \n from torch.utils._pytree import tree_map_only\n \n@@ -217,7 +218,12 @@ def _create_default_metadata_only_plan(state_dict: STATE_DICT_TYPE) -> SavePlan:\n \n \n def _create_write_items(fqn: str, object: Any) -> List[WriteItem]:\n-    if isinstance(object, DTensor):\n+    if isinstance(object, _Checkpointable):\n+        return object._create_write_items(fqn, object)\n+    elif isinstance(object, DTensor):\n+        # DTensor can contain a local tensor that is a tensor subclass\n+        if isinstance(object.to_local(), _Checkpointable):\n+            return object.to_local()._create_write_items(fqn, object)  # type: ignore[arg-type]\n         return [_create_write_items_for_dtensor(fqn, object)]\n     elif isinstance(object, ShardedTensor):\n         return [\n@@ -242,7 +248,12 @@ def _create_chunk_from_dtensor(tensor: DTensor) -> ChunkStorageMetadata:\n \n \n def _create_chunk_list(tensor: torch.Tensor) -> List[ChunkStorageMetadata]:\n-    if isinstance(tensor, DTensor):\n+    if isinstance(tensor, _Checkpointable):\n+        local_chunks = tensor._create_chunk_list(tensor)\n+    elif isinstance(tensor, DTensor):\n+        # DTensor can contain a local tensor that is a tensor subclass\n+        if isinstance(tensor.to_local(), _Checkpointable):\n+            return tensor.to_local()._create_chunk_list(tensor)  # type: ignore[arg-type]\n         local_chunks = [_create_chunk_from_dtensor(tensor)]\n     elif isinstance(tensor, ShardedTensor):\n         local_chunks = [\n"
        },
        {
            "name": "utils.py",
            "path": "torch/distributed/checkpoint/utils.py",
            "patches": [
                {
                    "old_start": 14,
                    "old_length": 6,
                    "new_start": 14,
                    "new_length": 7,
                    "hunk": "@@ -14,6 +14,7 @@ import torch.distributed as dist\n from torch.distributed._shard.sharded_tensor import ShardedTensor\n from torch.distributed._shard.sharded_tensor.shard import Shard\n from torch.distributed._tensor import DTensor\n+from torch.distributed.checkpoint.planner import _Checkpointable\n \n from .api import (\n     _is_wrapped_exception,\n"
                },
                {
                    "old_start": 301,
                    "old_length": 7,
                    "new_start": 302,
                    "new_length": 12,
                    "hunk": "@@ -301,7 +302,12 @@ def _find_shard(tensor: ShardedTensor, index: MetadataIndex) -> Shard:\n \n \n def find_tensor_shard(tensor: torch.Tensor, index: MetadataIndex) -> torch.Tensor:\n-    if isinstance(tensor, DTensor):\n+    if isinstance(tensor, _Checkpointable):\n+        return tensor._get_tensor_shard(tensor, index)\n+    elif isinstance(tensor, DTensor):\n+        # DTensor can contain a local tensor that is a tensor subclass\n+        if isinstance(tensor.to_local(), _Checkpointable):\n+            return tensor.to_local()._get_tensor_shard(tensor, index)  # type: ignore[arg-type]\n         return tensor.to_local()\n     if isinstance(tensor, ShardedTensor):\n         return _find_shard(tensor, index).tensor"
                }
            ],
            "whole_deleted": "-    if isinstance(tensor, DTensor):\n",
            "whole_added": "+from torch.distributed.checkpoint.planner import _Checkpointable\n+    if isinstance(tensor, _Checkpointable):\n+        return tensor._get_tensor_shard(tensor, index)\n+    elif isinstance(tensor, DTensor):\n+        # DTensor can contain a local tensor that is a tensor subclass\n+        if isinstance(tensor.to_local(), _Checkpointable):\n+            return tensor.to_local()._get_tensor_shard(tensor, index)  # type: ignore[arg-type]\n",
            "whole_hunk": "@@ -14,6 +14,7 @@ import torch.distributed as dist\n from torch.distributed._shard.sharded_tensor import ShardedTensor\n from torch.distributed._shard.sharded_tensor.shard import Shard\n from torch.distributed._tensor import DTensor\n+from torch.distributed.checkpoint.planner import _Checkpointable\n \n from .api import (\n     _is_wrapped_exception,\n@@ -301,7 +302,12 @@ def _find_shard(tensor: ShardedTensor, index: MetadataIndex) -> Shard:\n \n \n def find_tensor_shard(tensor: torch.Tensor, index: MetadataIndex) -> torch.Tensor:\n-    if isinstance(tensor, DTensor):\n+    if isinstance(tensor, _Checkpointable):\n+        return tensor._get_tensor_shard(tensor, index)\n+    elif isinstance(tensor, DTensor):\n+        # DTensor can contain a local tensor that is a tensor subclass\n+        if isinstance(tensor.to_local(), _Checkpointable):\n+            return tensor.to_local()._get_tensor_shard(tensor, index)  # type: ignore[arg-type]\n         return tensor.to_local()\n     if isinstance(tensor, ShardedTensor):\n         return _find_shard(tensor, index).tensor"
        }
    ]
},
{
    "Id": 473,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/ff432c048d353ff6944ccf50591c171372a924f7",
    "date": "2023-10-16T18:31:38+00:00",
    "message": "[easy] Remove duplicate exprs in produce_guards (#111270)\n\nSummary: We're checking the original guard.expr in the issued set instead of the simplified expr, leading to duplicate guards in cases where one expression simplifies to another.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111270\nApproved by: https://github.com/Chillee, https://github.com/ezyang",
    "label": "YES",
    "changes": [
        {
            "name": "symbolic_shapes.py",
            "path": "torch/fx/experimental/symbolic_shapes.py",
            "patches": [
                {
                    "old_start": 3431,
                    "old_length": 7,
                    "new_start": 3431,
                    "new_length": 7,
                    "hunk": "@@ -3431,7 +3431,7 @@ class ShapeEnv:\n             expr = self.simplify(guard.expr)\n \n             # Avoid re-issueing the same guard.\n-            if guard.expr in issued:\n+            if expr in issued:\n                 return\n \n             issued.add(expr)"
                }
            ],
            "whole_deleted": "-            if guard.expr in issued:\n",
            "whole_added": "+            if expr in issued:\n",
            "whole_hunk": "@@ -3431,7 +3431,7 @@ class ShapeEnv:\n             expr = self.simplify(guard.expr)\n \n             # Avoid re-issueing the same guard.\n-            if guard.expr in issued:\n+            if expr in issued:\n                 return\n \n             issued.add(expr)"
        }
    ]
},
{
    "Id": 490,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/c36b31d5302d31746f3f3bd64ed8d9acd8e36155",
    "date": "2023-10-05T05:35:47+00:00",
    "message": "`torch::nn::AdaptiveLogSoftmaxWithLoss`: check length of `cutoffs` (#106777)\n\nFixes #106698\n\nAlso added a check for python API, because current error message\n```\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/sehoon/pytorch-latest/torch/nn/modules/adaptive.py\", line 128, in __init__\n    or (min(cutoffs) <= 0) \\\nValueError: min() arg is an empty sequence\n```\nis not very comprehensible.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/106777\nApproved by: https://github.com/albanD",
    "label": "YES",
    "changes": [
        {
            "name": "adaptive.cpp",
            "path": "torch/csrc/api/src/nn/modules/adaptive.cpp",
            "patches": [
                {
                    "old_start": 24,
                    "old_length": 6,
                    "new_start": 24,
                    "new_length": 9,
                    "hunk": "@@ -24,6 +24,9 @@ AdaptiveLogSoftmaxWithLossImpl::AdaptiveLogSoftmaxWithLossImpl(\n }\n \n void AdaptiveLogSoftmaxWithLossImpl::reset() {\n+  TORCH_CHECK(\n+      options.cutoffs().size() > 0,\n+      \"cutoffs should be a sequence of length larger than 0\");\n   TORCH_CHECK(\n       std::is_sorted(options.cutoffs().begin(), options.cutoffs().end()) &&\n           *std::min_element(\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  TORCH_CHECK(\n+      options.cutoffs().size() > 0,\n+      \"cutoffs should be a sequence of length larger than 0\");\n",
            "whole_hunk": "@@ -24,6 +24,9 @@ AdaptiveLogSoftmaxWithLossImpl::AdaptiveLogSoftmaxWithLossImpl(\n }\n \n void AdaptiveLogSoftmaxWithLossImpl::reset() {\n+  TORCH_CHECK(\n+      options.cutoffs().size() > 0,\n+      \"cutoffs should be a sequence of length larger than 0\");\n   TORCH_CHECK(\n       std::is_sorted(options.cutoffs().begin(), options.cutoffs().end()) &&\n           *std::min_element(\n"
        },
        {
            "name": "adaptive.py",
            "path": "torch/nn/modules/adaptive.py",
            "patches": [
                {
                    "old_start": 124,
                    "old_length": 6,
                    "new_start": 124,
                    "new_length": 9,
                    "hunk": "@@ -124,6 +124,9 @@ class AdaptiveLogSoftmaxWithLoss(Module):\n \n         cutoffs = list(cutoffs)\n \n+        if (len(cutoffs) == 0):\n+            raise ValueError(\"cutoffs should be a sequence of length larger than 0\")\n+\n         if (cutoffs != sorted(cutoffs)) \\\n                 or (min(cutoffs) <= 0) \\\n                 or (max(cutoffs) > (n_classes - 1)) \\"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        if (len(cutoffs) == 0):\n+            raise ValueError(\"cutoffs should be a sequence of length larger than 0\")\n+\n",
            "whole_hunk": "@@ -124,6 +124,9 @@ class AdaptiveLogSoftmaxWithLoss(Module):\n \n         cutoffs = list(cutoffs)\n \n+        if (len(cutoffs) == 0):\n+            raise ValueError(\"cutoffs should be a sequence of length larger than 0\")\n+\n         if (cutoffs != sorted(cutoffs)) \\\n                 or (min(cutoffs) <= 0) \\\n                 or (max(cutoffs) > (n_classes - 1)) \\"
        }
    ]
},
{
    "Id": 368,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/c1d960aadd23bb261724c11f34d4a852ffaacfd6",
    "date": "2023-12-21T01:36:49+00:00",
    "message": "[Quant] [Inductor] add input shape check for quantized conv binary lowering (#115247)\n\nAdd inputs shape check for quantized conv binary lowering, since qconv2d_pointwise.binary does not yet support the case of broadcasting shape inputs.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115247\nApproved by: https://github.com/leslie-fang-intel, https://github.com/eellison",
    "label": "NO",
    "changes": [
        {
            "name": "test_mkldnn_pattern_matcher.py",
            "path": "test/inductor/test_mkldnn_pattern_matcher.py",
            "patches": [
                {
                    "old_start": 667,
                    "old_length": 6,
                    "new_start": 667,
                    "new_length": 46,
                    "hunk": "@@ -667,6 +667,46 @@ class TestPatternMatcher(TestPatternMatcherBase):\n     def test_qconv2d_add_relu_int8_mixed_bf16(self):\n         self._qconv2d_add_cpu_test_helper(use_relu=True, int8_mixed_bf16=True)\n \n+    @skipIfNoDynamoSupport\n+    @skipIfNoONEDNN\n+    @skipIfRocm\n+    def test_qconv2d_add_broadcast_shapes_cpu(self):\n+        r\"\"\"\n+        This testcase will quantize Conv2d->add pattern using broadcast shape inputs.\n+        Conv2d->Add fusion will fail for the broadcast shape inputs case.\n+        \"\"\"\n+\n+        class M(torch.nn.Module):\n+            def __init__(self, use_bias):\n+                super().__init__()\n+                self.conv = torch.nn.Conv2d(32, 32, kernel_size=3, stride=1)\n+\n+            def forward(self, x1, x2):\n+                return torch.add(self.conv(x1), x2)\n+\n+        bias_list = [True, False]\n+        for bias in bias_list:\n+            mod = M(bias).eval()\n+            x1 = torch.randn((2, 32, 9, 9))\n+            x2 = torch.randn((2, 32, 1, 1))\n+\n+            def matcher_check_fn():\n+                # 1. Dequant-Conv2D pattern matched in quantization weight prepack * 1\n+                self.assertEqual(\n+                    counters[\"inductor\"][\"qconv2d_weight_prepack_matcher_count\"], 1\n+                )\n+                # 2. Qconv2d Binary Unary fusion in post-grad fusion pass * 0\n+                self.assertEqual(\n+                    counters[\"inductor\"][\"qconv2d_binary_matcher_count\"], 0\n+                )\n+\n+            self._test_common(\n+                mod,\n+                (x1, x2),\n+                check_quantization=True,\n+                matcher_check_fn=matcher_check_fn,\n+            )\n+\n     @skipIfNoDynamoSupport\n     @skipIfNoONEDNN\n     @skipIfRocm\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    @skipIfNoDynamoSupport\n+    @skipIfNoONEDNN\n+    @skipIfRocm\n+    def test_qconv2d_add_broadcast_shapes_cpu(self):\n+        r\"\"\"\n+        This testcase will quantize Conv2d->add pattern using broadcast shape inputs.\n+        Conv2d->Add fusion will fail for the broadcast shape inputs case.\n+        \"\"\"\n+\n+        class M(torch.nn.Module):\n+            def __init__(self, use_bias):\n+                super().__init__()\n+                self.conv = torch.nn.Conv2d(32, 32, kernel_size=3, stride=1)\n+\n+            def forward(self, x1, x2):\n+                return torch.add(self.conv(x1), x2)\n+\n+        bias_list = [True, False]\n+        for bias in bias_list:\n+            mod = M(bias).eval()\n+            x1 = torch.randn((2, 32, 9, 9))\n+            x2 = torch.randn((2, 32, 1, 1))\n+\n+            def matcher_check_fn():\n+                # 1. Dequant-Conv2D pattern matched in quantization weight prepack * 1\n+                self.assertEqual(\n+                    counters[\"inductor\"][\"qconv2d_weight_prepack_matcher_count\"], 1\n+                )\n+                # 2. Qconv2d Binary Unary fusion in post-grad fusion pass * 0\n+                self.assertEqual(\n+                    counters[\"inductor\"][\"qconv2d_binary_matcher_count\"], 0\n+                )\n+\n+            self._test_common(\n+                mod,\n+                (x1, x2),\n+                check_quantization=True,\n+                matcher_check_fn=matcher_check_fn,\n+            )\n+\n",
            "whole_hunk": "@@ -667,6 +667,46 @@ class TestPatternMatcher(TestPatternMatcherBase):\n     def test_qconv2d_add_relu_int8_mixed_bf16(self):\n         self._qconv2d_add_cpu_test_helper(use_relu=True, int8_mixed_bf16=True)\n \n+    @skipIfNoDynamoSupport\n+    @skipIfNoONEDNN\n+    @skipIfRocm\n+    def test_qconv2d_add_broadcast_shapes_cpu(self):\n+        r\"\"\"\n+        This testcase will quantize Conv2d->add pattern using broadcast shape inputs.\n+        Conv2d->Add fusion will fail for the broadcast shape inputs case.\n+        \"\"\"\n+\n+        class M(torch.nn.Module):\n+            def __init__(self, use_bias):\n+                super().__init__()\n+                self.conv = torch.nn.Conv2d(32, 32, kernel_size=3, stride=1)\n+\n+            def forward(self, x1, x2):\n+                return torch.add(self.conv(x1), x2)\n+\n+        bias_list = [True, False]\n+        for bias in bias_list:\n+            mod = M(bias).eval()\n+            x1 = torch.randn((2, 32, 9, 9))\n+            x2 = torch.randn((2, 32, 1, 1))\n+\n+            def matcher_check_fn():\n+                # 1. Dequant-Conv2D pattern matched in quantization weight prepack * 1\n+                self.assertEqual(\n+                    counters[\"inductor\"][\"qconv2d_weight_prepack_matcher_count\"], 1\n+                )\n+                # 2. Qconv2d Binary Unary fusion in post-grad fusion pass * 0\n+                self.assertEqual(\n+                    counters[\"inductor\"][\"qconv2d_binary_matcher_count\"], 0\n+                )\n+\n+            self._test_common(\n+                mod,\n+                (x1, x2),\n+                check_quantization=True,\n+                matcher_check_fn=matcher_check_fn,\n+            )\n+\n     @skipIfNoDynamoSupport\n     @skipIfNoONEDNN\n     @skipIfRocm\n"
        },
        {
            "name": "quantization.py",
            "path": "torch/_inductor/fx_passes/quantization.py",
            "patches": [
                {
                    "old_start": 417,
                    "old_length": 6,
                    "new_start": 417,
                    "new_length": 8,
                    "hunk": "@@ -417,6 +417,8 @@ def _is_valid_quantized_conv_binary_optimization_pattern(output_dtype):\n     # Check if it's a valid Conv Binary Pattern:\n     # * qconv2d_pointwise should only has one users\n     # * Extra input of binary node comes from dequant pattern\n+    # * the two inputs of binary node should have attribute \"meta\" and should be tensors\n+    # * the two inputs of binary node should have the same shape\n     def fn(match):\n         qconv2d_node_after_weight_prepack = filter_nodes(\n             match.nodes, torch.ops.onednn.qconv2d_pointwise\n"
                },
                {
                    "old_start": 438,
                    "old_length": 6,
                    "new_start": 440,
                    "new_length": 23,
                    "hunk": "@@ -438,6 +440,23 @@ def _is_valid_quantized_conv_binary_optimization_pattern(output_dtype):\n                 extra_input_node.target != aten.mul.Tensor\n             ):\n                 return False\n+            if not (\n+                hasattr(binary_node_inputs[0], \"meta\")\n+                and isinstance(\n+                    binary_node_inputs[0].meta.get(\"val\", None), torch.Tensor\n+                )\n+            ) or not (\n+                hasattr(binary_node_inputs[1], \"meta\")\n+                and isinstance(\n+                    binary_node_inputs[1].meta.get(\"val\", None), torch.Tensor\n+                )\n+            ):\n+                return False\n+            if (\n+                binary_node_inputs[0].meta[\"val\"].size()\n+                != binary_node_inputs[1].meta[\"val\"].size()\n+            ):\n+                return False\n         return True\n \n     return fn"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    # * the two inputs of binary node should have attribute \"meta\" and should be tensors\n+    # * the two inputs of binary node should have the same shape\n+            if not (\n+                hasattr(binary_node_inputs[0], \"meta\")\n+                and isinstance(\n+                    binary_node_inputs[0].meta.get(\"val\", None), torch.Tensor\n+                )\n+            ) or not (\n+                hasattr(binary_node_inputs[1], \"meta\")\n+                and isinstance(\n+                    binary_node_inputs[1].meta.get(\"val\", None), torch.Tensor\n+                )\n+            ):\n+                return False\n+            if (\n+                binary_node_inputs[0].meta[\"val\"].size()\n+                != binary_node_inputs[1].meta[\"val\"].size()\n+            ):\n+                return False\n",
            "whole_hunk": "@@ -417,6 +417,8 @@ def _is_valid_quantized_conv_binary_optimization_pattern(output_dtype):\n     # Check if it's a valid Conv Binary Pattern:\n     # * qconv2d_pointwise should only has one users\n     # * Extra input of binary node comes from dequant pattern\n+    # * the two inputs of binary node should have attribute \"meta\" and should be tensors\n+    # * the two inputs of binary node should have the same shape\n     def fn(match):\n         qconv2d_node_after_weight_prepack = filter_nodes(\n             match.nodes, torch.ops.onednn.qconv2d_pointwise\n@@ -438,6 +440,23 @@ def _is_valid_quantized_conv_binary_optimization_pattern(output_dtype):\n                 extra_input_node.target != aten.mul.Tensor\n             ):\n                 return False\n+            if not (\n+                hasattr(binary_node_inputs[0], \"meta\")\n+                and isinstance(\n+                    binary_node_inputs[0].meta.get(\"val\", None), torch.Tensor\n+                )\n+            ) or not (\n+                hasattr(binary_node_inputs[1], \"meta\")\n+                and isinstance(\n+                    binary_node_inputs[1].meta.get(\"val\", None), torch.Tensor\n+                )\n+            ):\n+                return False\n+            if (\n+                binary_node_inputs[0].meta[\"val\"].size()\n+                != binary_node_inputs[1].meta[\"val\"].size()\n+            ):\n+                return False\n         return True\n \n     return fn"
        }
    ]
},
{
    "Id": 132,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/6b39146b3f33c2d00d2e1c0ae60ec32951333a08",
    "date": "2024-05-23T20:16:06+00:00",
    "message": "[pipelining] Validate stage input/output shape/dtype (#126732)\n\nAddress the classes of user errors stemming from (possibly)\nunintentional dynamic shapes usage or mismatch of configuration time and\nrun time data shapes/dtypes.\n\nThe goal is to ensure a clear error is raised rather than relying on some underlying\nerror to bubble up when a tensor shape is not compatible, or worse,\nhaving a silent correctness issue.\n\n**Classes of shape/dtype errors**\n* (a) error is thrown within the stage-module forward code, but may be\nhard to understand/trace back to an input issue\n* (b) silent correctness issue happens inside the stage-module forward,\nbut the correct output shape is still produced\nproduces the expected output shape\n* (c) the stage-module produces an output that is locally correct, but not\nmatching the expectation of the following stage, leading to a hang or\ncorrectness issue down the line\n\n**How validation helps**\n\nInput shape validation\n- improves debugability of case (a)\n- guards against case (b)\n- only needed on first stage, since subsequent stages use pre-allocated recv\n  buffers that can't change shape/size even if they wanted to\n\nOutput shape validation\n- guards against case (c)\n\nValidation of first stage input and all stages' outputs inductively verifies all shapes\n\nShape/dtype are most critical as they literally affect the number of\nbytes on the wire.  Strides and other tensor properties may also (?)\nmatter, and the validation function can be adjusted accordingly if needed.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/126732\nApproved by: https://github.com/kwen2501",
    "label": "NO",
    "changes": [
        {
            "name": "test_stage.py",
            "path": "test/distributed/pipelining/test_stage.py",
            "patches": [
                {
                    "old_start": 14,
                    "old_length": 6,
                    "new_start": 14,
                    "new_length": 7,
                    "hunk": "@@ -14,6 +14,7 @@ from torch.distributed.pipelining import (\n     PipelineStage,\n     ScheduleGPipe,\n )\n+from torch.distributed.pipelining._utils import PipeliningShapeError\n from torch.testing._internal.common_cuda import TEST_MULTIGPU\n from torch.testing._internal.common_distributed import (\n     MultiProcContinousTest,\n"
                },
                {
                    "old_start": 24,
                    "old_length": 7,
                    "new_start": 25,
                    "new_length": 7,
                    "hunk": "@@ -24,7 +25,7 @@ from torch.testing._internal.common_utils import (\n     parametrize,\n     skip_but_pass_in_sandcastle_if,\n )\n-\n+from torch.utils._pytree import tree_map_only\n \n d_hid = 512\n batch_size = 256\n"
                },
                {
                    "old_start": 33,
                    "old_length": 6,
                    "new_start": 34,
                    "new_length": 30,
                    "hunk": "@@ -33,6 +34,30 @@ chunks = 4\n torch.manual_seed(0)\n \n \n+def get_dtype_change_hook(new_dtype):\n+    \"\"\"A simple hook for simulating mixed precision\"\"\"\n+\n+    def dtype_change_hook(module, input, output):\n+        def f(x):\n+            return x.to(new_dtype)\n+\n+        return tree_map_only(torch.Tensor, f, output)\n+\n+    return dtype_change_hook\n+\n+\n+def get_flatten_hook():\n+    \"\"\"A simple hook for simulating wrong model output shape\"\"\"\n+\n+    def flatten_hook(module, input, output):\n+        def f(x):\n+            return x.flatten()\n+\n+        return tree_map_only(torch.Tensor, f, output)\n+\n+    return flatten_hook\n+\n+\n class StageTest(MultiProcContinousTest):\n     @classmethod\n     def backend_str(cls) -> str:\n"
                },
                {
                    "old_start": 74,
                    "old_length": 11,
                    "new_start": 99,
                    "new_length": 13,
                    "hunk": "@@ -74,11 +99,13 @@ class StageTest(MultiProcContinousTest):\n         schedule = ScheduleGPipe(stage, chunks)\n \n         # Run\n-        if self.rank == 0:\n-            schedule.step(x)\n-        else:\n-            out = schedule.step()\n+        def _run_step(x):\n+            if self.rank == 0:\n+                return schedule.step(x)\n+            else:\n+                return schedule.step()\n \n+        out = _run_step(x)\n         # Last rank checks result\n         if self.rank == self.world_size - 1:\n             ref_out = mod(x)\n"
                },
                {
                    "old_start": 90,
                    "old_length": 6,
                    "new_start": 117,
                    "new_length": 27,
                    "hunk": "@@ -90,6 +117,27 @@ class StageTest(MultiProcContinousTest):\n         old_keys = mod.state_dict().keys()\n         assert all(k in old_keys for k in submod_keys)\n \n+        if self.rank == 0:\n+            # intended to run this code on all ranks, but the problem is if rank0 throws,\n+            # it won't perform the send that unblocks rank 1.\n+\n+            # TODO(whc) can't test this until fixing args/kwargs issue\n+            # with self.assertRaisesRegex(PipeliningShapeError, \"shape mismatch\"):\n+            #     _run_step(torch.randn(batch_size + 1, d_hid, device=self.device))\n+\n+            with self.assertRaisesRegex(PipeliningShapeError, \"dtype mismatch\"):\n+                _run_step(x.to(torch.int32))\n+\n+            # output of stage's mlp layer will be flattened by this hook, the stage should err\n+            handle = stage.submod.register_forward_hook(get_flatten_hook())\n+            with self.assertRaisesRegex(PipeliningShapeError, \"shape mismatch\"):\n+                _run_step(x)\n+            handle.remove()\n+\n+            stage.submod.register_forward_hook(get_dtype_change_hook(torch.bfloat16))\n+            with self.assertRaisesRegex(PipeliningShapeError, \"dtype mismatch\"):\n+                _run_step(x)\n+\n     @requires_nccl()\n     @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"NCCL test requires 2+ GPUs\")\n     @parametrize(\"ModelClass\", [ModelWithKwargs])\n"
                },
                {
                    "old_start": 117,
                    "old_length": 12,
                    "new_start": 165,
                    "new_length": 14,
                    "hunk": "@@ -117,12 +165,14 @@ class StageTest(MultiProcContinousTest):\n         schedule = ScheduleGPipe(stage, chunks)\n \n         # Run\n-        if self.rank == 0:\n-            schedule.step(x, y=y)\n-        else:\n-            out = schedule.step()\n+        def _run_step(x):\n+            if self.rank == 0:\n+                return schedule.step(x, y=y)\n+            else:\n+                return schedule.step()\n \n         # Last rank checks result\n+        out = _run_step(x)\n         if self.rank == self.world_size - 1:\n             ref_out = mod(x, y=y)\n             torch.testing.assert_close(out, ref_out, atol=1e-3, rtol=5e-2)\n"
                },
                {
                    "old_start": 133,
                    "old_length": 6,
                    "new_start": 183,
                    "new_length": 23,
                    "hunk": "@@ -133,6 +183,23 @@ class StageTest(MultiProcContinousTest):\n         old_keys = mod.state_dict().keys()\n         assert all(k in old_keys for k in submod_keys)\n \n+        if self.rank == 0:\n+            with self.assertRaisesRegex(PipeliningShapeError, \"shape mismatch\"):\n+                _run_step(torch.randn(batch_size + 1, d_hid, device=self.device))\n+\n+            with self.assertRaisesRegex(PipeliningShapeError, \"dtype mismatch\"):\n+                _run_step(x.to(torch.int32))\n+\n+            # output of stage's mlp layer will be flattened by this hook, the stage should err\n+            handle = stage.submod.register_forward_hook(get_flatten_hook())\n+            with self.assertRaisesRegex(PipeliningShapeError, \"shape mismatch\"):\n+                _run_step(x)\n+            handle.remove()\n+\n+            stage.submod.register_forward_hook(get_dtype_change_hook(torch.bfloat16))\n+            with self.assertRaisesRegex(PipeliningShapeError, \"dtype mismatch\"):\n+                _run_step(x)\n+\n     @requires_nccl()\n     @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"NCCL test requires 2+ GPUs\")\n     def test_manual(self):\n"
                },
                {
                    "old_start": 155,
                    "old_length": 16,
                    "new_start": 222,
                    "new_length": 35,
                    "hunk": "@@ -155,16 +222,35 @@ class StageTest(MultiProcContinousTest):\n         schedule = ScheduleGPipe(stage, chunks)\n \n         # Run\n-        if self.rank == 0:\n-            schedule.step(x)\n-        else:\n-            out = schedule.step()\n+        def _run_step(x):\n+            if self.rank == 0:\n+                return schedule.step(x)\n+            else:\n+                return schedule.step()\n \n+        out = _run_step(x)\n         # Last rank checks result\n         if self.rank == self.world_size - 1:\n             ref_out = full_mod(x)\n             torch.testing.assert_close(out, ref_out)\n \n+        if self.rank == 0:\n+            with self.assertRaisesRegex(PipeliningShapeError, \"shape mismatch\"):\n+                _run_step(torch.randn(batch_size + 1, d_hid, device=self.device))\n+\n+            with self.assertRaisesRegex(PipeliningShapeError, \"dtype mismatch\"):\n+                _run_step(x.to(torch.int32))\n+\n+            # output of stage's mlp layer will be flattened by this hook, the stage should err\n+            handle = stage_mod.register_forward_hook(get_flatten_hook())\n+            with self.assertRaisesRegex(PipeliningShapeError, \"shape mismatch\"):\n+                _run_step(x)\n+            handle.remove()\n+\n+            stage_mod.register_forward_hook(get_dtype_change_hook(torch.bfloat16))\n+            with self.assertRaisesRegex(PipeliningShapeError, \"dtype mismatch\"):\n+                _run_step(x)\n+\n \n instantiate_parametrized_tests(StageTest)\n \n"
                }
            ],
            "whole_deleted": "-\n-        if self.rank == 0:\n-            schedule.step(x)\n-        else:\n-            out = schedule.step()\n-        if self.rank == 0:\n-            schedule.step(x, y=y)\n-        else:\n-            out = schedule.step()\n-        if self.rank == 0:\n-            schedule.step(x)\n-        else:\n-            out = schedule.step()\n",
            "whole_added": "+from torch.distributed.pipelining._utils import PipeliningShapeError\n+from torch.utils._pytree import tree_map_only\n+def get_dtype_change_hook(new_dtype):\n+    \"\"\"A simple hook for simulating mixed precision\"\"\"\n+\n+    def dtype_change_hook(module, input, output):\n+        def f(x):\n+            return x.to(new_dtype)\n+\n+        return tree_map_only(torch.Tensor, f, output)\n+\n+    return dtype_change_hook\n+\n+\n+def get_flatten_hook():\n+    \"\"\"A simple hook for simulating wrong model output shape\"\"\"\n+\n+    def flatten_hook(module, input, output):\n+        def f(x):\n+            return x.flatten()\n+\n+        return tree_map_only(torch.Tensor, f, output)\n+\n+    return flatten_hook\n+\n+\n+        def _run_step(x):\n+            if self.rank == 0:\n+                return schedule.step(x)\n+            else:\n+                return schedule.step()\n+        out = _run_step(x)\n+        if self.rank == 0:\n+            # intended to run this code on all ranks, but the problem is if rank0 throws,\n+            # it won't perform the send that unblocks rank 1.\n+\n+            # TODO(whc) can't test this until fixing args/kwargs issue\n+            # with self.assertRaisesRegex(PipeliningShapeError, \"shape mismatch\"):\n+            #     _run_step(torch.randn(batch_size + 1, d_hid, device=self.device))\n+\n+            with self.assertRaisesRegex(PipeliningShapeError, \"dtype mismatch\"):\n+                _run_step(x.to(torch.int32))\n+\n+            # output of stage's mlp layer will be flattened by this hook, the stage should err\n+            handle = stage.submod.register_forward_hook(get_flatten_hook())\n+            with self.assertRaisesRegex(PipeliningShapeError, \"shape mismatch\"):\n+                _run_step(x)\n+            handle.remove()\n+\n+            stage.submod.register_forward_hook(get_dtype_change_hook(torch.bfloat16))\n+            with self.assertRaisesRegex(PipeliningShapeError, \"dtype mismatch\"):\n+                _run_step(x)\n+\n+        def _run_step(x):\n+            if self.rank == 0:\n+                return schedule.step(x, y=y)\n+            else:\n+                return schedule.step()\n+        out = _run_step(x)\n+        if self.rank == 0:\n+            with self.assertRaisesRegex(PipeliningShapeError, \"shape mismatch\"):\n+                _run_step(torch.randn(batch_size + 1, d_hid, device=self.device))\n+\n+            with self.assertRaisesRegex(PipeliningShapeError, \"dtype mismatch\"):\n+                _run_step(x.to(torch.int32))\n+\n+            # output of stage's mlp layer will be flattened by this hook, the stage should err\n+            handle = stage.submod.register_forward_hook(get_flatten_hook())\n+            with self.assertRaisesRegex(PipeliningShapeError, \"shape mismatch\"):\n+                _run_step(x)\n+            handle.remove()\n+\n+            stage.submod.register_forward_hook(get_dtype_change_hook(torch.bfloat16))\n+            with self.assertRaisesRegex(PipeliningShapeError, \"dtype mismatch\"):\n+                _run_step(x)\n+\n+        def _run_step(x):\n+            if self.rank == 0:\n+                return schedule.step(x)\n+            else:\n+                return schedule.step()\n+        out = _run_step(x)\n+        if self.rank == 0:\n+            with self.assertRaisesRegex(PipeliningShapeError, \"shape mismatch\"):\n+                _run_step(torch.randn(batch_size + 1, d_hid, device=self.device))\n+\n+            with self.assertRaisesRegex(PipeliningShapeError, \"dtype mismatch\"):\n+                _run_step(x.to(torch.int32))\n+\n+            # output of stage's mlp layer will be flattened by this hook, the stage should err\n+            handle = stage_mod.register_forward_hook(get_flatten_hook())\n+            with self.assertRaisesRegex(PipeliningShapeError, \"shape mismatch\"):\n+                _run_step(x)\n+            handle.remove()\n+\n+            stage_mod.register_forward_hook(get_dtype_change_hook(torch.bfloat16))\n+            with self.assertRaisesRegex(PipeliningShapeError, \"dtype mismatch\"):\n+                _run_step(x)\n+\n",
            "whole_hunk": "@@ -14,6 +14,7 @@ from torch.distributed.pipelining import (\n     PipelineStage,\n     ScheduleGPipe,\n )\n+from torch.distributed.pipelining._utils import PipeliningShapeError\n from torch.testing._internal.common_cuda import TEST_MULTIGPU\n from torch.testing._internal.common_distributed import (\n     MultiProcContinousTest,\n@@ -24,7 +25,7 @@ from torch.testing._internal.common_utils import (\n     parametrize,\n     skip_but_pass_in_sandcastle_if,\n )\n-\n+from torch.utils._pytree import tree_map_only\n \n d_hid = 512\n batch_size = 256\n@@ -33,6 +34,30 @@ chunks = 4\n torch.manual_seed(0)\n \n \n+def get_dtype_change_hook(new_dtype):\n+    \"\"\"A simple hook for simulating mixed precision\"\"\"\n+\n+    def dtype_change_hook(module, input, output):\n+        def f(x):\n+            return x.to(new_dtype)\n+\n+        return tree_map_only(torch.Tensor, f, output)\n+\n+    return dtype_change_hook\n+\n+\n+def get_flatten_hook():\n+    \"\"\"A simple hook for simulating wrong model output shape\"\"\"\n+\n+    def flatten_hook(module, input, output):\n+        def f(x):\n+            return x.flatten()\n+\n+        return tree_map_only(torch.Tensor, f, output)\n+\n+    return flatten_hook\n+\n+\n class StageTest(MultiProcContinousTest):\n     @classmethod\n     def backend_str(cls) -> str:\n@@ -74,11 +99,13 @@ class StageTest(MultiProcContinousTest):\n         schedule = ScheduleGPipe(stage, chunks)\n \n         # Run\n-        if self.rank == 0:\n-            schedule.step(x)\n-        else:\n-            out = schedule.step()\n+        def _run_step(x):\n+            if self.rank == 0:\n+                return schedule.step(x)\n+            else:\n+                return schedule.step()\n \n+        out = _run_step(x)\n         # Last rank checks result\n         if self.rank == self.world_size - 1:\n             ref_out = mod(x)\n@@ -90,6 +117,27 @@ class StageTest(MultiProcContinousTest):\n         old_keys = mod.state_dict().keys()\n         assert all(k in old_keys for k in submod_keys)\n \n+        if self.rank == 0:\n+            # intended to run this code on all ranks, but the problem is if rank0 throws,\n+            # it won't perform the send that unblocks rank 1.\n+\n+            # TODO(whc) can't test this until fixing args/kwargs issue\n+            # with self.assertRaisesRegex(PipeliningShapeError, \"shape mismatch\"):\n+            #     _run_step(torch.randn(batch_size + 1, d_hid, device=self.device))\n+\n+            with self.assertRaisesRegex(PipeliningShapeError, \"dtype mismatch\"):\n+                _run_step(x.to(torch.int32))\n+\n+            # output of stage's mlp layer will be flattened by this hook, the stage should err\n+            handle = stage.submod.register_forward_hook(get_flatten_hook())\n+            with self.assertRaisesRegex(PipeliningShapeError, \"shape mismatch\"):\n+                _run_step(x)\n+            handle.remove()\n+\n+            stage.submod.register_forward_hook(get_dtype_change_hook(torch.bfloat16))\n+            with self.assertRaisesRegex(PipeliningShapeError, \"dtype mismatch\"):\n+                _run_step(x)\n+\n     @requires_nccl()\n     @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"NCCL test requires 2+ GPUs\")\n     @parametrize(\"ModelClass\", [ModelWithKwargs])\n@@ -117,12 +165,14 @@ class StageTest(MultiProcContinousTest):\n         schedule = ScheduleGPipe(stage, chunks)\n \n         # Run\n-        if self.rank == 0:\n-            schedule.step(x, y=y)\n-        else:\n-            out = schedule.step()\n+        def _run_step(x):\n+            if self.rank == 0:\n+                return schedule.step(x, y=y)\n+            else:\n+                return schedule.step()\n \n         # Last rank checks result\n+        out = _run_step(x)\n         if self.rank == self.world_size - 1:\n             ref_out = mod(x, y=y)\n             torch.testing.assert_close(out, ref_out, atol=1e-3, rtol=5e-2)\n@@ -133,6 +183,23 @@ class StageTest(MultiProcContinousTest):\n         old_keys = mod.state_dict().keys()\n         assert all(k in old_keys for k in submod_keys)\n \n+        if self.rank == 0:\n+            with self.assertRaisesRegex(PipeliningShapeError, \"shape mismatch\"):\n+                _run_step(torch.randn(batch_size + 1, d_hid, device=self.device))\n+\n+            with self.assertRaisesRegex(PipeliningShapeError, \"dtype mismatch\"):\n+                _run_step(x.to(torch.int32))\n+\n+            # output of stage's mlp layer will be flattened by this hook, the stage should err\n+            handle = stage.submod.register_forward_hook(get_flatten_hook())\n+            with self.assertRaisesRegex(PipeliningShapeError, \"shape mismatch\"):\n+                _run_step(x)\n+            handle.remove()\n+\n+            stage.submod.register_forward_hook(get_dtype_change_hook(torch.bfloat16))\n+            with self.assertRaisesRegex(PipeliningShapeError, \"dtype mismatch\"):\n+                _run_step(x)\n+\n     @requires_nccl()\n     @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"NCCL test requires 2+ GPUs\")\n     def test_manual(self):\n@@ -155,16 +222,35 @@ class StageTest(MultiProcContinousTest):\n         schedule = ScheduleGPipe(stage, chunks)\n \n         # Run\n-        if self.rank == 0:\n-            schedule.step(x)\n-        else:\n-            out = schedule.step()\n+        def _run_step(x):\n+            if self.rank == 0:\n+                return schedule.step(x)\n+            else:\n+                return schedule.step()\n \n+        out = _run_step(x)\n         # Last rank checks result\n         if self.rank == self.world_size - 1:\n             ref_out = full_mod(x)\n             torch.testing.assert_close(out, ref_out)\n \n+        if self.rank == 0:\n+            with self.assertRaisesRegex(PipeliningShapeError, \"shape mismatch\"):\n+                _run_step(torch.randn(batch_size + 1, d_hid, device=self.device))\n+\n+            with self.assertRaisesRegex(PipeliningShapeError, \"dtype mismatch\"):\n+                _run_step(x.to(torch.int32))\n+\n+            # output of stage's mlp layer will be flattened by this hook, the stage should err\n+            handle = stage_mod.register_forward_hook(get_flatten_hook())\n+            with self.assertRaisesRegex(PipeliningShapeError, \"shape mismatch\"):\n+                _run_step(x)\n+            handle.remove()\n+\n+            stage_mod.register_forward_hook(get_dtype_change_hook(torch.bfloat16))\n+            with self.assertRaisesRegex(PipeliningShapeError, \"dtype mismatch\"):\n+                _run_step(x)\n+\n \n instantiate_parametrized_tests(StageTest)\n \n"
        },
        {
            "name": "PipelineStage.py",
            "path": "torch/distributed/pipelining/PipelineStage.py",
            "patches": [
                {
                    "old_start": 16,
                    "old_length": 7,
                    "new_start": 16,
                    "new_length": 7,
                    "hunk": "@@ -16,7 +16,7 @@ from torch.nn.parallel import DistributedDataParallel\n from ._backward import stage_backward\n from ._debug import map_debug_info\n from ._IR import Pipe\n-from ._utils import flatten_args, modify_graph_op_device\n+from ._utils import flatten_args, modify_graph_op_device, validate_tensors_metadata\n \n \n __all__ = [\n"
                },
                {
                    "old_start": 32,
                    "old_length": 7,
                    "new_start": 32,
                    "new_length": 8,
                    "hunk": "@@ -32,7 +32,8 @@ class _RootArgPlaceholder:\n     Placeholder for model-level inputs.\n     \"\"\"\n \n-    pass\n+    def __init__(self, tensor):\n+        self.meta = tensor.to(\"meta\")\n \n \n class _RecvInfo:\n"
                },
                {
                    "old_start": 124,
                    "old_length": 6,
                    "new_start": 125,
                    "new_length": 7,
                    "hunk": "@@ -124,6 +125,7 @@ class _PipelineStageBase(ABC):\n             )\n \n         # Run time states\n+        self._outputs_meta: Optional[Tuple[torch.Tensor, ...]] = None\n         # map microbatch ID to list of forward tensor args\n         self.fwd_cache: Dict[int, Tuple[Any, List[torch.Tensor]]] = {}\n         # Current forward chunk id\n"
                },
                {
                    "old_start": 182,
                    "old_length": 6,
                    "new_start": 184,
                    "new_length": 25,
                    "hunk": "@@ -182,6 +184,25 @@ class _PipelineStageBase(ABC):\n         \"\"\"\n         return self.stage_index == self.num_stages - 1\n \n+    def _configure_outputs_meta(self, outputs_meta: Tuple[torch.Tensor, ...]):\n+        \"\"\"\n+        Track the output shapes/dtype of this stage since they determine the send operation(s) which must match\n+        recv operations of the next stage.  The next stage _will_ be freezing its recv buffers based on its initial\n+        configuration, so it's important to also freeze/validate the output side to avoid any send/recv mismatches\n+        which could show up as hangs, silent corruption, or other errors.\n+        \"\"\"\n+        assert (\n+            self._outputs_meta is None\n+        ), \"Attempting to reconfigure output_meta, which is not supported\"\n+        self._outputs_meta = tuple(outputs_meta)  # type: ignore[assignment]\n+\n+    def get_outputs_meta(self) -> Tuple[torch.Tensor, ...]:\n+        \"\"\"Get the output metadata (meta tensors) reprensenting the outputs of this stage\"\"\"\n+        assert (\n+            self._outputs_meta is not None\n+        ), \"Attempted to get_outputs_meta() without configuring output meta\"\n+        return self._outputs_meta\n+\n     def _create_grad_send_info(\n         self,\n         args_recv_info: Tuple,\n"
                },
                {
                    "old_start": 469,
                    "old_length": 6,
                    "new_start": 491,
                    "new_length": 8,
                    "hunk": "@@ -469,6 +491,8 @@ class _PipelineStageBase(ABC):\n             composite_args = self._retrieve_recv_activations()\n             composite_kwargs = {}\n \n+        self._validate_fwd_input(args, kwargs)\n+\n         # Compute forward\n         try:\n             output = self.forward_maybe_with_nosync(*composite_args, **composite_kwargs)\n"
                },
                {
                    "old_start": 504,
                    "old_length": 6,
                    "new_start": 528,
                    "new_length": 7,
                    "hunk": "@@ -504,6 +528,7 @@ class _PipelineStageBase(ABC):\n         logger.debug(\n             f\"{self.log_prefix} Forwarded chunk {self.fwd_chunk_id}, outputs: {map_debug_info(output)}\"  # noqa: G004\n         )\n+        self._validate_fwd_outputs(output_tuple)\n         self.fwd_chunk_id += 1\n         return output\n \n"
                },
                {
                    "old_start": 549,
                    "old_length": 6,
                    "new_start": 574,
                    "new_length": 39,
                    "hunk": "@@ -549,6 +574,39 @@ class _PipelineStageBase(ABC):\n         )\n         self.bwd_chunk_id += 1\n \n+    def _validate_fwd_input(self, args, kwargs):\n+        \"\"\"Raises a RuntimeError if shapes of input args/kwargs do not match the shapes configured for this stage.\"\"\"\n+\n+        if self.is_first:\n+            # TODO why is there a separate recv_info for each pipeline chunk?\n+            expected_args = self.args_recv_info[self.fwd_chunk_id]\n+        else:\n+            expected_args = tuple()\n+\n+        if len(kwargs):\n+            # TODO- need a mapping of kwarg to position in self.args_recv_info\n+            # without it, we just validate shapes for args and ignore kwargs\n+            expected_args = expected_args[: len(expected_args) - len(kwargs)]\n+\n+        # TODO- need a mapping of kwarg to position in self.args_recv_info\n+        # maybe it's impossible to tell whether the len mismatches because\n+        # (a) the user passed an extra arg or missed an arg\n+        # (b) the user did not pass a kwarg, which has a default value baked into expected_args\n+        expected_tensors_meta = [\n+            e.meta if isinstance(e, _RootArgPlaceholder) else e.buffer\n+            for e in expected_args\n+        ]\n+        validate_tensors_metadata(\"forward input args\", expected_tensors_meta, args)\n+\n+    def _validate_fwd_outputs(self, outputs: Tuple[torch.Tensor, ...]):\n+        \"\"\"Raises a RuntimeError if this stage produces an output of unexpected shape/dtype.\n+        Most likely, this could be cause either by incorrect user specification of output shapes, or becuase\n+        shape inference was done on the original model but then at runtime the model is wrapped with something like\n+        mixed precision which changes output dtype.\n+        \"\"\"\n+        expected_tensors_meta = self.get_outputs_meta()\n+        validate_tensors_metadata(\"forward outputs\", expected_tensors_meta, outputs)\n+\n \n class _PipelineStage(_PipelineStageBase):\n     def __init__(\n"
                },
                {
                    "old_start": 662,
                    "old_length": 10,
                    "new_start": 720,
                    "new_length": 11,
                    "hunk": "@@ -662,10 +720,11 @@ class _PipelineStage(_PipelineStageBase):\n             \"\"\"\n             Create a receive buffer for a placeholder.\n             \"\"\"\n+            example_value = placeholder.meta[\"val\"]\n             if arg_node.op == \"placeholder\":\n                 # This is a root level placeholder, thus an input argument to the entire model.\n                 # We are likely at stage 0, hence no need to create a receive buffer.\n-                return _RootArgPlaceholder()\n+                return _RootArgPlaceholder(example_value)\n \n             # Figure out the source stage of this input\n             while arg_node.target is operator.getitem:\n"
                },
                {
                    "old_start": 678,
                    "old_length": 7,
                    "new_start": 737,
                    "new_length": 6,
                    "hunk": "@@ -678,7 +737,6 @@ class _PipelineStage(_PipelineStageBase):\n             src_stage = self.get_stage_index_of_submod(arg_node.name)\n \n             # Create a receive buffer for this placeholder\n-            example_value = placeholder.meta[\"val\"]\n             logger.debug(\n                 f\"{self.log_prefix} \"  # noqa: G004\n                 f\"Creating recv buffer for input '{placeholder.name}' \"\n"
                },
                {
                    "old_start": 763,
                    "old_length": 9,
                    "new_start": 821,
                    "new_length": 21,
                    "hunk": "@@ -763,9 +821,21 @@ class _PipelineStage(_PipelineStageBase):\n                 if dst_rank is not None:\n                     dsts.append(dst_rank)\n \n+        output_node = self._get_output_node()\n+        output_vals: Tuple[torch.Tensor] = tuple(\n+            v.meta[\"val\"] for v in flatten_args(output_node.args)\n+        )\n+        self._configure_outputs_meta(output_vals)\n+\n         logger.debug(f\"{self.log_prefix} \" f\"Send info: {act_send_info}\")  # noqa: G004\n         return act_send_info\n \n+    def _get_output_node(self):\n+        output_nodes = [node for node in self.submod.graph.nodes if node.op == \"output\"]\n+        assert len(output_nodes) == 1\n+        output_node = output_nodes[0]\n+        return output_node\n+\n     def _create_grad_recv_info(\n         self,\n         act_send_info: Dict,\n"
                },
                {
                    "old_start": 775,
                    "old_length": 9,
                    "new_start": 845,
                    "new_length": 8,
                    "hunk": "@@ -775,9 +845,8 @@ class _PipelineStage(_PipelineStageBase):\n         \"\"\"\n         # Dict[output_index, _RecvInfo]\n         grad_recv_info: Dict[int, _RecvInfo] = {}\n-        output_nodes = [node for node in self.submod.graph.nodes if node.op == \"output\"]\n-        assert len(output_nodes) == 1\n-        output_node = output_nodes[0]\n+        output_node = self._get_output_node()\n+\n         # The output node may take multiple args, meaning the submod having multiple output values.\n         output_vals = flatten_args(output_node.args)\n \n"
                },
                {
                    "old_start": 1034,
                    "old_length": 6,
                    "new_start": 1103,
                    "new_length": 8,
                    "hunk": "@@ -1034,6 +1103,8 @@ class ManualPipelineStage(_PipelineStageBase):\n         else:\n             self.outputs = _create_empty_tensors(output_args, device)\n \n+        self._configure_outputs_meta(tuple(self.outputs))\n+\n         # these are the buffers used in backwards send/recv, they are allocated later\n         self.outputs_grad: List[torch.Tensor] = []\n \n"
                },
                {
                    "old_start": 1067,
                    "old_length": 7,
                    "new_start": 1138,
                    "new_length": 7,
                    "hunk": "@@ -1067,7 +1138,7 @@ class ManualPipelineStage(_PipelineStageBase):\n                 self.args_recv_info[chunk_id] = recv_infos\n             else:\n                 self.args_recv_info[chunk_id] = tuple(\n-                    [_RootArgPlaceholder() for _ in self.inputs]\n+                    [_RootArgPlaceholder(i) for i in self.inputs]\n                 )\n \n         # Send info during forward for each activation\n"
                }
            ],
            "whole_deleted": "-from ._utils import flatten_args, modify_graph_op_device\n-    pass\n-                return _RootArgPlaceholder()\n-            example_value = placeholder.meta[\"val\"]\n-        output_nodes = [node for node in self.submod.graph.nodes if node.op == \"output\"]\n-        assert len(output_nodes) == 1\n-        output_node = output_nodes[0]\n-                    [_RootArgPlaceholder() for _ in self.inputs]\n",
            "whole_added": "+from ._utils import flatten_args, modify_graph_op_device, validate_tensors_metadata\n+    def __init__(self, tensor):\n+        self.meta = tensor.to(\"meta\")\n+        self._outputs_meta: Optional[Tuple[torch.Tensor, ...]] = None\n+    def _configure_outputs_meta(self, outputs_meta: Tuple[torch.Tensor, ...]):\n+        \"\"\"\n+        Track the output shapes/dtype of this stage since they determine the send operation(s) which must match\n+        recv operations of the next stage.  The next stage _will_ be freezing its recv buffers based on its initial\n+        configuration, so it's important to also freeze/validate the output side to avoid any send/recv mismatches\n+        which could show up as hangs, silent corruption, or other errors.\n+        \"\"\"\n+        assert (\n+            self._outputs_meta is None\n+        ), \"Attempting to reconfigure output_meta, which is not supported\"\n+        self._outputs_meta = tuple(outputs_meta)  # type: ignore[assignment]\n+\n+    def get_outputs_meta(self) -> Tuple[torch.Tensor, ...]:\n+        \"\"\"Get the output metadata (meta tensors) reprensenting the outputs of this stage\"\"\"\n+        assert (\n+            self._outputs_meta is not None\n+        ), \"Attempted to get_outputs_meta() without configuring output meta\"\n+        return self._outputs_meta\n+\n+        self._validate_fwd_input(args, kwargs)\n+\n+        self._validate_fwd_outputs(output_tuple)\n+    def _validate_fwd_input(self, args, kwargs):\n+        \"\"\"Raises a RuntimeError if shapes of input args/kwargs do not match the shapes configured for this stage.\"\"\"\n+\n+        if self.is_first:\n+            # TODO why is there a separate recv_info for each pipeline chunk?\n+            expected_args = self.args_recv_info[self.fwd_chunk_id]\n+        else:\n+            expected_args = tuple()\n+\n+        if len(kwargs):\n+            # TODO- need a mapping of kwarg to position in self.args_recv_info\n+            # without it, we just validate shapes for args and ignore kwargs\n+            expected_args = expected_args[: len(expected_args) - len(kwargs)]\n+\n+        # TODO- need a mapping of kwarg to position in self.args_recv_info\n+        # maybe it's impossible to tell whether the len mismatches because\n+        # (a) the user passed an extra arg or missed an arg\n+        # (b) the user did not pass a kwarg, which has a default value baked into expected_args\n+        expected_tensors_meta = [\n+            e.meta if isinstance(e, _RootArgPlaceholder) else e.buffer\n+            for e in expected_args\n+        ]\n+        validate_tensors_metadata(\"forward input args\", expected_tensors_meta, args)\n+\n+    def _validate_fwd_outputs(self, outputs: Tuple[torch.Tensor, ...]):\n+        \"\"\"Raises a RuntimeError if this stage produces an output of unexpected shape/dtype.\n+        Most likely, this could be cause either by incorrect user specification of output shapes, or becuase\n+        shape inference was done on the original model but then at runtime the model is wrapped with something like\n+        mixed precision which changes output dtype.\n+        \"\"\"\n+        expected_tensors_meta = self.get_outputs_meta()\n+        validate_tensors_metadata(\"forward outputs\", expected_tensors_meta, outputs)\n+\n+            example_value = placeholder.meta[\"val\"]\n+                return _RootArgPlaceholder(example_value)\n+        output_node = self._get_output_node()\n+        output_vals: Tuple[torch.Tensor] = tuple(\n+            v.meta[\"val\"] for v in flatten_args(output_node.args)\n+        )\n+        self._configure_outputs_meta(output_vals)\n+\n+    def _get_output_node(self):\n+        output_nodes = [node for node in self.submod.graph.nodes if node.op == \"output\"]\n+        assert len(output_nodes) == 1\n+        output_node = output_nodes[0]\n+        return output_node\n+\n+        output_node = self._get_output_node()\n+\n+        self._configure_outputs_meta(tuple(self.outputs))\n+\n+                    [_RootArgPlaceholder(i) for i in self.inputs]\n",
            "whole_hunk": "@@ -16,7 +16,7 @@ from torch.nn.parallel import DistributedDataParallel\n from ._backward import stage_backward\n from ._debug import map_debug_info\n from ._IR import Pipe\n-from ._utils import flatten_args, modify_graph_op_device\n+from ._utils import flatten_args, modify_graph_op_device, validate_tensors_metadata\n \n \n __all__ = [\n@@ -32,7 +32,8 @@ class _RootArgPlaceholder:\n     Placeholder for model-level inputs.\n     \"\"\"\n \n-    pass\n+    def __init__(self, tensor):\n+        self.meta = tensor.to(\"meta\")\n \n \n class _RecvInfo:\n@@ -124,6 +125,7 @@ class _PipelineStageBase(ABC):\n             )\n \n         # Run time states\n+        self._outputs_meta: Optional[Tuple[torch.Tensor, ...]] = None\n         # map microbatch ID to list of forward tensor args\n         self.fwd_cache: Dict[int, Tuple[Any, List[torch.Tensor]]] = {}\n         # Current forward chunk id\n@@ -182,6 +184,25 @@ class _PipelineStageBase(ABC):\n         \"\"\"\n         return self.stage_index == self.num_stages - 1\n \n+    def _configure_outputs_meta(self, outputs_meta: Tuple[torch.Tensor, ...]):\n+        \"\"\"\n+        Track the output shapes/dtype of this stage since they determine the send operation(s) which must match\n+        recv operations of the next stage.  The next stage _will_ be freezing its recv buffers based on its initial\n+        configuration, so it's important to also freeze/validate the output side to avoid any send/recv mismatches\n+        which could show up as hangs, silent corruption, or other errors.\n+        \"\"\"\n+        assert (\n+            self._outputs_meta is None\n+        ), \"Attempting to reconfigure output_meta, which is not supported\"\n+        self._outputs_meta = tuple(outputs_meta)  # type: ignore[assignment]\n+\n+    def get_outputs_meta(self) -> Tuple[torch.Tensor, ...]:\n+        \"\"\"Get the output metadata (meta tensors) reprensenting the outputs of this stage\"\"\"\n+        assert (\n+            self._outputs_meta is not None\n+        ), \"Attempted to get_outputs_meta() without configuring output meta\"\n+        return self._outputs_meta\n+\n     def _create_grad_send_info(\n         self,\n         args_recv_info: Tuple,\n@@ -469,6 +491,8 @@ class _PipelineStageBase(ABC):\n             composite_args = self._retrieve_recv_activations()\n             composite_kwargs = {}\n \n+        self._validate_fwd_input(args, kwargs)\n+\n         # Compute forward\n         try:\n             output = self.forward_maybe_with_nosync(*composite_args, **composite_kwargs)\n@@ -504,6 +528,7 @@ class _PipelineStageBase(ABC):\n         logger.debug(\n             f\"{self.log_prefix} Forwarded chunk {self.fwd_chunk_id}, outputs: {map_debug_info(output)}\"  # noqa: G004\n         )\n+        self._validate_fwd_outputs(output_tuple)\n         self.fwd_chunk_id += 1\n         return output\n \n@@ -549,6 +574,39 @@ class _PipelineStageBase(ABC):\n         )\n         self.bwd_chunk_id += 1\n \n+    def _validate_fwd_input(self, args, kwargs):\n+        \"\"\"Raises a RuntimeError if shapes of input args/kwargs do not match the shapes configured for this stage.\"\"\"\n+\n+        if self.is_first:\n+            # TODO why is there a separate recv_info for each pipeline chunk?\n+            expected_args = self.args_recv_info[self.fwd_chunk_id]\n+        else:\n+            expected_args = tuple()\n+\n+        if len(kwargs):\n+            # TODO- need a mapping of kwarg to position in self.args_recv_info\n+            # without it, we just validate shapes for args and ignore kwargs\n+            expected_args = expected_args[: len(expected_args) - len(kwargs)]\n+\n+        # TODO- need a mapping of kwarg to position in self.args_recv_info\n+        # maybe it's impossible to tell whether the len mismatches because\n+        # (a) the user passed an extra arg or missed an arg\n+        # (b) the user did not pass a kwarg, which has a default value baked into expected_args\n+        expected_tensors_meta = [\n+            e.meta if isinstance(e, _RootArgPlaceholder) else e.buffer\n+            for e in expected_args\n+        ]\n+        validate_tensors_metadata(\"forward input args\", expected_tensors_meta, args)\n+\n+    def _validate_fwd_outputs(self, outputs: Tuple[torch.Tensor, ...]):\n+        \"\"\"Raises a RuntimeError if this stage produces an output of unexpected shape/dtype.\n+        Most likely, this could be cause either by incorrect user specification of output shapes, or becuase\n+        shape inference was done on the original model but then at runtime the model is wrapped with something like\n+        mixed precision which changes output dtype.\n+        \"\"\"\n+        expected_tensors_meta = self.get_outputs_meta()\n+        validate_tensors_metadata(\"forward outputs\", expected_tensors_meta, outputs)\n+\n \n class _PipelineStage(_PipelineStageBase):\n     def __init__(\n@@ -662,10 +720,11 @@ class _PipelineStage(_PipelineStageBase):\n             \"\"\"\n             Create a receive buffer for a placeholder.\n             \"\"\"\n+            example_value = placeholder.meta[\"val\"]\n             if arg_node.op == \"placeholder\":\n                 # This is a root level placeholder, thus an input argument to the entire model.\n                 # We are likely at stage 0, hence no need to create a receive buffer.\n-                return _RootArgPlaceholder()\n+                return _RootArgPlaceholder(example_value)\n \n             # Figure out the source stage of this input\n             while arg_node.target is operator.getitem:\n@@ -678,7 +737,6 @@ class _PipelineStage(_PipelineStageBase):\n             src_stage = self.get_stage_index_of_submod(arg_node.name)\n \n             # Create a receive buffer for this placeholder\n-            example_value = placeholder.meta[\"val\"]\n             logger.debug(\n                 f\"{self.log_prefix} \"  # noqa: G004\n                 f\"Creating recv buffer for input '{placeholder.name}' \"\n@@ -763,9 +821,21 @@ class _PipelineStage(_PipelineStageBase):\n                 if dst_rank is not None:\n                     dsts.append(dst_rank)\n \n+        output_node = self._get_output_node()\n+        output_vals: Tuple[torch.Tensor] = tuple(\n+            v.meta[\"val\"] for v in flatten_args(output_node.args)\n+        )\n+        self._configure_outputs_meta(output_vals)\n+\n         logger.debug(f\"{self.log_prefix} \" f\"Send info: {act_send_info}\")  # noqa: G004\n         return act_send_info\n \n+    def _get_output_node(self):\n+        output_nodes = [node for node in self.submod.graph.nodes if node.op == \"output\"]\n+        assert len(output_nodes) == 1\n+        output_node = output_nodes[0]\n+        return output_node\n+\n     def _create_grad_recv_info(\n         self,\n         act_send_info: Dict,\n@@ -775,9 +845,8 @@ class _PipelineStage(_PipelineStageBase):\n         \"\"\"\n         # Dict[output_index, _RecvInfo]\n         grad_recv_info: Dict[int, _RecvInfo] = {}\n-        output_nodes = [node for node in self.submod.graph.nodes if node.op == \"output\"]\n-        assert len(output_nodes) == 1\n-        output_node = output_nodes[0]\n+        output_node = self._get_output_node()\n+\n         # The output node may take multiple args, meaning the submod having multiple output values.\n         output_vals = flatten_args(output_node.args)\n \n@@ -1034,6 +1103,8 @@ class ManualPipelineStage(_PipelineStageBase):\n         else:\n             self.outputs = _create_empty_tensors(output_args, device)\n \n+        self._configure_outputs_meta(tuple(self.outputs))\n+\n         # these are the buffers used in backwards send/recv, they are allocated later\n         self.outputs_grad: List[torch.Tensor] = []\n \n@@ -1067,7 +1138,7 @@ class ManualPipelineStage(_PipelineStageBase):\n                 self.args_recv_info[chunk_id] = recv_infos\n             else:\n                 self.args_recv_info[chunk_id] = tuple(\n-                    [_RootArgPlaceholder() for _ in self.inputs]\n+                    [_RootArgPlaceholder(i) for i in self.inputs]\n                 )\n \n         # Send info during forward for each activation\n"
        },
        {
            "name": "_utils.py",
            "path": "torch/distributed/pipelining/_utils.py",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 6,
                    "new_start": 1,
                    "new_length": 6,
                    "hunk": "@@ -1,6 +1,6 @@\n # Copyright (c) Meta Platforms, Inc. and affiliates\n import logging\n-from typing import Dict, Optional\n+from typing import Dict, List, Optional, Tuple, Union\n \n import torch\n from torch import fx\n"
                },
                {
                    "old_start": 132,
                    "old_length": 3,
                    "new_start": 132,
                    "new_length": 35,
                    "hunk": "@@ -132,3 +132,35 @@ class QualnameMapMixin:\n             return self.tracer_qualname_map[name_before_split]\n         else:\n             return name_before_split\n+\n+\n+class PipeliningShapeError(RuntimeError):\n+    \"\"\"Shape mismatch between configured and runtime values.\"\"\"\n+\n+\n+def validate_tensor_metadata(desc, expected, given):\n+    if not expected.shape == given.shape:\n+        raise PipeliningShapeError(\n+            f\"{desc} has a shape mismatch: expected {expected.shape} actual {given.shape}\"\n+        )\n+    if not expected.dtype == given.dtype:\n+        raise PipeliningShapeError(\n+            f\"{desc} has a dtype mismatch: expected {expected.dtype} actual {given.dtype}\"\n+        )\n+    if not expected.stride() == given.stride():\n+        raise PipeliningShapeError(\n+            f\"{desc} has a stride mismatch: expected {expected.stride()} actual {given.stride()}\"\n+        )\n+\n+\n+def validate_tensors_metadata(\n+    desc,\n+    expected_tensors: Union[List[torch.Tensor], Tuple[torch.Tensor, ...]],\n+    actual_tensors: Union[List[torch.Tensor], Tuple[torch.Tensor, ...]],\n+):\n+    if len(expected_tensors) != len(actual_tensors):\n+        raise PipeliningShapeError(\n+            f\"Number of {desc} ({len(actual_tensors)}) does not match expected number ({len(expected_tensors)})\"\n+        )\n+    for i in range(len(expected_tensors)):\n+        validate_tensor_metadata(f\"{desc}[{i}]\", expected_tensors[i], actual_tensors[i])"
                }
            ],
            "whole_deleted": "-from typing import Dict, Optional\n",
            "whole_added": "+from typing import Dict, List, Optional, Tuple, Union\n+\n+\n+class PipeliningShapeError(RuntimeError):\n+    \"\"\"Shape mismatch between configured and runtime values.\"\"\"\n+\n+\n+def validate_tensor_metadata(desc, expected, given):\n+    if not expected.shape == given.shape:\n+        raise PipeliningShapeError(\n+            f\"{desc} has a shape mismatch: expected {expected.shape} actual {given.shape}\"\n+        )\n+    if not expected.dtype == given.dtype:\n+        raise PipeliningShapeError(\n+            f\"{desc} has a dtype mismatch: expected {expected.dtype} actual {given.dtype}\"\n+        )\n+    if not expected.stride() == given.stride():\n+        raise PipeliningShapeError(\n+            f\"{desc} has a stride mismatch: expected {expected.stride()} actual {given.stride()}\"\n+        )\n+\n+\n+def validate_tensors_metadata(\n+    desc,\n+    expected_tensors: Union[List[torch.Tensor], Tuple[torch.Tensor, ...]],\n+    actual_tensors: Union[List[torch.Tensor], Tuple[torch.Tensor, ...]],\n+):\n+    if len(expected_tensors) != len(actual_tensors):\n+        raise PipeliningShapeError(\n+            f\"Number of {desc} ({len(actual_tensors)}) does not match expected number ({len(expected_tensors)})\"\n+        )\n+    for i in range(len(expected_tensors)):\n+        validate_tensor_metadata(f\"{desc}[{i}]\", expected_tensors[i], actual_tensors[i])\n",
            "whole_hunk": "@@ -1,6 +1,6 @@\n # Copyright (c) Meta Platforms, Inc. and affiliates\n import logging\n-from typing import Dict, Optional\n+from typing import Dict, List, Optional, Tuple, Union\n \n import torch\n from torch import fx\n@@ -132,3 +132,35 @@ class QualnameMapMixin:\n             return self.tracer_qualname_map[name_before_split]\n         else:\n             return name_before_split\n+\n+\n+class PipeliningShapeError(RuntimeError):\n+    \"\"\"Shape mismatch between configured and runtime values.\"\"\"\n+\n+\n+def validate_tensor_metadata(desc, expected, given):\n+    if not expected.shape == given.shape:\n+        raise PipeliningShapeError(\n+            f\"{desc} has a shape mismatch: expected {expected.shape} actual {given.shape}\"\n+        )\n+    if not expected.dtype == given.dtype:\n+        raise PipeliningShapeError(\n+            f\"{desc} has a dtype mismatch: expected {expected.dtype} actual {given.dtype}\"\n+        )\n+    if not expected.stride() == given.stride():\n+        raise PipeliningShapeError(\n+            f\"{desc} has a stride mismatch: expected {expected.stride()} actual {given.stride()}\"\n+        )\n+\n+\n+def validate_tensors_metadata(\n+    desc,\n+    expected_tensors: Union[List[torch.Tensor], Tuple[torch.Tensor, ...]],\n+    actual_tensors: Union[List[torch.Tensor], Tuple[torch.Tensor, ...]],\n+):\n+    if len(expected_tensors) != len(actual_tensors):\n+        raise PipeliningShapeError(\n+            f\"Number of {desc} ({len(actual_tensors)}) does not match expected number ({len(expected_tensors)})\"\n+        )\n+    for i in range(len(expected_tensors)):\n+        validate_tensor_metadata(f\"{desc}[{i}]\", expected_tensors[i], actual_tensors[i])"
        }
    ]
},
{
    "Id": 364,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/362bc6d7cbcac57466a52701fac3ba3bfb668000",
    "date": "2023-12-27T01:22:49+00:00",
    "message": "Fixed a segfault issue when passing an empty kernel to quantized_max_\u2026 (#116342)\n\n\u2026pool1d.\n\nFixes #116323.\n\nReused the same check as for `max_pool1d`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116342\nApproved by: https://github.com/jerryzh168",
    "label": "YES",
    "changes": [
        {
            "name": "MaxPooling.cpp",
            "path": "aten/src/ATen/native/MaxPooling.cpp",
            "patches": [
                {
                    "old_start": 5,
                    "old_length": 7,
                    "new_start": 5,
                    "new_length": 6,
                    "hunk": "@@ -5,7 +5,6 @@\n #include <ATen/core/grad_mode.h>\n #include <ATen/native/DispatchStub.h>\n #include <ATen/native/MaxPooling.h>\n-#include <ATen/native/Pool.h>\n \n #ifndef AT_PER_OPERATOR_HEADERS\n #include <ATen/Functions.h>\n"
                },
                {
                    "old_start": 23,
                    "old_length": 64,
                    "new_start": 22,
                    "new_length": 6,
                    "hunk": "@@ -23,64 +22,6 @@ DEFINE_DISPATCH(max_pool1d_stub);\n \n namespace {\n \n-static void check_max_pool1d(\n-    const Tensor& self,\n-    IntArrayRef kernel_size,\n-    IntArrayRef stride,\n-    IntArrayRef padding,\n-    IntArrayRef dilation,\n-    bool ceil_mode) {\n-\n-  TORCH_CHECK(\n-      self.dim() == 2 || self.dim() == 3,\n-      \"max_pool1d() Expected 2D or 3D input tensor, but got \", self.sym_sizes());\n-  TORCH_CHECK(\n-      kernel_size.size() == 1,\n-      \"max_pool1d() kernel_size must be an int, list of ints or tuple of ints of size 1 but got size \",\n-      kernel_size.size());\n-  TORCH_CHECK(\n-      stride.empty() || stride.size() == 1,\n-      \"max_pool1d() stride must be None, an int, list of ints, or tuple of ints of size 1 but got size \",\n-      stride.size());\n-  TORCH_CHECK(\n-      padding.size() == 1,\n-      \"max_pool1d() padding must be an int, list of ints, or tuple of ints of size 1 but got size \",\n-      padding.size());\n-  TORCH_CHECK(\n-      dilation.size() == 1,\n-      \"max_pool1d() dilation must be an int, list of ints or tuple of ints of size 1 but got size \",\n-      dilation.size());\n-\n-  // If stride=None then set it to kernel_size\n-  if (stride.empty()) {\n-    stride = kernel_size;\n-  }\n-\n-  TORCH_CHECK(\n-      kernel_size[0] > 0,\n-      \"max_pool1d() kernel_size must be greater than zero, but got \",\n-      kernel_size[0]);\n-  TORCH_CHECK(\n-      stride[0] > 0, \"max_pool1d() stride must be greater than zero, but got \", stride[0]);\n-  TORCH_CHECK(\n-      padding[0] >= 0, \"max_pool1d() padding must be non-negative, but got \", padding[0]);\n-  TORCH_CHECK(\n-      padding[0] <= kernel_size[0] / 2,\n-      \"max_pool1d() padding should be at most half of kernel size, but got padding=\",\n-      padding[0],\n-      \" and kernel_size=\",\n-      kernel_size[0]);\n-  TORCH_CHECK(\n-      dilation[0] > 0, \"max_pool1d() dilation must be greater than zero, but got \", dilation[0]);\n-\n-  const int64_t OW = pooling_output_shape(self.sym_size(-1).guard_int(__FILE__, __LINE__), kernel_size[0], padding[0], stride[0], dilation[0], ceil_mode);\n-  TORCH_CHECK(OW > 0, \"max_pool1d() Invalid computed output size: \", OW);\n-}\n-\n-} // namespace\n-\n-namespace {\n-\n Tensor max_pool1d_impl(\n     const Tensor& self,\n     IntArrayRef kernel_size,\n"
                }
            ],
            "whole_deleted": "-#include <ATen/native/Pool.h>\n-static void check_max_pool1d(\n-    const Tensor& self,\n-    IntArrayRef kernel_size,\n-    IntArrayRef stride,\n-    IntArrayRef padding,\n-    IntArrayRef dilation,\n-    bool ceil_mode) {\n-\n-  TORCH_CHECK(\n-      self.dim() == 2 || self.dim() == 3,\n-      \"max_pool1d() Expected 2D or 3D input tensor, but got \", self.sym_sizes());\n-  TORCH_CHECK(\n-      kernel_size.size() == 1,\n-      \"max_pool1d() kernel_size must be an int, list of ints or tuple of ints of size 1 but got size \",\n-      kernel_size.size());\n-  TORCH_CHECK(\n-      stride.empty() || stride.size() == 1,\n-      \"max_pool1d() stride must be None, an int, list of ints, or tuple of ints of size 1 but got size \",\n-      stride.size());\n-  TORCH_CHECK(\n-      padding.size() == 1,\n-      \"max_pool1d() padding must be an int, list of ints, or tuple of ints of size 1 but got size \",\n-      padding.size());\n-  TORCH_CHECK(\n-      dilation.size() == 1,\n-      \"max_pool1d() dilation must be an int, list of ints or tuple of ints of size 1 but got size \",\n-      dilation.size());\n-\n-  // If stride=None then set it to kernel_size\n-  if (stride.empty()) {\n-    stride = kernel_size;\n-  }\n-\n-  TORCH_CHECK(\n-      kernel_size[0] > 0,\n-      \"max_pool1d() kernel_size must be greater than zero, but got \",\n-      kernel_size[0]);\n-  TORCH_CHECK(\n-      stride[0] > 0, \"max_pool1d() stride must be greater than zero, but got \", stride[0]);\n-  TORCH_CHECK(\n-      padding[0] >= 0, \"max_pool1d() padding must be non-negative, but got \", padding[0]);\n-  TORCH_CHECK(\n-      padding[0] <= kernel_size[0] / 2,\n-      \"max_pool1d() padding should be at most half of kernel size, but got padding=\",\n-      padding[0],\n-      \" and kernel_size=\",\n-      kernel_size[0]);\n-  TORCH_CHECK(\n-      dilation[0] > 0, \"max_pool1d() dilation must be greater than zero, but got \", dilation[0]);\n-\n-  const int64_t OW = pooling_output_shape(self.sym_size(-1).guard_int(__FILE__, __LINE__), kernel_size[0], padding[0], stride[0], dilation[0], ceil_mode);\n-  TORCH_CHECK(OW > 0, \"max_pool1d() Invalid computed output size: \", OW);\n-}\n-\n-} // namespace\n-\n-namespace {\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -5,7 +5,6 @@\n #include <ATen/core/grad_mode.h>\n #include <ATen/native/DispatchStub.h>\n #include <ATen/native/MaxPooling.h>\n-#include <ATen/native/Pool.h>\n \n #ifndef AT_PER_OPERATOR_HEADERS\n #include <ATen/Functions.h>\n@@ -23,64 +22,6 @@ DEFINE_DISPATCH(max_pool1d_stub);\n \n namespace {\n \n-static void check_max_pool1d(\n-    const Tensor& self,\n-    IntArrayRef kernel_size,\n-    IntArrayRef stride,\n-    IntArrayRef padding,\n-    IntArrayRef dilation,\n-    bool ceil_mode) {\n-\n-  TORCH_CHECK(\n-      self.dim() == 2 || self.dim() == 3,\n-      \"max_pool1d() Expected 2D or 3D input tensor, but got \", self.sym_sizes());\n-  TORCH_CHECK(\n-      kernel_size.size() == 1,\n-      \"max_pool1d() kernel_size must be an int, list of ints or tuple of ints of size 1 but got size \",\n-      kernel_size.size());\n-  TORCH_CHECK(\n-      stride.empty() || stride.size() == 1,\n-      \"max_pool1d() stride must be None, an int, list of ints, or tuple of ints of size 1 but got size \",\n-      stride.size());\n-  TORCH_CHECK(\n-      padding.size() == 1,\n-      \"max_pool1d() padding must be an int, list of ints, or tuple of ints of size 1 but got size \",\n-      padding.size());\n-  TORCH_CHECK(\n-      dilation.size() == 1,\n-      \"max_pool1d() dilation must be an int, list of ints or tuple of ints of size 1 but got size \",\n-      dilation.size());\n-\n-  // If stride=None then set it to kernel_size\n-  if (stride.empty()) {\n-    stride = kernel_size;\n-  }\n-\n-  TORCH_CHECK(\n-      kernel_size[0] > 0,\n-      \"max_pool1d() kernel_size must be greater than zero, but got \",\n-      kernel_size[0]);\n-  TORCH_CHECK(\n-      stride[0] > 0, \"max_pool1d() stride must be greater than zero, but got \", stride[0]);\n-  TORCH_CHECK(\n-      padding[0] >= 0, \"max_pool1d() padding must be non-negative, but got \", padding[0]);\n-  TORCH_CHECK(\n-      padding[0] <= kernel_size[0] / 2,\n-      \"max_pool1d() padding should be at most half of kernel size, but got padding=\",\n-      padding[0],\n-      \" and kernel_size=\",\n-      kernel_size[0]);\n-  TORCH_CHECK(\n-      dilation[0] > 0, \"max_pool1d() dilation must be greater than zero, but got \", dilation[0]);\n-\n-  const int64_t OW = pooling_output_shape(self.sym_size(-1).guard_int(__FILE__, __LINE__), kernel_size[0], padding[0], stride[0], dilation[0], ceil_mode);\n-  TORCH_CHECK(OW > 0, \"max_pool1d() Invalid computed output size: \", OW);\n-}\n-\n-} // namespace\n-\n-namespace {\n-\n Tensor max_pool1d_impl(\n     const Tensor& self,\n     IntArrayRef kernel_size,\n"
        },
        {
            "name": "MaxPooling.h",
            "path": "aten/src/ATen/native/MaxPooling.h",
            "patches": [
                {
                    "old_start": 3,
                    "old_length": 9,
                    "new_start": 3,
                    "new_length": 64,
                    "hunk": "@@ -3,9 +3,64 @@\n #include <ATen/core/Tensor.h>\n #include <ATen/Parallel.h>\n #include <ATen/native/DispatchStub.h>\n+#include <ATen/native/Pool.h>\n \n namespace at::native {\n \n+static void check_max_pool1d(\n+    const Tensor& self,\n+    IntArrayRef kernel_size,\n+    IntArrayRef stride,\n+    IntArrayRef padding,\n+    IntArrayRef dilation,\n+    bool ceil_mode) {\n+\n+  TORCH_CHECK(\n+      self.dim() == 2 || self.dim() == 3,\n+      \"max_pool1d() Expected 2D or 3D input tensor, but got \", self.sym_sizes());\n+  TORCH_CHECK(\n+      kernel_size.size() == 1,\n+      \"max_pool1d() kernel_size must be an int, list of ints or tuple of ints of size 1 but got size \",\n+      kernel_size.size());\n+  TORCH_CHECK(\n+      stride.empty() || stride.size() == 1,\n+      \"max_pool1d() stride must be None, an int, list of ints, or tuple of ints of size 1 but got size \",\n+      stride.size());\n+  TORCH_CHECK(\n+      padding.size() == 1,\n+      \"max_pool1d() padding must be an int, list of ints, or tuple of ints of size 1 but got size \",\n+      padding.size());\n+  TORCH_CHECK(\n+      dilation.size() == 1,\n+      \"max_pool1d() dilation must be an int, list of ints or tuple of ints of size 1 but got size \",\n+      dilation.size());\n+\n+  // If stride=None then set it to kernel_size\n+  if (stride.empty()) {\n+    stride = kernel_size;\n+  }\n+\n+  TORCH_CHECK(\n+      kernel_size[0] > 0,\n+      \"max_pool1d() kernel_size must be greater than zero, but got \",\n+      kernel_size[0]);\n+  TORCH_CHECK(\n+      stride[0] > 0, \"max_pool1d() stride must be greater than zero, but got \", stride[0]);\n+  TORCH_CHECK(\n+      padding[0] >= 0, \"max_pool1d() padding must be non-negative, but got \", padding[0]);\n+  TORCH_CHECK(\n+      padding[0] <= kernel_size[0] / 2,\n+      \"max_pool1d() padding should be at most half of kernel size, but got padding=\",\n+      padding[0],\n+      \" and kernel_size=\",\n+      kernel_size[0]);\n+  TORCH_CHECK(\n+      dilation[0] > 0, \"max_pool1d() dilation must be greater than zero, but got \", dilation[0]);\n+\n+  const int64_t OW = pooling_output_shape(self.sym_size(-1).guard_int(__FILE__, __LINE__), kernel_size[0], padding[0], stride[0], dilation[0], ceil_mode);\n+  TORCH_CHECK(OW > 0, \"max_pool1d() Invalid computed output size: \", OW);\n+}\n+\n // TODO(Heitor) Template by dimension\n struct PoolingParams1D {\n   int64_t NB; // Number of batches\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include <ATen/native/Pool.h>\n+static void check_max_pool1d(\n+    const Tensor& self,\n+    IntArrayRef kernel_size,\n+    IntArrayRef stride,\n+    IntArrayRef padding,\n+    IntArrayRef dilation,\n+    bool ceil_mode) {\n+\n+  TORCH_CHECK(\n+      self.dim() == 2 || self.dim() == 3,\n+      \"max_pool1d() Expected 2D or 3D input tensor, but got \", self.sym_sizes());\n+  TORCH_CHECK(\n+      kernel_size.size() == 1,\n+      \"max_pool1d() kernel_size must be an int, list of ints or tuple of ints of size 1 but got size \",\n+      kernel_size.size());\n+  TORCH_CHECK(\n+      stride.empty() || stride.size() == 1,\n+      \"max_pool1d() stride must be None, an int, list of ints, or tuple of ints of size 1 but got size \",\n+      stride.size());\n+  TORCH_CHECK(\n+      padding.size() == 1,\n+      \"max_pool1d() padding must be an int, list of ints, or tuple of ints of size 1 but got size \",\n+      padding.size());\n+  TORCH_CHECK(\n+      dilation.size() == 1,\n+      \"max_pool1d() dilation must be an int, list of ints or tuple of ints of size 1 but got size \",\n+      dilation.size());\n+\n+  // If stride=None then set it to kernel_size\n+  if (stride.empty()) {\n+    stride = kernel_size;\n+  }\n+\n+  TORCH_CHECK(\n+      kernel_size[0] > 0,\n+      \"max_pool1d() kernel_size must be greater than zero, but got \",\n+      kernel_size[0]);\n+  TORCH_CHECK(\n+      stride[0] > 0, \"max_pool1d() stride must be greater than zero, but got \", stride[0]);\n+  TORCH_CHECK(\n+      padding[0] >= 0, \"max_pool1d() padding must be non-negative, but got \", padding[0]);\n+  TORCH_CHECK(\n+      padding[0] <= kernel_size[0] / 2,\n+      \"max_pool1d() padding should be at most half of kernel size, but got padding=\",\n+      padding[0],\n+      \" and kernel_size=\",\n+      kernel_size[0]);\n+  TORCH_CHECK(\n+      dilation[0] > 0, \"max_pool1d() dilation must be greater than zero, but got \", dilation[0]);\n+\n+  const int64_t OW = pooling_output_shape(self.sym_size(-1).guard_int(__FILE__, __LINE__), kernel_size[0], padding[0], stride[0], dilation[0], ceil_mode);\n+  TORCH_CHECK(OW > 0, \"max_pool1d() Invalid computed output size: \", OW);\n+}\n+\n",
            "whole_hunk": "@@ -3,9 +3,64 @@\n #include <ATen/core/Tensor.h>\n #include <ATen/Parallel.h>\n #include <ATen/native/DispatchStub.h>\n+#include <ATen/native/Pool.h>\n \n namespace at::native {\n \n+static void check_max_pool1d(\n+    const Tensor& self,\n+    IntArrayRef kernel_size,\n+    IntArrayRef stride,\n+    IntArrayRef padding,\n+    IntArrayRef dilation,\n+    bool ceil_mode) {\n+\n+  TORCH_CHECK(\n+      self.dim() == 2 || self.dim() == 3,\n+      \"max_pool1d() Expected 2D or 3D input tensor, but got \", self.sym_sizes());\n+  TORCH_CHECK(\n+      kernel_size.size() == 1,\n+      \"max_pool1d() kernel_size must be an int, list of ints or tuple of ints of size 1 but got size \",\n+      kernel_size.size());\n+  TORCH_CHECK(\n+      stride.empty() || stride.size() == 1,\n+      \"max_pool1d() stride must be None, an int, list of ints, or tuple of ints of size 1 but got size \",\n+      stride.size());\n+  TORCH_CHECK(\n+      padding.size() == 1,\n+      \"max_pool1d() padding must be an int, list of ints, or tuple of ints of size 1 but got size \",\n+      padding.size());\n+  TORCH_CHECK(\n+      dilation.size() == 1,\n+      \"max_pool1d() dilation must be an int, list of ints or tuple of ints of size 1 but got size \",\n+      dilation.size());\n+\n+  // If stride=None then set it to kernel_size\n+  if (stride.empty()) {\n+    stride = kernel_size;\n+  }\n+\n+  TORCH_CHECK(\n+      kernel_size[0] > 0,\n+      \"max_pool1d() kernel_size must be greater than zero, but got \",\n+      kernel_size[0]);\n+  TORCH_CHECK(\n+      stride[0] > 0, \"max_pool1d() stride must be greater than zero, but got \", stride[0]);\n+  TORCH_CHECK(\n+      padding[0] >= 0, \"max_pool1d() padding must be non-negative, but got \", padding[0]);\n+  TORCH_CHECK(\n+      padding[0] <= kernel_size[0] / 2,\n+      \"max_pool1d() padding should be at most half of kernel size, but got padding=\",\n+      padding[0],\n+      \" and kernel_size=\",\n+      kernel_size[0]);\n+  TORCH_CHECK(\n+      dilation[0] > 0, \"max_pool1d() dilation must be greater than zero, but got \", dilation[0]);\n+\n+  const int64_t OW = pooling_output_shape(self.sym_size(-1).guard_int(__FILE__, __LINE__), kernel_size[0], padding[0], stride[0], dilation[0], ceil_mode);\n+  TORCH_CHECK(OW > 0, \"max_pool1d() Invalid computed output size: \", OW);\n+}\n+\n // TODO(Heitor) Template by dimension\n struct PoolingParams1D {\n   int64_t NB; // Number of batches\n"
        },
        {
            "name": "Pooling.cpp",
            "path": "aten/src/ATen/native/quantized/cpu/Pooling.cpp",
            "patches": [
                {
                    "old_start": 5,
                    "old_length": 6,
                    "new_start": 5,
                    "new_length": 7,
                    "hunk": "@@ -5,6 +5,7 @@\n #include <ATen/Parallel.h>\n #include <torch/library.h>\n #include <ATen/native/Pool.h>\n+#include <ATen/native/MaxPooling.h>\n #include <ATen/quantized/Quantizer.h>\n #include <ATen/native/quantized/cpu/QuantizedOps.h>\n #include <ATen/native/quantized/cpu/init_qnnpack.h>\n"
                },
                {
                    "old_start": 702,
                    "old_length": 6,
                    "new_start": 703,
                    "new_length": 7,
                    "hunk": "@@ -702,6 +703,7 @@ Tensor quantized_max_pool1d(\n     IntArrayRef padding,\n     IntArrayRef dilation,\n     bool ceil_mode) {\n+  check_max_pool1d(qx, kernel_size, stride, padding, dilation, ceil_mode);\n   // (C, L) -> (C, 1, L) => kSqueezeDim = 1\n   // (N, C, L) -> (N, C, 1, L) => kSqueezeDim = 2\n   const int32_t kSqueezeDim = qx.dim() - 1;\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include <ATen/native/MaxPooling.h>\n+  check_max_pool1d(qx, kernel_size, stride, padding, dilation, ceil_mode);\n",
            "whole_hunk": "@@ -5,6 +5,7 @@\n #include <ATen/Parallel.h>\n #include <torch/library.h>\n #include <ATen/native/Pool.h>\n+#include <ATen/native/MaxPooling.h>\n #include <ATen/quantized/Quantizer.h>\n #include <ATen/native/quantized/cpu/QuantizedOps.h>\n #include <ATen/native/quantized/cpu/init_qnnpack.h>\n@@ -702,6 +703,7 @@ Tensor quantized_max_pool1d(\n     IntArrayRef padding,\n     IntArrayRef dilation,\n     bool ceil_mode) {\n+  check_max_pool1d(qx, kernel_size, stride, padding, dilation, ceil_mode);\n   // (C, L) -> (C, 1, L) => kSqueezeDim = 1\n   // (N, C, L) -> (N, C, 1, L) => kSqueezeDim = 2\n   const int32_t kSqueezeDim = qx.dim() - 1;\n"
        },
        {
            "name": "test_pooling.py",
            "path": "test/nn/test_pooling.py",
            "patches": [
                {
                    "old_start": 378,
                    "old_length": 6,
                    "new_start": 378,
                    "new_length": 14,
                    "hunk": "@@ -378,6 +378,14 @@ class TestPoolingNN(NNTestCase):\n         with self.assertRaises(RuntimeError):\n             F.max_unpool3d(x, torch.zeros(x.shape, dtype=int), [1, 1])\n \n+    def test_quantized_max_pool1d_empty_kernel(self):\n+        # This used to segfault when called with an empty kernel\n+        # see https://github.com/pytorch/pytorch/issues/116323\n+        base = torch.randn(1)\n+        temp_tensor = torch.quantize_per_tensor(base, 0.1, 10, torch.quint2x4)\n+        with self.assertRaises(RuntimeError):\n+            torch.quantized_max_pool1d(temp_tensor, [])\n+\n class TestPoolingNNDeviceType(NNTestCase):\n     @onlyNativeDeviceTypes\n     @dtypes(torch.float, torch.double)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    def test_quantized_max_pool1d_empty_kernel(self):\n+        # This used to segfault when called with an empty kernel\n+        # see https://github.com/pytorch/pytorch/issues/116323\n+        base = torch.randn(1)\n+        temp_tensor = torch.quantize_per_tensor(base, 0.1, 10, torch.quint2x4)\n+        with self.assertRaises(RuntimeError):\n+            torch.quantized_max_pool1d(temp_tensor, [])\n+\n",
            "whole_hunk": "@@ -378,6 +378,14 @@ class TestPoolingNN(NNTestCase):\n         with self.assertRaises(RuntimeError):\n             F.max_unpool3d(x, torch.zeros(x.shape, dtype=int), [1, 1])\n \n+    def test_quantized_max_pool1d_empty_kernel(self):\n+        # This used to segfault when called with an empty kernel\n+        # see https://github.com/pytorch/pytorch/issues/116323\n+        base = torch.randn(1)\n+        temp_tensor = torch.quantize_per_tensor(base, 0.1, 10, torch.quint2x4)\n+        with self.assertRaises(RuntimeError):\n+            torch.quantized_max_pool1d(temp_tensor, [])\n+\n class TestPoolingNNDeviceType(NNTestCase):\n     @onlyNativeDeviceTypes\n     @dtypes(torch.float, torch.double)"
        }
    ]
},
{
    "Id": 184,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/43069c460e87ba77fd69d2193635179779328e27",
    "date": "2024-04-26T22:25:43+00:00",
    "message": "Correct check for Boolean list input type (#124899)\n\nSummary:\nThis diff fixes a bug in PyTorch where when creating a tensor from a List of booleans, PyTorch was throwing an error.\n\nThis fix resolves that issue. All credit goes to swolchok for identifying the root cause of the issue and suggesting this fix.\n\nTest Plan: Running our model end to end works as expected and no error occurs.\n\nDifferential Revision: D55990810\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/124899\nApproved by: https://github.com/zhxchen17",
    "label": "YES",
    "changes": [
        {
            "name": "register_special_ops.cpp",
            "path": "torch/csrc/jit/runtime/register_special_ops.cpp",
            "patches": [
                {
                    "old_start": 33,
                    "old_length": 7,
                    "new_start": 33,
                    "new_length": 7,
                    "hunk": "@@ -33,7 +33,7 @@ c10::AliasAnalysisKind aliasAnalysisConservative() {\n \n void checkListInputType(const c10::TypePtr& elem_type, bool empty_list) {\n   if (!elem_type->isSubtypeOf(*NumberType::get()) &&\n-      elem_type != BoolType::get()) {\n+      !elem_type->isSubtypeOf(*BoolType::get())) {\n     std::stringstream error;\n     error << \"Input must be of ints, floats, or bools, \"\n           << \"got \" << elem_type->repr_str();"
                }
            ],
            "whole_deleted": "-      elem_type != BoolType::get()) {\n",
            "whole_added": "+      !elem_type->isSubtypeOf(*BoolType::get())) {\n",
            "whole_hunk": "@@ -33,7 +33,7 @@ c10::AliasAnalysisKind aliasAnalysisConservative() {\n \n void checkListInputType(const c10::TypePtr& elem_type, bool empty_list) {\n   if (!elem_type->isSubtypeOf(*NumberType::get()) &&\n-      elem_type != BoolType::get()) {\n+      !elem_type->isSubtypeOf(*BoolType::get())) {\n     std::stringstream error;\n     error << \"Input must be of ints, floats, or bools, \"\n           << \"got \" << elem_type->repr_str();"
        }
    ]
},
{
    "Id": 73,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/40e8675fcbb233c98ec532607d5cd421ec850253",
    "date": "2024-06-21T21:40:23+00:00",
    "message": "[cuDNN] Graph-capturable cuDNN CTCLoss (#128271)\n\ncuDNN v8.x added a graph-capturable CTCLoss, which slots \"neatly\" into the `Tensor` variant\n\n~~WIP as cuDNN has a restriction on the max target length (255), but this is not checkable in the graph-capture case, so the UX around warnings/error-messages here might need to be tuned...~~\nCurrently checks restriction on max target length during warmup run(s), and bails out during capture if this constraint was violated during warmup.\n\nCC @ptrblck\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128271\nApproved by: https://github.com/ezyang",
    "label": "NO",
    "changes": [
        {
            "name": "Descriptors.h",
            "path": "aten/src/ATen/cudnn/Descriptors.h",
            "patches": [
                {
                    "old_start": 357,
                    "old_length": 6,
                    "new_start": 357,
                    "new_length": 15,
                    "hunk": "@@ -357,6 +357,15 @@ struct TORCH_CUDA_CPP_API CTCLossDescriptor\n     AT_CUDNN_CHECK(\n         cudnnSetCTCLossDescriptorEx(mut_desc(), datatype, normMode, gradMode));\n   }\n+  void set_v9(\n+      cudnnDataType_t datatype,\n+      cudnnLossNormalizationMode_t normMode,\n+      cudnnCTCGradMode_t gradMode,\n+      int maxLabelLength) {\n+    AT_CUDNN_CHECK(\n+        cudnnSetCTCLossDescriptor_v9(mut_desc(), datatype, normMode, gradMode, maxLabelLength));\n+  }\n+\n };\n \n struct TORCH_CUDA_CPP_API ActivationDescriptor\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  void set_v9(\n+      cudnnDataType_t datatype,\n+      cudnnLossNormalizationMode_t normMode,\n+      cudnnCTCGradMode_t gradMode,\n+      int maxLabelLength) {\n+    AT_CUDNN_CHECK(\n+        cudnnSetCTCLossDescriptor_v9(mut_desc(), datatype, normMode, gradMode, maxLabelLength));\n+  }\n+\n",
            "whole_hunk": "@@ -357,6 +357,15 @@ struct TORCH_CUDA_CPP_API CTCLossDescriptor\n     AT_CUDNN_CHECK(\n         cudnnSetCTCLossDescriptorEx(mut_desc(), datatype, normMode, gradMode));\n   }\n+  void set_v9(\n+      cudnnDataType_t datatype,\n+      cudnnLossNormalizationMode_t normMode,\n+      cudnnCTCGradMode_t gradMode,\n+      int maxLabelLength) {\n+    AT_CUDNN_CHECK(\n+        cudnnSetCTCLossDescriptor_v9(mut_desc(), datatype, normMode, gradMode, maxLabelLength));\n+  }\n+\n };\n \n struct TORCH_CUDA_CPP_API ActivationDescriptor\n"
        },
        {
            "name": "LossCTC.cpp",
            "path": "aten/src/ATen/native/LossCTC.cpp",
            "patches": [
                {
                    "old_start": 539,
                    "old_length": 8,
                    "new_start": 539,
                    "new_length": 13,
                    "hunk": "@@ -539,8 +539,13 @@ Tensor ctc_loss(const Tensor& log_probs_, const Tensor& targets, IntArrayRef inp\n \n // Convenience function accepting Tensors\n Tensor ctc_loss(const Tensor& log_probs, const Tensor& targets, const Tensor& input_lengths, const Tensor& target_lengths, int64_t BLANK, int64_t reduction, bool zero_infinity) {\n+  // we don't want to convert to IntArrayRef if we can dispatch to cuDNN (this allows graph-capturable ctc_loss)\n+  bool use_cudnn =\n+      (log_probs.device().type() == at::kCUDA) &&\n+      at::_use_cudnn_ctc_loss(\n+          log_probs, targets, input_lengths, target_lengths, BLANK);\n   if (at::areAnyTensorSubclassLike(\n-          {log_probs, targets, input_lengths, target_lengths})) {\n+          {log_probs, targets, input_lengths, target_lengths}) || use_cudnn) {\n     // Composite Compliant path for TensorSubclasses\n     return ctc_loss_impl(log_probs, targets, input_lengths, target_lengths, BLANK, reduction, zero_infinity);\n   }\n"
                }
            ],
            "whole_deleted": "-          {log_probs, targets, input_lengths, target_lengths})) {\n",
            "whole_added": "+  // we don't want to convert to IntArrayRef if we can dispatch to cuDNN (this allows graph-capturable ctc_loss)\n+  bool use_cudnn =\n+      (log_probs.device().type() == at::kCUDA) &&\n+      at::_use_cudnn_ctc_loss(\n+          log_probs, targets, input_lengths, target_lengths, BLANK);\n+          {log_probs, targets, input_lengths, target_lengths}) || use_cudnn) {\n",
            "whole_hunk": "@@ -539,8 +539,13 @@ Tensor ctc_loss(const Tensor& log_probs_, const Tensor& targets, IntArrayRef inp\n \n // Convenience function accepting Tensors\n Tensor ctc_loss(const Tensor& log_probs, const Tensor& targets, const Tensor& input_lengths, const Tensor& target_lengths, int64_t BLANK, int64_t reduction, bool zero_infinity) {\n+  // we don't want to convert to IntArrayRef if we can dispatch to cuDNN (this allows graph-capturable ctc_loss)\n+  bool use_cudnn =\n+      (log_probs.device().type() == at::kCUDA) &&\n+      at::_use_cudnn_ctc_loss(\n+          log_probs, targets, input_lengths, target_lengths, BLANK);\n   if (at::areAnyTensorSubclassLike(\n-          {log_probs, targets, input_lengths, target_lengths})) {\n+          {log_probs, targets, input_lengths, target_lengths}) || use_cudnn) {\n     // Composite Compliant path for TensorSubclasses\n     return ctc_loss_impl(log_probs, targets, input_lengths, target_lengths, BLANK, reduction, zero_infinity);\n   }\n"
        },
        {
            "name": "LossCTC.cpp",
            "path": "aten/src/ATen/native/cudnn/LossCTC.cpp",
            "patches": [
                {
                    "old_start": 2,
                    "old_length": 6,
                    "new_start": 2,
                    "new_length": 7,
                    "hunk": "@@ -2,6 +2,7 @@\n #include <ATen/Config.h>\n #include <ATen/core/Tensor.h>\n #include <ATen/cuda/CUDAConfig.h>\n+#include <ATen/cuda/CUDAGraphsUtils.cuh>\n #if AT_CUDNN_ENABLED()\n #include <ATen/cudnn/Descriptors.h>\n #endif\n"
                },
                {
                    "old_start": 80,
                    "old_length": 6,
                    "new_start": 81,
                    "new_length": 11,
                    "hunk": "@@ -80,6 +81,11 @@ std::tuple<Tensor, Tensor> _cudnn_ctc_loss_tensor(\n namespace at {\n namespace native {\n \n+namespace {\n+// \"cache\" whether we've previously failed the target lengths check\n+static bool tensor_failed_target_lengths_check = false;\n+} // namespace\n+\n bool _use_cudnn_ctc_loss(\n     const Tensor& log_probs,\n     const Tensor& targets,\n"
                },
                {
                    "old_start": 91,
                    "old_length": 9,
                    "new_start": 97,
                    "new_length": 8,
                    "hunk": "@@ -91,9 +97,8 @@ bool _use_cudnn_ctc_loss(\n   bool use_cudnn = ctx.userEnabledCuDNN() && (BLANK == 0) &&\n       (targets.dim() == 1) && (log_probs.scalar_type() == at::kFloat) &&\n       (targets.scalar_type() == at::kInt) &&\n-      (log_probs.device().type() == at::kCUDA) &&\n       (targets.device().type() == at::kCPU) && (targets.is_contiguous()) &&\n-      (log_probs.dim() == 3);\n+      (log_probs.device().type() == at::kCUDA) && (log_probs.dim() == 3);\n \n   if (use_cudnn) {\n     // we don't know that input_lengths and target_lengths have the same size\n"
                },
                {
                    "old_start": 118,
                    "old_length": 11,
                    "new_start": 123,
                    "new_length": 42,
                    "hunk": "@@ -118,11 +123,42 @@ bool _use_cudnn_ctc_loss_tensor(\n     const Tensor& input_lengths,\n     const Tensor& target_lengths,\n     int64_t BLANK) {\n-  Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());\n-  IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n-  return at::_use_cudnn_ctc_loss(log_probs, targets, il, tl, BLANK);\n+  auto& ctx = at::globalContext();\n+\n+  bool use_cudnn = ctx.userEnabledCuDNN() && (BLANK == 0) &&\n+      (targets.dim() == 1) && (log_probs.scalar_type() == at::kFloat) &&\n+      (targets.scalar_type() == at::kInt) &&\n+      (log_probs.device().type() == at::kCUDA) && (targets.is_contiguous()) &&\n+      (log_probs.dim() == 3) && (input_lengths.scalar_type() == at::kInt) &&\n+      (target_lengths.scalar_type() == at::kInt);\n+\n+  if (at::cuda::currentStreamCaptureStatus() == at::cuda::CaptureStatus::None) {\n+    Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n+    IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n+    for (const auto b : c10::irange(tl.size())) {\n+      // target length < 256 is documented, but we see illegal memory accesses\n+      // when target lengths > input lengths for CuDNN\n+      Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n+      Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n+      IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());\n+      IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n+      use_cudnn = use_cudnn && (tl[b] < 256) && (tl[b] <= il[b]);\n+      if (!use_cudnn) {\n+        tensor_failed_target_lengths_check = true;\n+        break;\n+      }\n+    }\n+  } else {\n+    use_cudnn = use_cudnn && !tensor_failed_target_lengths_check;\n+    if (tensor_failed_target_lengths_check) {\n+      TORCH_WARN(\n+          \"cuDNN max target length restriction < 256 cannot be checked during graph capture,\"\n+          \" but target length >= 256 was observed previously e.g., during warmup, so we\"\n+          \" presume it is unsafe to dispatch to cuDNN ctc_loss.\");\n+    }\n+  }\n+\n+  return use_cudnn;\n }\n \n std::tuple<Tensor, Tensor> _cudnn_ctc_loss(\n"
                },
                {
                    "old_start": 211,
                    "old_length": 19,
                    "new_start": 247,
                    "new_length": 92,
                    "hunk": "@@ -211,19 +247,92 @@ std::tuple<Tensor, Tensor> _cudnn_ctc_loss(\n }\n \n std::tuple<Tensor, Tensor> _cudnn_ctc_loss_tensor(\n-    const Tensor& log_probs,\n-    const Tensor& targets,\n+    const Tensor& log_probs_t,\n+    const Tensor& targets_t,\n     const Tensor& input_lengths,\n     const Tensor& target_lengths,\n     int64_t BLANK,\n     bool deterministic,\n     bool zero_infinity) {\n-  Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());\n-  IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n-  return at::_cudnn_ctc_loss(\n-      log_probs, targets, il, tl, BLANK, deterministic, zero_infinity);\n+  Tensor targets_t_ = targets_t;\n+  if (targets_t.device().type() == at::kCPU) {\n+    targets_t_ = targets_t.to(Device(at::kCUDA));\n+  }\n+  const CheckedFrom c = \"cudnn_ctc_loss\";\n+  const TensorArg log_probs{log_probs_t, \"log_probs\", 1};\n+  const TensorArg targets{targets_t_, \"targets\", 2};\n+  checkDim(c, log_probs, 3);\n+  checkScalarType(c, log_probs, kFloat);\n+  checkDim(c, targets, 1);\n+  checkScalarType(c, targets, kInt);\n+  checkContiguous(c, targets); // ?\n+  checkBackend(c, {*log_probs}, Backend::CUDA);\n+  checkBackend(c, {*targets}, Backend::CUDA);\n+  const auto batch_size = log_probs->size(1);\n+  int64_t input_lengths_size =\n+      input_lengths.sizes().size() ? input_lengths.size(0) : 1;\n+  int64_t target_lengths_size =\n+      target_lengths.sizes().size() ? target_lengths.size(0) : 1;\n+  TORCH_CHECK(\n+      input_lengths_size == batch_size,\n+      \"input_lengths needs to have size to match batch_size\");\n+  TORCH_CHECK(\n+      target_lengths_size == batch_size,\n+      \"target_lengths needs to have size to match batch_size\");\n+\n+  TORCH_CHECK(BLANK == 0, \"blank must be label 0 for cudnn_ctc_loss\");\n+  // checked in dispatch:\n+  // assert other conditions for cudnnCTCLoss: all label lengths <= 256\n+  // all input lengths = logprob.size(0)\n+\n+  const auto handle = getCudnnHandle();\n+\n+  const cudnnCTCLossAlgo_t algo =\n+      (deterministic ? CUDNN_CTC_LOSS_ALGO_DETERMINISTIC\n+                     : CUDNN_CTC_LOSS_ALGO_NON_DETERMINISTIC);\n+\n+  CTCLossDescriptor ctc_loss_desc;\n+\n+  // so the CuDNN gradient semantics have changed between 7.1 and 7.6,\n+  // this is CuDNN 7.6 only, see PyTorch 1.2 for older CuDNN.\n+  ctc_loss_desc.set_v9(\n+      CUDNN_DATA_FLOAT,\n+      CUDNN_LOSS_NORMALIZATION_SOFTMAX,\n+      CUDNN_CTC_SKIP_OOB_GRADIENTS,\n+      255);\n+  TensorDescriptor log_probs_desc{log_probs_t};\n+  Tensor grad = at::empty_like(log_probs_t, LEGACY_CONTIGUOUS_MEMORY_FORMAT);\n+  TensorDescriptor grad_desc{grad};\n+\n+  size_t workspace_size;\n+  AT_CUDNN_CHECK(cudnnGetCTCLossWorkspaceSize_v8(\n+      handle,\n+      algo,\n+      ctc_loss_desc.desc(),\n+      log_probs_desc.desc(),\n+      grad_desc.desc(),\n+      &workspace_size));\n+  Tensor workspace =\n+      at::empty(workspace_size, log_probs->options().dtype(kByte));\n+  Tensor costs = at::empty({log_probs->size(1)}, log_probs->options());\n+\n+  AT_CUDNN_CHECK(cudnnCTCLoss_v8(\n+      handle,\n+      algo,\n+      ctc_loss_desc.desc(),\n+      log_probs_desc.desc(),\n+      log_probs_t.data_ptr(),\n+      targets_t_.data_ptr<int>(),\n+      target_lengths.data_ptr<int>(),\n+      input_lengths.data_ptr<int>(),\n+      costs.data_ptr(),\n+      grad_desc.desc(),\n+      grad.data_ptr(),\n+      workspace_size,\n+      workspace.data_ptr()\n+\n+          ));\n+  return std::make_tuple(costs, grad);\n }\n \n } // namespace native\n"
                }
            ],
            "whole_deleted": "-      (log_probs.device().type() == at::kCUDA) &&\n-      (log_probs.dim() == 3);\n-  Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());\n-  IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n-  return at::_use_cudnn_ctc_loss(log_probs, targets, il, tl, BLANK);\n-    const Tensor& log_probs,\n-    const Tensor& targets,\n-  Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());\n-  IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n-  return at::_cudnn_ctc_loss(\n-      log_probs, targets, il, tl, BLANK, deterministic, zero_infinity);\n",
            "whole_added": "+#include <ATen/cuda/CUDAGraphsUtils.cuh>\n+namespace {\n+// \"cache\" whether we've previously failed the target lengths check\n+static bool tensor_failed_target_lengths_check = false;\n+} // namespace\n+\n+      (log_probs.device().type() == at::kCUDA) && (log_probs.dim() == 3);\n+  auto& ctx = at::globalContext();\n+\n+  bool use_cudnn = ctx.userEnabledCuDNN() && (BLANK == 0) &&\n+      (targets.dim() == 1) && (log_probs.scalar_type() == at::kFloat) &&\n+      (targets.scalar_type() == at::kInt) &&\n+      (log_probs.device().type() == at::kCUDA) && (targets.is_contiguous()) &&\n+      (log_probs.dim() == 3) && (input_lengths.scalar_type() == at::kInt) &&\n+      (target_lengths.scalar_type() == at::kInt);\n+\n+  if (at::cuda::currentStreamCaptureStatus() == at::cuda::CaptureStatus::None) {\n+    Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n+    IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n+    for (const auto b : c10::irange(tl.size())) {\n+      // target length < 256 is documented, but we see illegal memory accesses\n+      // when target lengths > input lengths for CuDNN\n+      Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n+      Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n+      IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());\n+      IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n+      use_cudnn = use_cudnn && (tl[b] < 256) && (tl[b] <= il[b]);\n+      if (!use_cudnn) {\n+        tensor_failed_target_lengths_check = true;\n+        break;\n+      }\n+    }\n+  } else {\n+    use_cudnn = use_cudnn && !tensor_failed_target_lengths_check;\n+    if (tensor_failed_target_lengths_check) {\n+      TORCH_WARN(\n+          \"cuDNN max target length restriction < 256 cannot be checked during graph capture,\"\n+          \" but target length >= 256 was observed previously e.g., during warmup, so we\"\n+          \" presume it is unsafe to dispatch to cuDNN ctc_loss.\");\n+    }\n+  }\n+\n+  return use_cudnn;\n+    const Tensor& log_probs_t,\n+    const Tensor& targets_t,\n+  Tensor targets_t_ = targets_t;\n+  if (targets_t.device().type() == at::kCPU) {\n+    targets_t_ = targets_t.to(Device(at::kCUDA));\n+  }\n+  const CheckedFrom c = \"cudnn_ctc_loss\";\n+  const TensorArg log_probs{log_probs_t, \"log_probs\", 1};\n+  const TensorArg targets{targets_t_, \"targets\", 2};\n+  checkDim(c, log_probs, 3);\n+  checkScalarType(c, log_probs, kFloat);\n+  checkDim(c, targets, 1);\n+  checkScalarType(c, targets, kInt);\n+  checkContiguous(c, targets); // ?\n+  checkBackend(c, {*log_probs}, Backend::CUDA);\n+  checkBackend(c, {*targets}, Backend::CUDA);\n+  const auto batch_size = log_probs->size(1);\n+  int64_t input_lengths_size =\n+      input_lengths.sizes().size() ? input_lengths.size(0) : 1;\n+  int64_t target_lengths_size =\n+      target_lengths.sizes().size() ? target_lengths.size(0) : 1;\n+  TORCH_CHECK(\n+      input_lengths_size == batch_size,\n+      \"input_lengths needs to have size to match batch_size\");\n+  TORCH_CHECK(\n+      target_lengths_size == batch_size,\n+      \"target_lengths needs to have size to match batch_size\");\n+\n+  TORCH_CHECK(BLANK == 0, \"blank must be label 0 for cudnn_ctc_loss\");\n+  // checked in dispatch:\n+  // assert other conditions for cudnnCTCLoss: all label lengths <= 256\n+  // all input lengths = logprob.size(0)\n+\n+  const auto handle = getCudnnHandle();\n+\n+  const cudnnCTCLossAlgo_t algo =\n+      (deterministic ? CUDNN_CTC_LOSS_ALGO_DETERMINISTIC\n+                     : CUDNN_CTC_LOSS_ALGO_NON_DETERMINISTIC);\n+\n+  CTCLossDescriptor ctc_loss_desc;\n+\n+  // so the CuDNN gradient semantics have changed between 7.1 and 7.6,\n+  // this is CuDNN 7.6 only, see PyTorch 1.2 for older CuDNN.\n+  ctc_loss_desc.set_v9(\n+      CUDNN_DATA_FLOAT,\n+      CUDNN_LOSS_NORMALIZATION_SOFTMAX,\n+      CUDNN_CTC_SKIP_OOB_GRADIENTS,\n+      255);\n+  TensorDescriptor log_probs_desc{log_probs_t};\n+  Tensor grad = at::empty_like(log_probs_t, LEGACY_CONTIGUOUS_MEMORY_FORMAT);\n+  TensorDescriptor grad_desc{grad};\n+\n+  size_t workspace_size;\n+  AT_CUDNN_CHECK(cudnnGetCTCLossWorkspaceSize_v8(\n+      handle,\n+      algo,\n+      ctc_loss_desc.desc(),\n+      log_probs_desc.desc(),\n+      grad_desc.desc(),\n+      &workspace_size));\n+  Tensor workspace =\n+      at::empty(workspace_size, log_probs->options().dtype(kByte));\n+  Tensor costs = at::empty({log_probs->size(1)}, log_probs->options());\n+\n+  AT_CUDNN_CHECK(cudnnCTCLoss_v8(\n+      handle,\n+      algo,\n+      ctc_loss_desc.desc(),\n+      log_probs_desc.desc(),\n+      log_probs_t.data_ptr(),\n+      targets_t_.data_ptr<int>(),\n+      target_lengths.data_ptr<int>(),\n+      input_lengths.data_ptr<int>(),\n+      costs.data_ptr(),\n+      grad_desc.desc(),\n+      grad.data_ptr(),\n+      workspace_size,\n+      workspace.data_ptr()\n+\n+          ));\n+  return std::make_tuple(costs, grad);\n",
            "whole_hunk": "@@ -2,6 +2,7 @@\n #include <ATen/Config.h>\n #include <ATen/core/Tensor.h>\n #include <ATen/cuda/CUDAConfig.h>\n+#include <ATen/cuda/CUDAGraphsUtils.cuh>\n #if AT_CUDNN_ENABLED()\n #include <ATen/cudnn/Descriptors.h>\n #endif\n@@ -80,6 +81,11 @@ std::tuple<Tensor, Tensor> _cudnn_ctc_loss_tensor(\n namespace at {\n namespace native {\n \n+namespace {\n+// \"cache\" whether we've previously failed the target lengths check\n+static bool tensor_failed_target_lengths_check = false;\n+} // namespace\n+\n bool _use_cudnn_ctc_loss(\n     const Tensor& log_probs,\n     const Tensor& targets,\n@@ -91,9 +97,8 @@ bool _use_cudnn_ctc_loss(\n   bool use_cudnn = ctx.userEnabledCuDNN() && (BLANK == 0) &&\n       (targets.dim() == 1) && (log_probs.scalar_type() == at::kFloat) &&\n       (targets.scalar_type() == at::kInt) &&\n-      (log_probs.device().type() == at::kCUDA) &&\n       (targets.device().type() == at::kCPU) && (targets.is_contiguous()) &&\n-      (log_probs.dim() == 3);\n+      (log_probs.device().type() == at::kCUDA) && (log_probs.dim() == 3);\n \n   if (use_cudnn) {\n     // we don't know that input_lengths and target_lengths have the same size\n@@ -118,11 +123,42 @@ bool _use_cudnn_ctc_loss_tensor(\n     const Tensor& input_lengths,\n     const Tensor& target_lengths,\n     int64_t BLANK) {\n-  Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());\n-  IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n-  return at::_use_cudnn_ctc_loss(log_probs, targets, il, tl, BLANK);\n+  auto& ctx = at::globalContext();\n+\n+  bool use_cudnn = ctx.userEnabledCuDNN() && (BLANK == 0) &&\n+      (targets.dim() == 1) && (log_probs.scalar_type() == at::kFloat) &&\n+      (targets.scalar_type() == at::kInt) &&\n+      (log_probs.device().type() == at::kCUDA) && (targets.is_contiguous()) &&\n+      (log_probs.dim() == 3) && (input_lengths.scalar_type() == at::kInt) &&\n+      (target_lengths.scalar_type() == at::kInt);\n+\n+  if (at::cuda::currentStreamCaptureStatus() == at::cuda::CaptureStatus::None) {\n+    Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n+    IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n+    for (const auto b : c10::irange(tl.size())) {\n+      // target length < 256 is documented, but we see illegal memory accesses\n+      // when target lengths > input lengths for CuDNN\n+      Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n+      Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n+      IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());\n+      IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n+      use_cudnn = use_cudnn && (tl[b] < 256) && (tl[b] <= il[b]);\n+      if (!use_cudnn) {\n+        tensor_failed_target_lengths_check = true;\n+        break;\n+      }\n+    }\n+  } else {\n+    use_cudnn = use_cudnn && !tensor_failed_target_lengths_check;\n+    if (tensor_failed_target_lengths_check) {\n+      TORCH_WARN(\n+          \"cuDNN max target length restriction < 256 cannot be checked during graph capture,\"\n+          \" but target length >= 256 was observed previously e.g., during warmup, so we\"\n+          \" presume it is unsafe to dispatch to cuDNN ctc_loss.\");\n+    }\n+  }\n+\n+  return use_cudnn;\n }\n \n std::tuple<Tensor, Tensor> _cudnn_ctc_loss(\n@@ -211,19 +247,92 @@ std::tuple<Tensor, Tensor> _cudnn_ctc_loss(\n }\n \n std::tuple<Tensor, Tensor> _cudnn_ctc_loss_tensor(\n-    const Tensor& log_probs,\n-    const Tensor& targets,\n+    const Tensor& log_probs_t,\n+    const Tensor& targets_t,\n     const Tensor& input_lengths,\n     const Tensor& target_lengths,\n     int64_t BLANK,\n     bool deterministic,\n     bool zero_infinity) {\n-  Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());\n-  IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n-  return at::_cudnn_ctc_loss(\n-      log_probs, targets, il, tl, BLANK, deterministic, zero_infinity);\n+  Tensor targets_t_ = targets_t;\n+  if (targets_t.device().type() == at::kCPU) {\n+    targets_t_ = targets_t.to(Device(at::kCUDA));\n+  }\n+  const CheckedFrom c = \"cudnn_ctc_loss\";\n+  const TensorArg log_probs{log_probs_t, \"log_probs\", 1};\n+  const TensorArg targets{targets_t_, \"targets\", 2};\n+  checkDim(c, log_probs, 3);\n+  checkScalarType(c, log_probs, kFloat);\n+  checkDim(c, targets, 1);\n+  checkScalarType(c, targets, kInt);\n+  checkContiguous(c, targets); // ?\n+  checkBackend(c, {*log_probs}, Backend::CUDA);\n+  checkBackend(c, {*targets}, Backend::CUDA);\n+  const auto batch_size = log_probs->size(1);\n+  int64_t input_lengths_size =\n+      input_lengths.sizes().size() ? input_lengths.size(0) : 1;\n+  int64_t target_lengths_size =\n+      target_lengths.sizes().size() ? target_lengths.size(0) : 1;\n+  TORCH_CHECK(\n+      input_lengths_size == batch_size,\n+      \"input_lengths needs to have size to match batch_size\");\n+  TORCH_CHECK(\n+      target_lengths_size == batch_size,\n+      \"target_lengths needs to have size to match batch_size\");\n+\n+  TORCH_CHECK(BLANK == 0, \"blank must be label 0 for cudnn_ctc_loss\");\n+  // checked in dispatch:\n+  // assert other conditions for cudnnCTCLoss: all label lengths <= 256\n+  // all input lengths = logprob.size(0)\n+\n+  const auto handle = getCudnnHandle();\n+\n+  const cudnnCTCLossAlgo_t algo =\n+      (deterministic ? CUDNN_CTC_LOSS_ALGO_DETERMINISTIC\n+                     : CUDNN_CTC_LOSS_ALGO_NON_DETERMINISTIC);\n+\n+  CTCLossDescriptor ctc_loss_desc;\n+\n+  // so the CuDNN gradient semantics have changed between 7.1 and 7.6,\n+  // this is CuDNN 7.6 only, see PyTorch 1.2 for older CuDNN.\n+  ctc_loss_desc.set_v9(\n+      CUDNN_DATA_FLOAT,\n+      CUDNN_LOSS_NORMALIZATION_SOFTMAX,\n+      CUDNN_CTC_SKIP_OOB_GRADIENTS,\n+      255);\n+  TensorDescriptor log_probs_desc{log_probs_t};\n+  Tensor grad = at::empty_like(log_probs_t, LEGACY_CONTIGUOUS_MEMORY_FORMAT);\n+  TensorDescriptor grad_desc{grad};\n+\n+  size_t workspace_size;\n+  AT_CUDNN_CHECK(cudnnGetCTCLossWorkspaceSize_v8(\n+      handle,\n+      algo,\n+      ctc_loss_desc.desc(),\n+      log_probs_desc.desc(),\n+      grad_desc.desc(),\n+      &workspace_size));\n+  Tensor workspace =\n+      at::empty(workspace_size, log_probs->options().dtype(kByte));\n+  Tensor costs = at::empty({log_probs->size(1)}, log_probs->options());\n+\n+  AT_CUDNN_CHECK(cudnnCTCLoss_v8(\n+      handle,\n+      algo,\n+      ctc_loss_desc.desc(),\n+      log_probs_desc.desc(),\n+      log_probs_t.data_ptr(),\n+      targets_t_.data_ptr<int>(),\n+      target_lengths.data_ptr<int>(),\n+      input_lengths.data_ptr<int>(),\n+      costs.data_ptr(),\n+      grad_desc.desc(),\n+      grad.data_ptr(),\n+      workspace_size,\n+      workspace.data_ptr()\n+\n+          ));\n+  return std::make_tuple(costs, grad);\n }\n \n } // namespace native\n"
        },
        {
            "name": "test_nn.py",
            "path": "test/test_nn.py",
            "patches": [
                {
                    "old_start": 11135,
                    "old_length": 6,
                    "new_start": 11135,
                    "new_length": 35,
                    "hunk": "@@ -11135,6 +11135,35 @@ class TestNNDeviceType(NNTestCase):\n         grad_cudnn, = torch.autograd.grad(loss_cudnn, log_probs, grad_out)\n         self.assertEqual(grad_cudnn, grad_native, atol=1e-4, rtol=0)\n \n+    @onlyCUDA\n+    @skipCUDAIfRocm(msg=\"skipped Cudnn test on ROCm\")\n+    @skipCUDAIfCudnnVersionLessThan(7600)\n+    def test_ctc_loss_cudnn_tensor(self, device):\n+        batch_size = 16\n+        input_length = 30\n+        num_labels = 101\n+        target_length = 15\n+        targets = torch.randint(1, num_labels, (batch_size * target_length,),\n+                                device='cuda', dtype=torch.long)\n+        log_probs = torch.log_softmax(torch.randn(input_length, batch_size, num_labels, device='cuda', dtype=torch.float), 2)\n+        log_probs.requires_grad_()\n+\n+        input_lengths = batch_size * [input_length]\n+        input_lengths = torch.linspace(start=15, end=input_length, steps=batch_size, dtype=torch.long, device='cuda')\n+        target_lengths = torch.tensor(batch_size * [target_length], dtype=torch.long, device='cuda')\n+        grad_out = torch.randn(batch_size, device='cuda', dtype=torch.float)\n+        with torch.backends.cudnn.flags(enabled=False):\n+            loss_native = torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, reduction='none')\n+            grad_native, = torch.autograd.grad(loss_native, log_probs, grad_out)\n+        loss_cudnn = torch.nn.functional.ctc_loss(log_probs,\n+                                                  targets.to('cuda', torch.int32),\n+                                                  input_lengths.to('cuda', torch.int32),\n+                                                  target_lengths.to('cuda', torch.int32),\n+                                                  reduction='none')\n+        self.assertTrue(\"Cudnn\" in str(loss_cudnn.grad_fn))\n+        grad_cudnn, = torch.autograd.grad(loss_cudnn, log_probs, grad_out)\n+        self.assertEqual(grad_cudnn, grad_native, atol=1e-4, rtol=0)\n+\n     @dtypesIfCUDA(torch.half, torch.float, torch.double)\n     @dtypes(torch.float)\n     @tf32_on_and_off(0.005)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    @onlyCUDA\n+    @skipCUDAIfRocm(msg=\"skipped Cudnn test on ROCm\")\n+    @skipCUDAIfCudnnVersionLessThan(7600)\n+    def test_ctc_loss_cudnn_tensor(self, device):\n+        batch_size = 16\n+        input_length = 30\n+        num_labels = 101\n+        target_length = 15\n+        targets = torch.randint(1, num_labels, (batch_size * target_length,),\n+                                device='cuda', dtype=torch.long)\n+        log_probs = torch.log_softmax(torch.randn(input_length, batch_size, num_labels, device='cuda', dtype=torch.float), 2)\n+        log_probs.requires_grad_()\n+\n+        input_lengths = batch_size * [input_length]\n+        input_lengths = torch.linspace(start=15, end=input_length, steps=batch_size, dtype=torch.long, device='cuda')\n+        target_lengths = torch.tensor(batch_size * [target_length], dtype=torch.long, device='cuda')\n+        grad_out = torch.randn(batch_size, device='cuda', dtype=torch.float)\n+        with torch.backends.cudnn.flags(enabled=False):\n+            loss_native = torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, reduction='none')\n+            grad_native, = torch.autograd.grad(loss_native, log_probs, grad_out)\n+        loss_cudnn = torch.nn.functional.ctc_loss(log_probs,\n+                                                  targets.to('cuda', torch.int32),\n+                                                  input_lengths.to('cuda', torch.int32),\n+                                                  target_lengths.to('cuda', torch.int32),\n+                                                  reduction='none')\n+        self.assertTrue(\"Cudnn\" in str(loss_cudnn.grad_fn))\n+        grad_cudnn, = torch.autograd.grad(loss_cudnn, log_probs, grad_out)\n+        self.assertEqual(grad_cudnn, grad_native, atol=1e-4, rtol=0)\n+\n",
            "whole_hunk": "@@ -11135,6 +11135,35 @@ class TestNNDeviceType(NNTestCase):\n         grad_cudnn, = torch.autograd.grad(loss_cudnn, log_probs, grad_out)\n         self.assertEqual(grad_cudnn, grad_native, atol=1e-4, rtol=0)\n \n+    @onlyCUDA\n+    @skipCUDAIfRocm(msg=\"skipped Cudnn test on ROCm\")\n+    @skipCUDAIfCudnnVersionLessThan(7600)\n+    def test_ctc_loss_cudnn_tensor(self, device):\n+        batch_size = 16\n+        input_length = 30\n+        num_labels = 101\n+        target_length = 15\n+        targets = torch.randint(1, num_labels, (batch_size * target_length,),\n+                                device='cuda', dtype=torch.long)\n+        log_probs = torch.log_softmax(torch.randn(input_length, batch_size, num_labels, device='cuda', dtype=torch.float), 2)\n+        log_probs.requires_grad_()\n+\n+        input_lengths = batch_size * [input_length]\n+        input_lengths = torch.linspace(start=15, end=input_length, steps=batch_size, dtype=torch.long, device='cuda')\n+        target_lengths = torch.tensor(batch_size * [target_length], dtype=torch.long, device='cuda')\n+        grad_out = torch.randn(batch_size, device='cuda', dtype=torch.float)\n+        with torch.backends.cudnn.flags(enabled=False):\n+            loss_native = torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, reduction='none')\n+            grad_native, = torch.autograd.grad(loss_native, log_probs, grad_out)\n+        loss_cudnn = torch.nn.functional.ctc_loss(log_probs,\n+                                                  targets.to('cuda', torch.int32),\n+                                                  input_lengths.to('cuda', torch.int32),\n+                                                  target_lengths.to('cuda', torch.int32),\n+                                                  reduction='none')\n+        self.assertTrue(\"Cudnn\" in str(loss_cudnn.grad_fn))\n+        grad_cudnn, = torch.autograd.grad(loss_cudnn, log_probs, grad_out)\n+        self.assertEqual(grad_cudnn, grad_native, atol=1e-4, rtol=0)\n+\n     @dtypesIfCUDA(torch.half, torch.float, torch.double)\n     @dtypes(torch.float)\n     @tf32_on_and_off(0.005)"
        }
    ]
},
{
    "Id": 220,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/8865425ff7f68a172f02963f2734e4d5b005cdba",
    "date": "2024-04-09T03:34:09+00:00",
    "message": "[minifier] Add config flag to ignore non-fp values (#123006)\n\nWhen minifying, the after-aot minifier ignores non-floating values by\ndefault but does check them when running the the initial graph dump step.\nThis means we may capture a graph that doesn't fail the tester and doesn't have\nany meaningful divergence.\n\nFor example, the derivative of `elu(x)` depends on `x > 0` so this value is\nsaved for backwards and so becomes a graph output. However, the difference\nbetween `FLT_MIN` and `0` in `x` is now enough to trigger an accuracy failure.\n\nI fix this by adding a config variable and environment variable to ignore these\nnon floating point values.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/123006\nApproved by: https://github.com/ezyang\nghstack dependencies: #123005",
    "label": "YES",
    "changes": [
        {
            "name": "config.py",
            "path": "torch/_dynamo/config.py",
            "patches": [
                {
                    "old_start": 187,
                    "old_length": 6,
                    "new_start": 187,
                    "new_length": 13,
                    "hunk": "@@ -187,6 +187,13 @@ repro_forward_only = os.environ.get(\"TORCHDYNAMO_REPRO_FORWARD_ONLY\") == \"1\"\n # [@compile_ignored: debug]\n repro_tolerance = 1e-3\n \n+\n+# Whether to ignore non-floating point values when checking accuracy.\n+# Checking accuracy of non-floating point values such as boolean tensors\n+# can lead to false positives.\n+# [@compile_ignored: debug]\n+repro_ignore_non_fp = os.environ.get(\"TORCHDYNAMO_REPRO_IGNORE_NON_FP\") == \"1\"\n+\n # If True, when testing if two models are the same, we will test them against\n # a third fp64 reference and only report a problem if the RMSE relative to the\n # fp64 is greater.  However, this will use more memory; you may disable this\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+\n+# Whether to ignore non-floating point values when checking accuracy.\n+# Checking accuracy of non-floating point values such as boolean tensors\n+# can lead to false positives.\n+# [@compile_ignored: debug]\n+repro_ignore_non_fp = os.environ.get(\"TORCHDYNAMO_REPRO_IGNORE_NON_FP\") == \"1\"\n+\n",
            "whole_hunk": "@@ -187,6 +187,13 @@ repro_forward_only = os.environ.get(\"TORCHDYNAMO_REPRO_FORWARD_ONLY\") == \"1\"\n # [@compile_ignored: debug]\n repro_tolerance = 1e-3\n \n+\n+# Whether to ignore non-floating point values when checking accuracy.\n+# Checking accuracy of non-floating point values such as boolean tensors\n+# can lead to false positives.\n+# [@compile_ignored: debug]\n+repro_ignore_non_fp = os.environ.get(\"TORCHDYNAMO_REPRO_IGNORE_NON_FP\") == \"1\"\n+\n # If True, when testing if two models are the same, we will test them against\n # a third fp64 reference and only report a problem if the RMSE relative to the\n # fp64 is greater.  However, this will use more memory; you may disable this\n"
        },
        {
            "name": "after_aot.py",
            "path": "torch/_dynamo/repro/after_aot.py",
            "patches": [
                {
                    "old_start": 143,
                    "old_length": 6,
                    "new_start": 143,
                    "new_length": 7,
                    "hunk": "@@ -143,6 +143,7 @@ def wrap_compiler_debug(unconfigured_compiler_fn, compiler_name: str):\n                     inner_compiled_fn,\n                     real_inputs,\n                     only_fwd=True,\n+                    ignore_non_fp=config.repro_ignore_non_fp,\n                 )\n \n                 if failed:\n"
                },
                {
                    "old_start": 709,
                    "old_length": 7,
                    "new_start": 710,
                    "new_length": 13,
                    "hunk": "@@ -709,7 +710,13 @@ def repro_run(options, mod, load_args):\n     if options.accuracy != \"\":\n         # We don't really respect --accuracy vs --strict-accuracy here, it\n         # seems counterintuitive\n-        if not same_two_models(mod, compiled, args, only_fwd=True):\n+        if not same_two_models(\n+            mod,\n+            compiled,\n+            args,\n+            only_fwd=True,\n+            ignore_non_fp=config.repro_ignore_non_fp,\n+        ):\n             raise AccuracyError(\"Bad accuracy detected\")\n     else:\n         need_sync = False\n"
                }
            ],
            "whole_deleted": "-        if not same_two_models(mod, compiled, args, only_fwd=True):\n",
            "whole_added": "+                    ignore_non_fp=config.repro_ignore_non_fp,\n+        if not same_two_models(\n+            mod,\n+            compiled,\n+            args,\n+            only_fwd=True,\n+            ignore_non_fp=config.repro_ignore_non_fp,\n+        ):\n",
            "whole_hunk": "@@ -143,6 +143,7 @@ def wrap_compiler_debug(unconfigured_compiler_fn, compiler_name: str):\n                     inner_compiled_fn,\n                     real_inputs,\n                     only_fwd=True,\n+                    ignore_non_fp=config.repro_ignore_non_fp,\n                 )\n \n                 if failed:\n@@ -709,7 +710,13 @@ def repro_run(options, mod, load_args):\n     if options.accuracy != \"\":\n         # We don't really respect --accuracy vs --strict-accuracy here, it\n         # seems counterintuitive\n-        if not same_two_models(mod, compiled, args, only_fwd=True):\n+        if not same_two_models(\n+            mod,\n+            compiled,\n+            args,\n+            only_fwd=True,\n+            ignore_non_fp=config.repro_ignore_non_fp,\n+        ):\n             raise AccuracyError(\"Bad accuracy detected\")\n     else:\n         need_sync = False\n"
        },
        {
            "name": "after_dynamo.py",
            "path": "torch/_dynamo/repro/after_dynamo.py",
            "patches": [
                {
                    "old_start": 46,
                    "old_length": 6,
                    "new_start": 46,
                    "new_length": 16,
                    "hunk": "@@ -46,6 +46,16 @@ use_buck = inductor_config.is_fbcode()\n # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n \n \n+def _accuracy_fails(gm, example_inputs, compiler_fn):\n+    return backend_accuracy_fails(\n+        gm,\n+        example_inputs,\n+        compiler_fn,\n+        only_fwd=config.repro_forward_only,\n+        ignore_non_fp=config.repro_ignore_non_fp,\n+    )\n+\n+\n def wrap_backend_debug(unconfigured_compiler_fn, compiler_name: str):\n     \"\"\"\n     A minifier decorator that wraps the TorchDynamo produced Fx graph modules.\n"
                },
                {
                    "old_start": 78,
                    "old_length": 7,
                    "new_start": 88,
                    "new_length": 7,
                    "hunk": "@@ -78,7 +88,7 @@ def wrap_backend_debug(unconfigured_compiler_fn, compiler_name: str):\n             if config.repro_level == 4:\n                 # Check Accuracy\n                 compiled_gm = compiler_fn(copy.deepcopy(gm), example_inputs)\n-                if backend_accuracy_fails(gm, example_inputs, compiler_fn):\n+                if _accuracy_fails(gm, example_inputs, compiler_fn):\n                     log.warning(\n                         \"Accuracy failed for the TorchDynamo produced graph. Creating script to minify the error.\"\n                     )\n"
                },
                {
                    "old_start": 304,
                    "old_length": 17,
                    "new_start": 314,
                    "new_length": 14,
                    "hunk": "@@ -304,17 +314,14 @@ def dynamo_accuracy_minifier_backend(gm, example_inputs, compiler_name):\n     gm.eval()\n \n     # Check Accuracy\n-    if backend_accuracy_fails(\n-        gm, example_inputs, compiler_fn, only_fwd=config.repro_forward_only\n-    ):\n+    if _accuracy_fails(gm, example_inputs, compiler_fn):\n         log.warning(\"Accuracy failed for the TorchDynamo produced graph\")\n         dump_state_fn = functools.partial(\n             dump_backend_state, compiler_name=compiler_name, check_accuracy=True\n         )\n         fails_fn = functools.partial(\n-            backend_accuracy_fails,\n+            _accuracy_fails,\n             compiler_fn=compiler_fn,\n-            only_fwd=config.repro_forward_only,\n         )\n         dump_state_fn(fx.GraphModule(gm, copy.deepcopy(gm.graph)), example_inputs)\n         minifier(\n"
                },
                {
                    "old_start": 424,
                    "old_length": 7,
                    "new_start": 431,
                    "new_length": 13,
                    "hunk": "@@ -424,7 +431,13 @@ def repro_run(options, mod, load_args):\n             # TODO: disable clone\n             args = run_load_args(options, mod, load_args)\n             assert same_two_models(mod, mod, args), \"Eager itself failed\"\n-            if not same_two_models(mod, opt_mod, args):\n+            if not same_two_models(\n+                mod,\n+                opt_mod,\n+                args,\n+                only_fwd=config.repro_forward_only,\n+                ignore_non_fp=config.repro_ignore_non_fp,\n+            ):\n                 raise AccuracyError(\"Dynamo failed\")\n     else:\n         with torch.cuda.amp.autocast(enabled=options.autocast):"
                }
            ],
            "whole_deleted": "-                if backend_accuracy_fails(gm, example_inputs, compiler_fn):\n-    if backend_accuracy_fails(\n-        gm, example_inputs, compiler_fn, only_fwd=config.repro_forward_only\n-    ):\n-            backend_accuracy_fails,\n-            only_fwd=config.repro_forward_only,\n-            if not same_two_models(mod, opt_mod, args):\n",
            "whole_added": "+def _accuracy_fails(gm, example_inputs, compiler_fn):\n+    return backend_accuracy_fails(\n+        gm,\n+        example_inputs,\n+        compiler_fn,\n+        only_fwd=config.repro_forward_only,\n+        ignore_non_fp=config.repro_ignore_non_fp,\n+    )\n+\n+\n+                if _accuracy_fails(gm, example_inputs, compiler_fn):\n+    if _accuracy_fails(gm, example_inputs, compiler_fn):\n+            _accuracy_fails,\n+            if not same_two_models(\n+                mod,\n+                opt_mod,\n+                args,\n+                only_fwd=config.repro_forward_only,\n+                ignore_non_fp=config.repro_ignore_non_fp,\n+            ):\n",
            "whole_hunk": "@@ -46,6 +46,16 @@ use_buck = inductor_config.is_fbcode()\n # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n \n \n+def _accuracy_fails(gm, example_inputs, compiler_fn):\n+    return backend_accuracy_fails(\n+        gm,\n+        example_inputs,\n+        compiler_fn,\n+        only_fwd=config.repro_forward_only,\n+        ignore_non_fp=config.repro_ignore_non_fp,\n+    )\n+\n+\n def wrap_backend_debug(unconfigured_compiler_fn, compiler_name: str):\n     \"\"\"\n     A minifier decorator that wraps the TorchDynamo produced Fx graph modules.\n@@ -78,7 +88,7 @@ def wrap_backend_debug(unconfigured_compiler_fn, compiler_name: str):\n             if config.repro_level == 4:\n                 # Check Accuracy\n                 compiled_gm = compiler_fn(copy.deepcopy(gm), example_inputs)\n-                if backend_accuracy_fails(gm, example_inputs, compiler_fn):\n+                if _accuracy_fails(gm, example_inputs, compiler_fn):\n                     log.warning(\n                         \"Accuracy failed for the TorchDynamo produced graph. Creating script to minify the error.\"\n                     )\n@@ -304,17 +314,14 @@ def dynamo_accuracy_minifier_backend(gm, example_inputs, compiler_name):\n     gm.eval()\n \n     # Check Accuracy\n-    if backend_accuracy_fails(\n-        gm, example_inputs, compiler_fn, only_fwd=config.repro_forward_only\n-    ):\n+    if _accuracy_fails(gm, example_inputs, compiler_fn):\n         log.warning(\"Accuracy failed for the TorchDynamo produced graph\")\n         dump_state_fn = functools.partial(\n             dump_backend_state, compiler_name=compiler_name, check_accuracy=True\n         )\n         fails_fn = functools.partial(\n-            backend_accuracy_fails,\n+            _accuracy_fails,\n             compiler_fn=compiler_fn,\n-            only_fwd=config.repro_forward_only,\n         )\n         dump_state_fn(fx.GraphModule(gm, copy.deepcopy(gm.graph)), example_inputs)\n         minifier(\n@@ -424,7 +431,13 @@ def repro_run(options, mod, load_args):\n             # TODO: disable clone\n             args = run_load_args(options, mod, load_args)\n             assert same_two_models(mod, mod, args), \"Eager itself failed\"\n-            if not same_two_models(mod, opt_mod, args):\n+            if not same_two_models(\n+                mod,\n+                opt_mod,\n+                args,\n+                only_fwd=config.repro_forward_only,\n+                ignore_non_fp=config.repro_ignore_non_fp,\n+            ):\n                 raise AccuracyError(\"Dynamo failed\")\n     else:\n         with torch.cuda.amp.autocast(enabled=options.autocast):"
        }
    ]
},
{
    "Id": 278,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/2ebf2c88baa4667d55eda92f4c8424db505af781",
    "date": "2024-02-28T00:37:33+00:00",
    "message": "Add test to check that COW inputs are not materialized (#119507)\n\nPart of #97856\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/119507\nApproved by: https://github.com/ezyang\nghstack dependencies: #120455",
    "label": "NO",
    "changes": [
        {
            "name": "test_ops.py",
            "path": "test/test_ops.py",
            "patches": [
                {
                    "old_start": 1539,
                    "old_length": 6,
                    "new_start": 1539,
                    "new_length": 56,
                    "hunk": "@@ -1539,6 +1539,56 @@ class TestCompositeCompliance(TestCase):\n             composite_compliance.check_forward_ad_formula(\n                 op.get_op(), args, kwargs, op.gradcheck_wrapper, self.assertEqual)\n \n+    @ops(op_db, allowed_dtypes=(torch.float,))\n+    def test_cow_input(self, device, dtype, op):\n+        samples = op.sample_inputs(device, dtype)\n+\n+        def is_strided_tensor(arg):\n+            return torch.is_tensor(arg) and arg.layout == torch.strided\n+\n+        for sample in samples:\n+            args_raw = [sample.input] + list(sample.args)\n+            kwargs = sample.kwargs\n+            args_copy = []\n+            args = []\n+\n+            # Convert strided tensor inputs to COW tensors\n+            for idx, arg in enumerate(args_raw):\n+                if is_strided_tensor(arg):\n+                    args_copy.append(arg.clone().detach())\n+                    args.append(torch._lazy_clone(arg))\n+                else:\n+                    if torch.is_tensor(arg):\n+                        args_copy.append(arg.clone().detach())\n+                    else:\n+                        args_copy.append(copy.deepcopy(arg))\n+                    args.append(arg)\n+\n+            res = op.get_op()(*args, **kwargs)\n+\n+            # Check that COW inputs remain COW after the op is executed\n+            for idx, arg in enumerate(args):\n+                if is_strided_tensor(arg):\n+                    is_cow = torch._C._is_cow_tensor(arg)\n+\n+                    if op.supports_cow_input_no_materialize:\n+                        self.assertTrue(\n+                            is_cow,\n+                            msg=(\n+                                f\"Argument {idx} unexpectedly materializes. \"\n+                                \"Either set `supports_cow_input_no_materialize=False` \"\n+                                \"in this operation's OpInfo or change the \"\n+                                \"implementation to avoid materialization.\"))\n+\n+                    if is_cow:\n+                        orig = args_copy[idx]\n+                        self.assertTrue(\n+                            torch.allclose(arg, orig, rtol=0, atol=0, equal_nan=True),\n+                            msg=(\n+                                f\"Argument {idx} avoided materialization, \"\n+                                \"but the operation mutated its data.\"\n+                            ))\n+\n     @ops(op_db, allowed_dtypes=(torch.float,))\n     def test_view_replay(self, device, dtype, op):\n         def _assert_match_metadata(a, b):\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    @ops(op_db, allowed_dtypes=(torch.float,))\n+    def test_cow_input(self, device, dtype, op):\n+        samples = op.sample_inputs(device, dtype)\n+\n+        def is_strided_tensor(arg):\n+            return torch.is_tensor(arg) and arg.layout == torch.strided\n+\n+        for sample in samples:\n+            args_raw = [sample.input] + list(sample.args)\n+            kwargs = sample.kwargs\n+            args_copy = []\n+            args = []\n+\n+            # Convert strided tensor inputs to COW tensors\n+            for idx, arg in enumerate(args_raw):\n+                if is_strided_tensor(arg):\n+                    args_copy.append(arg.clone().detach())\n+                    args.append(torch._lazy_clone(arg))\n+                else:\n+                    if torch.is_tensor(arg):\n+                        args_copy.append(arg.clone().detach())\n+                    else:\n+                        args_copy.append(copy.deepcopy(arg))\n+                    args.append(arg)\n+\n+            res = op.get_op()(*args, **kwargs)\n+\n+            # Check that COW inputs remain COW after the op is executed\n+            for idx, arg in enumerate(args):\n+                if is_strided_tensor(arg):\n+                    is_cow = torch._C._is_cow_tensor(arg)\n+\n+                    if op.supports_cow_input_no_materialize:\n+                        self.assertTrue(\n+                            is_cow,\n+                            msg=(\n+                                f\"Argument {idx} unexpectedly materializes. \"\n+                                \"Either set `supports_cow_input_no_materialize=False` \"\n+                                \"in this operation's OpInfo or change the \"\n+                                \"implementation to avoid materialization.\"))\n+\n+                    if is_cow:\n+                        orig = args_copy[idx]\n+                        self.assertTrue(\n+                            torch.allclose(arg, orig, rtol=0, atol=0, equal_nan=True),\n+                            msg=(\n+                                f\"Argument {idx} avoided materialization, \"\n+                                \"but the operation mutated its data.\"\n+                            ))\n+\n",
            "whole_hunk": "@@ -1539,6 +1539,56 @@ class TestCompositeCompliance(TestCase):\n             composite_compliance.check_forward_ad_formula(\n                 op.get_op(), args, kwargs, op.gradcheck_wrapper, self.assertEqual)\n \n+    @ops(op_db, allowed_dtypes=(torch.float,))\n+    def test_cow_input(self, device, dtype, op):\n+        samples = op.sample_inputs(device, dtype)\n+\n+        def is_strided_tensor(arg):\n+            return torch.is_tensor(arg) and arg.layout == torch.strided\n+\n+        for sample in samples:\n+            args_raw = [sample.input] + list(sample.args)\n+            kwargs = sample.kwargs\n+            args_copy = []\n+            args = []\n+\n+            # Convert strided tensor inputs to COW tensors\n+            for idx, arg in enumerate(args_raw):\n+                if is_strided_tensor(arg):\n+                    args_copy.append(arg.clone().detach())\n+                    args.append(torch._lazy_clone(arg))\n+                else:\n+                    if torch.is_tensor(arg):\n+                        args_copy.append(arg.clone().detach())\n+                    else:\n+                        args_copy.append(copy.deepcopy(arg))\n+                    args.append(arg)\n+\n+            res = op.get_op()(*args, **kwargs)\n+\n+            # Check that COW inputs remain COW after the op is executed\n+            for idx, arg in enumerate(args):\n+                if is_strided_tensor(arg):\n+                    is_cow = torch._C._is_cow_tensor(arg)\n+\n+                    if op.supports_cow_input_no_materialize:\n+                        self.assertTrue(\n+                            is_cow,\n+                            msg=(\n+                                f\"Argument {idx} unexpectedly materializes. \"\n+                                \"Either set `supports_cow_input_no_materialize=False` \"\n+                                \"in this operation's OpInfo or change the \"\n+                                \"implementation to avoid materialization.\"))\n+\n+                    if is_cow:\n+                        orig = args_copy[idx]\n+                        self.assertTrue(\n+                            torch.allclose(arg, orig, rtol=0, atol=0, equal_nan=True),\n+                            msg=(\n+                                f\"Argument {idx} avoided materialization, \"\n+                                \"but the operation mutated its data.\"\n+                            ))\n+\n     @ops(op_db, allowed_dtypes=(torch.float,))\n     def test_view_replay(self, device, dtype, op):\n         def _assert_match_metadata(a, b):\n"
        },
        {
            "name": "common_methods_invocations.py",
            "path": "torch/testing/_internal/common_methods_invocations.py",
            "patches": [
                {
                    "old_start": 10258,
                    "old_length": 6,
                    "new_start": 10258,
                    "new_length": 7,
                    "hunk": "@@ -10258,6 +10258,7 @@ op_db: List[OpInfo] = [\n            dtypes=floating_types_and(torch.float16, torch.bfloat16),\n            supports_out=False,\n            supports_autograd=False,\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_cauchy,\n            error_inputs_func=error_inputs_cauchy,\n            skips=(\n"
                },
                {
                    "old_start": 10288,
                    "old_length": 6,
                    "new_start": 10289,
                    "new_length": 7,
                    "hunk": "@@ -10288,6 +10289,7 @@ op_db: List[OpInfo] = [\n            dtypes=floating_types_and(torch.float16, torch.bfloat16),\n            supports_out=False,\n            supports_autograd=False,\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_exponential,\n            error_inputs_func=error_inputs_exponential,\n            skips=(\n"
                },
                {
                    "old_start": 10317,
                    "old_length": 6,
                    "new_start": 10319,
                    "new_length": 7,
                    "hunk": "@@ -10317,6 +10319,7 @@ op_db: List[OpInfo] = [\n            dtypes=floating_types_and(torch.float16, torch.bfloat16, torch.int8, torch.int16, torch.int32, torch.int64, torch.uint8),\n            supports_out=False,\n            supports_autograd=False,\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_geometric,\n            error_inputs_func=error_inputs_geometric,\n            skips=(\n"
                },
                {
                    "old_start": 10345,
                    "old_length": 6,
                    "new_start": 10348,
                    "new_length": 8,
                    "hunk": "@@ -10345,6 +10348,8 @@ op_db: List[OpInfo] = [\n            dtypes=floating_types_and(torch.float16, torch.bfloat16),\n            supports_out=False,\n            supports_autograd=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_log_normal,\n            error_inputs_func=error_inputs_log_normal,\n            skips=(\n"
                },
                {
                    "old_start": 10372,
                    "old_length": 6,
                    "new_start": 10377,
                    "new_length": 7,
                    "hunk": "@@ -10372,6 +10377,7 @@ op_db: List[OpInfo] = [\n            dtypes=floating_and_complex_types_and(torch.float16, torch.bfloat16),\n            supports_out=False,\n            supports_autograd=False,\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_normal,\n            error_inputs_func=error_inputs_normal,\n            skips=(\n"
                },
                {
                    "old_start": 10402,
                    "old_length": 6,
                    "new_start": 10408,
                    "new_length": 7,
                    "hunk": "@@ -10402,6 +10408,7 @@ op_db: List[OpInfo] = [\n            supports_out=False,\n            supports_autograd=False,\n            is_factory_function=False,\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_uniform,\n            error_inputs_func=error_inputs_uniform,\n            skips=(\n"
                },
                {
                    "old_start": 10687,
                    "old_length": 6,
                    "new_start": 10694,
                    "new_length": 8,
                    "hunk": "@@ -10687,6 +10694,8 @@ op_db: List[OpInfo] = [\n            # Reference: https://github.com/pytorch/pytorch/issues/50747\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            skips=(\n                # Reference: https://github.com/pytorch/pytorch/issues/50747\n                DecorateInfo(unittest.skip(\"Skipped!\"), 'TestCommon', 'test_variant_consistency_eager',\n"
                },
                {
                    "old_start": 11011,
                    "old_length": 6,
                    "new_start": 11020,
                    "new_length": 8,
                    "hunk": "@@ -11011,6 +11020,8 @@ op_db: List[OpInfo] = [\n            supports_out=False,\n            supports_gradgrad=False,\n            assert_autodiffed=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_cdist),\n     UnaryUfuncInfo('ceil',\n                    ref=np.ceil,\n"
                },
                {
                    "old_start": 11737,
                    "old_length": 6,
                    "new_start": 11748,
                    "new_length": 8,
                    "hunk": "@@ -11737,6 +11748,8 @@ op_db: List[OpInfo] = [\n     OpInfo('sparse.sampled_addmm',\n            dtypes=floating_and_complex_types(),\n            supports_autograd=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_sparse_sampled_addmm,\n            decorators=[\n                skipCUDAIf(not ((_get_torch_cuda_version() >= (11, 3))\n"
                },
                {
                    "old_start": 11785,
                    "old_length": 6,
                    "new_start": 11798,
                    "new_length": 8,
                    "hunk": "@@ -11785,6 +11798,8 @@ op_db: List[OpInfo] = [\n            supports_out=False,\n            supports_gradgrad=False,\n            supports_forward_ad=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_sparse_mm_reduce,\n            decorators=[onlyCPU],\n            skips=(\n"
                },
                {
                    "old_start": 12316,
                    "old_length": 6,
                    "new_start": 12331,
                    "new_length": 8,
                    "hunk": "@@ -12316,6 +12331,8 @@ op_db: List[OpInfo] = [\n            supports_fwgrad_bwgrad=True,\n            # https://github.com/pytorch/pytorch/issues/66357\n            check_batched_forward_grad=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            skips=(\n                # mexp does not support bf16 and fp16\n                DecorateInfo(unittest.skip('Skipped!'), 'TestInductorOpInfo', 'test_comprehensive',\n"
                },
                {
                    "old_start": 12723,
                    "old_length": 6,
                    "new_start": 12740,
                    "new_length": 8,
                    "hunk": "@@ -12723,6 +12740,8 @@ op_db: List[OpInfo] = [\n            assert_autodiffed=True,\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            supports_out=True),\n     OpInfo('softmax',\n            aliases=('special.softmax', 'nn.functional.softmax',),\n"
                },
                {
                    "old_start": 12744,
                    "old_length": 6,
                    "new_start": 12763,
                    "new_length": 8,
                    "hunk": "@@ -12744,6 +12763,8 @@ op_db: List[OpInfo] = [\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n         supports_out=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         skips=(\n             DecorateInfo(unittest.expectedFailure, 'TestCommon', 'test_noncontiguous_samples', device_type='cpu'),\n             DecorateInfo(unittest.expectedFailure, 'TestJit', 'test_variant_consistency_jit', dtypes=(torch.float32,)),\n"
                },
                {
                    "old_start": 12778,
                    "old_length": 6,
                    "new_start": 12799,
                    "new_length": 8,
                    "hunk": "@@ -12778,6 +12799,8 @@ op_db: List[OpInfo] = [\n         supports_out=False,\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         decorators=(\n             DecorateInfo(\n                 toleranceOverride({torch.float32: tol(atol=1e-5, rtol=1e-3)}),\n"
                },
                {
                    "old_start": 12901,
                    "old_length": 6,
                    "new_start": 12924,
                    "new_length": 8,
                    "hunk": "@@ -12901,6 +12924,8 @@ op_db: List[OpInfo] = [\n            supports_out=False,\n            assert_jit_shape_analysis=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_native_layer_norm,\n            error_inputs_func=error_inputs_native_layer_norm,\n            skips=(\n"
                },
                {
                    "old_start": 12919,
                    "old_length": 6,
                    "new_start": 12944,
                    "new_length": 8,
                    "hunk": "@@ -12919,6 +12944,8 @@ op_db: List[OpInfo] = [\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n            assert_jit_shape_analysis=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_native_batch_norm,\n            skips=(\n                # NotImplementedError: Could not run\n"
                },
                {
                    "old_start": 12946,
                    "old_length": 6,
                    "new_start": 12973,
                    "new_length": 8,
                    "hunk": "@@ -12946,6 +12973,8 @@ op_db: List[OpInfo] = [\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n            assert_jit_shape_analysis=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs__native_batch_norm_legit,\n            skips=(\n                # NotImplementedError: Could not run\n"
                },
                {
                    "old_start": 13109,
                    "old_length": 6,
                    "new_start": 13138,
                    "new_length": 8,
                    "hunk": "@@ -13109,6 +13138,8 @@ op_db: List[OpInfo] = [\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n         supports_out=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         dtypes=floating_types_and(torch.half, torch.bfloat16),\n         gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n         sample_inputs_func=sample_inputs_binary_cross_entropy_with_logits,\n"
                },
                {
                    "old_start": 13149,
                    "old_length": 6,
                    "new_start": 13180,
                    "new_length": 8,
                    "hunk": "@@ -13149,6 +13180,8 @@ op_db: List[OpInfo] = [\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n            assert_jit_shape_analysis=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n            decorators=(\n                DecorateInfo(\n"
                },
                {
                    "old_start": 13195,
                    "old_length": 6,
                    "new_start": 13228,
                    "new_length": 8,
                    "hunk": "@@ -13195,6 +13228,8 @@ op_db: List[OpInfo] = [\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n            assert_jit_shape_analysis=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n            decorators=[\n                DecorateInfo(\n"
                },
                {
                    "old_start": 13243,
                    "old_length": 6,
                    "new_start": 13278,
                    "new_length": 8,
                    "hunk": "@@ -13243,6 +13278,8 @@ op_db: List[OpInfo] = [\n            assert_jit_shape_analysis=True,\n            # Runs very slowly on slow-gradcheck - alternatively reduce input sizes\n            gradcheck_fast_mode=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n            decorators=[\n                DecorateInfo(\n"
                },
                {
                    "old_start": 13301,
                    "old_length": 6,
                    "new_start": 13338,
                    "new_length": 8,
                    "hunk": "@@ -13301,6 +13338,8 @@ op_db: List[OpInfo] = [\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n            assert_jit_shape_analysis=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n            decorators=(\n                DecorateInfo(\n"
                },
                {
                    "old_start": 13341,
                    "old_length": 6,
                    "new_start": 13380,
                    "new_length": 8,
                    "hunk": "@@ -13341,6 +13380,8 @@ op_db: List[OpInfo] = [\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n            assert_jit_shape_analysis=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            decorators=(\n                DecorateInfo(\n                    toleranceOverride({torch.chalf: tol(atol=6e-2, rtol=5e-2)}),\n"
                },
                {
                    "old_start": 13372,
                    "old_length": 6,
                    "new_start": 13413,
                    "new_length": 8,
                    "hunk": "@@ -13372,6 +13413,8 @@ op_db: List[OpInfo] = [\n            gradcheck_fast_mode=True,\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            decorators=(\n                DecorateInfo(\n                    toleranceOverride({torch.chalf: tol(atol=6e-2, rtol=5e-2)}),\n"
                },
                {
                    "old_start": 13414,
                    "old_length": 6,
                    "new_start": 13457,
                    "new_length": 8,
                    "hunk": "@@ -13414,6 +13457,8 @@ op_db: List[OpInfo] = [\n            supports_out=False,\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            error_inputs_func=error_inputs_group_norm,\n            decorators=[\n                # RuntimeError: Cannot insert a Tensor that requires grad as a constant.\n"
                },
                {
                    "old_start": 13429,
                    "old_length": 6,
                    "new_start": 13474,
                    "new_length": 8,
                    "hunk": "@@ -13429,6 +13474,8 @@ op_db: List[OpInfo] = [\n            supports_out=False,\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            decorators=[\n                # RuntimeError: Cannot insert a Tensor that requires grad as a constant.\n                # Consider making it a parameter or input, or detaching the gradient\n"
                },
                {
                    "old_start": 13448,
                    "old_length": 6,
                    "new_start": 13495,
                    "new_length": 8,
                    "hunk": "@@ -13448,6 +13495,8 @@ op_db: List[OpInfo] = [\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n            assert_jit_shape_analysis=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            decorators=[\n                DecorateInfo(\n                    toleranceOverride({torch.float32: tol(atol=1e-05, rtol=1e-03)}),\n"
                },
                {
                    "old_start": 13496,
                    "old_length": 6,
                    "new_start": 13545,
                    "new_length": 8,
                    "hunk": "@@ -13496,6 +13545,8 @@ op_db: List[OpInfo] = [\n            variant_test_name='reflect',\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            dtypes=all_types_and_complex_and(torch.bfloat16),\n            dtypesIfCUDA=all_types_and_complex_and(torch.half, torch.bfloat16),\n            sample_inputs_func=partial(sample_inputs_nn_pad, mode='reflect'),\n"
                },
                {
                    "old_start": 13511,
                    "old_length": 6,
                    "new_start": 13562,
                    "new_length": 8,
                    "hunk": "@@ -13511,6 +13562,8 @@ op_db: List[OpInfo] = [\n            variant_test_name='replicate',\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            dtypes=all_types_and_complex_and(torch.bfloat16),\n            dtypesIfCUDA=all_types_and_complex_and(torch.half, torch.bfloat16),\n            sample_inputs_func=partial(sample_inputs_nn_pad, mode='replicate'),\n"
                },
                {
                    "old_start": 13526,
                    "old_length": 6,
                    "new_start": 13579,
                    "new_length": 8,
                    "hunk": "@@ -13526,6 +13579,8 @@ op_db: List[OpInfo] = [\n            variant_test_name='replicate_negative',\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            dtypes=all_types_and_complex_and(torch.bfloat16),\n            dtypesIfCUDA=all_types_and_complex_and(torch.half, torch.bfloat16),\n            sample_inputs_func=sample_inputs_nn_pad_replicate_negative,\n"
                },
                {
                    "old_start": 13777,
                    "old_length": 6,
                    "new_start": 13832,
                    "new_length": 8,
                    "hunk": "@@ -13777,6 +13832,8 @@ op_db: List[OpInfo] = [\n         dtypesIfCUDA=floating_types_and(torch.bfloat16, torch.float16),\n         supports_out=False,\n         supports_gradgrad=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         sample_inputs_func=sample_inputs_multi_margin_loss,\n         reference_inputs_func=reference_inputs_multi_margin_loss,\n         error_inputs_func=error_inputs_multi_margin_loss,\n"
                },
                {
                    "old_start": 13794,
                    "old_length": 6,
                    "new_start": 13851,
                    "new_length": 8,
                    "hunk": "@@ -13794,6 +13851,8 @@ op_db: List[OpInfo] = [\n         dtypesIfCUDA=floating_types_and(torch.bfloat16, torch.float16),\n         supports_out=False,\n         supports_gradgrad=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         sample_inputs_func=sample_inputs_multilabel_margin_loss,\n         reference_inputs_func=reference_inputs_multilabel_margin_loss,\n         error_inputs_func=error_inputs_multilabel_margin_loss,\n"
                },
                {
                    "old_start": 13820,
                    "old_length": 6,
                    "new_start": 13879,
                    "new_length": 8,
                    "hunk": "@@ -13820,6 +13879,8 @@ op_db: List[OpInfo] = [\n         sample_inputs_func=sample_inputs_multilabel_soft_margin_loss,\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         decorators=(\n             DecorateInfo(\n                 toleranceOverride({torch.float32: tol(atol=1e-4, rtol=1e-4)}),\n"
                },
                {
                    "old_start": 13864,
                    "old_length": 6,
                    "new_start": 13925,
                    "new_length": 8,
                    "hunk": "@@ -13864,6 +13925,8 @@ op_db: List[OpInfo] = [\n            check_batched_forward_grad=False,\n            dtypes=floating_types_and(torch.bfloat16, torch.float16),\n            test_neg_view=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_fractional_max_pool2d,\n            decorators=(\n                # FIXME: AssertionError: False is not true : Tensors failed to compare as equal!\n"
                },
                {
                    "old_start": 13878,
                    "old_length": 6,
                    "new_start": 13941,
                    "new_length": 8,
                    "hunk": "@@ -13878,6 +13941,8 @@ op_db: List[OpInfo] = [\n            supports_out=False,\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            op=lambda input, *args, **kwargs:\n                wrapper_set_seed(torch.nn.functional.fractional_max_pool3d, input, *args, **kwargs),\n            # vmap does not support random operations\n"
                },
                {
                    "old_start": 14087,
                    "old_length": 6,
                    "new_start": 14152,
                    "new_length": 8,
                    "hunk": "@@ -14087,6 +14152,8 @@ op_db: List[OpInfo] = [\n            aten_name='linear',\n            supports_autograd=True,\n            supports_gradgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_linear,\n            dtypes=all_types_and_complex_and(torch.float16, torch.bfloat16),\n            dtypesIfROCM=floating_and_complex_types_and(torch.float16, torch.bfloat16),\n"
                },
                {
                    "old_start": 14174,
                    "old_length": 6,
                    "new_start": 14241,
                    "new_length": 8,
                    "hunk": "@@ -14174,6 +14241,8 @@ op_db: List[OpInfo] = [\n         assert_autodiffed=False,\n         supports_gradgrad=True,\n         supports_out=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         # test_reference_numerics only tests the case when the weight tensor is a scalar\n         sample_kwargs=sample_kwargs_prelu_scalar_weight,\n         error_inputs_func=error_inputs_prelu,\n"
                },
                {
                    "old_start": 14222,
                    "old_length": 6,
                    "new_start": 14291,
                    "new_length": 8,
                    "hunk": "@@ -14222,6 +14291,8 @@ op_db: List[OpInfo] = [\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n         supports_out=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         sample_kwargs=lambda device, dtype, input:\n             (dict(lower=0., upper=1., training=True), dict(lower=0., upper=1., training=True)),\n         sample_inputs_func=partial(sample_inputs_elementwise_unary, op_kwargs=dict(lower=0., upper=1., training=True)),\n"
                },
                {
                    "old_start": 14301,
                    "old_length": 6,
                    "new_start": 14372,
                    "new_length": 8,
                    "hunk": "@@ -14301,6 +14372,8 @@ op_db: List[OpInfo] = [\n         supports_forward_ad=False,\n         supports_fwgrad_bwgrad=True,\n         check_batched_forward_grad=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         decorators=[DecorateInfo(toleranceOverride(\n             {torch.float32: tol(atol=5e-05, rtol=5e-6)}), 'TestCommon',), ],\n         skips=(\n"
                },
                {
                    "old_start": 14399,
                    "old_length": 6,
                    "new_start": 14472,
                    "new_length": 8,
                    "hunk": "@@ -14399,6 +14472,8 @@ op_db: List[OpInfo] = [\n         supports_fwgrad_bwgrad=False,\n         supports_forward_ad=False,\n         check_batched_forward_grad=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         decorators=[skipCUDAIf(TEST_WITH_ROCM, \"ROCm doesn't support efficient attention\")],\n         skips=(\n             # Device mismatch due to philox seed and offset\n"
                },
                {
                    "old_start": 14515,
                    "old_length": 6,
                    "new_start": 14590,
                    "new_length": 8,
                    "hunk": "@@ -14515,6 +14590,8 @@ op_db: List[OpInfo] = [\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n         supports_gradgrad=True,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         # autodiff_nonfusible_nodes=[\"aten::log_sigmoid\"],\n         decorators=[\n             DecorateInfo(\n"
                },
                {
                    "old_start": 14612,
                    "old_length": 6,
                    "new_start": 14689,
                    "new_length": 8,
                    "hunk": "@@ -14612,6 +14689,8 @@ op_db: List[OpInfo] = [\n         assert_autodiffed=False,\n         supports_gradgrad=True,\n         supports_out=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         sample_kwargs=lambda device, dtype, input: ({'threshold': float.fromhex('0x1.3ap-3'),\n                                                     'value': -9},\n                                                     {'threshold': float.fromhex('0x1.3ap-3'),\n"
                },
                {
                    "old_start": 14707,
                    "old_length": 6,
                    "new_start": 14786,
                    "new_length": 8,
                    "hunk": "@@ -14707,6 +14786,8 @@ op_db: List[OpInfo] = [\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n            assert_jit_shape_analysis=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_batch_norm,\n            skips=(\n                # see https://github.com/pytorch/pytorch/issues/71286\n"
                },
                {
                    "old_start": 14729,
                    "old_length": 6,
                    "new_start": 14810,
                    "new_length": 8,
                    "hunk": "@@ -14729,6 +14810,8 @@ op_db: List[OpInfo] = [\n            supports_out=False,\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            decorators=[onlyCUDA, disablecuDNN],\n            skips=(\n                DecorateInfo(toleranceOverride({torch.float32: tol(atol=1e-03, rtol=1e-04)}),\n"
                },
                {
                    "old_start": 14746,
                    "old_length": 6,
                    "new_start": 14829,
                    "new_length": 8,
                    "hunk": "@@ -14746,6 +14829,8 @@ op_db: List[OpInfo] = [\n         supports_autograd=True,\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         decorators=(\n             # RuntimeError: expected int at position 0, but got: Tensor\n             DecorateInfo(\n"
                },
                {
                    "old_start": 14904,
                    "old_length": 6,
                    "new_start": 14989,
                    "new_length": 8,
                    "hunk": "@@ -14904,6 +14989,8 @@ op_db: List[OpInfo] = [\n            supports_gradgrad=True,\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            autodiff_nonfusible_nodes=[\"aten::gelu\"],\n            skips=(\n                # AssertionError: Tensor-likes are not close!\n"
                },
                {
                    "old_start": 15057,
                    "old_length": 6,
                    "new_start": 15144,
                    "new_length": 8,
                    "hunk": "@@ -15057,6 +15144,8 @@ op_db: List[OpInfo] = [\n            gradcheck_fast_mode=True,\n            supports_forward_ad=False,\n            supports_fwgrad_bwgrad=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_ormqr,\n            error_inputs_func=error_inputs_ormqr,\n            decorators=[skipCUDAIfNoCusolver, skipCPUIfNoLapack],\n"
                },
                {
                    "old_start": 15955,
                    "old_length": 6,
                    "new_start": 16044,
                    "new_length": 8,
                    "hunk": "@@ -15955,6 +16044,8 @@ op_db: List[OpInfo] = [\n            dtypes=floating_and_complex_types_and(torch.bfloat16, torch.half),\n            dtypesIfCUDA=floating_and_complex_types_and(torch.chalf, torch.half, torch.bfloat16),\n            sample_inputs_func=sample_inputs_lerp,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n            assert_autodiffed=True),\n"
                },
                {
                    "old_start": 17190,
                    "old_length": 6,
                    "new_start": 17281,
                    "new_length": 8,
                    "hunk": "@@ -17190,6 +17281,8 @@ op_db: List[OpInfo] = [\n                wrapper_set_seed(torch.Tensor.multinomial, inp, *args, **kwargs),\n            dtypes=floating_types_and(torch.bfloat16, torch.half),\n            supports_out=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_multinomial,\n            error_inputs_func=error_inputs_multinomial,\n            skips=(\n"
                },
                {
                    "old_start": 17274,
                    "old_length": 6,
                    "new_start": 17367,
                    "new_length": 8,
                    "hunk": "@@ -17274,6 +17367,8 @@ op_db: List[OpInfo] = [\n            supports_out=True,\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_bernoulli,\n            error_inputs_func=error_inputs_bernoulli,\n            skips=(\n"
                },
                {
                    "old_start": 17325,
                    "old_length": 6,
                    "new_start": 17420,
                    "new_length": 8,
                    "hunk": "@@ -17325,6 +17420,8 @@ op_db: List[OpInfo] = [\n            dtypesIfCUDA=_dispatch_dtypes(),  # histogram is only implemented on CPU\n            sample_inputs_func=sample_inputs_histogram,\n            supports_autograd=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            skips=(\n                # JIT tests don't work with Tensor keyword arguments\n                # https://github.com/pytorch/pytorch/issues/58507\n"
                },
                {
                    "old_start": 17344,
                    "old_length": 6,
                    "new_start": 17441,
                    "new_length": 8,
                    "hunk": "@@ -17344,6 +17441,8 @@ op_db: List[OpInfo] = [\n            sample_inputs_func=sample_inputs_histogramdd,\n            error_inputs_func=error_inputs_histogramdd,\n            supports_autograd=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            skips=(\n                # Not implemented on CUDA\n                DecorateInfo(unittest.expectedFailure, 'TestCommon', 'test_errors', device_type='cuda'),\n"
                },
                {
                    "old_start": 17358,
                    "old_length": 6,
                    "new_start": 17457,
                    "new_length": 8,
                    "hunk": "@@ -17358,6 +17457,8 @@ op_db: List[OpInfo] = [\n            sample_inputs_func=sample_inputs_histc,\n            supports_out=True,\n            supports_autograd=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            skips=(\n                # CUDA histc returns a float tensor but does not correctly warn when passed an integral out tensor\n                # \"AssertionError: RuntimeError not raised : Expected RuntimeError when doing an unsafe cast\n"
                },
                {
                    "old_start": 17381,
                    "old_length": 6,
                    "new_start": 17482,
                    "new_length": 8,
                    "hunk": "@@ -17381,6 +17482,8 @@ op_db: List[OpInfo] = [\n            reference_inputs_func=reference_inputs_bucketize,\n            error_inputs_func=error_inputs_bucketize,\n            supports_autograd=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            skips=(\n                # JIT tests don't work with Tensor keyword arguments\n                DecorateInfo(unittest.skip(\"Expected failure!\"), 'TestJit', 'test_variant_consistency_jit'),\n"
                },
                {
                    "old_start": 17390,
                    "old_length": 6,
                    "new_start": 17493,
                    "new_length": 8,
                    "hunk": "@@ -17390,6 +17493,8 @@ op_db: List[OpInfo] = [\n            dtypesIfCUDA=all_types_and(torch.bfloat16, torch.float16),\n            sample_inputs_func=sample_inputs_searchsorted,\n            supports_autograd=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            ref=reference_searchsorted,\n            skips=(\n                # JIT tests don't work with Tensor keyword arguments\n"
                },
                {
                    "old_start": 17864,
                    "old_length": 6,
                    "new_start": 17969,
                    "new_length": 7,
                    "hunk": "@@ -17864,6 +17969,7 @@ op_db: List[OpInfo] = [\n            supports_sparse_csc=True,\n            check_batched_grad=False,\n            check_batched_gradgrad=False,\n+           supports_cow_input_no_materialize=False,\n            skips=(\n                # NotImplementedError: Could not run 'aten::normal_' with arguments from the 'SparseCPU' backend\n                DecorateInfo(unittest.skip(\"\"), 'TestCommon', 'test_noncontiguous_samples'),\n"
                },
                {
                    "old_start": 18059,
                    "old_length": 6,
                    "new_start": 18165,
                    "new_length": 8,
                    "hunk": "@@ -18059,6 +18165,8 @@ op_db: List[OpInfo] = [\n         sample_inputs_func=sample_inputs_softmax_variant,\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         assert_autodiffed=True),\n     OpInfo(\n         'log_softmax',\n"
                },
                {
                    "old_start": 18146,
                    "old_length": 6,
                    "new_start": 18254,
                    "new_length": 8,
                    "hunk": "@@ -18146,6 +18254,8 @@ op_db: List[OpInfo] = [\n         dtypes=all_types_and_complex_and(torch.bfloat16, torch.float16, torch.bool),\n         supports_out=False,\n         supports_autograd=False,  # jiterator ops doesn't have backward defined\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         decorators=[\n             onlyCUDA,\n             DecorateInfo(toleranceOverride({torch.float16: tol(atol=1e-02, rtol=1e-02)}),\n"
                },
                {
                    "old_start": 18192,
                    "old_length": 6,
                    "new_start": 18302,
                    "new_length": 8,
                    "hunk": "@@ -18192,6 +18302,8 @@ op_db: List[OpInfo] = [\n         supports_out=False,\n         supports_autograd=False,  # jiterator ops doesn't have backward defined\n         supports_rhs_python_scalar=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         decorators=[onlyCUDA],\n         skips=(\n             # Jiterator ops doesn't support neg or conj view\n"
                },
                {
                    "old_start": 18217,
                    "old_length": 6,
                    "new_start": 18329,
                    "new_length": 8,
                    "hunk": "@@ -18217,6 +18329,8 @@ op_db: List[OpInfo] = [\n         sample_inputs_func=partial(sample_inputs_jiterator, num_inputs=4, alpha=3.14, beta=-4.20),\n         supports_out=False,\n         supports_autograd=False,  # jiterator ops doesn't have backward defined\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         decorators=[onlyCUDA],\n         skips=(\n             # Jiterator ops doesn't support neg or conj view\n"
                },
                {
                    "old_start": 18248,
                    "old_length": 6,
                    "new_start": 18362,
                    "new_length": 8,
                    "hunk": "@@ -18248,6 +18362,8 @@ op_db: List[OpInfo] = [\n         supports_out=False,\n         supports_autograd=False,  # jiterator ops doesn't have backward defined\n         supports_rhs_python_scalar=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         decorators=[onlyCUDA],\n         skips=(\n             # Jiterator ops doesn't support neg or conj view\n"
                },
                {
                    "old_start": 18279,
                    "old_length": 6,
                    "new_start": 18395,
                    "new_length": 8,
                    "hunk": "@@ -18279,6 +18395,8 @@ op_db: List[OpInfo] = [\n         sample_inputs_func=partial(sample_inputs_jiterator, num_inputs=2),\n         supports_out=False,\n         supports_autograd=False,  # jiterator ops doesn't have backward defined\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         decorators=[onlyCUDA],\n         skips=(\n             # Jiterator ops doesn't support neg or conj view\n"
                },
                {
                    "old_start": 18417,
                    "old_length": 6,
                    "new_start": 18535,
                    "new_length": 8,
                    "hunk": "@@ -18417,6 +18535,8 @@ op_db: List[OpInfo] = [\n         supports_fwgrad_bwgrad=True,\n         # https://github.com/pytorch/pytorch/issues/66357\n         check_batched_forward_grad=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         supports_out=False,\n         sample_inputs_func=sample_inputs_dropout,\n         inplace_variant=lambda input, *args, **kwargs:\n"
                },
                {
                    "old_start": 18428,
                    "old_length": 6,
                    "new_start": 18548,
                    "new_length": 8,
                    "hunk": "@@ -18428,6 +18548,8 @@ op_db: List[OpInfo] = [\n         dtypes=all_types_and(torch.float16, torch.bfloat16, torch.bool),\n         dtypesIfCUDA=floating_types_and(torch.float16, torch.bfloat16),\n         supports_out=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         sample_inputs_func=sample_inputs_dropout_backward,\n         skips=(\n             DecorateInfo(unittest.skip('Skipped!'), 'TestJit', 'test_variant_consistency_jit'),\n"
                },
                {
                    "old_start": 18563,
                    "old_length": 6,
                    "new_start": 18685,
                    "new_length": 8,
                    "hunk": "@@ -18563,6 +18685,8 @@ op_db: List[OpInfo] = [\n         error_inputs_func=error_inputs_embedding,\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         skips=(\n             # lambda impl\n             DecorateInfo(unittest.expectedFailure, 'TestJit', 'test_variant_consistency_jit'),\n"
                },
                {
                    "old_start": 18598,
                    "old_length": 6,
                    "new_start": 18722,
                    "new_length": 8,
                    "hunk": "@@ -18598,6 +18722,8 @@ op_db: List[OpInfo] = [\n         gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n         supports_out=False,\n         supports_gradgrad=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n     ),\n     OpInfo(\n         \"nn.functional.multi_head_attention_forward\",\n"
                },
                {
                    "old_start": 18637,
                    "old_length": 6,
                    "new_start": 18763,
                    "new_length": 8,
                    "hunk": "@@ -18637,6 +18763,8 @@ op_db: List[OpInfo] = [\n         supports_gradgrad=True,\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         # Runs very slowly on slow gradcheck - alternatively reduce input sizes\n         gradcheck_fast_mode=True,\n     ),\n"
                },
                {
                    "old_start": 18686,
                    "old_length": 6,
                    "new_start": 18814,
                    "new_length": 8,
                    "hunk": "@@ -18686,6 +18814,8 @@ op_db: List[OpInfo] = [\n         sample_inputs_func=sample_inputs_grid_sample,\n         reference_inputs_func=reference_inputs_grid_sample,\n         supports_gradgrad=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         gradcheck_nondet_tol=1e-15),\n     # TODO: delete this OpInfo once we add meta support for grid_sampler_3d\n     OpInfo(\n"
                },
                {
                    "old_start": 18695,
                    "old_length": 6,
                    "new_start": 18825,
                    "new_length": 8,
                    "hunk": "@@ -18695,6 +18825,8 @@ op_db: List[OpInfo] = [\n         supports_out=False,\n         sample_inputs_func=sample_inputs_grid_sampler_2d,\n         supports_gradgrad=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         gradcheck_nondet_tol=1e-15),\n     OpInfo(\n         \"argwhere\",\n"
                },
                {
                    "old_start": 19049,
                    "old_length": 6,
                    "new_start": 19181,
                    "new_length": 8,
                    "hunk": "@@ -19049,6 +19181,8 @@ op_db: List[OpInfo] = [\n         \"nn.functional.ctc_loss\",\n         dtypes=floating_types(),\n         supports_out=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         sample_inputs_func=sample_inputs_ctc_loss,\n         skips=(\n             # https://github.com/pytorch/pytorch/issues/67462\n"
                },
                {
                    "old_start": 19096,
                    "old_length": 6,
                    "new_start": 19230,
                    "new_length": 8,
                    "hunk": "@@ -19096,6 +19230,8 @@ op_db: List[OpInfo] = [\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n         assert_jit_shape_analysis=True,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         skips=(\n             # RuntimeError:\n             # undefined value tensor:\n"
                },
                {
                    "old_start": 19162,
                    "old_length": 6,
                    "new_start": 19298,
                    "new_length": 8,
                    "hunk": "@@ -19162,6 +19298,8 @@ op_db: List[OpInfo] = [\n         dtypes=floating_types(),\n         supports_out=False,\n         supports_gradgrad=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         skips=(\n             DecorateInfo(unittest.skip(\"Unsupported on MPS for now\"), 'TestCommon', 'test_numpy_ref_mps'),\n         )\n"
                },
                {
                    "old_start": 19236,
                    "old_length": 6,
                    "new_start": 19374,
                    "new_length": 8,
                    "hunk": "@@ -19236,6 +19374,8 @@ op_db: List[OpInfo] = [\n         supports_out=False,\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         skips=(\n             DecorateInfo(\n                 unittest.skip(\"Skipped!\"),\n"
                },
                {
                    "old_start": 19252,
                    "old_length": 6,
                    "new_start": 19392,
                    "new_length": 8,
                    "hunk": "@@ -19252,6 +19392,8 @@ op_db: List[OpInfo] = [\n         supports_out=False,\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         skips=(\n             DecorateInfo(\n                 unittest.skip(\"Skipped!\"),\n"
                },
                {
                    "old_start": 19345,
                    "old_length": 6,
                    "new_start": 19487,
                    "new_length": 8,
                    "hunk": "@@ -19345,6 +19487,8 @@ op_db: List[OpInfo] = [\n         supports_out=False,\n         # RuntimeError: derivative for aten::_segment_reduce_backward is not implemented\n         supports_gradgrad=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         sample_inputs_func=sample_inputs_segment_reduce,\n         skips=(\n             # FIXME: CUDA driver API confirmed a leak in\n"
                },
                {
                    "old_start": 19365,
                    "old_length": 6,
                    "new_start": 19509,
                    "new_length": 8,
                    "hunk": "@@ -19365,6 +19509,8 @@ op_db: List[OpInfo] = [\n         supports_out=False,\n         # RuntimeError: derivative for aten::_segment_reduce_backward is not implemented\n         supports_gradgrad=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         sample_inputs_func=partial(sample_inputs_segment_reduce, mode='offsets'),\n         skips=(\n             # FIXME: CUDA driver API confirmed a leak in\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+           supports_cow_input_no_materialize=False,\n+           supports_cow_input_no_materialize=False,\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           supports_cow_input_no_materialize=False,\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n+           supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n",
            "whole_hunk": "@@ -10258,6 +10258,7 @@ op_db: List[OpInfo] = [\n            dtypes=floating_types_and(torch.float16, torch.bfloat16),\n            supports_out=False,\n            supports_autograd=False,\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_cauchy,\n            error_inputs_func=error_inputs_cauchy,\n            skips=(\n@@ -10288,6 +10289,7 @@ op_db: List[OpInfo] = [\n            dtypes=floating_types_and(torch.float16, torch.bfloat16),\n            supports_out=False,\n            supports_autograd=False,\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_exponential,\n            error_inputs_func=error_inputs_exponential,\n            skips=(\n@@ -10317,6 +10319,7 @@ op_db: List[OpInfo] = [\n            dtypes=floating_types_and(torch.float16, torch.bfloat16, torch.int8, torch.int16, torch.int32, torch.int64, torch.uint8),\n            supports_out=False,\n            supports_autograd=False,\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_geometric,\n            error_inputs_func=error_inputs_geometric,\n            skips=(\n@@ -10345,6 +10348,8 @@ op_db: List[OpInfo] = [\n            dtypes=floating_types_and(torch.float16, torch.bfloat16),\n            supports_out=False,\n            supports_autograd=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_log_normal,\n            error_inputs_func=error_inputs_log_normal,\n            skips=(\n@@ -10372,6 +10377,7 @@ op_db: List[OpInfo] = [\n            dtypes=floating_and_complex_types_and(torch.float16, torch.bfloat16),\n            supports_out=False,\n            supports_autograd=False,\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_normal,\n            error_inputs_func=error_inputs_normal,\n            skips=(\n@@ -10402,6 +10408,7 @@ op_db: List[OpInfo] = [\n            supports_out=False,\n            supports_autograd=False,\n            is_factory_function=False,\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_uniform,\n            error_inputs_func=error_inputs_uniform,\n            skips=(\n@@ -10687,6 +10694,8 @@ op_db: List[OpInfo] = [\n            # Reference: https://github.com/pytorch/pytorch/issues/50747\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            skips=(\n                # Reference: https://github.com/pytorch/pytorch/issues/50747\n                DecorateInfo(unittest.skip(\"Skipped!\"), 'TestCommon', 'test_variant_consistency_eager',\n@@ -11011,6 +11020,8 @@ op_db: List[OpInfo] = [\n            supports_out=False,\n            supports_gradgrad=False,\n            assert_autodiffed=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_cdist),\n     UnaryUfuncInfo('ceil',\n                    ref=np.ceil,\n@@ -11737,6 +11748,8 @@ op_db: List[OpInfo] = [\n     OpInfo('sparse.sampled_addmm',\n            dtypes=floating_and_complex_types(),\n            supports_autograd=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_sparse_sampled_addmm,\n            decorators=[\n                skipCUDAIf(not ((_get_torch_cuda_version() >= (11, 3))\n@@ -11785,6 +11798,8 @@ op_db: List[OpInfo] = [\n            supports_out=False,\n            supports_gradgrad=False,\n            supports_forward_ad=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_sparse_mm_reduce,\n            decorators=[onlyCPU],\n            skips=(\n@@ -12316,6 +12331,8 @@ op_db: List[OpInfo] = [\n            supports_fwgrad_bwgrad=True,\n            # https://github.com/pytorch/pytorch/issues/66357\n            check_batched_forward_grad=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            skips=(\n                # mexp does not support bf16 and fp16\n                DecorateInfo(unittest.skip('Skipped!'), 'TestInductorOpInfo', 'test_comprehensive',\n@@ -12723,6 +12740,8 @@ op_db: List[OpInfo] = [\n            assert_autodiffed=True,\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            supports_out=True),\n     OpInfo('softmax',\n            aliases=('special.softmax', 'nn.functional.softmax',),\n@@ -12744,6 +12763,8 @@ op_db: List[OpInfo] = [\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n         supports_out=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         skips=(\n             DecorateInfo(unittest.expectedFailure, 'TestCommon', 'test_noncontiguous_samples', device_type='cpu'),\n             DecorateInfo(unittest.expectedFailure, 'TestJit', 'test_variant_consistency_jit', dtypes=(torch.float32,)),\n@@ -12778,6 +12799,8 @@ op_db: List[OpInfo] = [\n         supports_out=False,\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         decorators=(\n             DecorateInfo(\n                 toleranceOverride({torch.float32: tol(atol=1e-5, rtol=1e-3)}),\n@@ -12901,6 +12924,8 @@ op_db: List[OpInfo] = [\n            supports_out=False,\n            assert_jit_shape_analysis=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_native_layer_norm,\n            error_inputs_func=error_inputs_native_layer_norm,\n            skips=(\n@@ -12919,6 +12944,8 @@ op_db: List[OpInfo] = [\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n            assert_jit_shape_analysis=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_native_batch_norm,\n            skips=(\n                # NotImplementedError: Could not run\n@@ -12946,6 +12973,8 @@ op_db: List[OpInfo] = [\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n            assert_jit_shape_analysis=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs__native_batch_norm_legit,\n            skips=(\n                # NotImplementedError: Could not run\n@@ -13109,6 +13138,8 @@ op_db: List[OpInfo] = [\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n         supports_out=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         dtypes=floating_types_and(torch.half, torch.bfloat16),\n         gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n         sample_inputs_func=sample_inputs_binary_cross_entropy_with_logits,\n@@ -13149,6 +13180,8 @@ op_db: List[OpInfo] = [\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n            assert_jit_shape_analysis=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n            decorators=(\n                DecorateInfo(\n@@ -13195,6 +13228,8 @@ op_db: List[OpInfo] = [\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n            assert_jit_shape_analysis=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n            decorators=[\n                DecorateInfo(\n@@ -13243,6 +13278,8 @@ op_db: List[OpInfo] = [\n            assert_jit_shape_analysis=True,\n            # Runs very slowly on slow-gradcheck - alternatively reduce input sizes\n            gradcheck_fast_mode=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n            decorators=[\n                DecorateInfo(\n@@ -13301,6 +13338,8 @@ op_db: List[OpInfo] = [\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n            assert_jit_shape_analysis=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n            decorators=(\n                DecorateInfo(\n@@ -13341,6 +13380,8 @@ op_db: List[OpInfo] = [\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n            assert_jit_shape_analysis=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            decorators=(\n                DecorateInfo(\n                    toleranceOverride({torch.chalf: tol(atol=6e-2, rtol=5e-2)}),\n@@ -13372,6 +13413,8 @@ op_db: List[OpInfo] = [\n            gradcheck_fast_mode=True,\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            decorators=(\n                DecorateInfo(\n                    toleranceOverride({torch.chalf: tol(atol=6e-2, rtol=5e-2)}),\n@@ -13414,6 +13457,8 @@ op_db: List[OpInfo] = [\n            supports_out=False,\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            error_inputs_func=error_inputs_group_norm,\n            decorators=[\n                # RuntimeError: Cannot insert a Tensor that requires grad as a constant.\n@@ -13429,6 +13474,8 @@ op_db: List[OpInfo] = [\n            supports_out=False,\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            decorators=[\n                # RuntimeError: Cannot insert a Tensor that requires grad as a constant.\n                # Consider making it a parameter or input, or detaching the gradient\n@@ -13448,6 +13495,8 @@ op_db: List[OpInfo] = [\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n            assert_jit_shape_analysis=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            decorators=[\n                DecorateInfo(\n                    toleranceOverride({torch.float32: tol(atol=1e-05, rtol=1e-03)}),\n@@ -13496,6 +13545,8 @@ op_db: List[OpInfo] = [\n            variant_test_name='reflect',\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            dtypes=all_types_and_complex_and(torch.bfloat16),\n            dtypesIfCUDA=all_types_and_complex_and(torch.half, torch.bfloat16),\n            sample_inputs_func=partial(sample_inputs_nn_pad, mode='reflect'),\n@@ -13511,6 +13562,8 @@ op_db: List[OpInfo] = [\n            variant_test_name='replicate',\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            dtypes=all_types_and_complex_and(torch.bfloat16),\n            dtypesIfCUDA=all_types_and_complex_and(torch.half, torch.bfloat16),\n            sample_inputs_func=partial(sample_inputs_nn_pad, mode='replicate'),\n@@ -13526,6 +13579,8 @@ op_db: List[OpInfo] = [\n            variant_test_name='replicate_negative',\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            dtypes=all_types_and_complex_and(torch.bfloat16),\n            dtypesIfCUDA=all_types_and_complex_and(torch.half, torch.bfloat16),\n            sample_inputs_func=sample_inputs_nn_pad_replicate_negative,\n@@ -13777,6 +13832,8 @@ op_db: List[OpInfo] = [\n         dtypesIfCUDA=floating_types_and(torch.bfloat16, torch.float16),\n         supports_out=False,\n         supports_gradgrad=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         sample_inputs_func=sample_inputs_multi_margin_loss,\n         reference_inputs_func=reference_inputs_multi_margin_loss,\n         error_inputs_func=error_inputs_multi_margin_loss,\n@@ -13794,6 +13851,8 @@ op_db: List[OpInfo] = [\n         dtypesIfCUDA=floating_types_and(torch.bfloat16, torch.float16),\n         supports_out=False,\n         supports_gradgrad=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         sample_inputs_func=sample_inputs_multilabel_margin_loss,\n         reference_inputs_func=reference_inputs_multilabel_margin_loss,\n         error_inputs_func=error_inputs_multilabel_margin_loss,\n@@ -13820,6 +13879,8 @@ op_db: List[OpInfo] = [\n         sample_inputs_func=sample_inputs_multilabel_soft_margin_loss,\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         decorators=(\n             DecorateInfo(\n                 toleranceOverride({torch.float32: tol(atol=1e-4, rtol=1e-4)}),\n@@ -13864,6 +13925,8 @@ op_db: List[OpInfo] = [\n            check_batched_forward_grad=False,\n            dtypes=floating_types_and(torch.bfloat16, torch.float16),\n            test_neg_view=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_fractional_max_pool2d,\n            decorators=(\n                # FIXME: AssertionError: False is not true : Tensors failed to compare as equal!\n@@ -13878,6 +13941,8 @@ op_db: List[OpInfo] = [\n            supports_out=False,\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            op=lambda input, *args, **kwargs:\n                wrapper_set_seed(torch.nn.functional.fractional_max_pool3d, input, *args, **kwargs),\n            # vmap does not support random operations\n@@ -14087,6 +14152,8 @@ op_db: List[OpInfo] = [\n            aten_name='linear',\n            supports_autograd=True,\n            supports_gradgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_linear,\n            dtypes=all_types_and_complex_and(torch.float16, torch.bfloat16),\n            dtypesIfROCM=floating_and_complex_types_and(torch.float16, torch.bfloat16),\n@@ -14174,6 +14241,8 @@ op_db: List[OpInfo] = [\n         assert_autodiffed=False,\n         supports_gradgrad=True,\n         supports_out=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         # test_reference_numerics only tests the case when the weight tensor is a scalar\n         sample_kwargs=sample_kwargs_prelu_scalar_weight,\n         error_inputs_func=error_inputs_prelu,\n@@ -14222,6 +14291,8 @@ op_db: List[OpInfo] = [\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n         supports_out=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         sample_kwargs=lambda device, dtype, input:\n             (dict(lower=0., upper=1., training=True), dict(lower=0., upper=1., training=True)),\n         sample_inputs_func=partial(sample_inputs_elementwise_unary, op_kwargs=dict(lower=0., upper=1., training=True)),\n@@ -14301,6 +14372,8 @@ op_db: List[OpInfo] = [\n         supports_forward_ad=False,\n         supports_fwgrad_bwgrad=True,\n         check_batched_forward_grad=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         decorators=[DecorateInfo(toleranceOverride(\n             {torch.float32: tol(atol=5e-05, rtol=5e-6)}), 'TestCommon',), ],\n         skips=(\n@@ -14399,6 +14472,8 @@ op_db: List[OpInfo] = [\n         supports_fwgrad_bwgrad=False,\n         supports_forward_ad=False,\n         check_batched_forward_grad=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         decorators=[skipCUDAIf(TEST_WITH_ROCM, \"ROCm doesn't support efficient attention\")],\n         skips=(\n             # Device mismatch due to philox seed and offset\n@@ -14515,6 +14590,8 @@ op_db: List[OpInfo] = [\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n         supports_gradgrad=True,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         # autodiff_nonfusible_nodes=[\"aten::log_sigmoid\"],\n         decorators=[\n             DecorateInfo(\n@@ -14612,6 +14689,8 @@ op_db: List[OpInfo] = [\n         assert_autodiffed=False,\n         supports_gradgrad=True,\n         supports_out=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         sample_kwargs=lambda device, dtype, input: ({'threshold': float.fromhex('0x1.3ap-3'),\n                                                     'value': -9},\n                                                     {'threshold': float.fromhex('0x1.3ap-3'),\n@@ -14707,6 +14786,8 @@ op_db: List[OpInfo] = [\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n            assert_jit_shape_analysis=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_batch_norm,\n            skips=(\n                # see https://github.com/pytorch/pytorch/issues/71286\n@@ -14729,6 +14810,8 @@ op_db: List[OpInfo] = [\n            supports_out=False,\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            decorators=[onlyCUDA, disablecuDNN],\n            skips=(\n                DecorateInfo(toleranceOverride({torch.float32: tol(atol=1e-03, rtol=1e-04)}),\n@@ -14746,6 +14829,8 @@ op_db: List[OpInfo] = [\n         supports_autograd=True,\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         decorators=(\n             # RuntimeError: expected int at position 0, but got: Tensor\n             DecorateInfo(\n@@ -14904,6 +14989,8 @@ op_db: List[OpInfo] = [\n            supports_gradgrad=True,\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            autodiff_nonfusible_nodes=[\"aten::gelu\"],\n            skips=(\n                # AssertionError: Tensor-likes are not close!\n@@ -15057,6 +15144,8 @@ op_db: List[OpInfo] = [\n            gradcheck_fast_mode=True,\n            supports_forward_ad=False,\n            supports_fwgrad_bwgrad=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_ormqr,\n            error_inputs_func=error_inputs_ormqr,\n            decorators=[skipCUDAIfNoCusolver, skipCPUIfNoLapack],\n@@ -15955,6 +16044,8 @@ op_db: List[OpInfo] = [\n            dtypes=floating_and_complex_types_and(torch.bfloat16, torch.half),\n            dtypesIfCUDA=floating_and_complex_types_and(torch.chalf, torch.half, torch.bfloat16),\n            sample_inputs_func=sample_inputs_lerp,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n            assert_autodiffed=True),\n@@ -17190,6 +17281,8 @@ op_db: List[OpInfo] = [\n                wrapper_set_seed(torch.Tensor.multinomial, inp, *args, **kwargs),\n            dtypes=floating_types_and(torch.bfloat16, torch.half),\n            supports_out=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_multinomial,\n            error_inputs_func=error_inputs_multinomial,\n            skips=(\n@@ -17274,6 +17367,8 @@ op_db: List[OpInfo] = [\n            supports_out=True,\n            supports_forward_ad=True,\n            supports_fwgrad_bwgrad=True,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            sample_inputs_func=sample_inputs_bernoulli,\n            error_inputs_func=error_inputs_bernoulli,\n            skips=(\n@@ -17325,6 +17420,8 @@ op_db: List[OpInfo] = [\n            dtypesIfCUDA=_dispatch_dtypes(),  # histogram is only implemented on CPU\n            sample_inputs_func=sample_inputs_histogram,\n            supports_autograd=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            skips=(\n                # JIT tests don't work with Tensor keyword arguments\n                # https://github.com/pytorch/pytorch/issues/58507\n@@ -17344,6 +17441,8 @@ op_db: List[OpInfo] = [\n            sample_inputs_func=sample_inputs_histogramdd,\n            error_inputs_func=error_inputs_histogramdd,\n            supports_autograd=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            skips=(\n                # Not implemented on CUDA\n                DecorateInfo(unittest.expectedFailure, 'TestCommon', 'test_errors', device_type='cuda'),\n@@ -17358,6 +17457,8 @@ op_db: List[OpInfo] = [\n            sample_inputs_func=sample_inputs_histc,\n            supports_out=True,\n            supports_autograd=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            skips=(\n                # CUDA histc returns a float tensor but does not correctly warn when passed an integral out tensor\n                # \"AssertionError: RuntimeError not raised : Expected RuntimeError when doing an unsafe cast\n@@ -17381,6 +17482,8 @@ op_db: List[OpInfo] = [\n            reference_inputs_func=reference_inputs_bucketize,\n            error_inputs_func=error_inputs_bucketize,\n            supports_autograd=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            skips=(\n                # JIT tests don't work with Tensor keyword arguments\n                DecorateInfo(unittest.skip(\"Expected failure!\"), 'TestJit', 'test_variant_consistency_jit'),\n@@ -17390,6 +17493,8 @@ op_db: List[OpInfo] = [\n            dtypesIfCUDA=all_types_and(torch.bfloat16, torch.float16),\n            sample_inputs_func=sample_inputs_searchsorted,\n            supports_autograd=False,\n+           # TODO: Avoid COW materialize\n+           supports_cow_input_no_materialize=False,\n            ref=reference_searchsorted,\n            skips=(\n                # JIT tests don't work with Tensor keyword arguments\n@@ -17864,6 +17969,7 @@ op_db: List[OpInfo] = [\n            supports_sparse_csc=True,\n            check_batched_grad=False,\n            check_batched_gradgrad=False,\n+           supports_cow_input_no_materialize=False,\n            skips=(\n                # NotImplementedError: Could not run 'aten::normal_' with arguments from the 'SparseCPU' backend\n                DecorateInfo(unittest.skip(\"\"), 'TestCommon', 'test_noncontiguous_samples'),\n@@ -18059,6 +18165,8 @@ op_db: List[OpInfo] = [\n         sample_inputs_func=sample_inputs_softmax_variant,\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         assert_autodiffed=True),\n     OpInfo(\n         'log_softmax',\n@@ -18146,6 +18254,8 @@ op_db: List[OpInfo] = [\n         dtypes=all_types_and_complex_and(torch.bfloat16, torch.float16, torch.bool),\n         supports_out=False,\n         supports_autograd=False,  # jiterator ops doesn't have backward defined\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         decorators=[\n             onlyCUDA,\n             DecorateInfo(toleranceOverride({torch.float16: tol(atol=1e-02, rtol=1e-02)}),\n@@ -18192,6 +18302,8 @@ op_db: List[OpInfo] = [\n         supports_out=False,\n         supports_autograd=False,  # jiterator ops doesn't have backward defined\n         supports_rhs_python_scalar=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         decorators=[onlyCUDA],\n         skips=(\n             # Jiterator ops doesn't support neg or conj view\n@@ -18217,6 +18329,8 @@ op_db: List[OpInfo] = [\n         sample_inputs_func=partial(sample_inputs_jiterator, num_inputs=4, alpha=3.14, beta=-4.20),\n         supports_out=False,\n         supports_autograd=False,  # jiterator ops doesn't have backward defined\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         decorators=[onlyCUDA],\n         skips=(\n             # Jiterator ops doesn't support neg or conj view\n@@ -18248,6 +18362,8 @@ op_db: List[OpInfo] = [\n         supports_out=False,\n         supports_autograd=False,  # jiterator ops doesn't have backward defined\n         supports_rhs_python_scalar=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         decorators=[onlyCUDA],\n         skips=(\n             # Jiterator ops doesn't support neg or conj view\n@@ -18279,6 +18395,8 @@ op_db: List[OpInfo] = [\n         sample_inputs_func=partial(sample_inputs_jiterator, num_inputs=2),\n         supports_out=False,\n         supports_autograd=False,  # jiterator ops doesn't have backward defined\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         decorators=[onlyCUDA],\n         skips=(\n             # Jiterator ops doesn't support neg or conj view\n@@ -18417,6 +18535,8 @@ op_db: List[OpInfo] = [\n         supports_fwgrad_bwgrad=True,\n         # https://github.com/pytorch/pytorch/issues/66357\n         check_batched_forward_grad=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         supports_out=False,\n         sample_inputs_func=sample_inputs_dropout,\n         inplace_variant=lambda input, *args, **kwargs:\n@@ -18428,6 +18548,8 @@ op_db: List[OpInfo] = [\n         dtypes=all_types_and(torch.float16, torch.bfloat16, torch.bool),\n         dtypesIfCUDA=floating_types_and(torch.float16, torch.bfloat16),\n         supports_out=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         sample_inputs_func=sample_inputs_dropout_backward,\n         skips=(\n             DecorateInfo(unittest.skip('Skipped!'), 'TestJit', 'test_variant_consistency_jit'),\n@@ -18563,6 +18685,8 @@ op_db: List[OpInfo] = [\n         error_inputs_func=error_inputs_embedding,\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         skips=(\n             # lambda impl\n             DecorateInfo(unittest.expectedFailure, 'TestJit', 'test_variant_consistency_jit'),\n@@ -18598,6 +18722,8 @@ op_db: List[OpInfo] = [\n         gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n         supports_out=False,\n         supports_gradgrad=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n     ),\n     OpInfo(\n         \"nn.functional.multi_head_attention_forward\",\n@@ -18637,6 +18763,8 @@ op_db: List[OpInfo] = [\n         supports_gradgrad=True,\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         # Runs very slowly on slow gradcheck - alternatively reduce input sizes\n         gradcheck_fast_mode=True,\n     ),\n@@ -18686,6 +18814,8 @@ op_db: List[OpInfo] = [\n         sample_inputs_func=sample_inputs_grid_sample,\n         reference_inputs_func=reference_inputs_grid_sample,\n         supports_gradgrad=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         gradcheck_nondet_tol=1e-15),\n     # TODO: delete this OpInfo once we add meta support for grid_sampler_3d\n     OpInfo(\n@@ -18695,6 +18825,8 @@ op_db: List[OpInfo] = [\n         supports_out=False,\n         sample_inputs_func=sample_inputs_grid_sampler_2d,\n         supports_gradgrad=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         gradcheck_nondet_tol=1e-15),\n     OpInfo(\n         \"argwhere\",\n@@ -19049,6 +19181,8 @@ op_db: List[OpInfo] = [\n         \"nn.functional.ctc_loss\",\n         dtypes=floating_types(),\n         supports_out=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         sample_inputs_func=sample_inputs_ctc_loss,\n         skips=(\n             # https://github.com/pytorch/pytorch/issues/67462\n@@ -19096,6 +19230,8 @@ op_db: List[OpInfo] = [\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n         assert_jit_shape_analysis=True,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         skips=(\n             # RuntimeError:\n             # undefined value tensor:\n@@ -19162,6 +19298,8 @@ op_db: List[OpInfo] = [\n         dtypes=floating_types(),\n         supports_out=False,\n         supports_gradgrad=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         skips=(\n             DecorateInfo(unittest.skip(\"Unsupported on MPS for now\"), 'TestCommon', 'test_numpy_ref_mps'),\n         )\n@@ -19236,6 +19374,8 @@ op_db: List[OpInfo] = [\n         supports_out=False,\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         skips=(\n             DecorateInfo(\n                 unittest.skip(\"Skipped!\"),\n@@ -19252,6 +19392,8 @@ op_db: List[OpInfo] = [\n         supports_out=False,\n         supports_forward_ad=True,\n         supports_fwgrad_bwgrad=True,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         skips=(\n             DecorateInfo(\n                 unittest.skip(\"Skipped!\"),\n@@ -19345,6 +19487,8 @@ op_db: List[OpInfo] = [\n         supports_out=False,\n         # RuntimeError: derivative for aten::_segment_reduce_backward is not implemented\n         supports_gradgrad=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         sample_inputs_func=sample_inputs_segment_reduce,\n         skips=(\n             # FIXME: CUDA driver API confirmed a leak in\n@@ -19365,6 +19509,8 @@ op_db: List[OpInfo] = [\n         supports_out=False,\n         # RuntimeError: derivative for aten::_segment_reduce_backward is not implemented\n         supports_gradgrad=False,\n+        # TODO: Avoid COW materialize\n+        supports_cow_input_no_materialize=False,\n         sample_inputs_func=partial(sample_inputs_segment_reduce, mode='offsets'),\n         skips=(\n             # FIXME: CUDA driver API confirmed a leak in\n"
        },
        {
            "name": "core.py",
            "path": "torch/testing/_internal/opinfo/core.py",
            "patches": [
                {
                    "old_start": 774,
                    "old_length": 6,
                    "new_start": 774,
                    "new_length": 9,
                    "hunk": "@@ -774,6 +774,9 @@ class OpInfo:\n     # (e.g. functions like ones, zeros, methods like view, permute)\n     supports_varargs: bool = False\n \n+    # Whether the operation avoids materializing COW tensor inputs\n+    supports_cow_input_no_materialize: bool = True\n+\n     # wrapper function for gradcheck\n     gradcheck_wrapper: Callable = lambda op, *args, **kwargs: op(*args, **kwargs)\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    # Whether the operation avoids materializing COW tensor inputs\n+    supports_cow_input_no_materialize: bool = True\n+\n",
            "whole_hunk": "@@ -774,6 +774,9 @@ class OpInfo:\n     # (e.g. functions like ones, zeros, methods like view, permute)\n     supports_varargs: bool = False\n \n+    # Whether the operation avoids materializing COW tensor inputs\n+    supports_cow_input_no_materialize: bool = True\n+\n     # wrapper function for gradcheck\n     gradcheck_wrapper: Callable = lambda op, *args, **kwargs: op(*args, **kwargs)\n "
        }
    ]
},
{
    "Id": 82,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/f8d60e0e0a4420def0cf6b3bc0a0c0d46c93de1c",
    "date": "2024-06-16T11:27:13+00:00",
    "message": "[Inductor][CPP] Fix Half data type cse cache issue for CPP Backend (#128498)\n\n**Summary**\nFixing issue: https://github.com/pytorch/pytorch/issues/128263. After https://github.com/pytorch/pytorch/issues/115260, we cached the higher precision cse variable to avoid duplicate casting between buffers. However, it failed to check the original data type. This means if we convert `int32` to `bf16` for `store` and then convert `bf16` back to `fp32` for `load`, it would incorrectly hit the cache and reuse the `int32` cse var. This PR fixes the issue.\n\n**Test Plan**\n```\npython -u -m pytest -s -v test/inductor/test_cpu_repro.py -k test_issue_128263\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128498\nApproved by: https://github.com/jgong5, https://github.com/zhuhaozhe, https://github.com/jerryzh168",
    "label": "YES",
    "changes": [
        {
            "name": "test_cpu_repro.py",
            "path": "test/inductor/test_cpu_repro.py",
            "patches": [
                {
                    "old_start": 18,
                    "old_length": 7,
                    "new_start": 18,
                    "new_length": 7,
                    "hunk": "@@ -18,7 +18,7 @@ from torch import nn\n from torch._C import FileCheck\n from torch._dynamo.testing import rand_strided\n from torch._dynamo.utils import same\n-from torch._inductor import codecache, config, metrics\n+from torch._inductor import codecache, config, metrics, test_operators\n from torch._inductor.codegen.common import OptimizationContext\n from torch._inductor.codegen.cpp import (\n     CppOverrides,\n"
                },
                {
                    "old_start": 3659,
                    "old_length": 6,
                    "new_start": 3659,
                    "new_length": 30,
                    "hunk": "@@ -3659,6 +3659,30 @@ class CPUReproTests(TestCase):\n         self.common(fn, (x,))\n         assert metrics.generated_cpp_vec_kernel_count == 1\n \n+    def test_highp_to_lowp_cse_var_cache_with_store(self):\n+        # Fix issue: https://github.com/pytorch/pytorch/issues/128263\n+        input = torch.randn(5, 128, dtype=torch.float32)\n+        input2 = torch.randint(0, 10, (5, 128), dtype=torch.int8)\n+        input3 = torch.randn(128, 128, dtype=torch.float32)\n+\n+        class Model(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+\n+            def forward(self, x, x2, x3):\n+                x2 = x2.to(torch.int32)\n+                temp = test_operators.realize(x2.to(torch.float16))\n+                temp2 = temp.to(torch.float32)\n+                temp2 = temp2 * x\n+                return torch.mm(temp, x3.to(torch.float16)), temp2\n+\n+        metrics.reset()\n+        m = Model()\n+        self.common(\n+            m,\n+            (input, input2, input3),\n+        )\n+\n     def test_reduction_float_to_int64(self):\n         # https://github.com/pytorch/pytorch/issues/124821\n         def fn(x):\n"
                }
            ],
            "whole_deleted": "-from torch._inductor import codecache, config, metrics\n",
            "whole_added": "+from torch._inductor import codecache, config, metrics, test_operators\n+    def test_highp_to_lowp_cse_var_cache_with_store(self):\n+        # Fix issue: https://github.com/pytorch/pytorch/issues/128263\n+        input = torch.randn(5, 128, dtype=torch.float32)\n+        input2 = torch.randint(0, 10, (5, 128), dtype=torch.int8)\n+        input3 = torch.randn(128, 128, dtype=torch.float32)\n+\n+        class Model(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+\n+            def forward(self, x, x2, x3):\n+                x2 = x2.to(torch.int32)\n+                temp = test_operators.realize(x2.to(torch.float16))\n+                temp2 = temp.to(torch.float32)\n+                temp2 = temp2 * x\n+                return torch.mm(temp, x3.to(torch.float16)), temp2\n+\n+        metrics.reset()\n+        m = Model()\n+        self.common(\n+            m,\n+            (input, input2, input3),\n+        )\n+\n",
            "whole_hunk": "@@ -18,7 +18,7 @@ from torch import nn\n from torch._C import FileCheck\n from torch._dynamo.testing import rand_strided\n from torch._dynamo.utils import same\n-from torch._inductor import codecache, config, metrics\n+from torch._inductor import codecache, config, metrics, test_operators\n from torch._inductor.codegen.common import OptimizationContext\n from torch._inductor.codegen.cpp import (\n     CppOverrides,\n@@ -3659,6 +3659,30 @@ class CPUReproTests(TestCase):\n         self.common(fn, (x,))\n         assert metrics.generated_cpp_vec_kernel_count == 1\n \n+    def test_highp_to_lowp_cse_var_cache_with_store(self):\n+        # Fix issue: https://github.com/pytorch/pytorch/issues/128263\n+        input = torch.randn(5, 128, dtype=torch.float32)\n+        input2 = torch.randint(0, 10, (5, 128), dtype=torch.int8)\n+        input3 = torch.randn(128, 128, dtype=torch.float32)\n+\n+        class Model(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+\n+            def forward(self, x, x2, x3):\n+                x2 = x2.to(torch.int32)\n+                temp = test_operators.realize(x2.to(torch.float16))\n+                temp2 = temp.to(torch.float32)\n+                temp2 = temp2 * x\n+                return torch.mm(temp, x3.to(torch.float16)), temp2\n+\n+        metrics.reset()\n+        m = Model()\n+        self.common(\n+            m,\n+            (input, input2, input3),\n+        )\n+\n     def test_reduction_float_to_int64(self):\n         # https://github.com/pytorch/pytorch/issues/124821\n         def fn(x):\n"
        },
        {
            "name": "cpp.py",
            "path": "torch/_inductor/codegen/cpp.py",
            "patches": [
                {
                    "old_start": 188,
                    "old_length": 12,
                    "new_start": 188,
                    "new_length": 12,
                    "hunk": "@@ -188,12 +188,12 @@ def is_to_lowp_dtype(expr):\n     return any(to_expr in expr for to_expr in to_exprs)\n \n \n-def get_lowp_to_fp32_expr(lowp_var, kernel):\n+def get_lowp_to_high_prec_expr(lowp_var, dtype, kernel):\n     if isinstance(kernel, CppVecKernel):\n-        return f\"at::vec::convert<float>({lowp_var})\"\n+        return f\"at::vec::convert<{DTYPE_TO_CPP[dtype]}>({lowp_var})\"\n     else:\n         assert isinstance(kernel, CppKernel)\n-        return f\"c10::convert<float>({lowp_var})\"\n+        return f\"c10::convert<{DTYPE_TO_CPP[dtype]}>({lowp_var})\"\n \n \n index_value_name_counter = 1\n"
                },
                {
                    "old_start": 1614,
                    "old_length": 7,
                    "new_start": 1614,
                    "new_length": 7,
                    "hunk": "@@ -1614,7 +1614,7 @@ class CppKernel(Kernel):\n         finally:\n             self._load_mask = prior\n \n-    def cache_fp32_cse_var_before_lowp_store(self, var_to_store):\n+    def cache_high_prec_cse_var_before_lowp_store(self, var_to_store):\n         \"\"\"\n         https://github.com/pytorch/pytorch/issues/115260\n         For FusedSchedulerNode[node1, node2], the node2 loads what node1 stores and the buffer is\n"
                },
                {
                    "old_start": 1651,
                    "old_length": 26,
                    "new_start": 1651,
                    "new_length": 29,
                    "hunk": "@@ -1651,26 +1651,29 @@ class CppKernel(Kernel):\n             # only need to cache fp32 cse var while var_to_store is lowp data\n             return\n \n-        def find_fp32_var(var, cache):\n-            fp32_cse_var = None\n-            fp32_cse_var_name = None\n+        def find_high_prec_var(var, cache):\n+            high_prec_cse_var = None\n+            high_prec_cse_var_name = None\n             for expr, cse_var in cache.items():\n                 if cse_var == var:\n                     if is_to_lowp_dtype(expr):\n                         m = re.search(r\"tmp\\d+\", expr)\n                         if m is not None:\n-                            fp32_cse_var_name = m.group()\n-            if fp32_cse_var_name:\n+                            high_prec_cse_var_name = m.group()\n+            if high_prec_cse_var_name:\n                 for cse_var in cache.values():\n-                    if cse_var.name == fp32_cse_var_name:\n-                        fp32_cse_var = cse_var\n+                    if cse_var.name == high_prec_cse_var_name:\n+                        high_prec_cse_var = cse_var\n                         break\n-                assert fp32_cse_var is not None\n-            return fp32_cse_var\n+                assert high_prec_cse_var is not None\n+            return high_prec_cse_var\n \n-        fp32_var = find_fp32_var(var_to_store, self.cse.cache)\n-        if fp32_var:\n-            self.cse.cache[get_lowp_to_fp32_expr(var_to_store, self)] = fp32_var\n+        high_prec_var = find_high_prec_var(var_to_store, self.cse.cache)\n+        if high_prec_var and high_prec_var.dtype in DTYPE_TO_CPP:\n+            cache_key = get_lowp_to_high_prec_expr(\n+                var_to_store, high_prec_var.dtype, self\n+            )\n+            self.cse.cache[cache_key] = high_prec_var\n \n     def scale_index_with_offset(\n         self, index: sympy.Expr, scale=1, itervar_idx=-1, offset=0\n"
                },
                {
                    "old_start": 1749,
                    "old_length": 7,
                    "new_start": 1752,
                    "new_length": 7,
                    "hunk": "@@ -1749,7 +1752,7 @@ class CppKernel(Kernel):\n     def store(self, name, index, value, mode=None):\n         assert \"buf\" in name\n         var = self.args.output(name)\n-        self.cache_fp32_cse_var_before_lowp_store(value)\n+        self.cache_high_prec_cse_var_before_lowp_store(value)\n         index = self.rename_indexing(index)\n         if mode is None:\n             line = f\"{var}[{cexpr_index(index)}] = {value};\"\n"
                },
                {
                    "old_start": 2344,
                    "old_length": 7,
                    "new_start": 2347,
                    "new_length": 7,
                    "hunk": "@@ -2344,7 +2347,7 @@ class CppVecKernel(CppKernel):\n             value = self.broadcast(value)\n         opt_ctx: OptimizationContext = get_current_node_opt_ctx()\n         var = self.args.output(name)\n-        self.cache_fp32_cse_var_before_lowp_store(value)\n+        self.cache_high_prec_cse_var_before_lowp_store(value)\n         index = self.rename_indexing(index)\n         code = self._get_store_line(value, var, index, V.graph.get_dtype(name))\n         self.stores.splice(code.map(lambda x: DeferredLine(name, x)))"
                }
            ],
            "whole_deleted": "-def get_lowp_to_fp32_expr(lowp_var, kernel):\n-        return f\"at::vec::convert<float>({lowp_var})\"\n-        return f\"c10::convert<float>({lowp_var})\"\n-    def cache_fp32_cse_var_before_lowp_store(self, var_to_store):\n-        def find_fp32_var(var, cache):\n-            fp32_cse_var = None\n-            fp32_cse_var_name = None\n-                            fp32_cse_var_name = m.group()\n-            if fp32_cse_var_name:\n-                    if cse_var.name == fp32_cse_var_name:\n-                        fp32_cse_var = cse_var\n-                assert fp32_cse_var is not None\n-            return fp32_cse_var\n-        fp32_var = find_fp32_var(var_to_store, self.cse.cache)\n-        if fp32_var:\n-            self.cse.cache[get_lowp_to_fp32_expr(var_to_store, self)] = fp32_var\n-        self.cache_fp32_cse_var_before_lowp_store(value)\n-        self.cache_fp32_cse_var_before_lowp_store(value)\n",
            "whole_added": "+def get_lowp_to_high_prec_expr(lowp_var, dtype, kernel):\n+        return f\"at::vec::convert<{DTYPE_TO_CPP[dtype]}>({lowp_var})\"\n+        return f\"c10::convert<{DTYPE_TO_CPP[dtype]}>({lowp_var})\"\n+    def cache_high_prec_cse_var_before_lowp_store(self, var_to_store):\n+        def find_high_prec_var(var, cache):\n+            high_prec_cse_var = None\n+            high_prec_cse_var_name = None\n+                            high_prec_cse_var_name = m.group()\n+            if high_prec_cse_var_name:\n+                    if cse_var.name == high_prec_cse_var_name:\n+                        high_prec_cse_var = cse_var\n+                assert high_prec_cse_var is not None\n+            return high_prec_cse_var\n+        high_prec_var = find_high_prec_var(var_to_store, self.cse.cache)\n+        if high_prec_var and high_prec_var.dtype in DTYPE_TO_CPP:\n+            cache_key = get_lowp_to_high_prec_expr(\n+                var_to_store, high_prec_var.dtype, self\n+            )\n+            self.cse.cache[cache_key] = high_prec_var\n+        self.cache_high_prec_cse_var_before_lowp_store(value)\n+        self.cache_high_prec_cse_var_before_lowp_store(value)\n",
            "whole_hunk": "@@ -188,12 +188,12 @@ def is_to_lowp_dtype(expr):\n     return any(to_expr in expr for to_expr in to_exprs)\n \n \n-def get_lowp_to_fp32_expr(lowp_var, kernel):\n+def get_lowp_to_high_prec_expr(lowp_var, dtype, kernel):\n     if isinstance(kernel, CppVecKernel):\n-        return f\"at::vec::convert<float>({lowp_var})\"\n+        return f\"at::vec::convert<{DTYPE_TO_CPP[dtype]}>({lowp_var})\"\n     else:\n         assert isinstance(kernel, CppKernel)\n-        return f\"c10::convert<float>({lowp_var})\"\n+        return f\"c10::convert<{DTYPE_TO_CPP[dtype]}>({lowp_var})\"\n \n \n index_value_name_counter = 1\n@@ -1614,7 +1614,7 @@ class CppKernel(Kernel):\n         finally:\n             self._load_mask = prior\n \n-    def cache_fp32_cse_var_before_lowp_store(self, var_to_store):\n+    def cache_high_prec_cse_var_before_lowp_store(self, var_to_store):\n         \"\"\"\n         https://github.com/pytorch/pytorch/issues/115260\n         For FusedSchedulerNode[node1, node2], the node2 loads what node1 stores and the buffer is\n@@ -1651,26 +1651,29 @@ class CppKernel(Kernel):\n             # only need to cache fp32 cse var while var_to_store is lowp data\n             return\n \n-        def find_fp32_var(var, cache):\n-            fp32_cse_var = None\n-            fp32_cse_var_name = None\n+        def find_high_prec_var(var, cache):\n+            high_prec_cse_var = None\n+            high_prec_cse_var_name = None\n             for expr, cse_var in cache.items():\n                 if cse_var == var:\n                     if is_to_lowp_dtype(expr):\n                         m = re.search(r\"tmp\\d+\", expr)\n                         if m is not None:\n-                            fp32_cse_var_name = m.group()\n-            if fp32_cse_var_name:\n+                            high_prec_cse_var_name = m.group()\n+            if high_prec_cse_var_name:\n                 for cse_var in cache.values():\n-                    if cse_var.name == fp32_cse_var_name:\n-                        fp32_cse_var = cse_var\n+                    if cse_var.name == high_prec_cse_var_name:\n+                        high_prec_cse_var = cse_var\n                         break\n-                assert fp32_cse_var is not None\n-            return fp32_cse_var\n+                assert high_prec_cse_var is not None\n+            return high_prec_cse_var\n \n-        fp32_var = find_fp32_var(var_to_store, self.cse.cache)\n-        if fp32_var:\n-            self.cse.cache[get_lowp_to_fp32_expr(var_to_store, self)] = fp32_var\n+        high_prec_var = find_high_prec_var(var_to_store, self.cse.cache)\n+        if high_prec_var and high_prec_var.dtype in DTYPE_TO_CPP:\n+            cache_key = get_lowp_to_high_prec_expr(\n+                var_to_store, high_prec_var.dtype, self\n+            )\n+            self.cse.cache[cache_key] = high_prec_var\n \n     def scale_index_with_offset(\n         self, index: sympy.Expr, scale=1, itervar_idx=-1, offset=0\n@@ -1749,7 +1752,7 @@ class CppKernel(Kernel):\n     def store(self, name, index, value, mode=None):\n         assert \"buf\" in name\n         var = self.args.output(name)\n-        self.cache_fp32_cse_var_before_lowp_store(value)\n+        self.cache_high_prec_cse_var_before_lowp_store(value)\n         index = self.rename_indexing(index)\n         if mode is None:\n             line = f\"{var}[{cexpr_index(index)}] = {value};\"\n@@ -2344,7 +2347,7 @@ class CppVecKernel(CppKernel):\n             value = self.broadcast(value)\n         opt_ctx: OptimizationContext = get_current_node_opt_ctx()\n         var = self.args.output(name)\n-        self.cache_fp32_cse_var_before_lowp_store(value)\n+        self.cache_high_prec_cse_var_before_lowp_store(value)\n         index = self.rename_indexing(index)\n         code = self._get_store_line(value, var, index, V.graph.get_dtype(name))\n         self.stores.splice(code.map(lambda x: DeferredLine(name, x)))"
        }
    ]
},
{
    "Id": 382,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/7553c495147f3e21a1e27d392d277906a47768e7",
    "date": "2023-12-12T18:02:05+00:00",
    "message": "[S382174] Fix distributed debug w/ non-equal split (#115483)\n\nSummary:\nIn collectives, it's possible to have non-equal split that has a different implementation and the output tensor size will be different, e.g. https://www.internalfb.com/code/fbsource/[460afb1172b5]/fbcode/caffe2/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp?lines=3104. However, TORCH_DISTRIBUTED_DEBUG=DETAIL will assume the output tensor size is the same and does the check and will fail the job if they don't: https://fburl.com/code/mhte9ty8. c10d code should handle this.\n\nIdeally we should check the input size across ranks and make sure they're the same. Maybe for next diff.\n\nTest Plan: Test torchrec's TWRW w/ non-even split and it's working now.\n\nReviewed By: zhangruiskyline\n\nDifferential Revision: D52010942\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115483\nApproved by: https://github.com/kwen2501, https://github.com/fegin, https://github.com/XilunWu",
    "label": "YES",
    "changes": [
        {
            "name": "ProcessGroupWrapper.cpp",
            "path": "torch/csrc/distributed/c10d/ProcessGroupWrapper.cpp",
            "patches": [
                {
                    "old_start": 371,
                    "old_length": 6,
                    "new_start": 371,
                    "new_length": 15,
                    "hunk": "@@ -371,6 +371,15 @@ std::ostream& operator<<(\n   return output << collectiveInfo;\n }\n \n+bool check_same_size(const std::vector<at::Tensor>& input_tensors) {\n+  for (const auto& input_tensor : input_tensors) {\n+    if (!input_tensors[0].is_same_size(input_tensor)) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n } // namespace\n \n ProcessGroupWrapper::ProcessGroupWrapper(\n"
                },
                {
                    "old_start": 423,
                    "old_length": 7,
                    "new_start": 432,
                    "new_length": 11,
                    "hunk": "@@ -423,7 +432,11 @@ c10::intrusive_ptr<Work> ProcessGroupWrapper::allgather(\n     std::vector<std::vector<at::Tensor>>& outputTensors,\n     std::vector<at::Tensor>& inputTensors,\n     const AllgatherOptions& opts) {\n-  runCollectiveChecks(OpType::ALLGATHER, inputTensors);\n+  if (check_same_size(outputTensors.back())) {\n+    runCollectiveChecks(OpType::ALLGATHER, inputTensors);\n+  } else {\n+    runCollectiveChecks(OpType::ALLGATHER, {});\n+  }\n   return backend_->allgather(outputTensors, inputTensors, opts);\n }\n \n"
                },
                {
                    "old_start": 468,
                    "old_length": 7,
                    "new_start": 481,
                    "new_length": 11,
                    "hunk": "@@ -468,7 +481,11 @@ c10::intrusive_ptr<Work> ProcessGroupWrapper::reduce_scatter(\n     std::vector<at::Tensor>& outputTensors,\n     std::vector<std::vector<at::Tensor>>& inputTensors,\n     const ReduceScatterOptions& opts) {\n-  runCollectiveChecks(OpType::REDUCE_SCATTER, outputTensors);\n+  if (check_same_size(inputTensors.back())) {\n+    runCollectiveChecks(OpType::REDUCE_SCATTER, outputTensors);\n+  } else {\n+    runCollectiveChecks(OpType::REDUCE_SCATTER, {});\n+  }\n   return backend_->reduce_scatter(outputTensors, inputTensors, opts);\n }\n "
                }
            ],
            "whole_deleted": "-  runCollectiveChecks(OpType::ALLGATHER, inputTensors);\n-  runCollectiveChecks(OpType::REDUCE_SCATTER, outputTensors);\n",
            "whole_added": "+bool check_same_size(const std::vector<at::Tensor>& input_tensors) {\n+  for (const auto& input_tensor : input_tensors) {\n+    if (!input_tensors[0].is_same_size(input_tensor)) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+  if (check_same_size(outputTensors.back())) {\n+    runCollectiveChecks(OpType::ALLGATHER, inputTensors);\n+  } else {\n+    runCollectiveChecks(OpType::ALLGATHER, {});\n+  }\n+  if (check_same_size(inputTensors.back())) {\n+    runCollectiveChecks(OpType::REDUCE_SCATTER, outputTensors);\n+  } else {\n+    runCollectiveChecks(OpType::REDUCE_SCATTER, {});\n+  }\n",
            "whole_hunk": "@@ -371,6 +371,15 @@ std::ostream& operator<<(\n   return output << collectiveInfo;\n }\n \n+bool check_same_size(const std::vector<at::Tensor>& input_tensors) {\n+  for (const auto& input_tensor : input_tensors) {\n+    if (!input_tensors[0].is_same_size(input_tensor)) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n } // namespace\n \n ProcessGroupWrapper::ProcessGroupWrapper(\n@@ -423,7 +432,11 @@ c10::intrusive_ptr<Work> ProcessGroupWrapper::allgather(\n     std::vector<std::vector<at::Tensor>>& outputTensors,\n     std::vector<at::Tensor>& inputTensors,\n     const AllgatherOptions& opts) {\n-  runCollectiveChecks(OpType::ALLGATHER, inputTensors);\n+  if (check_same_size(outputTensors.back())) {\n+    runCollectiveChecks(OpType::ALLGATHER, inputTensors);\n+  } else {\n+    runCollectiveChecks(OpType::ALLGATHER, {});\n+  }\n   return backend_->allgather(outputTensors, inputTensors, opts);\n }\n \n@@ -468,7 +481,11 @@ c10::intrusive_ptr<Work> ProcessGroupWrapper::reduce_scatter(\n     std::vector<at::Tensor>& outputTensors,\n     std::vector<std::vector<at::Tensor>>& inputTensors,\n     const ReduceScatterOptions& opts) {\n-  runCollectiveChecks(OpType::REDUCE_SCATTER, outputTensors);\n+  if (check_same_size(inputTensors.back())) {\n+    runCollectiveChecks(OpType::REDUCE_SCATTER, outputTensors);\n+  } else {\n+    runCollectiveChecks(OpType::REDUCE_SCATTER, {});\n+  }\n   return backend_->reduce_scatter(outputTensors, inputTensors, opts);\n }\n "
        }
    ]
},
{
    "Id": 6,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/e14d1d10ef9d24bf43366ac1f05a5aa8b732707b",
    "date": "2024-07-18T00:43:53+00:00",
    "message": "Unwrap Identity in prepare indexing (#130967)\n\nWe wrap indexing calculation in the concat kernel in `Identity` so that we do not expand int32 intermediates to int64. This was causing an issue where the index simplified to an integer and would not hit an intended [path](https://github.com/pytorch/pytorch/blob/752c81789829cfce94f9664db97cc45aaae8ce32/torch/_inductor/codegen/triton.py#L1554) which would do wrapping with tl.full.\n\nI couldn't generate a minimal repro to add as test but I have a repro you can check here: P1483831261 There is already a test that we dont expand the int32 intermediates to int64.\n\nDifferential Revision: [D59871850](https://our.internmc.facebook.com/intern/diff/D59871850)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130967\nApproved by: https://github.com/Chillee, https://github.com/jansel",
    "label": "YES",
    "changes": [
        {
            "name": "simd.py",
            "path": "torch/_inductor/codegen/simd.py",
            "patches": [
                {
                    "old_start": 29,
                    "old_length": 7,
                    "new_start": 29,
                    "new_length": 7,
                    "hunk": "@@ -29,7 +29,7 @@ import sympy\n import torch\n import torch._logging\n \n-from torch.utils._sympy.functions import FloorDiv, ModularIndexing\n+from torch.utils._sympy.functions import FloorDiv, Identity, ModularIndexing\n from torch.utils._sympy.symbol import free_symbol_is_type, symbol_is_type, SymT\n from ..._dynamo.utils import counters\n from .. import config, ir, scheduler\n"
                },
                {
                    "old_start": 694,
                    "old_length": 7,
                    "new_start": 694,
                    "new_length": 16,
                    "hunk": "@@ -694,7 +694,16 @@ class SIMDKernel(Kernel):\n                     replacements = {a: V.graph.sizevars.lookup_precomputed_size(a)}\n                     index = sympy_subs(index, replacements)\n \n-        return self.codegen_indexing(self.simplify_indexing(index))\n+        simp_index = self.simplify_indexing(index)\n+\n+        # Now that we are done simplifying we can unwrap Identity so that downstream handling\n+        # for its contained expression will work. previously, tl.full wrapping of sympy.Integer\n+        # would not occur\n+        simp_index = (\n+            simp_index if not isinstance(simp_index, Identity) else simp_index.args[0]\n+        )\n+\n+        return self.codegen_indexing(simp_index)\n \n     def active_range_trees(self, reorder=False):\n         trees = ["
                }
            ],
            "whole_deleted": "-from torch.utils._sympy.functions import FloorDiv, ModularIndexing\n-        return self.codegen_indexing(self.simplify_indexing(index))\n",
            "whole_added": "+from torch.utils._sympy.functions import FloorDiv, Identity, ModularIndexing\n+        simp_index = self.simplify_indexing(index)\n+\n+        # Now that we are done simplifying we can unwrap Identity so that downstream handling\n+        # for its contained expression will work. previously, tl.full wrapping of sympy.Integer\n+        # would not occur\n+        simp_index = (\n+            simp_index if not isinstance(simp_index, Identity) else simp_index.args[0]\n+        )\n+\n+        return self.codegen_indexing(simp_index)\n",
            "whole_hunk": "@@ -29,7 +29,7 @@ import sympy\n import torch\n import torch._logging\n \n-from torch.utils._sympy.functions import FloorDiv, ModularIndexing\n+from torch.utils._sympy.functions import FloorDiv, Identity, ModularIndexing\n from torch.utils._sympy.symbol import free_symbol_is_type, symbol_is_type, SymT\n from ..._dynamo.utils import counters\n from .. import config, ir, scheduler\n@@ -694,7 +694,16 @@ class SIMDKernel(Kernel):\n                     replacements = {a: V.graph.sizevars.lookup_precomputed_size(a)}\n                     index = sympy_subs(index, replacements)\n \n-        return self.codegen_indexing(self.simplify_indexing(index))\n+        simp_index = self.simplify_indexing(index)\n+\n+        # Now that we are done simplifying we can unwrap Identity so that downstream handling\n+        # for its contained expression will work. previously, tl.full wrapping of sympy.Integer\n+        # would not occur\n+        simp_index = (\n+            simp_index if not isinstance(simp_index, Identity) else simp_index.args[0]\n+        )\n+\n+        return self.codegen_indexing(simp_index)\n \n     def active_range_trees(self, reorder=False):\n         trees = ["
        }
    ]
},
{
    "Id": 218,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/15745a52b0d7dc5c3d0ecc061b1604288064f641",
    "date": "2024-04-09T10:00:30+00:00",
    "message": "Inductor: don't change the stride_order of FlexibleLayout if it's already the same as required (#122945)\n\n## Pitch\nFixes https://github.com/pytorch/pytorch/issues/122489.\nDon't change the `stride_order` of `FlexibleLayout` if it already has the stride with the order required.\n\n## Description\nFor a layout that's both contiguous and channels last contiguous (for example `size=[s0, 1, 28, 28]`, `stride=[784, 784, 28, 1]` where the `C` dim is `1`), the behavior of calling  [require_stride_order](https://github.com/pytorch/pytorch/blob/069270db60112704c6387cf994cf9b7298731699/torch/_inductor/ir.py#L4053) (where the order is specified as channels last) on it is different when it's a `FixedLayout` or a `FlexibleLayout`.\n\n- For a `FixedLayout`, the size and stride is unchanged after the call: `size=[s0, 1, 28, 28]`, `stride=[784, 784, 28, 1]`.\n- For a `FlexibleLayout`, it will become `size=[s0, 1, 28, 28]`, `stride=[784, 1, 28, 1])`.\n\nWhen weight is not prepacked (in dynamic shapes cases), the Conv extern kernel returns output in channels **first** for input with `size=[s0, 1, 28, 28]`, `stride=[784, 784, 28, 1]` but output in channels **last** for `size=[s0, 1, 28, 28]`, `stride=[784, 1, 28, 1])`.\n\nIn this PR, for a `FlexibleLayout`, we add a check to see if it already has the stride in the required order. If that's the case, we don't change its stride order when freezing it. This makes the behavior of  calling [require_stride_order](https://github.com/pytorch/pytorch/blob/069270db60112704c6387cf994cf9b7298731699/torch/_inductor/ir.py#L4053) aligned for `FixedLayout` and `FlexibleLayout`.\n\n## Additional context\nFor a `FixedLayout`, when calling  [require_stride_order](https://github.com/pytorch/pytorch/blob/069270db60112704c6387cf994cf9b7298731699/torch/_inductor/ir.py#L4053), it will firstly run into [x.get_layout().is_stride_ordered(order)](https://github.com/pytorch/pytorch/blob/069270db60112704c6387cf994cf9b7298731699/torch/_inductor/ir.py#L4067-L4070) to check if it's already ordered as expected.\n\nIf it is a `FlexibleLayout`, when calling  [require_stride_order](https://github.com/pytorch/pytorch/blob/069270db60112704c6387cf994cf9b7298731699/torch/_inductor/ir.py#L4053),  it runs into [as_storage_and_layout](https://github.com/pytorch/pytorch/blob/069270db60112704c6387cf994cf9b7298731699/torch/_inductor/ir.py#L4063-L4065), which will always [freeze_layout_with_stride_order](https://github.com/pytorch/pytorch/blob/069270db60112704c6387cf994cf9b7298731699/torch/_inductor/ir.py#L1805) and will always call [as_stride_order](https://github.com/pytorch/pytorch/blob/069270db60112704c6387cf994cf9b7298731699/torch/_inductor/ir.py#L2909), without checking if the default stride of this `FlexibleLayout` (which has been realized before) is already as expected ([link](https://github.com/pytorch/pytorch/blob/069270db60112704c6387cf994cf9b7298731699/torch/_inductor/ir.py#L2693-L2700)).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/122945\nApproved by: https://github.com/jgong5, https://github.com/jansel",
    "label": "YES",
    "changes": [
        {
            "name": "test_cpu_repro.py",
            "path": "test/inductor/test_cpu_repro.py",
            "patches": [
                {
                    "old_start": 399,
                    "old_length": 13,
                    "new_start": 399,
                    "new_length": 19,
                    "hunk": "@@ -399,13 +399,19 @@ class CPUReproTests(TestCase):\n         # Reproducer from the maml_omniglot model in Torchbench\n         in_channel = 1\n         out_channel = 3\n-        mod = M(in_channel, out_channel).eval()\n-        v = torch.randn(5, in_channel, 15, 15)\n-        with torch.no_grad():\n-            self.common(\n-                mod,\n-                (v,),\n-            )\n+        amp_enabled_configs = [False]\n+        if torch.ops.mkldnn._is_mkldnn_bf16_supported():\n+            # When amp is enabled here, the input to Conv is a FlexibleLayout.\n+            # While it's disabled, the input is a FixedLayout.\n+            amp_enabled_configs.append(True)\n+        for amp_enabled in amp_enabled_configs:\n+            mod = M(in_channel, out_channel).eval()\n+            v = torch.randn(5, in_channel, 15, 15)\n+            with torch.no_grad(), torch.cpu.amp.autocast(enabled=amp_enabled):\n+                self.common(\n+                    mod,\n+                    (v,),\n+                )\n \n     @unittest.skipIf(not torch._C._has_mkldnn, \"MKLDNN is not enabled\")\n     @patch(\"torch.cuda.is_available\", lambda: False)\n"
                }
            ],
            "whole_deleted": "-        mod = M(in_channel, out_channel).eval()\n-        v = torch.randn(5, in_channel, 15, 15)\n-        with torch.no_grad():\n-            self.common(\n-                mod,\n-                (v,),\n-            )\n",
            "whole_added": "+        amp_enabled_configs = [False]\n+        if torch.ops.mkldnn._is_mkldnn_bf16_supported():\n+            # When amp is enabled here, the input to Conv is a FlexibleLayout.\n+            # While it's disabled, the input is a FixedLayout.\n+            amp_enabled_configs.append(True)\n+        for amp_enabled in amp_enabled_configs:\n+            mod = M(in_channel, out_channel).eval()\n+            v = torch.randn(5, in_channel, 15, 15)\n+            with torch.no_grad(), torch.cpu.amp.autocast(enabled=amp_enabled):\n+                self.common(\n+                    mod,\n+                    (v,),\n+                )\n",
            "whole_hunk": "@@ -399,13 +399,19 @@ class CPUReproTests(TestCase):\n         # Reproducer from the maml_omniglot model in Torchbench\n         in_channel = 1\n         out_channel = 3\n-        mod = M(in_channel, out_channel).eval()\n-        v = torch.randn(5, in_channel, 15, 15)\n-        with torch.no_grad():\n-            self.common(\n-                mod,\n-                (v,),\n-            )\n+        amp_enabled_configs = [False]\n+        if torch.ops.mkldnn._is_mkldnn_bf16_supported():\n+            # When amp is enabled here, the input to Conv is a FlexibleLayout.\n+            # While it's disabled, the input is a FixedLayout.\n+            amp_enabled_configs.append(True)\n+        for amp_enabled in amp_enabled_configs:\n+            mod = M(in_channel, out_channel).eval()\n+            v = torch.randn(5, in_channel, 15, 15)\n+            with torch.no_grad(), torch.cpu.amp.autocast(enabled=amp_enabled):\n+                self.common(\n+                    mod,\n+                    (v,),\n+                )\n \n     @unittest.skipIf(not torch._C._has_mkldnn, \"MKLDNN is not enabled\")\n     @patch(\"torch.cuda.is_available\", lambda: False)\n"
        },
        {
            "name": "ir.py",
            "path": "torch/_inductor/ir.py",
            "patches": [
                {
                    "old_start": 4086,
                    "old_length": 9,
                    "new_start": 4086,
                    "new_length": 25,
                    "hunk": "@@ -4086,9 +4086,25 @@ class ExternKernel(InputsKernel):\n             while isinstance(x.get_layout(), NonOwningLayout):\n                 x = x.get_layout().view\n             if isinstance(x.get_layout(), FlexibleLayout):\n+                # If the the FlexibleLayout already has the size and stride in the required order,\n+                # freeze it to a FixedLayout by using its current size and stride.\n+                # The behavior of using its current size and stride or the given order can be different\n+                # if the size and stride has ambiguilty, for example for a 4D input where the iC = 1:\n+                # size=[s0, 1, 28, 28], stride=[784, 784, 28, 1]. If the required order is [3, 0, 2, 1] (channels last),\n+                # the current size and stride already satisfies this order.\n+                # However by freezing it to the required order, the layout will be changed to:\n+                # size=[s0, 1, 28, 28], stride=[784, 1, 28, 1]), which is not actually necessary.\n+\n                 # fix flexiblelayout to be FixedLayout with stride_order\n                 as_storage_and_layout(\n-                    x, freeze=True, want_contiguous=False, stride_order=order\n+                    x,\n+                    freeze=True,\n+                    want_contiguous=False,\n+                    stride_order=get_stride_order(\n+                        V.graph.sizevars.size_hints(x.get_layout().stride)\n+                    )\n+                    if is_stride_order_storage_and_layout(x, order)\n+                    else order,\n                 )\n                 return x\n             elif isinstance("
                }
            ],
            "whole_deleted": "-                    x, freeze=True, want_contiguous=False, stride_order=order\n",
            "whole_added": "+                # If the the FlexibleLayout already has the size and stride in the required order,\n+                # freeze it to a FixedLayout by using its current size and stride.\n+                # The behavior of using its current size and stride or the given order can be different\n+                # if the size and stride has ambiguilty, for example for a 4D input where the iC = 1:\n+                # size=[s0, 1, 28, 28], stride=[784, 784, 28, 1]. If the required order is [3, 0, 2, 1] (channels last),\n+                # the current size and stride already satisfies this order.\n+                # However by freezing it to the required order, the layout will be changed to:\n+                # size=[s0, 1, 28, 28], stride=[784, 1, 28, 1]), which is not actually necessary.\n+\n+                    x,\n+                    freeze=True,\n+                    want_contiguous=False,\n+                    stride_order=get_stride_order(\n+                        V.graph.sizevars.size_hints(x.get_layout().stride)\n+                    )\n+                    if is_stride_order_storage_and_layout(x, order)\n+                    else order,\n",
            "whole_hunk": "@@ -4086,9 +4086,25 @@ class ExternKernel(InputsKernel):\n             while isinstance(x.get_layout(), NonOwningLayout):\n                 x = x.get_layout().view\n             if isinstance(x.get_layout(), FlexibleLayout):\n+                # If the the FlexibleLayout already has the size and stride in the required order,\n+                # freeze it to a FixedLayout by using its current size and stride.\n+                # The behavior of using its current size and stride or the given order can be different\n+                # if the size and stride has ambiguilty, for example for a 4D input where the iC = 1:\n+                # size=[s0, 1, 28, 28], stride=[784, 784, 28, 1]. If the required order is [3, 0, 2, 1] (channels last),\n+                # the current size and stride already satisfies this order.\n+                # However by freezing it to the required order, the layout will be changed to:\n+                # size=[s0, 1, 28, 28], stride=[784, 1, 28, 1]), which is not actually necessary.\n+\n                 # fix flexiblelayout to be FixedLayout with stride_order\n                 as_storage_and_layout(\n-                    x, freeze=True, want_contiguous=False, stride_order=order\n+                    x,\n+                    freeze=True,\n+                    want_contiguous=False,\n+                    stride_order=get_stride_order(\n+                        V.graph.sizevars.size_hints(x.get_layout().stride)\n+                    )\n+                    if is_stride_order_storage_and_layout(x, order)\n+                    else order,\n                 )\n                 return x\n             elif isinstance("
        }
    ]
},
{
    "Id": 447,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/679ca510b0fc5d027c69f2946db8785c16cbb244",
    "date": "2023-11-06T16:56:09+00:00",
    "message": "Revert \"[Cmake] Check that gcc-9.4 or newer is used (#112858)\"\n\nThis reverts commit ad894cd0728e97c649cd9b33e1f98b18fa12a1da.\n\nReverted https://github.com/pytorch/pytorch/pull/112858 on behalf of https://github.com/PaliC due to breaking internal tests (check diff for test page) ([comment](https://github.com/pytorch/pytorch/pull/112858#issuecomment-1795485009))",
    "label": "YES",
    "changes": [
        {
            "name": "CMakeLists.txt",
            "path": "CMakeLists.txt",
            "patches": [
                {
                    "old_start": 43,
                    "old_length": 12,
                    "new_start": 43,
                    "new_length": 7,
                    "hunk": "@@ -43,12 +43,7 @@ set(CMAKE_C_STANDARD   11 CACHE STRING \"The C standard whose features are reques\n # ---[ Utils\n include(cmake/public/utils.cmake)\n \n-# --- [ Check that minimal gcc version is 9.4+\n-if(CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 9.4)\n-  message(FATAL \"GCC-9.4 or newer is required to compile PyTorch, but found ${CMAKE_CXX_COMPILER_VERSION}\")\n-endif()\n-\n-if(LINUX)\n+if(CMAKE_SYSTEM_NAME STREQUAL \"Linux\")\n   include(cmake/CheckAbi.cmake)\n   string(APPEND CMAKE_CXX_FLAGS \" -D_GLIBCXX_USE_CXX11_ABI=${GLIBCXX_USE_CXX11_ABI}\")\n   string(APPEND CMAKE_CUDA_FLAGS \" -D_GLIBCXX_USE_CXX11_ABI=${GLIBCXX_USE_CXX11_ABI}\")\n"
                },
                {
                    "old_start": 827,
                    "old_length": 7,
                    "new_start": 822,
                    "new_length": 7,
                    "hunk": "@@ -827,7 +822,7 @@ endif()\n include(cmake/Allowlist.cmake)\n \n # ---[ Set link flag, handle additional deps for gcc 4.8 and above\n-if(CMAKE_COMPILER_IS_GNUCXX AND NOT ANDROID)\n+if(CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 4.8.0 AND NOT ANDROID)\n   message(STATUS \"GCC ${CMAKE_CXX_COMPILER_VERSION}: Adding gcc and gcc_s libs to link line\")\n   list(APPEND Caffe2_DEPENDENCY_LIBS gcc_s gcc)\n endif()\n"
                },
                {
                    "old_start": 856,
                    "old_length": 19,
                    "new_start": 851,
                    "new_length": 26,
                    "hunk": "@@ -856,19 +851,26 @@ if(NOT MSVC)\n   append_cxx_flag_if_supported(\"-Wno-unused-result\" CMAKE_CXX_FLAGS)\n   append_cxx_flag_if_supported(\"-Wno-strict-overflow\" CMAKE_CXX_FLAGS)\n   append_cxx_flag_if_supported(\"-Wno-strict-aliasing\" CMAKE_CXX_FLAGS)\n-  append_cxx_flag_if_supported(\"-Wno-stringop-overflow\" CMAKE_CXX_FLAGS)\n   append_cxx_flag_if_supported(\"-Wvla-extension\" CMAKE_CXX_FLAGS)\n-  append_cxx_flag_if_supported(\"-Wsuggest-override\" CMAKE_CXX_FLAGS)\n   append_cxx_flag_if_supported(\"-Wnewline-eof\" CMAKE_CXX_FLAGS)\n   append_cxx_flag_if_supported(\"-Winconsistent-missing-override\" CMAKE_CXX_FLAGS)\n   append_cxx_flag_if_supported(\"-Winconsistent-missing-destructor-override\" CMAKE_CXX_FLAGS)\n   if(\"${CMAKE_CXX_COMPILER_ID}\" MATCHES \"Clang\")\n     string(APPEND CMAKE_CXX_FLAGS \" -Wno-pass-failed\")\n   endif()\n+  if(CMAKE_COMPILER_IS_GNUCXX AND NOT (CMAKE_CXX_COMPILER_VERSION VERSION_LESS 7.0.0))\n+    string(APPEND CMAKE_CXX_FLAGS \" -Wno-stringop-overflow\")\n+  endif()\n   if(CMAKE_COMPILER_IS_GNUCXX)\n     # Suppress \"The ABI for passing parameters with 64-byte alignment has changed in GCC 4.6\"\n     string(APPEND CMAKE_CXX_FLAGS \" -Wno-psabi\")\n   endif()\n+  if(NOT CMAKE_COMPILER_IS_GNUCXX OR GCC_VERSION VERSION_GREATER_EQUAL 9.2)\n+    # Prior to GCC 9.2, this warning misfires when a method is\n+    # labeled \"final\".\n+    # https://gcc.gnu.org/bugzilla/show_bug.cgi?id=78010\n+    append_cxx_flag_if_supported(\"-Wsuggest-override\" CMAKE_CXX_FLAGS)\n+  endif()\n \n   # Use ld.gold if available, fall back to ld.bfd (the default ld) if not\n   if(USE_GOLD_LINKER)\n"
                }
            ],
            "whole_deleted": "-# --- [ Check that minimal gcc version is 9.4+\n-if(CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 9.4)\n-  message(FATAL \"GCC-9.4 or newer is required to compile PyTorch, but found ${CMAKE_CXX_COMPILER_VERSION}\")\n-endif()\n-\n-if(LINUX)\n-if(CMAKE_COMPILER_IS_GNUCXX AND NOT ANDROID)\n-  append_cxx_flag_if_supported(\"-Wno-stringop-overflow\" CMAKE_CXX_FLAGS)\n-  append_cxx_flag_if_supported(\"-Wsuggest-override\" CMAKE_CXX_FLAGS)\n",
            "whole_added": "+if(CMAKE_SYSTEM_NAME STREQUAL \"Linux\")\n+if(CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 4.8.0 AND NOT ANDROID)\n+  if(CMAKE_COMPILER_IS_GNUCXX AND NOT (CMAKE_CXX_COMPILER_VERSION VERSION_LESS 7.0.0))\n+    string(APPEND CMAKE_CXX_FLAGS \" -Wno-stringop-overflow\")\n+  endif()\n+  if(NOT CMAKE_COMPILER_IS_GNUCXX OR GCC_VERSION VERSION_GREATER_EQUAL 9.2)\n+    # Prior to GCC 9.2, this warning misfires when a method is\n+    # labeled \"final\".\n+    # https://gcc.gnu.org/bugzilla/show_bug.cgi?id=78010\n+    append_cxx_flag_if_supported(\"-Wsuggest-override\" CMAKE_CXX_FLAGS)\n+  endif()\n",
            "whole_hunk": "@@ -43,12 +43,7 @@ set(CMAKE_C_STANDARD   11 CACHE STRING \"The C standard whose features are reques\n # ---[ Utils\n include(cmake/public/utils.cmake)\n \n-# --- [ Check that minimal gcc version is 9.4+\n-if(CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 9.4)\n-  message(FATAL \"GCC-9.4 or newer is required to compile PyTorch, but found ${CMAKE_CXX_COMPILER_VERSION}\")\n-endif()\n-\n-if(LINUX)\n+if(CMAKE_SYSTEM_NAME STREQUAL \"Linux\")\n   include(cmake/CheckAbi.cmake)\n   string(APPEND CMAKE_CXX_FLAGS \" -D_GLIBCXX_USE_CXX11_ABI=${GLIBCXX_USE_CXX11_ABI}\")\n   string(APPEND CMAKE_CUDA_FLAGS \" -D_GLIBCXX_USE_CXX11_ABI=${GLIBCXX_USE_CXX11_ABI}\")\n@@ -827,7 +822,7 @@ endif()\n include(cmake/Allowlist.cmake)\n \n # ---[ Set link flag, handle additional deps for gcc 4.8 and above\n-if(CMAKE_COMPILER_IS_GNUCXX AND NOT ANDROID)\n+if(CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 4.8.0 AND NOT ANDROID)\n   message(STATUS \"GCC ${CMAKE_CXX_COMPILER_VERSION}: Adding gcc and gcc_s libs to link line\")\n   list(APPEND Caffe2_DEPENDENCY_LIBS gcc_s gcc)\n endif()\n@@ -856,19 +851,26 @@ if(NOT MSVC)\n   append_cxx_flag_if_supported(\"-Wno-unused-result\" CMAKE_CXX_FLAGS)\n   append_cxx_flag_if_supported(\"-Wno-strict-overflow\" CMAKE_CXX_FLAGS)\n   append_cxx_flag_if_supported(\"-Wno-strict-aliasing\" CMAKE_CXX_FLAGS)\n-  append_cxx_flag_if_supported(\"-Wno-stringop-overflow\" CMAKE_CXX_FLAGS)\n   append_cxx_flag_if_supported(\"-Wvla-extension\" CMAKE_CXX_FLAGS)\n-  append_cxx_flag_if_supported(\"-Wsuggest-override\" CMAKE_CXX_FLAGS)\n   append_cxx_flag_if_supported(\"-Wnewline-eof\" CMAKE_CXX_FLAGS)\n   append_cxx_flag_if_supported(\"-Winconsistent-missing-override\" CMAKE_CXX_FLAGS)\n   append_cxx_flag_if_supported(\"-Winconsistent-missing-destructor-override\" CMAKE_CXX_FLAGS)\n   if(\"${CMAKE_CXX_COMPILER_ID}\" MATCHES \"Clang\")\n     string(APPEND CMAKE_CXX_FLAGS \" -Wno-pass-failed\")\n   endif()\n+  if(CMAKE_COMPILER_IS_GNUCXX AND NOT (CMAKE_CXX_COMPILER_VERSION VERSION_LESS 7.0.0))\n+    string(APPEND CMAKE_CXX_FLAGS \" -Wno-stringop-overflow\")\n+  endif()\n   if(CMAKE_COMPILER_IS_GNUCXX)\n     # Suppress \"The ABI for passing parameters with 64-byte alignment has changed in GCC 4.6\"\n     string(APPEND CMAKE_CXX_FLAGS \" -Wno-psabi\")\n   endif()\n+  if(NOT CMAKE_COMPILER_IS_GNUCXX OR GCC_VERSION VERSION_GREATER_EQUAL 9.2)\n+    # Prior to GCC 9.2, this warning misfires when a method is\n+    # labeled \"final\".\n+    # https://gcc.gnu.org/bugzilla/show_bug.cgi?id=78010\n+    append_cxx_flag_if_supported(\"-Wsuggest-override\" CMAKE_CXX_FLAGS)\n+  endif()\n \n   # Use ld.gold if available, fall back to ld.bfd (the default ld) if not\n   if(USE_GOLD_LINKER)\n"
        },
        {
            "name": "CMakeLists.txt",
            "path": "caffe2/CMakeLists.txt",
            "patches": [
                {
                    "old_start": 533,
                    "old_length": 7,
                    "new_start": 533,
                    "new_length": 7,
                    "hunk": "@@ -533,7 +533,7 @@ if(NOT MSVC)\n   set_source_files_properties(${TORCH_SRC_DIR}/csrc/jit/tensorexpr/llvm_jit.cpp PROPERTIES COMPILE_FLAGS -Wno-noexcept-type)\n endif()\n # Disable certain warnings for GCC-9.X\n-if(CMAKE_COMPILER_IS_GNUCXX)\n+if(CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 9.0.0))\n   # See https://github.com/pytorch/pytorch/issues/38856\n   set_source_files_properties(${TORCH_SRC_DIR}/csrc/jit/tensorexpr/llvm_jit.cpp PROPERTIES COMPILE_FLAGS \"-Wno-redundant-move -Wno-noexcept-type\")\n   set_source_files_properties(${TORCH_SRC_DIR}/csrc/jit/tensorexpr/llvm_codegen.cpp PROPERTIES COMPILE_FLAGS \"-Wno-init-list-lifetime\")\n"
                }
            ],
            "whole_deleted": "-if(CMAKE_COMPILER_IS_GNUCXX)\n",
            "whole_added": "+if(CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 9.0.0))\n",
            "whole_hunk": "@@ -533,7 +533,7 @@ if(NOT MSVC)\n   set_source_files_properties(${TORCH_SRC_DIR}/csrc/jit/tensorexpr/llvm_jit.cpp PROPERTIES COMPILE_FLAGS -Wno-noexcept-type)\n endif()\n # Disable certain warnings for GCC-9.X\n-if(CMAKE_COMPILER_IS_GNUCXX)\n+if(CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 9.0.0))\n   # See https://github.com/pytorch/pytorch/issues/38856\n   set_source_files_properties(${TORCH_SRC_DIR}/csrc/jit/tensorexpr/llvm_jit.cpp PROPERTIES COMPILE_FLAGS \"-Wno-redundant-move -Wno-noexcept-type\")\n   set_source_files_properties(${TORCH_SRC_DIR}/csrc/jit/tensorexpr/llvm_codegen.cpp PROPERTIES COMPILE_FLAGS \"-Wno-init-list-lifetime\")\n"
        },
        {
            "name": "pybind_state.h",
            "path": "caffe2/python/pybind_state.h",
            "patches": [
                {
                    "old_start": 257,
                    "old_length": 7,
                    "new_start": 257,
                    "new_length": 7,
                    "hunk": "@@ -257,7 +257,7 @@ class TensorFeeder : public BlobFeederBase {\n       const DeviceOption& option,\n       PyArrayObject* original_array,\n       Blob* blob,\n-      bool in_place) override {\n+      bool in_place) {\n     if (in_place) {\n       FeedTensor(\n           option,\n"
                }
            ],
            "whole_deleted": "-      bool in_place) override {\n",
            "whole_added": "+      bool in_place) {\n",
            "whole_hunk": "@@ -257,7 +257,7 @@ class TensorFeeder : public BlobFeederBase {\n       const DeviceOption& option,\n       PyArrayObject* original_array,\n       Blob* blob,\n-      bool in_place) override {\n+      bool in_place) {\n     if (in_place) {\n       FeedTensor(\n           option,\n"
        },
        {
            "name": "Codegen.cmake",
            "path": "cmake/Codegen.cmake",
            "patches": [
                {
                    "old_start": 325,
                    "old_length": 7,
                    "new_start": 325,
                    "new_length": 7,
                    "hunk": "@@ -325,7 +325,7 @@ if(INTERN_BUILD_ATEN_OPS)\n         set(EXTRA_FLAGS \"-DCPU_CAPABILITY=${CPU_CAPABILITY} -DCPU_CAPABILITY_${CPU_CAPABILITY}\")\n       endif(MSVC)\n       # Disable certain warnings for GCC-9.X\n-      if(CMAKE_COMPILER_IS_GNUCXX)\n+      if(CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 9.0.0))\n         if((\"${NAME}\" STREQUAL \"native/cpu/GridSamplerKernel.cpp\") AND (\"${CPU_CAPABILITY}\" STREQUAL \"DEFAULT\"))\n           # See https://github.com/pytorch/pytorch/issues/38855\n           set(EXTRA_FLAGS \"${EXTRA_FLAGS} -Wno-uninitialized\")\n"
                }
            ],
            "whole_deleted": "-      if(CMAKE_COMPILER_IS_GNUCXX)\n",
            "whole_added": "+      if(CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 9.0.0))\n",
            "whole_hunk": "@@ -325,7 +325,7 @@ if(INTERN_BUILD_ATEN_OPS)\n         set(EXTRA_FLAGS \"-DCPU_CAPABILITY=${CPU_CAPABILITY} -DCPU_CAPABILITY_${CPU_CAPABILITY}\")\n       endif(MSVC)\n       # Disable certain warnings for GCC-9.X\n-      if(CMAKE_COMPILER_IS_GNUCXX)\n+      if(CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 9.0.0))\n         if((\"${NAME}\" STREQUAL \"native/cpu/GridSamplerKernel.cpp\") AND (\"${CPU_CAPABILITY}\" STREQUAL \"DEFAULT\"))\n           # See https://github.com/pytorch/pytorch/issues/38855\n           set(EXTRA_FLAGS \"${EXTRA_FLAGS} -Wno-uninitialized\")\n"
        },
        {
            "name": "Dependencies.cmake",
            "path": "cmake/Dependencies.cmake",
            "patches": [
                {
                    "old_start": 633,
                    "old_length": 6,
                    "new_start": 633,
                    "new_length": 14,
                    "hunk": "@@ -633,6 +633,14 @@ if(USE_XNNPACK AND NOT USE_SYSTEM_XNNPACK)\n \n     # Revert to whatever it was before\n     set(CMAKE_POSITION_INDEPENDENT_CODE ${__caffe2_CMAKE_POSITION_INDEPENDENT_CODE_FLAG})\n+\n+    # Workaround for https://github.com/pytorch/pytorch/issues/47292\n+    if(CMAKE_BUILD_TYPE STREQUAL \"Debug\" AND CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_LESS 7.5.0))\n+      # Compiling qu8-requantization/precise-psimd.c without any optimization flags on gcc-7.4 or older i\n+      # Fails with internal compiler error\n+      # Workaround by forcing -O1 for XNNPACK (i.e. build it with RelWithDebInfo)\n+      set_property(TARGET XNNPACK APPEND_STRING PROPERTY COMPILE_FLAGS \"-O1\")\n+    endif()\n   endif()\n \n   include_directories(SYSTEM ${XNNPACK_INCLUDE_DIR})\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+\n+    # Workaround for https://github.com/pytorch/pytorch/issues/47292\n+    if(CMAKE_BUILD_TYPE STREQUAL \"Debug\" AND CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_LESS 7.5.0))\n+      # Compiling qu8-requantization/precise-psimd.c without any optimization flags on gcc-7.4 or older i\n+      # Fails with internal compiler error\n+      # Workaround by forcing -O1 for XNNPACK (i.e. build it with RelWithDebInfo)\n+      set_property(TARGET XNNPACK APPEND_STRING PROPERTY COMPILE_FLAGS \"-O1\")\n+    endif()\n",
            "whole_hunk": "@@ -633,6 +633,14 @@ if(USE_XNNPACK AND NOT USE_SYSTEM_XNNPACK)\n \n     # Revert to whatever it was before\n     set(CMAKE_POSITION_INDEPENDENT_CODE ${__caffe2_CMAKE_POSITION_INDEPENDENT_CODE_FLAG})\n+\n+    # Workaround for https://github.com/pytorch/pytorch/issues/47292\n+    if(CMAKE_BUILD_TYPE STREQUAL \"Debug\" AND CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_LESS 7.5.0))\n+      # Compiling qu8-requantization/precise-psimd.c without any optimization flags on gcc-7.4 or older i\n+      # Fails with internal compiler error\n+      # Workaround by forcing -O1 for XNNPACK (i.e. build it with RelWithDebInfo)\n+      set_property(TARGET XNNPACK APPEND_STRING PROPERTY COMPILE_FLAGS \"-O1\")\n+    endif()\n   endif()\n \n   include_directories(SYSTEM ${XNNPACK_INCLUDE_DIR})\n"
        },
        {
            "name": "net_observer_reporter_print.h",
            "path": "modules/observers/net_observer_reporter_print.h",
            "patches": [
                {
                    "old_start": 10,
                    "old_length": 7,
                    "new_start": 10,
                    "new_length": 7,
                    "hunk": "@@ -10,7 +10,7 @@ namespace caffe2 {\n class CAFFE2_OBSERVER_API NetObserverReporterPrint : public NetObserverReporter {\n  public:\n   static const std::string IDENTIFIER;\n-  void report(NetBase* net, std::map<std::string, PerformanceInformation>&) override;\n+  void report(NetBase* net, std::map<std::string, PerformanceInformation>&);\n };\n \n } // namespace caffe2\n"
                }
            ],
            "whole_deleted": "-  void report(NetBase* net, std::map<std::string, PerformanceInformation>&) override;\n",
            "whole_added": "+  void report(NetBase* net, std::map<std::string, PerformanceInformation>&);\n",
            "whole_hunk": "@@ -10,7 +10,7 @@ namespace caffe2 {\n class CAFFE2_OBSERVER_API NetObserverReporterPrint : public NetObserverReporter {\n  public:\n   static const std::string IDENTIFIER;\n-  void report(NetBase* net, std::map<std::string, PerformanceInformation>&) override;\n+  void report(NetBase* net, std::map<std::string, PerformanceInformation>&);\n };\n \n } // namespace caffe2\n"
        },
        {
            "name": "CMakeLists.txt",
            "path": "test/cpp/api/CMakeLists.txt",
            "patches": [
                {
                    "old_start": 57,
                    "old_length": 6,
                    "new_start": 57,
                    "new_length": 14,
                    "hunk": "@@ -57,6 +57,14 @@ if(USE_CUDA)\n   target_compile_definitions(test_api PRIVATE \"USE_CUDA\")\n endif()\n \n+# Workaround for https://github.com/pytorch/pytorch/issues/40941\n+if(USE_OPENMP AND CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_LESS 8.0.0))\n+  # Compiling transformer.cpp or pow_test.cpp with -O2+ and both -fuse-openmp and -faligned-newout any optimization\n+  # Fails with internal compiler error in gcc-7.5 or older\n+  # Workaround by compiling the tests without openmp (which they are not using anyway)\n+  set_property(TARGET test_api APPEND_STRING PROPERTY COMPILE_FLAGS \"-fno-openmp\")\n+endif()\n+\n if(NOT MSVC)\n   # Clang has an unfixed bug leading to spurious missing braces\n   # warnings, see https://bugs.llvm.org/show_bug.cgi?id=21629\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+# Workaround for https://github.com/pytorch/pytorch/issues/40941\n+if(USE_OPENMP AND CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_LESS 8.0.0))\n+  # Compiling transformer.cpp or pow_test.cpp with -O2+ and both -fuse-openmp and -faligned-newout any optimization\n+  # Fails with internal compiler error in gcc-7.5 or older\n+  # Workaround by compiling the tests without openmp (which they are not using anyway)\n+  set_property(TARGET test_api APPEND_STRING PROPERTY COMPILE_FLAGS \"-fno-openmp\")\n+endif()\n+\n",
            "whole_hunk": "@@ -57,6 +57,14 @@ if(USE_CUDA)\n   target_compile_definitions(test_api PRIVATE \"USE_CUDA\")\n endif()\n \n+# Workaround for https://github.com/pytorch/pytorch/issues/40941\n+if(USE_OPENMP AND CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_LESS 8.0.0))\n+  # Compiling transformer.cpp or pow_test.cpp with -O2+ and both -fuse-openmp and -faligned-newout any optimization\n+  # Fails with internal compiler error in gcc-7.5 or older\n+  # Workaround by compiling the tests without openmp (which they are not using anyway)\n+  set_property(TARGET test_api APPEND_STRING PROPERTY COMPILE_FLAGS \"-fno-openmp\")\n+endif()\n+\n if(NOT MSVC)\n   # Clang has an unfixed bug leading to spurious missing braces\n   # warnings, see https://bugs.llvm.org/show_bug.cgi?id=21629\n"
        },
        {
            "name": "CMakeLists.txt",
            "path": "torch/CMakeLists.txt",
            "patches": [
                {
                    "old_start": 248,
                    "old_length": 7,
                    "new_start": 248,
                    "new_length": 7,
                    "hunk": "@@ -248,7 +248,7 @@ if(USE_DISTRIBUTED)\n       append_filelist(\"libtorch_python_distributed_sources\" TORCH_PYTHON_SRCS)\n     endif()\n     # Disable certain warnings for GCC-9.X\n-    if(CMAKE_COMPILER_IS_GNUCXX)\n+    if(CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 9.0.0))\n       set_source_files_properties(${TORCH_SRC_DIR}/csrc/distributed/autograd/init.cpp PROPERTIES COMPILE_FLAGS \"-Wno-cast-function-type\")\n       set_source_files_properties(${TORCH_SRC_DIR}/csrc/distributed/rpc/testing/init.cpp PROPERTIES COMPILE_FLAGS \"-Wno-cast-function-type\")\n       set_source_files_properties(${TORCH_SRC_DIR}/csrc/distributed/c10d/init.cpp PROPERTIES COMPILE_FLAGS \"-Wno-cast-function-type\")\n"
                },
                {
                    "old_start": 321,
                    "old_length": 7,
                    "new_start": 321,
                    "new_length": 7,
                    "hunk": "@@ -321,7 +321,7 @@ set_source_files_properties(\n     )\n \n # Disable certain warnings for GCC-9.X\n-if(CMAKE_COMPILER_IS_GNUCXX)\n+if(CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 9.0.0))\n   set_source_files_properties(${TORCH_SRC_DIR}/csrc/Module.cpp PROPERTIES COMPILE_FLAGS \"-Wno-cast-function-type\")\n   set_source_files_properties(${TORCH_SRC_DIR}/csrc/autograd/python_variable.cpp PROPERTIES COMPILE_FLAGS \"-Wno-cast-function-type\")\n endif()\n"
                }
            ],
            "whole_deleted": "-    if(CMAKE_COMPILER_IS_GNUCXX)\n-if(CMAKE_COMPILER_IS_GNUCXX)\n",
            "whole_added": "+    if(CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 9.0.0))\n+if(CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 9.0.0))\n",
            "whole_hunk": "@@ -248,7 +248,7 @@ if(USE_DISTRIBUTED)\n       append_filelist(\"libtorch_python_distributed_sources\" TORCH_PYTHON_SRCS)\n     endif()\n     # Disable certain warnings for GCC-9.X\n-    if(CMAKE_COMPILER_IS_GNUCXX)\n+    if(CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 9.0.0))\n       set_source_files_properties(${TORCH_SRC_DIR}/csrc/distributed/autograd/init.cpp PROPERTIES COMPILE_FLAGS \"-Wno-cast-function-type\")\n       set_source_files_properties(${TORCH_SRC_DIR}/csrc/distributed/rpc/testing/init.cpp PROPERTIES COMPILE_FLAGS \"-Wno-cast-function-type\")\n       set_source_files_properties(${TORCH_SRC_DIR}/csrc/distributed/c10d/init.cpp PROPERTIES COMPILE_FLAGS \"-Wno-cast-function-type\")\n@@ -321,7 +321,7 @@ set_source_files_properties(\n     )\n \n # Disable certain warnings for GCC-9.X\n-if(CMAKE_COMPILER_IS_GNUCXX)\n+if(CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 9.0.0))\n   set_source_files_properties(${TORCH_SRC_DIR}/csrc/Module.cpp PROPERTIES COMPILE_FLAGS \"-Wno-cast-function-type\")\n   set_source_files_properties(${TORCH_SRC_DIR}/csrc/autograd/python_variable.cpp PROPERTIES COMPILE_FLAGS \"-Wno-cast-function-type\")\n endif()\n"
        },
        {
            "name": "python_hook.h",
            "path": "torch/csrc/autograd/python_hook.h",
            "patches": [
                {
                    "old_start": 49,
                    "old_length": 7,
                    "new_start": 49,
                    "new_length": 7,
                    "hunk": "@@ -49,7 +49,7 @@ struct PyFunctionTensorPostAccGradHooks : public PostAccumulateGradHook {\n   void compiled_args(torch::dynamo::autograd::CompiledNodeArgs& args) override;\n   void apply_with_saved(\n       Variable& tensor,\n-      torch::dynamo::autograd::SwapSavedVariables& saved) override;\n+      torch::dynamo::autograd::SwapSavedVariables& saved);\n   PyObject* dict;\n };\n "
                }
            ],
            "whole_deleted": "-      torch::dynamo::autograd::SwapSavedVariables& saved) override;\n",
            "whole_added": "+      torch::dynamo::autograd::SwapSavedVariables& saved);\n",
            "whole_hunk": "@@ -49,7 +49,7 @@ struct PyFunctionTensorPostAccGradHooks : public PostAccumulateGradHook {\n   void compiled_args(torch::dynamo::autograd::CompiledNodeArgs& args) override;\n   void apply_with_saved(\n       Variable& tensor,\n-      torch::dynamo::autograd::SwapSavedVariables& saved) override;\n+      torch::dynamo::autograd::SwapSavedVariables& saved);\n   PyObject* dict;\n };\n "
        }
    ]
},
{
    "Id": 395,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/0fef82b3dfb4a5da1555a3c046950594b83e2898",
    "date": "2023-12-02T04:16:37+00:00",
    "message": "[dcp] fix fsdp state_dict to use run_check=False (#114995)\n\nfrom_local with replicate placement would run mesh_broadcast if\nrun_check=True, by default from_local have run_check=True, but for FSDP\nstate_dict case we are for sure that these are replica already, so we\ndon't need to check/force check it.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114995\nApproved by: https://github.com/fegin, https://github.com/XilunWu, https://github.com/wz337",
    "label": "YES",
    "changes": [
        {
            "name": "_shard_utils.py",
            "path": "torch/distributed/fsdp/_shard_utils.py",
            "patches": [
                {
                    "old_start": 99,
                    "old_length": 8,
                    "new_start": 99,
                    "new_length": 9,
                    "hunk": "@@ -99,8 +99,9 @@ def _create_chunk_dtensor(\n     shard_placements = [Replicate() for _ in range(device_mesh.ndim)]\n     shard_placements[-1] = DShard(0)  # type: ignore[call-overload]\n \n-    return DTensor.from_local(tensor, device_mesh, replicate_placements).redistribute(\n-        device_mesh=device_mesh,\n+    return DTensor.from_local(\n+        tensor, device_mesh, replicate_placements, run_check=False\n+    ).redistribute(\n         placements=shard_placements,\n     )\n "
                }
            ],
            "whole_deleted": "-    return DTensor.from_local(tensor, device_mesh, replicate_placements).redistribute(\n-        device_mesh=device_mesh,\n",
            "whole_added": "+    return DTensor.from_local(\n+        tensor, device_mesh, replicate_placements, run_check=False\n+    ).redistribute(\n",
            "whole_hunk": "@@ -99,8 +99,9 @@ def _create_chunk_dtensor(\n     shard_placements = [Replicate() for _ in range(device_mesh.ndim)]\n     shard_placements[-1] = DShard(0)  # type: ignore[call-overload]\n \n-    return DTensor.from_local(tensor, device_mesh, replicate_placements).redistribute(\n-        device_mesh=device_mesh,\n+    return DTensor.from_local(\n+        tensor, device_mesh, replicate_placements, run_check=False\n+    ).redistribute(\n         placements=shard_placements,\n     )\n "
        }
    ]
},
{
    "Id": 167,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/9782439277fb545ec4e14340ecf75df86ddc4f0f",
    "date": "2024-05-07T16:56:17+00:00",
    "message": "[Profiler] Do not emit a warning when using CPU profiler (#125654)\n\nThis fixes a logic regression introduced by https://github.com/pytorch/pytorch/pull/123247 where\n```python\nif self.use_device and self.use_device != _get_privateuse1_backend_name():\n```\nwas replaced with\n```python\n        VALID_DEVICE_OPTIONS = [\"cuda\", \"xpu\", \"privateuseone\"]\n        if self.use_device not in VALID_DEVICE_OPTIONS:\n```\n\nThat triggers a warning every time code is invoke with `self.use_device` set to None\n\nThis change also skips all the checks which are useless if `use_device` is None to begin with\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125654\nApproved by: https://github.com/aaronenyeshi",
    "label": "YES",
    "changes": [
        {
            "name": "profiler.py",
            "path": "torch/autograd/profiler.py",
            "patches": [
                {
                    "old_start": 241,
                    "old_length": 21,
                    "new_start": 241,
                    "new_length": 22,
                    "hunk": "@@ -241,21 +241,22 @@ class profile:\n                 use_kineto\n             ), \"Device-only events supported only with Kineto (use_kineto=True)\"\n \n-        VALID_DEVICE_OPTIONS = [\"cuda\", \"xpu\"]\n-        if _get_privateuse1_backend_name() != \"privateuseone\":\n-            VALID_DEVICE_OPTIONS.append(_get_privateuse1_backend_name())\n-        if self.use_device not in VALID_DEVICE_OPTIONS:\n-            warn(f\"The {self.use_device} is not a valid device option.\")\n-            self.use_device = None\n-\n-        if self.use_device == \"cuda\" and not torch.cuda.is_available():\n-            warn(\"CUDA is not available, disabling CUDA profiling\")\n-            self.use_cuda = False\n-            self.use_device = None\n-\n-        if self.use_device == \"xpu\" and not torch.xpu.is_available():\n-            warn(\"XPU is not available, disabling XPU profiling\")\n-            self.use_device = None\n+        if self.use_device is not None:\n+            VALID_DEVICE_OPTIONS = [\"cuda\", \"xpu\"]\n+            if _get_privateuse1_backend_name() != \"privateuseone\":\n+                VALID_DEVICE_OPTIONS.append(_get_privateuse1_backend_name())\n+            if self.use_device not in VALID_DEVICE_OPTIONS:\n+                warn(f\"The {self.use_device} is not a valid device option.\")\n+                self.use_device = None\n+\n+            if self.use_device == \"cuda\" and not torch.cuda.is_available():\n+                warn(\"CUDA is not available, disabling CUDA profiling\")\n+                self.use_cuda = False\n+                self.use_device = None\n+\n+            if self.use_device == \"xpu\" and not torch.xpu.is_available():\n+                warn(\"XPU is not available, disabling XPU profiling\")\n+                self.use_device = None\n \n         self.kineto_activities = set()\n         if self.use_cpu:"
                }
            ],
            "whole_deleted": "-        VALID_DEVICE_OPTIONS = [\"cuda\", \"xpu\"]\n-        if _get_privateuse1_backend_name() != \"privateuseone\":\n-            VALID_DEVICE_OPTIONS.append(_get_privateuse1_backend_name())\n-        if self.use_device not in VALID_DEVICE_OPTIONS:\n-            warn(f\"The {self.use_device} is not a valid device option.\")\n-            self.use_device = None\n-\n-        if self.use_device == \"cuda\" and not torch.cuda.is_available():\n-            warn(\"CUDA is not available, disabling CUDA profiling\")\n-            self.use_cuda = False\n-            self.use_device = None\n-\n-        if self.use_device == \"xpu\" and not torch.xpu.is_available():\n-            warn(\"XPU is not available, disabling XPU profiling\")\n-            self.use_device = None\n",
            "whole_added": "+        if self.use_device is not None:\n+            VALID_DEVICE_OPTIONS = [\"cuda\", \"xpu\"]\n+            if _get_privateuse1_backend_name() != \"privateuseone\":\n+                VALID_DEVICE_OPTIONS.append(_get_privateuse1_backend_name())\n+            if self.use_device not in VALID_DEVICE_OPTIONS:\n+                warn(f\"The {self.use_device} is not a valid device option.\")\n+                self.use_device = None\n+\n+            if self.use_device == \"cuda\" and not torch.cuda.is_available():\n+                warn(\"CUDA is not available, disabling CUDA profiling\")\n+                self.use_cuda = False\n+                self.use_device = None\n+\n+            if self.use_device == \"xpu\" and not torch.xpu.is_available():\n+                warn(\"XPU is not available, disabling XPU profiling\")\n+                self.use_device = None\n",
            "whole_hunk": "@@ -241,21 +241,22 @@ class profile:\n                 use_kineto\n             ), \"Device-only events supported only with Kineto (use_kineto=True)\"\n \n-        VALID_DEVICE_OPTIONS = [\"cuda\", \"xpu\"]\n-        if _get_privateuse1_backend_name() != \"privateuseone\":\n-            VALID_DEVICE_OPTIONS.append(_get_privateuse1_backend_name())\n-        if self.use_device not in VALID_DEVICE_OPTIONS:\n-            warn(f\"The {self.use_device} is not a valid device option.\")\n-            self.use_device = None\n-\n-        if self.use_device == \"cuda\" and not torch.cuda.is_available():\n-            warn(\"CUDA is not available, disabling CUDA profiling\")\n-            self.use_cuda = False\n-            self.use_device = None\n-\n-        if self.use_device == \"xpu\" and not torch.xpu.is_available():\n-            warn(\"XPU is not available, disabling XPU profiling\")\n-            self.use_device = None\n+        if self.use_device is not None:\n+            VALID_DEVICE_OPTIONS = [\"cuda\", \"xpu\"]\n+            if _get_privateuse1_backend_name() != \"privateuseone\":\n+                VALID_DEVICE_OPTIONS.append(_get_privateuse1_backend_name())\n+            if self.use_device not in VALID_DEVICE_OPTIONS:\n+                warn(f\"The {self.use_device} is not a valid device option.\")\n+                self.use_device = None\n+\n+            if self.use_device == \"cuda\" and not torch.cuda.is_available():\n+                warn(\"CUDA is not available, disabling CUDA profiling\")\n+                self.use_cuda = False\n+                self.use_device = None\n+\n+            if self.use_device == \"xpu\" and not torch.xpu.is_available():\n+                warn(\"XPU is not available, disabling XPU profiling\")\n+                self.use_device = None\n \n         self.kineto_activities = set()\n         if self.use_cpu:"
        }
    ]
},
{
    "Id": 467,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/bf998a2c5d549cf4856c7becfca4a169bf68b709",
    "date": "2023-10-25T03:48:36+00:00",
    "message": "[quant][pt2e][be] Cleanup observer insertion logic (#111828)\n\nSummary:\natt, after SharedQuantizationSpec bug fix we are doing some checks before hand, this can simplify the logic when we insert observers\n\nTest Plan:\npython test/test_quantization.py TestQuantizePT2E\n\nCIs\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111828\nApproved by: https://github.com/kimishpatel\nghstack dependencies: #111827",
    "label": "NO",
    "changes": [
        {
            "name": "test_xnnpack_quantizer.py",
            "path": "test/quantization/pt2e/test_xnnpack_quantizer.py",
            "patches": [
                {
                    "old_start": 365,
                    "old_length": 12,
                    "new_start": 365,
                    "new_length": 16,
                    "hunk": "@@ -365,12 +365,16 @@ class TestXNNPACKQuantizer(PT2EQuantizationTestCase):\n \n         m = prepare_pt2e(m, quantizer)\n         m(*example_inputs)\n-        self.assertEqual(\n-            id(m.activation_post_process_2), id(m.activation_post_process_3)\n-        )\n-        self.assertEqual(\n-            id(m.activation_post_process_3), id(m.activation_post_process_4)\n-        )\n+        act_post_processes_pairs = []\n+        for n in m.graph.nodes:\n+            if n.target in [\n+                torch.ops.aten.view.default,\n+                torch.ops.aten.hardtanh.default,\n+            ]:\n+                input_act = getattr(m, n.args[0].target)\n+                output_act = getattr(m, list(n.users)[0].target)\n+                self.assertEqual(id(input_act), id(output_act))\n+\n         m = convert_pt2e(m, fold_quantize=True)\n         node_occurrence = {\n             # input and output are using quantize_per_tensor and weight is using quantize_per_channel\n"
                }
            ],
            "whole_deleted": "-        self.assertEqual(\n-            id(m.activation_post_process_2), id(m.activation_post_process_3)\n-        )\n-        self.assertEqual(\n-            id(m.activation_post_process_3), id(m.activation_post_process_4)\n-        )\n",
            "whole_added": "+        act_post_processes_pairs = []\n+        for n in m.graph.nodes:\n+            if n.target in [\n+                torch.ops.aten.view.default,\n+                torch.ops.aten.hardtanh.default,\n+            ]:\n+                input_act = getattr(m, n.args[0].target)\n+                output_act = getattr(m, list(n.users)[0].target)\n+                self.assertEqual(id(input_act), id(output_act))\n+\n",
            "whole_hunk": "@@ -365,12 +365,16 @@ class TestXNNPACKQuantizer(PT2EQuantizationTestCase):\n \n         m = prepare_pt2e(m, quantizer)\n         m(*example_inputs)\n-        self.assertEqual(\n-            id(m.activation_post_process_2), id(m.activation_post_process_3)\n-        )\n-        self.assertEqual(\n-            id(m.activation_post_process_3), id(m.activation_post_process_4)\n-        )\n+        act_post_processes_pairs = []\n+        for n in m.graph.nodes:\n+            if n.target in [\n+                torch.ops.aten.view.default,\n+                torch.ops.aten.hardtanh.default,\n+            ]:\n+                input_act = getattr(m, n.args[0].target)\n+                output_act = getattr(m, list(n.users)[0].target)\n+                self.assertEqual(id(input_act), id(output_act))\n+\n         m = convert_pt2e(m, fold_quantize=True)\n         node_occurrence = {\n             # input and output are using quantize_per_tensor and weight is using quantize_per_channel\n"
        },
        {
            "name": "prepare.py",
            "path": "torch/ao/quantization/pt2e/prepare.py",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 9,
                    "new_start": 1,
                    "new_length": 6,
                    "hunk": "@@ -1,9 +1,6 @@\n import torch\n from torch._subclasses import FakeTensor\n from torch.ao.quantization.fx.prepare import (\n-    _get_arg_as_input_act_obs_or_fq,\n-    _get_output_act_obs_or_fq,\n-    _get_dtype_and_is_dynamic,\n     _insert_obs_or_fq,\n     _save_state,\n     _is_activation_post_process_node,\n"
                },
                {
                    "old_start": 21,
                    "old_length": 7,
                    "new_start": 18,
                    "new_length": 6,
                    "hunk": "@@ -21,7 +18,6 @@ from torch.ao.quantization.qconfig import QConfigAny\n from torch.ao.quantization.fx.custom_config import PrepareCustomConfig\n from typing import Dict, Tuple, Union, Any, Optional\n from torch.ao.quantization.quantizer import (\n-    QuantizationAnnotation,\n     EdgeOrNode,\n     SharedQuantizationSpec,\n     QuantizationSpecBase,\n"
                },
                {
                    "old_start": 260,
                    "old_length": 70,
                    "new_start": 256,
                    "new_length": 56,
                    "hunk": "@@ -260,70 +256,56 @@ def _maybe_insert_input_observer_for_arg_or_kwarg(\n     # default (no observer)\n     new_arg = arg\n \n-    quantization_annotation = node.meta.get(\"quantization_annotation\", QuantizationAnnotation())\n-    arg_as_input_act_obs_or_fq = _get_arg_as_input_act_obs_or_fq(arg, node, named_modules, obs_or_fq_map, is_qat)\n-    arg_as_input_target_dtype, arg_as_input_target_is_dynamic = _get_dtype_and_is_dynamic(arg_as_input_act_obs_or_fq)\n-\n-    arg_as_output_act_obs_or_fq = _get_output_act_obs_or_fq(arg, named_modules, obs_or_fq_map, is_qat)\n-    arg_as_output_target_dtype, arg_as_output_target_is_dynamic = _get_dtype_and_is_dynamic(arg_as_output_act_obs_or_fq)\n-\n-    if arg_as_input_target_is_dynamic or arg_as_input_target_dtype not in [torch.float, None]:\n-        if arg_as_input_target_dtype == arg_as_output_target_dtype and \\\n-           arg_as_input_target_is_dynamic == arg_as_output_target_is_dynamic:\n-            assert _is_activation_post_process_node(arg, named_modules)\n-            assert arg_as_input_act_obs_or_fq is not None\n-            observed_arg = arg.args[0]\n-            assert isinstance(observed_arg, Node), f\"expect observed argument to be a Node, but got: {type(observed_arg)}\"\n-            assert observed_arg in obs_or_fq_map, \\\n-                f\"can't find a sharing group for node: {observed_arg}\"\n-            # reuse the existing obs/fq\n-            arg_as_input_act_obs_or_fq = obs_or_fq_map[observed_arg]\n-            # we don't need to insert new observer node\n-            new_arg = arg\n-        else:\n-            # skip inserting new observers if there is an observer inserted for the arg before\n-            # that has the same dtype that we want to insert here\n-            # alternatively we could have a dedup pass after we insert all observers to deduplicate\n-            # observers\n-            # Example:\n-            # arg -> existing_obs -> conv1\n-            #    \\ -> conv2\n-            #\n-            # instead of inserting new observers we will have:\n-            # arg -> existing_obs -> conv1\n-            #                   \\ -> conv2\n-            existing_obs_node = None\n-            for maybe_obs_node in arg.users.keys():\n-                if maybe_obs_node.op == 'call_module':\n-                    maybe_obs_mod = named_modules[maybe_obs_node.target]  # type: ignore[index]\n-                    if (\n-                        type(maybe_obs_mod) == type(arg_as_input_act_obs_or_fq) and\n-                        maybe_obs_mod.dtype == arg_as_input_target_dtype\n-                    ):\n-                        arg_as_input_act_obs_or_fq = maybe_obs_mod  # type: ignore[assignment]\n-                        existing_obs_node = maybe_obs_node\n-                        break\n-\n-            assert arg_as_input_act_obs_or_fq is not None\n-            if existing_obs_node is None:\n-                maybe_observed_arg = arg\n-                # When quantizing two layers with different configs we can have\n-                # conv2d (int8) -> avgpool(uint8)\n-                # In this case observer insertion for avgpool will come here but the input\n-                # to avgpool will be output observer of conv2d\n-                # Now the obs map that we update must correspond to the original input of\n-                # avgpool and not the output obs of conv2d\n-                # This is because when referring to the edge, quantizer would refer to\n-                # original input and not the observed one.\n-                while _is_activation_post_process_node(arg, named_modules):\n-                    arg = arg.args[0]  # type: ignore[assignment]\n-                arg_as_input_act_obs_or_fq = obs_or_fq_map[(arg, node)]\n-                new_obs_node = _insert_obs_or_fq(\n-                    maybe_observed_arg, arg_as_input_act_obs_or_fq, model, named_modules, model.graph)\n-                # override this arg to be the observed arg\n-                new_arg = new_obs_node\n-            else:\n-                new_arg = existing_obs_node\n+    # find the original `arg` node to the current node, skipping inserted observer/fake_quant nodes\n+    original_arg = arg\n+    while _is_activation_post_process_node(original_arg, named_modules):\n+        original_arg = original_arg.args[0]  # type: ignore[assignment]\n+    assert isinstance(original_arg, Node), f\"expect original argument to be a Node, but got: {type(original_arg)}\"\n+\n+    input_edge = (original_arg, node)\n+    if input_edge not in obs_or_fq_map:\n+        return new_arg\n+    # input_edge needs to be observed\n+    input_edge_obs_or_fq = obs_or_fq_map[input_edge]\n+    if input_edge_obs_or_fq is None:\n+        return new_arg\n+\n+    arg_as_output_obs_or_fq = obs_or_fq_map.get(original_arg, None)\n+    # the arg is observed as the output and is using the same instance as the input_edge\n+    # we'll reuse the inserted observer/fake_quant\n+    if arg_as_output_obs_or_fq is not None and id(arg_as_output_obs_or_fq) == id(input_edge_obs_or_fq):\n+        return new_arg\n+\n+    # otherwise, we'll insert a new observer/fake_quant node\n+\n+    existing_obs_node = None\n+    # skip inserting new observers if there is an observer inserted for the arg before\n+    # that has the same dtype that we want to insert here\n+    # alternatively we could have a dedup pass after we insert all observers to deduplicate\n+    # observers\n+    # Example:\n+    # conv1 -> obs1 -> existing_obs -> conv2\n+    #             \\ -> conv3\n+    #\n+    # instead of inserting new observers we will have:\n+    # conv1 -> obs1 -> existing_obs -> conv2\n+    #                            \\ -> conv3\n+    for maybe_obs_node in arg.users.keys():\n+        if not _is_activation_post_process_node(maybe_obs_node, named_modules):\n+            continue\n+        maybe_obs_mod = named_modules[maybe_obs_node.target]  # type: ignore[index]\n+        if (\n+            type(maybe_obs_mod) == type(input_edge_obs_or_fq) and\n+            maybe_obs_mod.dtype == input_edge_obs_or_fq.dtype\n+        ):\n+            input_edge_obs_or_fq = maybe_obs_mod  # type: ignore[assignment]\n+            existing_obs_node = maybe_obs_node\n+            break\n+\n+    if existing_obs_node is None:\n+        new_arg = _insert_obs_or_fq(arg, input_edge_obs_or_fq, model, named_modules, model.graph)\n+    else:\n+        new_arg = existing_obs_node\n \n     return new_arg\n \n"
                }
            ],
            "whole_deleted": "-    _get_arg_as_input_act_obs_or_fq,\n-    _get_output_act_obs_or_fq,\n-    _get_dtype_and_is_dynamic,\n-    QuantizationAnnotation,\n-    quantization_annotation = node.meta.get(\"quantization_annotation\", QuantizationAnnotation())\n-    arg_as_input_act_obs_or_fq = _get_arg_as_input_act_obs_or_fq(arg, node, named_modules, obs_or_fq_map, is_qat)\n-    arg_as_input_target_dtype, arg_as_input_target_is_dynamic = _get_dtype_and_is_dynamic(arg_as_input_act_obs_or_fq)\n-\n-    arg_as_output_act_obs_or_fq = _get_output_act_obs_or_fq(arg, named_modules, obs_or_fq_map, is_qat)\n-    arg_as_output_target_dtype, arg_as_output_target_is_dynamic = _get_dtype_and_is_dynamic(arg_as_output_act_obs_or_fq)\n-\n-    if arg_as_input_target_is_dynamic or arg_as_input_target_dtype not in [torch.float, None]:\n-        if arg_as_input_target_dtype == arg_as_output_target_dtype and \\\n-           arg_as_input_target_is_dynamic == arg_as_output_target_is_dynamic:\n-            assert _is_activation_post_process_node(arg, named_modules)\n-            assert arg_as_input_act_obs_or_fq is not None\n-            observed_arg = arg.args[0]\n-            assert isinstance(observed_arg, Node), f\"expect observed argument to be a Node, but got: {type(observed_arg)}\"\n-            assert observed_arg in obs_or_fq_map, \\\n-                f\"can't find a sharing group for node: {observed_arg}\"\n-            # reuse the existing obs/fq\n-            arg_as_input_act_obs_or_fq = obs_or_fq_map[observed_arg]\n-            # we don't need to insert new observer node\n-            new_arg = arg\n-        else:\n-            # skip inserting new observers if there is an observer inserted for the arg before\n-            # that has the same dtype that we want to insert here\n-            # alternatively we could have a dedup pass after we insert all observers to deduplicate\n-            # observers\n-            # Example:\n-            # arg -> existing_obs -> conv1\n-            #    \\ -> conv2\n-            #\n-            # instead of inserting new observers we will have:\n-            # arg -> existing_obs -> conv1\n-            #                   \\ -> conv2\n-            existing_obs_node = None\n-            for maybe_obs_node in arg.users.keys():\n-                if maybe_obs_node.op == 'call_module':\n-                    maybe_obs_mod = named_modules[maybe_obs_node.target]  # type: ignore[index]\n-                    if (\n-                        type(maybe_obs_mod) == type(arg_as_input_act_obs_or_fq) and\n-                        maybe_obs_mod.dtype == arg_as_input_target_dtype\n-                    ):\n-                        arg_as_input_act_obs_or_fq = maybe_obs_mod  # type: ignore[assignment]\n-                        existing_obs_node = maybe_obs_node\n-                        break\n-\n-            assert arg_as_input_act_obs_or_fq is not None\n-            if existing_obs_node is None:\n-                maybe_observed_arg = arg\n-                # When quantizing two layers with different configs we can have\n-                # conv2d (int8) -> avgpool(uint8)\n-                # In this case observer insertion for avgpool will come here but the input\n-                # to avgpool will be output observer of conv2d\n-                # Now the obs map that we update must correspond to the original input of\n-                # avgpool and not the output obs of conv2d\n-                # This is because when referring to the edge, quantizer would refer to\n-                # original input and not the observed one.\n-                while _is_activation_post_process_node(arg, named_modules):\n-                    arg = arg.args[0]  # type: ignore[assignment]\n-                arg_as_input_act_obs_or_fq = obs_or_fq_map[(arg, node)]\n-                new_obs_node = _insert_obs_or_fq(\n-                    maybe_observed_arg, arg_as_input_act_obs_or_fq, model, named_modules, model.graph)\n-                # override this arg to be the observed arg\n-                new_arg = new_obs_node\n-            else:\n-                new_arg = existing_obs_node\n",
            "whole_added": "+    # find the original `arg` node to the current node, skipping inserted observer/fake_quant nodes\n+    original_arg = arg\n+    while _is_activation_post_process_node(original_arg, named_modules):\n+        original_arg = original_arg.args[0]  # type: ignore[assignment]\n+    assert isinstance(original_arg, Node), f\"expect original argument to be a Node, but got: {type(original_arg)}\"\n+\n+    input_edge = (original_arg, node)\n+    if input_edge not in obs_or_fq_map:\n+        return new_arg\n+    # input_edge needs to be observed\n+    input_edge_obs_or_fq = obs_or_fq_map[input_edge]\n+    if input_edge_obs_or_fq is None:\n+        return new_arg\n+\n+    arg_as_output_obs_or_fq = obs_or_fq_map.get(original_arg, None)\n+    # the arg is observed as the output and is using the same instance as the input_edge\n+    # we'll reuse the inserted observer/fake_quant\n+    if arg_as_output_obs_or_fq is not None and id(arg_as_output_obs_or_fq) == id(input_edge_obs_or_fq):\n+        return new_arg\n+\n+    # otherwise, we'll insert a new observer/fake_quant node\n+\n+    existing_obs_node = None\n+    # skip inserting new observers if there is an observer inserted for the arg before\n+    # that has the same dtype that we want to insert here\n+    # alternatively we could have a dedup pass after we insert all observers to deduplicate\n+    # observers\n+    # Example:\n+    # conv1 -> obs1 -> existing_obs -> conv2\n+    #             \\ -> conv3\n+    #\n+    # instead of inserting new observers we will have:\n+    # conv1 -> obs1 -> existing_obs -> conv2\n+    #                            \\ -> conv3\n+    for maybe_obs_node in arg.users.keys():\n+        if not _is_activation_post_process_node(maybe_obs_node, named_modules):\n+            continue\n+        maybe_obs_mod = named_modules[maybe_obs_node.target]  # type: ignore[index]\n+        if (\n+            type(maybe_obs_mod) == type(input_edge_obs_or_fq) and\n+            maybe_obs_mod.dtype == input_edge_obs_or_fq.dtype\n+        ):\n+            input_edge_obs_or_fq = maybe_obs_mod  # type: ignore[assignment]\n+            existing_obs_node = maybe_obs_node\n+            break\n+\n+    if existing_obs_node is None:\n+        new_arg = _insert_obs_or_fq(arg, input_edge_obs_or_fq, model, named_modules, model.graph)\n+    else:\n+        new_arg = existing_obs_node\n",
            "whole_hunk": "@@ -1,9 +1,6 @@\n import torch\n from torch._subclasses import FakeTensor\n from torch.ao.quantization.fx.prepare import (\n-    _get_arg_as_input_act_obs_or_fq,\n-    _get_output_act_obs_or_fq,\n-    _get_dtype_and_is_dynamic,\n     _insert_obs_or_fq,\n     _save_state,\n     _is_activation_post_process_node,\n@@ -21,7 +18,6 @@ from torch.ao.quantization.qconfig import QConfigAny\n from torch.ao.quantization.fx.custom_config import PrepareCustomConfig\n from typing import Dict, Tuple, Union, Any, Optional\n from torch.ao.quantization.quantizer import (\n-    QuantizationAnnotation,\n     EdgeOrNode,\n     SharedQuantizationSpec,\n     QuantizationSpecBase,\n@@ -260,70 +256,56 @@ def _maybe_insert_input_observer_for_arg_or_kwarg(\n     # default (no observer)\n     new_arg = arg\n \n-    quantization_annotation = node.meta.get(\"quantization_annotation\", QuantizationAnnotation())\n-    arg_as_input_act_obs_or_fq = _get_arg_as_input_act_obs_or_fq(arg, node, named_modules, obs_or_fq_map, is_qat)\n-    arg_as_input_target_dtype, arg_as_input_target_is_dynamic = _get_dtype_and_is_dynamic(arg_as_input_act_obs_or_fq)\n-\n-    arg_as_output_act_obs_or_fq = _get_output_act_obs_or_fq(arg, named_modules, obs_or_fq_map, is_qat)\n-    arg_as_output_target_dtype, arg_as_output_target_is_dynamic = _get_dtype_and_is_dynamic(arg_as_output_act_obs_or_fq)\n-\n-    if arg_as_input_target_is_dynamic or arg_as_input_target_dtype not in [torch.float, None]:\n-        if arg_as_input_target_dtype == arg_as_output_target_dtype and \\\n-           arg_as_input_target_is_dynamic == arg_as_output_target_is_dynamic:\n-            assert _is_activation_post_process_node(arg, named_modules)\n-            assert arg_as_input_act_obs_or_fq is not None\n-            observed_arg = arg.args[0]\n-            assert isinstance(observed_arg, Node), f\"expect observed argument to be a Node, but got: {type(observed_arg)}\"\n-            assert observed_arg in obs_or_fq_map, \\\n-                f\"can't find a sharing group for node: {observed_arg}\"\n-            # reuse the existing obs/fq\n-            arg_as_input_act_obs_or_fq = obs_or_fq_map[observed_arg]\n-            # we don't need to insert new observer node\n-            new_arg = arg\n-        else:\n-            # skip inserting new observers if there is an observer inserted for the arg before\n-            # that has the same dtype that we want to insert here\n-            # alternatively we could have a dedup pass after we insert all observers to deduplicate\n-            # observers\n-            # Example:\n-            # arg -> existing_obs -> conv1\n-            #    \\ -> conv2\n-            #\n-            # instead of inserting new observers we will have:\n-            # arg -> existing_obs -> conv1\n-            #                   \\ -> conv2\n-            existing_obs_node = None\n-            for maybe_obs_node in arg.users.keys():\n-                if maybe_obs_node.op == 'call_module':\n-                    maybe_obs_mod = named_modules[maybe_obs_node.target]  # type: ignore[index]\n-                    if (\n-                        type(maybe_obs_mod) == type(arg_as_input_act_obs_or_fq) and\n-                        maybe_obs_mod.dtype == arg_as_input_target_dtype\n-                    ):\n-                        arg_as_input_act_obs_or_fq = maybe_obs_mod  # type: ignore[assignment]\n-                        existing_obs_node = maybe_obs_node\n-                        break\n-\n-            assert arg_as_input_act_obs_or_fq is not None\n-            if existing_obs_node is None:\n-                maybe_observed_arg = arg\n-                # When quantizing two layers with different configs we can have\n-                # conv2d (int8) -> avgpool(uint8)\n-                # In this case observer insertion for avgpool will come here but the input\n-                # to avgpool will be output observer of conv2d\n-                # Now the obs map that we update must correspond to the original input of\n-                # avgpool and not the output obs of conv2d\n-                # This is because when referring to the edge, quantizer would refer to\n-                # original input and not the observed one.\n-                while _is_activation_post_process_node(arg, named_modules):\n-                    arg = arg.args[0]  # type: ignore[assignment]\n-                arg_as_input_act_obs_or_fq = obs_or_fq_map[(arg, node)]\n-                new_obs_node = _insert_obs_or_fq(\n-                    maybe_observed_arg, arg_as_input_act_obs_or_fq, model, named_modules, model.graph)\n-                # override this arg to be the observed arg\n-                new_arg = new_obs_node\n-            else:\n-                new_arg = existing_obs_node\n+    # find the original `arg` node to the current node, skipping inserted observer/fake_quant nodes\n+    original_arg = arg\n+    while _is_activation_post_process_node(original_arg, named_modules):\n+        original_arg = original_arg.args[0]  # type: ignore[assignment]\n+    assert isinstance(original_arg, Node), f\"expect original argument to be a Node, but got: {type(original_arg)}\"\n+\n+    input_edge = (original_arg, node)\n+    if input_edge not in obs_or_fq_map:\n+        return new_arg\n+    # input_edge needs to be observed\n+    input_edge_obs_or_fq = obs_or_fq_map[input_edge]\n+    if input_edge_obs_or_fq is None:\n+        return new_arg\n+\n+    arg_as_output_obs_or_fq = obs_or_fq_map.get(original_arg, None)\n+    # the arg is observed as the output and is using the same instance as the input_edge\n+    # we'll reuse the inserted observer/fake_quant\n+    if arg_as_output_obs_or_fq is not None and id(arg_as_output_obs_or_fq) == id(input_edge_obs_or_fq):\n+        return new_arg\n+\n+    # otherwise, we'll insert a new observer/fake_quant node\n+\n+    existing_obs_node = None\n+    # skip inserting new observers if there is an observer inserted for the arg before\n+    # that has the same dtype that we want to insert here\n+    # alternatively we could have a dedup pass after we insert all observers to deduplicate\n+    # observers\n+    # Example:\n+    # conv1 -> obs1 -> existing_obs -> conv2\n+    #             \\ -> conv3\n+    #\n+    # instead of inserting new observers we will have:\n+    # conv1 -> obs1 -> existing_obs -> conv2\n+    #                            \\ -> conv3\n+    for maybe_obs_node in arg.users.keys():\n+        if not _is_activation_post_process_node(maybe_obs_node, named_modules):\n+            continue\n+        maybe_obs_mod = named_modules[maybe_obs_node.target]  # type: ignore[index]\n+        if (\n+            type(maybe_obs_mod) == type(input_edge_obs_or_fq) and\n+            maybe_obs_mod.dtype == input_edge_obs_or_fq.dtype\n+        ):\n+            input_edge_obs_or_fq = maybe_obs_mod  # type: ignore[assignment]\n+            existing_obs_node = maybe_obs_node\n+            break\n+\n+    if existing_obs_node is None:\n+        new_arg = _insert_obs_or_fq(arg, input_edge_obs_or_fq, model, named_modules, model.graph)\n+    else:\n+        new_arg = existing_obs_node\n \n     return new_arg\n \n"
        },
        {
            "name": "quantizer.py",
            "path": "torch/ao/quantization/quantizer/quantizer.py",
            "patches": [
                {
                    "old_start": 138,
                    "old_length": 7,
                    "new_start": 138,
                    "new_length": 9,
                    "hunk": "@@ -138,7 +138,9 @@ class QuantizationAnnotation:\n     \"\"\"\n \n     # a map from torch.fx.Node to a type of QuantizationSpecBase\n-    input_qspec_map: Dict[Node, QuantizationSpecBase] = field(default_factory=dict)\n+    input_qspec_map: Dict[Node, Optional[QuantizationSpecBase]] = field(\n+        default_factory=dict\n+    )\n \n     # How the output of this node is quantized, expressed as QuantizationSpec\n     # TODO: change the value to QuantizationSpec in a separate PR\n"
                }
            ],
            "whole_deleted": "-    input_qspec_map: Dict[Node, QuantizationSpecBase] = field(default_factory=dict)\n",
            "whole_added": "+    input_qspec_map: Dict[Node, Optional[QuantizationSpecBase]] = field(\n+        default_factory=dict\n+    )\n",
            "whole_hunk": "@@ -138,7 +138,9 @@ class QuantizationAnnotation:\n     \"\"\"\n \n     # a map from torch.fx.Node to a type of QuantizationSpecBase\n-    input_qspec_map: Dict[Node, QuantizationSpecBase] = field(default_factory=dict)\n+    input_qspec_map: Dict[Node, Optional[QuantizationSpecBase]] = field(\n+        default_factory=dict\n+    )\n \n     # How the output of this node is quantized, expressed as QuantizationSpec\n     # TODO: change the value to QuantizationSpec in a separate PR\n"
        },
        {
            "name": "xnnpack_quantizer.py",
            "path": "torch/ao/quantization/quantizer/xnnpack_quantizer.py",
            "patches": [
                {
                    "old_start": 154,
                    "old_length": 12,
                    "new_start": 154,
                    "new_length": 7,
                    "hunk": "@@ -154,12 +154,7 @@ def get_symmetric_quantization_config(\n         ),\n     )\n \n-    bias_observer_or_fake_quant_ctr: _ObserverOrFakeQuantizeConstructor = (\n-        PlaceholderObserver\n-    )\n-    bias_quantization_spec = QuantizationSpec(\n-        dtype=torch.float, observer_or_fake_quant_ctr=bias_observer_or_fake_quant_ctr\n-    )\n+    bias_quantization_spec = None\n     if is_dynamic:\n         quantization_config = QuantizationConfig(\n             act_quantization_spec,"
                }
            ],
            "whole_deleted": "-    bias_observer_or_fake_quant_ctr: _ObserverOrFakeQuantizeConstructor = (\n-        PlaceholderObserver\n-    )\n-    bias_quantization_spec = QuantizationSpec(\n-        dtype=torch.float, observer_or_fake_quant_ctr=bias_observer_or_fake_quant_ctr\n-    )\n",
            "whole_added": "+    bias_quantization_spec = None\n",
            "whole_hunk": "@@ -154,12 +154,7 @@ def get_symmetric_quantization_config(\n         ),\n     )\n \n-    bias_observer_or_fake_quant_ctr: _ObserverOrFakeQuantizeConstructor = (\n-        PlaceholderObserver\n-    )\n-    bias_quantization_spec = QuantizationSpec(\n-        dtype=torch.float, observer_or_fake_quant_ctr=bias_observer_or_fake_quant_ctr\n-    )\n+    bias_quantization_spec = None\n     if is_dynamic:\n         quantization_config = QuantizationConfig(\n             act_quantization_spec,"
        }
    ]
},
{
    "Id": 172,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/9aa7699185e4ec39077e3046dfd63244dffa9ddb",
    "date": "2024-05-03T23:44:05+00:00",
    "message": "[FSDP2] Computed grad divide factors at runtime (#125484)\n\n**Context**\nWe are interested in supporting the case where HSDP reduce-scatters but does not all-reduce in a microbatch backward. This saves communication while still saving memory. Only on the last microbatch do we need to both reduce-scatter and all-reduce. This is not implemented yet and will hopefully come in a future PR.\n\nThere is one notable part of doing this. On the last microbatch, we need to perform an accumulation step after reduce-scatter and before all-reduce. If not, then the preceding microbatch's gradients will not be contributed across the replica group. (In other words, we cannot simply accumulate _after_ all-reduce.)\n\nConsider 32 GPUs with 4-way replication and 8-way sharding and 2 microbatches, and focus on global rank 0.\n- After the first microbatch, rank 0 will have its shard of $\\frac{1}{8} \\sum_{i \\in S(0)} g_i^{(1)}$, where we define $S(0) = \\{0, 1, \\dots, 7\\}$ to be the ranks in its shard group and we define the $(1)$ superscript to denote the first microbatch.\n- Upon the second microbatch, rank 0 after its reduce-scatter will additionally have its shard of $\\frac{1}{8} \\sum_{i \\in S(0)} g_i^{(2)}$. If we only all-reduce this, then this second microbatch's gradients become $\\frac{1}{32} \\sum_{i=0, 1, \\dots, 31} g_i^{(2)}$, so in total, rank 0 has $\\frac{1}{8} \\sum_{i \\in S(0)} g_i^{(1)} + \\frac{1}{32} \\sum_{i=0, 1, \\dots, 31} g_i^{(2)}$, which is wrong.\n- Importantly, we must accumulate $\\frac{1}{8} \\sum_{i \\in S(0)} g_i^{(1)}  + \\frac{1}{8} \\sum_{i \\in S(0)} g_i^{(2)} = \\frac{1}{8}\\sum_{i \\in S(0)} (g_i^{(1)} + g_i^{(2)})$ first before all-reducing to get $\\frac{1}{32} \\sum_{i=0, 1, \\dots, 31} (g_i^{(1)} + g_i^{(2)})$.\n\nNow, note how under this approach, we want a factor of $\\frac{1}{8}$ only (i.e. reciprocal of the shard group size), not $\\frac{1}{32}$, for the first microbatch's gradients.\n- For bf16/fp32, since we use `ReduceOp.AVG` and we only reduce-scatter on the first microbatch, we correctly have a factor of $\\frac{1}{8}$ on the first microbatch.\n- For fp16, since we precompute the gradient divide factors at init time assuming always reducing over both shard and replica groups, we incorrectly have a factor of $\\frac{1}{32}$ on the first microbatch, deviating from the bf16/fp32 case.\n\nWe can address this issue by matching the bf16/fp32 vs. fp16 semantics by computing the divide factors at runtime based on which process groups were passed into the reduction function (`foreach_reduce`).\n\n**Additional Notes**\nHow to implement the HSDP reduce-scatter but no all-reduce is not entirely clear yet. (What is the cleanest way to do this?) We need to store the partial reduce-scatter output and check for it upon the next backward. We should also be sure to error if the set of parameters receiving gradients changes, in which case we cannot support this easily. Anyway, we will implement this in a follow-up.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125484\nApproved by: https://github.com/wanchaol\nghstack dependencies: #125431, #125479",
    "label": "NO",
    "changes": [
        {
            "name": "test_fully_shard_comm.py",
            "path": "test/distributed/_composable/fsdp/test_fully_shard_comm.py",
            "patches": [
                {
                    "old_start": 18,
                    "old_length": 6,
                    "new_start": 18,
                    "new_length": 8,
                    "hunk": "@@ -18,6 +18,8 @@ from torch.distributed._composable.fsdp import (\n     OffloadPolicy,\n )\n from torch.distributed._composable.fsdp._fsdp_collectives import (\n+    _div_if_needed,\n+    _get_gradient_divide_factors,\n     foreach_all_gather,\n     foreach_all_gather_copy_out,\n     foreach_reduce,\n"
                },
                {
                    "old_start": 207,
                    "old_length": 6,
                    "new_start": 209,
                    "new_length": 18,
                    "hunk": "@@ -207,6 +209,18 @@ class TestFullyShardCollectiveOps(FSDPTestMultiThread):\n                 reduce_scatter_dtype=torch.float32,\n             )\n \n+    @unittest.skipIf(not TEST_CUDA, \"no cuda\")\n+    def test_reduce_scatter_fp16(self):\n+        param_sizes = self._get_param_sizes()\n+        default_stream = torch.cuda.current_stream()\n+        stream = torch.cuda.Stream()\n+        for reduce_scatter_stream in (default_stream, stream):\n+            self._test_reduce_scatter(\n+                param_sizes,\n+                reduce_scatter_stream=reduce_scatter_stream,\n+                reduce_scatter_dtype=torch.float16,\n+            )\n+\n     def _test_reduce_scatter(\n         self,\n         param_sizes: List[torch.Size],\n"
                },
                {
                    "old_start": 238,
                    "old_length": 17,
                    "new_start": 252,
                    "new_length": 24,
                    "hunk": "@@ -238,17 +252,24 @@ class TestFullyShardCollectiveOps(FSDPTestMultiThread):\n             orig_dtype=orig_params[0].dtype,\n             reduce_dtype=reduce_scatter_dtype,\n             device=self.device,\n-            divide_factors=fsdp_param_group._grad_divide_factors,\n             all_reduce_group=None,\n             all_reduce_stream=all_reduce_stream,\n         )\n         torch.cuda.current_stream().wait_event(view_out_event)\n \n         # Check reduce-scatter correctness\n+        predivide_factor, postdivide_factor = _get_gradient_divide_factors(\n+            group, None, reduce_scatter_dtype\n+        )\n         reduced_grads = [grad.detach().clone() for grad in unsharded_grads]\n         for grad in reduced_grads:\n-            dist.all_reduce(grad, group=group)\n-            grad /= self.world_size\n+            _div_if_needed(grad, predivide_factor)\n+            dist.all_reduce(\n+                grad,\n+                group=group,\n+                op=dist.ReduceOp.AVG if predivide_factor is None else dist.ReduceOp.SUM,\n+            )\n+            _div_if_needed(grad, postdivide_factor)\n         for fsdp_param, reduced_grad in zip(fsdp_params, reduced_grads):\n             sharded_grad = fsdp_param.sharded_param.grad\n             self.assertIsInstance(sharded_grad, DTensor)\n"
                }
            ],
            "whole_deleted": "-            divide_factors=fsdp_param_group._grad_divide_factors,\n-            dist.all_reduce(grad, group=group)\n-            grad /= self.world_size\n",
            "whole_added": "+    _div_if_needed,\n+    _get_gradient_divide_factors,\n+    @unittest.skipIf(not TEST_CUDA, \"no cuda\")\n+    def test_reduce_scatter_fp16(self):\n+        param_sizes = self._get_param_sizes()\n+        default_stream = torch.cuda.current_stream()\n+        stream = torch.cuda.Stream()\n+        for reduce_scatter_stream in (default_stream, stream):\n+            self._test_reduce_scatter(\n+                param_sizes,\n+                reduce_scatter_stream=reduce_scatter_stream,\n+                reduce_scatter_dtype=torch.float16,\n+            )\n+\n+        predivide_factor, postdivide_factor = _get_gradient_divide_factors(\n+            group, None, reduce_scatter_dtype\n+        )\n+            _div_if_needed(grad, predivide_factor)\n+            dist.all_reduce(\n+                grad,\n+                group=group,\n+                op=dist.ReduceOp.AVG if predivide_factor is None else dist.ReduceOp.SUM,\n+            )\n+            _div_if_needed(grad, postdivide_factor)\n",
            "whole_hunk": "@@ -18,6 +18,8 @@ from torch.distributed._composable.fsdp import (\n     OffloadPolicy,\n )\n from torch.distributed._composable.fsdp._fsdp_collectives import (\n+    _div_if_needed,\n+    _get_gradient_divide_factors,\n     foreach_all_gather,\n     foreach_all_gather_copy_out,\n     foreach_reduce,\n@@ -207,6 +209,18 @@ class TestFullyShardCollectiveOps(FSDPTestMultiThread):\n                 reduce_scatter_dtype=torch.float32,\n             )\n \n+    @unittest.skipIf(not TEST_CUDA, \"no cuda\")\n+    def test_reduce_scatter_fp16(self):\n+        param_sizes = self._get_param_sizes()\n+        default_stream = torch.cuda.current_stream()\n+        stream = torch.cuda.Stream()\n+        for reduce_scatter_stream in (default_stream, stream):\n+            self._test_reduce_scatter(\n+                param_sizes,\n+                reduce_scatter_stream=reduce_scatter_stream,\n+                reduce_scatter_dtype=torch.float16,\n+            )\n+\n     def _test_reduce_scatter(\n         self,\n         param_sizes: List[torch.Size],\n@@ -238,17 +252,24 @@ class TestFullyShardCollectiveOps(FSDPTestMultiThread):\n             orig_dtype=orig_params[0].dtype,\n             reduce_dtype=reduce_scatter_dtype,\n             device=self.device,\n-            divide_factors=fsdp_param_group._grad_divide_factors,\n             all_reduce_group=None,\n             all_reduce_stream=all_reduce_stream,\n         )\n         torch.cuda.current_stream().wait_event(view_out_event)\n \n         # Check reduce-scatter correctness\n+        predivide_factor, postdivide_factor = _get_gradient_divide_factors(\n+            group, None, reduce_scatter_dtype\n+        )\n         reduced_grads = [grad.detach().clone() for grad in unsharded_grads]\n         for grad in reduced_grads:\n-            dist.all_reduce(grad, group=group)\n-            grad /= self.world_size\n+            _div_if_needed(grad, predivide_factor)\n+            dist.all_reduce(\n+                grad,\n+                group=group,\n+                op=dist.ReduceOp.AVG if predivide_factor is None else dist.ReduceOp.SUM,\n+            )\n+            _div_if_needed(grad, postdivide_factor)\n         for fsdp_param, reduced_grad in zip(fsdp_params, reduced_grads):\n             sharded_grad = fsdp_param.sharded_param.grad\n             self.assertIsInstance(sharded_grad, DTensor)\n"
        },
        {
            "name": "_fsdp_collectives.py",
            "path": "torch/distributed/_composable/fsdp/_fsdp_collectives.py",
            "patches": [
                {
                    "old_start": 125,
                    "old_length": 7,
                    "new_start": 125,
                    "new_length": 6,
                    "hunk": "@@ -125,7 +125,6 @@ def foreach_reduce(\n     orig_dtype: torch.dtype,\n     reduce_dtype: Optional[torch.dtype],\n     device: torch.device,\n-    divide_factors: Union[Tuple[None, None], Tuple[float, float]],\n     all_reduce_group: Optional[dist.ProcessGroup],\n     all_reduce_stream: torch.cuda.Stream,\n ) -> torch.cuda.Event:\n"
                },
                {
                    "old_start": 142,
                    "old_length": 7,
                    "new_start": 141,
                    "new_length": 9,
                    "hunk": "@@ -142,7 +141,9 @@ def foreach_reduce(\n         )\n     grad_dtype = unsharded_grads[0].dtype\n     reduce_dtype = reduce_dtype or grad_dtype\n-    predivide_factor, postdivide_factor = divide_factors\n+    predivide_factor, postdivide_factor = _get_gradient_divide_factors(\n+        reduce_scatter_group, all_reduce_group, reduce_dtype\n+    )\n     world_size = reduce_scatter_group.size()\n     padded_unsharded_sizes = tuple(\n         _get_dim0_padded_size(grad.size(), world_size) for grad in unsharded_grads\n"
                },
                {
                    "old_start": 166,
                    "old_length": 18,
                    "new_start": 167,
                    "new_length": 22,
                    "hunk": "@@ -166,18 +167,22 @@ def foreach_reduce(\n             (reduce_scatter_output_numel,)\n         )\n         _div_if_needed(reduce_scatter_input, predivide_factor)\n-        _reduce_scatter(\n-            post_reduce_output,\n-            reduce_scatter_input,\n-            reduce_scatter_group,\n-            divide_factors,\n+        dist.reduce_scatter_tensor(\n+            output=post_reduce_output,\n+            input=reduce_scatter_input,\n+            group=reduce_scatter_group,\n+            op=ReduceOp.AVG if predivide_factor is None else ReduceOp.SUM,\n         )\n     view_out_stream = reduce_scatter_stream\n     if all_reduce_group is not None:\n         view_out_stream = all_reduce_stream\n         all_reduce_stream.wait_stream(reduce_scatter_stream)\n         with torch.cuda.stream(all_reduce_stream):\n-            _all_reduce(post_reduce_output, all_reduce_group, divide_factors)\n+            dist.all_reduce(\n+                post_reduce_output,\n+                group=all_reduce_group,\n+                op=ReduceOp.AVG if predivide_factor is None else ReduceOp.SUM,\n+            )\n     with torch.cuda.stream(view_out_stream):\n         _div_if_needed(post_reduce_output, postdivide_factor)\n         post_reduce_output = _to_dtype_if_needed(post_reduce_output, orig_dtype)\n"
                },
                {
                    "old_start": 257,
                    "old_length": 30,
                    "new_start": 262,
                    "new_length": 27,
                    "hunk": "@@ -257,30 +262,27 @@ def _get_all_gather_input_metadatas(\n     )\n \n \n-def _reduce_scatter(\n-    output: torch.Tensor,\n-    input: torch.Tensor,\n-    group: dist.ProcessGroup,\n-    divide_factors: Union[Tuple[None, None], Tuple[float, float]],\n-) -> None:\n-    if divide_factors[0]:\n-        dist.reduce_scatter_tensor(output, input, group=group)\n-    else:\n-        # Using NCCL's reduce-scatter to do the division by world size saves\n-        # extra memory read/write from a separate division kernel\n-        dist.reduce_scatter_tensor(output, input, op=ReduceOp.AVG, group=group)\n-\n-\n-def _all_reduce(\n-    tensor: torch.Tensor,\n-    group: dist.ProcessGroup,\n-    divide_factors: Union[Tuple[None, None], Tuple[float, float]],\n-) -> None:\n-    if divide_factors[0]:\n-        dist.all_reduce(tensor, group=group)\n-    else:\n-        # saves extra memory read/write from a separate division kernel\n-        dist.all_reduce(tensor, op=ReduceOp.AVG, group=group)\n+def _get_gradient_divide_factors(\n+    reduce_scatter_group: dist.ProcessGroup,\n+    all_reduce_group: Optional[dist.ProcessGroup],\n+    reduce_dtype: torch.dtype,\n+) -> Union[Tuple[None, None], Tuple[float, float]]:\n+    # For fp32/bf16, we do not need to worry about overflow/underflow, so we\n+    # use NCCL's built-in division to avoid separate div kernels\n+    if reduce_dtype in (torch.float32, torch.bfloat16):\n+        return None, None\n+    data_parallel_size = reduce_scatter_group.size()\n+    if all_reduce_group is not None:\n+        data_parallel_size *= all_reduce_group.size()\n+    # Since fp16 has smaller dynamic range than fp32/bf16, we want to avoid\n+    # overflow/underflow. For N data parallel workers, each worker computes\n+    # g_i, and they collectively reduce (g_1 + ... + g_N) / N. To avoid\n+    # overflow/underflow, we divide by ~sqrt(N) before/after the reduction.\n+    factor: int = 1\n+    while data_parallel_size % factor == 0 and data_parallel_size / factor > factor:\n+        factor *= 2\n+    factor = float(factor)\n+    return (factor, data_parallel_size / factor)\n \n \n def _div_if_needed(tensor: torch.Tensor, div_factor: Optional[float]) -> None:\n"
                }
            ],
            "whole_deleted": "-    divide_factors: Union[Tuple[None, None], Tuple[float, float]],\n-    predivide_factor, postdivide_factor = divide_factors\n-        _reduce_scatter(\n-            post_reduce_output,\n-            reduce_scatter_input,\n-            reduce_scatter_group,\n-            divide_factors,\n-            _all_reduce(post_reduce_output, all_reduce_group, divide_factors)\n-def _reduce_scatter(\n-    output: torch.Tensor,\n-    input: torch.Tensor,\n-    group: dist.ProcessGroup,\n-    divide_factors: Union[Tuple[None, None], Tuple[float, float]],\n-) -> None:\n-    if divide_factors[0]:\n-        dist.reduce_scatter_tensor(output, input, group=group)\n-    else:\n-        # Using NCCL's reduce-scatter to do the division by world size saves\n-        # extra memory read/write from a separate division kernel\n-        dist.reduce_scatter_tensor(output, input, op=ReduceOp.AVG, group=group)\n-\n-\n-def _all_reduce(\n-    tensor: torch.Tensor,\n-    group: dist.ProcessGroup,\n-    divide_factors: Union[Tuple[None, None], Tuple[float, float]],\n-) -> None:\n-    if divide_factors[0]:\n-        dist.all_reduce(tensor, group=group)\n-    else:\n-        # saves extra memory read/write from a separate division kernel\n-        dist.all_reduce(tensor, op=ReduceOp.AVG, group=group)\n",
            "whole_added": "+    predivide_factor, postdivide_factor = _get_gradient_divide_factors(\n+        reduce_scatter_group, all_reduce_group, reduce_dtype\n+    )\n+        dist.reduce_scatter_tensor(\n+            output=post_reduce_output,\n+            input=reduce_scatter_input,\n+            group=reduce_scatter_group,\n+            op=ReduceOp.AVG if predivide_factor is None else ReduceOp.SUM,\n+            dist.all_reduce(\n+                post_reduce_output,\n+                group=all_reduce_group,\n+                op=ReduceOp.AVG if predivide_factor is None else ReduceOp.SUM,\n+            )\n+def _get_gradient_divide_factors(\n+    reduce_scatter_group: dist.ProcessGroup,\n+    all_reduce_group: Optional[dist.ProcessGroup],\n+    reduce_dtype: torch.dtype,\n+) -> Union[Tuple[None, None], Tuple[float, float]]:\n+    # For fp32/bf16, we do not need to worry about overflow/underflow, so we\n+    # use NCCL's built-in division to avoid separate div kernels\n+    if reduce_dtype in (torch.float32, torch.bfloat16):\n+        return None, None\n+    data_parallel_size = reduce_scatter_group.size()\n+    if all_reduce_group is not None:\n+        data_parallel_size *= all_reduce_group.size()\n+    # Since fp16 has smaller dynamic range than fp32/bf16, we want to avoid\n+    # overflow/underflow. For N data parallel workers, each worker computes\n+    # g_i, and they collectively reduce (g_1 + ... + g_N) / N. To avoid\n+    # overflow/underflow, we divide by ~sqrt(N) before/after the reduction.\n+    factor: int = 1\n+    while data_parallel_size % factor == 0 and data_parallel_size / factor > factor:\n+        factor *= 2\n+    factor = float(factor)\n+    return (factor, data_parallel_size / factor)\n",
            "whole_hunk": "@@ -125,7 +125,6 @@ def foreach_reduce(\n     orig_dtype: torch.dtype,\n     reduce_dtype: Optional[torch.dtype],\n     device: torch.device,\n-    divide_factors: Union[Tuple[None, None], Tuple[float, float]],\n     all_reduce_group: Optional[dist.ProcessGroup],\n     all_reduce_stream: torch.cuda.Stream,\n ) -> torch.cuda.Event:\n@@ -142,7 +141,9 @@ def foreach_reduce(\n         )\n     grad_dtype = unsharded_grads[0].dtype\n     reduce_dtype = reduce_dtype or grad_dtype\n-    predivide_factor, postdivide_factor = divide_factors\n+    predivide_factor, postdivide_factor = _get_gradient_divide_factors(\n+        reduce_scatter_group, all_reduce_group, reduce_dtype\n+    )\n     world_size = reduce_scatter_group.size()\n     padded_unsharded_sizes = tuple(\n         _get_dim0_padded_size(grad.size(), world_size) for grad in unsharded_grads\n@@ -166,18 +167,22 @@ def foreach_reduce(\n             (reduce_scatter_output_numel,)\n         )\n         _div_if_needed(reduce_scatter_input, predivide_factor)\n-        _reduce_scatter(\n-            post_reduce_output,\n-            reduce_scatter_input,\n-            reduce_scatter_group,\n-            divide_factors,\n+        dist.reduce_scatter_tensor(\n+            output=post_reduce_output,\n+            input=reduce_scatter_input,\n+            group=reduce_scatter_group,\n+            op=ReduceOp.AVG if predivide_factor is None else ReduceOp.SUM,\n         )\n     view_out_stream = reduce_scatter_stream\n     if all_reduce_group is not None:\n         view_out_stream = all_reduce_stream\n         all_reduce_stream.wait_stream(reduce_scatter_stream)\n         with torch.cuda.stream(all_reduce_stream):\n-            _all_reduce(post_reduce_output, all_reduce_group, divide_factors)\n+            dist.all_reduce(\n+                post_reduce_output,\n+                group=all_reduce_group,\n+                op=ReduceOp.AVG if predivide_factor is None else ReduceOp.SUM,\n+            )\n     with torch.cuda.stream(view_out_stream):\n         _div_if_needed(post_reduce_output, postdivide_factor)\n         post_reduce_output = _to_dtype_if_needed(post_reduce_output, orig_dtype)\n@@ -257,30 +262,27 @@ def _get_all_gather_input_metadatas(\n     )\n \n \n-def _reduce_scatter(\n-    output: torch.Tensor,\n-    input: torch.Tensor,\n-    group: dist.ProcessGroup,\n-    divide_factors: Union[Tuple[None, None], Tuple[float, float]],\n-) -> None:\n-    if divide_factors[0]:\n-        dist.reduce_scatter_tensor(output, input, group=group)\n-    else:\n-        # Using NCCL's reduce-scatter to do the division by world size saves\n-        # extra memory read/write from a separate division kernel\n-        dist.reduce_scatter_tensor(output, input, op=ReduceOp.AVG, group=group)\n-\n-\n-def _all_reduce(\n-    tensor: torch.Tensor,\n-    group: dist.ProcessGroup,\n-    divide_factors: Union[Tuple[None, None], Tuple[float, float]],\n-) -> None:\n-    if divide_factors[0]:\n-        dist.all_reduce(tensor, group=group)\n-    else:\n-        # saves extra memory read/write from a separate division kernel\n-        dist.all_reduce(tensor, op=ReduceOp.AVG, group=group)\n+def _get_gradient_divide_factors(\n+    reduce_scatter_group: dist.ProcessGroup,\n+    all_reduce_group: Optional[dist.ProcessGroup],\n+    reduce_dtype: torch.dtype,\n+) -> Union[Tuple[None, None], Tuple[float, float]]:\n+    # For fp32/bf16, we do not need to worry about overflow/underflow, so we\n+    # use NCCL's built-in division to avoid separate div kernels\n+    if reduce_dtype in (torch.float32, torch.bfloat16):\n+        return None, None\n+    data_parallel_size = reduce_scatter_group.size()\n+    if all_reduce_group is not None:\n+        data_parallel_size *= all_reduce_group.size()\n+    # Since fp16 has smaller dynamic range than fp32/bf16, we want to avoid\n+    # overflow/underflow. For N data parallel workers, each worker computes\n+    # g_i, and they collectively reduce (g_1 + ... + g_N) / N. To avoid\n+    # overflow/underflow, we divide by ~sqrt(N) before/after the reduction.\n+    factor: int = 1\n+    while data_parallel_size % factor == 0 and data_parallel_size / factor > factor:\n+        factor *= 2\n+    factor = float(factor)\n+    return (factor, data_parallel_size / factor)\n \n \n def _div_if_needed(tensor: torch.Tensor, div_factor: Optional[float]) -> None:\n"
        },
        {
            "name": "_fsdp_param_group.py",
            "path": "torch/distributed/_composable/fsdp/_fsdp_param_group.py",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 6,
                    "new_start": 1,
                    "new_length": 6,
                    "hunk": "@@ -1,6 +1,6 @@\n import contextlib\n \n-from typing import Any, cast, Dict, List, NamedTuple, Optional, Set, Tuple, Union\n+from typing import Any, cast, Dict, List, NamedTuple, Optional, Set, Tuple\n \n import torch\n import torch.distributed as dist\n"
                },
                {
                    "old_start": 164,
                    "old_length": 32,
                    "new_start": 164,
                    "new_length": 6,
                    "hunk": "@@ -164,32 +164,6 @@ class FSDPParamGroup:\n             )\n         self._reduce_dtype = next(iter(reduce_dtypes))\n \n-    def _init_grad_divide_factors(self):\n-        data_parallel_world_size = 1\n-        data_parallel_world_size *= self.mesh_info.shard_mesh_size\n-        if self._is_hsdp:\n-            data_parallel_world_size *= self.mesh_info.replicate_mesh_size\n-        if self._reduce_dtype in (torch.float32, torch.bfloat16):\n-            # Use NCCL's AVG op to divide after reduction since it is more\n-            # performant and fp32 has sufficient precision\n-            self._grad_divide_factors: Union[Tuple[None, None], Tuple[float, float]] = (\n-                None,\n-                None,\n-            )\n-            return\n-        # Since fp16 has smaller dynamic range than fp32/bf16, we want to avoid\n-        # overflow/underflow. For N data parallel workers, each worker computes\n-        # g_i, and they collectively reduce (g_1 + ... + g_N) / N. To avoid\n-        # overflow/underflow, we divide by ~sqrt(N) before/after the reduction.\n-        factor: int = 1\n-        while (\n-            data_parallel_world_size % factor == 0\n-            and data_parallel_world_size / factor > factor\n-        ):\n-            factor *= 2\n-        factor = float(factor)\n-        self._grad_divide_factors = (factor, data_parallel_world_size / factor)\n-\n     def lazy_init(self):\n         # Lazy init should be idempotent\n         param_names_on_meta = [\n"
                },
                {
                    "old_start": 207,
                    "old_length": 7,
                    "new_start": 181,
                    "new_length": 6,
                    "hunk": "@@ -207,7 +181,6 @@ class FSDPParamGroup:\n         # Initialize mixed precision attributes lazily in case the user changes\n         # the parameter dtypes after construction time but before forward\n         self._init_mp_dtypes()\n-        self._init_grad_divide_factors()\n         self._register_state_dict_hooks()\n \n     # Runtime #\n"
                },
                {
                    "old_start": 346,
                    "old_length": 7,
                    "new_start": 319,
                    "new_length": 6,
                    "hunk": "@@ -346,7 +319,6 @@ class FSDPParamGroup:\n                 self._orig_dtype,\n                 self._reduce_dtype,\n                 self.device,\n-                self._grad_divide_factors,\n                 self._all_reduce_process_group\n                 if self._is_hsdp and self.all_reduce_grads\n                 else None,"
                }
            ],
            "whole_deleted": "-from typing import Any, cast, Dict, List, NamedTuple, Optional, Set, Tuple, Union\n-    def _init_grad_divide_factors(self):\n-        data_parallel_world_size = 1\n-        data_parallel_world_size *= self.mesh_info.shard_mesh_size\n-        if self._is_hsdp:\n-            data_parallel_world_size *= self.mesh_info.replicate_mesh_size\n-        if self._reduce_dtype in (torch.float32, torch.bfloat16):\n-            # Use NCCL's AVG op to divide after reduction since it is more\n-            # performant and fp32 has sufficient precision\n-            self._grad_divide_factors: Union[Tuple[None, None], Tuple[float, float]] = (\n-                None,\n-                None,\n-            )\n-            return\n-        # Since fp16 has smaller dynamic range than fp32/bf16, we want to avoid\n-        # overflow/underflow. For N data parallel workers, each worker computes\n-        # g_i, and they collectively reduce (g_1 + ... + g_N) / N. To avoid\n-        # overflow/underflow, we divide by ~sqrt(N) before/after the reduction.\n-        factor: int = 1\n-        while (\n-            data_parallel_world_size % factor == 0\n-            and data_parallel_world_size / factor > factor\n-        ):\n-            factor *= 2\n-        factor = float(factor)\n-        self._grad_divide_factors = (factor, data_parallel_world_size / factor)\n-\n-        self._init_grad_divide_factors()\n-                self._grad_divide_factors,\n",
            "whole_added": "+from typing import Any, cast, Dict, List, NamedTuple, Optional, Set, Tuple\n",
            "whole_hunk": "@@ -1,6 +1,6 @@\n import contextlib\n \n-from typing import Any, cast, Dict, List, NamedTuple, Optional, Set, Tuple, Union\n+from typing import Any, cast, Dict, List, NamedTuple, Optional, Set, Tuple\n \n import torch\n import torch.distributed as dist\n@@ -164,32 +164,6 @@ class FSDPParamGroup:\n             )\n         self._reduce_dtype = next(iter(reduce_dtypes))\n \n-    def _init_grad_divide_factors(self):\n-        data_parallel_world_size = 1\n-        data_parallel_world_size *= self.mesh_info.shard_mesh_size\n-        if self._is_hsdp:\n-            data_parallel_world_size *= self.mesh_info.replicate_mesh_size\n-        if self._reduce_dtype in (torch.float32, torch.bfloat16):\n-            # Use NCCL's AVG op to divide after reduction since it is more\n-            # performant and fp32 has sufficient precision\n-            self._grad_divide_factors: Union[Tuple[None, None], Tuple[float, float]] = (\n-                None,\n-                None,\n-            )\n-            return\n-        # Since fp16 has smaller dynamic range than fp32/bf16, we want to avoid\n-        # overflow/underflow. For N data parallel workers, each worker computes\n-        # g_i, and they collectively reduce (g_1 + ... + g_N) / N. To avoid\n-        # overflow/underflow, we divide by ~sqrt(N) before/after the reduction.\n-        factor: int = 1\n-        while (\n-            data_parallel_world_size % factor == 0\n-            and data_parallel_world_size / factor > factor\n-        ):\n-            factor *= 2\n-        factor = float(factor)\n-        self._grad_divide_factors = (factor, data_parallel_world_size / factor)\n-\n     def lazy_init(self):\n         # Lazy init should be idempotent\n         param_names_on_meta = [\n@@ -207,7 +181,6 @@ class FSDPParamGroup:\n         # Initialize mixed precision attributes lazily in case the user changes\n         # the parameter dtypes after construction time but before forward\n         self._init_mp_dtypes()\n-        self._init_grad_divide_factors()\n         self._register_state_dict_hooks()\n \n     # Runtime #\n@@ -346,7 +319,6 @@ class FSDPParamGroup:\n                 self._orig_dtype,\n                 self._reduce_dtype,\n                 self.device,\n-                self._grad_divide_factors,\n                 self._all_reduce_process_group\n                 if self._is_hsdp and self.all_reduce_grads\n                 else None,"
        }
    ]
},
{
    "Id": 81,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/0f81473d7b4a1bf09246410712df22541be7caf3",
    "date": "2024-06-17T13:41:15+00:00",
    "message": "Update fake tensor error checks for bool tensor subtraction (#128492)\n\nFixes #127003\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128492\nApproved by: https://github.com/soulitzer",
    "label": "YES",
    "changes": [
        {
            "name": "test_binary_ufuncs.py",
            "path": "test/test_binary_ufuncs.py",
            "patches": [
                {
                    "old_start": 3694,
                    "old_length": 8,
                    "new_start": 3694,
                    "new_length": 6,
                    "hunk": "@@ -3694,8 +3694,6 @@ class TestBinaryUfuncs(TestCase):\n             actual = op(x, y, alpha=alpha)\n             self.assertTrue(not (actual.isnan() or actual.isinf()))\n \n-    # https://github.com/pytorch/pytorch/issues/127003\n-    @xfailIfTorchDynamo\n     def test_sub_typing(self, device):\n         m1 = torch.tensor(\n             [True, False, False, True, False, False], dtype=torch.bool, device=device\n"
                }
            ],
            "whole_deleted": "-    # https://github.com/pytorch/pytorch/issues/127003\n-    @xfailIfTorchDynamo\n",
            "whole_added": "",
            "whole_hunk": "@@ -3694,8 +3694,6 @@ class TestBinaryUfuncs(TestCase):\n             actual = op(x, y, alpha=alpha)\n             self.assertTrue(not (actual.isnan() or actual.isinf()))\n \n-    # https://github.com/pytorch/pytorch/issues/127003\n-    @xfailIfTorchDynamo\n     def test_sub_typing(self, device):\n         m1 = torch.tensor(\n             [True, False, False, True, False, False], dtype=torch.bool, device=device\n"
        },
        {
            "name": "__init__.py",
            "path": "torch/_refs/__init__.py",
            "patches": [
                {
                    "old_start": 1712,
                    "old_length": 6,
                    "new_start": 1712,
                    "new_length": 15,
                    "hunk": "@@ -1712,6 +1712,15 @@ def sub(\n \n     a, b = _maybe_broadcast(a, b)\n \n+    if isinstance(a, TensorLike) and isinstance(b, TensorLike):\n+        torch._check(\n+            not utils.is_boolean_dtype(a.dtype) and not utils.is_boolean_dtype(b.dtype),\n+            lambda: (\n+                \"Subtraction, the `-` operator, with two bool tensors is not supported. \"\n+                \"Use the `^` or `logical_xor()` operator instead.\"\n+            ),\n+        )\n+\n     if alpha != 1:\n         dtype = a.dtype if isinstance(a, TensorLike) else b.dtype  # type: ignore[union-attr]\n         python_type = utils.dtype_to_type(dtype)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    if isinstance(a, TensorLike) and isinstance(b, TensorLike):\n+        torch._check(\n+            not utils.is_boolean_dtype(a.dtype) and not utils.is_boolean_dtype(b.dtype),\n+            lambda: (\n+                \"Subtraction, the `-` operator, with two bool tensors is not supported. \"\n+                \"Use the `^` or `logical_xor()` operator instead.\"\n+            ),\n+        )\n+\n",
            "whole_hunk": "@@ -1712,6 +1712,15 @@ def sub(\n \n     a, b = _maybe_broadcast(a, b)\n \n+    if isinstance(a, TensorLike) and isinstance(b, TensorLike):\n+        torch._check(\n+            not utils.is_boolean_dtype(a.dtype) and not utils.is_boolean_dtype(b.dtype),\n+            lambda: (\n+                \"Subtraction, the `-` operator, with two bool tensors is not supported. \"\n+                \"Use the `^` or `logical_xor()` operator instead.\"\n+            ),\n+        )\n+\n     if alpha != 1:\n         dtype = a.dtype if isinstance(a, TensorLike) else b.dtype  # type: ignore[union-attr]\n         python_type = utils.dtype_to_type(dtype)"
        }
    ]
},
{
    "Id": 544,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/610f64d72a53c644591e83fbecdd8a8f1702f548",
    "date": "2023-08-23T07:16:14+00:00",
    "message": "inductor: also check index_exp when  select tiling var (#106765)\n\nFor select tiling var, currently, we only consider load and store which do not consider index exp, and meet accuracy issues:\n\nbefore(the index exp ```i1-1``` can not be vectrized):\n```\ncpp_fused_constant_pad_nd_mul_0 = async_compile.cpp('''\n#include \"/tmp/torchinductor_xiaobing/i5/ci5uspp363v3ky6jkccllm3bxudy2fkdpqinkqhmpehfihejs7ko.h\"\nextern \"C\" void kernel(const float* in_ptr0,\n                       const float* in_ptr1,\n                       float* out_ptr0)\n{\n    #pragma omp parallel num_threads(40)\n    {\n        {\n            #pragma omp for\n            for(long i0=static_cast<long>(0L); i0<static_cast<long>(64L); i0+=static_cast<long>(1L))\n            {\n                #pragma GCC ivdep\n                for(long i1=static_cast<long>(0L); i1<static_cast<long>(3136L); i1+=static_cast<long>(16L))\n                {\n                    #pragma GCC ivdep\n                    for(long i2=static_cast<long>(0L); i2<static_cast<long>(8L); i2+=static_cast<long>(1L))\n                    {\n                        auto tmp0 = at::vec::Vectorized<int>(static_cast<int>((-1L) + i1));\n                        auto tmp1 = at::vec::Vectorized<int>(static_cast<int>(0));\n                        auto tmp2 = to_float_mask(tmp0 >= tmp1);\n                        auto tmp3 = [&]\n                        {\n                            auto tmp4 = ([&]() { __at_align__ float tmpbuf[16]; for (long i1_inner = 0; i1_inner < 16; i1_inner++) tmpbuf[i1_inner] = in_ptr0[static_cast<long>((-8L) + i2 + (8L*i1) + (8L*i1_inner) + (25088L*i0))]; return at::vec::Vectorized<float>::loadu(tmpbuf); })();\n                            auto tmp5 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>((-1L) + i1 + (3136L*i2) + (25088L*i0)));\n                            auto tmp6 = tmp4 * tmp5;\n                            return tmp6;\n                        }\n                        ;\n                        auto tmp7 = decltype(tmp3())::blendv(at::vec::Vectorized<float>(0.0), tmp3(), to_float_mask(tmp2));\n                        { __at_align__ float tmpbuf[16*sizeof(float)/sizeof(float)]; tmp7.store(tmpbuf); for (long i1_inner = 0; i1_inner < 16; i1_inner++) out_ptr0[static_cast<long>(i2 + (8L*i1) + (8L*i1_inner) + (25096L*i0))] = tmpbuf[i1_inner]; }\n                    }\n                }\n                #pragma GCC ivdep\n                for(long i1=static_cast<long>(3136L); i1<static_cast<long>(3137L); i1+=static_cast<long>(1L))\n                {\n                    #pragma GCC ivdep\n                    for(long i2=static_cast<long>(0L); i2<static_cast<long>(8L); i2+=static_cast<long>(1L))\n                    {\n                        auto tmp0 = static_cast<long>((-1L) + i1);\n                        auto tmp1 = static_cast<long>(0);\n                        auto tmp2 = tmp0 >= tmp1;\n                        auto tmp3 = [&]\n                        {\n                            auto tmp4 = in_ptr0[static_cast<long>((-8L) + i2 + (8L*i1) + (25088L*i0))];\n                            auto tmp5 = in_ptr1[static_cast<long>((-1L) + i1 + (3136L*i2) + (25088L*i0))];\n                            auto tmp6 = decltype(tmp4)(tmp4 * tmp5);\n                            return tmp6;\n                        }\n                        ;\n                        auto tmp7 = tmp2 ? tmp3() : static_cast<decltype(tmp3())>(0.0);\n                        out_ptr0[static_cast<long>(i2 + (8L*i1) + (25096L*i0))] = tmp7;\n                    }\n                }\n            }\n        }\n    }\n}\n```\n\nafter:\n```\ncpp_fused_constant_pad_nd_mul_0 = async_compile.cpp('''\n#include \"/tmp/torchinductor_xiaobing/i5/ci5uspp363v3ky6jkccllm3bxudy2fkdpqinkqhmpehfihejs7ko.h\"\nextern \"C\" void kernel(const float* in_ptr0,\n                       const float* in_ptr1,\n                       float* out_ptr0)\n{\n    #pragma omp parallel num_threads(40)\n    {\n        {\n            #pragma omp for\n            for(long i0=static_cast<long>(0L); i0<static_cast<long>(64L); i0+=static_cast<long>(1L))\n            {\n                #pragma GCC ivdep\n                for(long i1=static_cast<long>(0L); i1<static_cast<long>(3137L); i1+=static_cast<long>(1L))\n                {\n                    #pragma omp simd simdlen(8)\n                    for(long i2=static_cast<long>(0L); i2<static_cast<long>(8L); i2+=static_cast<long>(1L))\n                    {\n                        auto tmp0 = static_cast<long>((-1L) + i1);\n                        auto tmp1 = static_cast<long>(0);\n                        auto tmp2 = tmp0 >= tmp1;\n                        auto tmp3 = [&]\n                        {\n                            auto tmp4 = in_ptr0[static_cast<long>((-8L) + i2 + (8L*i1) + (25088L*i0))];\n                            auto tmp5 = in_ptr1[static_cast<long>((-1L) + i1 + (3136L*i2) + (25088L*i0))];\n                            auto tmp6 = decltype(tmp4)(tmp4 * tmp5);\n                            return tmp6;\n                        }\n                        ;\n                        auto tmp7 = tmp2 ? tmp3() : static_cast<decltype(tmp3())>(0.0);\n                        out_ptr0[static_cast<long>(i2 + (8L*i1) + (25096L*i0))] = tmp7;\n                    }\n                }\n            }\n        }\n    }\n}\n''')\n\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/106765\nApproved by: https://github.com/jgong5, https://github.com/jansel",
    "label": "NO",
    "changes": [
        {
            "name": "test_cpu_repro.py",
            "path": "test/inductor/test_cpu_repro.py",
            "patches": [
                {
                    "old_start": 2386,
                    "old_length": 6,
                    "new_start": 2386,
                    "new_length": 17,
                    "hunk": "@@ -2386,6 +2386,17 @@ class CPUReproTests(TestCase):\n \n         self.common(fn, ())\n \n+    def test_select_tiliing_with_index_expr(self):\n+        def fn(x, y):\n+            x = torch.ops.aten.view.default(x, [8, 8, 8, 3136])\n+            x = torch.ops.aten.permute.default(x, [0, 1, 3, 2])\n+            y = torch.ops.aten.mul.Tensor(y, x)\n+            return torch.ops.aten.constant_pad_nd.default(y, [0, 0, 1, 0, 0, 0], 0.0)\n+\n+        x = torch.randn(8, 64, 56, 56)\n+        y = torch.randn(8, 8, 3136, 8)\n+        self.common(fn, (x, y))\n+\n \n if __name__ == \"__main__\":\n     from torch._dynamo.test_case import run_tests\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    def test_select_tiliing_with_index_expr(self):\n+        def fn(x, y):\n+            x = torch.ops.aten.view.default(x, [8, 8, 8, 3136])\n+            x = torch.ops.aten.permute.default(x, [0, 1, 3, 2])\n+            y = torch.ops.aten.mul.Tensor(y, x)\n+            return torch.ops.aten.constant_pad_nd.default(y, [0, 0, 1, 0, 0, 0], 0.0)\n+\n+        x = torch.randn(8, 64, 56, 56)\n+        y = torch.randn(8, 8, 3136, 8)\n+        self.common(fn, (x, y))\n+\n",
            "whole_hunk": "@@ -2386,6 +2386,17 @@ class CPUReproTests(TestCase):\n \n         self.common(fn, ())\n \n+    def test_select_tiliing_with_index_expr(self):\n+        def fn(x, y):\n+            x = torch.ops.aten.view.default(x, [8, 8, 8, 3136])\n+            x = torch.ops.aten.permute.default(x, [0, 1, 3, 2])\n+            y = torch.ops.aten.mul.Tensor(y, x)\n+            return torch.ops.aten.constant_pad_nd.default(y, [0, 0, 1, 0, 0, 0], 0.0)\n+\n+        x = torch.randn(8, 64, 56, 56)\n+        y = torch.randn(8, 8, 3136, 8)\n+        self.common(fn, (x, y))\n+\n \n if __name__ == \"__main__\":\n     from torch._dynamo.test_case import run_tests\n"
        },
        {
            "name": "cpp.py",
            "path": "torch/_inductor/codegen/cpp.py",
            "patches": [
                {
                    "old_start": 2649,
                    "old_length": 14,
                    "new_start": 2649,
                    "new_length": 19,
                    "hunk": "@@ -2649,14 +2649,19 @@ class CppKernelProxy(CppKernel):\n             tiling_factor = self.picked_vec_isa.nelements(dtype=dtype)\n             tiling_indices = select_tiling_indices()\n             if tiling_indices:\n-                with CppVecKernelChecker(\n-                    deepcopy(self.kernel_group.args),\n-                    parallel_num_threads(),\n-                    tiling_factor,\n-                    tiling_indices[-1],\n-                ) as vec_checker:\n-                    run(vec_checker)\n-                if vec_checker.simd_vec:\n+                could_vec = True\n+                for tiling_indice in tiling_indices:\n+                    with CppVecKernelChecker(\n+                        deepcopy(self.kernel_group.args),\n+                        parallel_num_threads(),\n+                        tiling_factor,\n+                        tiling_indice,\n+                    ) as vec_checker:\n+                        run(vec_checker)\n+                        could_vec = could_vec and vec_checker.simd_vec\n+                        if not could_vec:\n+                            break\n+                if could_vec:\n                     if len(tiling_indices) == 1:\n                         return [tiling_factor], tiling_indices\n                     if len(tiling_indices) == 2:"
                }
            ],
            "whole_deleted": "-                with CppVecKernelChecker(\n-                    deepcopy(self.kernel_group.args),\n-                    parallel_num_threads(),\n-                    tiling_factor,\n-                    tiling_indices[-1],\n-                ) as vec_checker:\n-                    run(vec_checker)\n-                if vec_checker.simd_vec:\n",
            "whole_added": "+                could_vec = True\n+                for tiling_indice in tiling_indices:\n+                    with CppVecKernelChecker(\n+                        deepcopy(self.kernel_group.args),\n+                        parallel_num_threads(),\n+                        tiling_factor,\n+                        tiling_indice,\n+                    ) as vec_checker:\n+                        run(vec_checker)\n+                        could_vec = could_vec and vec_checker.simd_vec\n+                        if not could_vec:\n+                            break\n+                if could_vec:\n",
            "whole_hunk": "@@ -2649,14 +2649,19 @@ class CppKernelProxy(CppKernel):\n             tiling_factor = self.picked_vec_isa.nelements(dtype=dtype)\n             tiling_indices = select_tiling_indices()\n             if tiling_indices:\n-                with CppVecKernelChecker(\n-                    deepcopy(self.kernel_group.args),\n-                    parallel_num_threads(),\n-                    tiling_factor,\n-                    tiling_indices[-1],\n-                ) as vec_checker:\n-                    run(vec_checker)\n-                if vec_checker.simd_vec:\n+                could_vec = True\n+                for tiling_indice in tiling_indices:\n+                    with CppVecKernelChecker(\n+                        deepcopy(self.kernel_group.args),\n+                        parallel_num_threads(),\n+                        tiling_factor,\n+                        tiling_indice,\n+                    ) as vec_checker:\n+                        run(vec_checker)\n+                        could_vec = could_vec and vec_checker.simd_vec\n+                        if not could_vec:\n+                            break\n+                if could_vec:\n                     if len(tiling_indices) == 1:\n                         return [tiling_factor], tiling_indices\n                     if len(tiling_indices) == 2:"
        }
    ]
},
{
    "Id": 185,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332",
    "date": "2024-04-26T15:35:53+00:00",
    "message": "Fix mypy issues in fake_tensor.py (#124428)\n\nfake_tensor.py had mypy error ignored. That seems less than desirable.\n\nAlso added SafePyObjectT<T> which is a tagged wrapper around a SafePyObject but provides static type checking (with no other guarantees).\n\nUsed `SafePyObjectT<TorchDispatchModeKey>` on some of the TorchDispatchModeTLS API to ensure that we don't accidentally inject a different type than expected into the stack.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/124428\nApproved by: https://github.com/malfet",
    "label": "YES",
    "changes": [
        {
            "name": "SafePyObject.h",
            "path": "c10/core/SafePyObject.h",
            "patches": [
                {
                    "old_start": 55,
                    "old_length": 6,
                    "new_start": 55,
                    "new_length": 22,
                    "hunk": "@@ -55,6 +55,22 @@ struct C10_API SafePyObject {\n   c10::impl::PyInterpreter* pyinterpreter_;\n };\n \n+// A newtype wrapper around SafePyObject for type safety when a python object\n+// represents a specific type. Note that `T` is only used as a tag and isn't\n+// actually used for any true purpose.\n+template <typename T>\n+struct SafePyObjectT : private SafePyObject {\n+  SafePyObjectT(PyObject* data, c10::impl::PyInterpreter* pyinterpreter)\n+      : SafePyObject(data, pyinterpreter) {}\n+  SafePyObjectT(SafePyObjectT&& other) noexcept : SafePyObject(other) {}\n+  SafePyObjectT(SafePyObjectT const&) = delete;\n+  SafePyObjectT& operator=(SafePyObjectT const&) = delete;\n+\n+  using SafePyObject::ptr;\n+  using SafePyObject::pyinterpreter;\n+  using SafePyObject::release;\n+};\n+\n // Like SafePyObject, but non-owning.  Good for references to global PyObjects\n // that will be leaked on interpreter exit.  You get a copy constructor/assign\n // this way.\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+// A newtype wrapper around SafePyObject for type safety when a python object\n+// represents a specific type. Note that `T` is only used as a tag and isn't\n+// actually used for any true purpose.\n+template <typename T>\n+struct SafePyObjectT : private SafePyObject {\n+  SafePyObjectT(PyObject* data, c10::impl::PyInterpreter* pyinterpreter)\n+      : SafePyObject(data, pyinterpreter) {}\n+  SafePyObjectT(SafePyObjectT&& other) noexcept : SafePyObject(other) {}\n+  SafePyObjectT(SafePyObjectT const&) = delete;\n+  SafePyObjectT& operator=(SafePyObjectT const&) = delete;\n+\n+  using SafePyObject::ptr;\n+  using SafePyObject::pyinterpreter;\n+  using SafePyObject::release;\n+};\n+\n",
            "whole_hunk": "@@ -55,6 +55,22 @@ struct C10_API SafePyObject {\n   c10::impl::PyInterpreter* pyinterpreter_;\n };\n \n+// A newtype wrapper around SafePyObject for type safety when a python object\n+// represents a specific type. Note that `T` is only used as a tag and isn't\n+// actually used for any true purpose.\n+template <typename T>\n+struct SafePyObjectT : private SafePyObject {\n+  SafePyObjectT(PyObject* data, c10::impl::PyInterpreter* pyinterpreter)\n+      : SafePyObject(data, pyinterpreter) {}\n+  SafePyObjectT(SafePyObjectT&& other) noexcept : SafePyObject(other) {}\n+  SafePyObjectT(SafePyObjectT const&) = delete;\n+  SafePyObjectT& operator=(SafePyObjectT const&) = delete;\n+\n+  using SafePyObject::ptr;\n+  using SafePyObject::pyinterpreter;\n+  using SafePyObject::release;\n+};\n+\n // Like SafePyObject, but non-owning.  Good for references to global PyObjects\n // that will be leaked on interpreter exit.  You get a copy constructor/assign\n // this way.\n"
        },
        {
            "name": "TorchDispatchModeTLS.cpp",
            "path": "c10/core/impl/TorchDispatchModeTLS.cpp",
            "patches": [
                {
                    "old_start": 25,
                    "old_length": 7,
                    "new_start": 25,
                    "new_length": 7,
                    "hunk": "@@ -25,7 +25,7 @@ bool TorchDispatchModeTLS::any_modes_set(bool skip_infra_modes) {\n }\n \n void TorchDispatchModeTLS::push_non_infra_mode_onto_stack(\n-    std::shared_ptr<SafePyObject> mode) {\n+    std::shared_ptr<PyObject_TorchDispatchMode> mode) {\n   if (!any_modes_set()) {\n     c10::impl::tls_set_dispatch_key_included(DispatchKey::Python, true);\n     c10::impl::tls_set_dispatch_key_included(\n"
                },
                {
                    "old_start": 34,
                    "old_length": 8,
                    "new_start": 34,
                    "new_length": 9,
                    "hunk": "@@ -34,8 +34,9 @@ void TorchDispatchModeTLS::push_non_infra_mode_onto_stack(\n   torchDispatchModeState.stack_.push_back(std::move(mode));\n }\n \n-const std::shared_ptr<SafePyObject> TorchDispatchModeTLS::pop_stack() {\n-  std::shared_ptr<SafePyObject> out;\n+const std::shared_ptr<PyObject_TorchDispatchMode> TorchDispatchModeTLS::\n+    pop_stack() {\n+  std::shared_ptr<PyObject_TorchDispatchMode> out;\n   if (!torchDispatchModeState.stack_.empty()) {\n     out = torchDispatchModeState.stack_.back();\n     torchDispatchModeState.stack_.pop_back();\n"
                },
                {
                    "old_start": 60,
                    "old_length": 8,
                    "new_start": 61,
                    "new_length": 9,
                    "hunk": "@@ -60,8 +61,9 @@ const std::shared_ptr<SafePyObject> TorchDispatchModeTLS::pop_stack() {\n   }\n   return out;\n }\n-const std::tuple<std::shared_ptr<SafePyObject>, TorchDispatchModeKey>\n-TorchDispatchModeTLS::pop_highest_infra_mode() {\n+const std::\n+    tuple<std::shared_ptr<PyObject_TorchDispatchMode>, TorchDispatchModeKey>\n+    TorchDispatchModeTLS::pop_highest_infra_mode() {\n   for (int64_t i = static_cast<size_t>(TorchDispatchModeKey::NUM_MODE_KEYS) - 1;\n        i >= 0;\n        --i) {\n"
                },
                {
                    "old_start": 82,
                    "old_length": 8,
                    "new_start": 84,
                    "new_length": 8,
                    "hunk": "@@ -82,8 +84,8 @@ TorchDispatchModeTLS::pop_highest_infra_mode() {\n       false, \"Called pop_highest_infra_mode, but no infra modes were active.\")\n }\n \n-const std::shared_ptr<SafePyObject>& TorchDispatchModeTLS::get_stack_at(\n-    int64_t idx) {\n+const std::shared_ptr<PyObject_TorchDispatchMode>& TorchDispatchModeTLS::\n+    get_stack_at(int64_t idx) {\n   TORCH_CHECK(idx < stack_len(), \"Tried to get stack at idx that's too big\");\n   // Our \"logical\" stack includes both:\n   // - any user modes (the entire torchDispatchModeState.stack_)\n"
                },
                {
                    "old_start": 119,
                    "old_length": 13,
                    "new_start": 121,
                    "new_length": 13,
                    "hunk": "@@ -119,13 +121,13 @@ int64_t TorchDispatchModeTLS::stack_len() {\n   return stack_len + infra_modes_len;\n }\n \n-const c10::optional<std::shared_ptr<SafePyObject>> TorchDispatchModeTLS::\n-    get_mode(TorchDispatchModeKey mode_key) {\n+const c10::optional<std::shared_ptr<PyObject_TorchDispatchMode>>\n+TorchDispatchModeTLS::get_mode(TorchDispatchModeKey mode_key) {\n   return torchDispatchModeState.infra_modes_[static_cast<size_t>(mode_key)];\n }\n \n void TorchDispatchModeTLS::set_mode(\n-    const std::shared_ptr<SafePyObject>& mode,\n+    const std::shared_ptr<PyObject_TorchDispatchMode>& mode,\n     TorchDispatchModeKey mode_key) {\n   TORCH_CHECK(\n       torchDispatchModeState.infra_modes_[static_cast<size_t>(mode_key)] ==\n"
                },
                {
                    "old_start": 143,
                    "old_length": 8,
                    "new_start": 145,
                    "new_length": 8,
                    "hunk": "@@ -143,8 +145,8 @@ void TorchDispatchModeTLS::set_mode(\n   torchDispatchModeState.infra_modes_[static_cast<size_t>(mode_key)] = mode;\n }\n \n-const c10::optional<std::shared_ptr<SafePyObject>> TorchDispatchModeTLS::\n-    unset_mode(TorchDispatchModeKey mode_key) {\n+const c10::optional<std::shared_ptr<PyObject_TorchDispatchMode>>\n+TorchDispatchModeTLS::unset_mode(TorchDispatchModeKey mode_key) {\n   auto out = torchDispatchModeState.infra_modes_[static_cast<size_t>(mode_key)];\n   torchDispatchModeState.infra_modes_[static_cast<size_t>(mode_key)] =\n       c10::nullopt;\n"
                }
            ],
            "whole_deleted": "-    std::shared_ptr<SafePyObject> mode) {\n-const std::shared_ptr<SafePyObject> TorchDispatchModeTLS::pop_stack() {\n-  std::shared_ptr<SafePyObject> out;\n-const std::tuple<std::shared_ptr<SafePyObject>, TorchDispatchModeKey>\n-TorchDispatchModeTLS::pop_highest_infra_mode() {\n-const std::shared_ptr<SafePyObject>& TorchDispatchModeTLS::get_stack_at(\n-    int64_t idx) {\n-const c10::optional<std::shared_ptr<SafePyObject>> TorchDispatchModeTLS::\n-    get_mode(TorchDispatchModeKey mode_key) {\n-    const std::shared_ptr<SafePyObject>& mode,\n-const c10::optional<std::shared_ptr<SafePyObject>> TorchDispatchModeTLS::\n-    unset_mode(TorchDispatchModeKey mode_key) {\n",
            "whole_added": "+    std::shared_ptr<PyObject_TorchDispatchMode> mode) {\n+const std::shared_ptr<PyObject_TorchDispatchMode> TorchDispatchModeTLS::\n+    pop_stack() {\n+  std::shared_ptr<PyObject_TorchDispatchMode> out;\n+const std::\n+    tuple<std::shared_ptr<PyObject_TorchDispatchMode>, TorchDispatchModeKey>\n+    TorchDispatchModeTLS::pop_highest_infra_mode() {\n+const std::shared_ptr<PyObject_TorchDispatchMode>& TorchDispatchModeTLS::\n+    get_stack_at(int64_t idx) {\n+const c10::optional<std::shared_ptr<PyObject_TorchDispatchMode>>\n+TorchDispatchModeTLS::get_mode(TorchDispatchModeKey mode_key) {\n+    const std::shared_ptr<PyObject_TorchDispatchMode>& mode,\n+const c10::optional<std::shared_ptr<PyObject_TorchDispatchMode>>\n+TorchDispatchModeTLS::unset_mode(TorchDispatchModeKey mode_key) {\n",
            "whole_hunk": "@@ -25,7 +25,7 @@ bool TorchDispatchModeTLS::any_modes_set(bool skip_infra_modes) {\n }\n \n void TorchDispatchModeTLS::push_non_infra_mode_onto_stack(\n-    std::shared_ptr<SafePyObject> mode) {\n+    std::shared_ptr<PyObject_TorchDispatchMode> mode) {\n   if (!any_modes_set()) {\n     c10::impl::tls_set_dispatch_key_included(DispatchKey::Python, true);\n     c10::impl::tls_set_dispatch_key_included(\n@@ -34,8 +34,9 @@ void TorchDispatchModeTLS::push_non_infra_mode_onto_stack(\n   torchDispatchModeState.stack_.push_back(std::move(mode));\n }\n \n-const std::shared_ptr<SafePyObject> TorchDispatchModeTLS::pop_stack() {\n-  std::shared_ptr<SafePyObject> out;\n+const std::shared_ptr<PyObject_TorchDispatchMode> TorchDispatchModeTLS::\n+    pop_stack() {\n+  std::shared_ptr<PyObject_TorchDispatchMode> out;\n   if (!torchDispatchModeState.stack_.empty()) {\n     out = torchDispatchModeState.stack_.back();\n     torchDispatchModeState.stack_.pop_back();\n@@ -60,8 +61,9 @@ const std::shared_ptr<SafePyObject> TorchDispatchModeTLS::pop_stack() {\n   }\n   return out;\n }\n-const std::tuple<std::shared_ptr<SafePyObject>, TorchDispatchModeKey>\n-TorchDispatchModeTLS::pop_highest_infra_mode() {\n+const std::\n+    tuple<std::shared_ptr<PyObject_TorchDispatchMode>, TorchDispatchModeKey>\n+    TorchDispatchModeTLS::pop_highest_infra_mode() {\n   for (int64_t i = static_cast<size_t>(TorchDispatchModeKey::NUM_MODE_KEYS) - 1;\n        i >= 0;\n        --i) {\n@@ -82,8 +84,8 @@ TorchDispatchModeTLS::pop_highest_infra_mode() {\n       false, \"Called pop_highest_infra_mode, but no infra modes were active.\")\n }\n \n-const std::shared_ptr<SafePyObject>& TorchDispatchModeTLS::get_stack_at(\n-    int64_t idx) {\n+const std::shared_ptr<PyObject_TorchDispatchMode>& TorchDispatchModeTLS::\n+    get_stack_at(int64_t idx) {\n   TORCH_CHECK(idx < stack_len(), \"Tried to get stack at idx that's too big\");\n   // Our \"logical\" stack includes both:\n   // - any user modes (the entire torchDispatchModeState.stack_)\n@@ -119,13 +121,13 @@ int64_t TorchDispatchModeTLS::stack_len() {\n   return stack_len + infra_modes_len;\n }\n \n-const c10::optional<std::shared_ptr<SafePyObject>> TorchDispatchModeTLS::\n-    get_mode(TorchDispatchModeKey mode_key) {\n+const c10::optional<std::shared_ptr<PyObject_TorchDispatchMode>>\n+TorchDispatchModeTLS::get_mode(TorchDispatchModeKey mode_key) {\n   return torchDispatchModeState.infra_modes_[static_cast<size_t>(mode_key)];\n }\n \n void TorchDispatchModeTLS::set_mode(\n-    const std::shared_ptr<SafePyObject>& mode,\n+    const std::shared_ptr<PyObject_TorchDispatchMode>& mode,\n     TorchDispatchModeKey mode_key) {\n   TORCH_CHECK(\n       torchDispatchModeState.infra_modes_[static_cast<size_t>(mode_key)] ==\n@@ -143,8 +145,8 @@ void TorchDispatchModeTLS::set_mode(\n   torchDispatchModeState.infra_modes_[static_cast<size_t>(mode_key)] = mode;\n }\n \n-const c10::optional<std::shared_ptr<SafePyObject>> TorchDispatchModeTLS::\n-    unset_mode(TorchDispatchModeKey mode_key) {\n+const c10::optional<std::shared_ptr<PyObject_TorchDispatchMode>>\n+TorchDispatchModeTLS::unset_mode(TorchDispatchModeKey mode_key) {\n   auto out = torchDispatchModeState.infra_modes_[static_cast<size_t>(mode_key)];\n   torchDispatchModeState.infra_modes_[static_cast<size_t>(mode_key)] =\n       c10::nullopt;\n"
        },
        {
            "name": "TorchDispatchModeTLS.h",
            "path": "c10/core/impl/TorchDispatchModeTLS.h",
            "patches": [
                {
                    "old_start": 12,
                    "old_length": 31,
                    "new_start": 12,
                    "new_length": 35,
                    "hunk": "@@ -12,31 +12,35 @@ enum class TorchDispatchModeKey : int8_t {\n   NUM_MODE_KEYS\n };\n \n+using PyObject_TorchDispatchMode = SafePyObjectT<TorchDispatchModeKey>;\n+\n struct C10_API TorchDispatchModeTLS {\n   // This API is NOT invariant safe.\n   // It must not take in an infra mode that uses TorchDispatchModeKey\n   // If you're pushing an infra mode onto the stack, we expect\n   // you to use set_mode\n   static void push_non_infra_mode_onto_stack(\n-      std::shared_ptr<SafePyObject> mode);\n+      std::shared_ptr<PyObject_TorchDispatchMode> mode);\n   // Pops the top mode of the stack,\n   // giving precedence to user modes before attempting to pop\n   // any infra modes\n-  static const std::shared_ptr<SafePyObject> pop_stack();\n+  static const std::shared_ptr<PyObject_TorchDispatchMode> pop_stack();\n   // Returns the highest-priority infra mode on the stack,\n   // along with its mode key.\n-  static const std::tuple<std::shared_ptr<SafePyObject>, TorchDispatchModeKey>\n-  pop_highest_infra_mode();\n+  static const std::\n+      tuple<std::shared_ptr<PyObject_TorchDispatchMode>, TorchDispatchModeKey>\n+      pop_highest_infra_mode();\n \n-  static const std::shared_ptr<SafePyObject>& get_stack_at(int64_t idx);\n+  static const std::shared_ptr<PyObject_TorchDispatchMode>& get_stack_at(\n+      int64_t idx);\n   static int64_t stack_len();\n \n-  static const c10::optional<std::shared_ptr<SafePyObject>> get_mode(\n-      TorchDispatchModeKey mode_key);\n-  static const c10::optional<std::shared_ptr<SafePyObject>> unset_mode(\n-      TorchDispatchModeKey mode_key);\n+  static const c10::optional<std::shared_ptr<PyObject_TorchDispatchMode>>\n+  get_mode(TorchDispatchModeKey mode_key);\n+  static const c10::optional<std::shared_ptr<PyObject_TorchDispatchMode>>\n+  unset_mode(TorchDispatchModeKey mode_key);\n   static void set_mode(\n-      const std::shared_ptr<SafePyObject>& mode,\n+      const std::shared_ptr<PyObject_TorchDispatchMode>& mode,\n       TorchDispatchModeKey mode_key);\n \n   static const TorchDispatchModeTLS& get_state();\n"
                },
                {
                    "old_start": 45,
                    "old_length": 13,
                    "new_start": 49,
                    "new_length": 13,
                    "hunk": "@@ -45,13 +49,13 @@ struct C10_API TorchDispatchModeTLS {\n   static bool any_modes_set(bool skip_infra_modes = false);\n \n  private:\n-  std::vector<std::shared_ptr<c10::SafePyObject>> stack_;\n+  std::vector<std::shared_ptr<PyObject_TorchDispatchMode>> stack_;\n   // Users are allowed to push multiple ProxyTorchDispatchMode objects onto the\n   // stack\n   // However, we only allow a single FakeTensorMode onto the stack at a time\n   // (Pushing additional FakeTensorModes onto the stack is a no-op)\n   std::array<\n-      c10::optional<std::shared_ptr<c10::SafePyObject>>,\n+      c10::optional<std::shared_ptr<PyObject_TorchDispatchMode>>,\n       static_cast<size_t>(TorchDispatchModeKey::NUM_MODE_KEYS)>\n       infra_modes_;\n };\n"
                }
            ],
            "whole_deleted": "-      std::shared_ptr<SafePyObject> mode);\n-  static const std::shared_ptr<SafePyObject> pop_stack();\n-  static const std::tuple<std::shared_ptr<SafePyObject>, TorchDispatchModeKey>\n-  pop_highest_infra_mode();\n-  static const std::shared_ptr<SafePyObject>& get_stack_at(int64_t idx);\n-  static const c10::optional<std::shared_ptr<SafePyObject>> get_mode(\n-      TorchDispatchModeKey mode_key);\n-  static const c10::optional<std::shared_ptr<SafePyObject>> unset_mode(\n-      TorchDispatchModeKey mode_key);\n-      const std::shared_ptr<SafePyObject>& mode,\n-  std::vector<std::shared_ptr<c10::SafePyObject>> stack_;\n-      c10::optional<std::shared_ptr<c10::SafePyObject>>,\n",
            "whole_added": "+using PyObject_TorchDispatchMode = SafePyObjectT<TorchDispatchModeKey>;\n+\n+      std::shared_ptr<PyObject_TorchDispatchMode> mode);\n+  static const std::shared_ptr<PyObject_TorchDispatchMode> pop_stack();\n+  static const std::\n+      tuple<std::shared_ptr<PyObject_TorchDispatchMode>, TorchDispatchModeKey>\n+      pop_highest_infra_mode();\n+  static const std::shared_ptr<PyObject_TorchDispatchMode>& get_stack_at(\n+      int64_t idx);\n+  static const c10::optional<std::shared_ptr<PyObject_TorchDispatchMode>>\n+  get_mode(TorchDispatchModeKey mode_key);\n+  static const c10::optional<std::shared_ptr<PyObject_TorchDispatchMode>>\n+  unset_mode(TorchDispatchModeKey mode_key);\n+      const std::shared_ptr<PyObject_TorchDispatchMode>& mode,\n+  std::vector<std::shared_ptr<PyObject_TorchDispatchMode>> stack_;\n+      c10::optional<std::shared_ptr<PyObject_TorchDispatchMode>>,\n",
            "whole_hunk": "@@ -12,31 +12,35 @@ enum class TorchDispatchModeKey : int8_t {\n   NUM_MODE_KEYS\n };\n \n+using PyObject_TorchDispatchMode = SafePyObjectT<TorchDispatchModeKey>;\n+\n struct C10_API TorchDispatchModeTLS {\n   // This API is NOT invariant safe.\n   // It must not take in an infra mode that uses TorchDispatchModeKey\n   // If you're pushing an infra mode onto the stack, we expect\n   // you to use set_mode\n   static void push_non_infra_mode_onto_stack(\n-      std::shared_ptr<SafePyObject> mode);\n+      std::shared_ptr<PyObject_TorchDispatchMode> mode);\n   // Pops the top mode of the stack,\n   // giving precedence to user modes before attempting to pop\n   // any infra modes\n-  static const std::shared_ptr<SafePyObject> pop_stack();\n+  static const std::shared_ptr<PyObject_TorchDispatchMode> pop_stack();\n   // Returns the highest-priority infra mode on the stack,\n   // along with its mode key.\n-  static const std::tuple<std::shared_ptr<SafePyObject>, TorchDispatchModeKey>\n-  pop_highest_infra_mode();\n+  static const std::\n+      tuple<std::shared_ptr<PyObject_TorchDispatchMode>, TorchDispatchModeKey>\n+      pop_highest_infra_mode();\n \n-  static const std::shared_ptr<SafePyObject>& get_stack_at(int64_t idx);\n+  static const std::shared_ptr<PyObject_TorchDispatchMode>& get_stack_at(\n+      int64_t idx);\n   static int64_t stack_len();\n \n-  static const c10::optional<std::shared_ptr<SafePyObject>> get_mode(\n-      TorchDispatchModeKey mode_key);\n-  static const c10::optional<std::shared_ptr<SafePyObject>> unset_mode(\n-      TorchDispatchModeKey mode_key);\n+  static const c10::optional<std::shared_ptr<PyObject_TorchDispatchMode>>\n+  get_mode(TorchDispatchModeKey mode_key);\n+  static const c10::optional<std::shared_ptr<PyObject_TorchDispatchMode>>\n+  unset_mode(TorchDispatchModeKey mode_key);\n   static void set_mode(\n-      const std::shared_ptr<SafePyObject>& mode,\n+      const std::shared_ptr<PyObject_TorchDispatchMode>& mode,\n       TorchDispatchModeKey mode_key);\n \n   static const TorchDispatchModeTLS& get_state();\n@@ -45,13 +49,13 @@ struct C10_API TorchDispatchModeTLS {\n   static bool any_modes_set(bool skip_infra_modes = false);\n \n  private:\n-  std::vector<std::shared_ptr<c10::SafePyObject>> stack_;\n+  std::vector<std::shared_ptr<PyObject_TorchDispatchMode>> stack_;\n   // Users are allowed to push multiple ProxyTorchDispatchMode objects onto the\n   // stack\n   // However, we only allow a single FakeTensorMode onto the stack at a time\n   // (Pushing additional FakeTensorModes onto the stack is a no-op)\n   std::array<\n-      c10::optional<std::shared_ptr<c10::SafePyObject>>,\n+      c10::optional<std::shared_ptr<PyObject_TorchDispatchMode>>,\n       static_cast<size_t>(TorchDispatchModeKey::NUM_MODE_KEYS)>\n       infra_modes_;\n };\n"
        },
        {
            "name": "__init__.pyi.in",
            "path": "torch/_C/__init__.pyi.in",
            "patches": [
                {
                    "old_start": 55,
                    "old_length": 6,
                    "new_start": 55,
                    "new_length": 7,
                    "hunk": "@@ -55,6 +55,7 @@ from torch.types import (\n )\n \n from torch._prims_common import DeviceLikeType\n+from torch.utils._python_dispatch import TorchDispatchMode\n \n # This module is defined in torch/csrc/Module.cpp\n \n"
                },
                {
                    "old_start": 1332,
                    "old_length": 11,
                    "new_start": 1333,
                    "new_length": 11,
                    "hunk": "@@ -1332,11 +1333,11 @@ def _pop_torch_function_stack() -> Any: ...\n def _get_function_stack_at(idx: _int) -> Any: ...\n def _len_torch_function_stack() -> _int: ...\n def _set_torch_dispatch_mode(cls: Any) -> None: ...\n-def _push_on_torch_dispatch_stack(cls: Any) -> None: ...\n+def _push_on_torch_dispatch_stack(cls: TorchDispatchMode) -> None: ...\n def _pop_torch_dispatch_stack(mode_key: Optional[torch._C._TorchDispatchModeKey] = None) -> Any: ...\n def _get_dispatch_mode(mode_key: Optional[torch._C._TorchDispatchModeKey]) -> Any: ...\n-def _unset_dispatch_mode(mode: torch._C._TorchDispatchModeKey) -> Any: ...\n-def _set_dispatch_mode(mode: Any) -> None: ...\n+def _unset_dispatch_mode(mode: torch._C._TorchDispatchModeKey) -> Optional[TorchDispatchMode]: ...\n+def _set_dispatch_mode(mode: TorchDispatchMode) -> None: ...\n def _get_dispatch_stack_at(idx: _int) -> Any: ...\n def _len_torch_dispatch_stack() -> _int: ...\n def _activate_gpu_trace() -> None: ...\n"
                },
                {
                    "old_start": 1549,
                    "old_length": 6,
                    "new_start": 1550,
                    "new_length": 8,
                    "hunk": "@@ -1549,6 +1550,8 @@ def _dispatch_pystub(name: str, overload: str) -> Optional[Tuple[str, str]]: ...\n def _dispatch_is_alias_key(dispatch: _dispatchkey) -> _bool: ...\n def _functionality_to_backend_keys(dispatch: _dispatchkey) -> List[DispatchKey]: ...\n def _functionalization_reapply_views_tls() -> _bool: ...\n+def _only_lift_cpu_tensors() -> _bool: ...\n+def _set_only_lift_cpu_tensors(value: _bool) -> None: ...\n def _set_throw_on_mutable_data_ptr(tensor: Tensor) -> None: ...\n def _set_warn_deprecated_on_mutable_data_ptr(tensor: Tensor) -> None: ...\n \n"
                },
                {
                    "old_start": 2260,
                    "old_length": 6,
                    "new_start": 2263,
                    "new_length": 7,
                    "hunk": "@@ -2260,6 +2263,7 @@ def _register_py_class_for_device(device: str, cls: Any) -> None: ...\n # Defined in torch/csrc/Module.cpp\n def _current_graph_task_id() -> _int: ...\n def _current_autograd_node() -> _Node: ...\n+def _dispatch_key_set(Tensor) -> str: ...\n \n # Defined in torch/csrc/Exceptions.cpp\n class OutOfMemoryError(RuntimeError): ...\n"
                }
            ],
            "whole_deleted": "-def _push_on_torch_dispatch_stack(cls: Any) -> None: ...\n-def _unset_dispatch_mode(mode: torch._C._TorchDispatchModeKey) -> Any: ...\n-def _set_dispatch_mode(mode: Any) -> None: ...\n",
            "whole_added": "+from torch.utils._python_dispatch import TorchDispatchMode\n+def _push_on_torch_dispatch_stack(cls: TorchDispatchMode) -> None: ...\n+def _unset_dispatch_mode(mode: torch._C._TorchDispatchModeKey) -> Optional[TorchDispatchMode]: ...\n+def _set_dispatch_mode(mode: TorchDispatchMode) -> None: ...\n+def _only_lift_cpu_tensors() -> _bool: ...\n+def _set_only_lift_cpu_tensors(value: _bool) -> None: ...\n+def _dispatch_key_set(Tensor) -> str: ...\n",
            "whole_hunk": "@@ -55,6 +55,7 @@ from torch.types import (\n )\n \n from torch._prims_common import DeviceLikeType\n+from torch.utils._python_dispatch import TorchDispatchMode\n \n # This module is defined in torch/csrc/Module.cpp\n \n@@ -1332,11 +1333,11 @@ def _pop_torch_function_stack() -> Any: ...\n def _get_function_stack_at(idx: _int) -> Any: ...\n def _len_torch_function_stack() -> _int: ...\n def _set_torch_dispatch_mode(cls: Any) -> None: ...\n-def _push_on_torch_dispatch_stack(cls: Any) -> None: ...\n+def _push_on_torch_dispatch_stack(cls: TorchDispatchMode) -> None: ...\n def _pop_torch_dispatch_stack(mode_key: Optional[torch._C._TorchDispatchModeKey] = None) -> Any: ...\n def _get_dispatch_mode(mode_key: Optional[torch._C._TorchDispatchModeKey]) -> Any: ...\n-def _unset_dispatch_mode(mode: torch._C._TorchDispatchModeKey) -> Any: ...\n-def _set_dispatch_mode(mode: Any) -> None: ...\n+def _unset_dispatch_mode(mode: torch._C._TorchDispatchModeKey) -> Optional[TorchDispatchMode]: ...\n+def _set_dispatch_mode(mode: TorchDispatchMode) -> None: ...\n def _get_dispatch_stack_at(idx: _int) -> Any: ...\n def _len_torch_dispatch_stack() -> _int: ...\n def _activate_gpu_trace() -> None: ...\n@@ -1549,6 +1550,8 @@ def _dispatch_pystub(name: str, overload: str) -> Optional[Tuple[str, str]]: ...\n def _dispatch_is_alias_key(dispatch: _dispatchkey) -> _bool: ...\n def _functionality_to_backend_keys(dispatch: _dispatchkey) -> List[DispatchKey]: ...\n def _functionalization_reapply_views_tls() -> _bool: ...\n+def _only_lift_cpu_tensors() -> _bool: ...\n+def _set_only_lift_cpu_tensors(value: _bool) -> None: ...\n def _set_throw_on_mutable_data_ptr(tensor: Tensor) -> None: ...\n def _set_warn_deprecated_on_mutable_data_ptr(tensor: Tensor) -> None: ...\n \n@@ -2260,6 +2263,7 @@ def _register_py_class_for_device(device: str, cls: Any) -> None: ...\n # Defined in torch/csrc/Module.cpp\n def _current_graph_task_id() -> _int: ...\n def _current_autograd_node() -> _Node: ...\n+def _dispatch_key_set(Tensor) -> str: ...\n \n # Defined in torch/csrc/Exceptions.cpp\n class OutOfMemoryError(RuntimeError): ...\n"
        },
        {
            "name": "_ops.py",
            "path": "torch/_ops.py",
            "patches": [
                {
                    "old_start": 1238,
                    "old_length": 4,
                    "new_start": 1238,
                    "new_length": 4,
                    "hunk": "@@ -1238,4 +1238,4 @@ class _Ops(types.ModuleType):\n \n \n # The ops \"namespace\"\n-ops = _Ops()\n+ops: _Ops = _Ops()\n"
                }
            ],
            "whole_deleted": "-ops = _Ops()\n",
            "whole_added": "+ops: _Ops = _Ops()\n",
            "whole_hunk": "@@ -1238,4 +1238,4 @@ class _Ops(types.ModuleType):\n \n \n # The ops \"namespace\"\n-ops = _Ops()\n+ops: _Ops = _Ops()\n"
        },
        {
            "name": "fake_tensor.py",
            "path": "torch/_subclasses/fake_tensor.py",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 5,
                    "new_start": 1,
                    "new_length": 3,
                    "hunk": "@@ -1,5 +1,3 @@\n-# mypy: ignore-errors\n-\n import contextlib\n import functools\n import logging\n"
                },
                {
                    "old_start": 8,
                    "old_length": 7,
                    "new_start": 6,
                    "new_length": 18,
                    "hunk": "@@ -8,7 +6,18 @@ import traceback\n import weakref\n from collections import defaultdict\n from dataclasses import dataclass\n-from typing import Any, Dict, List, Optional, Tuple, Type, TYPE_CHECKING, TypeVar\n+from typing import (\n+    Any,\n+    cast,\n+    Dict,\n+    List,\n+    Optional,\n+    Tuple,\n+    Type,\n+    TYPE_CHECKING,\n+    TypeVar,\n+    Union,\n+)\n from weakref import ReferenceType\n \n import torch\n"
                },
                {
                    "old_start": 30,
                    "old_length": 6,
                    "new_start": 39,
                    "new_length": 7,
                    "hunk": "@@ -30,6 +39,7 @@ from torch._utils import render_call\n from torch.fx.operator_schemas import normalize_function\n from torch.multiprocessing.reductions import StorageWeakRef\n from torch.overrides import TorchFunctionMode\n+from torch.types import _bool\n from torch.utils._mode_utils import no_dispatch\n from torch.utils._python_dispatch import (\n     is_traceable_wrapper_subclass,\n"
                },
                {
                    "old_start": 42,
                    "old_length": 6,
                    "new_start": 52,
                    "new_length": 13,
                    "hunk": "@@ -42,6 +52,13 @@ from torch.utils._traceback import CapturedTraceback\n if TYPE_CHECKING:\n     from torch.fx.experimental.symbolic_shapes import ShapeEnv\n \n+\n+class _Unassigned:\n+    pass\n+\n+\n+_UNASSIGNED = _Unassigned()\n+\n DimList = List\n \n log = logging.getLogger(__name__)\n"
                },
                {
                    "old_start": 718,
                    "old_length": 7,
                    "new_start": 735,
                    "new_length": 7,
                    "hunk": "@@ -718,7 +735,7 @@ def extract_tensor_metadata(t: torch.Tensor) -> \"TensorMetadata\":\n     \"\"\"\n     Extract the TensorMetadata of a tensor.\n     \"\"\"\n-    memory_format = suggest_memory_format(t)\n+    memory_format: Optional[torch.memory_format] = suggest_memory_format(t)\n     if is_sparse_any(t) or not t.is_contiguous(memory_format=memory_format):\n         memory_format = None\n \n"
                },
                {
                    "old_start": 806,
                    "old_length": 10,
                    "new_start": 823,
                    "new_length": 11,
                    "hunk": "@@ -806,10 +823,11 @@ class FakeTensorMode(TorchDispatchMode):\n     cache: Dict[_DispatchCacheKey, _DispatchCacheEntry] = {}\n     cache_hits: int = 0\n     cache_misses: int = 0\n-    cache_bypasses = defaultdict(int)\n+    cache_bypasses: Dict[str, int] = defaultdict(int)\n     # Every time you retrace using the same fake tensor mode, you should\n     # advance the epoch so we don't reuse unbacked memos\n     epoch: int = 0\n+    in_kernel_invocation: bool = False\n \n     def __init__(\n         self,\n"
                },
                {
                    "old_start": 860,
                    "old_length": 7,
                    "new_start": 878,
                    "new_length": 9,
                    "hunk": "@@ -860,7 +878,9 @@ class FakeTensorMode(TorchDispatchMode):\n         # in_kernel_invocation\n         # If another fake mode was already active when we enter, we also stash it here.\n         # That way when we exit, we know to re-enable the previous fake mode.\n-        self.enter_stack: List[Tuple[bool, Optional[FakeTensorMode]]] = []\n+        self.enter_stack: List[\n+            Tuple[bool, Optional[TorchDispatchMode], Optional[_bool]]\n+        ] = []\n \n         self.shape_env: ShapeEnv = shape_env\n \n"
                },
                {
                    "old_start": 972,
                    "old_length": 7,
                    "new_start": 992,
                    "new_length": 7,
                    "hunk": "@@ -972,7 +992,7 @@ class FakeTensorMode(TorchDispatchMode):\n         Lookup a cache entry for the given arguments. If none exists, dispatch\n         and cache the result (if the result is eligible for caching).\n         \"\"\"\n-        output = unassigned = object()\n+        output: Union[FakeTensor, _Unassigned] = _UNASSIGNED\n         try:\n             key = self._cache_key(func, args, kwargs)\n             entry = FakeTensorMode.cache.get(key, None)\n"
                },
                {
                    "old_start": 991,
                    "old_length": 7,
                    "new_start": 1011,
                    "new_length": 7,
                    "hunk": "@@ -991,7 +1011,7 @@ class FakeTensorMode(TorchDispatchMode):\n         except _BypassDispatchCache as e:\n             FakeTensorMode.cache_bypasses[e.reason] += 1\n \n-        if output is unassigned:\n+        if output is _UNASSIGNED:\n             output = self._dispatch_impl(func, types, args, kwargs)\n \n         return output\n"
                },
                {
                    "old_start": 1066,
                    "old_length": 7,
                    "new_start": 1086,
                    "new_length": 7,
                    "hunk": "@@ -1066,7 +1086,7 @@ class FakeTensorMode(TorchDispatchMode):\n         if isinstance(args, dict):\n             args = list(args.keys()) + list(args.values())\n \n-        result = []\n+        result: List[Any] = []\n         for arg in args:\n             if isinstance(arg, FakeTensor):\n                 if not self.is_our_fake(arg):\n"
                },
                {
                    "old_start": 1177,
                    "old_length": 7,
                    "new_start": 1197,
                    "new_length": 7,
                    "hunk": "@@ -1177,7 +1197,7 @@ class FakeTensorMode(TorchDispatchMode):\n \n         # Synthesize a new FakeTensor with the cached metadata.\n         metadata = entry.metadata\n-        assert not metadata.is_sparse\n+        assert metadata and not metadata.is_sparse\n \n         empty = torch.empty_strided(\n             metadata.shape,\n"
                },
                {
                    "old_start": 1195,
                    "old_length": 7,
                    "new_start": 1215,
                    "new_length": 7,
                    "hunk": "@@ -1195,7 +1215,7 @@ class FakeTensorMode(TorchDispatchMode):\n \n         if func.is_view:\n             # For view ops, the storage should be the same as the tensor input.\n-            storage = args[entry.view_idx].untyped_storage()\n+            storage = args[cast(int, entry.view_idx)].untyped_storage()\n             with in_kernel_invocation_manager(self):\n                 empty.set_(\n                     storage, metadata.storage_offset, metadata.shape, metadata.stride\n"
                },
                {
                    "old_start": 1263,
                    "old_length": 7,
                    "new_start": 1283,
                    "new_length": 7,
                    "hunk": "@@ -1263,7 +1283,7 @@ class FakeTensorMode(TorchDispatchMode):\n         else:\n             return self._dispatch_impl(func, types, args, kwargs)\n \n-    def _dispatch_impl(self, func, types, args, kwargs):\n+    def _dispatch_impl(self, func, types, args, kwargs) -> FakeTensor:\n         flat_args, args_spec = pytree.tree_flatten((args, kwargs))\n \n         flat_arg_fake_tensors = [\n"
                },
                {
                    "old_start": 1557,
                    "old_length": 7,
                    "new_start": 1577,
                    "new_length": 7,
                    "hunk": "@@ -1557,7 +1577,7 @@ class FakeTensorMode(TorchDispatchMode):\n         If not, try to convert them to fake tensors.\n         Returns the original args, kwargs, and a flattened list of (args, kwargs) that are fake tensors.\n         \"\"\"\n-        flat_arg_fake_tensors = []\n+        flat_arg_fake_tensors: List[Any] = []\n \n         def validate(x):\n             if not isinstance(x, torch.Tensor):\n"
                },
                {
                    "old_start": 1684,
                    "old_length": 7,
                    "new_start": 1704,
                    "new_length": 7,
                    "hunk": "@@ -1684,7 +1704,7 @@ class FakeTensorMode(TorchDispatchMode):\n         source: Optional[Source] = None,\n         symbolic_context=None,\n     ):\n-        shape_env = self.shape_env\n+        shape_env: Optional[ShapeEnv] = self.shape_env\n         if static_shapes is None:\n             static_shapes = self.static_shapes\n         if static_shapes:\n"
                }
            ],
            "whole_deleted": "-# mypy: ignore-errors\n-\n-from typing import Any, Dict, List, Optional, Tuple, Type, TYPE_CHECKING, TypeVar\n-    memory_format = suggest_memory_format(t)\n-    cache_bypasses = defaultdict(int)\n-        self.enter_stack: List[Tuple[bool, Optional[FakeTensorMode]]] = []\n-        output = unassigned = object()\n-        if output is unassigned:\n-        result = []\n-        assert not metadata.is_sparse\n-            storage = args[entry.view_idx].untyped_storage()\n-    def _dispatch_impl(self, func, types, args, kwargs):\n-        flat_arg_fake_tensors = []\n-        shape_env = self.shape_env\n",
            "whole_added": "+from typing import (\n+    Any,\n+    cast,\n+    Dict,\n+    List,\n+    Optional,\n+    Tuple,\n+    Type,\n+    TYPE_CHECKING,\n+    TypeVar,\n+    Union,\n+)\n+from torch.types import _bool\n+\n+class _Unassigned:\n+    pass\n+\n+\n+_UNASSIGNED = _Unassigned()\n+\n+    memory_format: Optional[torch.memory_format] = suggest_memory_format(t)\n+    cache_bypasses: Dict[str, int] = defaultdict(int)\n+    in_kernel_invocation: bool = False\n+        self.enter_stack: List[\n+            Tuple[bool, Optional[TorchDispatchMode], Optional[_bool]]\n+        ] = []\n+        output: Union[FakeTensor, _Unassigned] = _UNASSIGNED\n+        if output is _UNASSIGNED:\n+        result: List[Any] = []\n+        assert metadata and not metadata.is_sparse\n+            storage = args[cast(int, entry.view_idx)].untyped_storage()\n+    def _dispatch_impl(self, func, types, args, kwargs) -> FakeTensor:\n+        flat_arg_fake_tensors: List[Any] = []\n+        shape_env: Optional[ShapeEnv] = self.shape_env\n",
            "whole_hunk": "@@ -1,5 +1,3 @@\n-# mypy: ignore-errors\n-\n import contextlib\n import functools\n import logging\n@@ -8,7 +6,18 @@ import traceback\n import weakref\n from collections import defaultdict\n from dataclasses import dataclass\n-from typing import Any, Dict, List, Optional, Tuple, Type, TYPE_CHECKING, TypeVar\n+from typing import (\n+    Any,\n+    cast,\n+    Dict,\n+    List,\n+    Optional,\n+    Tuple,\n+    Type,\n+    TYPE_CHECKING,\n+    TypeVar,\n+    Union,\n+)\n from weakref import ReferenceType\n \n import torch\n@@ -30,6 +39,7 @@ from torch._utils import render_call\n from torch.fx.operator_schemas import normalize_function\n from torch.multiprocessing.reductions import StorageWeakRef\n from torch.overrides import TorchFunctionMode\n+from torch.types import _bool\n from torch.utils._mode_utils import no_dispatch\n from torch.utils._python_dispatch import (\n     is_traceable_wrapper_subclass,\n@@ -42,6 +52,13 @@ from torch.utils._traceback import CapturedTraceback\n if TYPE_CHECKING:\n     from torch.fx.experimental.symbolic_shapes import ShapeEnv\n \n+\n+class _Unassigned:\n+    pass\n+\n+\n+_UNASSIGNED = _Unassigned()\n+\n DimList = List\n \n log = logging.getLogger(__name__)\n@@ -718,7 +735,7 @@ def extract_tensor_metadata(t: torch.Tensor) -> \"TensorMetadata\":\n     \"\"\"\n     Extract the TensorMetadata of a tensor.\n     \"\"\"\n-    memory_format = suggest_memory_format(t)\n+    memory_format: Optional[torch.memory_format] = suggest_memory_format(t)\n     if is_sparse_any(t) or not t.is_contiguous(memory_format=memory_format):\n         memory_format = None\n \n@@ -806,10 +823,11 @@ class FakeTensorMode(TorchDispatchMode):\n     cache: Dict[_DispatchCacheKey, _DispatchCacheEntry] = {}\n     cache_hits: int = 0\n     cache_misses: int = 0\n-    cache_bypasses = defaultdict(int)\n+    cache_bypasses: Dict[str, int] = defaultdict(int)\n     # Every time you retrace using the same fake tensor mode, you should\n     # advance the epoch so we don't reuse unbacked memos\n     epoch: int = 0\n+    in_kernel_invocation: bool = False\n \n     def __init__(\n         self,\n@@ -860,7 +878,9 @@ class FakeTensorMode(TorchDispatchMode):\n         # in_kernel_invocation\n         # If another fake mode was already active when we enter, we also stash it here.\n         # That way when we exit, we know to re-enable the previous fake mode.\n-        self.enter_stack: List[Tuple[bool, Optional[FakeTensorMode]]] = []\n+        self.enter_stack: List[\n+            Tuple[bool, Optional[TorchDispatchMode], Optional[_bool]]\n+        ] = []\n \n         self.shape_env: ShapeEnv = shape_env\n \n@@ -972,7 +992,7 @@ class FakeTensorMode(TorchDispatchMode):\n         Lookup a cache entry for the given arguments. If none exists, dispatch\n         and cache the result (if the result is eligible for caching).\n         \"\"\"\n-        output = unassigned = object()\n+        output: Union[FakeTensor, _Unassigned] = _UNASSIGNED\n         try:\n             key = self._cache_key(func, args, kwargs)\n             entry = FakeTensorMode.cache.get(key, None)\n@@ -991,7 +1011,7 @@ class FakeTensorMode(TorchDispatchMode):\n         except _BypassDispatchCache as e:\n             FakeTensorMode.cache_bypasses[e.reason] += 1\n \n-        if output is unassigned:\n+        if output is _UNASSIGNED:\n             output = self._dispatch_impl(func, types, args, kwargs)\n \n         return output\n@@ -1066,7 +1086,7 @@ class FakeTensorMode(TorchDispatchMode):\n         if isinstance(args, dict):\n             args = list(args.keys()) + list(args.values())\n \n-        result = []\n+        result: List[Any] = []\n         for arg in args:\n             if isinstance(arg, FakeTensor):\n                 if not self.is_our_fake(arg):\n@@ -1177,7 +1197,7 @@ class FakeTensorMode(TorchDispatchMode):\n \n         # Synthesize a new FakeTensor with the cached metadata.\n         metadata = entry.metadata\n-        assert not metadata.is_sparse\n+        assert metadata and not metadata.is_sparse\n \n         empty = torch.empty_strided(\n             metadata.shape,\n@@ -1195,7 +1215,7 @@ class FakeTensorMode(TorchDispatchMode):\n \n         if func.is_view:\n             # For view ops, the storage should be the same as the tensor input.\n-            storage = args[entry.view_idx].untyped_storage()\n+            storage = args[cast(int, entry.view_idx)].untyped_storage()\n             with in_kernel_invocation_manager(self):\n                 empty.set_(\n                     storage, metadata.storage_offset, metadata.shape, metadata.stride\n@@ -1263,7 +1283,7 @@ class FakeTensorMode(TorchDispatchMode):\n         else:\n             return self._dispatch_impl(func, types, args, kwargs)\n \n-    def _dispatch_impl(self, func, types, args, kwargs):\n+    def _dispatch_impl(self, func, types, args, kwargs) -> FakeTensor:\n         flat_args, args_spec = pytree.tree_flatten((args, kwargs))\n \n         flat_arg_fake_tensors = [\n@@ -1557,7 +1577,7 @@ class FakeTensorMode(TorchDispatchMode):\n         If not, try to convert them to fake tensors.\n         Returns the original args, kwargs, and a flattened list of (args, kwargs) that are fake tensors.\n         \"\"\"\n-        flat_arg_fake_tensors = []\n+        flat_arg_fake_tensors: List[Any] = []\n \n         def validate(x):\n             if not isinstance(x, torch.Tensor):\n@@ -1684,7 +1704,7 @@ class FakeTensorMode(TorchDispatchMode):\n         source: Optional[Source] = None,\n         symbolic_context=None,\n     ):\n-        shape_env = self.shape_env\n+        shape_env: Optional[ShapeEnv] = self.shape_env\n         if static_shapes is None:\n             static_shapes = self.static_shapes\n         if static_shapes:\n"
        },
        {
            "name": "init.cpp",
            "path": "torch/csrc/autograd/init.cpp",
            "patches": [
                {
                    "old_start": 1097,
                    "old_length": 11,
                    "new_start": 1097,
                    "new_length": 13,
                    "hunk": "@@ -1097,11 +1097,13 @@ static PyObject* push_on_torch_dispatch_stack(\n     if (maybe_mode_key_obj) {\n       mode_key = py::cast<c10::impl::TorchDispatchModeKey>(maybe_mode_key_obj);\n       c10::impl::TorchDispatchModeTLS::set_mode(\n-          std::make_shared<c10::SafePyObject>(arg, getPyInterpreter()),\n+          std::make_shared<c10::impl::PyObject_TorchDispatchMode>(\n+              arg, getPyInterpreter()),\n           mode_key.value());\n     } else {\n       c10::impl::TorchDispatchModeTLS::push_non_infra_mode_onto_stack(\n-          std::make_shared<c10::SafePyObject>(arg, getPyInterpreter()));\n+          std::make_shared<c10::impl::PyObject_TorchDispatchMode>(\n+              arg, getPyInterpreter()));\n     }\n     Py_INCREF(arg);\n   }\n"
                },
                {
                    "old_start": 1165,
                    "old_length": 7,
                    "new_start": 1167,
                    "new_length": 9,
                    "hunk": "@@ -1165,7 +1167,9 @@ static PyObject* set_dispatch_mode(PyObject* _unused, PyObject* mode) {\n \n   Py_INCREF(mode);\n   c10::impl::TorchDispatchModeTLS::set_mode(\n-      std::make_shared<c10::SafePyObject>(mode, getPyInterpreter()), mode_key);\n+      std::make_shared<c10::impl::PyObject_TorchDispatchMode>(\n+          mode, getPyInterpreter()),\n+      mode_key);\n \n   Py_RETURN_NONE;\n   END_HANDLE_TH_ERRORS\n"
                }
            ],
            "whole_deleted": "-          std::make_shared<c10::SafePyObject>(arg, getPyInterpreter()),\n-          std::make_shared<c10::SafePyObject>(arg, getPyInterpreter()));\n-      std::make_shared<c10::SafePyObject>(mode, getPyInterpreter()), mode_key);\n",
            "whole_added": "+          std::make_shared<c10::impl::PyObject_TorchDispatchMode>(\n+              arg, getPyInterpreter()),\n+          std::make_shared<c10::impl::PyObject_TorchDispatchMode>(\n+              arg, getPyInterpreter()));\n+      std::make_shared<c10::impl::PyObject_TorchDispatchMode>(\n+          mode, getPyInterpreter()),\n+      mode_key);\n",
            "whole_hunk": "@@ -1097,11 +1097,13 @@ static PyObject* push_on_torch_dispatch_stack(\n     if (maybe_mode_key_obj) {\n       mode_key = py::cast<c10::impl::TorchDispatchModeKey>(maybe_mode_key_obj);\n       c10::impl::TorchDispatchModeTLS::set_mode(\n-          std::make_shared<c10::SafePyObject>(arg, getPyInterpreter()),\n+          std::make_shared<c10::impl::PyObject_TorchDispatchMode>(\n+              arg, getPyInterpreter()),\n           mode_key.value());\n     } else {\n       c10::impl::TorchDispatchModeTLS::push_non_infra_mode_onto_stack(\n-          std::make_shared<c10::SafePyObject>(arg, getPyInterpreter()));\n+          std::make_shared<c10::impl::PyObject_TorchDispatchMode>(\n+              arg, getPyInterpreter()));\n     }\n     Py_INCREF(arg);\n   }\n@@ -1165,7 +1167,9 @@ static PyObject* set_dispatch_mode(PyObject* _unused, PyObject* mode) {\n \n   Py_INCREF(mode);\n   c10::impl::TorchDispatchModeTLS::set_mode(\n-      std::make_shared<c10::SafePyObject>(mode, getPyInterpreter()), mode_key);\n+      std::make_shared<c10::impl::PyObject_TorchDispatchMode>(\n+          mode, getPyInterpreter()),\n+      mode_key);\n \n   Py_RETURN_NONE;\n   END_HANDLE_TH_ERRORS\n"
        },
        {
            "name": "torch_dispatch_mode.h",
            "path": "torch/csrc/utils/torch_dispatch_mode.h",
            "patches": [
                {
                    "old_start": 29,
                    "old_length": 12,
                    "new_start": 29,
                    "new_length": 12,
                    "hunk": "@@ -29,12 +29,12 @@ struct StashTorchDispatchModeGuard {\n     }\n   }\n \n-  const std::shared_ptr<c10::SafePyObject>& get_cur_mode() {\n+  const std::shared_ptr<c10::impl::PyObject_TorchDispatchMode>& get_cur_mode() {\n     return saved_mode_;\n   }\n \n  private:\n-  std::shared_ptr<at::SafePyObject> saved_mode_;\n+  std::shared_ptr<c10::impl::PyObject_TorchDispatchMode> saved_mode_;\n   c10::optional<c10::impl::TorchDispatchModeKey> saved_mode_key_;\n };\n \n"
                }
            ],
            "whole_deleted": "-  const std::shared_ptr<c10::SafePyObject>& get_cur_mode() {\n-  std::shared_ptr<at::SafePyObject> saved_mode_;\n",
            "whole_added": "+  const std::shared_ptr<c10::impl::PyObject_TorchDispatchMode>& get_cur_mode() {\n+  std::shared_ptr<c10::impl::PyObject_TorchDispatchMode> saved_mode_;\n",
            "whole_hunk": "@@ -29,12 +29,12 @@ struct StashTorchDispatchModeGuard {\n     }\n   }\n \n-  const std::shared_ptr<c10::SafePyObject>& get_cur_mode() {\n+  const std::shared_ptr<c10::impl::PyObject_TorchDispatchMode>& get_cur_mode() {\n     return saved_mode_;\n   }\n \n  private:\n-  std::shared_ptr<at::SafePyObject> saved_mode_;\n+  std::shared_ptr<c10::impl::PyObject_TorchDispatchMode> saved_mode_;\n   c10::optional<c10::impl::TorchDispatchModeKey> saved_mode_key_;\n };\n \n"
        },
        {
            "name": "_python_dispatch.py",
            "path": "torch/utils/_python_dispatch.py",
            "patches": [
                {
                    "old_start": 159,
                    "old_length": 7,
                    "new_start": 159,
                    "new_length": 7,
                    "hunk": "@@ -159,7 +159,7 @@ def _get_current_dispatch_mode_stack():\n     return [_get_dispatch_stack_at(i) for i in range(stack_len)]\n \n \n-def _push_mode(mode):\n+def _push_mode(mode: TorchDispatchMode):\n     k = mode._dispatch_key if hasattr(mode, \"_dispatch_key\") else None\n     assert k is None or k == torch._C.DispatchKey.PreDispatch\n     if k is None:"
                }
            ],
            "whole_deleted": "-def _push_mode(mode):\n",
            "whole_added": "+def _push_mode(mode: TorchDispatchMode):\n",
            "whole_hunk": "@@ -159,7 +159,7 @@ def _get_current_dispatch_mode_stack():\n     return [_get_dispatch_stack_at(i) for i in range(stack_len)]\n \n \n-def _push_mode(mode):\n+def _push_mode(mode: TorchDispatchMode):\n     k = mode._dispatch_key if hasattr(mode, \"_dispatch_key\") else None\n     assert k is None or k == torch._C.DispatchKey.PreDispatch\n     if k is None:"
        }
    ]
},
{
    "Id": 289,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/4240304da4ddc42335b0219bae11f072ca240fe5",
    "date": "2024-02-14T03:09:09+00:00",
    "message": "[TorchElastic] Handle SystemExit with code == 0 (#119697)\n\nSummary:\nFix for a case where --run-path option fails to exit if the script exits with non-error status code.\nWhen there is an error exit code, run-path correctly detects an error and fails when calling spawn.join(). However for-non error case, current behavior is to check the return value of the operation and the fix is to return None so that our MP code detects an exit.\n\nTest Plan:\ncat /tmp/script.py\n~~~\nimport sys\ndef main():\n    exit_code = 1\n    if len(sys.argv) > 1:\n        exit_code = int(sys.argv[1])\n    sys.exit(exit_code)\n\nif __name__==\"__main__\":\n    main()\n~~~\n\nCase of exit code with 0 (prior behavior - never exits):\ntorchrun --run-path /tmp/script.py 0\n\n~~~\n[2024-02-12 09:20:57,523] torch.distributed.elastic.multiprocessing.redirects: [WARNING] NOTE: Redirects are currently not supported in Windows or MacOs.\n[2024-02-12 09:20:58,980] torch.distributed.elastic.multiprocessing.redirects: [WARNING] NOTE: Redirects are currently not supported in Windows or MacOs.\n(conda:pytorch) \u279c  workspace echo $?\n0\n~~~\n\nExisting behavior for non-zero exit code still works:\ntorchrun --run-path /tmp/script.py\n~~~\n(conda:pytorch) \u279c  workspace torchrun --run-path /tmp/script.py\n[2024-02-12 09:16:20,667] torch.distributed.elastic.multiprocessing.redirects: [WARNING] NOTE: Redirects are currently not supported in Windows or MacOs.\n[2024-02-12 09:16:22,197] torch.distributed.elastic.multiprocessing.redirects: [WARNING] NOTE: Redirects are currently not supported in Windows or MacOs.\n[2024-02-12 09:16:25,795] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 64668) of fn: run_script_path (start_method: spawn)\n[2024-02-12 09:16:25,795] torch.distributed.elastic.multiprocessing.api: [ERROR] Traceback (most recent call last):\n[2024-02-12 09:16:25,795] torch.distributed.elastic.multiprocessing.api: [ERROR]   File \"/Users/kurman/workspace/pytorch/torch/distributed/elastic/multiprocessing/api.py\", line 441, in _poll\n[2024-02-12 09:16:25,795] torch.distributed.elastic.multiprocessing.api: [ERROR]     self._pc.join(-1)\n[2024-02-12 09:16:25,795] torch.distributed.elastic.multiprocessing.api: [ERROR]   File \"/Users/kurman/workspace/pytorch/torch/multiprocessing/spawn.py\", line 177, in join\n[2024-02-12 09:16:25,795] torch.distributed.elastic.multiprocessing.api: [ERROR]     raise ProcessExitedException(\n[2024-02-12 09:16:25,795] torch.distributed.elastic.multiprocessing.api: [ERROR] torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with exit code 1\nTraceback (most recent call last):\n  File \"/Users/kurman/miniconda3/envs/pytorch/bin/torchrun\", line 33, in <module>\n    sys.exit(load_entry_point('torch', 'console_scripts', 'torchrun')())\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/kurman/workspace/pytorch/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/kurman/workspace/pytorch/torch/distributed/run.py\", line 812, in main\n    run(args)\n  File \"/Users/kurman/workspace/pytorch/torch/distributed/run.py\", line 803, in run\n    elastic_launch(\n  File \"/Users/kurman/workspace/pytorch/torch/distributed/launcher/api.py\", line 135, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/kurman/workspace/pytorch/torch/distributed/launcher/api.py\", line 268, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:\n============================================================\nrun_script_path FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2024-02-12_09:16:25\n  host      : kurman-mbp.dhcp.thefacebook.com\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 64668)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n~~~\n\nDifferential Revision: D53653874\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/119697\nApproved by: https://github.com/wconstab",
    "label": "YES",
    "changes": [
        {
            "name": "api_test.py",
            "path": "test/distributed/elastic/multiprocessing/errors/api_test.py",
            "patches": [
                {
                    "old_start": 28,
                    "old_length": 6,
                    "new_start": 28,
                    "new_length": 13,
                    "hunk": "@@ -28,6 +28,13 @@ def raise_exception_fn():\n     raise SentinelError(\"foobar\")\n \n \n+@record\n+def raise_system_exit_exception_fn(exit_code: int = 1):\n+    exp = SystemExit()\n+    exp.code = exit_code\n+    raise exp\n+\n+\n @record\n def good_fn():\n     print(\"hello world\")\n"
                },
                {
                    "old_start": 175,
                    "old_length": 6,
                    "new_start": 182,
                    "new_length": 21,
                    "hunk": "@@ -175,6 +182,21 @@ class ApiTest(unittest.TestCase):\n             self.assertIsNotNone(err[\"message\"][\"extraInfo\"][\"py_callstack\"])\n             self.assertIsNotNone(err[\"message\"][\"extraInfo\"][\"timestamp\"])\n \n+    def test_record_system_exit(self):\n+        with mock.patch.dict(os.environ, {}):\n+            raise_system_exit_exception_fn(exit_code=0)\n+\n+        # no error file should have been generated\n+        self.assertFalse(os.path.isfile(self.test_error_file))\n+\n+    def test_record_system_exit_erronr(self):\n+        with mock.patch.dict(os.environ, {}):\n+            with self.assertRaises(SystemExit):\n+                raise_system_exit_exception_fn()\n+\n+        # no error file should have been generated\n+        self.assertFalse(os.path.isfile(self.test_error_file))\n+\n     def test_record_no_error_file(self):\n         with mock.patch.dict(os.environ, {}):\n             with self.assertRaises(SentinelError):\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+@record\n+def raise_system_exit_exception_fn(exit_code: int = 1):\n+    exp = SystemExit()\n+    exp.code = exit_code\n+    raise exp\n+\n+\n+    def test_record_system_exit(self):\n+        with mock.patch.dict(os.environ, {}):\n+            raise_system_exit_exception_fn(exit_code=0)\n+\n+        # no error file should have been generated\n+        self.assertFalse(os.path.isfile(self.test_error_file))\n+\n+    def test_record_system_exit_erronr(self):\n+        with mock.patch.dict(os.environ, {}):\n+            with self.assertRaises(SystemExit):\n+                raise_system_exit_exception_fn()\n+\n+        # no error file should have been generated\n+        self.assertFalse(os.path.isfile(self.test_error_file))\n+\n",
            "whole_hunk": "@@ -28,6 +28,13 @@ def raise_exception_fn():\n     raise SentinelError(\"foobar\")\n \n \n+@record\n+def raise_system_exit_exception_fn(exit_code: int = 1):\n+    exp = SystemExit()\n+    exp.code = exit_code\n+    raise exp\n+\n+\n @record\n def good_fn():\n     print(\"hello world\")\n@@ -175,6 +182,21 @@ class ApiTest(unittest.TestCase):\n             self.assertIsNotNone(err[\"message\"][\"extraInfo\"][\"py_callstack\"])\n             self.assertIsNotNone(err[\"message\"][\"extraInfo\"][\"timestamp\"])\n \n+    def test_record_system_exit(self):\n+        with mock.patch.dict(os.environ, {}):\n+            raise_system_exit_exception_fn(exit_code=0)\n+\n+        # no error file should have been generated\n+        self.assertFalse(os.path.isfile(self.test_error_file))\n+\n+    def test_record_system_exit_erronr(self):\n+        with mock.patch.dict(os.environ, {}):\n+            with self.assertRaises(SystemExit):\n+                raise_system_exit_exception_fn()\n+\n+        # no error file should have been generated\n+        self.assertFalse(os.path.isfile(self.test_error_file))\n+\n     def test_record_no_error_file(self):\n         with mock.patch.dict(os.environ, {}):\n             with self.assertRaises(SentinelError):\n"
        },
        {
            "name": "__init__.py",
            "path": "torch/distributed/elastic/multiprocessing/errors/__init__.py",
            "patches": [
                {
                    "old_start": 345,
                    "old_length": 6,
                    "new_start": 345,
                    "new_length": 13,
                    "hunk": "@@ -345,6 +345,13 @@ def record(\n             error_handler.initialize()\n             try:\n                 return f(*args, **kwargs)\n+            except SystemExit as se:\n+                # For run_path based entrypoints, SystemExit with code = 0 will never exit.\n+                # Handling it here by returning a value:\n+                if se.code == 0:\n+                    return None\n+                else:\n+                    raise\n             except ChildFailedError as e:\n                 rank, failure = e.get_first_failure()\n                 if failure.error_file != _NOT_AVAILABLE:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+            except SystemExit as se:\n+                # For run_path based entrypoints, SystemExit with code = 0 will never exit.\n+                # Handling it here by returning a value:\n+                if se.code == 0:\n+                    return None\n+                else:\n+                    raise\n",
            "whole_hunk": "@@ -345,6 +345,13 @@ def record(\n             error_handler.initialize()\n             try:\n                 return f(*args, **kwargs)\n+            except SystemExit as se:\n+                # For run_path based entrypoints, SystemExit with code = 0 will never exit.\n+                # Handling it here by returning a value:\n+                if se.code == 0:\n+                    return None\n+                else:\n+                    raise\n             except ChildFailedError as e:\n                 rank, failure = e.get_first_failure()\n                 if failure.error_file != _NOT_AVAILABLE:"
        }
    ]
},
{
    "Id": 515,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/9a95b4bc7bbc66f0513d01e1c2af8e1ead5c1077",
    "date": "2023-09-16T20:53:55+00:00",
    "message": "[dtensor] quick fix to #109306 (#109428)\n\nLooks like the op argument schema type check is not reliable.. for\nthings like aten.div.Tensor(Tensor, Tensor), the second argument can still be\na float/scalar for some reason, switch to check with the instance type\ndirectly\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109428\nApproved by: https://github.com/awgu, https://github.com/fegin",
    "label": "YES",
    "changes": [
        {
            "name": "op_schema.py",
            "path": "torch/distributed/_tensor/op_schema.py",
            "patches": [
                {
                    "old_start": 142,
                    "old_length": 7,
                    "new_start": 142,
                    "new_length": 7,
                    "hunk": "@@ -142,7 +142,7 @@ class RuntimeSchemaInfo:\n     # args/kwargs which would affect sharding propagation results. All args after this\n     # index would be hashed to our sharding cache.\n     # Note that only a few ops need this information, e.g. view, transpose, var.dim, etc.\n-    static_argnum: int = -1\n+    static_argnum: int = 100\n     # This static_kwargkey records static kwarg names which would affect sharding prop\n     static_kwargkey: Optional[List[str]] = None\n     # TODO: make use of this field\n"
                },
                {
                    "old_start": 193,
                    "old_length": 20,
                    "new_start": 193,
                    "new_length": 15,
                    "hunk": "@@ -193,20 +193,15 @@ class OpSchema:\n         )\n \n     def arg_type_tensor_or_tensor_list_like(self, arg_idx: int) -> bool:\n-        op_arg_type = self.op._schema.arguments[arg_idx].type\n-        is_tensor = isinstance(op_arg_type, torch.TensorType)\n+        arg = self.args_schema[arg_idx]\n+        is_tensor = isinstance(arg, DTensorSpec)\n         if is_tensor:\n             return True\n \n-        is_list_like = isinstance(op_arg_type, torch.ListType)\n-        if not is_list_like:\n+        if not isinstance(arg, list):\n             return False\n \n-        elem_type = op_arg_type.getElementType()\n-        return isinstance(elem_type, torch.TensorType) or (\n-            isinstance(elem_type, torch.OptionalType)\n-            and isinstance(elem_type.getElementType(), torch.TensorType)\n-        )\n+        return all(isinstance(e, DTensorSpec) or e is None for e in arg)\n \n     def __hash__(self) -> int:\n         # Only hash args and kwargs that op indicates to hash"
                }
            ],
            "whole_deleted": "-    static_argnum: int = -1\n-        op_arg_type = self.op._schema.arguments[arg_idx].type\n-        is_tensor = isinstance(op_arg_type, torch.TensorType)\n-        is_list_like = isinstance(op_arg_type, torch.ListType)\n-        if not is_list_like:\n-        elem_type = op_arg_type.getElementType()\n-        return isinstance(elem_type, torch.TensorType) or (\n-            isinstance(elem_type, torch.OptionalType)\n-            and isinstance(elem_type.getElementType(), torch.TensorType)\n-        )\n",
            "whole_added": "+    static_argnum: int = 100\n+        arg = self.args_schema[arg_idx]\n+        is_tensor = isinstance(arg, DTensorSpec)\n+        if not isinstance(arg, list):\n+        return all(isinstance(e, DTensorSpec) or e is None for e in arg)\n",
            "whole_hunk": "@@ -142,7 +142,7 @@ class RuntimeSchemaInfo:\n     # args/kwargs which would affect sharding propagation results. All args after this\n     # index would be hashed to our sharding cache.\n     # Note that only a few ops need this information, e.g. view, transpose, var.dim, etc.\n-    static_argnum: int = -1\n+    static_argnum: int = 100\n     # This static_kwargkey records static kwarg names which would affect sharding prop\n     static_kwargkey: Optional[List[str]] = None\n     # TODO: make use of this field\n@@ -193,20 +193,15 @@ class OpSchema:\n         )\n \n     def arg_type_tensor_or_tensor_list_like(self, arg_idx: int) -> bool:\n-        op_arg_type = self.op._schema.arguments[arg_idx].type\n-        is_tensor = isinstance(op_arg_type, torch.TensorType)\n+        arg = self.args_schema[arg_idx]\n+        is_tensor = isinstance(arg, DTensorSpec)\n         if is_tensor:\n             return True\n \n-        is_list_like = isinstance(op_arg_type, torch.ListType)\n-        if not is_list_like:\n+        if not isinstance(arg, list):\n             return False\n \n-        elem_type = op_arg_type.getElementType()\n-        return isinstance(elem_type, torch.TensorType) or (\n-            isinstance(elem_type, torch.OptionalType)\n-            and isinstance(elem_type.getElementType(), torch.TensorType)\n-        )\n+        return all(isinstance(e, DTensorSpec) or e is None for e in arg)\n \n     def __hash__(self) -> int:\n         # Only hash args and kwargs that op indicates to hash"
        }
    ]
},
{
    "Id": 505,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/56ef200c2dc8a1f1e269861b7a6e02e99d3b56a1",
    "date": "2023-09-22T12:12:10+00:00",
    "message": "When doing typed typecheck, also check signature with symint removed (#109727)\n\nSee the test case for what we didn't catch (SymInt vs const SymInt&\nmismatch.)\n\nIt's necessary to test for both, because we will fall back to the\nnon-SymInt signature if there is no SymInt unboxed kernel available.\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109727\nApproved by: https://github.com/zou3519",
    "label": "YES",
    "changes": [
        {
            "name": "KernelFunction.h",
            "path": "aten/src/ATen/core/boxing/KernelFunction.h",
            "patches": [
                {
                    "old_start": 18,
                    "old_length": 10,
                    "new_start": 18,
                    "new_length": 10,
                    "hunk": "@@ -18,10 +18,10 @@ class KernelFunction;\n template <typename T>\n using has_symint =\n   guts::disjunction<\n-    std::is_same<c10::SymInt, std::decay_t<T>>,\n-    std::is_same<c10::SymIntArrayRef, std::decay_t<T>>,\n-    std::is_same<at::OptionalSymIntArrayRef, std::decay_t<T>>,\n-    std::is_same<c10::optional<c10::SymInt>, std::decay_t<T>>\n+    std::is_same<c10::SymInt, T>,\n+    std::is_same<c10::SymIntArrayRef, T>,\n+    std::is_same<at::OptionalSymIntArrayRef, T>,\n+    std::is_same<c10::optional<c10::SymInt>, T>\n   >;\n \n template <typename T>\n"
                },
                {
                    "old_start": 65,
                    "old_length": 6,
                    "new_start": 65,
                    "new_length": 14,
                    "hunk": "@@ -65,6 +65,14 @@ using fn_has_symint = typename guts::typelist::true_for_any_type<\n   typename guts::infer_function_traits<T>::type::parameter_types\n >;\n \n+template <typename T>\n+struct fn_remove_symint;\n+\n+template <typename Ret, typename... Args>\n+struct fn_remove_symint<Ret(Args...)> {\n+  using type = Ret(typename remove_symint<Args>::type...);\n+};\n+\n /**\n  * KernelFunction is similar to std::function but stores a kernel function.\n  * You can create a KernelFunction from a boxed or unboxed function/functor/lambda\n"
                }
            ],
            "whole_deleted": "-    std::is_same<c10::SymInt, std::decay_t<T>>,\n-    std::is_same<c10::SymIntArrayRef, std::decay_t<T>>,\n-    std::is_same<at::OptionalSymIntArrayRef, std::decay_t<T>>,\n-    std::is_same<c10::optional<c10::SymInt>, std::decay_t<T>>\n",
            "whole_added": "+    std::is_same<c10::SymInt, T>,\n+    std::is_same<c10::SymIntArrayRef, T>,\n+    std::is_same<at::OptionalSymIntArrayRef, T>,\n+    std::is_same<c10::optional<c10::SymInt>, T>\n+template <typename T>\n+struct fn_remove_symint;\n+\n+template <typename Ret, typename... Args>\n+struct fn_remove_symint<Ret(Args...)> {\n+  using type = Ret(typename remove_symint<Args>::type...);\n+};\n+\n",
            "whole_hunk": "@@ -18,10 +18,10 @@ class KernelFunction;\n template <typename T>\n using has_symint =\n   guts::disjunction<\n-    std::is_same<c10::SymInt, std::decay_t<T>>,\n-    std::is_same<c10::SymIntArrayRef, std::decay_t<T>>,\n-    std::is_same<at::OptionalSymIntArrayRef, std::decay_t<T>>,\n-    std::is_same<c10::optional<c10::SymInt>, std::decay_t<T>>\n+    std::is_same<c10::SymInt, T>,\n+    std::is_same<c10::SymIntArrayRef, T>,\n+    std::is_same<at::OptionalSymIntArrayRef, T>,\n+    std::is_same<c10::optional<c10::SymInt>, T>\n   >;\n \n template <typename T>\n@@ -65,6 +65,14 @@ using fn_has_symint = typename guts::typelist::true_for_any_type<\n   typename guts::infer_function_traits<T>::type::parameter_types\n >;\n \n+template <typename T>\n+struct fn_remove_symint;\n+\n+template <typename Ret, typename... Args>\n+struct fn_remove_symint<Ret(Args...)> {\n+  using type = Ret(typename remove_symint<Args>::type...);\n+};\n+\n /**\n  * KernelFunction is similar to std::function but stores a kernel function.\n  * You can create a KernelFunction from a boxed or unboxed function/functor/lambda\n"
        },
        {
            "name": "make_boxed_from_unboxed_functor.h",
            "path": "aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h",
            "patches": [
                {
                    "old_start": 179,
                    "old_length": 10,
                    "new_start": 179,
                    "new_length": 6,
                    "hunk": "@@ -179,10 +179,6 @@ namespace impl {\n       \"You tried to register a kernel with an unsupported input type: std::array<Scalar, N>. Please use std::array<int64_t, N> instead.\");\n   };\n \n-  // The following specialisations of assert_is_valid_input_type are technically not\n-  // necessary since we would hit the base case and show an error message\n-  // there if they didn't exist, but we can show a better error message\n-  // in some common error scenarios.\n   template<class T, bool AllowDeprecatedTypes>\n   struct assert_is_valid_input_type<T, AllowDeprecatedTypes, std::enable_if_t<std::is_same<float, T>::value>> {\n     // There is no reason to support float when we have double. Keep the API lean.\n"
                },
                {
                    "old_start": 204,
                    "old_length": 6,
                    "new_start": 200,
                    "new_length": 14,
                    "hunk": "@@ -204,6 +200,14 @@ namespace impl {\n     static_assert(guts::false_t<T>::value,\n       \"You tried to register a kernel with an unsupported integral input type. Please use int64_t instead.\");\n   };\n+  template<class T, bool AllowDeprecatedTypes>\n+  struct assert_is_valid_input_type<T, AllowDeprecatedTypes, std::enable_if_t<std::is_same<const c10::SymInt&, T>::value>> {\n+    static_assert(guts::false_t<T>::value,\n+      \"You tried to register a kernel taking c10::SymInt by reference. Please accept it by value instead.\");\n+  };\n+\n+  // TODO: it probably would be good to tighten this up quite a bit more with\n+  // an explicit list for everything\n \n   //\n   // assert_is_valid_output_type\n"
                }
            ],
            "whole_deleted": "-  // The following specialisations of assert_is_valid_input_type are technically not\n-  // necessary since we would hit the base case and show an error message\n-  // there if they didn't exist, but we can show a better error message\n-  // in some common error scenarios.\n",
            "whole_added": "+  template<class T, bool AllowDeprecatedTypes>\n+  struct assert_is_valid_input_type<T, AllowDeprecatedTypes, std::enable_if_t<std::is_same<const c10::SymInt&, T>::value>> {\n+    static_assert(guts::false_t<T>::value,\n+      \"You tried to register a kernel taking c10::SymInt by reference. Please accept it by value instead.\");\n+  };\n+\n+  // TODO: it probably would be good to tighten this up quite a bit more with\n+  // an explicit list for everything\n",
            "whole_hunk": "@@ -179,10 +179,6 @@ namespace impl {\n       \"You tried to register a kernel with an unsupported input type: std::array<Scalar, N>. Please use std::array<int64_t, N> instead.\");\n   };\n \n-  // The following specialisations of assert_is_valid_input_type are technically not\n-  // necessary since we would hit the base case and show an error message\n-  // there if they didn't exist, but we can show a better error message\n-  // in some common error scenarios.\n   template<class T, bool AllowDeprecatedTypes>\n   struct assert_is_valid_input_type<T, AllowDeprecatedTypes, std::enable_if_t<std::is_same<float, T>::value>> {\n     // There is no reason to support float when we have double. Keep the API lean.\n@@ -204,6 +200,14 @@ namespace impl {\n     static_assert(guts::false_t<T>::value,\n       \"You tried to register a kernel with an unsupported integral input type. Please use int64_t instead.\");\n   };\n+  template<class T, bool AllowDeprecatedTypes>\n+  struct assert_is_valid_input_type<T, AllowDeprecatedTypes, std::enable_if_t<std::is_same<const c10::SymInt&, T>::value>> {\n+    static_assert(guts::false_t<T>::value,\n+      \"You tried to register a kernel taking c10::SymInt by reference. Please accept it by value instead.\");\n+  };\n+\n+  // TODO: it probably would be good to tighten this up quite a bit more with\n+  // an explicit list for everything\n \n   //\n   // assert_is_valid_output_type\n"
        },
        {
            "name": "OperatorEntry.h",
            "path": "aten/src/ATen/core/dispatch/OperatorEntry.h",
            "patches": [
                {
                    "old_start": 166,
                    "old_length": 6,
                    "new_start": 166,
                    "new_length": 10,
                    "hunk": "@@ -166,6 +166,10 @@ public:\n   template<class FuncType>\n   inline void assertSignatureIsCorrect() {\n     assertSignatureIsCorrect(CppSignature::make<FuncType>(), fn_has_symint<FuncType>::value);\n+    if (fn_has_symint<FuncType>::value) {\n+      static_assert(!fn_has_symint<typename fn_remove_symint<FuncType>::type>::value);\n+      assertSignatureIsCorrect<typename fn_remove_symint<FuncType>::type>();\n+    }\n   }\n \n   void assertSignatureIsCorrect(const CppSignature& call_signature, bool has_symint) const;\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    if (fn_has_symint<FuncType>::value) {\n+      static_assert(!fn_has_symint<typename fn_remove_symint<FuncType>::type>::value);\n+      assertSignatureIsCorrect<typename fn_remove_symint<FuncType>::type>();\n+    }\n",
            "whole_hunk": "@@ -166,6 +166,10 @@ public:\n   template<class FuncType>\n   inline void assertSignatureIsCorrect() {\n     assertSignatureIsCorrect(CppSignature::make<FuncType>(), fn_has_symint<FuncType>::value);\n+    if (fn_has_symint<FuncType>::value) {\n+      static_assert(!fn_has_symint<typename fn_remove_symint<FuncType>::type>::value);\n+      assertSignatureIsCorrect<typename fn_remove_symint<FuncType>::type>();\n+    }\n   }\n \n   void assertSignatureIsCorrect(const CppSignature& call_signature, bool has_symint) const;\n"
        },
        {
            "name": "op_registration_test.cpp",
            "path": "aten/src/ATen/core/op_registration/op_registration_test.cpp",
            "patches": [
                {
                    "old_start": 2157,
                    "old_length": 6,
                    "new_start": 2157,
                    "new_length": 70,
                    "hunk": "@@ -2157,6 +2157,70 @@ TEST(OperatorRegistrationTest, getRegistrationsForDispatchKey) {\n   ASSERT_TRUE(std::includes(all_ops.begin(), all_ops.end(), cpu_ops.begin(), cpu_ops.end(), cmp_lambda));\n }\n \n+Tensor symint_op(const Tensor& self, int64_t length) {\n+  return self.clone();\n+}\n+\n+TEST(OperatorRegistrationTest, TestSymNonSymCompatibility) {\n+  auto m = MAKE_TORCH_LIBRARY(_test);\n+  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n+  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n+  m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op));\n+\n+  auto opHandle = c10::Dispatcher::singleton().findSchemaOrThrow(\n+      \"_test::symint_op\", \"\");\n+\n+  opHandle.typed<Tensor(const Tensor&, int64_t)>().call(dummyTensor(c10::DispatchKey::CPU), 4);\n+  opHandle.typed<Tensor(const Tensor&, c10::SymInt)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+\n+  expectThrows<c10::Error>([&] {\n+    opHandle.typed<Tensor(const Tensor&, const c10::SymInt&)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+  }, \"Tried to access or call an operator with a wrong signature\");\n+}\n+\n+Tensor symint_op2(const Tensor& self, c10::SymInt length) {\n+  return self.clone();\n+}\n+\n+TEST(OperatorRegistrationTest, TestSymSymCompatibility) {\n+  auto m = MAKE_TORCH_LIBRARY(_test);\n+  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n+  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n+  m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op2));\n+\n+  auto opHandle = c10::Dispatcher::singleton().findSchemaOrThrow(\n+      \"_test::symint_op\", \"\");\n+\n+  opHandle.typed<Tensor(const Tensor&, int64_t)>().call(dummyTensor(c10::DispatchKey::CPU), 4);\n+  opHandle.typed<Tensor(const Tensor&, c10::SymInt)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+  // TODO: We should reject this on principle, but today it accidentally works\n+  // due to going through the boxed calling convention.\n+  //\n+  // First, we attempt to test if const SymInt& has SymInt. It does not,\n+  // because we only accept something as SymInt if it has exactly SymInt in\n+  // its signature. So we check if there is a non-symint kernel. But there is\n+  // no non-SymInt kernel, because we only registered a real SymInt kernel.\n+  // When this occurs, we fall back to the boxed calling convention.  And the\n+  // boxed calling convention can deal with const SymInt& fine, as during\n+  // boxing it will just create a SymInt to push onto the argument stack and\n+  // everything is fine.\n+  opHandle.typed<Tensor(const Tensor&, const c10::SymInt&)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+}\n+\n+Tensor symint_op3(const Tensor& self, const c10::SymInt& length) {\n+  return self.clone();\n+}\n+\n+TEST(OperatorRegistrationTest, TestSymSymRefCompatibility) {\n+  auto m = MAKE_TORCH_LIBRARY(_test);\n+  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n+  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n+\n+  expectThrows<c10::Error>([&] {\n+    m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op3));\n+  }, \"doesn't match the expected function schema\");\n+}\n+\n }\n \n #pragma GCC diagnostic pop"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+Tensor symint_op(const Tensor& self, int64_t length) {\n+  return self.clone();\n+}\n+\n+TEST(OperatorRegistrationTest, TestSymNonSymCompatibility) {\n+  auto m = MAKE_TORCH_LIBRARY(_test);\n+  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n+  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n+  m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op));\n+\n+  auto opHandle = c10::Dispatcher::singleton().findSchemaOrThrow(\n+      \"_test::symint_op\", \"\");\n+\n+  opHandle.typed<Tensor(const Tensor&, int64_t)>().call(dummyTensor(c10::DispatchKey::CPU), 4);\n+  opHandle.typed<Tensor(const Tensor&, c10::SymInt)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+\n+  expectThrows<c10::Error>([&] {\n+    opHandle.typed<Tensor(const Tensor&, const c10::SymInt&)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+  }, \"Tried to access or call an operator with a wrong signature\");\n+}\n+\n+Tensor symint_op2(const Tensor& self, c10::SymInt length) {\n+  return self.clone();\n+}\n+\n+TEST(OperatorRegistrationTest, TestSymSymCompatibility) {\n+  auto m = MAKE_TORCH_LIBRARY(_test);\n+  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n+  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n+  m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op2));\n+\n+  auto opHandle = c10::Dispatcher::singleton().findSchemaOrThrow(\n+      \"_test::symint_op\", \"\");\n+\n+  opHandle.typed<Tensor(const Tensor&, int64_t)>().call(dummyTensor(c10::DispatchKey::CPU), 4);\n+  opHandle.typed<Tensor(const Tensor&, c10::SymInt)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+  // TODO: We should reject this on principle, but today it accidentally works\n+  // due to going through the boxed calling convention.\n+  //\n+  // First, we attempt to test if const SymInt& has SymInt. It does not,\n+  // because we only accept something as SymInt if it has exactly SymInt in\n+  // its signature. So we check if there is a non-symint kernel. But there is\n+  // no non-SymInt kernel, because we only registered a real SymInt kernel.\n+  // When this occurs, we fall back to the boxed calling convention.  And the\n+  // boxed calling convention can deal with const SymInt& fine, as during\n+  // boxing it will just create a SymInt to push onto the argument stack and\n+  // everything is fine.\n+  opHandle.typed<Tensor(const Tensor&, const c10::SymInt&)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+}\n+\n+Tensor symint_op3(const Tensor& self, const c10::SymInt& length) {\n+  return self.clone();\n+}\n+\n+TEST(OperatorRegistrationTest, TestSymSymRefCompatibility) {\n+  auto m = MAKE_TORCH_LIBRARY(_test);\n+  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n+  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n+\n+  expectThrows<c10::Error>([&] {\n+    m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op3));\n+  }, \"doesn't match the expected function schema\");\n+}\n+\n",
            "whole_hunk": "@@ -2157,6 +2157,70 @@ TEST(OperatorRegistrationTest, getRegistrationsForDispatchKey) {\n   ASSERT_TRUE(std::includes(all_ops.begin(), all_ops.end(), cpu_ops.begin(), cpu_ops.end(), cmp_lambda));\n }\n \n+Tensor symint_op(const Tensor& self, int64_t length) {\n+  return self.clone();\n+}\n+\n+TEST(OperatorRegistrationTest, TestSymNonSymCompatibility) {\n+  auto m = MAKE_TORCH_LIBRARY(_test);\n+  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n+  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n+  m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op));\n+\n+  auto opHandle = c10::Dispatcher::singleton().findSchemaOrThrow(\n+      \"_test::symint_op\", \"\");\n+\n+  opHandle.typed<Tensor(const Tensor&, int64_t)>().call(dummyTensor(c10::DispatchKey::CPU), 4);\n+  opHandle.typed<Tensor(const Tensor&, c10::SymInt)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+\n+  expectThrows<c10::Error>([&] {\n+    opHandle.typed<Tensor(const Tensor&, const c10::SymInt&)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+  }, \"Tried to access or call an operator with a wrong signature\");\n+}\n+\n+Tensor symint_op2(const Tensor& self, c10::SymInt length) {\n+  return self.clone();\n+}\n+\n+TEST(OperatorRegistrationTest, TestSymSymCompatibility) {\n+  auto m = MAKE_TORCH_LIBRARY(_test);\n+  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n+  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n+  m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op2));\n+\n+  auto opHandle = c10::Dispatcher::singleton().findSchemaOrThrow(\n+      \"_test::symint_op\", \"\");\n+\n+  opHandle.typed<Tensor(const Tensor&, int64_t)>().call(dummyTensor(c10::DispatchKey::CPU), 4);\n+  opHandle.typed<Tensor(const Tensor&, c10::SymInt)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+  // TODO: We should reject this on principle, but today it accidentally works\n+  // due to going through the boxed calling convention.\n+  //\n+  // First, we attempt to test if const SymInt& has SymInt. It does not,\n+  // because we only accept something as SymInt if it has exactly SymInt in\n+  // its signature. So we check if there is a non-symint kernel. But there is\n+  // no non-SymInt kernel, because we only registered a real SymInt kernel.\n+  // When this occurs, we fall back to the boxed calling convention.  And the\n+  // boxed calling convention can deal with const SymInt& fine, as during\n+  // boxing it will just create a SymInt to push onto the argument stack and\n+  // everything is fine.\n+  opHandle.typed<Tensor(const Tensor&, const c10::SymInt&)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+}\n+\n+Tensor symint_op3(const Tensor& self, const c10::SymInt& length) {\n+  return self.clone();\n+}\n+\n+TEST(OperatorRegistrationTest, TestSymSymRefCompatibility) {\n+  auto m = MAKE_TORCH_LIBRARY(_test);\n+  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n+  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n+\n+  expectThrows<c10::Error>([&] {\n+    m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op3));\n+  }, \"doesn't match the expected function schema\");\n+}\n+\n }\n \n #pragma GCC diagnostic pop"
        }
    ]
},
{
    "Id": 250,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/7a4e4511845dbeefe4d16c321b2a93ac72b76d93",
    "date": "2024-03-11T02:18:43+00:00",
    "message": "[Dynamo] Fix function overrides (#120885)\n\nTo check existence of `__torch_function__`, the code intended to iterate each element but got `TupleVariable` when the ordinary `has_torch_function()` was being used. Needs further unpack in this case\n\nFixes #120653\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/120885\nApproved by: https://github.com/yanboliang",
    "label": "YES",
    "changes": [
        {
            "name": "torch.py",
            "path": "torch/_dynamo/variables/torch.py",
            "patches": [
                {
                    "old_start": 399,
                    "old_length": 8,
                    "new_start": 399,
                    "new_length": 13,
                    "hunk": "@@ -399,8 +399,13 @@ class TorchInGraphFunctionVariable(BaseTorchVariable):\n             torch.overrides.has_torch_function_unary,\n         ):\n             assert not kwargs\n+            elems = (\n+                args[0].unpack_var_sequence(tx)\n+                if len(args) == 1 and isinstance(args[0], TupleVariable)\n+                else args\n+            )\n             return ConstantVariable.create(\n-                any(has_torch_function(a) for a in args),\n+                any(has_torch_function(x) for x in elems),\n             )\n         elif any(\n             self.value is method"
                }
            ],
            "whole_deleted": "-                any(has_torch_function(a) for a in args),\n",
            "whole_added": "+            elems = (\n+                args[0].unpack_var_sequence(tx)\n+                if len(args) == 1 and isinstance(args[0], TupleVariable)\n+                else args\n+            )\n+                any(has_torch_function(x) for x in elems),\n",
            "whole_hunk": "@@ -399,8 +399,13 @@ class TorchInGraphFunctionVariable(BaseTorchVariable):\n             torch.overrides.has_torch_function_unary,\n         ):\n             assert not kwargs\n+            elems = (\n+                args[0].unpack_var_sequence(tx)\n+                if len(args) == 1 and isinstance(args[0], TupleVariable)\n+                else args\n+            )\n             return ConstantVariable.create(\n-                any(has_torch_function(a) for a in args),\n+                any(has_torch_function(x) for x in elems),\n             )\n         elif any(\n             self.value is method"
        }
    ]
},
{
    "Id": 480,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/8209bbbd061c563cd6461c883afa44b0fc1b8f9f",
    "date": "2023-10-13T08:46:17+00:00",
    "message": "[AOTInductor] Improve validation for C++ wrapper codegen (#111102)\n\nIt's a reimplementation of #111089\n\n1. When using fake inputs make sure they are on the same device as the original inputs.\n2. Don't change the value of self.cpp_wrapper from True to False if can't generate a C++ wrapper, instead have a check and fail early to avoid producing Python code for C++ compiler.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111102\nApproved by: https://github.com/desertfire, https://github.com/jgong5, https://github.com/chunyuan-w",
    "label": "NO",
    "changes": [
        {
            "name": "test_aot_inductor.py",
            "path": "test/inductor/test_aot_inductor.py",
            "patches": [
                {
                    "old_start": 11,
                    "old_length": 6,
                    "new_start": 11,
                    "new_length": 7,
                    "hunk": "@@ -11,6 +11,7 @@ import torch._inductor\n import torch.fx._pytree as fx_pytree\n from torch._dynamo.testing import same\n from torch._inductor import config\n+from torch._inductor.exc import CppWrapperCodeGenError\n from torch._inductor.utils import aot_inductor_launcher\n \n from torch.testing import FileCheck\n"
                },
                {
                    "old_start": 833,
                    "old_length": 6,
                    "new_start": 834,
                    "new_length": 57,
                    "hunk": "@@ -833,6 +834,57 @@ class AOTInductorTestsTemplate:\n                     exactly=True,\n                 ).run(src_code)\n \n+    def test_fake_tensor_device_validation(self):\n+        if self.device != \"cuda\":\n+            raise unittest.SkipTest(\"requires CUDA\")\n+\n+        class Model(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+\n+            def forward(self, x, y):\n+                return x + y\n+\n+        example_inputs = (torch.randn(10, 10), torch.randn(10, 10))\n+\n+        # Export on CPU\n+        exported_program = torch._export.export(\n+            Model(),\n+            example_inputs,\n+            constraints=[],\n+        )\n+\n+        # Compile exported model on CUDA\n+        gm = exported_program.graph_module.to(self.device)\n+        with self.assertRaisesRegex(ValueError, \"Device mismatch between fake input\"):\n+            torch._inductor.aot_compile(\n+                gm, tuple(i.to(self.device) for i in example_inputs)\n+            )\n+\n+    @unittest.mock.patch(\"torch._inductor.graph.supported_dtype_of_cpp_wrapper\")\n+    def test_unsupported_input_dtype(self, supported_dtype_of_cpp_wrapper_mock):\n+        supported_dtype_of_cpp_wrapper_mock.return_value = False\n+\n+        class Model(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+\n+            def forward(self, x, y):\n+                return x + y\n+\n+        example_inputs = (\n+            torch.randn(10, 10).to(self.device),\n+            torch.randn(10, 10).to(self.device),\n+        )\n+        with self.assertRaisesRegex(\n+            CppWrapperCodeGenError, \"Unsupported input dtype torch.float32\"\n+        ):\n+            torch._export.aot_compile(Model(), example_inputs)\n+\n+        supported_dtype_of_cpp_wrapper_mock.assert_called_once_with(\n+            torch.float32, self.device == \"cuda\"\n+        )\n+\n \n class AOTInductorTestABICompatibleCpu(TestCase):\n     device = \"cpu\"\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+from torch._inductor.exc import CppWrapperCodeGenError\n+    def test_fake_tensor_device_validation(self):\n+        if self.device != \"cuda\":\n+            raise unittest.SkipTest(\"requires CUDA\")\n+\n+        class Model(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+\n+            def forward(self, x, y):\n+                return x + y\n+\n+        example_inputs = (torch.randn(10, 10), torch.randn(10, 10))\n+\n+        # Export on CPU\n+        exported_program = torch._export.export(\n+            Model(),\n+            example_inputs,\n+            constraints=[],\n+        )\n+\n+        # Compile exported model on CUDA\n+        gm = exported_program.graph_module.to(self.device)\n+        with self.assertRaisesRegex(ValueError, \"Device mismatch between fake input\"):\n+            torch._inductor.aot_compile(\n+                gm, tuple(i.to(self.device) for i in example_inputs)\n+            )\n+\n+    @unittest.mock.patch(\"torch._inductor.graph.supported_dtype_of_cpp_wrapper\")\n+    def test_unsupported_input_dtype(self, supported_dtype_of_cpp_wrapper_mock):\n+        supported_dtype_of_cpp_wrapper_mock.return_value = False\n+\n+        class Model(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+\n+            def forward(self, x, y):\n+                return x + y\n+\n+        example_inputs = (\n+            torch.randn(10, 10).to(self.device),\n+            torch.randn(10, 10).to(self.device),\n+        )\n+        with self.assertRaisesRegex(\n+            CppWrapperCodeGenError, \"Unsupported input dtype torch.float32\"\n+        ):\n+            torch._export.aot_compile(Model(), example_inputs)\n+\n+        supported_dtype_of_cpp_wrapper_mock.assert_called_once_with(\n+            torch.float32, self.device == \"cuda\"\n+        )\n+\n",
            "whole_hunk": "@@ -11,6 +11,7 @@ import torch._inductor\n import torch.fx._pytree as fx_pytree\n from torch._dynamo.testing import same\n from torch._inductor import config\n+from torch._inductor.exc import CppWrapperCodeGenError\n from torch._inductor.utils import aot_inductor_launcher\n \n from torch.testing import FileCheck\n@@ -833,6 +834,57 @@ class AOTInductorTestsTemplate:\n                     exactly=True,\n                 ).run(src_code)\n \n+    def test_fake_tensor_device_validation(self):\n+        if self.device != \"cuda\":\n+            raise unittest.SkipTest(\"requires CUDA\")\n+\n+        class Model(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+\n+            def forward(self, x, y):\n+                return x + y\n+\n+        example_inputs = (torch.randn(10, 10), torch.randn(10, 10))\n+\n+        # Export on CPU\n+        exported_program = torch._export.export(\n+            Model(),\n+            example_inputs,\n+            constraints=[],\n+        )\n+\n+        # Compile exported model on CUDA\n+        gm = exported_program.graph_module.to(self.device)\n+        with self.assertRaisesRegex(ValueError, \"Device mismatch between fake input\"):\n+            torch._inductor.aot_compile(\n+                gm, tuple(i.to(self.device) for i in example_inputs)\n+            )\n+\n+    @unittest.mock.patch(\"torch._inductor.graph.supported_dtype_of_cpp_wrapper\")\n+    def test_unsupported_input_dtype(self, supported_dtype_of_cpp_wrapper_mock):\n+        supported_dtype_of_cpp_wrapper_mock.return_value = False\n+\n+        class Model(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+\n+            def forward(self, x, y):\n+                return x + y\n+\n+        example_inputs = (\n+            torch.randn(10, 10).to(self.device),\n+            torch.randn(10, 10).to(self.device),\n+        )\n+        with self.assertRaisesRegex(\n+            CppWrapperCodeGenError, \"Unsupported input dtype torch.float32\"\n+        ):\n+            torch._export.aot_compile(Model(), example_inputs)\n+\n+        supported_dtype_of_cpp_wrapper_mock.assert_called_once_with(\n+            torch.float32, self.device == \"cuda\"\n+        )\n+\n \n class AOTInductorTestABICompatibleCpu(TestCase):\n     device = \"cpu\"\n"
        },
        {
            "name": "test_cpu_repro.py",
            "path": "test/inductor/test_cpu_repro.py",
            "patches": [
                {
                    "old_start": 12,
                    "old_length": 6,
                    "new_start": 12,
                    "new_length": 7,
                    "hunk": "@@ -12,6 +12,7 @@ import numpy as np\n import sympy\n import torch\n from torch._C import FileCheck\n+from torch._dynamo.exc import BackendCompilerFailed\n from torch._dynamo.testing import rand_strided\n from torch._dynamo.utils import same\n from torch._inductor import codecache, config, metrics\n"
                },
                {
                    "old_start": 1261,
                    "old_length": 8,
                    "new_start": 1262,
                    "new_length": 17,
                    "hunk": "@@ -1261,8 +1262,17 @@ class CPUReproTests(TestCase):\n                     with config.patch({\"cpp_wrapper\": cpp_wrapper_flag}):\n                         torch._dynamo.reset()\n                         metrics.reset()\n-                        self.common(fn, (value, mask))\n-                        assert metrics.generated_cpp_vec_kernel_count >= 1\n+                        # fp16 inputs are not supported for C++ wrappers on CPU yet\n+                        if cpp_wrapper_flag and dtype == torch.float16:\n+                            with self.assertRaisesRegex(\n+                                BackendCompilerFailed,\n+                                \"Unsupported input dtype torch.float16\",\n+                            ):\n+                                self.common(fn, (value, mask))\n+                            assert metrics.generated_cpp_vec_kernel_count == 0\n+                        else:\n+                            self.common(fn, (value, mask))\n+                            assert metrics.generated_cpp_vec_kernel_count >= 1\n \n     def test_load_same_bool_tensor_twice(self):\n         @torch._dynamo.optimize(\"inductor\")\n"
                },
                {
                    "old_start": 1399,
                    "old_length": 8,
                    "new_start": 1409,
                    "new_length": 17,
                    "hunk": "@@ -1399,8 +1409,17 @@ class CPUReproTests(TestCase):\n                     with config.patch({\"cpp_wrapper\": cpp_wrapper_flag}):\n                         torch._dynamo.reset()\n                         metrics.reset()\n-                        self.common(fn, (x,))\n-                        assert metrics.generated_cpp_vec_kernel_count == 1\n+                        # fp16 inputs are not supported for C++ wrappers on CPU yet\n+                        if cpp_wrapper_flag and dtype == torch.float16:\n+                            with self.assertRaisesRegex(\n+                                BackendCompilerFailed,\n+                                \"Unsupported input dtype torch.float16\",\n+                            ):\n+                                self.common(fn, (x,))\n+                            assert metrics.generated_cpp_vec_kernel_count == 0\n+                        else:\n+                            self.common(fn, (x,))\n+                            assert metrics.generated_cpp_vec_kernel_count == 1\n \n     @unittest.skipIf(\n         not codecache.valid_vec_isa_list(), \"Does not support vectorization\"\n"
                }
            ],
            "whole_deleted": "-                        self.common(fn, (value, mask))\n-                        assert metrics.generated_cpp_vec_kernel_count >= 1\n-                        self.common(fn, (x,))\n-                        assert metrics.generated_cpp_vec_kernel_count == 1\n",
            "whole_added": "+from torch._dynamo.exc import BackendCompilerFailed\n+                        # fp16 inputs are not supported for C++ wrappers on CPU yet\n+                        if cpp_wrapper_flag and dtype == torch.float16:\n+                            with self.assertRaisesRegex(\n+                                BackendCompilerFailed,\n+                                \"Unsupported input dtype torch.float16\",\n+                            ):\n+                                self.common(fn, (value, mask))\n+                            assert metrics.generated_cpp_vec_kernel_count == 0\n+                        else:\n+                            self.common(fn, (value, mask))\n+                            assert metrics.generated_cpp_vec_kernel_count >= 1\n+                        # fp16 inputs are not supported for C++ wrappers on CPU yet\n+                        if cpp_wrapper_flag and dtype == torch.float16:\n+                            with self.assertRaisesRegex(\n+                                BackendCompilerFailed,\n+                                \"Unsupported input dtype torch.float16\",\n+                            ):\n+                                self.common(fn, (x,))\n+                            assert metrics.generated_cpp_vec_kernel_count == 0\n+                        else:\n+                            self.common(fn, (x,))\n+                            assert metrics.generated_cpp_vec_kernel_count == 1\n",
            "whole_hunk": "@@ -12,6 +12,7 @@ import numpy as np\n import sympy\n import torch\n from torch._C import FileCheck\n+from torch._dynamo.exc import BackendCompilerFailed\n from torch._dynamo.testing import rand_strided\n from torch._dynamo.utils import same\n from torch._inductor import codecache, config, metrics\n@@ -1261,8 +1262,17 @@ class CPUReproTests(TestCase):\n                     with config.patch({\"cpp_wrapper\": cpp_wrapper_flag}):\n                         torch._dynamo.reset()\n                         metrics.reset()\n-                        self.common(fn, (value, mask))\n-                        assert metrics.generated_cpp_vec_kernel_count >= 1\n+                        # fp16 inputs are not supported for C++ wrappers on CPU yet\n+                        if cpp_wrapper_flag and dtype == torch.float16:\n+                            with self.assertRaisesRegex(\n+                                BackendCompilerFailed,\n+                                \"Unsupported input dtype torch.float16\",\n+                            ):\n+                                self.common(fn, (value, mask))\n+                            assert metrics.generated_cpp_vec_kernel_count == 0\n+                        else:\n+                            self.common(fn, (value, mask))\n+                            assert metrics.generated_cpp_vec_kernel_count >= 1\n \n     def test_load_same_bool_tensor_twice(self):\n         @torch._dynamo.optimize(\"inductor\")\n@@ -1399,8 +1409,17 @@ class CPUReproTests(TestCase):\n                     with config.patch({\"cpp_wrapper\": cpp_wrapper_flag}):\n                         torch._dynamo.reset()\n                         metrics.reset()\n-                        self.common(fn, (x,))\n-                        assert metrics.generated_cpp_vec_kernel_count == 1\n+                        # fp16 inputs are not supported for C++ wrappers on CPU yet\n+                        if cpp_wrapper_flag and dtype == torch.float16:\n+                            with self.assertRaisesRegex(\n+                                BackendCompilerFailed,\n+                                \"Unsupported input dtype torch.float16\",\n+                            ):\n+                                self.common(fn, (x,))\n+                            assert metrics.generated_cpp_vec_kernel_count == 0\n+                        else:\n+                            self.common(fn, (x,))\n+                            assert metrics.generated_cpp_vec_kernel_count == 1\n \n     @unittest.skipIf(\n         not codecache.valid_vec_isa_list(), \"Does not support vectorization\"\n"
        },
        {
            "name": "compile_fx.py",
            "path": "torch/_inductor/compile_fx.py",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 10,
                    "new_start": 1,
                    "new_length": 10,
                    "hunk": "@@ -1,10 +1,10 @@\n import contextlib\n import dataclasses\n import functools\n-import itertools\n import logging\n import sys\n import warnings\n+from itertools import count\n \n from typing import Any, Callable, Dict, FrozenSet, List, Optional, Sequence, Union\n from unittest import mock\n"
                },
                {
                    "old_start": 859,
                    "old_length": 7,
                    "new_start": 859,
                    "new_length": 7,
                    "hunk": "@@ -859,7 +859,7 @@ def compile_fx_aot(\n         )\n \n \n-_graph_counter = itertools.count(0)\n+_graph_counter = count(0)\n \n \n def fw_compiler_freezing(\n"
                },
                {
                    "old_start": 974,
                    "old_length": 6,
                    "new_start": 974,
                    "new_length": 14,
                    "hunk": "@@ -974,6 +974,14 @@ def compile_fx(\n                     if node.op == \"placeholder\"\n                 ]\n                 if all(v is not None for v in fake_inputs):\n+                    # Validate devices before switching to fake tensors.\n+                    for idx, fi, i in zip(count(), fake_inputs, inputs_):\n+                        if fi.device != i.device:\n+                            raise ValueError(\n+                                f\"Device mismatch between fake input and example input at position #{idx}: \"\n+                                f\"{fi.device} vs {i.device}. If the model was exported via torch.export(), \"\n+                                \"make sure torch.export() and torch.aot_compile() run on the same device.\"\n+                            )\n                     inputs_ = fake_inputs\n             return compile_fx(\n                 model_,\n"
                }
            ],
            "whole_deleted": "-import itertools\n-_graph_counter = itertools.count(0)\n",
            "whole_added": "+from itertools import count\n+_graph_counter = count(0)\n+                    # Validate devices before switching to fake tensors.\n+                    for idx, fi, i in zip(count(), fake_inputs, inputs_):\n+                        if fi.device != i.device:\n+                            raise ValueError(\n+                                f\"Device mismatch between fake input and example input at position #{idx}: \"\n+                                f\"{fi.device} vs {i.device}. If the model was exported via torch.export(), \"\n+                                \"make sure torch.export() and torch.aot_compile() run on the same device.\"\n+                            )\n",
            "whole_hunk": "@@ -1,10 +1,10 @@\n import contextlib\n import dataclasses\n import functools\n-import itertools\n import logging\n import sys\n import warnings\n+from itertools import count\n \n from typing import Any, Callable, Dict, FrozenSet, List, Optional, Sequence, Union\n from unittest import mock\n@@ -859,7 +859,7 @@ def compile_fx_aot(\n         )\n \n \n-_graph_counter = itertools.count(0)\n+_graph_counter = count(0)\n \n \n def fw_compiler_freezing(\n@@ -974,6 +974,14 @@ def compile_fx(\n                     if node.op == \"placeholder\"\n                 ]\n                 if all(v is not None for v in fake_inputs):\n+                    # Validate devices before switching to fake tensors.\n+                    for idx, fi, i in zip(count(), fake_inputs, inputs_):\n+                        if fi.device != i.device:\n+                            raise ValueError(\n+                                f\"Device mismatch between fake input and example input at position #{idx}: \"\n+                                f\"{fi.device} vs {i.device}. If the model was exported via torch.export(), \"\n+                                \"make sure torch.export() and torch.aot_compile() run on the same device.\"\n+                            )\n                     inputs_ = fake_inputs\n             return compile_fx(\n                 model_,\n"
        },
        {
            "name": "exc.py",
            "path": "torch/_inductor/exc.py",
            "patches": [
                {
                    "old_start": 67,
                    "old_length": 6,
                    "new_start": 67,
                    "new_length": 11,
                    "hunk": "@@ -67,6 +67,11 @@ class InvalidCxxCompiler(RuntimeError):\n         )\n \n \n+class CppWrapperCodeGenError(RuntimeError):\n+    def __init__(self, msg: str):\n+        super().__init__(f\"C++ wrapper codegen error: {msg}\")\n+\n+\n class CppCompileError(RuntimeError):\n     def __init__(self, cmd: list[str], output: str):\n         if isinstance(output, bytes):\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+class CppWrapperCodeGenError(RuntimeError):\n+    def __init__(self, msg: str):\n+        super().__init__(f\"C++ wrapper codegen error: {msg}\")\n+\n+\n",
            "whole_hunk": "@@ -67,6 +67,11 @@ class InvalidCxxCompiler(RuntimeError):\n         )\n \n \n+class CppWrapperCodeGenError(RuntimeError):\n+    def __init__(self, msg: str):\n+        super().__init__(f\"C++ wrapper codegen error: {msg}\")\n+\n+\n class CppCompileError(RuntimeError):\n     def __init__(self, cmd: list[str], output: str):\n         if isinstance(output, bytes):\n"
        },
        {
            "name": "graph.py",
            "path": "torch/_inductor/graph.py",
            "patches": [
                {
                    "old_start": 26,
                    "old_length": 7,
                    "new_start": 26,
                    "new_length": 7,
                    "hunk": "@@ -26,7 +26,7 @@ from torch.fx.experimental.symbolic_shapes import (\n )\n from torch.utils._mode_utils import no_dispatch\n \n-from . import config, ir, metrics\n+from . import config, ir\n from .codegen.common import (\n     get_scheduling_for_device,\n     get_wrapper_codegen_for_device,\n"
                },
                {
                    "old_start": 34,
                    "old_length": 6,
                    "new_start": 34,
                    "new_length": 7,
                    "hunk": "@@ -34,6 +34,7 @@ from .codegen.common import (\n )\n from .codegen.wrapper import CppWrapperCodeGen, CudaWrapperCodeGen, WrapperCodeGen\n from .exc import (\n+    CppWrapperCodeGenError,\n     LoweringException,\n     MissingOperatorWithDecomp,\n     MissingOperatorWithoutDecomp,\n"
                },
                {
                    "old_start": 463,
                    "old_length": 11,
                    "new_start": 464,
                    "new_length": 6,
                    "hunk": "@@ -463,11 +464,6 @@ class GraphLowering(torch.fx.Interpreter):\n     def run(self, *args):\n         return super().run(*args)\n \n-    def disable_cpp_wrapper(self, cond):\n-        metrics.disable_cpp_wrapper += 1\n-        self.cpp_wrapper = False\n-        log.debug(\"Set cpp_wrapper to False due to %s\", cond)\n-\n     def register_buffer(self, buffer: ir.ComputedBuffer):\n         name = f\"buf{len(self.buffers)}\"\n         self.buffers.append(buffer)\n"
                },
                {
                    "old_start": 716,
                    "old_length": 6,
                    "new_start": 712,
                    "new_length": 15,
                    "hunk": "@@ -716,6 +712,15 @@ class GraphLowering(torch.fx.Interpreter):\n         for buf in self.buffers:\n             buf.decide_layout()\n \n+    @contextmanager\n+    def set_current_node(self, node: torch.fx.Node):\n+        old = self.current_node\n+        try:\n+            self.current_node = node\n+            yield\n+        finally:\n+            self.current_node = old\n+\n     def run_node(self, n: torch.fx.Node):\n         log.debug(\"lowering %s\", LazyString(lambda: n.format_node()))\n         origins = {n}\n"
                },
                {
                    "old_start": 869,
                    "old_length": 15,
                    "new_start": 874,
                    "new_length": 13,
                    "hunk": "@@ -869,15 +874,13 @@ class GraphLowering(torch.fx.Interpreter):\n \n         return result\n \n-    def check_cpp_codegen_disabled(self):\n+    def validate_can_generate_cpp_wrapper(self):\n         if config.disable_cpp_codegen:\n-            self.disable_cpp_wrapper(\"cpp codegen disabled\")\n+            raise CppWrapperCodeGenError(\"C++ codegen is disabled\")\n \n-    def check_platform(self):\n         if sys.platform != \"linux\":\n-            self.disable_cpp_wrapper(\"platform not linux\")\n+            raise CppWrapperCodeGenError(f\"Unsupported platform {sys.platform}\")\n \n-    def check_input_for_cpp_buffer(self):\n         for value in self.graph_inputs.values():\n             dtype = None\n             if isinstance(value, TensorBox):\n"
                },
                {
                    "old_start": 888,
                    "old_length": 35,
                    "new_start": 891,
                    "new_length": 16,
                    "hunk": "@@ -888,35 +891,16 @@ class GraphLowering(torch.fx.Interpreter):\n                 dtype = may_get_constant_buffer_dtype(value)\n \n             if not supported_dtype_of_cpp_wrapper(dtype, self.cuda):\n-                self.disable_cpp_wrapper(\"unsupported inputs dtype\")\n-\n-    @contextmanager\n-    def set_current_node(self, node: torch.fx.Node):\n-        old = self.current_node\n-        try:\n-            self.current_node = node\n-            yield\n-        finally:\n-            self.current_node = old\n-\n-    def check_cpp_wrapper(self):\n-        self.check_cpp_codegen_disabled()\n-        self.check_platform()\n-        self.check_input_for_cpp_buffer()\n+                raise CppWrapperCodeGenError(f\"Unsupported input dtype {dtype}\")\n \n     def init_wrapper_code(self):\n         self.cuda = \"cuda\" in self.device_types\n         if self.cpp_wrapper:\n-            self.check_cpp_wrapper()\n-            # Re-check self.cpp_wrapper because it might be disabled due to failed checking\n-            if self.cuda:\n-                assert self.cpp_wrapper, \"CudaWrapperCodeGen hit unsupported case\"\n-\n-            if self.cpp_wrapper:\n-                self.wrapper_code = (\n-                    CudaWrapperCodeGen() if self.cuda else CppWrapperCodeGen()\n-                )\n-                return\n+            self.validate_can_generate_cpp_wrapper()\n+            self.wrapper_code = (\n+                CudaWrapperCodeGen() if self.cuda else CppWrapperCodeGen()\n+            )\n+            return\n \n         device_types = self.device_types.copy()\n         # In terms of some operations that don't have input tensors, we need to"
                }
            ],
            "whole_deleted": "-from . import config, ir, metrics\n-    def disable_cpp_wrapper(self, cond):\n-        metrics.disable_cpp_wrapper += 1\n-        self.cpp_wrapper = False\n-        log.debug(\"Set cpp_wrapper to False due to %s\", cond)\n-\n-    def check_cpp_codegen_disabled(self):\n-            self.disable_cpp_wrapper(\"cpp codegen disabled\")\n-    def check_platform(self):\n-            self.disable_cpp_wrapper(\"platform not linux\")\n-    def check_input_for_cpp_buffer(self):\n-                self.disable_cpp_wrapper(\"unsupported inputs dtype\")\n-\n-    @contextmanager\n-    def set_current_node(self, node: torch.fx.Node):\n-        old = self.current_node\n-        try:\n-            self.current_node = node\n-            yield\n-        finally:\n-            self.current_node = old\n-\n-    def check_cpp_wrapper(self):\n-        self.check_cpp_codegen_disabled()\n-        self.check_platform()\n-        self.check_input_for_cpp_buffer()\n-            self.check_cpp_wrapper()\n-            # Re-check self.cpp_wrapper because it might be disabled due to failed checking\n-            if self.cuda:\n-                assert self.cpp_wrapper, \"CudaWrapperCodeGen hit unsupported case\"\n-\n-            if self.cpp_wrapper:\n-                self.wrapper_code = (\n-                    CudaWrapperCodeGen() if self.cuda else CppWrapperCodeGen()\n-                )\n-                return\n",
            "whole_added": "+from . import config, ir\n+    CppWrapperCodeGenError,\n+    @contextmanager\n+    def set_current_node(self, node: torch.fx.Node):\n+        old = self.current_node\n+        try:\n+            self.current_node = node\n+            yield\n+        finally:\n+            self.current_node = old\n+\n+    def validate_can_generate_cpp_wrapper(self):\n+            raise CppWrapperCodeGenError(\"C++ codegen is disabled\")\n+            raise CppWrapperCodeGenError(f\"Unsupported platform {sys.platform}\")\n+                raise CppWrapperCodeGenError(f\"Unsupported input dtype {dtype}\")\n+            self.validate_can_generate_cpp_wrapper()\n+            self.wrapper_code = (\n+                CudaWrapperCodeGen() if self.cuda else CppWrapperCodeGen()\n+            )\n+            return\n",
            "whole_hunk": "@@ -26,7 +26,7 @@ from torch.fx.experimental.symbolic_shapes import (\n )\n from torch.utils._mode_utils import no_dispatch\n \n-from . import config, ir, metrics\n+from . import config, ir\n from .codegen.common import (\n     get_scheduling_for_device,\n     get_wrapper_codegen_for_device,\n@@ -34,6 +34,7 @@ from .codegen.common import (\n )\n from .codegen.wrapper import CppWrapperCodeGen, CudaWrapperCodeGen, WrapperCodeGen\n from .exc import (\n+    CppWrapperCodeGenError,\n     LoweringException,\n     MissingOperatorWithDecomp,\n     MissingOperatorWithoutDecomp,\n@@ -463,11 +464,6 @@ class GraphLowering(torch.fx.Interpreter):\n     def run(self, *args):\n         return super().run(*args)\n \n-    def disable_cpp_wrapper(self, cond):\n-        metrics.disable_cpp_wrapper += 1\n-        self.cpp_wrapper = False\n-        log.debug(\"Set cpp_wrapper to False due to %s\", cond)\n-\n     def register_buffer(self, buffer: ir.ComputedBuffer):\n         name = f\"buf{len(self.buffers)}\"\n         self.buffers.append(buffer)\n@@ -716,6 +712,15 @@ class GraphLowering(torch.fx.Interpreter):\n         for buf in self.buffers:\n             buf.decide_layout()\n \n+    @contextmanager\n+    def set_current_node(self, node: torch.fx.Node):\n+        old = self.current_node\n+        try:\n+            self.current_node = node\n+            yield\n+        finally:\n+            self.current_node = old\n+\n     def run_node(self, n: torch.fx.Node):\n         log.debug(\"lowering %s\", LazyString(lambda: n.format_node()))\n         origins = {n}\n@@ -869,15 +874,13 @@ class GraphLowering(torch.fx.Interpreter):\n \n         return result\n \n-    def check_cpp_codegen_disabled(self):\n+    def validate_can_generate_cpp_wrapper(self):\n         if config.disable_cpp_codegen:\n-            self.disable_cpp_wrapper(\"cpp codegen disabled\")\n+            raise CppWrapperCodeGenError(\"C++ codegen is disabled\")\n \n-    def check_platform(self):\n         if sys.platform != \"linux\":\n-            self.disable_cpp_wrapper(\"platform not linux\")\n+            raise CppWrapperCodeGenError(f\"Unsupported platform {sys.platform}\")\n \n-    def check_input_for_cpp_buffer(self):\n         for value in self.graph_inputs.values():\n             dtype = None\n             if isinstance(value, TensorBox):\n@@ -888,35 +891,16 @@ class GraphLowering(torch.fx.Interpreter):\n                 dtype = may_get_constant_buffer_dtype(value)\n \n             if not supported_dtype_of_cpp_wrapper(dtype, self.cuda):\n-                self.disable_cpp_wrapper(\"unsupported inputs dtype\")\n-\n-    @contextmanager\n-    def set_current_node(self, node: torch.fx.Node):\n-        old = self.current_node\n-        try:\n-            self.current_node = node\n-            yield\n-        finally:\n-            self.current_node = old\n-\n-    def check_cpp_wrapper(self):\n-        self.check_cpp_codegen_disabled()\n-        self.check_platform()\n-        self.check_input_for_cpp_buffer()\n+                raise CppWrapperCodeGenError(f\"Unsupported input dtype {dtype}\")\n \n     def init_wrapper_code(self):\n         self.cuda = \"cuda\" in self.device_types\n         if self.cpp_wrapper:\n-            self.check_cpp_wrapper()\n-            # Re-check self.cpp_wrapper because it might be disabled due to failed checking\n-            if self.cuda:\n-                assert self.cpp_wrapper, \"CudaWrapperCodeGen hit unsupported case\"\n-\n-            if self.cpp_wrapper:\n-                self.wrapper_code = (\n-                    CudaWrapperCodeGen() if self.cuda else CppWrapperCodeGen()\n-                )\n-                return\n+            self.validate_can_generate_cpp_wrapper()\n+            self.wrapper_code = (\n+                CudaWrapperCodeGen() if self.cuda else CppWrapperCodeGen()\n+            )\n+            return\n \n         device_types = self.device_types.copy()\n         # In terms of some operations that don't have input tensors, we need to"
        }
    ]
},
{
    "Id": 511,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/7564f04389478df3fce478547dbee0dce9434f1a",
    "date": "2023-09-20T06:33:37+00:00",
    "message": "[generate_opcheck_tests] add type checking (#109638)\n\nTest Plan:\n- lintrunner\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109638\nApproved by: https://github.com/bdhirsh, https://github.com/soulitzer\nghstack dependencies: #109637",
    "label": "NO",
    "changes": [
        {
            "name": "generate_tests.py",
            "path": "torch/testing/_internal/optests/generate_tests.py",
            "patches": [
                {
                    "old_start": 4,
                    "old_length": 6,
                    "new_start": 4,
                    "new_length": 7,
                    "hunk": "@@ -4,6 +4,7 @@ import functools\n import json\n import os\n import tempfile\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n \n import torch\n \n"
                },
                {
                    "old_start": 20,
                    "old_length": 14,
                    "new_start": 21,
                    "new_length": 18,
                    "hunk": "@@ -20,14 +21,18 @@ from torch.testing._internal.optests import (\n )\n \n \n-def safe_schema_check(op, args, kwargs):\n+def safe_schema_check(\n+    op: torch._ops.OpOverload, args: Tuple[Any, ...], kwargs: Dict[str, Any]\n+) -> Any:\n     args, kwargs = deepcopy_tensors((args, kwargs))\n     with SchemaCheckMode():\n         result = op(*args, **kwargs)\n         return result\n \n \n-def safe_autograd_registration_check(op, args, kwargs):\n+def safe_autograd_registration_check(\n+    op: torch._ops.OpOverload, args: Tuple[Any, ...], kwargs: Dict[str, Any]\n+) -> None:\n     # Don't perform autograd_registration_check if none of the inputs require grad.\n     if not pytree.tree_any_only(\n         torch.Tensor, lambda x: x.requires_grad, (args, kwargs)\n"
                },
                {
                    "old_start": 37,
                    "old_length": 12,
                    "new_start": 42,
                    "new_length": 19,
                    "hunk": "@@ -37,12 +42,19 @@ def safe_autograd_registration_check(op, args, kwargs):\n     return autograd_registration_check(op, args, kwargs)\n \n \n-def safe_fake_check(op, args, kwargs):\n+def safe_fake_check(\n+    op: torch._ops.OpOverload, args: Tuple[Any, ...], kwargs: Dict[str, Any]\n+) -> None:\n     args, kwargs = deepcopy_tensors((args, kwargs))\n     return fake_check(op, args, kwargs)\n \n \n-def safe_aot_autograd_check(op, args, kwargs, dynamic):\n+def safe_aot_autograd_check(\n+    op: torch._ops.OpOverload,\n+    args: Tuple[Any, ...],\n+    kwargs: Dict[str, Any],\n+    dynamic: bool,\n+) -> Any:\n     def func(*args, **kwargs):\n         args, kwargs = pytree.tree_map_only(torch.Tensor, torch.clone, (args, kwargs))\n         return op(*args, **kwargs)\n"
                },
                {
                    "old_start": 52,
                    "old_length": 7,
                    "new_start": 64,
                    "new_length": 7,
                    "hunk": "@@ -52,7 +64,7 @@ def safe_aot_autograd_check(op, args, kwargs, dynamic):\n     return aot_autograd_check(func, args, kwargs, dynamic, check_gradients=\"auto\")\n \n \n-def deepcopy_tensors(inputs):\n+def deepcopy_tensors(inputs: Any) -> Any:\n     return pytree.tree_map_only(torch.Tensor, clone_input, inputs)\n \n \n"
                },
                {
                    "old_start": 81,
                    "old_length": 12,
                    "new_start": 93,
                    "new_length": 12,
                    "hunk": "@@ -81,12 +93,12 @@ GDOC = \"https://docs.google.com/document/d/1Pj5HRZvdOq3xpFpbEjUZp2hBovhy7Wnxw14m\n \n \n def generate_opcheck_tests(\n-    testcase,\n-    namespaces,\n-    failures_dict_path,\n-    additional_decorators,\n-    test_utils,\n-):\n+    testcase: Any,\n+    namespaces: List[str],\n+    failures_dict_path: str,\n+    additional_decorators: List[Callable],\n+    test_utils: List[str],\n+) -> None:\n     \"\"\"Given an existing TestCase, use the existing tests to generate\n     additional validation tests for custom operators.\n \n"
                },
                {
                    "old_start": 196,
                    "old_length": 7,
                    "new_start": 208,
                    "new_length": 7,
                    "hunk": "@@ -196,7 +208,7 @@ def generate_opcheck_tests(\n TEST_OPTIONS = (\"xfail\", \"skip\", \"xsuccess\")\n \n \n-def validate_failures_dict_formatting(failures_dict_path):\n+def validate_failures_dict_formatting(failures_dict_path: str) -> None:\n     with open(failures_dict_path) as fp:\n         actual = fp.read()\n     failures_dict = FailuresDict.load(failures_dict_path)\n"
                },
                {
                    "old_start": 219,
                    "old_length": 7,
                    "new_start": 231,
                    "new_length": 9,
                    "hunk": "@@ -219,7 +231,9 @@ def validate_failures_dict_formatting(failures_dict_path):\n     )\n \n \n-def validate_failures_dict_structure(failure_dict, test_utils, testcase):\n+def validate_failures_dict_structure(\n+    failure_dict: \"FailuresDict\", test_utils: List[str], testcase: Any\n+) -> None:\n     \"\"\"Validates the failures dict.\n \n     The failure dict looks something like the following.\n"
                },
                {
                    "old_start": 280,
                    "old_length": 7,
                    "new_start": 294,
                    "new_length": 7,
                    "hunk": "@@ -280,7 +294,7 @@ def validate_failures_dict_structure(failure_dict, test_utils, testcase):\n                 )\n \n \n-def should_update_failures_dict():\n+def should_update_failures_dict() -> bool:\n     key = \"PYTORCH_OPCHECK_ACCEPT\"\n     return key in os.environ and os.environ[key] == \"1\"\n \n"
                },
                {
                    "old_start": 293,
                    "old_length": 12,
                    "new_start": 307,
                    "new_length": 12,
                    "hunk": "@@ -293,12 +307,12 @@ class OpCheckMode(TorchFunctionMode):\n \n     def __init__(\n         self,\n-        namespaces,\n-        test_util_name,\n-        test_util,\n-        failures_dict,\n-        test_name,\n-        failures_dict_path,\n+        namespaces: List[str],\n+        test_util_name: str,\n+        test_util: Callable,\n+        failures_dict: \"FailuresDict\",\n+        test_name: str,\n+        failures_dict_path: str,\n     ):\n         # We will intercept calls to ops with these namespaces\n         self.namespaces = namespaces\n"
                },
                {
                    "old_start": 318,
                    "old_length": 7,
                    "new_start": 332,
                    "new_length": 7,
                    "hunk": "@@ -318,7 +332,7 @@ class OpCheckMode(TorchFunctionMode):\n         # Maps qualname -> List[exception]\n         self.seen_ops_to_errors = {}\n \n-    def maybe_raise_errors_on_exit(self):\n+    def maybe_raise_errors_on_exit(self) -> None:\n         # Check expected failures first\n         for qualname in self.seen_ops_to_errors.keys():\n             option = self.failures_dict.get_status(qualname, self.test_name)\n"
                },
                {
                    "old_start": 440,
                    "old_length": 7,
                    "new_start": 454,
                    "new_length": 7,
                    "hunk": "@@ -440,7 +454,7 @@ class OpCheckMode(TorchFunctionMode):\n         return result\n \n \n-def should_print_repro():\n+def should_print_repro() -> None:\n     \"\"\"If set, the tests generated by `generate_opcheck_tests` will print a\n     repro command on failure.\n \n"
                },
                {
                    "old_start": 458,
                    "old_length": 7,
                    "new_start": 472,
                    "new_length": 14,
                    "hunk": "@@ -458,7 +472,14 @@ def should_print_repro():\n     return value == \"1\" or value == 1\n \n \n-def opcheck(op, args, kwargs=None, *, test_utils=\"ALL\", raise_exception=True):\n+def opcheck(\n+    op: torch._ops.OperatorBase,\n+    args: Tuple[Any, ...],\n+    kwargs: Optional[Dict[str, Any]] = None,\n+    *,\n+    test_utils: Union[str, List[str]] = \"ALL\",\n+    raise_exception: bool = True,\n+) -> Dict[str, str]:\n     \"\"\"Given an operator and some sample arguments, tests if the operator is\n     registered correctly.\n \n"
                },
                {
                    "old_start": 533,
                    "old_length": 7,
                    "new_start": 554,
                    "new_length": 12,
                    "hunk": "@@ -533,7 +554,12 @@ class OpCheckError(Exception):\n     pass\n \n \n-def generate_repro(test, op, args, kwargs):\n+def generate_repro(\n+    test: str,\n+    op: torch._ops.OpOverload,\n+    args: Tuple[Any, ...],\n+    kwargs: Dict[str, Any],\n+) -> str:\n     now = datetime.datetime.now()\n     unix_timestamp = datetime.datetime.timestamp(now) * 1000\n     path = os.path.join(tempfile.gettempdir(), \"pytorch_opcheck_safe_to_delete\")\n"
                },
                {
                    "old_start": 557,
                    "old_length": 7,
                    "new_start": 583,
                    "new_length": 9,
                    "hunk": "@@ -557,7 +583,9 @@ def generate_repro(test, op, args, kwargs):\n     return repro_command\n \n \n-def resolve_unique_overload_or_throw(op: torch._ops.OpOverloadPacket):\n+def resolve_unique_overload_or_throw(\n+    op: torch._ops.OpOverloadPacket,\n+) -> torch._ops.OpOverload:\n     all_schemas = torch._C._jit_get_schemas_for_operator(op._qualified_op_name)\n     if len(all_schemas) != 1:\n         raise RuntimeError(\n"
                },
                {
                    "old_start": 575,
                    "old_length": 13,
                    "new_start": 603,
                    "new_length": 16,
                    "hunk": "@@ -575,13 +603,16 @@ def resolve_unique_overload_or_throw(op: torch._ops.OpOverloadPacket):\n DUMP_OPTIONS = {\"indent\": 2, \"sort_keys\": True}\n \n \n+FailuresDictData = Dict[str, Dict[str, Dict[str, str]]]\n+\n+\n class FailuresDict:\n-    def __init__(self, path, data):\n+    def __init__(self, path: str, data: FailuresDictData):\n         self.path = path\n         self.data = data\n \n     @staticmethod\n-    def load(path, *, create_file=False):\n+    def load(path, *, create_file=False) -> \"FailuresDict\":\n         if create_file and not os.path.exists(path):\n             result = FailuresDict(path, {})\n             FailuresDict.save()\n"
                },
                {
                    "old_start": 592,
                    "old_length": 7,
                    "new_start": 623,
                    "new_length": 7,
                    "hunk": "@@ -592,7 +623,7 @@ class FailuresDict:\n         assert \"_version\" in dct and dct[\"_version\"] == 1\n         return FailuresDict(path, dct[\"data\"])\n \n-    def _save(self, to_str=False):\n+    def _save(self, to_str=False) -> Optional[str]:\n         to_dump = {\n             \"_description\": (\n                 f\"This is a dict containing failures for tests autogenerated by \"\n"
                },
                {
                    "old_start": 611,
                    "old_length": 10,
                    "new_start": 642,
                    "new_length": 10,
                    "hunk": "@@ -611,10 +642,10 @@ class FailuresDict:\n             fp.write(serialized)\n         return None\n \n-    def save(self):\n+    def save(self) -> None:\n         return self._save()\n \n-    def get_status(self, qualname, test_name):\n+    def get_status(self, qualname: str, test_name: str) -> str:\n         if qualname not in self.data:\n             return \"xsuccess\"\n         dct = self.data[qualname]\n"
                },
                {
                    "old_start": 622,
                    "old_length": 7,
                    "new_start": 653,
                    "new_length": 14,
                    "hunk": "@@ -622,7 +653,14 @@ class FailuresDict:\n             return \"xsuccess\"\n         return dct[test_name][\"status\"]\n \n-    def set_status(self, qualname, test_name, status, *, comment=None):\n+    def set_status(\n+        self,\n+        qualname: str,\n+        test_name: str,\n+        status: str,\n+        *,\n+        comment: Optional[str] = None,\n+    ):\n         if qualname not in self.data:\n             self.data[qualname] = {}\n         dct = self.data[qualname]"
                }
            ],
            "whole_deleted": "-def safe_schema_check(op, args, kwargs):\n-def safe_autograd_registration_check(op, args, kwargs):\n-def safe_fake_check(op, args, kwargs):\n-def safe_aot_autograd_check(op, args, kwargs, dynamic):\n-def deepcopy_tensors(inputs):\n-    testcase,\n-    namespaces,\n-    failures_dict_path,\n-    additional_decorators,\n-    test_utils,\n-):\n-def validate_failures_dict_formatting(failures_dict_path):\n-def validate_failures_dict_structure(failure_dict, test_utils, testcase):\n-def should_update_failures_dict():\n-        namespaces,\n-        test_util_name,\n-        test_util,\n-        failures_dict,\n-        test_name,\n-        failures_dict_path,\n-    def maybe_raise_errors_on_exit(self):\n-def should_print_repro():\n-def opcheck(op, args, kwargs=None, *, test_utils=\"ALL\", raise_exception=True):\n-def generate_repro(test, op, args, kwargs):\n-def resolve_unique_overload_or_throw(op: torch._ops.OpOverloadPacket):\n-    def __init__(self, path, data):\n-    def load(path, *, create_file=False):\n-    def _save(self, to_str=False):\n-    def save(self):\n-    def get_status(self, qualname, test_name):\n-    def set_status(self, qualname, test_name, status, *, comment=None):\n",
            "whole_added": "+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n+def safe_schema_check(\n+    op: torch._ops.OpOverload, args: Tuple[Any, ...], kwargs: Dict[str, Any]\n+) -> Any:\n+def safe_autograd_registration_check(\n+    op: torch._ops.OpOverload, args: Tuple[Any, ...], kwargs: Dict[str, Any]\n+) -> None:\n+def safe_fake_check(\n+    op: torch._ops.OpOverload, args: Tuple[Any, ...], kwargs: Dict[str, Any]\n+) -> None:\n+def safe_aot_autograd_check(\n+    op: torch._ops.OpOverload,\n+    args: Tuple[Any, ...],\n+    kwargs: Dict[str, Any],\n+    dynamic: bool,\n+) -> Any:\n+def deepcopy_tensors(inputs: Any) -> Any:\n+    testcase: Any,\n+    namespaces: List[str],\n+    failures_dict_path: str,\n+    additional_decorators: List[Callable],\n+    test_utils: List[str],\n+) -> None:\n+def validate_failures_dict_formatting(failures_dict_path: str) -> None:\n+def validate_failures_dict_structure(\n+    failure_dict: \"FailuresDict\", test_utils: List[str], testcase: Any\n+) -> None:\n+def should_update_failures_dict() -> bool:\n+        namespaces: List[str],\n+        test_util_name: str,\n+        test_util: Callable,\n+        failures_dict: \"FailuresDict\",\n+        test_name: str,\n+        failures_dict_path: str,\n+    def maybe_raise_errors_on_exit(self) -> None:\n+def should_print_repro() -> None:\n+def opcheck(\n+    op: torch._ops.OperatorBase,\n+    args: Tuple[Any, ...],\n+    kwargs: Optional[Dict[str, Any]] = None,\n+    *,\n+    test_utils: Union[str, List[str]] = \"ALL\",\n+    raise_exception: bool = True,\n+) -> Dict[str, str]:\n+def generate_repro(\n+    test: str,\n+    op: torch._ops.OpOverload,\n+    args: Tuple[Any, ...],\n+    kwargs: Dict[str, Any],\n+) -> str:\n+def resolve_unique_overload_or_throw(\n+    op: torch._ops.OpOverloadPacket,\n+) -> torch._ops.OpOverload:\n+FailuresDictData = Dict[str, Dict[str, Dict[str, str]]]\n+\n+\n+    def __init__(self, path: str, data: FailuresDictData):\n+    def load(path, *, create_file=False) -> \"FailuresDict\":\n+    def _save(self, to_str=False) -> Optional[str]:\n+    def save(self) -> None:\n+    def get_status(self, qualname: str, test_name: str) -> str:\n+    def set_status(\n+        self,\n+        qualname: str,\n+        test_name: str,\n+        status: str,\n+        *,\n+        comment: Optional[str] = None,\n+    ):\n",
            "whole_hunk": "@@ -4,6 +4,7 @@ import functools\n import json\n import os\n import tempfile\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n \n import torch\n \n@@ -20,14 +21,18 @@ from torch.testing._internal.optests import (\n )\n \n \n-def safe_schema_check(op, args, kwargs):\n+def safe_schema_check(\n+    op: torch._ops.OpOverload, args: Tuple[Any, ...], kwargs: Dict[str, Any]\n+) -> Any:\n     args, kwargs = deepcopy_tensors((args, kwargs))\n     with SchemaCheckMode():\n         result = op(*args, **kwargs)\n         return result\n \n \n-def safe_autograd_registration_check(op, args, kwargs):\n+def safe_autograd_registration_check(\n+    op: torch._ops.OpOverload, args: Tuple[Any, ...], kwargs: Dict[str, Any]\n+) -> None:\n     # Don't perform autograd_registration_check if none of the inputs require grad.\n     if not pytree.tree_any_only(\n         torch.Tensor, lambda x: x.requires_grad, (args, kwargs)\n@@ -37,12 +42,19 @@ def safe_autograd_registration_check(op, args, kwargs):\n     return autograd_registration_check(op, args, kwargs)\n \n \n-def safe_fake_check(op, args, kwargs):\n+def safe_fake_check(\n+    op: torch._ops.OpOverload, args: Tuple[Any, ...], kwargs: Dict[str, Any]\n+) -> None:\n     args, kwargs = deepcopy_tensors((args, kwargs))\n     return fake_check(op, args, kwargs)\n \n \n-def safe_aot_autograd_check(op, args, kwargs, dynamic):\n+def safe_aot_autograd_check(\n+    op: torch._ops.OpOverload,\n+    args: Tuple[Any, ...],\n+    kwargs: Dict[str, Any],\n+    dynamic: bool,\n+) -> Any:\n     def func(*args, **kwargs):\n         args, kwargs = pytree.tree_map_only(torch.Tensor, torch.clone, (args, kwargs))\n         return op(*args, **kwargs)\n@@ -52,7 +64,7 @@ def safe_aot_autograd_check(op, args, kwargs, dynamic):\n     return aot_autograd_check(func, args, kwargs, dynamic, check_gradients=\"auto\")\n \n \n-def deepcopy_tensors(inputs):\n+def deepcopy_tensors(inputs: Any) -> Any:\n     return pytree.tree_map_only(torch.Tensor, clone_input, inputs)\n \n \n@@ -81,12 +93,12 @@ GDOC = \"https://docs.google.com/document/d/1Pj5HRZvdOq3xpFpbEjUZp2hBovhy7Wnxw14m\n \n \n def generate_opcheck_tests(\n-    testcase,\n-    namespaces,\n-    failures_dict_path,\n-    additional_decorators,\n-    test_utils,\n-):\n+    testcase: Any,\n+    namespaces: List[str],\n+    failures_dict_path: str,\n+    additional_decorators: List[Callable],\n+    test_utils: List[str],\n+) -> None:\n     \"\"\"Given an existing TestCase, use the existing tests to generate\n     additional validation tests for custom operators.\n \n@@ -196,7 +208,7 @@ def generate_opcheck_tests(\n TEST_OPTIONS = (\"xfail\", \"skip\", \"xsuccess\")\n \n \n-def validate_failures_dict_formatting(failures_dict_path):\n+def validate_failures_dict_formatting(failures_dict_path: str) -> None:\n     with open(failures_dict_path) as fp:\n         actual = fp.read()\n     failures_dict = FailuresDict.load(failures_dict_path)\n@@ -219,7 +231,9 @@ def validate_failures_dict_formatting(failures_dict_path):\n     )\n \n \n-def validate_failures_dict_structure(failure_dict, test_utils, testcase):\n+def validate_failures_dict_structure(\n+    failure_dict: \"FailuresDict\", test_utils: List[str], testcase: Any\n+) -> None:\n     \"\"\"Validates the failures dict.\n \n     The failure dict looks something like the following.\n@@ -280,7 +294,7 @@ def validate_failures_dict_structure(failure_dict, test_utils, testcase):\n                 )\n \n \n-def should_update_failures_dict():\n+def should_update_failures_dict() -> bool:\n     key = \"PYTORCH_OPCHECK_ACCEPT\"\n     return key in os.environ and os.environ[key] == \"1\"\n \n@@ -293,12 +307,12 @@ class OpCheckMode(TorchFunctionMode):\n \n     def __init__(\n         self,\n-        namespaces,\n-        test_util_name,\n-        test_util,\n-        failures_dict,\n-        test_name,\n-        failures_dict_path,\n+        namespaces: List[str],\n+        test_util_name: str,\n+        test_util: Callable,\n+        failures_dict: \"FailuresDict\",\n+        test_name: str,\n+        failures_dict_path: str,\n     ):\n         # We will intercept calls to ops with these namespaces\n         self.namespaces = namespaces\n@@ -318,7 +332,7 @@ class OpCheckMode(TorchFunctionMode):\n         # Maps qualname -> List[exception]\n         self.seen_ops_to_errors = {}\n \n-    def maybe_raise_errors_on_exit(self):\n+    def maybe_raise_errors_on_exit(self) -> None:\n         # Check expected failures first\n         for qualname in self.seen_ops_to_errors.keys():\n             option = self.failures_dict.get_status(qualname, self.test_name)\n@@ -440,7 +454,7 @@ class OpCheckMode(TorchFunctionMode):\n         return result\n \n \n-def should_print_repro():\n+def should_print_repro() -> None:\n     \"\"\"If set, the tests generated by `generate_opcheck_tests` will print a\n     repro command on failure.\n \n@@ -458,7 +472,14 @@ def should_print_repro():\n     return value == \"1\" or value == 1\n \n \n-def opcheck(op, args, kwargs=None, *, test_utils=\"ALL\", raise_exception=True):\n+def opcheck(\n+    op: torch._ops.OperatorBase,\n+    args: Tuple[Any, ...],\n+    kwargs: Optional[Dict[str, Any]] = None,\n+    *,\n+    test_utils: Union[str, List[str]] = \"ALL\",\n+    raise_exception: bool = True,\n+) -> Dict[str, str]:\n     \"\"\"Given an operator and some sample arguments, tests if the operator is\n     registered correctly.\n \n@@ -533,7 +554,12 @@ class OpCheckError(Exception):\n     pass\n \n \n-def generate_repro(test, op, args, kwargs):\n+def generate_repro(\n+    test: str,\n+    op: torch._ops.OpOverload,\n+    args: Tuple[Any, ...],\n+    kwargs: Dict[str, Any],\n+) -> str:\n     now = datetime.datetime.now()\n     unix_timestamp = datetime.datetime.timestamp(now) * 1000\n     path = os.path.join(tempfile.gettempdir(), \"pytorch_opcheck_safe_to_delete\")\n@@ -557,7 +583,9 @@ def generate_repro(test, op, args, kwargs):\n     return repro_command\n \n \n-def resolve_unique_overload_or_throw(op: torch._ops.OpOverloadPacket):\n+def resolve_unique_overload_or_throw(\n+    op: torch._ops.OpOverloadPacket,\n+) -> torch._ops.OpOverload:\n     all_schemas = torch._C._jit_get_schemas_for_operator(op._qualified_op_name)\n     if len(all_schemas) != 1:\n         raise RuntimeError(\n@@ -575,13 +603,16 @@ def resolve_unique_overload_or_throw(op: torch._ops.OpOverloadPacket):\n DUMP_OPTIONS = {\"indent\": 2, \"sort_keys\": True}\n \n \n+FailuresDictData = Dict[str, Dict[str, Dict[str, str]]]\n+\n+\n class FailuresDict:\n-    def __init__(self, path, data):\n+    def __init__(self, path: str, data: FailuresDictData):\n         self.path = path\n         self.data = data\n \n     @staticmethod\n-    def load(path, *, create_file=False):\n+    def load(path, *, create_file=False) -> \"FailuresDict\":\n         if create_file and not os.path.exists(path):\n             result = FailuresDict(path, {})\n             FailuresDict.save()\n@@ -592,7 +623,7 @@ class FailuresDict:\n         assert \"_version\" in dct and dct[\"_version\"] == 1\n         return FailuresDict(path, dct[\"data\"])\n \n-    def _save(self, to_str=False):\n+    def _save(self, to_str=False) -> Optional[str]:\n         to_dump = {\n             \"_description\": (\n                 f\"This is a dict containing failures for tests autogenerated by \"\n@@ -611,10 +642,10 @@ class FailuresDict:\n             fp.write(serialized)\n         return None\n \n-    def save(self):\n+    def save(self) -> None:\n         return self._save()\n \n-    def get_status(self, qualname, test_name):\n+    def get_status(self, qualname: str, test_name: str) -> str:\n         if qualname not in self.data:\n             return \"xsuccess\"\n         dct = self.data[qualname]\n@@ -622,7 +653,14 @@ class FailuresDict:\n             return \"xsuccess\"\n         return dct[test_name][\"status\"]\n \n-    def set_status(self, qualname, test_name, status, *, comment=None):\n+    def set_status(\n+        self,\n+        qualname: str,\n+        test_name: str,\n+        status: str,\n+        *,\n+        comment: Optional[str] = None,\n+    ):\n         if qualname not in self.data:\n             self.data[qualname] = {}\n         dct = self.data[qualname]"
        }
    ]
},
{
    "Id": 72,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/73ba226d986711e2474da52ec67bf9719c902611",
    "date": "2024-06-22T12:38:17+00:00",
    "message": "[inductor] Linear time dead node elimination (#129082)\n\nThe nodes are already topologically sorted by this point, so DCEing a chain of\nnodes will take one full iteration per node. Simply reversing the iteration\norder means all users will be removed before checking a node.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/129082\nApproved by: https://github.com/lezcano",
    "label": "NO",
    "changes": [
        {
            "name": "scheduler.py",
            "path": "torch/_inductor/scheduler.py",
            "patches": [
                {
                    "old_start": 1767,
                    "old_length": 27,
                    "new_start": 1767,
                    "new_length": 27,
                    "hunk": "@@ -1767,27 +1767,27 @@ class Scheduler:\n         \"\"\"\n         Remove any nodes without users\n         \"\"\"\n-        again = True  # repeat until a fixed point\n-        while again:\n-            updated_nodes = []\n-            for node in self.nodes:\n+        # self.nodes is in topological order, so by iterating in reverse order\n+        # we have visited (and potentially removed) all users before visiting a\n+        # given node.\n+        updated_nodes = []\n+        for node in reversed(self.nodes):\n \n-                def can_eliminate_user(user: NodeUser) -> bool:\n-                    return user.is_weak or user.get_name() in V.graph.removed_buffers\n+            def can_eliminate_user(user: NodeUser) -> bool:\n+                return user.is_weak or user.get_name() in V.graph.removed_buffers\n \n-                can_eliminate = not node.has_side_effects() and all(\n-                    can_eliminate_user(u) for u in node.users\n-                )\n+            can_eliminate = not node.has_side_effects() and all(\n+                can_eliminate_user(u) for u in node.users\n+            )\n \n-                if not can_eliminate:\n-                    updated_nodes.append(node)\n-                else:\n-                    # dead code\n-                    log.debug(\"removed dead node: %s\", node.get_name())\n-                    V.graph.removed_buffers.add(node.get_name())\n+            if not can_eliminate:\n+                updated_nodes.append(node)\n+            else:\n+                # dead code\n+                log.debug(\"removed dead node: %s\", node.get_name())\n+                V.graph.removed_buffers.add(node.get_name())\n \n-            again = len(self.nodes) > len(updated_nodes)\n-            self.nodes = updated_nodes\n+        self.nodes = list(reversed(updated_nodes))\n \n         # Prune any WeakDeps no longer needed\n         for node in self.nodes:"
                }
            ],
            "whole_deleted": "-        again = True  # repeat until a fixed point\n-        while again:\n-            updated_nodes = []\n-            for node in self.nodes:\n-                def can_eliminate_user(user: NodeUser) -> bool:\n-                    return user.is_weak or user.get_name() in V.graph.removed_buffers\n-                can_eliminate = not node.has_side_effects() and all(\n-                    can_eliminate_user(u) for u in node.users\n-                )\n-                if not can_eliminate:\n-                    updated_nodes.append(node)\n-                else:\n-                    # dead code\n-                    log.debug(\"removed dead node: %s\", node.get_name())\n-                    V.graph.removed_buffers.add(node.get_name())\n-            again = len(self.nodes) > len(updated_nodes)\n-            self.nodes = updated_nodes\n",
            "whole_added": "+        # self.nodes is in topological order, so by iterating in reverse order\n+        # we have visited (and potentially removed) all users before visiting a\n+        # given node.\n+        updated_nodes = []\n+        for node in reversed(self.nodes):\n+            def can_eliminate_user(user: NodeUser) -> bool:\n+                return user.is_weak or user.get_name() in V.graph.removed_buffers\n+            can_eliminate = not node.has_side_effects() and all(\n+                can_eliminate_user(u) for u in node.users\n+            )\n+            if not can_eliminate:\n+                updated_nodes.append(node)\n+            else:\n+                # dead code\n+                log.debug(\"removed dead node: %s\", node.get_name())\n+                V.graph.removed_buffers.add(node.get_name())\n+        self.nodes = list(reversed(updated_nodes))\n",
            "whole_hunk": "@@ -1767,27 +1767,27 @@ class Scheduler:\n         \"\"\"\n         Remove any nodes without users\n         \"\"\"\n-        again = True  # repeat until a fixed point\n-        while again:\n-            updated_nodes = []\n-            for node in self.nodes:\n+        # self.nodes is in topological order, so by iterating in reverse order\n+        # we have visited (and potentially removed) all users before visiting a\n+        # given node.\n+        updated_nodes = []\n+        for node in reversed(self.nodes):\n \n-                def can_eliminate_user(user: NodeUser) -> bool:\n-                    return user.is_weak or user.get_name() in V.graph.removed_buffers\n+            def can_eliminate_user(user: NodeUser) -> bool:\n+                return user.is_weak or user.get_name() in V.graph.removed_buffers\n \n-                can_eliminate = not node.has_side_effects() and all(\n-                    can_eliminate_user(u) for u in node.users\n-                )\n+            can_eliminate = not node.has_side_effects() and all(\n+                can_eliminate_user(u) for u in node.users\n+            )\n \n-                if not can_eliminate:\n-                    updated_nodes.append(node)\n-                else:\n-                    # dead code\n-                    log.debug(\"removed dead node: %s\", node.get_name())\n-                    V.graph.removed_buffers.add(node.get_name())\n+            if not can_eliminate:\n+                updated_nodes.append(node)\n+            else:\n+                # dead code\n+                log.debug(\"removed dead node: %s\", node.get_name())\n+                V.graph.removed_buffers.add(node.get_name())\n \n-            again = len(self.nodes) > len(updated_nodes)\n-            self.nodes = updated_nodes\n+        self.nodes = list(reversed(updated_nodes))\n \n         # Prune any WeakDeps no longer needed\n         for node in self.nodes:"
        }
    ]
},
{
    "Id": 148,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/2973c9bb884df8f3d7e846738ee6679fb022609e",
    "date": "2024-05-14T21:07:21+00:00",
    "message": "[export] add SchemaCheckMode testing for pre-dispatch export, OpInfo (#125481)\n\nThis adds a new dispatch mode, PreDispatchSchemaCheckMode, built on top of SchemaCheckMode, used for verifying op schemas for functionalization for PreDispatch IR. More specifically, the mode runs in eager mode on concrete inputs, checking if op schemas incorrectly claim to be functional, but are aliasing or mutating. This mode is pushed to the pre-dispatch mode stack, and run before decompositions.\n\nCurrent testing is hooked up to OpInfo, containing 1103 tests on 600 unique ops. Below is a list of ops that fail testing. One caveat is we only raise errors on ops that claim to be functional - if an op schema admits aliasing or mutating but fails testing for the other, it still may decompose further and become functional.\n\nList of failed ops:\n```\naten.atleast_1d.default\naten.atleast_2d.default\naten.atleast_3d.default\naten.cartesian_prod.default\naten.conj_physical.default\naten.alpha_dropout.default\naten.feature_dropout.default\naten.feature_alpha_dropout.default\naten.unsafe_chunk.default\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125481\nApproved by: https://github.com/tugsbayasgalan",
    "label": "YES",
    "changes": [
        {
            "name": "opinfo_schema.py",
            "path": "test/export/opinfo_schema.py",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 108,
                    "hunk": "@@ -0,0 +1,108 @@\n+# Owner(s): [\"oncall: export\"]\n+\n+import torch\n+from torch._dispatch.python import enable_python_dispatcher\n+from torch._subclasses.schema_check_mode import SchemaCheckMode\n+from torch.fx.operator_schemas import normalize_function\n+from torch.testing._internal.common_device_type import (\n+    instantiate_device_type_tests,\n+    ops,\n+)\n+from torch.testing._internal.common_methods_invocations import op_db\n+from torch.testing._internal.common_utils import TestCase\n+from torch.utils._pytree import tree_map\n+\n+# Simplified naming for C++ classes\n+SchemaArgument = torch._C._SchemaArgument\n+SchemaArgType = torch._C._SchemaArgType\n+SchemaInfo = torch._C._SchemaInfo\n+\n+test_classes = {}\n+\n+\n+class PreDispatchSchemaCheckMode(SchemaCheckMode):\n+    \"\"\"\n+    Dispatch mode built on top of SchemaCheckMode that checks for incorrect op schemas\n+    for PreDispatch IR. This is meant to run ops in eager mode on concrete inputs, to\n+    see if they incorrectly claim to be functional (aliasing or mutating).\n+\n+    If an op is claimed to be functional and either is detected, an error is raised.\n+    Errors will be silenced if the schema admits aliasing or mutation - the op may\n+    later decompose and become functional.\n+    \"\"\"\n+\n+    def __init__(self):\n+        self._dispatch_key = torch._C.DispatchKey.PreDispatch\n+        super().__init__()\n+\n+    def _may_alias_or_mutate(self, func, types, args, kwargs):\n+        def unwrap(e):\n+            if isinstance(e, torch.Tensor) and not type(e) == torch.Tensor:\n+                try:\n+                    return e.elem\n+                except AttributeError as t:\n+                    return e\n+            return e\n+\n+        # get arguments, outputs\n+        schema_info = SchemaInfo(func._schema)\n+        pre_arguments = normalize_function(\n+            func, args, kwargs, normalize_to_only_use_kwargs=True\n+        ).kwargs\n+        schema_info.add_argument_values(pre_arguments)\n+        out = func(*args, **kwargs)\n+        tuple_out = out if isinstance(out, tuple) else (out,)\n+        tuple_out = tree_map(unwrap, tuple_out)\n+\n+        # check schema\n+        for i in range(len(func._schema.arguments)):\n+            for j in range(len(tuple_out)):\n+                if schema_info.may_contain_alias(\n+                    SchemaArgument(SchemaArgType.output, j),\n+                    SchemaArgument(SchemaArgType.input, i),\n+                ):\n+                    return True\n+            if schema_info.is_mutable(\n+                SchemaArgument(SchemaArgType.input, i),\n+            ):\n+                return True\n+\n+        return False\n+\n+    # creating this just so we have access to the offending op\n+    def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n+        try:\n+            return super().__torch_dispatch__(func, types, args=args, kwargs=kwargs)\n+        except RuntimeError as e:\n+            # check if schema claims to be either aliasing or mutating\n+            alias_or_mutate = self._may_alias_or_mutate(func, types, args, kwargs)\n+            if (\n+                not alias_or_mutate\n+            ):  # if schema is aliasing or mutating, will decompose further\n+                msg = e.args[0]\n+                e.args = (\n+                    f\"\"\"SchemaCheckMode failed with the following error on op <{func}>, meaning\n+    this op contains aliasing or mutations, despite claiming to be functional:\\n\\n\"\"\"\n+                    + msg,\n+                )\n+                raise e\n+\n+\n+class TestOpInfo(TestCase):\n+    @ops(op_db, allowed_dtypes=(torch.float, torch.int))\n+    def test_schema_check_op(self, device, dtype, op):\n+        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)\n+        inputs = next(sample_inputs_itr)\n+        args = [inputs.input] + list(inputs.args)\n+        kwargs = inputs.kwargs\n+        with enable_python_dispatcher():\n+            with PreDispatchSchemaCheckMode():\n+                op.op(*args, **kwargs)\n+\n+\n+instantiate_device_type_tests(TestOpInfo, globals())\n+\n+if __name__ == \"__main__\":\n+    from torch._dynamo.test_case import run_tests\n+\n+    run_tests()\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+# Owner(s): [\"oncall: export\"]\n+\n+import torch\n+from torch._dispatch.python import enable_python_dispatcher\n+from torch._subclasses.schema_check_mode import SchemaCheckMode\n+from torch.fx.operator_schemas import normalize_function\n+from torch.testing._internal.common_device_type import (\n+    instantiate_device_type_tests,\n+    ops,\n+)\n+from torch.testing._internal.common_methods_invocations import op_db\n+from torch.testing._internal.common_utils import TestCase\n+from torch.utils._pytree import tree_map\n+\n+# Simplified naming for C++ classes\n+SchemaArgument = torch._C._SchemaArgument\n+SchemaArgType = torch._C._SchemaArgType\n+SchemaInfo = torch._C._SchemaInfo\n+\n+test_classes = {}\n+\n+\n+class PreDispatchSchemaCheckMode(SchemaCheckMode):\n+    \"\"\"\n+    Dispatch mode built on top of SchemaCheckMode that checks for incorrect op schemas\n+    for PreDispatch IR. This is meant to run ops in eager mode on concrete inputs, to\n+    see if they incorrectly claim to be functional (aliasing or mutating).\n+\n+    If an op is claimed to be functional and either is detected, an error is raised.\n+    Errors will be silenced if the schema admits aliasing or mutation - the op may\n+    later decompose and become functional.\n+    \"\"\"\n+\n+    def __init__(self):\n+        self._dispatch_key = torch._C.DispatchKey.PreDispatch\n+        super().__init__()\n+\n+    def _may_alias_or_mutate(self, func, types, args, kwargs):\n+        def unwrap(e):\n+            if isinstance(e, torch.Tensor) and not type(e) == torch.Tensor:\n+                try:\n+                    return e.elem\n+                except AttributeError as t:\n+                    return e\n+            return e\n+\n+        # get arguments, outputs\n+        schema_info = SchemaInfo(func._schema)\n+        pre_arguments = normalize_function(\n+            func, args, kwargs, normalize_to_only_use_kwargs=True\n+        ).kwargs\n+        schema_info.add_argument_values(pre_arguments)\n+        out = func(*args, **kwargs)\n+        tuple_out = out if isinstance(out, tuple) else (out,)\n+        tuple_out = tree_map(unwrap, tuple_out)\n+\n+        # check schema\n+        for i in range(len(func._schema.arguments)):\n+            for j in range(len(tuple_out)):\n+                if schema_info.may_contain_alias(\n+                    SchemaArgument(SchemaArgType.output, j),\n+                    SchemaArgument(SchemaArgType.input, i),\n+                ):\n+                    return True\n+            if schema_info.is_mutable(\n+                SchemaArgument(SchemaArgType.input, i),\n+            ):\n+                return True\n+\n+        return False\n+\n+    # creating this just so we have access to the offending op\n+    def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n+        try:\n+            return super().__torch_dispatch__(func, types, args=args, kwargs=kwargs)\n+        except RuntimeError as e:\n+            # check if schema claims to be either aliasing or mutating\n+            alias_or_mutate = self._may_alias_or_mutate(func, types, args, kwargs)\n+            if (\n+                not alias_or_mutate\n+            ):  # if schema is aliasing or mutating, will decompose further\n+                msg = e.args[0]\n+                e.args = (\n+                    f\"\"\"SchemaCheckMode failed with the following error on op <{func}>, meaning\n+    this op contains aliasing or mutations, despite claiming to be functional:\\n\\n\"\"\"\n+                    + msg,\n+                )\n+                raise e\n+\n+\n+class TestOpInfo(TestCase):\n+    @ops(op_db, allowed_dtypes=(torch.float, torch.int))\n+    def test_schema_check_op(self, device, dtype, op):\n+        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)\n+        inputs = next(sample_inputs_itr)\n+        args = [inputs.input] + list(inputs.args)\n+        kwargs = inputs.kwargs\n+        with enable_python_dispatcher():\n+            with PreDispatchSchemaCheckMode():\n+                op.op(*args, **kwargs)\n+\n+\n+instantiate_device_type_tests(TestOpInfo, globals())\n+\n+if __name__ == \"__main__\":\n+    from torch._dynamo.test_case import run_tests\n+\n+    run_tests()\n",
            "whole_hunk": "@@ -0,0 +1,108 @@\n+# Owner(s): [\"oncall: export\"]\n+\n+import torch\n+from torch._dispatch.python import enable_python_dispatcher\n+from torch._subclasses.schema_check_mode import SchemaCheckMode\n+from torch.fx.operator_schemas import normalize_function\n+from torch.testing._internal.common_device_type import (\n+    instantiate_device_type_tests,\n+    ops,\n+)\n+from torch.testing._internal.common_methods_invocations import op_db\n+from torch.testing._internal.common_utils import TestCase\n+from torch.utils._pytree import tree_map\n+\n+# Simplified naming for C++ classes\n+SchemaArgument = torch._C._SchemaArgument\n+SchemaArgType = torch._C._SchemaArgType\n+SchemaInfo = torch._C._SchemaInfo\n+\n+test_classes = {}\n+\n+\n+class PreDispatchSchemaCheckMode(SchemaCheckMode):\n+    \"\"\"\n+    Dispatch mode built on top of SchemaCheckMode that checks for incorrect op schemas\n+    for PreDispatch IR. This is meant to run ops in eager mode on concrete inputs, to\n+    see if they incorrectly claim to be functional (aliasing or mutating).\n+\n+    If an op is claimed to be functional and either is detected, an error is raised.\n+    Errors will be silenced if the schema admits aliasing or mutation - the op may\n+    later decompose and become functional.\n+    \"\"\"\n+\n+    def __init__(self):\n+        self._dispatch_key = torch._C.DispatchKey.PreDispatch\n+        super().__init__()\n+\n+    def _may_alias_or_mutate(self, func, types, args, kwargs):\n+        def unwrap(e):\n+            if isinstance(e, torch.Tensor) and not type(e) == torch.Tensor:\n+                try:\n+                    return e.elem\n+                except AttributeError as t:\n+                    return e\n+            return e\n+\n+        # get arguments, outputs\n+        schema_info = SchemaInfo(func._schema)\n+        pre_arguments = normalize_function(\n+            func, args, kwargs, normalize_to_only_use_kwargs=True\n+        ).kwargs\n+        schema_info.add_argument_values(pre_arguments)\n+        out = func(*args, **kwargs)\n+        tuple_out = out if isinstance(out, tuple) else (out,)\n+        tuple_out = tree_map(unwrap, tuple_out)\n+\n+        # check schema\n+        for i in range(len(func._schema.arguments)):\n+            for j in range(len(tuple_out)):\n+                if schema_info.may_contain_alias(\n+                    SchemaArgument(SchemaArgType.output, j),\n+                    SchemaArgument(SchemaArgType.input, i),\n+                ):\n+                    return True\n+            if schema_info.is_mutable(\n+                SchemaArgument(SchemaArgType.input, i),\n+            ):\n+                return True\n+\n+        return False\n+\n+    # creating this just so we have access to the offending op\n+    def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n+        try:\n+            return super().__torch_dispatch__(func, types, args=args, kwargs=kwargs)\n+        except RuntimeError as e:\n+            # check if schema claims to be either aliasing or mutating\n+            alias_or_mutate = self._may_alias_or_mutate(func, types, args, kwargs)\n+            if (\n+                not alias_or_mutate\n+            ):  # if schema is aliasing or mutating, will decompose further\n+                msg = e.args[0]\n+                e.args = (\n+                    f\"\"\"SchemaCheckMode failed with the following error on op <{func}>, meaning\n+    this op contains aliasing or mutations, despite claiming to be functional:\\n\\n\"\"\"\n+                    + msg,\n+                )\n+                raise e\n+\n+\n+class TestOpInfo(TestCase):\n+    @ops(op_db, allowed_dtypes=(torch.float, torch.int))\n+    def test_schema_check_op(self, device, dtype, op):\n+        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)\n+        inputs = next(sample_inputs_itr)\n+        args = [inputs.input] + list(inputs.args)\n+        kwargs = inputs.kwargs\n+        with enable_python_dispatcher():\n+            with PreDispatchSchemaCheckMode():\n+                op.op(*args, **kwargs)\n+\n+\n+instantiate_device_type_tests(TestOpInfo, globals())\n+\n+if __name__ == \"__main__\":\n+    from torch._dynamo.test_case import run_tests\n+\n+    run_tests()\n"
        },
        {
            "name": "_ops.py",
            "path": "torch/_ops.py",
            "patches": [
                {
                    "old_start": 412,
                    "old_length": 14,
                    "new_start": 412,
                    "new_length": 22,
                    "hunk": "@@ -412,14 +412,22 @@ def key_extractor(tensors, key_mask):\n \n \n # Mode stack for PreDispatchKey\n-# it should always have two keys with\n+# it should always have three keys with\n # priority given to FunctionalTensorMode and\n # then ProxyTorchDispatchMode. It means that\n # slot 0 belongs to ProxyTorchDispatchMode and\n # slot 1 belongs to FunctionalTensorMode.\n+#\n+# SchemaCheckMode is separate from the other 2,\n+# and is only valid when the stack is empty.\n+# SchemaCheckMode is for testing purposes, and\n+# is meant to run in eager mode on concrete inputs,\n+# checking for incorrect schemas in regards to\n+# aliasing or mutating ops.\n class _ModeStackStateForPreDispatch:\n     def __init__(self):\n         self.__infra_modes = [None, None]\n+        self._schema_check_mode = None\n \n     def set(self, index, mode):\n         assert index < len(self.__infra_modes)\n"
                },
                {
                    "old_start": 430,
                    "old_length": 28,
                    "new_start": 438,
                    "new_length": 36,
                    "hunk": "@@ -430,28 +438,36 @@ class _ModeStackStateForPreDispatch:\n         return self.__infra_modes[index]\n \n     def count(self):\n-        return len([i for i in self.__infra_modes if i is not None])\n+        return len([i for i in self.__infra_modes if i is not None]) + int(\n+            self._schema_check_mode is not None\n+        )\n \n \n _mode_stack_state_for_pre_dispatch = _ModeStackStateForPreDispatch()\n \n \n-def unset_mode_pre_dispatch(mode_key):\n+def unset_mode_pre_dispatch(mode_key, schema_check=False):\n     current_mode_stack_pre_dispatch = mode_stack_state_for_pre_dispatch()\n-    assert mode_key in (\n+    assert mode_key is None or mode_key in (\n         torch._C._TorchDispatchModeKey.PROXY,\n         torch._C._TorchDispatchModeKey.FUNCTIONAL,\n     )\n+    if schema_check:\n+        assert mode_key is None\n \n     def _unset_mode():\n         if mode_key == torch._C._TorchDispatchModeKey.PROXY:\n             current_mode = current_mode_stack_pre_dispatch.get(0)\n             mode_stack_state_for_pre_dispatch().set(0, None)\n             return current_mode\n-        else:\n+        elif mode_key == torch._C._TorchDispatchModeKey.FUNCTIONAL:\n             current_mode = current_mode_stack_pre_dispatch.get(1)\n             mode_stack_state_for_pre_dispatch().set(1, None)\n             return current_mode\n+        else:\n+            current_mode = mode_stack_state_for_pre_dispatch()._schema_check_mode\n+            mode_stack_state_for_pre_dispatch()._schema_check_mode = None\n+            return current_mode\n \n     current_mode = _unset_mode()\n \n"
                },
                {
                    "old_start": 470,
                    "old_length": 12,
                    "new_start": 486,
                    "new_length": 27,
                    "hunk": "@@ -470,12 +486,27 @@ def unset_mode_pre_dispatch(mode_key):\n \n def _set_mode_pre_dispatch(mode):\n     from torch._subclasses.functional_tensor import FunctionalTensorMode\n+    from torch._subclasses.schema_check_mode import SchemaCheckMode\n     from torch.fx.experimental.proxy_tensor import ProxyTorchDispatchMode\n \n-    assert isinstance(mode, (FunctionalTensorMode, ProxyTorchDispatchMode))\n+    assert isinstance(\n+        mode,\n+        (\n+            FunctionalTensorMode,\n+            ProxyTorchDispatchMode,\n+            SchemaCheckMode,\n+        ),\n+    )\n \n     previous_mode_stack_len = _len_torch_dispatch_stack_pre_dispatch()\n-    if isinstance(mode, FunctionalTensorMode):\n+    if isinstance(mode, SchemaCheckMode):\n+        current_mode = mode_stack_state_for_pre_dispatch()._schema_check_mode\n+        if previous_mode_stack_len > 0:\n+            raise AssertionError(\n+                \"SchemaCheckMode for pre-dispatch must be used exclusively, found other modes on the stack\"\n+            )\n+        mode_stack_state_for_pre_dispatch()._schema_check_mode = mode\n+    elif isinstance(mode, FunctionalTensorMode):\n         current_mode = mode_stack_state_for_pre_dispatch().get(1)\n         assert current_mode is None\n         mode_stack_state_for_pre_dispatch().set(1, mode)\n"
                },
                {
                    "old_start": 501,
                    "old_length": 9,
                    "new_start": 532,
                    "new_length": 10,
                    "hunk": "@@ -501,9 +532,10 @@ def _pop_mode_from_pre_dispatch():\n     if pre_dispatch_len == 0:\n         raise AssertionError(\"Trying to pop empty mode stack\")\n \n+    if mode_stack._schema_check_mode is not None:\n+        return unset_mode_pre_dispatch(None, schema_check=True)\n     if mode_stack.get(1) is not None:\n         return unset_mode_pre_dispatch(torch._C._TorchDispatchModeKey.FUNCTIONAL)\n-\n     if mode_stack.get(0) is not None:\n         return unset_mode_pre_dispatch(torch._C._TorchDispatchModeKey.PROXY)\n \n"
                },
                {
                    "old_start": 519,
                    "old_length": 19,
                    "new_start": 551,
                    "new_length": 23,
                    "hunk": "@@ -519,19 +551,23 @@ def _get_dispatch_mode_pre_dispatch(mode_key):\n     )\n     if mode_key == torch._C._TorchDispatchModeKey.PROXY:\n         return mode_stack_state_for_pre_dispatch().get(0)\n-    return mode_stack_state_for_pre_dispatch().get(1)\n+    else:\n+        return mode_stack_state_for_pre_dispatch().get(1)\n \n \n def _get_current_dispatch_mode_pre_dispatch():\n-    stack_len = mode_stack_state_for_pre_dispatch().count()\n-    if stack_len == 2:\n-        return mode_stack_state_for_pre_dispatch().get(1)\n-    if stack_len == 1:\n-        return (\n-            mode_stack_state_for_pre_dispatch().get(1)\n-            if mode_stack_state_for_pre_dispatch().get(1) is not None\n-            else mode_stack_state_for_pre_dispatch().get(0)\n-        )\n+    if mode_stack_state_for_pre_dispatch()._schema_check_mode is not None:\n+        return mode_stack_state_for_pre_dispatch()._schema_check_mode\n+    else:\n+        stack_len = mode_stack_state_for_pre_dispatch().count()\n+        if stack_len == 2:\n+            return mode_stack_state_for_pre_dispatch().get(1)\n+        if stack_len == 1:\n+            return (\n+                mode_stack_state_for_pre_dispatch().get(1)\n+                if mode_stack_state_for_pre_dispatch().get(1) is not None\n+                else mode_stack_state_for_pre_dispatch().get(0)\n+            )\n     return None\n \n \n"
                }
            ],
            "whole_deleted": "-# it should always have two keys with\n-        return len([i for i in self.__infra_modes if i is not None])\n-def unset_mode_pre_dispatch(mode_key):\n-    assert mode_key in (\n-        else:\n-    assert isinstance(mode, (FunctionalTensorMode, ProxyTorchDispatchMode))\n-    if isinstance(mode, FunctionalTensorMode):\n-\n-    return mode_stack_state_for_pre_dispatch().get(1)\n-    stack_len = mode_stack_state_for_pre_dispatch().count()\n-    if stack_len == 2:\n-        return mode_stack_state_for_pre_dispatch().get(1)\n-    if stack_len == 1:\n-        return (\n-            mode_stack_state_for_pre_dispatch().get(1)\n-            if mode_stack_state_for_pre_dispatch().get(1) is not None\n-            else mode_stack_state_for_pre_dispatch().get(0)\n-        )\n",
            "whole_added": "+# it should always have three keys with\n+#\n+# SchemaCheckMode is separate from the other 2,\n+# and is only valid when the stack is empty.\n+# SchemaCheckMode is for testing purposes, and\n+# is meant to run in eager mode on concrete inputs,\n+# checking for incorrect schemas in regards to\n+# aliasing or mutating ops.\n+        self._schema_check_mode = None\n+        return len([i for i in self.__infra_modes if i is not None]) + int(\n+            self._schema_check_mode is not None\n+        )\n+def unset_mode_pre_dispatch(mode_key, schema_check=False):\n+    assert mode_key is None or mode_key in (\n+    if schema_check:\n+        assert mode_key is None\n+        elif mode_key == torch._C._TorchDispatchModeKey.FUNCTIONAL:\n+        else:\n+            current_mode = mode_stack_state_for_pre_dispatch()._schema_check_mode\n+            mode_stack_state_for_pre_dispatch()._schema_check_mode = None\n+            return current_mode\n+    from torch._subclasses.schema_check_mode import SchemaCheckMode\n+    assert isinstance(\n+        mode,\n+        (\n+            FunctionalTensorMode,\n+            ProxyTorchDispatchMode,\n+            SchemaCheckMode,\n+        ),\n+    )\n+    if isinstance(mode, SchemaCheckMode):\n+        current_mode = mode_stack_state_for_pre_dispatch()._schema_check_mode\n+        if previous_mode_stack_len > 0:\n+            raise AssertionError(\n+                \"SchemaCheckMode for pre-dispatch must be used exclusively, found other modes on the stack\"\n+            )\n+        mode_stack_state_for_pre_dispatch()._schema_check_mode = mode\n+    elif isinstance(mode, FunctionalTensorMode):\n+    if mode_stack._schema_check_mode is not None:\n+        return unset_mode_pre_dispatch(None, schema_check=True)\n+    else:\n+        return mode_stack_state_for_pre_dispatch().get(1)\n+    if mode_stack_state_for_pre_dispatch()._schema_check_mode is not None:\n+        return mode_stack_state_for_pre_dispatch()._schema_check_mode\n+    else:\n+        stack_len = mode_stack_state_for_pre_dispatch().count()\n+        if stack_len == 2:\n+            return mode_stack_state_for_pre_dispatch().get(1)\n+        if stack_len == 1:\n+            return (\n+                mode_stack_state_for_pre_dispatch().get(1)\n+                if mode_stack_state_for_pre_dispatch().get(1) is not None\n+                else mode_stack_state_for_pre_dispatch().get(0)\n+            )\n",
            "whole_hunk": "@@ -412,14 +412,22 @@ def key_extractor(tensors, key_mask):\n \n \n # Mode stack for PreDispatchKey\n-# it should always have two keys with\n+# it should always have three keys with\n # priority given to FunctionalTensorMode and\n # then ProxyTorchDispatchMode. It means that\n # slot 0 belongs to ProxyTorchDispatchMode and\n # slot 1 belongs to FunctionalTensorMode.\n+#\n+# SchemaCheckMode is separate from the other 2,\n+# and is only valid when the stack is empty.\n+# SchemaCheckMode is for testing purposes, and\n+# is meant to run in eager mode on concrete inputs,\n+# checking for incorrect schemas in regards to\n+# aliasing or mutating ops.\n class _ModeStackStateForPreDispatch:\n     def __init__(self):\n         self.__infra_modes = [None, None]\n+        self._schema_check_mode = None\n \n     def set(self, index, mode):\n         assert index < len(self.__infra_modes)\n@@ -430,28 +438,36 @@ class _ModeStackStateForPreDispatch:\n         return self.__infra_modes[index]\n \n     def count(self):\n-        return len([i for i in self.__infra_modes if i is not None])\n+        return len([i for i in self.__infra_modes if i is not None]) + int(\n+            self._schema_check_mode is not None\n+        )\n \n \n _mode_stack_state_for_pre_dispatch = _ModeStackStateForPreDispatch()\n \n \n-def unset_mode_pre_dispatch(mode_key):\n+def unset_mode_pre_dispatch(mode_key, schema_check=False):\n     current_mode_stack_pre_dispatch = mode_stack_state_for_pre_dispatch()\n-    assert mode_key in (\n+    assert mode_key is None or mode_key in (\n         torch._C._TorchDispatchModeKey.PROXY,\n         torch._C._TorchDispatchModeKey.FUNCTIONAL,\n     )\n+    if schema_check:\n+        assert mode_key is None\n \n     def _unset_mode():\n         if mode_key == torch._C._TorchDispatchModeKey.PROXY:\n             current_mode = current_mode_stack_pre_dispatch.get(0)\n             mode_stack_state_for_pre_dispatch().set(0, None)\n             return current_mode\n-        else:\n+        elif mode_key == torch._C._TorchDispatchModeKey.FUNCTIONAL:\n             current_mode = current_mode_stack_pre_dispatch.get(1)\n             mode_stack_state_for_pre_dispatch().set(1, None)\n             return current_mode\n+        else:\n+            current_mode = mode_stack_state_for_pre_dispatch()._schema_check_mode\n+            mode_stack_state_for_pre_dispatch()._schema_check_mode = None\n+            return current_mode\n \n     current_mode = _unset_mode()\n \n@@ -470,12 +486,27 @@ def unset_mode_pre_dispatch(mode_key):\n \n def _set_mode_pre_dispatch(mode):\n     from torch._subclasses.functional_tensor import FunctionalTensorMode\n+    from torch._subclasses.schema_check_mode import SchemaCheckMode\n     from torch.fx.experimental.proxy_tensor import ProxyTorchDispatchMode\n \n-    assert isinstance(mode, (FunctionalTensorMode, ProxyTorchDispatchMode))\n+    assert isinstance(\n+        mode,\n+        (\n+            FunctionalTensorMode,\n+            ProxyTorchDispatchMode,\n+            SchemaCheckMode,\n+        ),\n+    )\n \n     previous_mode_stack_len = _len_torch_dispatch_stack_pre_dispatch()\n-    if isinstance(mode, FunctionalTensorMode):\n+    if isinstance(mode, SchemaCheckMode):\n+        current_mode = mode_stack_state_for_pre_dispatch()._schema_check_mode\n+        if previous_mode_stack_len > 0:\n+            raise AssertionError(\n+                \"SchemaCheckMode for pre-dispatch must be used exclusively, found other modes on the stack\"\n+            )\n+        mode_stack_state_for_pre_dispatch()._schema_check_mode = mode\n+    elif isinstance(mode, FunctionalTensorMode):\n         current_mode = mode_stack_state_for_pre_dispatch().get(1)\n         assert current_mode is None\n         mode_stack_state_for_pre_dispatch().set(1, mode)\n@@ -501,9 +532,10 @@ def _pop_mode_from_pre_dispatch():\n     if pre_dispatch_len == 0:\n         raise AssertionError(\"Trying to pop empty mode stack\")\n \n+    if mode_stack._schema_check_mode is not None:\n+        return unset_mode_pre_dispatch(None, schema_check=True)\n     if mode_stack.get(1) is not None:\n         return unset_mode_pre_dispatch(torch._C._TorchDispatchModeKey.FUNCTIONAL)\n-\n     if mode_stack.get(0) is not None:\n         return unset_mode_pre_dispatch(torch._C._TorchDispatchModeKey.PROXY)\n \n@@ -519,19 +551,23 @@ def _get_dispatch_mode_pre_dispatch(mode_key):\n     )\n     if mode_key == torch._C._TorchDispatchModeKey.PROXY:\n         return mode_stack_state_for_pre_dispatch().get(0)\n-    return mode_stack_state_for_pre_dispatch().get(1)\n+    else:\n+        return mode_stack_state_for_pre_dispatch().get(1)\n \n \n def _get_current_dispatch_mode_pre_dispatch():\n-    stack_len = mode_stack_state_for_pre_dispatch().count()\n-    if stack_len == 2:\n-        return mode_stack_state_for_pre_dispatch().get(1)\n-    if stack_len == 1:\n-        return (\n-            mode_stack_state_for_pre_dispatch().get(1)\n-            if mode_stack_state_for_pre_dispatch().get(1) is not None\n-            else mode_stack_state_for_pre_dispatch().get(0)\n-        )\n+    if mode_stack_state_for_pre_dispatch()._schema_check_mode is not None:\n+        return mode_stack_state_for_pre_dispatch()._schema_check_mode\n+    else:\n+        stack_len = mode_stack_state_for_pre_dispatch().count()\n+        if stack_len == 2:\n+            return mode_stack_state_for_pre_dispatch().get(1)\n+        if stack_len == 1:\n+            return (\n+                mode_stack_state_for_pre_dispatch().get(1)\n+                if mode_stack_state_for_pre_dispatch().get(1) is not None\n+                else mode_stack_state_for_pre_dispatch().get(0)\n+            )\n     return None\n \n \n"
        },
        {
            "name": "schema_check_mode.py",
            "path": "torch/_subclasses/schema_check_mode.py",
            "patches": [
                {
                    "old_start": 6,
                    "old_length": 7,
                    "new_start": 6,
                    "new_length": 6,
                    "hunk": "@@ -6,7 +6,6 @@ from itertools import combinations\n \n import torch\n from torch.fx.operator_schemas import normalize_function\n-from torch.testing._internal.jit_utils import clone_inputs\n from torch.utils import _pytree as pytree\n from torch.utils._python_dispatch import TorchDispatchMode\n from torch.utils._pytree import tree_map\n"
                },
                {
                    "old_start": 27,
                    "old_length": 6,
                    "new_start": 26,
                    "new_length": 38,
                    "hunk": "@@ -27,6 +26,38 @@ SchemaInfo = torch._C._SchemaInfo\n #  - Checks for aliasing on all inputs\n \n \n+# move these 2 functions here to avoid numpy dependency in testing/_internal/common_utils.py\n+\n+\n+def is_iterable_of_tensors(iterable):\n+    # Tensor itself is iterable so we check this first\n+    if isinstance(iterable, torch.Tensor):\n+        return False\n+    try:\n+        if len(iterable) == 0:\n+            return False\n+        for t in iter(iterable):\n+            if not isinstance(t, torch.Tensor):\n+                return False\n+    except TypeError as te:\n+        return False\n+    return True\n+\n+\n+def clone_inputs(args):\n+    inputs = []\n+\n+    for arg in args:\n+        if isinstance(arg, torch.Tensor):\n+            inputs.append(arg.detach().clone())\n+        elif is_iterable_of_tensors(arg):\n+            inputs.append([t.detach().clone() for t in arg])\n+        else:\n+            inputs.append(arg)\n+\n+    return inputs\n+\n+\n class SchemaCheckMode(TorchDispatchMode):\n     def __init__(self):\n         # Information recorded for testing purposes. For example:\n"
                }
            ],
            "whole_deleted": "-from torch.testing._internal.jit_utils import clone_inputs\n",
            "whole_added": "+# move these 2 functions here to avoid numpy dependency in testing/_internal/common_utils.py\n+\n+\n+def is_iterable_of_tensors(iterable):\n+    # Tensor itself is iterable so we check this first\n+    if isinstance(iterable, torch.Tensor):\n+        return False\n+    try:\n+        if len(iterable) == 0:\n+            return False\n+        for t in iter(iterable):\n+            if not isinstance(t, torch.Tensor):\n+                return False\n+    except TypeError as te:\n+        return False\n+    return True\n+\n+\n+def clone_inputs(args):\n+    inputs = []\n+\n+    for arg in args:\n+        if isinstance(arg, torch.Tensor):\n+            inputs.append(arg.detach().clone())\n+        elif is_iterable_of_tensors(arg):\n+            inputs.append([t.detach().clone() for t in arg])\n+        else:\n+            inputs.append(arg)\n+\n+    return inputs\n+\n+\n",
            "whole_hunk": "@@ -6,7 +6,6 @@ from itertools import combinations\n \n import torch\n from torch.fx.operator_schemas import normalize_function\n-from torch.testing._internal.jit_utils import clone_inputs\n from torch.utils import _pytree as pytree\n from torch.utils._python_dispatch import TorchDispatchMode\n from torch.utils._pytree import tree_map\n@@ -27,6 +26,38 @@ SchemaInfo = torch._C._SchemaInfo\n #  - Checks for aliasing on all inputs\n \n \n+# move these 2 functions here to avoid numpy dependency in testing/_internal/common_utils.py\n+\n+\n+def is_iterable_of_tensors(iterable):\n+    # Tensor itself is iterable so we check this first\n+    if isinstance(iterable, torch.Tensor):\n+        return False\n+    try:\n+        if len(iterable) == 0:\n+            return False\n+        for t in iter(iterable):\n+            if not isinstance(t, torch.Tensor):\n+                return False\n+    except TypeError as te:\n+        return False\n+    return True\n+\n+\n+def clone_inputs(args):\n+    inputs = []\n+\n+    for arg in args:\n+        if isinstance(arg, torch.Tensor):\n+            inputs.append(arg.detach().clone())\n+        elif is_iterable_of_tensors(arg):\n+            inputs.append([t.detach().clone() for t in arg])\n+        else:\n+            inputs.append(arg)\n+\n+    return inputs\n+\n+\n class SchemaCheckMode(TorchDispatchMode):\n     def __init__(self):\n         # Information recorded for testing purposes. For example:\n"
        },
        {
            "name": "_python_dispatch.py",
            "path": "torch/utils/_python_dispatch.py",
            "patches": [
                {
                    "old_start": 205,
                    "old_length": 6,
                    "new_start": 205,
                    "new_length": 7,
                    "hunk": "@@ -205,6 +205,7 @@ def _disable_current_modes():\n     )\n     from torch._subclasses.functional_tensor import FunctionalTensorMode\n     from torch.fx.experimental.proxy_tensor import ProxyTorchDispatchMode\n+    from torch._subclasses.schema_check_mode import SchemaCheckMode\n \n     mode_len_pre_dispatch = _len_torch_dispatch_stack_pre_dispatch()\n     old_pre_dispatch_modes = [\n"
                },
                {
                    "old_start": 213,
                    "old_length": 12,
                    "new_start": 214,
                    "new_length": 15,
                    "hunk": "@@ -213,12 +214,15 @@ def _disable_current_modes():\n \n     has_proxy_mode_in_pre_dispatch = False\n     has_functional_mode_in_pre_dispatch = False\n+    has_schema_check_mode_in_pre_dispatch = False\n \n     for i in old_pre_dispatch_modes:\n         if isinstance(i, ProxyTorchDispatchMode):\n             has_proxy_mode_in_pre_dispatch = True\n         if isinstance(i, FunctionalTensorMode):\n             has_functional_mode_in_pre_dispatch = True\n+        if isinstance(i, SchemaCheckMode):\n+            has_schema_check_mode_in_pre_dispatch = True\n \n     mode_len = _len_torch_dispatch_stack()\n     old_modes = [_pop_mode() for _ in range(mode_len)]\n"
                },
                {
                    "old_start": 235,
                    "old_length": 6,
                    "new_start": 239,
                    "new_length": 13,
                    "hunk": "@@ -235,6 +239,13 @@ def _disable_current_modes():\n             raise AssertionError(\n                 \"Can't have ProxyTorchDispatchMode available both in PreDispatch and Python Key\"\n             )\n+        if (\n+            isinstance(old, SchemaCheckMode)\n+            and has_schema_check_mode_in_pre_dispatch\n+        ):\n+            raise AssertionError(\n+                \"Can't have SchemaCheckMode available both in PreDispatch and Python Key\"\n+            )\n \n     # Manually disable proxy and fake modes, if any are active\n     try:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    from torch._subclasses.schema_check_mode import SchemaCheckMode\n+    has_schema_check_mode_in_pre_dispatch = False\n+        if isinstance(i, SchemaCheckMode):\n+            has_schema_check_mode_in_pre_dispatch = True\n+        if (\n+            isinstance(old, SchemaCheckMode)\n+            and has_schema_check_mode_in_pre_dispatch\n+        ):\n+            raise AssertionError(\n+                \"Can't have SchemaCheckMode available both in PreDispatch and Python Key\"\n+            )\n",
            "whole_hunk": "@@ -205,6 +205,7 @@ def _disable_current_modes():\n     )\n     from torch._subclasses.functional_tensor import FunctionalTensorMode\n     from torch.fx.experimental.proxy_tensor import ProxyTorchDispatchMode\n+    from torch._subclasses.schema_check_mode import SchemaCheckMode\n \n     mode_len_pre_dispatch = _len_torch_dispatch_stack_pre_dispatch()\n     old_pre_dispatch_modes = [\n@@ -213,12 +214,15 @@ def _disable_current_modes():\n \n     has_proxy_mode_in_pre_dispatch = False\n     has_functional_mode_in_pre_dispatch = False\n+    has_schema_check_mode_in_pre_dispatch = False\n \n     for i in old_pre_dispatch_modes:\n         if isinstance(i, ProxyTorchDispatchMode):\n             has_proxy_mode_in_pre_dispatch = True\n         if isinstance(i, FunctionalTensorMode):\n             has_functional_mode_in_pre_dispatch = True\n+        if isinstance(i, SchemaCheckMode):\n+            has_schema_check_mode_in_pre_dispatch = True\n \n     mode_len = _len_torch_dispatch_stack()\n     old_modes = [_pop_mode() for _ in range(mode_len)]\n@@ -235,6 +239,13 @@ def _disable_current_modes():\n             raise AssertionError(\n                 \"Can't have ProxyTorchDispatchMode available both in PreDispatch and Python Key\"\n             )\n+        if (\n+            isinstance(old, SchemaCheckMode)\n+            and has_schema_check_mode_in_pre_dispatch\n+        ):\n+            raise AssertionError(\n+                \"Can't have SchemaCheckMode available both in PreDispatch and Python Key\"\n+            )\n \n     # Manually disable proxy and fake modes, if any are active\n     try:"
        }
    ]
},
{
    "Id": 76,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/d3e8b8bf47206c27b6c5fdc021f7c2c3a8009521",
    "date": "2024-06-19T08:09:31+00:00",
    "message": "Remove cuda check in the CUDAGraph destructor (#127382)\n\nFixes #125804\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/127382\nApproved by: https://github.com/eqy, https://github.com/eellison",
    "label": "YES",
    "changes": [
        {
            "name": "CUDAGeneratorImpl.cpp",
            "path": "aten/src/ATen/cuda/CUDAGeneratorImpl.cpp",
            "patches": [
                {
                    "old_start": 152,
                    "old_length": 9,
                    "new_start": 152,
                    "new_length": 6,
                    "hunk": "@@ -152,9 +152,6 @@ void CUDAGeneratorState::register_graph(cuda::CUDAGraph* graph) {\n  * Unregisters a CUDA graph from the RNG state.\n  */\n void CUDAGeneratorState::unregister_graph(cuda::CUDAGraph* graph) {\n-  // Ensures that the RNG state is not currently being captured.\n-  at::cuda::assertNotCapturing(\n-      \"Cannot unregister the state during capturing stage.\");\n   // Verify the graph was previously registered.\n   TORCH_CHECK(\n       registered_graphs_.find(graph) != registered_graphs_.end(),"
                }
            ],
            "whole_deleted": "-  // Ensures that the RNG state is not currently being captured.\n-  at::cuda::assertNotCapturing(\n-      \"Cannot unregister the state during capturing stage.\");\n",
            "whole_added": "",
            "whole_hunk": "@@ -152,9 +152,6 @@ void CUDAGeneratorState::register_graph(cuda::CUDAGraph* graph) {\n  * Unregisters a CUDA graph from the RNG state.\n  */\n void CUDAGeneratorState::unregister_graph(cuda::CUDAGraph* graph) {\n-  // Ensures that the RNG state is not currently being captured.\n-  at::cuda::assertNotCapturing(\n-      \"Cannot unregister the state during capturing stage.\");\n   // Verify the graph was previously registered.\n   TORCH_CHECK(\n       registered_graphs_.find(graph) != registered_graphs_.end(),"
        }
    ]
},
{
    "Id": 2,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/e98135d1ad2f999fec649ecd21b35f3d5676be43",
    "date": "2024-07-18T08:27:53+00:00",
    "message": "[aota] Needs autograd if an input requires_grad, agnostic to enable_grad (#128890)\n\nReland of:  https://github.com/pytorch/pytorch/pull/128016\n\nSummary from previous PR:\nWe assume only two possible mutually exclusive scenarios:\n\nRunning compiled region for training (Any of inputs has requires_grad)\n\nProduced differentiable outputs should have requires_grad.\nRunning compiled region for inference (None of inputs has requires_grad)\n\nAll outputs do not have requires_grad.\nEven if user runs the region under no_grad(), but has an input Tensor with requires_grad - we go Training scenario (1).\n\nWith current state that means:\n1/ needs_autograd should not check torch.is_grad_enabled(), only that any of inputs requires_grad\n2/ if needs_autograd => trace_joint (We are in training scenario 1.) => always run compiled region under with.enable_grad()\n\nChanges in partitioner?\n\nInference and Training graphs had difference in return container, list/tuple.\nThe changes in partitioner are done to unify and return always tuple.\nAs a result - some changes in test_aotdispatch.py for graph contents list -> tuple.\n\nWhy was revert?\n\nThere was a regression of hf_Reformer model on inference.\n```\nTORCHINDUCTOR_FX_GRAPH_CACHE=0 python benchmarks/dynamo/torchbench.py --performance --inference --bfloat16 --backend inductor --device cuda --only hf_Reformer --cold-start-latency --use-eval-mode\n```\n\nBecause one of the compiled graphs contained outputs, which are aliases to the inputs that are nn.Parameter(requires_grad=True).\n\nEven if inference bencharmsk torchbench runs inside with` torch.no_grad()` - alias (specifically for hf_Reformer - expand) ops preserve requires_grad.\n\nAs a result we started compiling training graph instead of inference.\n\nFix for view ops:\n\nIf we have outputs, that are aliases to inputs that requires_grad, those outputs requires grad is not a reason to generate training graph.\n\nThis is handled in aot_autograd.py, where output_and_mutation_safe are calculated.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128890\nApproved by: https://github.com/bdhirsh",
    "label": "YES",
    "changes": [
        {
            "name": "test_dtensor_compile.py",
            "path": "test/distributed/_tensor/test_dtensor_compile.py",
            "patches": [
                {
                    "old_start": 530,
                    "old_length": 7,
                    "new_start": 530,
                    "new_length": 7,
                    "hunk": "@@ -530,7 +530,7 @@ def forward(self, primals_1):\n     wait_tensor = torch.ops._c10d_functional.wait_tensor.default(primals_1)\n     sin = torch.ops.aten.sin.default(wait_tensor)\n     sin_1 = torch.ops.aten.sin.default(sin);  sin = None\n-    return [sin_1, primals_1, wait_tensor]\"\"\",\n+    return (sin_1, primals_1, wait_tensor)\"\"\",\n         )\n \n     @unittest.skipIf(not has_triton(), \"Inductor+gpu needs triton and recent GPU arch\")\n"
                }
            ],
            "whole_deleted": "-    return [sin_1, primals_1, wait_tensor]\"\"\",\n",
            "whole_added": "+    return (sin_1, primals_1, wait_tensor)\"\"\",\n",
            "whole_hunk": "@@ -530,7 +530,7 @@ def forward(self, primals_1):\n     wait_tensor = torch.ops._c10d_functional.wait_tensor.default(primals_1)\n     sin = torch.ops.aten.sin.default(wait_tensor)\n     sin_1 = torch.ops.aten.sin.default(sin);  sin = None\n-    return [sin_1, primals_1, wait_tensor]\"\"\",\n+    return (sin_1, primals_1, wait_tensor)\"\"\",\n         )\n \n     @unittest.skipIf(not has_triton(), \"Inductor+gpu needs triton and recent GPU arch\")\n"
        },
        {
            "name": "test_aotdispatch.py",
            "path": "test/functorch/test_aotdispatch.py",
            "patches": [
                {
                    "old_start": 573,
                    "old_length": 7,
                    "new_start": 573,
                    "new_length": 7,
                    "hunk": "@@ -573,7 +573,7 @@ def forward(self, primals_1):\n     clone = torch.ops.aten.clone.default(primals_1);  primals_1 = None\n     mul = torch.ops.aten.mul.Tensor(clone, 2);  clone = None\n     mul_1 = torch.ops.aten.mul.Tensor(mul, 3)\n-    return [mul, mul_1]\"\"\",\n+    return (mul, mul_1)\"\"\",\n         )\n \n     def test_input_mutation_set__input_mutation(self):\n"
                },
                {
                    "old_start": 643,
                    "old_length": 7,
                    "new_start": 643,
                    "new_length": 7,
                    "hunk": "@@ -643,7 +643,7 @@ def forward(self, primals_1, primals_2):\n     add = torch.ops.aten.add.Tensor(mul, mul)\n     set_ = torch.ops.aten.set_.source_Tensor(primals_1, mul);  primals_1 = None\n     copy_ = torch.ops.aten.copy_.default(primals_2, mul);  primals_2 = mul = None\n-    return [add]\"\"\",\n+    return (add,)\"\"\",\n         )\n \n     # This is a (hopefully) extremely rare case that is difficult to handle,\n"
                },
                {
                    "old_start": 722,
                    "old_length": 7,
                    "new_start": 722,
                    "new_length": 7,
                    "hunk": "@@ -722,7 +722,7 @@ def forward(self, primals_1):\n     alias = torch.ops.aten.alias.default(primals_1);  primals_1 = None\n     view = torch.ops.aten.view.default(arange, [3, 3]);  arange = None\n     add = torch.ops.aten.add.Tensor(alias, view);  alias = view = None\n-    return [add]\"\"\",\n+    return (add,)\"\"\",\n         )\n \n     @unittest.skipIf(IS_WINDOWS, \"TODO: need to fix the test case\")\n"
                },
                {
                    "old_start": 1020,
                    "old_length": 7,
                    "new_start": 1020,
                    "new_length": 7,
                    "hunk": "@@ -1020,7 +1020,7 @@ def forward(self, primals_1):\n def forward(self, primals_1, primals_2, primals_3):\n     t = torch.ops.aten.t.default(primals_1);  primals_1 = None\n     addmm = torch.ops.aten.addmm.default(primals_2, primals_3, t);  primals_2 = None\n-    return [addmm, primals_3, t]\"\"\",\n+    return (addmm, primals_3, t)\"\"\",\n         )\n \n         with torch.inference_mode():\n"
                },
                {
                    "old_start": 1071,
                    "old_length": 7,
                    "new_start": 1071,
                    "new_length": 7,
                    "hunk": "@@ -1071,7 +1071,7 @@ def forward(self, arg0_1, arg1_1, arg2_1):\n def forward(self, primals_1):\n     clone = torch.ops.aten.clone.default(primals_1);  primals_1 = None\n     mul = torch.ops.aten.mul.Tensor(clone, 2);  clone = None\n-    return [mul, mul]\"\"\",\n+    return (mul, mul)\"\"\",\n         )\n \n     def test_input_mutation_multiple(self):\n"
                },
                {
                    "old_start": 1100,
                    "old_length": 7,
                    "new_start": 1100,
                    "new_length": 7,
                    "hunk": "@@ -1100,7 +1100,7 @@ def forward(self, primals_1, primals_2, primals_3):\n     mul_1 = torch.ops.aten.mul.Tensor(clone_1, 2);  clone_1 = None\n     add = torch.ops.aten.add.Tensor(mul, primals_2);  primals_2 = None\n     add_1 = torch.ops.aten.add.Tensor(add, mul_1);  add = None\n-    return [mul, mul_1, add_1]\"\"\",\n+    return (mul, mul_1, add_1)\"\"\",\n         )\n \n     def test_input_mutation_return(self):\n"
                },
                {
                    "old_start": 1171,
                    "old_length": 7,
                    "new_start": 1171,
                    "new_length": 7,
                    "hunk": "@@ -1171,7 +1171,7 @@ def forward(self, primals_1):\n     copy = torch.ops.aten.copy.default(primals_1, ones);  ones = None\n     add = torch.ops.aten.add.Tensor(copy, 1)\n     copy_ = torch.ops.aten.copy_.default(primals_1, copy);  primals_1 = copy = None\n-    return [add]\"\"\",\n+    return (add,)\"\"\",\n         )\n \n     def test_input_mutation_storage_resize_down(self):\n"
                },
                {
                    "old_start": 1204,
                    "old_length": 7,
                    "new_start": 1204,
                    "new_length": 7,
                    "hunk": "@@ -1204,7 +1204,7 @@ def forward(self, primals_1):\n def forward(self, primals_1):\n     sin = torch.ops.aten.sin.default(primals_1)\n     resize_storage_bytes_ = torch.ops.inductor.resize_storage_bytes_.default(primals_1, 0)\n-    return [sin, primals_1]\"\"\",\n+    return (sin, primals_1)\"\"\",\n         )\n \n     #     def test_input_mutation_storage_resize_up_down(self):\n"
                },
                {
                    "old_start": 1305,
                    "old_length": 7,
                    "new_start": 1305,
                    "new_length": 7,
                    "hunk": "@@ -1305,7 +1305,7 @@ def forward(self, primals_1, primals_2):\n     sin = torch.ops.aten.sin.default(cat)\n     resize_storage_bytes_ = torch.ops.inductor.resize_storage_bytes_.default(cat, 0)\n     set_ = torch.ops.aten.set_.source_Tensor(primals_1, cat);  primals_1 = None\n-    return [sin, cat]\"\"\",\n+    return (sin, cat)\"\"\",\n         )\n \n     def test_input_mutation_storage_resize_before_set_(self):\n"
                },
                {
                    "old_start": 1401,
                    "old_length": 7,
                    "new_start": 1401,
                    "new_length": 7,
                    "hunk": "@@ -1401,7 +1401,7 @@ def forward(self, primals_1):\n     view_1 = torch.ops.aten.view.default(mul, [4]);  mul = None\n     add = torch.ops.aten.add.Tensor(view_1, 1)\n     copy_ = torch.ops.aten.copy_.default(primals_1, view_1);  primals_1 = view_1 = None\n-    return [add]\"\"\",\n+    return (add,)\"\"\",\n         )\n \n     def test_input_mutation_requires_grad_no_grad(self):\n"
                },
                {
                    "old_start": 1423,
                    "old_length": 7,
                    "new_start": 1423,
                    "new_length": 7,
                    "hunk": "@@ -1423,7 +1423,7 @@ def forward(self, primals_1):\n     mul = torch.ops.aten.mul.Tensor(primals_1, 2)\n     add = torch.ops.aten.add.Tensor(mul, 3)\n     copy_ = torch.ops.aten.copy_.default(primals_1, mul);  primals_1 = mul = None\n-    return [add]\"\"\",\n+    return (add,)\"\"\",\n         )\n \n     def test_input_mutation_requires_grad_no_grad_inference_graph(self):\n"
                },
                {
                    "old_start": 1547,
                    "old_length": 9,
                    "new_start": 1547,
                    "new_length": 9,
                    "hunk": "@@ -1547,9 +1547,9 @@ def forward(self, arg0_1):\n         self.assertExpectedInline(\n             fw_graph.code.strip(),\n             \"\"\"\\\n-def forward(self, primals_1):\n-    view = torch.ops.aten.view.default(primals_1, [-1]);  primals_1 = None\n-    return [view]\"\"\",\n+def forward(self, arg0_1):\n+    view = torch.ops.aten.view.default(arg0_1, [-1]);  arg0_1 = None\n+    return (view,)\"\"\",\n         )\n \n     def test_input_output_view_mutate_multiple(self):\n"
                },
                {
                    "old_start": 1581,
                    "old_length": 7,
                    "new_start": 1581,
                    "new_length": 7,
                    "hunk": "@@ -1581,7 +1581,7 @@ def forward(self, primals_1, primals_2, primals_3):\n     mul_1 = torch.ops.aten.mul.Tensor(clone_1, 3);  clone_1 = None\n     view = torch.ops.aten.view.default(primals_2, [2, 2]);  primals_2 = None\n     view_2 = torch.ops.aten.view.default(mul_1, [2, 2])\n-    return [mul, mul_1, view, view_2]\"\"\",\n+    return (mul, mul_1, view, view_2)\"\"\",\n         )\n \n     def test_input_output_view_metadata_mutate_multiple(self):\n"
                },
                {
                    "old_start": 1615,
                    "old_length": 7,
                    "new_start": 1615,
                    "new_length": 7,
                    "hunk": "@@ -1615,7 +1615,7 @@ def forward(self, primals_1, primals_2, primals_3):\n     view_1 = torch.ops.aten.view.default(primals_1, [2, 2]);  primals_1 = None\n     view_3 = torch.ops.aten.view.default(t, [2, 2])\n     view_4 = torch.ops.aten.view.default(mul, [2, 2])\n-    return [mul, t, view_1, view_4, view_3]\"\"\",\n+    return (mul, t, view_1, view_4, view_3)\"\"\",\n         )\n \n     def test_input_mutation_and_output_view(self):\n"
                },
                {
                    "old_start": 1637,
                    "old_length": 7,
                    "new_start": 1637,
                    "new_length": 7,
                    "hunk": "@@ -1637,7 +1637,7 @@ def forward(self, primals_1):\n     clone = torch.ops.aten.clone.default(primals_1);  primals_1 = None\n     add = torch.ops.aten.add.Tensor(clone, 1);  clone = None\n     view_1 = torch.ops.aten.view.default(add, [-1])\n-    return [add, view_1]\"\"\",\n+    return (add, view_1)\"\"\",\n         )\n \n     def test_input_mutation_output_view_multiple(self):\n"
                },
                {
                    "old_start": 1671,
                    "old_length": 7,
                    "new_start": 1671,
                    "new_length": 7,
                    "hunk": "@@ -1671,7 +1671,7 @@ def forward(self, primals_1, primals_2, primals_3, primals_4):\n     add_1 = torch.ops.aten.add.Tensor(primals_4, 1);  primals_4 = None\n     diagonal = torch.ops.aten.diagonal.default(transpose)\n     add_2 = torch.ops.aten.add.Tensor(primals_1, add);  primals_1 = None\n-    return [transpose, add, add_1, diagonal, add_2]\"\"\",\n+    return (transpose, add, add_1, diagonal, add_2)\"\"\",\n         )\n \n     def test_output_aliases_intermediate_single(self):\n"
                },
                {
                    "old_start": 1691,
                    "old_length": 7,
                    "new_start": 1691,
                    "new_length": 7,
                    "hunk": "@@ -1691,7 +1691,7 @@ def forward(self, primals_1, primals_2, primals_3, primals_4):\n def forward(self, primals_1):\n     mul = torch.ops.aten.mul.Tensor(primals_1, 3);  primals_1 = None\n     view = torch.ops.aten.view.default(mul, [-1]);  mul = None\n-    return [view]\"\"\",\n+    return (view,)\"\"\",\n         )\n \n     def test_output_aliases_input_multi_output_view_should_raise_autograd_error(self):\n"
                },
                {
                    "old_start": 1926,
                    "old_length": 7,
                    "new_start": 1926,
                    "new_length": 7,
                    "hunk": "@@ -1926,7 +1926,7 @@ def forward(self, primals_1, primals_2):\n     mul = torch.ops.aten.mul.Tensor(primals_1, 3);  primals_1 = None\n     view = torch.ops.aten.view.default(mul, [-1]);  mul = None\n     add = torch.ops.aten.add.Tensor(primals_2, 1);  primals_2 = None\n-    return [view, add]\"\"\",\n+    return (view, add)\"\"\",\n         )\n \n     def test_output_aliases_intermediate_returned_multiple_times(self):\n"
                },
                {
                    "old_start": 1957,
                    "old_length": 7,
                    "new_start": 1957,
                    "new_length": 7,
                    "hunk": "@@ -1957,7 +1957,7 @@ def forward(self, primals_1):\n     mul = torch.ops.aten.mul.Tensor(primals_1, 3);  primals_1 = None\n     view = torch.ops.aten.view.default(mul, [-1])\n     view_1 = torch.ops.aten.view.default(mul, [-1])\n-    return [view, view_1, mul]\"\"\",\n+    return (view, view_1, mul)\"\"\",\n         )\n \n     def test_output_aliases_intermediate_and_returned(self):\n"
                },
                {
                    "old_start": 1977,
                    "old_length": 7,
                    "new_start": 1977,
                    "new_length": 7,
                    "hunk": "@@ -1977,7 +1977,7 @@ def forward(self, primals_1):\n def forward(self, primals_1):\n     mul = torch.ops.aten.mul.Tensor(primals_1, 3);  primals_1 = None\n     view = torch.ops.aten.view.default(mul, [-1])\n-    return [view, mul]\"\"\",\n+    return (view, mul)\"\"\",\n         )\n \n     def test_output_aliases_intermediate_and_returned_flipped(self):\n"
                },
                {
                    "old_start": 1997,
                    "old_length": 7,
                    "new_start": 1997,
                    "new_length": 7,
                    "hunk": "@@ -1997,7 +1997,7 @@ def forward(self, primals_1):\n def forward(self, primals_1):\n     mul = torch.ops.aten.mul.Tensor(primals_1, 3);  primals_1 = None\n     view = torch.ops.aten.view.default(mul, [-1])\n-    return [mul, view]\"\"\",\n+    return (mul, view)\"\"\",\n         )\n \n     def test_output_aliases_intermediate_and_returned_different_grad(self):\n"
                },
                {
                    "old_start": 2021,
                    "old_length": 7,
                    "new_start": 2021,
                    "new_length": 7,
                    "hunk": "@@ -2021,7 +2021,7 @@ def forward(self, primals_1):\n     detach = torch.ops.aten.detach.default(select);  select = None\n     detach_1 = torch.ops.aten.detach.default(detach);  detach = None\n     detach_2 = torch.ops.aten.detach.default(detach_1);  detach_1 = None\n-    return [view, mul, detach_2]\"\"\",\n+    return (view, mul, detach_2)\"\"\",\n         )\n \n     def test_output_aliases_intermediate_inplace_view(self):\n"
                },
                {
                    "old_start": 2057,
                    "old_length": 7,
                    "new_start": 2057,
                    "new_length": 7,
                    "hunk": "@@ -2057,7 +2057,7 @@ def forward(self, primals_1):\n     mul = torch.ops.aten.mul.Tensor(primals_1, 3)\n     t = torch.ops.aten.t.default(mul);  mul = None\n     add = torch.ops.aten.add.Tensor(primals_1, 1);  primals_1 = None\n-    return [t, add]\"\"\",\n+    return (t, add)\"\"\",\n         )\n \n     def test_output_aliases_intermediate_inplace_view_and_view(self):\n"
                },
                {
                    "old_start": 2094,
                    "old_length": 7,
                    "new_start": 2094,
                    "new_length": 7,
                    "hunk": "@@ -2094,7 +2094,7 @@ def forward(self, primals_1):\n     view = torch.ops.aten.view.default(mul, [-1])\n     transpose = torch.ops.aten.transpose.int(mul_1, 1, 0);  mul_1 = None\n     transpose_1 = torch.ops.aten.transpose.int(mul, 1, 0)\n-    return [view, transpose, transpose_1, mul]\"\"\",\n+    return (view, transpose, transpose_1, mul)\"\"\",\n         )\n \n     def test_output_all_alias_types(self):\n"
                },
                {
                    "old_start": 2130,
                    "old_length": 7,
                    "new_start": 2130,
                    "new_length": 7,
                    "hunk": "@@ -2130,7 +2130,7 @@ def forward(self, primals_1):\n     squeeze = torch.ops.aten.squeeze.default(mul)\n     transpose_1 = torch.ops.aten.transpose.int(mul, 1, 0)\n     unsqueeze = torch.ops.aten.unsqueeze.default(transpose, 0)\n-    return [transpose, squeeze, transpose_1, unsqueeze, mul]\"\"\",\n+    return (transpose, squeeze, transpose_1, unsqueeze, mul)\"\"\",\n         )\n \n     @parametrize(\"req_grad\", [False, True])\n"
                },
                {
                    "old_start": 2180,
                    "old_length": 7,
                    "new_start": 2180,
                    "new_length": 7,
                    "hunk": "@@ -2180,7 +2180,7 @@ def forward(self, primals_1):\n     t_4 = torch.ops.aten.t.default(t_2)\n     t_6 = torch.ops.aten.t.default(t_2);  t_2 = None\n     view_1 = torch.ops.aten.view.default(t_6, [3, 3]);  t_6 = None\n-    return [t_4, view_1]\"\"\",\n+    return (t_4, view_1)\"\"\",\n         )\n \n     def test_view_and_inplace_view(self):\n"
                },
                {
                    "old_start": 2199,
                    "old_length": 12,
                    "new_start": 2199,
                    "new_length": 11,
                    "hunk": "@@ -2199,12 +2199,11 @@ def forward(self, primals_1):\n         self.assertExpectedInline(\n             fw_graph.code.strip(),\n             \"\"\"\\\n-def forward(self, primals_1, primals_2):\n-    view = torch.ops.aten.view.default(primals_1, [3, 3]);  primals_1 = None\n-    t = torch.ops.aten.t.default(view);  view = None\n-    view_1 = torch.ops.aten.view.default(primals_2, [3, 3]);  primals_2 = None\n-    view_2 = torch.ops.aten.view.default(t, [3, 3])\n-    return [t, view_1, view_2]\"\"\",\n+def forward(self, arg0_1, arg1_1):\n+    t = torch.ops.aten.t.default(arg0_1);  arg0_1 = None\n+    view = torch.ops.aten.view.default(arg1_1, [3, 3]);  arg1_1 = None\n+    view_1 = torch.ops.aten.view.default(t, [3, 3])\n+    return (t, view, view_1)\"\"\",\n         )\n \n     def test_view_detach(self):\n"
                },
                {
                    "old_start": 2236,
                    "old_length": 7,
                    "new_start": 2235,
                    "new_length": 7,
                    "hunk": "@@ -2236,7 +2235,7 @@ def forward(self, primals_1, primals_2):\n def forward(self, primals_1, primals_2):\n     mul = torch.ops.aten.mul.Tensor(primals_1, 3);  primals_1 = None\n     mul_1 = torch.ops.aten.mul.Tensor(primals_2, 4);  primals_2 = None\n-    return [mul, mul_1]\"\"\",\n+    return (mul, mul_1)\"\"\",\n         )\n \n     # This is a torture test:\n"
                },
                {
                    "old_start": 2756,
                    "old_length": 7,
                    "new_start": 2755,
                    "new_length": 7,
                    "hunk": "@@ -2756,7 +2755,7 @@ def forward(self, primals_1):\n     as_strided_2 = torch.ops.aten.as_strided.default(as_strided_scatter, [2], [1], 0)\n     as_strided_5 = torch.ops.aten.as_strided.default(as_strided_scatter, [2], [1], 0)\n     add_1 = torch.ops.aten.add.Tensor(as_strided_2, as_strided_5);  as_strided_2 = as_strided_5 = None\n-    return [as_strided_scatter, add_1]\"\"\",\n+    return (as_strided_scatter, add_1)\"\"\",\n         )  # noqa: B950\n \n     def test_input_mutation_aliases_other_input2(self):\n"
                },
                {
                    "old_start": 2789,
                    "old_length": 7,
                    "new_start": 2788,
                    "new_length": 7,
                    "hunk": "@@ -2789,7 +2788,7 @@ def forward(self, primals_1):\n     as_strided_2 = torch.ops.aten.as_strided.default(as_strided_scatter, [2], [1], 0)\n     as_strided_5 = torch.ops.aten.as_strided.default(as_strided_scatter, [2, 2], [2, 1], 0)\n     add_1 = torch.ops.aten.add.Tensor(as_strided_2, as_strided_5);  as_strided_2 = as_strided_5 = None\n-    return [as_strided_scatter, add_1]\"\"\",\n+    return (as_strided_scatter, add_1)\"\"\",\n         )  # noqa: B950\n \n     def test_input_mutation_aliases_and_output_alias(self):\n"
                },
                {
                    "old_start": 2820,
                    "old_length": 7,
                    "new_start": 2819,
                    "new_length": 7,
                    "hunk": "@@ -2820,7 +2819,7 @@ def forward(self, primals_1):\n     as_strided_scatter = torch.ops.aten.as_strided_scatter.default(clone, add, [4], [1], 0);  clone = add = None\n     as_strided_8 = torch.ops.aten.as_strided.default(as_strided_scatter, [4], [1], 0)\n     view_1 = torch.ops.aten.view.default(as_strided_8, [4]);  as_strided_8 = None\n-    return [as_strided_scatter, view_1]\"\"\",\n+    return (as_strided_scatter, view_1)\"\"\",\n         )  # noqa: B950\n \n     def test_input_aliased_with_mutation_output_alias(self):\n"
                },
                {
                    "old_start": 2857,
                    "old_length": 7,
                    "new_start": 2856,
                    "new_length": 7,
                    "hunk": "@@ -2857,7 +2856,7 @@ def forward(self, primals_1, primals_2):\n     add = torch.ops.aten.add.Tensor(primals_2, 1);  primals_2 = None\n     as_strided_7 = torch.ops.aten.as_strided.default(as_strided_scatter, [4], [1], 0)\n     view_1 = torch.ops.aten.view.default(as_strided_7, [-1]);  as_strided_7 = None\n-    return [as_strided_scatter, add, view_1]\"\"\",\n+    return (as_strided_scatter, add, view_1)\"\"\",\n         )  # noqa: B950\n \n     def test_input_metadata_mutation_aliases(self):\n"
                },
                {
                    "old_start": 2886,
                    "old_length": 7,
                    "new_start": 2885,
                    "new_length": 7,
                    "hunk": "@@ -2886,7 +2885,7 @@ def forward(self, primals_1, primals_2):\n def forward(self, primals_1, primals_2):\n     t = torch.ops.aten.t.default(primals_1);  primals_1 = None\n     add = torch.ops.aten.add.Tensor(t, primals_2);  t = primals_2 = None\n-    return [add]\"\"\",\n+    return (add,)\"\"\",\n         )\n \n     def test_input_mutation_aliases_and_none_require_gradients(self):\n"
                },
                {
                    "old_start": 2929,
                    "old_length": 7,
                    "new_start": 2928,
                    "new_length": 7,
                    "hunk": "@@ -2929,7 +2928,7 @@ def forward(self, primals_1, primals_2):\n     as_strided_3 = torch.ops.aten.as_strided.default(as_strided_scatter, [4], [1], 0)\n     add = torch.ops.aten.add.Tensor(as_strided_3, 1);  as_strided_3 = None\n     add_1 = torch.ops.aten.add.Tensor(primals_2, 1);  primals_2 = None\n-    return [as_strided_scatter, add, add_1]\"\"\",\n+    return (as_strided_scatter, add, add_1)\"\"\",\n         )  # noqa: B950\n \n     @skipIfDynamoInput(\"Fails with dynamo\")\n"
                },
                {
                    "old_start": 2988,
                    "old_length": 7,
                    "new_start": 2987,
                    "new_length": 7,
                    "hunk": "@@ -2988,7 +2987,7 @@ def forward(self, primals_1, primals_2, primals_3):\n     add_2 = torch.ops.aten.add.Tensor(add_1, unsqueeze_1);  add_1 = None\n     as_strided_14 = torch.ops.aten.as_strided.default(as_strided_scatter, [4], [1], 0)\n     view_2 = torch.ops.aten.view.default(as_strided_14, [-1]);  as_strided_14 = None\n-    return [as_strided_scatter, add_2, view_2, unsqueeze_1]\"\"\",\n+    return (as_strided_scatter, add_2, view_2, unsqueeze_1)\"\"\",\n         )  # noqa: B950\n \n     @unittest.skipIf(not torch.cuda.is_available(), \"CUDA is unavailable\")\n"
                },
                {
                    "old_start": 3062,
                    "old_length": 7,
                    "new_start": 3061,
                    "new_length": 7,
                    "hunk": "@@ -3062,7 +3061,7 @@ def forward(self, primals_1, primals_2):\n     view_1 = torch.ops.aten.view.default(add, [-1])\n     t_1 = torch.ops.aten.t.default(t)\n     unsqueeze = torch.ops.aten.unsqueeze.default(view_1, 0)\n-    return [as_strided_scatter, t, view_1, t_1, unsqueeze, add]\"\"\",\n+    return (as_strided_scatter, t, view_1, t_1, unsqueeze, add)\"\"\",\n         )  # noqa: B950\n \n     def test_dynamic_shape_output_not_in_bw_graph(self):\n"
                },
                {
                    "old_start": 3090,
                    "old_length": 7,
                    "new_start": 3089,
                    "new_length": 7,
                    "hunk": "@@ -3090,7 +3089,7 @@ def forward(self, primals_1, primals_2):\n             bw_graph_cell[0].code.strip(),\n             \"\"\"\\\n def forward(self, tangents_1):\n-    return [tangents_1]\"\"\",\n+    return (tangents_1,)\"\"\",\n         )\n \n     def test_no_grad_input_output(self):\n"
                },
                {
                    "old_start": 3610,
                    "old_length": 7,
                    "new_start": 3609,
                    "new_length": 7,
                    "hunk": "@@ -3610,7 +3609,7 @@ def forward(self, primals_1, primals_2, primals_3, primals_4):\n     sum_2 = torch.ops.aten.sum.default(add)\n     add_1 = torch.ops.aten.add.Tensor(sum_1, sum_2);  sum_1 = sum_2 = None\n     copy_ = torch.ops.aten.copy_.default(primals_3, add);  primals_3 = add = None\n-    return [add_1, primals_1, primals_2, primals_4, mul]\"\"\",\n+    return (add_1, primals_1, primals_2, primals_4, mul)\"\"\",\n         )\n \n         self.assertEqual(out_ref, out_test)\n"
                },
                {
                    "old_start": 3665,
                    "old_length": 7,
                    "new_start": 3664,
                    "new_length": 7,
                    "hunk": "@@ -3665,7 +3664,7 @@ def forward(self, primals_1, primals_2, primals_3):\n     sum_2 = torch.ops.aten.sum.default(add)\n     add_1 = torch.ops.aten.add.Tensor(sum_1, sum_2);  sum_1 = sum_2 = None\n     copy_ = torch.ops.aten.copy_.default(primals_2, add);  primals_2 = add = None\n-    return [add_1, primals_1, primals_3]\"\"\",\n+    return (add_1, primals_1, primals_3)\"\"\",\n         )\n         self.assertEqual(out_ref, out_test)\n \n"
                },
                {
                    "old_start": 3723,
                    "old_length": 7,
                    "new_start": 3722,
                    "new_length": 7,
                    "hunk": "@@ -3723,7 +3722,7 @@ def forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals\n     copy_ = torch.ops.aten.copy_.default(primals_3, getitem_3);  primals_3 = None\n     copy__1 = torch.ops.aten.copy_.default(primals_4, getitem_4);  primals_4 = None\n     copy__2 = torch.ops.aten.copy_.default(primals_5, add);  primals_5 = add = None\n-    return [getitem, primals_1, primals_6, getitem_1, getitem_2, getitem_3, getitem_4]\"\"\",  # noqa: B950\n+    return (getitem, primals_1, primals_6, getitem_1, getitem_2, getitem_3, getitem_4)\"\"\",  # noqa: B950\n         )\n \n         self.assertEqual(out_ref, out_test)\n"
                },
                {
                    "old_start": 3743,
                    "old_length": 7,
                    "new_start": 3742,
                    "new_length": 7,
                    "hunk": "@@ -3743,7 +3742,7 @@ def forward(self, primals_1, primals_6, getitem_1, getitem_2, getitem_3, getitem\n     getitem_5 = native_batch_norm_backward[0]\n     getitem_6 = native_batch_norm_backward[1]\n     getitem_7 = native_batch_norm_backward[2];  native_batch_norm_backward = None\n-    return [getitem_6, getitem_7, None, None, None, getitem_5]\"\"\",  # noqa: B950\n+    return (getitem_6, getitem_7, None, None, None, getitem_5)\"\"\",  # noqa: B950\n         )\n \n         self.assertEqual(inp_ref.grad, inp_test.grad)\n"
                },
                {
                    "old_start": 3784,
                    "old_length": 7,
                    "new_start": 3783,
                    "new_length": 7,
                    "hunk": "@@ -3784,7 +3783,7 @@ def forward(self, primals_1, primals_6, getitem_1, getitem_2, getitem_3, getitem\n def forward(self, primals_1, primals_2):\n     clone = torch.ops.aten.clone.default(primals_1);  primals_1 = None\n     add = torch.ops.aten.add.Tensor(clone, primals_2);  clone = primals_2 = None\n-    return [add, add]\"\"\",\n+    return (add, add)\"\"\",\n         )  # noqa: B950\n \n         self.assertEqual(out_ref, out_test)\n"
                },
                {
                    "old_start": 3796,
                    "old_length": 7,
                    "new_start": 3795,
                    "new_length": 7,
                    "hunk": "@@ -3796,7 +3795,7 @@ def forward(self, primals_1, primals_2):\n             bw_graph_cell[0].code.strip(),\n             \"\"\"\\\n def forward(self, tangents_1):\n-    return [None, tangents_1]\"\"\",\n+    return (None, tangents_1)\"\"\",\n         )  # noqa: B950\n \n     def test_real_weights_in_symbolic_mode(self):\n"
                },
                {
                    "old_start": 3884,
                    "old_length": 6,
                    "new_start": 3883,
                    "new_length": 7,
                    "hunk": "@@ -3884,6 +3883,7 @@ def forward(self, tangents_1):\n         # since they are the only ones that will get reconstructed.\n         def wrapper(g, *args, **kwargs):\n             outs = g(*args, **kwargs)\n+            outs = list(outs)\n             for i in output_view_indices:\n                 outs[i] = NoViewReplayTensor(outs[i])\n             return outs\n"
                },
                {
                    "old_start": 5397,
                    "old_length": 7,
                    "new_start": 5397,
                    "new_length": 7,
                    "hunk": "@@ -5397,7 +5397,7 @@ def forward(self, primals_1, primals_2, primals_3):\n     div = torch.ops.aten.div.Tensor(primals_3, 2);  primals_3 = None\n     add = torch.ops.aten.add.Tensor(mul, div);  mul = None\n     add_1 = torch.ops.aten.add.Tensor(mul_1, div);  mul_1 = div = None\n-    return [add, add_1]\"\"\",\n+    return (add, add_1)\"\"\",\n         )\n \n         # Important pieces of the graph:\n"
                },
                {
                    "old_start": 5415,
                    "old_length": 7,
                    "new_start": 5415,
                    "new_length": 7,
                    "hunk": "@@ -5415,7 +5415,7 @@ def forward(self, tangents_1, tangents_2):\n     div_2 = torch.ops.aten.div.Tensor(tangents_2, 2)\n     mul_2 = torch.ops.aten.mul.Tensor(tangents_1, 6);  tangents_1 = None\n     mul_3 = torch.ops.aten.mul.Tensor(tangents_2, 6);  tangents_2 = None\n-    return [mul_2, mul_3, div_1, div_2]\"\"\",\n+    return (mul_2, mul_3, div_1, div_2)\"\"\",\n         )\n \n     def test_aot_dispatch_inference(self):\n"
                },
                {
                    "old_start": 5730,
                    "old_length": 6,
                    "new_start": 5730,
                    "new_length": 59,
                    "hunk": "@@ -5730,6 +5730,59 @@ def forward(self, tangents_1, tangents_2):\n         self.assertEqual(a_ref_base.grad.a, a_test_base.grad.a)\n         self.assertEqual(a_ref_base.grad.b, a_test_base.grad.b)\n \n+    def test_aot_dispatch_output_requires_grad_in_no_grad(self):\n+        def fn(x):\n+            out1 = x.sin()\n+            with torch.enable_grad():\n+                out2 = x.cos()\n+            return out1, out2\n+\n+        inp_fns = [\n+            lambda: torch.ones(10, requires_grad=True),\n+            lambda: torch.ones(10, requires_grad=False),\n+        ]\n+\n+        compiled_f = aot_function(fn, nop)\n+        for inp_fn in inp_fns:\n+            with torch.no_grad():\n+                ref_x = inp_fn()\n+                ref_out = fn(ref_x)\n+                x = inp_fn()\n+                out = compiled_f(x)\n+                for r, o in zip(ref_out, out):\n+                    self.assertEqual(r.requires_grad, o.requires_grad)\n+            if ref_x.requires_grad:\n+                with torch.enable_grad():\n+                    (ref_out[0] + ref_out[1]).sum().backward()\n+                    (out[0] + out[1]).sum().backward()\n+                    self.assertEqual(ref_x.grad, x.grad)\n+                    assert torch.allclose(ref_x.grad, x.grad, atol=1e-3, rtol=1e-3)\n+\n+    def test_aot_dispatch_output_requires_grad_in_no_grad_views(self):\n+        # view-type ops preserve requires_grad even in no_grad.\n+        def fn(x):\n+            return x.view(-1), x.sin()\n+\n+        inference_graph_cell = [None]\n+        inference_compiler = make_boxed_compiler(\n+            partial(extract_graph, graph_cell=inference_graph_cell)\n+        )\n+        compiled_fn = aot_function(fn, nop, inference_compiler=inference_compiler)\n+\n+        inp_x0 = torch.ones(2, 3, requires_grad=True)\n+        # Clone in no_grad will make requires_grad=False tensors, keep clone outside of no_grad\n+        ref_x0 = inp_x0.clone()\n+        x0 = inp_x0.clone()\n+        with torch.no_grad():\n+            ref_out1, ref_out2 = fn(ref_x0)\n+\n+            out1, out2 = compiled_fn(x0)\n+            # Assert that we executed inference graph\n+            self.assertTrue(inference_graph_cell[0] is not None)\n+\n+            self.assertEqual(ref_out1.requires_grad, out1.requires_grad)\n+            self.assertEqual(ref_out2.requires_grad, out2.requires_grad)\n+\n \n class TestAOTModuleSimplified(AOTTestCase):\n     def test_aot_module_simplified(self):\n"
                }
            ],
            "whole_deleted": "-    return [mul, mul_1]\"\"\",\n-    return [add]\"\"\",\n-    return [add]\"\"\",\n-    return [addmm, primals_3, t]\"\"\",\n-    return [mul, mul]\"\"\",\n-    return [mul, mul_1, add_1]\"\"\",\n-    return [add]\"\"\",\n-    return [sin, primals_1]\"\"\",\n-    return [sin, cat]\"\"\",\n-    return [add]\"\"\",\n-    return [add]\"\"\",\n-def forward(self, primals_1):\n-    view = torch.ops.aten.view.default(primals_1, [-1]);  primals_1 = None\n-    return [view]\"\"\",\n-    return [mul, mul_1, view, view_2]\"\"\",\n-    return [mul, t, view_1, view_4, view_3]\"\"\",\n-    return [add, view_1]\"\"\",\n-    return [transpose, add, add_1, diagonal, add_2]\"\"\",\n-    return [view]\"\"\",\n-    return [view, add]\"\"\",\n-    return [view, view_1, mul]\"\"\",\n-    return [view, mul]\"\"\",\n-    return [mul, view]\"\"\",\n-    return [view, mul, detach_2]\"\"\",\n-    return [t, add]\"\"\",\n-    return [view, transpose, transpose_1, mul]\"\"\",\n-    return [transpose, squeeze, transpose_1, unsqueeze, mul]\"\"\",\n-    return [t_4, view_1]\"\"\",\n-def forward(self, primals_1, primals_2):\n-    view = torch.ops.aten.view.default(primals_1, [3, 3]);  primals_1 = None\n-    t = torch.ops.aten.t.default(view);  view = None\n-    view_1 = torch.ops.aten.view.default(primals_2, [3, 3]);  primals_2 = None\n-    view_2 = torch.ops.aten.view.default(t, [3, 3])\n-    return [t, view_1, view_2]\"\"\",\n-    return [mul, mul_1]\"\"\",\n-    return [as_strided_scatter, add_1]\"\"\",\n-    return [as_strided_scatter, add_1]\"\"\",\n-    return [as_strided_scatter, view_1]\"\"\",\n-    return [as_strided_scatter, add, view_1]\"\"\",\n-    return [add]\"\"\",\n-    return [as_strided_scatter, add, add_1]\"\"\",\n-    return [as_strided_scatter, add_2, view_2, unsqueeze_1]\"\"\",\n-    return [as_strided_scatter, t, view_1, t_1, unsqueeze, add]\"\"\",\n-    return [tangents_1]\"\"\",\n-    return [add_1, primals_1, primals_2, primals_4, mul]\"\"\",\n-    return [add_1, primals_1, primals_3]\"\"\",\n-    return [getitem, primals_1, primals_6, getitem_1, getitem_2, getitem_3, getitem_4]\"\"\",  # noqa: B950\n-    return [getitem_6, getitem_7, None, None, None, getitem_5]\"\"\",  # noqa: B950\n-    return [add, add]\"\"\",\n-    return [None, tangents_1]\"\"\",\n-    return [add, add_1]\"\"\",\n-    return [mul_2, mul_3, div_1, div_2]\"\"\",\n",
            "whole_added": "+    return (mul, mul_1)\"\"\",\n+    return (add,)\"\"\",\n+    return (add,)\"\"\",\n+    return (addmm, primals_3, t)\"\"\",\n+    return (mul, mul)\"\"\",\n+    return (mul, mul_1, add_1)\"\"\",\n+    return (add,)\"\"\",\n+    return (sin, primals_1)\"\"\",\n+    return (sin, cat)\"\"\",\n+    return (add,)\"\"\",\n+    return (add,)\"\"\",\n+def forward(self, arg0_1):\n+    view = torch.ops.aten.view.default(arg0_1, [-1]);  arg0_1 = None\n+    return (view,)\"\"\",\n+    return (mul, mul_1, view, view_2)\"\"\",\n+    return (mul, t, view_1, view_4, view_3)\"\"\",\n+    return (add, view_1)\"\"\",\n+    return (transpose, add, add_1, diagonal, add_2)\"\"\",\n+    return (view,)\"\"\",\n+    return (view, add)\"\"\",\n+    return (view, view_1, mul)\"\"\",\n+    return (view, mul)\"\"\",\n+    return (mul, view)\"\"\",\n+    return (view, mul, detach_2)\"\"\",\n+    return (t, add)\"\"\",\n+    return (view, transpose, transpose_1, mul)\"\"\",\n+    return (transpose, squeeze, transpose_1, unsqueeze, mul)\"\"\",\n+    return (t_4, view_1)\"\"\",\n+def forward(self, arg0_1, arg1_1):\n+    t = torch.ops.aten.t.default(arg0_1);  arg0_1 = None\n+    view = torch.ops.aten.view.default(arg1_1, [3, 3]);  arg1_1 = None\n+    view_1 = torch.ops.aten.view.default(t, [3, 3])\n+    return (t, view, view_1)\"\"\",\n+    return (mul, mul_1)\"\"\",\n+    return (as_strided_scatter, add_1)\"\"\",\n+    return (as_strided_scatter, add_1)\"\"\",\n+    return (as_strided_scatter, view_1)\"\"\",\n+    return (as_strided_scatter, add, view_1)\"\"\",\n+    return (add,)\"\"\",\n+    return (as_strided_scatter, add, add_1)\"\"\",\n+    return (as_strided_scatter, add_2, view_2, unsqueeze_1)\"\"\",\n+    return (as_strided_scatter, t, view_1, t_1, unsqueeze, add)\"\"\",\n+    return (tangents_1,)\"\"\",\n+    return (add_1, primals_1, primals_2, primals_4, mul)\"\"\",\n+    return (add_1, primals_1, primals_3)\"\"\",\n+    return (getitem, primals_1, primals_6, getitem_1, getitem_2, getitem_3, getitem_4)\"\"\",  # noqa: B950\n+    return (getitem_6, getitem_7, None, None, None, getitem_5)\"\"\",  # noqa: B950\n+    return (add, add)\"\"\",\n+    return (None, tangents_1)\"\"\",\n+            outs = list(outs)\n+    return (add, add_1)\"\"\",\n+    return (mul_2, mul_3, div_1, div_2)\"\"\",\n+    def test_aot_dispatch_output_requires_grad_in_no_grad(self):\n+        def fn(x):\n+            out1 = x.sin()\n+            with torch.enable_grad():\n+                out2 = x.cos()\n+            return out1, out2\n+\n+        inp_fns = [\n+            lambda: torch.ones(10, requires_grad=True),\n+            lambda: torch.ones(10, requires_grad=False),\n+        ]\n+\n+        compiled_f = aot_function(fn, nop)\n+        for inp_fn in inp_fns:\n+            with torch.no_grad():\n+                ref_x = inp_fn()\n+                ref_out = fn(ref_x)\n+                x = inp_fn()\n+                out = compiled_f(x)\n+                for r, o in zip(ref_out, out):\n+                    self.assertEqual(r.requires_grad, o.requires_grad)\n+            if ref_x.requires_grad:\n+                with torch.enable_grad():\n+                    (ref_out[0] + ref_out[1]).sum().backward()\n+                    (out[0] + out[1]).sum().backward()\n+                    self.assertEqual(ref_x.grad, x.grad)\n+                    assert torch.allclose(ref_x.grad, x.grad, atol=1e-3, rtol=1e-3)\n+\n+    def test_aot_dispatch_output_requires_grad_in_no_grad_views(self):\n+        # view-type ops preserve requires_grad even in no_grad.\n+        def fn(x):\n+            return x.view(-1), x.sin()\n+\n+        inference_graph_cell = [None]\n+        inference_compiler = make_boxed_compiler(\n+            partial(extract_graph, graph_cell=inference_graph_cell)\n+        )\n+        compiled_fn = aot_function(fn, nop, inference_compiler=inference_compiler)\n+\n+        inp_x0 = torch.ones(2, 3, requires_grad=True)\n+        # Clone in no_grad will make requires_grad=False tensors, keep clone outside of no_grad\n+        ref_x0 = inp_x0.clone()\n+        x0 = inp_x0.clone()\n+        with torch.no_grad():\n+            ref_out1, ref_out2 = fn(ref_x0)\n+\n+            out1, out2 = compiled_fn(x0)\n+            # Assert that we executed inference graph\n+            self.assertTrue(inference_graph_cell[0] is not None)\n+\n+            self.assertEqual(ref_out1.requires_grad, out1.requires_grad)\n+            self.assertEqual(ref_out2.requires_grad, out2.requires_grad)\n+\n",
            "whole_hunk": "@@ -573,7 +573,7 @@ def forward(self, primals_1):\n     clone = torch.ops.aten.clone.default(primals_1);  primals_1 = None\n     mul = torch.ops.aten.mul.Tensor(clone, 2);  clone = None\n     mul_1 = torch.ops.aten.mul.Tensor(mul, 3)\n-    return [mul, mul_1]\"\"\",\n+    return (mul, mul_1)\"\"\",\n         )\n \n     def test_input_mutation_set__input_mutation(self):\n@@ -643,7 +643,7 @@ def forward(self, primals_1, primals_2):\n     add = torch.ops.aten.add.Tensor(mul, mul)\n     set_ = torch.ops.aten.set_.source_Tensor(primals_1, mul);  primals_1 = None\n     copy_ = torch.ops.aten.copy_.default(primals_2, mul);  primals_2 = mul = None\n-    return [add]\"\"\",\n+    return (add,)\"\"\",\n         )\n \n     # This is a (hopefully) extremely rare case that is difficult to handle,\n@@ -722,7 +722,7 @@ def forward(self, primals_1):\n     alias = torch.ops.aten.alias.default(primals_1);  primals_1 = None\n     view = torch.ops.aten.view.default(arange, [3, 3]);  arange = None\n     add = torch.ops.aten.add.Tensor(alias, view);  alias = view = None\n-    return [add]\"\"\",\n+    return (add,)\"\"\",\n         )\n \n     @unittest.skipIf(IS_WINDOWS, \"TODO: need to fix the test case\")\n@@ -1020,7 +1020,7 @@ def forward(self, primals_1):\n def forward(self, primals_1, primals_2, primals_3):\n     t = torch.ops.aten.t.default(primals_1);  primals_1 = None\n     addmm = torch.ops.aten.addmm.default(primals_2, primals_3, t);  primals_2 = None\n-    return [addmm, primals_3, t]\"\"\",\n+    return (addmm, primals_3, t)\"\"\",\n         )\n \n         with torch.inference_mode():\n@@ -1071,7 +1071,7 @@ def forward(self, arg0_1, arg1_1, arg2_1):\n def forward(self, primals_1):\n     clone = torch.ops.aten.clone.default(primals_1);  primals_1 = None\n     mul = torch.ops.aten.mul.Tensor(clone, 2);  clone = None\n-    return [mul, mul]\"\"\",\n+    return (mul, mul)\"\"\",\n         )\n \n     def test_input_mutation_multiple(self):\n@@ -1100,7 +1100,7 @@ def forward(self, primals_1, primals_2, primals_3):\n     mul_1 = torch.ops.aten.mul.Tensor(clone_1, 2);  clone_1 = None\n     add = torch.ops.aten.add.Tensor(mul, primals_2);  primals_2 = None\n     add_1 = torch.ops.aten.add.Tensor(add, mul_1);  add = None\n-    return [mul, mul_1, add_1]\"\"\",\n+    return (mul, mul_1, add_1)\"\"\",\n         )\n \n     def test_input_mutation_return(self):\n@@ -1171,7 +1171,7 @@ def forward(self, primals_1):\n     copy = torch.ops.aten.copy.default(primals_1, ones);  ones = None\n     add = torch.ops.aten.add.Tensor(copy, 1)\n     copy_ = torch.ops.aten.copy_.default(primals_1, copy);  primals_1 = copy = None\n-    return [add]\"\"\",\n+    return (add,)\"\"\",\n         )\n \n     def test_input_mutation_storage_resize_down(self):\n@@ -1204,7 +1204,7 @@ def forward(self, primals_1):\n def forward(self, primals_1):\n     sin = torch.ops.aten.sin.default(primals_1)\n     resize_storage_bytes_ = torch.ops.inductor.resize_storage_bytes_.default(primals_1, 0)\n-    return [sin, primals_1]\"\"\",\n+    return (sin, primals_1)\"\"\",\n         )\n \n     #     def test_input_mutation_storage_resize_up_down(self):\n@@ -1305,7 +1305,7 @@ def forward(self, primals_1, primals_2):\n     sin = torch.ops.aten.sin.default(cat)\n     resize_storage_bytes_ = torch.ops.inductor.resize_storage_bytes_.default(cat, 0)\n     set_ = torch.ops.aten.set_.source_Tensor(primals_1, cat);  primals_1 = None\n-    return [sin, cat]\"\"\",\n+    return (sin, cat)\"\"\",\n         )\n \n     def test_input_mutation_storage_resize_before_set_(self):\n@@ -1401,7 +1401,7 @@ def forward(self, primals_1):\n     view_1 = torch.ops.aten.view.default(mul, [4]);  mul = None\n     add = torch.ops.aten.add.Tensor(view_1, 1)\n     copy_ = torch.ops.aten.copy_.default(primals_1, view_1);  primals_1 = view_1 = None\n-    return [add]\"\"\",\n+    return (add,)\"\"\",\n         )\n \n     def test_input_mutation_requires_grad_no_grad(self):\n@@ -1423,7 +1423,7 @@ def forward(self, primals_1):\n     mul = torch.ops.aten.mul.Tensor(primals_1, 2)\n     add = torch.ops.aten.add.Tensor(mul, 3)\n     copy_ = torch.ops.aten.copy_.default(primals_1, mul);  primals_1 = mul = None\n-    return [add]\"\"\",\n+    return (add,)\"\"\",\n         )\n \n     def test_input_mutation_requires_grad_no_grad_inference_graph(self):\n@@ -1547,9 +1547,9 @@ def forward(self, arg0_1):\n         self.assertExpectedInline(\n             fw_graph.code.strip(),\n             \"\"\"\\\n-def forward(self, primals_1):\n-    view = torch.ops.aten.view.default(primals_1, [-1]);  primals_1 = None\n-    return [view]\"\"\",\n+def forward(self, arg0_1):\n+    view = torch.ops.aten.view.default(arg0_1, [-1]);  arg0_1 = None\n+    return (view,)\"\"\",\n         )\n \n     def test_input_output_view_mutate_multiple(self):\n@@ -1581,7 +1581,7 @@ def forward(self, primals_1, primals_2, primals_3):\n     mul_1 = torch.ops.aten.mul.Tensor(clone_1, 3);  clone_1 = None\n     view = torch.ops.aten.view.default(primals_2, [2, 2]);  primals_2 = None\n     view_2 = torch.ops.aten.view.default(mul_1, [2, 2])\n-    return [mul, mul_1, view, view_2]\"\"\",\n+    return (mul, mul_1, view, view_2)\"\"\",\n         )\n \n     def test_input_output_view_metadata_mutate_multiple(self):\n@@ -1615,7 +1615,7 @@ def forward(self, primals_1, primals_2, primals_3):\n     view_1 = torch.ops.aten.view.default(primals_1, [2, 2]);  primals_1 = None\n     view_3 = torch.ops.aten.view.default(t, [2, 2])\n     view_4 = torch.ops.aten.view.default(mul, [2, 2])\n-    return [mul, t, view_1, view_4, view_3]\"\"\",\n+    return (mul, t, view_1, view_4, view_3)\"\"\",\n         )\n \n     def test_input_mutation_and_output_view(self):\n@@ -1637,7 +1637,7 @@ def forward(self, primals_1):\n     clone = torch.ops.aten.clone.default(primals_1);  primals_1 = None\n     add = torch.ops.aten.add.Tensor(clone, 1);  clone = None\n     view_1 = torch.ops.aten.view.default(add, [-1])\n-    return [add, view_1]\"\"\",\n+    return (add, view_1)\"\"\",\n         )\n \n     def test_input_mutation_output_view_multiple(self):\n@@ -1671,7 +1671,7 @@ def forward(self, primals_1, primals_2, primals_3, primals_4):\n     add_1 = torch.ops.aten.add.Tensor(primals_4, 1);  primals_4 = None\n     diagonal = torch.ops.aten.diagonal.default(transpose)\n     add_2 = torch.ops.aten.add.Tensor(primals_1, add);  primals_1 = None\n-    return [transpose, add, add_1, diagonal, add_2]\"\"\",\n+    return (transpose, add, add_1, diagonal, add_2)\"\"\",\n         )\n \n     def test_output_aliases_intermediate_single(self):\n@@ -1691,7 +1691,7 @@ def forward(self, primals_1, primals_2, primals_3, primals_4):\n def forward(self, primals_1):\n     mul = torch.ops.aten.mul.Tensor(primals_1, 3);  primals_1 = None\n     view = torch.ops.aten.view.default(mul, [-1]);  mul = None\n-    return [view]\"\"\",\n+    return (view,)\"\"\",\n         )\n \n     def test_output_aliases_input_multi_output_view_should_raise_autograd_error(self):\n@@ -1926,7 +1926,7 @@ def forward(self, primals_1, primals_2):\n     mul = torch.ops.aten.mul.Tensor(primals_1, 3);  primals_1 = None\n     view = torch.ops.aten.view.default(mul, [-1]);  mul = None\n     add = torch.ops.aten.add.Tensor(primals_2, 1);  primals_2 = None\n-    return [view, add]\"\"\",\n+    return (view, add)\"\"\",\n         )\n \n     def test_output_aliases_intermediate_returned_multiple_times(self):\n@@ -1957,7 +1957,7 @@ def forward(self, primals_1):\n     mul = torch.ops.aten.mul.Tensor(primals_1, 3);  primals_1 = None\n     view = torch.ops.aten.view.default(mul, [-1])\n     view_1 = torch.ops.aten.view.default(mul, [-1])\n-    return [view, view_1, mul]\"\"\",\n+    return (view, view_1, mul)\"\"\",\n         )\n \n     def test_output_aliases_intermediate_and_returned(self):\n@@ -1977,7 +1977,7 @@ def forward(self, primals_1):\n def forward(self, primals_1):\n     mul = torch.ops.aten.mul.Tensor(primals_1, 3);  primals_1 = None\n     view = torch.ops.aten.view.default(mul, [-1])\n-    return [view, mul]\"\"\",\n+    return (view, mul)\"\"\",\n         )\n \n     def test_output_aliases_intermediate_and_returned_flipped(self):\n@@ -1997,7 +1997,7 @@ def forward(self, primals_1):\n def forward(self, primals_1):\n     mul = torch.ops.aten.mul.Tensor(primals_1, 3);  primals_1 = None\n     view = torch.ops.aten.view.default(mul, [-1])\n-    return [mul, view]\"\"\",\n+    return (mul, view)\"\"\",\n         )\n \n     def test_output_aliases_intermediate_and_returned_different_grad(self):\n@@ -2021,7 +2021,7 @@ def forward(self, primals_1):\n     detach = torch.ops.aten.detach.default(select);  select = None\n     detach_1 = torch.ops.aten.detach.default(detach);  detach = None\n     detach_2 = torch.ops.aten.detach.default(detach_1);  detach_1 = None\n-    return [view, mul, detach_2]\"\"\",\n+    return (view, mul, detach_2)\"\"\",\n         )\n \n     def test_output_aliases_intermediate_inplace_view(self):\n@@ -2057,7 +2057,7 @@ def forward(self, primals_1):\n     mul = torch.ops.aten.mul.Tensor(primals_1, 3)\n     t = torch.ops.aten.t.default(mul);  mul = None\n     add = torch.ops.aten.add.Tensor(primals_1, 1);  primals_1 = None\n-    return [t, add]\"\"\",\n+    return (t, add)\"\"\",\n         )\n \n     def test_output_aliases_intermediate_inplace_view_and_view(self):\n@@ -2094,7 +2094,7 @@ def forward(self, primals_1):\n     view = torch.ops.aten.view.default(mul, [-1])\n     transpose = torch.ops.aten.transpose.int(mul_1, 1, 0);  mul_1 = None\n     transpose_1 = torch.ops.aten.transpose.int(mul, 1, 0)\n-    return [view, transpose, transpose_1, mul]\"\"\",\n+    return (view, transpose, transpose_1, mul)\"\"\",\n         )\n \n     def test_output_all_alias_types(self):\n@@ -2130,7 +2130,7 @@ def forward(self, primals_1):\n     squeeze = torch.ops.aten.squeeze.default(mul)\n     transpose_1 = torch.ops.aten.transpose.int(mul, 1, 0)\n     unsqueeze = torch.ops.aten.unsqueeze.default(transpose, 0)\n-    return [transpose, squeeze, transpose_1, unsqueeze, mul]\"\"\",\n+    return (transpose, squeeze, transpose_1, unsqueeze, mul)\"\"\",\n         )\n \n     @parametrize(\"req_grad\", [False, True])\n@@ -2180,7 +2180,7 @@ def forward(self, primals_1):\n     t_4 = torch.ops.aten.t.default(t_2)\n     t_6 = torch.ops.aten.t.default(t_2);  t_2 = None\n     view_1 = torch.ops.aten.view.default(t_6, [3, 3]);  t_6 = None\n-    return [t_4, view_1]\"\"\",\n+    return (t_4, view_1)\"\"\",\n         )\n \n     def test_view_and_inplace_view(self):\n@@ -2199,12 +2199,11 @@ def forward(self, primals_1):\n         self.assertExpectedInline(\n             fw_graph.code.strip(),\n             \"\"\"\\\n-def forward(self, primals_1, primals_2):\n-    view = torch.ops.aten.view.default(primals_1, [3, 3]);  primals_1 = None\n-    t = torch.ops.aten.t.default(view);  view = None\n-    view_1 = torch.ops.aten.view.default(primals_2, [3, 3]);  primals_2 = None\n-    view_2 = torch.ops.aten.view.default(t, [3, 3])\n-    return [t, view_1, view_2]\"\"\",\n+def forward(self, arg0_1, arg1_1):\n+    t = torch.ops.aten.t.default(arg0_1);  arg0_1 = None\n+    view = torch.ops.aten.view.default(arg1_1, [3, 3]);  arg1_1 = None\n+    view_1 = torch.ops.aten.view.default(t, [3, 3])\n+    return (t, view, view_1)\"\"\",\n         )\n \n     def test_view_detach(self):\n@@ -2236,7 +2235,7 @@ def forward(self, primals_1, primals_2):\n def forward(self, primals_1, primals_2):\n     mul = torch.ops.aten.mul.Tensor(primals_1, 3);  primals_1 = None\n     mul_1 = torch.ops.aten.mul.Tensor(primals_2, 4);  primals_2 = None\n-    return [mul, mul_1]\"\"\",\n+    return (mul, mul_1)\"\"\",\n         )\n \n     # This is a torture test:\n@@ -2756,7 +2755,7 @@ def forward(self, primals_1):\n     as_strided_2 = torch.ops.aten.as_strided.default(as_strided_scatter, [2], [1], 0)\n     as_strided_5 = torch.ops.aten.as_strided.default(as_strided_scatter, [2], [1], 0)\n     add_1 = torch.ops.aten.add.Tensor(as_strided_2, as_strided_5);  as_strided_2 = as_strided_5 = None\n-    return [as_strided_scatter, add_1]\"\"\",\n+    return (as_strided_scatter, add_1)\"\"\",\n         )  # noqa: B950\n \n     def test_input_mutation_aliases_other_input2(self):\n@@ -2789,7 +2788,7 @@ def forward(self, primals_1):\n     as_strided_2 = torch.ops.aten.as_strided.default(as_strided_scatter, [2], [1], 0)\n     as_strided_5 = torch.ops.aten.as_strided.default(as_strided_scatter, [2, 2], [2, 1], 0)\n     add_1 = torch.ops.aten.add.Tensor(as_strided_2, as_strided_5);  as_strided_2 = as_strided_5 = None\n-    return [as_strided_scatter, add_1]\"\"\",\n+    return (as_strided_scatter, add_1)\"\"\",\n         )  # noqa: B950\n \n     def test_input_mutation_aliases_and_output_alias(self):\n@@ -2820,7 +2819,7 @@ def forward(self, primals_1):\n     as_strided_scatter = torch.ops.aten.as_strided_scatter.default(clone, add, [4], [1], 0);  clone = add = None\n     as_strided_8 = torch.ops.aten.as_strided.default(as_strided_scatter, [4], [1], 0)\n     view_1 = torch.ops.aten.view.default(as_strided_8, [4]);  as_strided_8 = None\n-    return [as_strided_scatter, view_1]\"\"\",\n+    return (as_strided_scatter, view_1)\"\"\",\n         )  # noqa: B950\n \n     def test_input_aliased_with_mutation_output_alias(self):\n@@ -2857,7 +2856,7 @@ def forward(self, primals_1, primals_2):\n     add = torch.ops.aten.add.Tensor(primals_2, 1);  primals_2 = None\n     as_strided_7 = torch.ops.aten.as_strided.default(as_strided_scatter, [4], [1], 0)\n     view_1 = torch.ops.aten.view.default(as_strided_7, [-1]);  as_strided_7 = None\n-    return [as_strided_scatter, add, view_1]\"\"\",\n+    return (as_strided_scatter, add, view_1)\"\"\",\n         )  # noqa: B950\n \n     def test_input_metadata_mutation_aliases(self):\n@@ -2886,7 +2885,7 @@ def forward(self, primals_1, primals_2):\n def forward(self, primals_1, primals_2):\n     t = torch.ops.aten.t.default(primals_1);  primals_1 = None\n     add = torch.ops.aten.add.Tensor(t, primals_2);  t = primals_2 = None\n-    return [add]\"\"\",\n+    return (add,)\"\"\",\n         )\n \n     def test_input_mutation_aliases_and_none_require_gradients(self):\n@@ -2929,7 +2928,7 @@ def forward(self, primals_1, primals_2):\n     as_strided_3 = torch.ops.aten.as_strided.default(as_strided_scatter, [4], [1], 0)\n     add = torch.ops.aten.add.Tensor(as_strided_3, 1);  as_strided_3 = None\n     add_1 = torch.ops.aten.add.Tensor(primals_2, 1);  primals_2 = None\n-    return [as_strided_scatter, add, add_1]\"\"\",\n+    return (as_strided_scatter, add, add_1)\"\"\",\n         )  # noqa: B950\n \n     @skipIfDynamoInput(\"Fails with dynamo\")\n@@ -2988,7 +2987,7 @@ def forward(self, primals_1, primals_2, primals_3):\n     add_2 = torch.ops.aten.add.Tensor(add_1, unsqueeze_1);  add_1 = None\n     as_strided_14 = torch.ops.aten.as_strided.default(as_strided_scatter, [4], [1], 0)\n     view_2 = torch.ops.aten.view.default(as_strided_14, [-1]);  as_strided_14 = None\n-    return [as_strided_scatter, add_2, view_2, unsqueeze_1]\"\"\",\n+    return (as_strided_scatter, add_2, view_2, unsqueeze_1)\"\"\",\n         )  # noqa: B950\n \n     @unittest.skipIf(not torch.cuda.is_available(), \"CUDA is unavailable\")\n@@ -3062,7 +3061,7 @@ def forward(self, primals_1, primals_2):\n     view_1 = torch.ops.aten.view.default(add, [-1])\n     t_1 = torch.ops.aten.t.default(t)\n     unsqueeze = torch.ops.aten.unsqueeze.default(view_1, 0)\n-    return [as_strided_scatter, t, view_1, t_1, unsqueeze, add]\"\"\",\n+    return (as_strided_scatter, t, view_1, t_1, unsqueeze, add)\"\"\",\n         )  # noqa: B950\n \n     def test_dynamic_shape_output_not_in_bw_graph(self):\n@@ -3090,7 +3089,7 @@ def forward(self, primals_1, primals_2):\n             bw_graph_cell[0].code.strip(),\n             \"\"\"\\\n def forward(self, tangents_1):\n-    return [tangents_1]\"\"\",\n+    return (tangents_1,)\"\"\",\n         )\n \n     def test_no_grad_input_output(self):\n@@ -3610,7 +3609,7 @@ def forward(self, primals_1, primals_2, primals_3, primals_4):\n     sum_2 = torch.ops.aten.sum.default(add)\n     add_1 = torch.ops.aten.add.Tensor(sum_1, sum_2);  sum_1 = sum_2 = None\n     copy_ = torch.ops.aten.copy_.default(primals_3, add);  primals_3 = add = None\n-    return [add_1, primals_1, primals_2, primals_4, mul]\"\"\",\n+    return (add_1, primals_1, primals_2, primals_4, mul)\"\"\",\n         )\n \n         self.assertEqual(out_ref, out_test)\n@@ -3665,7 +3664,7 @@ def forward(self, primals_1, primals_2, primals_3):\n     sum_2 = torch.ops.aten.sum.default(add)\n     add_1 = torch.ops.aten.add.Tensor(sum_1, sum_2);  sum_1 = sum_2 = None\n     copy_ = torch.ops.aten.copy_.default(primals_2, add);  primals_2 = add = None\n-    return [add_1, primals_1, primals_3]\"\"\",\n+    return (add_1, primals_1, primals_3)\"\"\",\n         )\n         self.assertEqual(out_ref, out_test)\n \n@@ -3723,7 +3722,7 @@ def forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals\n     copy_ = torch.ops.aten.copy_.default(primals_3, getitem_3);  primals_3 = None\n     copy__1 = torch.ops.aten.copy_.default(primals_4, getitem_4);  primals_4 = None\n     copy__2 = torch.ops.aten.copy_.default(primals_5, add);  primals_5 = add = None\n-    return [getitem, primals_1, primals_6, getitem_1, getitem_2, getitem_3, getitem_4]\"\"\",  # noqa: B950\n+    return (getitem, primals_1, primals_6, getitem_1, getitem_2, getitem_3, getitem_4)\"\"\",  # noqa: B950\n         )\n \n         self.assertEqual(out_ref, out_test)\n@@ -3743,7 +3742,7 @@ def forward(self, primals_1, primals_6, getitem_1, getitem_2, getitem_3, getitem\n     getitem_5 = native_batch_norm_backward[0]\n     getitem_6 = native_batch_norm_backward[1]\n     getitem_7 = native_batch_norm_backward[2];  native_batch_norm_backward = None\n-    return [getitem_6, getitem_7, None, None, None, getitem_5]\"\"\",  # noqa: B950\n+    return (getitem_6, getitem_7, None, None, None, getitem_5)\"\"\",  # noqa: B950\n         )\n \n         self.assertEqual(inp_ref.grad, inp_test.grad)\n@@ -3784,7 +3783,7 @@ def forward(self, primals_1, primals_6, getitem_1, getitem_2, getitem_3, getitem\n def forward(self, primals_1, primals_2):\n     clone = torch.ops.aten.clone.default(primals_1);  primals_1 = None\n     add = torch.ops.aten.add.Tensor(clone, primals_2);  clone = primals_2 = None\n-    return [add, add]\"\"\",\n+    return (add, add)\"\"\",\n         )  # noqa: B950\n \n         self.assertEqual(out_ref, out_test)\n@@ -3796,7 +3795,7 @@ def forward(self, primals_1, primals_2):\n             bw_graph_cell[0].code.strip(),\n             \"\"\"\\\n def forward(self, tangents_1):\n-    return [None, tangents_1]\"\"\",\n+    return (None, tangents_1)\"\"\",\n         )  # noqa: B950\n \n     def test_real_weights_in_symbolic_mode(self):\n@@ -3884,6 +3883,7 @@ def forward(self, tangents_1):\n         # since they are the only ones that will get reconstructed.\n         def wrapper(g, *args, **kwargs):\n             outs = g(*args, **kwargs)\n+            outs = list(outs)\n             for i in output_view_indices:\n                 outs[i] = NoViewReplayTensor(outs[i])\n             return outs\n@@ -5397,7 +5397,7 @@ def forward(self, primals_1, primals_2, primals_3):\n     div = torch.ops.aten.div.Tensor(primals_3, 2);  primals_3 = None\n     add = torch.ops.aten.add.Tensor(mul, div);  mul = None\n     add_1 = torch.ops.aten.add.Tensor(mul_1, div);  mul_1 = div = None\n-    return [add, add_1]\"\"\",\n+    return (add, add_1)\"\"\",\n         )\n \n         # Important pieces of the graph:\n@@ -5415,7 +5415,7 @@ def forward(self, tangents_1, tangents_2):\n     div_2 = torch.ops.aten.div.Tensor(tangents_2, 2)\n     mul_2 = torch.ops.aten.mul.Tensor(tangents_1, 6);  tangents_1 = None\n     mul_3 = torch.ops.aten.mul.Tensor(tangents_2, 6);  tangents_2 = None\n-    return [mul_2, mul_3, div_1, div_2]\"\"\",\n+    return (mul_2, mul_3, div_1, div_2)\"\"\",\n         )\n \n     def test_aot_dispatch_inference(self):\n@@ -5730,6 +5730,59 @@ def forward(self, tangents_1, tangents_2):\n         self.assertEqual(a_ref_base.grad.a, a_test_base.grad.a)\n         self.assertEqual(a_ref_base.grad.b, a_test_base.grad.b)\n \n+    def test_aot_dispatch_output_requires_grad_in_no_grad(self):\n+        def fn(x):\n+            out1 = x.sin()\n+            with torch.enable_grad():\n+                out2 = x.cos()\n+            return out1, out2\n+\n+        inp_fns = [\n+            lambda: torch.ones(10, requires_grad=True),\n+            lambda: torch.ones(10, requires_grad=False),\n+        ]\n+\n+        compiled_f = aot_function(fn, nop)\n+        for inp_fn in inp_fns:\n+            with torch.no_grad():\n+                ref_x = inp_fn()\n+                ref_out = fn(ref_x)\n+                x = inp_fn()\n+                out = compiled_f(x)\n+                for r, o in zip(ref_out, out):\n+                    self.assertEqual(r.requires_grad, o.requires_grad)\n+            if ref_x.requires_grad:\n+                with torch.enable_grad():\n+                    (ref_out[0] + ref_out[1]).sum().backward()\n+                    (out[0] + out[1]).sum().backward()\n+                    self.assertEqual(ref_x.grad, x.grad)\n+                    assert torch.allclose(ref_x.grad, x.grad, atol=1e-3, rtol=1e-3)\n+\n+    def test_aot_dispatch_output_requires_grad_in_no_grad_views(self):\n+        # view-type ops preserve requires_grad even in no_grad.\n+        def fn(x):\n+            return x.view(-1), x.sin()\n+\n+        inference_graph_cell = [None]\n+        inference_compiler = make_boxed_compiler(\n+            partial(extract_graph, graph_cell=inference_graph_cell)\n+        )\n+        compiled_fn = aot_function(fn, nop, inference_compiler=inference_compiler)\n+\n+        inp_x0 = torch.ones(2, 3, requires_grad=True)\n+        # Clone in no_grad will make requires_grad=False tensors, keep clone outside of no_grad\n+        ref_x0 = inp_x0.clone()\n+        x0 = inp_x0.clone()\n+        with torch.no_grad():\n+            ref_out1, ref_out2 = fn(ref_x0)\n+\n+            out1, out2 = compiled_fn(x0)\n+            # Assert that we executed inference graph\n+            self.assertTrue(inference_graph_cell[0] is not None)\n+\n+            self.assertEqual(ref_out1.requires_grad, out1.requires_grad)\n+            self.assertEqual(ref_out2.requires_grad, out2.requires_grad)\n+\n \n class TestAOTModuleSimplified(AOTTestCase):\n     def test_aot_module_simplified(self):\n"
        },
        {
            "name": "test_flex_attention.py",
            "path": "test/inductor/test_flex_attention.py",
            "patches": [
                {
                    "old_start": 1529,
                    "old_length": 7,
                    "new_start": 1529,
                    "new_length": 7,
                    "hunk": "@@ -1529,7 +1529,7 @@ class GraphModule(torch.nn.Module):\n         getitem_4: \"f64[2, 2, 128, 4]\" = flex_attention_backward[0]\n         getitem_5: \"f64[2, 2, 128, 4]\" = flex_attention_backward[1]\n         getitem_6: \"f64[2, 2, 128, 4]\" = flex_attention_backward[2];  flex_attention_backward = None\n-        return [getitem_4, getitem_5, getitem_6]\n+        return (getitem_4, getitem_5, getitem_6)\n \n     class <lambda>(torch.nn.Module):\n         def forward(self, arg0_1: \"f64[]\", arg1_1: \"i32[]\", arg2_1: \"i32[]\", arg3_1: \"i32[]\", arg4_1: \"i32[]\"):\n"
                }
            ],
            "whole_deleted": "-        return [getitem_4, getitem_5, getitem_6]\n",
            "whole_added": "+        return (getitem_4, getitem_5, getitem_6)\n",
            "whole_hunk": "@@ -1529,7 +1529,7 @@ class GraphModule(torch.nn.Module):\n         getitem_4: \"f64[2, 2, 128, 4]\" = flex_attention_backward[0]\n         getitem_5: \"f64[2, 2, 128, 4]\" = flex_attention_backward[1]\n         getitem_6: \"f64[2, 2, 128, 4]\" = flex_attention_backward[2];  flex_attention_backward = None\n-        return [getitem_4, getitem_5, getitem_6]\n+        return (getitem_4, getitem_5, getitem_6)\n \n     class <lambda>(torch.nn.Module):\n         def forward(self, arg0_1: \"f64[]\", arg1_1: \"i32[]\", arg2_1: \"i32[]\", arg3_1: \"i32[]\", arg4_1: \"i32[]\"):\n"
        },
        {
            "name": "runtime_wrappers.py",
            "path": "torch/_functorch/_aot_autograd/runtime_wrappers.py",
            "patches": [
                {
                    "old_start": 304,
                    "old_length": 7,
                    "new_start": 304,
                    "new_length": 13,
                    "hunk": "@@ -304,7 +304,13 @@ def _create_runtime_wrapper(\n             for idx in indices_of_inps_to_detach:\n                 if isinstance(args_[idx], torch.Tensor):\n                     args_[idx] = args_[idx].detach()\n-            with torch.autograd._force_original_view_tracking(True):\n+\n+            # It's possible to have trace_joint inside user specified with no_grad() region,\n+            # if there is a nested with enable_grad(), that forces some outputs to require gradients.\n+            # Therefore, we unconditionally turn on enable_grad() for compiled_fn execution.\n+            with torch.autograd._force_original_view_tracking(\n+                True\n+            ), torch.enable_grad():\n                 all_outs = call_func_at_runtime_with_args(\n                     compiled_fn, args_, disable_amp=disable_amp, steal_args=True\n                 )\n"
                }
            ],
            "whole_deleted": "-            with torch.autograd._force_original_view_tracking(True):\n",
            "whole_added": "+\n+            # It's possible to have trace_joint inside user specified with no_grad() region,\n+            # if there is a nested with enable_grad(), that forces some outputs to require gradients.\n+            # Therefore, we unconditionally turn on enable_grad() for compiled_fn execution.\n+            with torch.autograd._force_original_view_tracking(\n+                True\n+            ), torch.enable_grad():\n",
            "whole_hunk": "@@ -304,7 +304,13 @@ def _create_runtime_wrapper(\n             for idx in indices_of_inps_to_detach:\n                 if isinstance(args_[idx], torch.Tensor):\n                     args_[idx] = args_[idx].detach()\n-            with torch.autograd._force_original_view_tracking(True):\n+\n+            # It's possible to have trace_joint inside user specified with no_grad() region,\n+            # if there is a nested with enable_grad(), that forces some outputs to require gradients.\n+            # Therefore, we unconditionally turn on enable_grad() for compiled_fn execution.\n+            with torch.autograd._force_original_view_tracking(\n+                True\n+            ), torch.enable_grad():\n                 all_outs = call_func_at_runtime_with_args(\n                     compiled_fn, args_, disable_amp=disable_amp, steal_args=True\n                 )\n"
        },
        {
            "name": "aot_autograd.py",
            "path": "torch/_functorch/aot_autograd.py",
            "patches": [
                {
                    "old_start": 572,
                    "old_length": 9,
                    "new_start": 572,
                    "new_length": 8,
                    "hunk": "@@ -572,9 +572,8 @@ def create_aot_dispatcher_function(\n \n         fake_flat_args = process_inputs(flat_args)\n \n-        needs_autograd = (\n-            any(x.requires_grad for x in fake_flat_args if isinstance(x, Tensor))\n-            and torch.is_grad_enabled()\n+        needs_autograd = any(\n+            x.requires_grad for x in fake_flat_args if isinstance(x, Tensor)\n         )\n \n         with enable_python_dispatcher():\n"
                },
                {
                    "old_start": 600,
                    "old_length": 7,
                    "new_start": 599,
                    "new_length": 17,
                    "hunk": "@@ -600,7 +599,17 @@ def create_aot_dispatcher_function(\n                 )\n \n                 output_and_mutation_safe = not any(\n-                    x.requires_grad for x in fw_metadata.output_info\n+                    x.requires_grad\n+                    # view-type operations preserve requires_grad even in no_grad.\n+                    # Do not count aliases of inputs with requires_grad as reason to make a training graph,\n+                    # as AOTAutograd will perform view-replay to regenerate the view outputs at runtime,\n+                    # setting their grad_fn properly.\n+                    and not (\n+                        x.output_type\n+                        in (OutputType.alias_of_input, OutputType.is_input)\n+                        and fw_metadata.input_info[x.base_idx].requires_grad\n+                    )\n+                    for x in fw_metadata.output_info\n                 ) and not any(\n                     x.requires_grad\n                     and x.mutates_data\n"
                }
            ],
            "whole_deleted": "-        needs_autograd = (\n-            any(x.requires_grad for x in fake_flat_args if isinstance(x, Tensor))\n-            and torch.is_grad_enabled()\n-                    x.requires_grad for x in fw_metadata.output_info\n",
            "whole_added": "+        needs_autograd = any(\n+            x.requires_grad for x in fake_flat_args if isinstance(x, Tensor)\n+                    x.requires_grad\n+                    # view-type operations preserve requires_grad even in no_grad.\n+                    # Do not count aliases of inputs with requires_grad as reason to make a training graph,\n+                    # as AOTAutograd will perform view-replay to regenerate the view outputs at runtime,\n+                    # setting their grad_fn properly.\n+                    and not (\n+                        x.output_type\n+                        in (OutputType.alias_of_input, OutputType.is_input)\n+                        and fw_metadata.input_info[x.base_idx].requires_grad\n+                    )\n+                    for x in fw_metadata.output_info\n",
            "whole_hunk": "@@ -572,9 +572,8 @@ def create_aot_dispatcher_function(\n \n         fake_flat_args = process_inputs(flat_args)\n \n-        needs_autograd = (\n-            any(x.requires_grad for x in fake_flat_args if isinstance(x, Tensor))\n-            and torch.is_grad_enabled()\n+        needs_autograd = any(\n+            x.requires_grad for x in fake_flat_args if isinstance(x, Tensor)\n         )\n \n         with enable_python_dispatcher():\n@@ -600,7 +599,17 @@ def create_aot_dispatcher_function(\n                 )\n \n                 output_and_mutation_safe = not any(\n-                    x.requires_grad for x in fw_metadata.output_info\n+                    x.requires_grad\n+                    # view-type operations preserve requires_grad even in no_grad.\n+                    # Do not count aliases of inputs with requires_grad as reason to make a training graph,\n+                    # as AOTAutograd will perform view-replay to regenerate the view outputs at runtime,\n+                    # setting their grad_fn properly.\n+                    and not (\n+                        x.output_type\n+                        in (OutputType.alias_of_input, OutputType.is_input)\n+                        and fw_metadata.input_info[x.base_idx].requires_grad\n+                    )\n+                    for x in fw_metadata.output_info\n                 ) and not any(\n                     x.requires_grad\n                     and x.mutates_data\n"
        },
        {
            "name": "partitioners.py",
            "path": "torch/_functorch/partitioners.py",
            "patches": [
                {
                    "old_start": 204,
                    "old_length": 7,
                    "new_start": 204,
                    "new_length": 7,
                    "hunk": "@@ -204,7 +204,7 @@ def _extract_graph_with_inputs_outputs(\n             output_values.append(env[x])\n         else:\n             output_values.append(x)\n-    new_graph.output(output_values)\n+    new_graph.output(tuple(output_values))\n \n     new_graph.eliminate_dead_code()\n     new_graph.lint()\n"
                },
                {
                    "old_start": 727,
                    "old_length": 7,
                    "new_start": 727,
                    "new_length": 7,
                    "hunk": "@@ -727,7 +727,7 @@ def functionalize_rng_ops(\n     sym_node_start_idx = len(fw_outputs) - num_sym_nodes\n     outputs = (\n         fw_outputs[:sym_node_start_idx]\n-        + fw_rng_state_outputs\n+        + tuple(fw_rng_state_outputs)\n         + fw_outputs[sym_node_start_idx:]\n     )\n     fw_module.graph.output(outputs)"
                }
            ],
            "whole_deleted": "-    new_graph.output(output_values)\n-        + fw_rng_state_outputs\n",
            "whole_added": "+    new_graph.output(tuple(output_values))\n+        + tuple(fw_rng_state_outputs)\n",
            "whole_hunk": "@@ -204,7 +204,7 @@ def _extract_graph_with_inputs_outputs(\n             output_values.append(env[x])\n         else:\n             output_values.append(x)\n-    new_graph.output(output_values)\n+    new_graph.output(tuple(output_values))\n \n     new_graph.eliminate_dead_code()\n     new_graph.lint()\n@@ -727,7 +727,7 @@ def functionalize_rng_ops(\n     sym_node_start_idx = len(fw_outputs) - num_sym_nodes\n     outputs = (\n         fw_outputs[:sym_node_start_idx]\n-        + fw_rng_state_outputs\n+        + tuple(fw_rng_state_outputs)\n         + fw_outputs[sym_node_start_idx:]\n     )\n     fw_module.graph.output(outputs)"
        }
    ]
},
{
    "Id": 165,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/0fd1fc17c3a53cb4cede1992d431d5384b2813f3",
    "date": "2024-05-07T22:15:20+00:00",
    "message": "[MPS] Fix `abs` for complex types (#125662)\n\nBy calling `realPartOfTensor:` if input type is complex on Sonoma and fall back to `at::view_as_real` trick on Ventura.\n\nSplit `unary_op` template into `unary_op` and `unary_op_noresize`, which skips resize and empty checks\n\nMarked `abs`, `isclose` and `nn.functional.softsign` OpInfo tests as supported by complex types\n\nFixes https://github.com/pytorch/pytorch/issues/125135\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125662\nApproved by: https://github.com/kulinseth",
    "label": "YES",
    "changes": [
        {
            "name": "MPSGraphSonomaOps.h",
            "path": "aten/src/ATen/native/mps/MPSGraphSonomaOps.h",
            "patches": [
                {
                    "old_start": 25,
                    "old_length": 6,
                    "new_start": 25,
                    "new_length": 10,
                    "hunk": "@@ -25,6 +25,10 @@ typedef NS_ENUM(NSUInteger, MPSGraphFFTScalingMode)\n -(MPSGraphTensor * _Nonnull) conjugateWithTensor:(MPSGraphTensor * _Nonnull) tensor\n                                             name:(NSString * _Nullable) name;\n \n+-(MPSGraphTensor * _Nonnull) realPartOfTensor:(MPSGraphTensor * _Nonnull) tensor\n+                                         name:(NSString * _Nullable) name;\n+\n+\n -(MPSGraphTensor * _Nonnull) fastFourierTransformWithTensor:(MPSGraphTensor * _Nonnull) tensor\n                                                        axes:(NSArray<NSNumber *> * _Nonnull) axes\n                                                  descriptor:(MPSGraphFFTDescriptor * _Nonnull) descriptor\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+-(MPSGraphTensor * _Nonnull) realPartOfTensor:(MPSGraphTensor * _Nonnull) tensor\n+                                         name:(NSString * _Nullable) name;\n+\n+\n",
            "whole_hunk": "@@ -25,6 +25,10 @@ typedef NS_ENUM(NSUInteger, MPSGraphFFTScalingMode)\n -(MPSGraphTensor * _Nonnull) conjugateWithTensor:(MPSGraphTensor * _Nonnull) tensor\n                                             name:(NSString * _Nullable) name;\n \n+-(MPSGraphTensor * _Nonnull) realPartOfTensor:(MPSGraphTensor * _Nonnull) tensor\n+                                         name:(NSString * _Nullable) name;\n+\n+\n -(MPSGraphTensor * _Nonnull) fastFourierTransformWithTensor:(MPSGraphTensor * _Nonnull) tensor\n                                                        axes:(NSArray<NSNumber *> * _Nonnull) axes\n                                                  descriptor:(MPSGraphFFTDescriptor * _Nonnull) descriptor\n"
        },
        {
            "name": "UnaryOps.mm",
            "path": "aten/src/ATen/native/mps/operations/UnaryOps.mm",
            "patches": [
                {
                    "old_start": 75,
                    "old_length": 23,
                    "new_start": 75,
                    "new_length": 10,
                    "hunk": "@@ -75,23 +75,10 @@ static bool is_empty_tensor(const Tensor& self) {\n   return self.numel() == 0;\n }\n \n-static void unary_op(const Tensor& self,\n-                     const Tensor& output_,\n-                     std::string op_name,\n-                     UnaryOpBlock unaryBlock,\n-                     is_noop_p is_noop = is_empty_tensor) {\n+static void unary_op_noresize(const Tensor& self, const Tensor& output_, std::string op_name, UnaryOpBlock unaryBlock) {\n   TORCH_CHECK(!(!is_macos_13_or_newer() && self.scalar_type() == ScalarType::Byte),\n               \"MPS support unary op with uint8 natively starting from macOS 13.0\");\n \n-  if (!output_.is_same_size(self)) {\n-    output_.resize_(self.sizes());\n-  }\n-\n-  if (is_noop(self)) {\n-    output_.copy_(self);\n-    return;\n-  }\n-\n   auto output = output_;\n   bool needsCopyToOutput = false;\n   if (output.storage_offset() || !output.is_contiguous()) {\n"
                },
                {
                    "old_start": 139,
                    "old_length": 6,
                    "new_start": 126,
                    "new_length": 23,
                    "hunk": "@@ -139,6 +126,23 @@ static void unary_op(const Tensor& self,\n   }\n }\n \n+static void unary_op(const Tensor& self,\n+                     const Tensor& output_,\n+                     std::string op_name,\n+                     UnaryOpBlock unaryBlock,\n+                     is_noop_p is_noop = is_empty_tensor) {\n+  if (!output_.is_same_size(self)) {\n+    output_.resize_(self.sizes());\n+  }\n+\n+  if (is_noop(self)) {\n+    output_.copy_(self);\n+    return;\n+  }\n+\n+  unary_op_noresize(self, output_, op_name, unaryBlock);\n+}\n+\n MPSGraphTensor* trunc_tensor(MPSGraph* mpsGraph, MPSGraphTensor* inputTensor) {\n   // Rounding is a no-op for integral types, and also a reasonable workaround\n   // For MPSGraph bug on Apple Silicon, that throws `Function floorOp_i64 was not found in the library`\n"
                },
                {
                    "old_start": 168,
                    "old_length": 6,
                    "new_start": 172,
                    "new_length": 12,
                    "hunk": "@@ -168,6 +172,12 @@ MPSGraphTensor* log1p(MPSGraph* mpsGraph, MPSGraphTensor* inputTensor) {\n   return [mpsGraph logarithmWithTensor:addedTensor name:nil];\n }\n \n+static MPSGraphTensor* lengthOfComplexAsReal(MPSGraph* mpsGraph, MPSGraphTensor* inputTensor) {\n+  auto squares = [mpsGraph squareWithTensor:inputTensor name:nil];\n+  auto sumSquares = [mpsGraph reductionSumWithTensor:squares axis:-1 name:nil];\n+  return [mpsGraph squareRootWithTensor:sumSquares name:nil];\n+}\n+\n } // namespace mps\n \n TORCH_IMPL_FUNC(trunc_out_mps)(const Tensor& self, const Tensor& output) {\n"
                },
                {
                    "old_start": 226,
                    "old_length": 14,
                    "new_start": 236,
                    "new_length": 6,
                    "hunk": "@@ -226,14 +236,6 @@ CREATE_MPS_STRUCTURED_UNARY_ROUNDING_TORCH_IMPL_FUNC(round_out_mps, round)\n     });                                                                                                          \\\n   }\n \n-#define CREATE_MPS_UNARY_TORCH_IMPL_FUNC(func_out, func_stub)                                                    \\\n-  Tensor& func_out(const Tensor& self, Tensor& output) {                                                         \\\n-    mps::unary_op(self, output, #func_out, ^MPSGraphTensor*(MPSGraph * mpsGraph, MPSGraphTensor * inputTensor) { \\\n-      return [mpsGraph func_stub##WithTensor:inputTensor name:nil];                                              \\\n-    });                                                                                                          \\\n-    return output;                                                                                               \\\n-  }\n-\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(exp_out_mps, exponent)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(exp2_out_mps, exponentBase2)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(reciprocal_out_mps, reciprocal)\n"
                },
                {
                    "old_start": 257,
                    "old_length": 7,
                    "new_start": 259,
                    "new_length": 35,
                    "hunk": "@@ -257,7 +259,35 @@ CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(asinh_out_mps, asinh)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(acosh_out_mps, acosh)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(atanh_out_mps, atanh)\n \n-CREATE_MPS_UNARY_TORCH_IMPL_FUNC(abs_out_mps, absolute)\n+Tensor& abs_out_mps(const Tensor& self, Tensor& output) {\n+  using namespace mps;\n+\n+  if (!output.is_same_size(self)) {\n+    output.resize_(self.sizes());\n+  }\n+\n+  if (self.numel() == 0) {\n+    return output;\n+  }\n+\n+  if (supportsComplex() || !self.is_complex()) {\n+    unary_op_noresize(self, output, \"abs_out_mps\", ^MPSGraphTensor*(MPSGraph* mpsGraph, MPSGraphTensor* inputTensor) {\n+      auto rc = [mpsGraph absoluteWithTensor:inputTensor name:nil];\n+      if (self.is_complex()) {\n+        rc = [mpsGraph realPartOfTensor:rc name:nil];\n+      }\n+      return rc;\n+    });\n+  } else {\n+    Tensor realInput = at::view_as_real(self);\n+    unary_op_noresize(\n+        realInput, output, \"abs_out_mps\", ^MPSGraphTensor*(MPSGraph* mpsGraph, MPSGraphTensor* inputTensor) {\n+          auto rc = lengthOfComplexAsReal(mpsGraph, inputTensor);\n+          return [mpsGraph reshapeTensor:rc withShape:getMPSShape(output) name:nil];\n+        });\n+  }\n+  return output;\n+}\n \n Tensor& logical_not_out_mps(const Tensor& self, Tensor& output) {\n   auto bool_self = self.to(ScalarType::Bool);\n"
                },
                {
                    "old_start": 484,
                    "old_length": 9,
                    "new_start": 514,
                    "new_length": 7,
                    "hunk": "@@ -484,9 +514,7 @@ TORCH_IMPL_FUNC(sgn_out_mps)(const Tensor& self, const Tensor& output) {\n   Tensor realOutput = at::view_as_real(output);\n \n   auto complex_sgn_op = [&](MPSGraph* mpsGraph, MPSGraphTensor* inputTensor) -> MPSGraphTensor* {\n-    MPSGraphTensor* squares = [mpsGraph squareWithTensor:inputTensor name:nil];\n-    MPSGraphTensor* sumSquares = [mpsGraph reductionSumWithTensor:squares axis:-1 name:nil];\n-    MPSGraphTensor* norm = [mpsGraph squareRootWithTensor:sumSquares name:nil];\n+    MPSGraphTensor* norm = mps::lengthOfComplexAsReal(mpsGraph, inputTensor);\n     MPSGraphTensor* zero = [mpsGraph constantWithScalar:0.0 dataType:norm.dataType];\n     MPSGraphTensor* isZero = [mpsGraph equalWithPrimaryTensor:norm secondaryTensor:zero name:nil];\n     MPSGraphTensor* sgnTensor = [mpsGraph divisionWithPrimaryTensor:inputTensor secondaryTensor:norm name:nil];\n"
                }
            ],
            "whole_deleted": "-static void unary_op(const Tensor& self,\n-                     const Tensor& output_,\n-                     std::string op_name,\n-                     UnaryOpBlock unaryBlock,\n-                     is_noop_p is_noop = is_empty_tensor) {\n-  if (!output_.is_same_size(self)) {\n-    output_.resize_(self.sizes());\n-  }\n-\n-  if (is_noop(self)) {\n-    output_.copy_(self);\n-    return;\n-  }\n-\n-#define CREATE_MPS_UNARY_TORCH_IMPL_FUNC(func_out, func_stub)                                                    \\\n-  Tensor& func_out(const Tensor& self, Tensor& output) {                                                         \\\n-    mps::unary_op(self, output, #func_out, ^MPSGraphTensor*(MPSGraph * mpsGraph, MPSGraphTensor * inputTensor) { \\\n-      return [mpsGraph func_stub##WithTensor:inputTensor name:nil];                                              \\\n-    });                                                                                                          \\\n-    return output;                                                                                               \\\n-  }\n-\n-CREATE_MPS_UNARY_TORCH_IMPL_FUNC(abs_out_mps, absolute)\n-    MPSGraphTensor* squares = [mpsGraph squareWithTensor:inputTensor name:nil];\n-    MPSGraphTensor* sumSquares = [mpsGraph reductionSumWithTensor:squares axis:-1 name:nil];\n-    MPSGraphTensor* norm = [mpsGraph squareRootWithTensor:sumSquares name:nil];\n",
            "whole_added": "+static void unary_op_noresize(const Tensor& self, const Tensor& output_, std::string op_name, UnaryOpBlock unaryBlock) {\n+static void unary_op(const Tensor& self,\n+                     const Tensor& output_,\n+                     std::string op_name,\n+                     UnaryOpBlock unaryBlock,\n+                     is_noop_p is_noop = is_empty_tensor) {\n+  if (!output_.is_same_size(self)) {\n+    output_.resize_(self.sizes());\n+  }\n+\n+  if (is_noop(self)) {\n+    output_.copy_(self);\n+    return;\n+  }\n+\n+  unary_op_noresize(self, output_, op_name, unaryBlock);\n+}\n+\n+static MPSGraphTensor* lengthOfComplexAsReal(MPSGraph* mpsGraph, MPSGraphTensor* inputTensor) {\n+  auto squares = [mpsGraph squareWithTensor:inputTensor name:nil];\n+  auto sumSquares = [mpsGraph reductionSumWithTensor:squares axis:-1 name:nil];\n+  return [mpsGraph squareRootWithTensor:sumSquares name:nil];\n+}\n+\n+Tensor& abs_out_mps(const Tensor& self, Tensor& output) {\n+  using namespace mps;\n+\n+  if (!output.is_same_size(self)) {\n+    output.resize_(self.sizes());\n+  }\n+\n+  if (self.numel() == 0) {\n+    return output;\n+  }\n+\n+  if (supportsComplex() || !self.is_complex()) {\n+    unary_op_noresize(self, output, \"abs_out_mps\", ^MPSGraphTensor*(MPSGraph* mpsGraph, MPSGraphTensor* inputTensor) {\n+      auto rc = [mpsGraph absoluteWithTensor:inputTensor name:nil];\n+      if (self.is_complex()) {\n+        rc = [mpsGraph realPartOfTensor:rc name:nil];\n+      }\n+      return rc;\n+    });\n+  } else {\n+    Tensor realInput = at::view_as_real(self);\n+    unary_op_noresize(\n+        realInput, output, \"abs_out_mps\", ^MPSGraphTensor*(MPSGraph* mpsGraph, MPSGraphTensor* inputTensor) {\n+          auto rc = lengthOfComplexAsReal(mpsGraph, inputTensor);\n+          return [mpsGraph reshapeTensor:rc withShape:getMPSShape(output) name:nil];\n+        });\n+  }\n+  return output;\n+}\n+    MPSGraphTensor* norm = mps::lengthOfComplexAsReal(mpsGraph, inputTensor);\n",
            "whole_hunk": "@@ -75,23 +75,10 @@ static bool is_empty_tensor(const Tensor& self) {\n   return self.numel() == 0;\n }\n \n-static void unary_op(const Tensor& self,\n-                     const Tensor& output_,\n-                     std::string op_name,\n-                     UnaryOpBlock unaryBlock,\n-                     is_noop_p is_noop = is_empty_tensor) {\n+static void unary_op_noresize(const Tensor& self, const Tensor& output_, std::string op_name, UnaryOpBlock unaryBlock) {\n   TORCH_CHECK(!(!is_macos_13_or_newer() && self.scalar_type() == ScalarType::Byte),\n               \"MPS support unary op with uint8 natively starting from macOS 13.0\");\n \n-  if (!output_.is_same_size(self)) {\n-    output_.resize_(self.sizes());\n-  }\n-\n-  if (is_noop(self)) {\n-    output_.copy_(self);\n-    return;\n-  }\n-\n   auto output = output_;\n   bool needsCopyToOutput = false;\n   if (output.storage_offset() || !output.is_contiguous()) {\n@@ -139,6 +126,23 @@ static void unary_op(const Tensor& self,\n   }\n }\n \n+static void unary_op(const Tensor& self,\n+                     const Tensor& output_,\n+                     std::string op_name,\n+                     UnaryOpBlock unaryBlock,\n+                     is_noop_p is_noop = is_empty_tensor) {\n+  if (!output_.is_same_size(self)) {\n+    output_.resize_(self.sizes());\n+  }\n+\n+  if (is_noop(self)) {\n+    output_.copy_(self);\n+    return;\n+  }\n+\n+  unary_op_noresize(self, output_, op_name, unaryBlock);\n+}\n+\n MPSGraphTensor* trunc_tensor(MPSGraph* mpsGraph, MPSGraphTensor* inputTensor) {\n   // Rounding is a no-op for integral types, and also a reasonable workaround\n   // For MPSGraph bug on Apple Silicon, that throws `Function floorOp_i64 was not found in the library`\n@@ -168,6 +172,12 @@ MPSGraphTensor* log1p(MPSGraph* mpsGraph, MPSGraphTensor* inputTensor) {\n   return [mpsGraph logarithmWithTensor:addedTensor name:nil];\n }\n \n+static MPSGraphTensor* lengthOfComplexAsReal(MPSGraph* mpsGraph, MPSGraphTensor* inputTensor) {\n+  auto squares = [mpsGraph squareWithTensor:inputTensor name:nil];\n+  auto sumSquares = [mpsGraph reductionSumWithTensor:squares axis:-1 name:nil];\n+  return [mpsGraph squareRootWithTensor:sumSquares name:nil];\n+}\n+\n } // namespace mps\n \n TORCH_IMPL_FUNC(trunc_out_mps)(const Tensor& self, const Tensor& output) {\n@@ -226,14 +236,6 @@ CREATE_MPS_STRUCTURED_UNARY_ROUNDING_TORCH_IMPL_FUNC(round_out_mps, round)\n     });                                                                                                          \\\n   }\n \n-#define CREATE_MPS_UNARY_TORCH_IMPL_FUNC(func_out, func_stub)                                                    \\\n-  Tensor& func_out(const Tensor& self, Tensor& output) {                                                         \\\n-    mps::unary_op(self, output, #func_out, ^MPSGraphTensor*(MPSGraph * mpsGraph, MPSGraphTensor * inputTensor) { \\\n-      return [mpsGraph func_stub##WithTensor:inputTensor name:nil];                                              \\\n-    });                                                                                                          \\\n-    return output;                                                                                               \\\n-  }\n-\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(exp_out_mps, exponent)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(exp2_out_mps, exponentBase2)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(reciprocal_out_mps, reciprocal)\n@@ -257,7 +259,35 @@ CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(asinh_out_mps, asinh)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(acosh_out_mps, acosh)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(atanh_out_mps, atanh)\n \n-CREATE_MPS_UNARY_TORCH_IMPL_FUNC(abs_out_mps, absolute)\n+Tensor& abs_out_mps(const Tensor& self, Tensor& output) {\n+  using namespace mps;\n+\n+  if (!output.is_same_size(self)) {\n+    output.resize_(self.sizes());\n+  }\n+\n+  if (self.numel() == 0) {\n+    return output;\n+  }\n+\n+  if (supportsComplex() || !self.is_complex()) {\n+    unary_op_noresize(self, output, \"abs_out_mps\", ^MPSGraphTensor*(MPSGraph* mpsGraph, MPSGraphTensor* inputTensor) {\n+      auto rc = [mpsGraph absoluteWithTensor:inputTensor name:nil];\n+      if (self.is_complex()) {\n+        rc = [mpsGraph realPartOfTensor:rc name:nil];\n+      }\n+      return rc;\n+    });\n+  } else {\n+    Tensor realInput = at::view_as_real(self);\n+    unary_op_noresize(\n+        realInput, output, \"abs_out_mps\", ^MPSGraphTensor*(MPSGraph* mpsGraph, MPSGraphTensor* inputTensor) {\n+          auto rc = lengthOfComplexAsReal(mpsGraph, inputTensor);\n+          return [mpsGraph reshapeTensor:rc withShape:getMPSShape(output) name:nil];\n+        });\n+  }\n+  return output;\n+}\n \n Tensor& logical_not_out_mps(const Tensor& self, Tensor& output) {\n   auto bool_self = self.to(ScalarType::Bool);\n@@ -484,9 +514,7 @@ TORCH_IMPL_FUNC(sgn_out_mps)(const Tensor& self, const Tensor& output) {\n   Tensor realOutput = at::view_as_real(output);\n \n   auto complex_sgn_op = [&](MPSGraph* mpsGraph, MPSGraphTensor* inputTensor) -> MPSGraphTensor* {\n-    MPSGraphTensor* squares = [mpsGraph squareWithTensor:inputTensor name:nil];\n-    MPSGraphTensor* sumSquares = [mpsGraph reductionSumWithTensor:squares axis:-1 name:nil];\n-    MPSGraphTensor* norm = [mpsGraph squareRootWithTensor:sumSquares name:nil];\n+    MPSGraphTensor* norm = mps::lengthOfComplexAsReal(mpsGraph, inputTensor);\n     MPSGraphTensor* zero = [mpsGraph constantWithScalar:0.0 dataType:norm.dataType];\n     MPSGraphTensor* isZero = [mpsGraph equalWithPrimaryTensor:norm secondaryTensor:zero name:nil];\n     MPSGraphTensor* sgnTensor = [mpsGraph divisionWithPrimaryTensor:inputTensor secondaryTensor:norm name:nil];\n"
        },
        {
            "name": "test_mps.py",
            "path": "test/test_mps.py",
            "patches": [
                {
                    "old_start": 227,
                    "old_length": 6,
                    "new_start": 227,
                    "new_length": 7,
                    "hunk": "@@ -227,6 +227,7 @@ def mps_ops_modifier(ops):\n         '__radd__',\n         '__rmul__',\n         '__getitem__',\n+        'abs',\n         'add',\n         'argwhere',\n         'atleast_1d',\n"
                },
                {
                    "old_start": 286,
                    "old_length": 8,
                    "new_start": 287,
                    "new_length": 8,
                    "hunk": "@@ -286,8 +287,8 @@ def mps_ops_modifier(ops):\n         'narrow_copy',\n         'nn.functional.conv1d',\n         'nn.functional.conv_transpose1d',\n-        'nn.functional.padcircular',\n         'nn.functional.feature_alpha_dropoutwithout_train',\n+        'nn.functional.padcircular',\n         'nn.functional.unfold',\n         'nonzero',\n         'ones',\n"
                },
                {
                    "old_start": 383,
                    "old_length": 6,
                    "new_start": 384,
                    "new_length": 7,
                    "hunk": "@@ -383,6 +384,7 @@ def mps_ops_modifier(ops):\n         'half',\n         'hstack',\n         'int',\n+        'isclose',\n         'isnan',\n         'ldexp',\n         'log10',\n"
                },
                {
                    "old_start": 403,
                    "old_length": 12,
                    "new_start": 405,
                    "new_length": 13,
                    "hunk": "@@ -403,12 +405,13 @@ def mps_ops_modifier(ops):\n         'mean',\n         'ne',\n         'neg',\n-        'nn.functional.rms_norm',\n         'nn.functional.padconstant',\n         'nn.functional.padreflect',\n         'nn.functional.padreplicate',\n         'nn.functional.pixel_shuffle',\n         'nn.functional.pixel_unshuffle',\n+        'nn.functional.rms_norm',\n+        'nn.functional.softsign',\n         'nn.functional.tanhshrink',\n         'prod',\n         'reciprocal',"
                }
            ],
            "whole_deleted": "-        'nn.functional.padcircular',\n-        'nn.functional.rms_norm',\n",
            "whole_added": "+        'abs',\n+        'nn.functional.padcircular',\n+        'isclose',\n+        'nn.functional.rms_norm',\n+        'nn.functional.softsign',\n",
            "whole_hunk": "@@ -227,6 +227,7 @@ def mps_ops_modifier(ops):\n         '__radd__',\n         '__rmul__',\n         '__getitem__',\n+        'abs',\n         'add',\n         'argwhere',\n         'atleast_1d',\n@@ -286,8 +287,8 @@ def mps_ops_modifier(ops):\n         'narrow_copy',\n         'nn.functional.conv1d',\n         'nn.functional.conv_transpose1d',\n-        'nn.functional.padcircular',\n         'nn.functional.feature_alpha_dropoutwithout_train',\n+        'nn.functional.padcircular',\n         'nn.functional.unfold',\n         'nonzero',\n         'ones',\n@@ -383,6 +384,7 @@ def mps_ops_modifier(ops):\n         'half',\n         'hstack',\n         'int',\n+        'isclose',\n         'isnan',\n         'ldexp',\n         'log10',\n@@ -403,12 +405,13 @@ def mps_ops_modifier(ops):\n         'mean',\n         'ne',\n         'neg',\n-        'nn.functional.rms_norm',\n         'nn.functional.padconstant',\n         'nn.functional.padreflect',\n         'nn.functional.padreplicate',\n         'nn.functional.pixel_shuffle',\n         'nn.functional.pixel_unshuffle',\n+        'nn.functional.rms_norm',\n+        'nn.functional.softsign',\n         'nn.functional.tanhshrink',\n         'prod',\n         'reciprocal',"
        }
    ]
},
{
    "Id": 415,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/7731c97e065cffe4efe82698980a2c8297816dbd",
    "date": "2023-11-17T09:29:45+00:00",
    "message": "Revert \"Fix checking symbolic shapes inside torch._check (#113811)\"\n\nThis reverts commit 7f224f6714419f3d56e64a66079340b0e914a2ca.\n\nReverted https://github.com/pytorch/pytorch/pull/113811 on behalf of https://github.com/jeanschmidt due to Breaking inductor tests on main ([comment](https://github.com/pytorch/pytorch/pull/113811#issuecomment-1816024288))",
    "label": "YES",
    "changes": [
        {
            "name": "test_misc.py",
            "path": "test/dynamo/test_misc.py",
            "patches": [
                {
                    "old_start": 822,
                    "old_length": 25,
                    "new_start": 822,
                    "new_length": 6,
                    "hunk": "@@ -822,25 +822,6 @@ utils_device.CURRENT_DEVICE == None\"\"\",\n         f(torch.tensor([4]))\n         self.assertEqual(cnts.frame_count, 1)\n \n-    @torch._dynamo.config.patch(capture_scalar_outputs=True)\n-    def test_torch_check_symbolic_shape_rel(self):\n-        cnts = torch._dynamo.testing.CompileCounter()\n-\n-        @torch.compile(backend=cnts, fullgraph=True)\n-        def f(x):\n-            y = x.item()\n-            torch._check(x.shape[0] == 1)\n-            torch._check(x.shape[0] != 2)\n-            torch._check(x.shape[0] >= 0)\n-            torch._check(x.shape[0] > 0)\n-            torch._check(x.shape[0] < 4)\n-            torch._check(x.shape[0] <= 3)\n-            return torch.arange(0, y)\n-\n-        f(torch.tensor([3]))\n-        f(torch.tensor([4]))\n-        self.assertEqual(cnts.frame_count, 1)\n-\n     @torch._dynamo.config.patch(capture_scalar_outputs=True)\n     def test_torch_check_is_size(self):\n         cnts = torch._dynamo.testing.CompileCounter()\n"
                }
            ],
            "whole_deleted": "-    @torch._dynamo.config.patch(capture_scalar_outputs=True)\n-    def test_torch_check_symbolic_shape_rel(self):\n-        cnts = torch._dynamo.testing.CompileCounter()\n-\n-        @torch.compile(backend=cnts, fullgraph=True)\n-        def f(x):\n-            y = x.item()\n-            torch._check(x.shape[0] == 1)\n-            torch._check(x.shape[0] != 2)\n-            torch._check(x.shape[0] >= 0)\n-            torch._check(x.shape[0] > 0)\n-            torch._check(x.shape[0] < 4)\n-            torch._check(x.shape[0] <= 3)\n-            return torch.arange(0, y)\n-\n-        f(torch.tensor([3]))\n-        f(torch.tensor([4]))\n-        self.assertEqual(cnts.frame_count, 1)\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -822,25 +822,6 @@ utils_device.CURRENT_DEVICE == None\"\"\",\n         f(torch.tensor([4]))\n         self.assertEqual(cnts.frame_count, 1)\n \n-    @torch._dynamo.config.patch(capture_scalar_outputs=True)\n-    def test_torch_check_symbolic_shape_rel(self):\n-        cnts = torch._dynamo.testing.CompileCounter()\n-\n-        @torch.compile(backend=cnts, fullgraph=True)\n-        def f(x):\n-            y = x.item()\n-            torch._check(x.shape[0] == 1)\n-            torch._check(x.shape[0] != 2)\n-            torch._check(x.shape[0] >= 0)\n-            torch._check(x.shape[0] > 0)\n-            torch._check(x.shape[0] < 4)\n-            torch._check(x.shape[0] <= 3)\n-            return torch.arange(0, y)\n-\n-        f(torch.tensor([3]))\n-        f(torch.tensor([4]))\n-        self.assertEqual(cnts.frame_count, 1)\n-\n     @torch._dynamo.config.patch(capture_scalar_outputs=True)\n     def test_torch_check_is_size(self):\n         cnts = torch._dynamo.testing.CompileCounter()\n"
        },
        {
            "name": "common.py",
            "path": "torch/_inductor/codegen/common.py",
            "patches": [
                {
                    "old_start": 311,
                    "old_length": 8,
                    "new_start": 311,
                    "new_length": 8,
                    "hunk": "@@ -311,8 +311,8 @@ class ExprPrinter(Printer):\n         else:  # exp == 0\n             return \"1\"\n \n-    def _print_Relational(self, expr):\n-        return f\" {expr.rel_op} \".join(map(self.paren, map(self._print, expr.args)))\n+    def _print_Unequality(self, expr):\n+        return \" != \".join(map(self.paren, map(self._print, expr.args)))\n \n     def _print_Mul(self, expr):\n         return \"*\".join(map(self.paren, map(self._print, expr.args)))\n"
                }
            ],
            "whole_deleted": "-    def _print_Relational(self, expr):\n-        return f\" {expr.rel_op} \".join(map(self.paren, map(self._print, expr.args)))\n",
            "whole_added": "+    def _print_Unequality(self, expr):\n+        return \" != \".join(map(self.paren, map(self._print, expr.args)))\n",
            "whole_hunk": "@@ -311,8 +311,8 @@ class ExprPrinter(Printer):\n         else:  # exp == 0\n             return \"1\"\n \n-    def _print_Relational(self, expr):\n-        return f\" {expr.rel_op} \".join(map(self.paren, map(self._print, expr.args)))\n+    def _print_Unequality(self, expr):\n+        return \" != \".join(map(self.paren, map(self._print, expr.args)))\n \n     def _print_Mul(self, expr):\n         return \"*\".join(map(self.paren, map(self._print, expr.args)))\n"
        },
        {
            "name": "symbolic_shapes.py",
            "path": "torch/fx/experimental/symbolic_shapes.py",
            "patches": [
                {
                    "old_start": 161,
                    "old_length": 13,
                    "new_start": 161,
                    "new_length": 13,
                    "hunk": "@@ -161,13 +161,13 @@ def is_concrete_bool(a: Union[bool, SymBool]):\n def tensor_has_hints(t):\n     return all(has_hint(s) for s in t.size())\n \n-def _iterate_exprs(val: Union[SymInt, torch.Tensor]) -> Iterable[sympy.Basic]:\n+def _iterate_exprs(val: Union[SymInt, torch.Tensor]) -> Iterable[sympy.Expr]:\n     if isinstance(val, SymTypes):\n         # This allow applies to the jagged layout NestedTensor case as\n         # singleton ints are not symbolic\n         if is_symbolic(val):\n             yield val.node.expr\n-    elif isinstance(val, sympy.Basic):\n+    elif isinstance(val, sympy.Expr):\n         yield val\n     elif isinstance(val, (int, float, bool)):\n         pass"
                }
            ],
            "whole_deleted": "-def _iterate_exprs(val: Union[SymInt, torch.Tensor]) -> Iterable[sympy.Basic]:\n-    elif isinstance(val, sympy.Basic):\n",
            "whole_added": "+def _iterate_exprs(val: Union[SymInt, torch.Tensor]) -> Iterable[sympy.Expr]:\n+    elif isinstance(val, sympy.Expr):\n",
            "whole_hunk": "@@ -161,13 +161,13 @@ def is_concrete_bool(a: Union[bool, SymBool]):\n def tensor_has_hints(t):\n     return all(has_hint(s) for s in t.size())\n \n-def _iterate_exprs(val: Union[SymInt, torch.Tensor]) -> Iterable[sympy.Basic]:\n+def _iterate_exprs(val: Union[SymInt, torch.Tensor]) -> Iterable[sympy.Expr]:\n     if isinstance(val, SymTypes):\n         # This allow applies to the jagged layout NestedTensor case as\n         # singleton ints are not symbolic\n         if is_symbolic(val):\n             yield val.node.expr\n-    elif isinstance(val, sympy.Basic):\n+    elif isinstance(val, sympy.Expr):\n         yield val\n     elif isinstance(val, (int, float, bool)):\n         pass"
        }
    ]
},
{
    "Id": 494,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/2ead6c2f6eaa76eb897d8dd87061bbbdf0824314",
    "date": "2023-09-30T09:12:56+00:00",
    "message": "Skip launching kernels with zero grid in AOT Inductor (#110312)\n\nSummary: with the grid computed in terms of unbacked `SymInt`s, it can happen that the grid is zero size. This causes CUDA error on `cuLaunchKernel` in the AOT Inductor codegen.\n\nIn this PR, when the grid contains unbacked `SymInt`s, a check is added around the `launchKernel` in the AOT Inductor's C++ wrapper codegen to make sure that the grid is not zero-size.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110312\nApproved by: https://github.com/chenyang78",
    "label": "YES",
    "changes": [
        {
            "name": "test_aot_inductor.py",
            "path": "test/inductor/test_aot_inductor.py",
            "patches": [
                {
                    "old_start": 596,
                    "old_length": 6,
                    "new_start": 596,
                    "new_length": 27,
                    "hunk": "@@ -596,6 +596,27 @@ class AOTInductorTestsTemplate:\n         )\n         self.check_model(Model(), example_inputs)\n \n+    def test_zero_grid_with_unbacked_symbols(self):\n+        class Repro(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+\n+            def forward(self, x, y):\n+                nz = torch.nonzero(x)\n+                b = torch.ones_like(nz, dtype=torch.float16)\n+                c = torch.zeros_like(nz, dtype=torch.float16)\n+                d = (b + c) @ y\n+                return d.sum()\n+\n+        example_inputs = (\n+            torch.tensor([1, 1, 1], device=\"cuda\"),\n+            torch.randn((1, 32), dtype=torch.float16, device=\"cuda\"),\n+        )\n+        with torch._dynamo.config.patch(\n+            {\"add_runtime_assertions_for_inline_constraints\": False}\n+        ):\n+            self.check_model(Repro(), example_inputs)\n+\n \n class AOTInductorTestABICompatibleCpu(TestCase):\n     device = \"cpu\"\n"
                },
                {
                    "old_start": 621,
                    "old_length": 6,
                    "new_start": 642,
                    "new_length": 9,
                    "hunk": "@@ -621,6 +642,9 @@ copy_tests(\n         \"test_sdpa\": TestFailure((\"abi_compatible_cpu\",)),\n         \"test_sdpa_2\": TestFailure((\"abi_compatible_cpu\",)),\n         \"test_simple_dynamic\": TestFailure((\"abi_compatible_cpu\",)),\n+        \"test_zero_grid_with_unbacked_symbols\": TestFailure(\n+            (\"abi_compatible_cpu\",), is_skip=True\n+        ),\n     },\n )\n \n"
                },
                {
                    "old_start": 633,
                    "old_length": 7,
                    "new_start": 657,
                    "new_length": 15,
                    "hunk": "@@ -633,7 +657,15 @@ class AOTInductorTestABICompatibleCuda(TestCase):\n \n \n copy_tests(\n-    AOTInductorTestsTemplate, AOTInductorTestABICompatibleCuda, \"abi_compatible_cuda\"\n+    AOTInductorTestsTemplate,\n+    AOTInductorTestABICompatibleCuda,\n+    \"abi_compatible_cuda\",\n+    # test_failures, xfail by default, set is_skip=True to skip\n+    {\n+        \"test_zero_grid_with_unbacked_symbols\": TestFailure(\n+            (\"abi_compatible_cuda\",), is_skip=True\n+        ),\n+    },\n )\n \n \n"
                }
            ],
            "whole_deleted": "-    AOTInductorTestsTemplate, AOTInductorTestABICompatibleCuda, \"abi_compatible_cuda\"\n",
            "whole_added": "+    def test_zero_grid_with_unbacked_symbols(self):\n+        class Repro(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+\n+            def forward(self, x, y):\n+                nz = torch.nonzero(x)\n+                b = torch.ones_like(nz, dtype=torch.float16)\n+                c = torch.zeros_like(nz, dtype=torch.float16)\n+                d = (b + c) @ y\n+                return d.sum()\n+\n+        example_inputs = (\n+            torch.tensor([1, 1, 1], device=\"cuda\"),\n+            torch.randn((1, 32), dtype=torch.float16, device=\"cuda\"),\n+        )\n+        with torch._dynamo.config.patch(\n+            {\"add_runtime_assertions_for_inline_constraints\": False}\n+        ):\n+            self.check_model(Repro(), example_inputs)\n+\n+        \"test_zero_grid_with_unbacked_symbols\": TestFailure(\n+            (\"abi_compatible_cpu\",), is_skip=True\n+        ),\n+    AOTInductorTestsTemplate,\n+    AOTInductorTestABICompatibleCuda,\n+    \"abi_compatible_cuda\",\n+    # test_failures, xfail by default, set is_skip=True to skip\n+    {\n+        \"test_zero_grid_with_unbacked_symbols\": TestFailure(\n+            (\"abi_compatible_cuda\",), is_skip=True\n+        ),\n+    },\n",
            "whole_hunk": "@@ -596,6 +596,27 @@ class AOTInductorTestsTemplate:\n         )\n         self.check_model(Model(), example_inputs)\n \n+    def test_zero_grid_with_unbacked_symbols(self):\n+        class Repro(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+\n+            def forward(self, x, y):\n+                nz = torch.nonzero(x)\n+                b = torch.ones_like(nz, dtype=torch.float16)\n+                c = torch.zeros_like(nz, dtype=torch.float16)\n+                d = (b + c) @ y\n+                return d.sum()\n+\n+        example_inputs = (\n+            torch.tensor([1, 1, 1], device=\"cuda\"),\n+            torch.randn((1, 32), dtype=torch.float16, device=\"cuda\"),\n+        )\n+        with torch._dynamo.config.patch(\n+            {\"add_runtime_assertions_for_inline_constraints\": False}\n+        ):\n+            self.check_model(Repro(), example_inputs)\n+\n \n class AOTInductorTestABICompatibleCpu(TestCase):\n     device = \"cpu\"\n@@ -621,6 +642,9 @@ copy_tests(\n         \"test_sdpa\": TestFailure((\"abi_compatible_cpu\",)),\n         \"test_sdpa_2\": TestFailure((\"abi_compatible_cpu\",)),\n         \"test_simple_dynamic\": TestFailure((\"abi_compatible_cpu\",)),\n+        \"test_zero_grid_with_unbacked_symbols\": TestFailure(\n+            (\"abi_compatible_cpu\",), is_skip=True\n+        ),\n     },\n )\n \n@@ -633,7 +657,15 @@ class AOTInductorTestABICompatibleCuda(TestCase):\n \n \n copy_tests(\n-    AOTInductorTestsTemplate, AOTInductorTestABICompatibleCuda, \"abi_compatible_cuda\"\n+    AOTInductorTestsTemplate,\n+    AOTInductorTestABICompatibleCuda,\n+    \"abi_compatible_cuda\",\n+    # test_failures, xfail by default, set is_skip=True to skip\n+    {\n+        \"test_zero_grid_with_unbacked_symbols\": TestFailure(\n+            (\"abi_compatible_cuda\",), is_skip=True\n+        ),\n+    },\n )\n \n \n"
        },
        {
            "name": "config.py",
            "path": "torch/_dynamo/config.py",
            "patches": [
                {
                    "old_start": 343,
                    "old_length": 6,
                    "new_start": 343,
                    "new_length": 8,
                    "hunk": "@@ -343,6 +343,8 @@ inject_BUILD_SET_unimplemented_TESTING_ONLY = False\n # lists, and incorrectly issue guards.\n inject_EVALUATE_EXPR_flip_equality_TESTING_ONLY = False\n \n+add_runtime_assertions_for_inline_constraints = True\n+\n _autograd_backward_strict_mode_banned_ops = [\n     \"stride\",\n     \"requires_grad\",\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+add_runtime_assertions_for_inline_constraints = True\n+\n",
            "whole_hunk": "@@ -343,6 +343,8 @@ inject_BUILD_SET_unimplemented_TESTING_ONLY = False\n # lists, and incorrectly issue guards.\n inject_EVALUATE_EXPR_flip_equality_TESTING_ONLY = False\n \n+add_runtime_assertions_for_inline_constraints = True\n+\n _autograd_backward_strict_mode_banned_ops = [\n     \"stride\",\n     \"requires_grad\",\n"
        },
        {
            "name": "__init__.py",
            "path": "torch/_export/__init__.py",
            "patches": [
                {
                    "old_start": 730,
                    "old_length": 10,
                    "new_start": 730,
                    "new_length": 11,
                    "hunk": "@@ -730,10 +730,11 @@ def _export(\n         (args, kwargs),\n     )\n \n-    if len(range_constraints) > 0 or len(equality_constraints) > 0:\n-        exported_program = exported_program._transform(\n-            _AddRuntimeAssertionsForInlineConstraintsPass(range_constraints, equality_constraints)\n-        )\n+    if torch._dynamo.config.add_runtime_assertions_for_inline_constraints:\n+        if len(range_constraints) > 0 or len(equality_constraints) > 0:\n+            exported_program = exported_program._transform(\n+                _AddRuntimeAssertionsForInlineConstraintsPass(range_constraints, equality_constraints)\n+            )\n     exported_program = lift_constant_tensor_pass(exported_program)\n \n     return exported_program._transform(_ReplaceSymSizeOpPass())\n"
                }
            ],
            "whole_deleted": "-    if len(range_constraints) > 0 or len(equality_constraints) > 0:\n-        exported_program = exported_program._transform(\n-            _AddRuntimeAssertionsForInlineConstraintsPass(range_constraints, equality_constraints)\n-        )\n",
            "whole_added": "+    if torch._dynamo.config.add_runtime_assertions_for_inline_constraints:\n+        if len(range_constraints) > 0 or len(equality_constraints) > 0:\n+            exported_program = exported_program._transform(\n+                _AddRuntimeAssertionsForInlineConstraintsPass(range_constraints, equality_constraints)\n+            )\n",
            "whole_hunk": "@@ -730,10 +730,11 @@ def _export(\n         (args, kwargs),\n     )\n \n-    if len(range_constraints) > 0 or len(equality_constraints) > 0:\n-        exported_program = exported_program._transform(\n-            _AddRuntimeAssertionsForInlineConstraintsPass(range_constraints, equality_constraints)\n-        )\n+    if torch._dynamo.config.add_runtime_assertions_for_inline_constraints:\n+        if len(range_constraints) > 0 or len(equality_constraints) > 0:\n+            exported_program = exported_program._transform(\n+                _AddRuntimeAssertionsForInlineConstraintsPass(range_constraints, equality_constraints)\n+            )\n     exported_program = lift_constant_tensor_pass(exported_program)\n \n     return exported_program._transform(_ReplaceSymSizeOpPass())\n"
        },
        {
            "name": "wrapper.py",
            "path": "torch/_inductor/codegen/wrapper.py",
            "patches": [
                {
                    "old_start": 12,
                    "old_length": 7,
                    "new_start": 12,
                    "new_length": 7,
                    "hunk": "@@ -12,7 +12,7 @@ from sympy import Expr\n \n import torch\n from torch._dynamo.utils import counters, dynamo_timed\n-from torch.fx.experimental.symbolic_shapes import SymTypes\n+from torch.fx.experimental.symbolic_shapes import free_unbacked_symbols, SymTypes\n from torch.fx.node import _get_qualified_name\n \n from .. import codecache, config, ir\n"
                },
                {
                    "old_start": 1950,
                    "old_length": 6,
                    "new_start": 1950,
                    "new_length": 10,
                    "hunk": "@@ -1950,6 +1950,10 @@ class CudaWrapperCodeGen(CppWrapperCodeGen):\n                 uint32_t grid_x;\n                 uint32_t grid_y;\n                 uint32_t grid_z;\n+\n+                bool is_non_zero() {\n+                    return grid_x > 0 && grid_y > 0 && grid_z > 0;\n+                }\n             };\n \n             }  // anonymous namespace\n"
                },
                {
                    "old_start": 2123,
                    "old_length": 12,
                    "new_start": 2127,
                    "new_length": 14,
                    "hunk": "@@ -2123,12 +2127,14 @@ class CudaWrapperCodeGen(CppWrapperCodeGen):\n             grid, (list, tuple)\n         ), f\"expected grid to be a list or tuple but got: {grid=}\"\n \n-        grid_args = [\n-            self.grid_expr_printer(V.graph.sizevars.simplify(item)) for item in grid\n-        ]\n+        grid = [V.graph.sizevars.simplify(item) for item in grid]\n+        grid_has_unbacked_symbols = any(free_unbacked_symbols(item) for item in grid)\n+        grid_args = [self.grid_expr_printer(item) for item in grid]\n         grid_args_str = \", \".join(grid_args)\n         self.writeline(f\"Grid {grid_name} = Grid({grid_args_str});\")\n \n+        if grid_has_unbacked_symbols:\n+            self.writeline(f\"if ({grid_name}.is_non_zero()) {{\")\n         self.writeline(\n             \"launchKernel({}, {}, {}, {}, {}, {}, {}, {});\".format(\n                 name,\n"
                },
                {
                    "old_start": 2141,
                    "old_length": 3,
                    "new_start": 2147,
                    "new_length": 5,
                    "hunk": "@@ -2141,3 +2147,5 @@ class CudaWrapperCodeGen(CppWrapperCodeGen):\n                 stream,\n             )\n         )\n+        if grid_has_unbacked_symbols:\n+            self.writeline(\"}\")\n"
                }
            ],
            "whole_deleted": "-from torch.fx.experimental.symbolic_shapes import SymTypes\n-        grid_args = [\n-            self.grid_expr_printer(V.graph.sizevars.simplify(item)) for item in grid\n-        ]\n",
            "whole_added": "+from torch.fx.experimental.symbolic_shapes import free_unbacked_symbols, SymTypes\n+\n+                bool is_non_zero() {\n+                    return grid_x > 0 && grid_y > 0 && grid_z > 0;\n+                }\n+        grid = [V.graph.sizevars.simplify(item) for item in grid]\n+        grid_has_unbacked_symbols = any(free_unbacked_symbols(item) for item in grid)\n+        grid_args = [self.grid_expr_printer(item) for item in grid]\n+        if grid_has_unbacked_symbols:\n+            self.writeline(f\"if ({grid_name}.is_non_zero()) {{\")\n+        if grid_has_unbacked_symbols:\n+            self.writeline(\"}\")\n",
            "whole_hunk": "@@ -12,7 +12,7 @@ from sympy import Expr\n \n import torch\n from torch._dynamo.utils import counters, dynamo_timed\n-from torch.fx.experimental.symbolic_shapes import SymTypes\n+from torch.fx.experimental.symbolic_shapes import free_unbacked_symbols, SymTypes\n from torch.fx.node import _get_qualified_name\n \n from .. import codecache, config, ir\n@@ -1950,6 +1950,10 @@ class CudaWrapperCodeGen(CppWrapperCodeGen):\n                 uint32_t grid_x;\n                 uint32_t grid_y;\n                 uint32_t grid_z;\n+\n+                bool is_non_zero() {\n+                    return grid_x > 0 && grid_y > 0 && grid_z > 0;\n+                }\n             };\n \n             }  // anonymous namespace\n@@ -2123,12 +2127,14 @@ class CudaWrapperCodeGen(CppWrapperCodeGen):\n             grid, (list, tuple)\n         ), f\"expected grid to be a list or tuple but got: {grid=}\"\n \n-        grid_args = [\n-            self.grid_expr_printer(V.graph.sizevars.simplify(item)) for item in grid\n-        ]\n+        grid = [V.graph.sizevars.simplify(item) for item in grid]\n+        grid_has_unbacked_symbols = any(free_unbacked_symbols(item) for item in grid)\n+        grid_args = [self.grid_expr_printer(item) for item in grid]\n         grid_args_str = \", \".join(grid_args)\n         self.writeline(f\"Grid {grid_name} = Grid({grid_args_str});\")\n \n+        if grid_has_unbacked_symbols:\n+            self.writeline(f\"if ({grid_name}.is_non_zero()) {{\")\n         self.writeline(\n             \"launchKernel({}, {}, {}, {}, {}, {}, {}, {});\".format(\n                 name,\n@@ -2141,3 +2147,5 @@ class CudaWrapperCodeGen(CppWrapperCodeGen):\n                 stream,\n             )\n         )\n+        if grid_has_unbacked_symbols:\n+            self.writeline(\"}\")\n"
        },
        {
            "name": "ir.py",
            "path": "torch/_inductor/ir.py",
            "patches": [
                {
                    "old_start": 4081,
                    "old_length": 14,
                    "new_start": 4081,
                    "new_length": 20,
                    "hunk": "@@ -4081,14 +4081,20 @@ class MultiOutput(ExternKernel):\n         symbols_to_define = self.get_unbacked_symbol_defs()\n         for i, s in enumerate(self.get_size()):\n             if s in symbols_to_define:\n-                wrapper.writeline(f\"{s} = {self.get_name()}.size({i})\")\n+                wrapper.writeline(\n+                    f\"{wrapper.declare}{s} = {self.get_name()}.size({i}){wrapper.ending}\"\n+                )\n                 symbols_to_define.remove(s)\n         for i, s in enumerate(self.get_stride()):\n             if s in symbols_to_define:\n-                wrapper.writeline(f\"{s} = {self.get_name()}.stride({i})\")\n+                wrapper.writeline(\n+                    f\"{wrapper.declare}{s} = {self.get_name()}.stride({i}){wrapper.ending}\"\n+                )\n                 symbols_to_define.remove(s)\n         if (s := self.get_offset()) in symbols_to_define:\n-            wrapper.writeline(f\"{s} = {self.get_name()}.storage_offset()\")\n+            wrapper.writeline(\n+                f\"{wrapper.declare}{s} = {self.get_name()}.storage_offset(){wrapper.ending}\"\n+            )\n             symbols_to_define.remove(s)\n         assert (\n             not symbols_to_define"
                }
            ],
            "whole_deleted": "-                wrapper.writeline(f\"{s} = {self.get_name()}.size({i})\")\n-                wrapper.writeline(f\"{s} = {self.get_name()}.stride({i})\")\n-            wrapper.writeline(f\"{s} = {self.get_name()}.storage_offset()\")\n",
            "whole_added": "+                wrapper.writeline(\n+                    f\"{wrapper.declare}{s} = {self.get_name()}.size({i}){wrapper.ending}\"\n+                )\n+                wrapper.writeline(\n+                    f\"{wrapper.declare}{s} = {self.get_name()}.stride({i}){wrapper.ending}\"\n+                )\n+            wrapper.writeline(\n+                f\"{wrapper.declare}{s} = {self.get_name()}.storage_offset(){wrapper.ending}\"\n+            )\n",
            "whole_hunk": "@@ -4081,14 +4081,20 @@ class MultiOutput(ExternKernel):\n         symbols_to_define = self.get_unbacked_symbol_defs()\n         for i, s in enumerate(self.get_size()):\n             if s in symbols_to_define:\n-                wrapper.writeline(f\"{s} = {self.get_name()}.size({i})\")\n+                wrapper.writeline(\n+                    f\"{wrapper.declare}{s} = {self.get_name()}.size({i}){wrapper.ending}\"\n+                )\n                 symbols_to_define.remove(s)\n         for i, s in enumerate(self.get_stride()):\n             if s in symbols_to_define:\n-                wrapper.writeline(f\"{s} = {self.get_name()}.stride({i})\")\n+                wrapper.writeline(\n+                    f\"{wrapper.declare}{s} = {self.get_name()}.stride({i}){wrapper.ending}\"\n+                )\n                 symbols_to_define.remove(s)\n         if (s := self.get_offset()) in symbols_to_define:\n-            wrapper.writeline(f\"{s} = {self.get_name()}.storage_offset()\")\n+            wrapper.writeline(\n+                f\"{wrapper.declare}{s} = {self.get_name()}.storage_offset(){wrapper.ending}\"\n+            )\n             symbols_to_define.remove(s)\n         assert (\n             not symbols_to_define"
        }
    ]
},
{
    "Id": 209,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/606c4f1367c7eb4a49aa4a9538dd2b1eb92485d6",
    "date": "2024-04-16T21:18:22+00:00",
    "message": "[PT] [ST] fix test_sharded_tensor (#124103)\n\nSummary:\nhttps://github.com/pytorch/pytorch/pull/123230 formalizes the rank validation to support sub groups.\n\nIt broke a few UTs, some of which got fixed in https://github.com/pytorch/pytorch/pull/123778\n\nThis is to fix the remaining one reported by DanilBaibak\n\nTest Plan: CI\n\nDifferential Revision: D56155076\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/124103\nApproved by: https://github.com/fegin",
    "label": "YES",
    "changes": [
        {
            "name": "test_sharded_tensor.py",
            "path": "test/distributed/_shard/sharded_tensor/test_sharded_tensor.py",
            "patches": [
                {
                    "old_start": 840,
                    "old_length": 8,
                    "new_start": 840,
                    "new_length": 8,
                    "hunk": "@@ -840,8 +840,8 @@ class TestShardedTensorChunked(ShardedTensorTestBase):\n         spec = ChunkShardingSpec(\n             dim=0,\n             placements=[\n-                \"rank:1/cuda:2\",\n-                \"rank:2/cuda:3\",\n+                \"rank:2/cuda:2\",\n+                \"rank:3/cuda:3\",\n             ],\n         )\n \n"
                },
                {
                    "old_start": 866,
                    "old_length": 7,
                    "new_start": 866,
                    "new_length": 7,
                    "hunk": "@@ -866,7 +866,7 @@ class TestShardedTensorChunked(ShardedTensorTestBase):\n         for shard_rank, shard_metadata in enumerate(shards_metadata):\n             self.assertEqual([shard_rank * 5, 0], shard_metadata.shard_offsets)\n             self.assertEqual([5, 20], shard_metadata.shard_sizes)\n-            self.assertEqual(f'rank:{shard_rank + 1}/cuda:{shard_rank + 2}', str(shard_metadata.placement))\n+            self.assertEqual(f'rank:{shard_rank + 2}/cuda:{shard_rank + 2}', str(shard_metadata.placement))\n \n         # Validate remote shards.\n         remote_shards = st.remote_shards()\n"
                },
                {
                    "old_start": 880,
                    "old_length": 7,
                    "new_start": 880,
                    "new_length": 7,
                    "hunk": "@@ -880,7 +880,7 @@ class TestShardedTensorChunked(ShardedTensorTestBase):\n             for remote_shard in shards:\n                 shard = remote_shard.to_here()\n                 self.assertEqual(rpc_rank, remote_shard.owner().id)\n-                self.assertEqual(f'rank:{rpc_rank - 1}/cuda:{rpc_rank}', str(shard.metadata.placement))\n+                self.assertEqual(f'rank:{rpc_rank}/cuda:{rpc_rank}', str(shard.metadata.placement))\n                 self.assertEqual((5, 20), shard.tensor.size())\n \n     @with_comms\n"
                },
                {
                    "old_start": 981,
                    "old_length": 7,
                    "new_start": 981,
                    "new_length": 10,
                    "hunk": "@@ -981,7 +981,10 @@ class TestShardedTensorChunked(ShardedTensorTestBase):\n                 sharded_tensor.empty(spec, 10, 20)\n \n         spec = ChunkShardingSpec(dim=0, placements=[\"rank:5/cuda:1\"])\n-        with self.assertRaisesRegex(ValueError, 'Invalid rank'):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Global rank 5 does not exist in input process group\"\n+        ):\n             sharded_tensor.empty(spec, 10, 20)\n \n         spec = ChunkShardingSpec(dim=0, placements=[\"rank:0/cuda:1\"])\n"
                },
                {
                    "old_start": 1180,
                    "old_length": 10,
                    "new_start": 1183,
                    "new_length": 10,
                    "hunk": "@@ -1180,10 +1183,10 @@ class TestShardedTensorChunked(ShardedTensorTestBase):\n         spec = ChunkShardingSpec(\n             dim=0,\n             placements=[\n-                \"rank:0/cuda:0\",\n-                \"rank:1/cuda:1\",\n-                \"rank:0/cuda:2\",\n-                \"rank:1/cuda:3\",\n+                \"rank:2/cuda:0\",\n+                \"rank:3/cuda:1\",\n+                \"rank:2/cuda:2\",\n+                \"rank:3/cuda:3\",\n             ],\n         )\n \n"
                },
                {
                    "old_start": 1945,
                    "old_length": 12,
                    "new_start": 1948,
                    "new_length": 12,
                    "hunk": "@@ -1945,12 +1948,12 @@ class TestShardedTensorEnumerable(ShardedTensorTestBase):\n             ShardMetadata(\n                 shard_offsets=[0, 0],\n                 shard_sizes=[5, 5],\n-                placement=\"rank:0/cuda:1\",\n+                placement=\"rank:1/cuda:1\",\n             ),\n             ShardMetadata(\n                 shard_offsets=[5, 0],\n                 shard_sizes=[5, 5],\n-                placement=\"rank:2/cuda:3\",\n+                placement=\"rank:3/cuda:3\",\n             ),\n         ])\n \n"
                },
                {
                    "old_start": 1967,
                    "old_length": 7,
                    "new_start": 1970,
                    "new_length": 7,
                    "hunk": "@@ -1967,7 +1970,7 @@ class TestShardedTensorEnumerable(ShardedTensorTestBase):\n             # Verify local shard metadata.\n             self.assertEqual((self.rank // 2 * 5, 0), local_shard.metadata.shard_offsets)\n             self.assertEqual((5, 5), local_shard.metadata.shard_sizes)\n-            self.assertEqual(f'rank:{self.rank - 1}/cuda:{self.rank}', str(local_shard.metadata.placement))\n+            self.assertEqual(f'rank:{self.rank}/cuda:{self.rank}', str(local_shard.metadata.placement))\n \n         # Verify global metadata.\n         st_metadata = st.metadata()\n"
                },
                {
                    "old_start": 1976,
                    "old_length": 7,
                    "new_start": 1979,
                    "new_length": 7,
                    "hunk": "@@ -1976,7 +1979,7 @@ class TestShardedTensorEnumerable(ShardedTensorTestBase):\n         for rank, shard_metadata in enumerate(shards_metadata):\n             self.assertEqual((rank * 5, 0), shard_metadata.shard_offsets)\n             self.assertEqual((5, 5), shard_metadata.shard_sizes)\n-            self.assertEqual(f'rank:{rank * 2}/cuda:{rank * 2 + 1}', str(shard_metadata.placement))\n+            self.assertEqual(f'rank:{rank * 2 + 1}/cuda:{rank * 2 + 1}', str(shard_metadata.placement))\n \n         # Validate remote shards.\n         remote_shards = st.remote_shards()\n"
                },
                {
                    "old_start": 1985,
                    "old_length": 7,
                    "new_start": 1988,
                    "new_length": 6,
                    "hunk": "@@ -1985,7 +1988,6 @@ class TestShardedTensorEnumerable(ShardedTensorTestBase):\n         else:\n             self.assertEqual(2, len(remote_shards))\n \n-        owners = {}\n         for rpc_rank, shards in remote_shards.items():\n             self.assertEqual(1, len(shards))\n \n"
                },
                {
                    "old_start": 2457,
                    "old_length": 7,
                    "new_start": 2459,
                    "new_length": 7,
                    "hunk": "@@ -2457,7 +2459,7 @@ class TestShardedTensorFromLocalShards(ShardedTensorTestBase):\n             local_shard_metadata = ShardMetadata(\n                 shard_offsets=[5 * (self.rank - 1), 0],\n                 shard_sizes=[5, 5],\n-                placement=f\"rank:{self.rank - 1}/cuda:{self.rank}\"\n+                placement=f\"rank:{self.rank}/cuda:{self.rank}\"\n             )\n             local_shards = [sharded_tensor.Shard(torch.randn(5, 5, device=f\"cuda:{self.rank}\"), local_shard_metadata)]\n \n"
                },
                {
                    "old_start": 2471,
                    "old_length": 7,
                    "new_start": 2473,
                    "new_length": 7,
                    "hunk": "@@ -2471,7 +2473,7 @@ class TestShardedTensorFromLocalShards(ShardedTensorTestBase):\n             # Verify local shard metadata.\n             self.assertEqual(((self.rank - 1) * 5, 0), local_shard.metadata.shard_offsets)\n             self.assertEqual((5, 5), local_shard.metadata.shard_sizes)\n-            self.assertEqual(f'rank:{self.rank - 1}/cuda:{self.rank}', str(local_shard.metadata.placement))\n+            self.assertEqual(f'rank:{self.rank}/cuda:{self.rank}', str(local_shard.metadata.placement))\n \n             # Verify global metadata.\n             st_metadata = st.metadata()\n"
                },
                {
                    "old_start": 2480,
                    "old_length": 7,
                    "new_start": 2482,
                    "new_length": 7,
                    "hunk": "@@ -2480,7 +2482,7 @@ class TestShardedTensorFromLocalShards(ShardedTensorTestBase):\n             for rank, shard_metadata in enumerate(shards_metadata):\n                 self.assertEqual((rank * 5, 0), shard_metadata.shard_offsets)\n                 self.assertEqual((5, 5), shard_metadata.shard_sizes)\n-                self.assertEqual(f'rank:{rank}/cuda:{rank + 1}', str(shard_metadata.placement))\n+                self.assertEqual(f'rank:{rank + 1}/cuda:{rank + 1}', str(shard_metadata.placement))\n \n \n     @with_comms\n"
                }
            ],
            "whole_deleted": "-                \"rank:1/cuda:2\",\n-                \"rank:2/cuda:3\",\n-            self.assertEqual(f'rank:{shard_rank + 1}/cuda:{shard_rank + 2}', str(shard_metadata.placement))\n-                self.assertEqual(f'rank:{rpc_rank - 1}/cuda:{rpc_rank}', str(shard.metadata.placement))\n-        with self.assertRaisesRegex(ValueError, 'Invalid rank'):\n-                \"rank:0/cuda:0\",\n-                \"rank:1/cuda:1\",\n-                \"rank:0/cuda:2\",\n-                \"rank:1/cuda:3\",\n-                placement=\"rank:0/cuda:1\",\n-                placement=\"rank:2/cuda:3\",\n-            self.assertEqual(f'rank:{self.rank - 1}/cuda:{self.rank}', str(local_shard.metadata.placement))\n-            self.assertEqual(f'rank:{rank * 2}/cuda:{rank * 2 + 1}', str(shard_metadata.placement))\n-        owners = {}\n-                placement=f\"rank:{self.rank - 1}/cuda:{self.rank}\"\n-            self.assertEqual(f'rank:{self.rank - 1}/cuda:{self.rank}', str(local_shard.metadata.placement))\n-                self.assertEqual(f'rank:{rank}/cuda:{rank + 1}', str(shard_metadata.placement))\n",
            "whole_added": "+                \"rank:2/cuda:2\",\n+                \"rank:3/cuda:3\",\n+            self.assertEqual(f'rank:{shard_rank + 2}/cuda:{shard_rank + 2}', str(shard_metadata.placement))\n+                self.assertEqual(f'rank:{rpc_rank}/cuda:{rpc_rank}', str(shard.metadata.placement))\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Global rank 5 does not exist in input process group\"\n+        ):\n+                \"rank:2/cuda:0\",\n+                \"rank:3/cuda:1\",\n+                \"rank:2/cuda:2\",\n+                \"rank:3/cuda:3\",\n+                placement=\"rank:1/cuda:1\",\n+                placement=\"rank:3/cuda:3\",\n+            self.assertEqual(f'rank:{self.rank}/cuda:{self.rank}', str(local_shard.metadata.placement))\n+            self.assertEqual(f'rank:{rank * 2 + 1}/cuda:{rank * 2 + 1}', str(shard_metadata.placement))\n+                placement=f\"rank:{self.rank}/cuda:{self.rank}\"\n+            self.assertEqual(f'rank:{self.rank}/cuda:{self.rank}', str(local_shard.metadata.placement))\n+                self.assertEqual(f'rank:{rank + 1}/cuda:{rank + 1}', str(shard_metadata.placement))\n",
            "whole_hunk": "@@ -840,8 +840,8 @@ class TestShardedTensorChunked(ShardedTensorTestBase):\n         spec = ChunkShardingSpec(\n             dim=0,\n             placements=[\n-                \"rank:1/cuda:2\",\n-                \"rank:2/cuda:3\",\n+                \"rank:2/cuda:2\",\n+                \"rank:3/cuda:3\",\n             ],\n         )\n \n@@ -866,7 +866,7 @@ class TestShardedTensorChunked(ShardedTensorTestBase):\n         for shard_rank, shard_metadata in enumerate(shards_metadata):\n             self.assertEqual([shard_rank * 5, 0], shard_metadata.shard_offsets)\n             self.assertEqual([5, 20], shard_metadata.shard_sizes)\n-            self.assertEqual(f'rank:{shard_rank + 1}/cuda:{shard_rank + 2}', str(shard_metadata.placement))\n+            self.assertEqual(f'rank:{shard_rank + 2}/cuda:{shard_rank + 2}', str(shard_metadata.placement))\n \n         # Validate remote shards.\n         remote_shards = st.remote_shards()\n@@ -880,7 +880,7 @@ class TestShardedTensorChunked(ShardedTensorTestBase):\n             for remote_shard in shards:\n                 shard = remote_shard.to_here()\n                 self.assertEqual(rpc_rank, remote_shard.owner().id)\n-                self.assertEqual(f'rank:{rpc_rank - 1}/cuda:{rpc_rank}', str(shard.metadata.placement))\n+                self.assertEqual(f'rank:{rpc_rank}/cuda:{rpc_rank}', str(shard.metadata.placement))\n                 self.assertEqual((5, 20), shard.tensor.size())\n \n     @with_comms\n@@ -981,7 +981,10 @@ class TestShardedTensorChunked(ShardedTensorTestBase):\n                 sharded_tensor.empty(spec, 10, 20)\n \n         spec = ChunkShardingSpec(dim=0, placements=[\"rank:5/cuda:1\"])\n-        with self.assertRaisesRegex(ValueError, 'Invalid rank'):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Global rank 5 does not exist in input process group\"\n+        ):\n             sharded_tensor.empty(spec, 10, 20)\n \n         spec = ChunkShardingSpec(dim=0, placements=[\"rank:0/cuda:1\"])\n@@ -1180,10 +1183,10 @@ class TestShardedTensorChunked(ShardedTensorTestBase):\n         spec = ChunkShardingSpec(\n             dim=0,\n             placements=[\n-                \"rank:0/cuda:0\",\n-                \"rank:1/cuda:1\",\n-                \"rank:0/cuda:2\",\n-                \"rank:1/cuda:3\",\n+                \"rank:2/cuda:0\",\n+                \"rank:3/cuda:1\",\n+                \"rank:2/cuda:2\",\n+                \"rank:3/cuda:3\",\n             ],\n         )\n \n@@ -1945,12 +1948,12 @@ class TestShardedTensorEnumerable(ShardedTensorTestBase):\n             ShardMetadata(\n                 shard_offsets=[0, 0],\n                 shard_sizes=[5, 5],\n-                placement=\"rank:0/cuda:1\",\n+                placement=\"rank:1/cuda:1\",\n             ),\n             ShardMetadata(\n                 shard_offsets=[5, 0],\n                 shard_sizes=[5, 5],\n-                placement=\"rank:2/cuda:3\",\n+                placement=\"rank:3/cuda:3\",\n             ),\n         ])\n \n@@ -1967,7 +1970,7 @@ class TestShardedTensorEnumerable(ShardedTensorTestBase):\n             # Verify local shard metadata.\n             self.assertEqual((self.rank // 2 * 5, 0), local_shard.metadata.shard_offsets)\n             self.assertEqual((5, 5), local_shard.metadata.shard_sizes)\n-            self.assertEqual(f'rank:{self.rank - 1}/cuda:{self.rank}', str(local_shard.metadata.placement))\n+            self.assertEqual(f'rank:{self.rank}/cuda:{self.rank}', str(local_shard.metadata.placement))\n \n         # Verify global metadata.\n         st_metadata = st.metadata()\n@@ -1976,7 +1979,7 @@ class TestShardedTensorEnumerable(ShardedTensorTestBase):\n         for rank, shard_metadata in enumerate(shards_metadata):\n             self.assertEqual((rank * 5, 0), shard_metadata.shard_offsets)\n             self.assertEqual((5, 5), shard_metadata.shard_sizes)\n-            self.assertEqual(f'rank:{rank * 2}/cuda:{rank * 2 + 1}', str(shard_metadata.placement))\n+            self.assertEqual(f'rank:{rank * 2 + 1}/cuda:{rank * 2 + 1}', str(shard_metadata.placement))\n \n         # Validate remote shards.\n         remote_shards = st.remote_shards()\n@@ -1985,7 +1988,6 @@ class TestShardedTensorEnumerable(ShardedTensorTestBase):\n         else:\n             self.assertEqual(2, len(remote_shards))\n \n-        owners = {}\n         for rpc_rank, shards in remote_shards.items():\n             self.assertEqual(1, len(shards))\n \n@@ -2457,7 +2459,7 @@ class TestShardedTensorFromLocalShards(ShardedTensorTestBase):\n             local_shard_metadata = ShardMetadata(\n                 shard_offsets=[5 * (self.rank - 1), 0],\n                 shard_sizes=[5, 5],\n-                placement=f\"rank:{self.rank - 1}/cuda:{self.rank}\"\n+                placement=f\"rank:{self.rank}/cuda:{self.rank}\"\n             )\n             local_shards = [sharded_tensor.Shard(torch.randn(5, 5, device=f\"cuda:{self.rank}\"), local_shard_metadata)]\n \n@@ -2471,7 +2473,7 @@ class TestShardedTensorFromLocalShards(ShardedTensorTestBase):\n             # Verify local shard metadata.\n             self.assertEqual(((self.rank - 1) * 5, 0), local_shard.metadata.shard_offsets)\n             self.assertEqual((5, 5), local_shard.metadata.shard_sizes)\n-            self.assertEqual(f'rank:{self.rank - 1}/cuda:{self.rank}', str(local_shard.metadata.placement))\n+            self.assertEqual(f'rank:{self.rank}/cuda:{self.rank}', str(local_shard.metadata.placement))\n \n             # Verify global metadata.\n             st_metadata = st.metadata()\n@@ -2480,7 +2482,7 @@ class TestShardedTensorFromLocalShards(ShardedTensorTestBase):\n             for rank, shard_metadata in enumerate(shards_metadata):\n                 self.assertEqual((rank * 5, 0), shard_metadata.shard_offsets)\n                 self.assertEqual((5, 5), shard_metadata.shard_sizes)\n-                self.assertEqual(f'rank:{rank}/cuda:{rank + 1}', str(shard_metadata.placement))\n+                self.assertEqual(f'rank:{rank + 1}/cuda:{rank + 1}', str(shard_metadata.placement))\n \n \n     @with_comms\n"
        },
        {
            "name": "api.py",
            "path": "torch/distributed/_shard/sharded_tensor/api.py",
            "patches": [
                {
                    "old_start": 262,
                    "old_length": 7,
                    "new_start": 262,
                    "new_length": 7,
                    "hunk": "@@ -262,7 +262,7 @@ class ShardedTensor(ShardedTensorBase):\n \n         self._metadata.tensor_properties.memory_format = memory_format\n \n-        current_rank = dist.get_rank(self._process_group)\n+        current_rank = dist.get_rank()  # global rank\n \n         for shard_metadata in self._metadata.shards_metadata:\n             rank, device = _parse_and_validate_remote_device(self._process_group, shard_metadata.placement)\n"
                }
            ],
            "whole_deleted": "-        current_rank = dist.get_rank(self._process_group)\n",
            "whole_added": "+        current_rank = dist.get_rank()  # global rank\n",
            "whole_hunk": "@@ -262,7 +262,7 @@ class ShardedTensor(ShardedTensorBase):\n \n         self._metadata.tensor_properties.memory_format = memory_format\n \n-        current_rank = dist.get_rank(self._process_group)\n+        current_rank = dist.get_rank()  # global rank\n \n         for shard_metadata in self._metadata.shards_metadata:\n             rank, device = _parse_and_validate_remote_device(self._process_group, shard_metadata.placement)\n"
        },
        {
            "name": "utils.py",
            "path": "torch/distributed/_shard/sharded_tensor/utils.py",
            "patches": [
                {
                    "old_start": 23,
                    "old_length": 7,
                    "new_start": 23,
                    "new_length": 7,
                    "hunk": "@@ -23,7 +23,7 @@ def _parse_and_validate_remote_device(pg, remote_device):\n     device = remote_device.device()\n \n     # Validate rank, skip validation if rank is not part of process group.\n-    if not c10d._rank_not_in_group(pg):\n+    if rank is not None and not c10d._rank_not_in_group(pg):\n         pg_global_ranks = c10d.get_process_group_ranks(pg)\n         if rank not in pg_global_ranks:\n             raise ValueError("
                }
            ],
            "whole_deleted": "-    if not c10d._rank_not_in_group(pg):\n",
            "whole_added": "+    if rank is not None and not c10d._rank_not_in_group(pg):\n",
            "whole_hunk": "@@ -23,7 +23,7 @@ def _parse_and_validate_remote_device(pg, remote_device):\n     device = remote_device.device()\n \n     # Validate rank, skip validation if rank is not part of process group.\n-    if not c10d._rank_not_in_group(pg):\n+    if rank is not None and not c10d._rank_not_in_group(pg):\n         pg_global_ranks = c10d.get_process_group_ranks(pg)\n         if rank not in pg_global_ranks:\n             raise ValueError("
        }
    ]
},
{
    "Id": 354,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/4926146537df5a39846dae0e551d394001588b13",
    "date": "2024-01-04T01:06:27+00:00",
    "message": "[Inductor] Fix Conv Binary Inplace Fusion issue (#115153)\n\n**Summary**\nTake this Pattern as example\n```\n  #      ReLU\n  #     /   \\\n  #  Conv1\n  #   /      \\\n  # Conv2\n  #   \\      /\n  #      Add\n```\nThe current `ConvBinaryInplace` check will fail to perform Inplace fusion (using outplace fusion instead) due to `ReLU` having 2 users. However, if all users of `ReLU` are ancestor nodes of `Conv2`, we should be able to proceed with the `ConvBinaryInplace` fusion. This diff relaxes the `ConvBinaryInplace` check accordingly.\n\n**TestPlan**\n```\npython -m pytest test_mkldnn_pattern_matcher.py -k test_conv2d_binary_inplace_fusion_pass_cpu\npython -m pytest test_mkldnn_pattern_matcher.py -k test_conv2d_binary_inplace_fusion_failed_cpu\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115153\nApproved by: https://github.com/CaoE, https://github.com/jgong5",
    "label": "YES",
    "changes": [
        {
            "name": "test_mkldnn_pattern_matcher.py",
            "path": "test/inductor/test_mkldnn_pattern_matcher.py",
            "patches": [
                {
                    "old_start": 1537,
                    "old_length": 7,
                    "new_start": 1537,
                    "new_length": 7,
                    "hunk": "@@ -1537,7 +1537,7 @@ class TestPatternMatcher(TestPatternMatcherBase):\n     def test_conv2d_binary_inplace_fusion_pass_cpu(\n         self, include_ops=None, exclude_ops=None\n     ):\n-        class Model(torch.nn.Module):\n+        class Model_v1(torch.nn.Module):\n             def __init__(self):\n                 super().__init__()\n                 self.conv = torch.nn.Conv2d(\n"
                },
                {
                    "old_start": 1548,
                    "old_length": 18,
                    "new_start": 1548,
                    "new_length": 42,
                    "hunk": "@@ -1548,18 +1548,42 @@ class TestPatternMatcher(TestPatternMatcherBase):\n                 conv_out = self.conv(x)\n                 return torch.add(conv_out, other.relu())\n \n-        inputs = [\n-            torch.randn(1, 3, 28, 28).to(memory_format=torch.channels_last),\n+        class Model_v2(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.conv = torch.nn.Conv2d(\n+                    in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1\n+                )\n+                self.conv2 = torch.nn.Conv2d(\n+                    in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1\n+                )\n+                self.conv3 = torch.nn.Conv2d(\n+                    in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1\n+                )\n+\n+            def forward(self, x, _):\n+                conv_out1 = self.conv(x)\n+                pow_out = torch.pow(conv_out1, 2)\n+                conv_out2 = self.conv2(pow_out)\n+                conv_out3 = self.conv3(conv_out2)\n+                res = torch.add(conv_out3, pow_out)\n+                return res\n+\n+        input = torch.randn(1, 3, 28, 28).to(memory_format=torch.channels_last)\n+        others = [\n+            torch.randn(1, 32, 28, 28).to(memory_format=torch.channels_last),\n             torch.randn(1, 32, 28, 28).to(memory_format=torch.channels_last),\n         ]\n-        mod = Model().to(memory_format=torch.channels_last).eval()\n+        mod_v1 = Model_v1().to(memory_format=torch.channels_last).eval()\n+        mod_v2 = Model_v2().to(memory_format=torch.channels_last).eval()\n \n         if include_ops is None:\n             include_ops = [\"mkldnn._convolution_pointwise_.binary\"]\n         if exclude_ops is None:\n             exclude_ops = [\"mkldnn._convolution_pointwise.binary\"]\n \n-        self._test_code_common(mod, inputs, include_ops, exclude_ops)\n+        for other, mod in zip(others, [mod_v1, mod_v2]):\n+            self._test_code_common(mod, (input, other), include_ops, exclude_ops)\n \n     def test_conv2d_binary_inplace_fusion_failed_cpu(\n         self, include_ops=None, exclude_ops=None\n"
                },
                {
                    "old_start": 1588,
                    "old_length": 20,
                    "new_start": 1612,
                    "new_length": 40,
                    "hunk": "@@ -1588,20 +1612,40 @@ class TestPatternMatcher(TestPatternMatcherBase):\n                 conv_out = self.conv(x)\n                 return torch.add(conv_out, other[1:2, :, :, :]), other\n \n+        class Model_v3(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.conv = torch.nn.Conv2d(\n+                    in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1\n+                )\n+                self.conv2 = torch.nn.Conv2d(\n+                    in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1\n+                )\n+\n+            def forward(self, x, _):\n+                pow_out = torch.pow(self.conv(x), 2)\n+                other2 = F.relu(pow_out)\n+                conv_out2 = self.conv2(pow_out)\n+                res = torch.add(conv_out2, pow_out)\n+                res = res + other2\n+                return res\n+\n         input = torch.randn(1, 3, 28, 28).to(memory_format=torch.channels_last)\n         others = [\n             torch.randn(1, 32, 28, 28).to(memory_format=torch.channels_last),\n             torch.randn(2, 32, 28, 28).to(memory_format=torch.channels_last),\n+            torch.randn(1, 32, 28, 28).to(memory_format=torch.channels_last),\n         ]\n         mod_v1 = Model_v1().to(memory_format=torch.channels_last).eval()\n         mod_v2 = Model_v2().to(memory_format=torch.channels_last).eval()\n+        mod_v3 = Model_v3().to(memory_format=torch.channels_last).eval()\n \n         if include_ops is None:\n             include_ops = [\"mkldnn._convolution_pointwise.binary\"]\n         if exclude_ops is None:\n             exclude_ops = [\"mkldnn._convolution_pointwise_.binary\"]\n \n-        for other, mod in zip(others, [mod_v1, mod_v2]):\n+        for other, mod in zip(others, [mod_v1, mod_v2, mod_v3]):\n             self._test_code_common(mod, (input, other), include_ops, exclude_ops)\n \n     def test_conv2d_binary_fusion_failed(self):\n"
                }
            ],
            "whole_deleted": "-        class Model(torch.nn.Module):\n-        inputs = [\n-            torch.randn(1, 3, 28, 28).to(memory_format=torch.channels_last),\n-        mod = Model().to(memory_format=torch.channels_last).eval()\n-        self._test_code_common(mod, inputs, include_ops, exclude_ops)\n-        for other, mod in zip(others, [mod_v1, mod_v2]):\n",
            "whole_added": "+        class Model_v1(torch.nn.Module):\n+        class Model_v2(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.conv = torch.nn.Conv2d(\n+                    in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1\n+                )\n+                self.conv2 = torch.nn.Conv2d(\n+                    in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1\n+                )\n+                self.conv3 = torch.nn.Conv2d(\n+                    in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1\n+                )\n+\n+            def forward(self, x, _):\n+                conv_out1 = self.conv(x)\n+                pow_out = torch.pow(conv_out1, 2)\n+                conv_out2 = self.conv2(pow_out)\n+                conv_out3 = self.conv3(conv_out2)\n+                res = torch.add(conv_out3, pow_out)\n+                return res\n+\n+        input = torch.randn(1, 3, 28, 28).to(memory_format=torch.channels_last)\n+        others = [\n+            torch.randn(1, 32, 28, 28).to(memory_format=torch.channels_last),\n+        mod_v1 = Model_v1().to(memory_format=torch.channels_last).eval()\n+        mod_v2 = Model_v2().to(memory_format=torch.channels_last).eval()\n+        for other, mod in zip(others, [mod_v1, mod_v2]):\n+            self._test_code_common(mod, (input, other), include_ops, exclude_ops)\n+        class Model_v3(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.conv = torch.nn.Conv2d(\n+                    in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1\n+                )\n+                self.conv2 = torch.nn.Conv2d(\n+                    in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1\n+                )\n+\n+            def forward(self, x, _):\n+                pow_out = torch.pow(self.conv(x), 2)\n+                other2 = F.relu(pow_out)\n+                conv_out2 = self.conv2(pow_out)\n+                res = torch.add(conv_out2, pow_out)\n+                res = res + other2\n+                return res\n+\n+            torch.randn(1, 32, 28, 28).to(memory_format=torch.channels_last),\n+        mod_v3 = Model_v3().to(memory_format=torch.channels_last).eval()\n+        for other, mod in zip(others, [mod_v1, mod_v2, mod_v3]):\n",
            "whole_hunk": "@@ -1537,7 +1537,7 @@ class TestPatternMatcher(TestPatternMatcherBase):\n     def test_conv2d_binary_inplace_fusion_pass_cpu(\n         self, include_ops=None, exclude_ops=None\n     ):\n-        class Model(torch.nn.Module):\n+        class Model_v1(torch.nn.Module):\n             def __init__(self):\n                 super().__init__()\n                 self.conv = torch.nn.Conv2d(\n@@ -1548,18 +1548,42 @@ class TestPatternMatcher(TestPatternMatcherBase):\n                 conv_out = self.conv(x)\n                 return torch.add(conv_out, other.relu())\n \n-        inputs = [\n-            torch.randn(1, 3, 28, 28).to(memory_format=torch.channels_last),\n+        class Model_v2(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.conv = torch.nn.Conv2d(\n+                    in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1\n+                )\n+                self.conv2 = torch.nn.Conv2d(\n+                    in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1\n+                )\n+                self.conv3 = torch.nn.Conv2d(\n+                    in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1\n+                )\n+\n+            def forward(self, x, _):\n+                conv_out1 = self.conv(x)\n+                pow_out = torch.pow(conv_out1, 2)\n+                conv_out2 = self.conv2(pow_out)\n+                conv_out3 = self.conv3(conv_out2)\n+                res = torch.add(conv_out3, pow_out)\n+                return res\n+\n+        input = torch.randn(1, 3, 28, 28).to(memory_format=torch.channels_last)\n+        others = [\n+            torch.randn(1, 32, 28, 28).to(memory_format=torch.channels_last),\n             torch.randn(1, 32, 28, 28).to(memory_format=torch.channels_last),\n         ]\n-        mod = Model().to(memory_format=torch.channels_last).eval()\n+        mod_v1 = Model_v1().to(memory_format=torch.channels_last).eval()\n+        mod_v2 = Model_v2().to(memory_format=torch.channels_last).eval()\n \n         if include_ops is None:\n             include_ops = [\"mkldnn._convolution_pointwise_.binary\"]\n         if exclude_ops is None:\n             exclude_ops = [\"mkldnn._convolution_pointwise.binary\"]\n \n-        self._test_code_common(mod, inputs, include_ops, exclude_ops)\n+        for other, mod in zip(others, [mod_v1, mod_v2]):\n+            self._test_code_common(mod, (input, other), include_ops, exclude_ops)\n \n     def test_conv2d_binary_inplace_fusion_failed_cpu(\n         self, include_ops=None, exclude_ops=None\n@@ -1588,20 +1612,40 @@ class TestPatternMatcher(TestPatternMatcherBase):\n                 conv_out = self.conv(x)\n                 return torch.add(conv_out, other[1:2, :, :, :]), other\n \n+        class Model_v3(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.conv = torch.nn.Conv2d(\n+                    in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1\n+                )\n+                self.conv2 = torch.nn.Conv2d(\n+                    in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1\n+                )\n+\n+            def forward(self, x, _):\n+                pow_out = torch.pow(self.conv(x), 2)\n+                other2 = F.relu(pow_out)\n+                conv_out2 = self.conv2(pow_out)\n+                res = torch.add(conv_out2, pow_out)\n+                res = res + other2\n+                return res\n+\n         input = torch.randn(1, 3, 28, 28).to(memory_format=torch.channels_last)\n         others = [\n             torch.randn(1, 32, 28, 28).to(memory_format=torch.channels_last),\n             torch.randn(2, 32, 28, 28).to(memory_format=torch.channels_last),\n+            torch.randn(1, 32, 28, 28).to(memory_format=torch.channels_last),\n         ]\n         mod_v1 = Model_v1().to(memory_format=torch.channels_last).eval()\n         mod_v2 = Model_v2().to(memory_format=torch.channels_last).eval()\n+        mod_v3 = Model_v3().to(memory_format=torch.channels_last).eval()\n \n         if include_ops is None:\n             include_ops = [\"mkldnn._convolution_pointwise.binary\"]\n         if exclude_ops is None:\n             exclude_ops = [\"mkldnn._convolution_pointwise_.binary\"]\n \n-        for other, mod in zip(others, [mod_v1, mod_v2]):\n+        for other, mod in zip(others, [mod_v1, mod_v2, mod_v3]):\n             self._test_code_common(mod, (input, other), include_ops, exclude_ops)\n \n     def test_conv2d_binary_fusion_failed(self):\n"
        },
        {
            "name": "mkldnn_fusion.py",
            "path": "torch/_inductor/fx_passes/mkldnn_fusion.py",
            "patches": [
                {
                    "old_start": 366,
                    "old_length": 12,
                    "new_start": 366,
                    "new_length": 69,
                    "hunk": "@@ -366,12 +366,69 @@ if torch._C._has_mkldnn:\n \n         return fn\n \n+    def _get_remaining_users(extra_input_node, compute_node):\n+        # Think about this pattern:\n+        #      ReLU\n+        #     /   \\\n+        #  Conv1\n+        #   /      \\\n+        # Conv2\n+        #   \\      /\n+        #      Add\n+        # Although, the extra input node (ReLU) has more than 1 users: Conv1 and Add.\n+        # The Conv1 is the ancestor node of the current compute node (Conv2).\n+        # This indicates that the buffer of ReLU has completed all its usage,\n+        # So we can safely make changes to it now by doing Conv2->Add inplace fusion.\n+        # Take above case as example:\n+        # * extra_input_node: ReLU\n+        # * compute_node: Conv2\n+        # _get_remaining_users will return the users of extra_input_node which are not\n+        # ancestor node of compute_node.\n+        def _is_ancestor_node(_current_node, _ancestor_node):\n+            # Check whether _ancestor_node is the ancestor node of _current_node\n+            _node_list = [_current_node]\n+            _visited_nodes = set()\n+            while len(_node_list) != 0:\n+                _current_node = _node_list.pop(0)\n+                if _current_node not in _visited_nodes:\n+                    _visited_nodes.add(_current_node)\n+                    if _current_node == _ancestor_node:\n+                        return True\n+                    elif isinstance(\n+                        _current_node, torch.fx.Node\n+                    ) and _current_node.op not in [\"placeholder\", \"output\", \"get_attr\"]:\n+                        for input in _current_node.all_input_nodes:\n+                            _node_list.append(input)  # noqa: PERF402\n+            return False\n+\n+        return [\n+            user\n+            for user in list(extra_input_node.users)\n+            if not _is_ancestor_node(compute_node, user)\n+        ]\n+\n     def _is_valid_computation_binary_inplace(computation_op, binary_op, other_index):\n         def fn(match):\n             if not _is_valid_computation_binary(computation_op, binary_op)(match):\n                 return False\n             binary_nodes = filter_nodes(match.nodes, binary_op)\n-            if any(len(n.args[other_index].users) > 1 for n in binary_nodes):\n+\n+            def _get_compute_node(_binary_node, _other_index):\n+                assert (\n+                    len(_binary_node.all_input_nodes) == 2\n+                ), \"Binary node should have 2 input nodes.\"\n+                _compute_index = 1 if (_other_index == 0) else 0\n+                return _binary_node.args[_compute_index]\n+\n+            if any(\n+                len(\n+                    _get_remaining_users(\n+                        n.args[other_index], _get_compute_node(n, other_index)\n+                    )\n+                )\n+                > 1\n+                for n in binary_nodes\n+            ):\n                 return False\n             if any(\n                 n.args[other_index].op in [\"placeholder\", \"output\"]"
                }
            ],
            "whole_deleted": "-            if any(len(n.args[other_index].users) > 1 for n in binary_nodes):\n",
            "whole_added": "+    def _get_remaining_users(extra_input_node, compute_node):\n+        # Think about this pattern:\n+        #      ReLU\n+        #     /   \\\n+        #  Conv1\n+        #   /      \\\n+        # Conv2\n+        #   \\      /\n+        #      Add\n+        # Although, the extra input node (ReLU) has more than 1 users: Conv1 and Add.\n+        # The Conv1 is the ancestor node of the current compute node (Conv2).\n+        # This indicates that the buffer of ReLU has completed all its usage,\n+        # So we can safely make changes to it now by doing Conv2->Add inplace fusion.\n+        # Take above case as example:\n+        # * extra_input_node: ReLU\n+        # * compute_node: Conv2\n+        # _get_remaining_users will return the users of extra_input_node which are not\n+        # ancestor node of compute_node.\n+        def _is_ancestor_node(_current_node, _ancestor_node):\n+            # Check whether _ancestor_node is the ancestor node of _current_node\n+            _node_list = [_current_node]\n+            _visited_nodes = set()\n+            while len(_node_list) != 0:\n+                _current_node = _node_list.pop(0)\n+                if _current_node not in _visited_nodes:\n+                    _visited_nodes.add(_current_node)\n+                    if _current_node == _ancestor_node:\n+                        return True\n+                    elif isinstance(\n+                        _current_node, torch.fx.Node\n+                    ) and _current_node.op not in [\"placeholder\", \"output\", \"get_attr\"]:\n+                        for input in _current_node.all_input_nodes:\n+                            _node_list.append(input)  # noqa: PERF402\n+            return False\n+\n+        return [\n+            user\n+            for user in list(extra_input_node.users)\n+            if not _is_ancestor_node(compute_node, user)\n+        ]\n+\n+\n+            def _get_compute_node(_binary_node, _other_index):\n+                assert (\n+                    len(_binary_node.all_input_nodes) == 2\n+                ), \"Binary node should have 2 input nodes.\"\n+                _compute_index = 1 if (_other_index == 0) else 0\n+                return _binary_node.args[_compute_index]\n+\n+            if any(\n+                len(\n+                    _get_remaining_users(\n+                        n.args[other_index], _get_compute_node(n, other_index)\n+                    )\n+                )\n+                > 1\n+                for n in binary_nodes\n+            ):\n",
            "whole_hunk": "@@ -366,12 +366,69 @@ if torch._C._has_mkldnn:\n \n         return fn\n \n+    def _get_remaining_users(extra_input_node, compute_node):\n+        # Think about this pattern:\n+        #      ReLU\n+        #     /   \\\n+        #  Conv1\n+        #   /      \\\n+        # Conv2\n+        #   \\      /\n+        #      Add\n+        # Although, the extra input node (ReLU) has more than 1 users: Conv1 and Add.\n+        # The Conv1 is the ancestor node of the current compute node (Conv2).\n+        # This indicates that the buffer of ReLU has completed all its usage,\n+        # So we can safely make changes to it now by doing Conv2->Add inplace fusion.\n+        # Take above case as example:\n+        # * extra_input_node: ReLU\n+        # * compute_node: Conv2\n+        # _get_remaining_users will return the users of extra_input_node which are not\n+        # ancestor node of compute_node.\n+        def _is_ancestor_node(_current_node, _ancestor_node):\n+            # Check whether _ancestor_node is the ancestor node of _current_node\n+            _node_list = [_current_node]\n+            _visited_nodes = set()\n+            while len(_node_list) != 0:\n+                _current_node = _node_list.pop(0)\n+                if _current_node not in _visited_nodes:\n+                    _visited_nodes.add(_current_node)\n+                    if _current_node == _ancestor_node:\n+                        return True\n+                    elif isinstance(\n+                        _current_node, torch.fx.Node\n+                    ) and _current_node.op not in [\"placeholder\", \"output\", \"get_attr\"]:\n+                        for input in _current_node.all_input_nodes:\n+                            _node_list.append(input)  # noqa: PERF402\n+            return False\n+\n+        return [\n+            user\n+            for user in list(extra_input_node.users)\n+            if not _is_ancestor_node(compute_node, user)\n+        ]\n+\n     def _is_valid_computation_binary_inplace(computation_op, binary_op, other_index):\n         def fn(match):\n             if not _is_valid_computation_binary(computation_op, binary_op)(match):\n                 return False\n             binary_nodes = filter_nodes(match.nodes, binary_op)\n-            if any(len(n.args[other_index].users) > 1 for n in binary_nodes):\n+\n+            def _get_compute_node(_binary_node, _other_index):\n+                assert (\n+                    len(_binary_node.all_input_nodes) == 2\n+                ), \"Binary node should have 2 input nodes.\"\n+                _compute_index = 1 if (_other_index == 0) else 0\n+                return _binary_node.args[_compute_index]\n+\n+            if any(\n+                len(\n+                    _get_remaining_users(\n+                        n.args[other_index], _get_compute_node(n, other_index)\n+                    )\n+                )\n+                > 1\n+                for n in binary_nodes\n+            ):\n                 return False\n             if any(\n                 n.args[other_index].op in [\"placeholder\", \"output\"]"
        }
    ]
},
{
    "Id": 77,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/1877b7896c237567285804ecc138bc86180a7ced",
    "date": "2024-06-18T18:18:50+00:00",
    "message": "[checkpoint] Clean up selective activation checkpoint and make public (#125795)\n\n### bc-breaking for existing users of the private API:\n- Existing policy functions must now change their return value to be [CheckpointPolicy](https://github.com/pytorch/pytorch/blob/c0b40ab42e38a208351911496b7153511304f8da/torch/utils/checkpoint.py#L1204-L1230)  Enum instead of bool.\n   - To restore previous behavior, return `PREFER_RECOMPUTE` instead of `False` and `{PREFER,MUST}_SAVE` instead of `True` depending whether you prefer the compiler to override your policy.\n- Policy function now accepts a `ctx` object instead of `mode` for its first argument.\n   - To restore previous behavior, `mode = \"recompute\" if ctx.is_recompute else \"forward\"`.\n- Existing calls to `_pt2_selective_checkpoint_context_fn_gen` must be renamed to `create_selective_checkpoint_contexts `. The way you use the API remains the same. It would've been nice to do something different (not make the user have to use functools.partial?), but this was the easiest to compile (idk if this should actually be a constraint).\n\nRelated doc: https://docs.google.com/document/d/1BKyizkZPdri9mHqdDOLAUpkI7SbbKfLHRFVVpK9ZWqo/edit\n\nMemory considerations:\n- As with the existing SAC, cached values are cleared upon first use.\n- We error if the user wishes to backward a second time on a region forwarded with SAC enabled.\n\nIn-place:\n- We use version counting to enforce that if any cached tensor has been mutated. In-place operations not mutating cached tensors are allowed.\n- `allow_cache_entry_mutation=True` can be passed to disable this check (useful in the case of auto AC where the user is cleverly also saves the output of the in-place)\n\nRandomness, views\n- Currently in this PR, we don't do anything special for randomness or views, the author of the policy function is expected to handle them properly. (Would it would be beneficial to error? - we either want to save all or recompute all random tensors)\n\nTensor object preservation\n- ~We guarantee that if a tensor does not requires grad, and it is saved, then what you get out is the same tensor object.~ UPDATE: We guarantee that if a tensor is of non-differentiable dtype AND it is not a view, and it is saved, then what you get out is the same tensor object. This is a nice guarantee for nested tensors which care about the object identity of of the offsets tensor.\n\nPolicy function\n- Enum values are `{MUST,PREFER}_{SAVE,RECOMPUTE}` (bikeshed welcome). Alternatively there was `{SAVE,RECOMPUTE}_{NON_,}OVERRIDABLE`. The former was preferred bc it seemed clearer that two `MUST` clashing should error, versus it is ambiguous whether two `NON_OVERRIDABLE` being stacked should silently ignore or error.\n- The usage of Enum today. There actually is NO API to stack SAC policies today. The only thing the Enum should matter for in the near term is the compiler. The stacking SAC policy would be useful if someone wants to implement something like simple FSDP, but it is not perfect because with a policy of `PREFER_SAVE` you are actually saving more than autograd would save normally (would be fixed with AC v3).\n- The number of times we call the policy_fn is something that should be documented as part of public API. We call the policy function for all ops except ~~detach~~ UPDATE :  metadata ops listed in `torch.utils.checkpoint.SAC_IGNORED_OPS`) because these ops may be called a different number of times by AC itself between forward and recompute.\n- The policy function can be a stateful object (we do NOT make separate copies of this object for forward/recompute, the user is expected to handle that via is_recompute see below).\nTensors guaranteed to be the same tensor as-is\n- Policy function signature takes ctx object as its first argument. The ctx function is an object encapsulating info that may be useful to the user, it currently only holds \"is_recompute\". Adding this indirection gives us flexibility to add more attrs later if necessary.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125795\nApproved by: https://github.com/Chillee, https://github.com/fmassa",
    "label": "NO",
    "changes": [
        {
            "name": "checkpoint.rst",
            "path": "docs/source/checkpoint.rst",
            "patches": [
                {
                    "old_start": 35,
                    "old_length": 3,
                    "new_start": 35,
                    "new_length": 6,
                    "hunk": "@@ -35,3 +35,6 @@ torch.utils.checkpoint\n .. autofunction:: checkpoint\n .. autofunction:: checkpoint_sequential\n .. autofunction:: set_checkpoint_debug_enabled\n+.. autoclass:: CheckpointPolicy\n+.. autoclass:: SelectiveCheckpointContext\n+.. autofunction:: create_selective_checkpoint_contexts\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+.. autoclass:: CheckpointPolicy\n+.. autoclass:: SelectiveCheckpointContext\n+.. autofunction:: create_selective_checkpoint_contexts\n",
            "whole_hunk": "@@ -35,3 +35,6 @@ torch.utils.checkpoint\n .. autofunction:: checkpoint\n .. autofunction:: checkpoint_sequential\n .. autofunction:: set_checkpoint_debug_enabled\n+.. autoclass:: CheckpointPolicy\n+.. autoclass:: SelectiveCheckpointContext\n+.. autofunction:: create_selective_checkpoint_contexts\n"
        },
        {
            "name": "test_activation_checkpointing.py",
            "path": "test/dynamo/test_activation_checkpointing.py",
            "patches": [
                {
                    "old_start": 19,
                    "old_length": 7,
                    "new_start": 19,
                    "new_length": 11,
                    "hunk": "@@ -19,7 +19,11 @@ from torch._higher_order_ops.wrap import tag_activation_checkpoint\n from torch.testing._internal.common_utils import IS_WINDOWS, skipIfRocm\n from torch.testing._internal.inductor_utils import HAS_CUDA\n from torch.testing._internal.two_tensor import TwoTensor\n-from torch.utils.checkpoint import _pt2_selective_checkpoint_context_fn_gen, checkpoint\n+from torch.utils.checkpoint import (\n+    checkpoint,\n+    CheckpointPolicy,\n+    create_selective_checkpoint_contexts,\n+)\n \n requires_cuda = unittest.skipUnless(HAS_CUDA, \"requires cuda\")\n requires_distributed = functools.partial(\n"
                },
                {
                    "old_start": 105,
                    "old_length": 8,
                    "new_start": 109,
                    "new_length": 11,
                    "hunk": "@@ -105,8 +109,11 @@ def op_count(gm):\n \n \n def _get_custom_policy(no_recompute_list=None):\n-    def _custom_policy(mode, func, *args, **kwargs):\n-        return func in no_recompute_list\n+    def _custom_policy(ctx, func, *args, **kwargs):\n+        if func in no_recompute_list:\n+            return CheckpointPolicy.MUST_SAVE\n+        else:\n+            return CheckpointPolicy.PREFER_RECOMPUTE\n \n     return _custom_policy\n \n"
                },
                {
                    "old_start": 530,
                    "old_length": 7,
                    "new_start": 537,
                    "new_length": 7,
                    "hunk": "@@ -530,7 +537,7 @@ class ActivationCheckpointingViaTagsTests(torch._dynamo.test_case.TestCase):\n             no_recompute_list = [\n                 torch.ops.aten.mm.default,\n             ]\n-            return _pt2_selective_checkpoint_context_fn_gen(\n+            return create_selective_checkpoint_contexts(\n                 _get_custom_policy(no_recompute_list=no_recompute_list)\n             )\n \n"
                },
                {
                    "old_start": 580,
                    "old_length": 7,
                    "new_start": 587,
                    "new_length": 7,
                    "hunk": "@@ -580,7 +587,7 @@ class ActivationCheckpointingViaTagsTests(torch._dynamo.test_case.TestCase):\n             no_recompute_list = [\n                 torch.ops.aten.mm.default,\n             ]\n-            return _pt2_selective_checkpoint_context_fn_gen(\n+            return create_selective_checkpoint_contexts(\n                 _get_custom_policy(no_recompute_list=no_recompute_list)\n             )\n \n"
                },
                {
                    "old_start": 650,
                    "old_length": 7,
                    "new_start": 657,
                    "new_length": 7,
                    "hunk": "@@ -650,7 +657,7 @@ class ActivationCheckpointingViaTagsTests(torch._dynamo.test_case.TestCase):\n \n         def selective_checkpointing_context_fn():\n             meta = {}\n-            return _pt2_selective_checkpoint_context_fn_gen(_get_custom_policy(meta))\n+            return create_selective_checkpoint_contexts(_get_custom_policy(meta))\n \n         def gn(x, y):\n             return torch.sigmoid(\n"
                },
                {
                    "old_start": 698,
                    "old_length": 7,
                    "new_start": 705,
                    "new_length": 7,
                    "hunk": "@@ -698,7 +705,7 @@ class ActivationCheckpointingViaTagsTests(torch._dynamo.test_case.TestCase):\n     )\n     def test_compile_selective_checkpoint_partial_ctx_fn(self):\n         def selective_checkpointing_context_fn(no_recompute_list):\n-            return _pt2_selective_checkpoint_context_fn_gen(\n+            return create_selective_checkpoint_contexts(\n                 _get_custom_policy(no_recompute_list=no_recompute_list)\n             )\n \n"
                },
                {
                    "old_start": 751,
                    "old_length": 7,
                    "new_start": 758,
                    "new_length": 7,
                    "hunk": "@@ -751,7 +758,7 @@ class ActivationCheckpointingViaTagsTests(torch._dynamo.test_case.TestCase):\n                 torch.ops.aten.mm.default,\n                 torch.ops.aten.sigmoid.default,\n             ]\n-            return _pt2_selective_checkpoint_context_fn_gen(\n+            return create_selective_checkpoint_contexts(\n                 _get_custom_policy(no_recompute_list=no_recompute_list),\n             )\n \n"
                },
                {
                    "old_start": 803,
                    "old_length": 7,
                    "new_start": 810,
                    "new_length": 7,
                    "hunk": "@@ -803,7 +810,7 @@ class ActivationCheckpointingViaTagsTests(torch._dynamo.test_case.TestCase):\n                 torch.ops.aten.mm.default,\n                 torch.ops.aten.sigmoid.default,\n             ]\n-            return _pt2_selective_checkpoint_context_fn_gen(\n+            return create_selective_checkpoint_contexts(\n                 _get_custom_policy(no_recompute_list=no_recompute_list)\n             )\n \n"
                },
                {
                    "old_start": 854,
                    "old_length": 7,
                    "new_start": 861,
                    "new_length": 7,
                    "hunk": "@@ -854,7 +861,7 @@ class ActivationCheckpointingViaTagsTests(torch._dynamo.test_case.TestCase):\n                 no_recompute_list = [\n                     torch.ops.aten.sigmoid.default,\n                 ]\n-                return _pt2_selective_checkpoint_context_fn_gen(\n+                return create_selective_checkpoint_contexts(\n                     _get_custom_policy(no_recompute_list=no_recompute_list)\n                 )\n \n"
                }
            ],
            "whole_deleted": "-from torch.utils.checkpoint import _pt2_selective_checkpoint_context_fn_gen, checkpoint\n-    def _custom_policy(mode, func, *args, **kwargs):\n-        return func in no_recompute_list\n-            return _pt2_selective_checkpoint_context_fn_gen(\n-            return _pt2_selective_checkpoint_context_fn_gen(\n-            return _pt2_selective_checkpoint_context_fn_gen(_get_custom_policy(meta))\n-            return _pt2_selective_checkpoint_context_fn_gen(\n-            return _pt2_selective_checkpoint_context_fn_gen(\n-            return _pt2_selective_checkpoint_context_fn_gen(\n-                return _pt2_selective_checkpoint_context_fn_gen(\n",
            "whole_added": "+from torch.utils.checkpoint import (\n+    checkpoint,\n+    CheckpointPolicy,\n+    create_selective_checkpoint_contexts,\n+)\n+    def _custom_policy(ctx, func, *args, **kwargs):\n+        if func in no_recompute_list:\n+            return CheckpointPolicy.MUST_SAVE\n+        else:\n+            return CheckpointPolicy.PREFER_RECOMPUTE\n+            return create_selective_checkpoint_contexts(\n+            return create_selective_checkpoint_contexts(\n+            return create_selective_checkpoint_contexts(_get_custom_policy(meta))\n+            return create_selective_checkpoint_contexts(\n+            return create_selective_checkpoint_contexts(\n+            return create_selective_checkpoint_contexts(\n+                return create_selective_checkpoint_contexts(\n",
            "whole_hunk": "@@ -19,7 +19,11 @@ from torch._higher_order_ops.wrap import tag_activation_checkpoint\n from torch.testing._internal.common_utils import IS_WINDOWS, skipIfRocm\n from torch.testing._internal.inductor_utils import HAS_CUDA\n from torch.testing._internal.two_tensor import TwoTensor\n-from torch.utils.checkpoint import _pt2_selective_checkpoint_context_fn_gen, checkpoint\n+from torch.utils.checkpoint import (\n+    checkpoint,\n+    CheckpointPolicy,\n+    create_selective_checkpoint_contexts,\n+)\n \n requires_cuda = unittest.skipUnless(HAS_CUDA, \"requires cuda\")\n requires_distributed = functools.partial(\n@@ -105,8 +109,11 @@ def op_count(gm):\n \n \n def _get_custom_policy(no_recompute_list=None):\n-    def _custom_policy(mode, func, *args, **kwargs):\n-        return func in no_recompute_list\n+    def _custom_policy(ctx, func, *args, **kwargs):\n+        if func in no_recompute_list:\n+            return CheckpointPolicy.MUST_SAVE\n+        else:\n+            return CheckpointPolicy.PREFER_RECOMPUTE\n \n     return _custom_policy\n \n@@ -530,7 +537,7 @@ class ActivationCheckpointingViaTagsTests(torch._dynamo.test_case.TestCase):\n             no_recompute_list = [\n                 torch.ops.aten.mm.default,\n             ]\n-            return _pt2_selective_checkpoint_context_fn_gen(\n+            return create_selective_checkpoint_contexts(\n                 _get_custom_policy(no_recompute_list=no_recompute_list)\n             )\n \n@@ -580,7 +587,7 @@ class ActivationCheckpointingViaTagsTests(torch._dynamo.test_case.TestCase):\n             no_recompute_list = [\n                 torch.ops.aten.mm.default,\n             ]\n-            return _pt2_selective_checkpoint_context_fn_gen(\n+            return create_selective_checkpoint_contexts(\n                 _get_custom_policy(no_recompute_list=no_recompute_list)\n             )\n \n@@ -650,7 +657,7 @@ class ActivationCheckpointingViaTagsTests(torch._dynamo.test_case.TestCase):\n \n         def selective_checkpointing_context_fn():\n             meta = {}\n-            return _pt2_selective_checkpoint_context_fn_gen(_get_custom_policy(meta))\n+            return create_selective_checkpoint_contexts(_get_custom_policy(meta))\n \n         def gn(x, y):\n             return torch.sigmoid(\n@@ -698,7 +705,7 @@ class ActivationCheckpointingViaTagsTests(torch._dynamo.test_case.TestCase):\n     )\n     def test_compile_selective_checkpoint_partial_ctx_fn(self):\n         def selective_checkpointing_context_fn(no_recompute_list):\n-            return _pt2_selective_checkpoint_context_fn_gen(\n+            return create_selective_checkpoint_contexts(\n                 _get_custom_policy(no_recompute_list=no_recompute_list)\n             )\n \n@@ -751,7 +758,7 @@ class ActivationCheckpointingViaTagsTests(torch._dynamo.test_case.TestCase):\n                 torch.ops.aten.mm.default,\n                 torch.ops.aten.sigmoid.default,\n             ]\n-            return _pt2_selective_checkpoint_context_fn_gen(\n+            return create_selective_checkpoint_contexts(\n                 _get_custom_policy(no_recompute_list=no_recompute_list),\n             )\n \n@@ -803,7 +810,7 @@ class ActivationCheckpointingViaTagsTests(torch._dynamo.test_case.TestCase):\n                 torch.ops.aten.mm.default,\n                 torch.ops.aten.sigmoid.default,\n             ]\n-            return _pt2_selective_checkpoint_context_fn_gen(\n+            return create_selective_checkpoint_contexts(\n                 _get_custom_policy(no_recompute_list=no_recompute_list)\n             )\n \n@@ -854,7 +861,7 @@ class ActivationCheckpointingViaTagsTests(torch._dynamo.test_case.TestCase):\n                 no_recompute_list = [\n                     torch.ops.aten.sigmoid.default,\n                 ]\n-                return _pt2_selective_checkpoint_context_fn_gen(\n+                return create_selective_checkpoint_contexts(\n                     _get_custom_policy(no_recompute_list=no_recompute_list)\n                 )\n \n"
        },
        {
            "name": "test_autograd.py",
            "path": "test/test_autograd.py",
            "patches": [
                {
                    "old_start": 2,
                    "old_length": 6,
                    "new_start": 2,
                    "new_length": 7,
                    "hunk": "@@ -2,6 +2,7 @@\n \n import collections\n import contextlib\n+import functools\n import gc\n import io\n import math\n"
                },
                {
                    "old_start": 79,
                    "old_length": 8,
                    "new_start": 80,
                    "new_length": 14,
                    "hunk": "@@ -79,8 +80,14 @@ from torch.testing._internal.common_utils import (\n )\n from torch.utils._mode_utils import no_dispatch\n from torch.utils._python_dispatch import TorchDispatchMode\n-from torch.utils.checkpoint import checkpoint, checkpoint_sequential\n+from torch.utils.checkpoint import (\n+    checkpoint,\n+    checkpoint_sequential,\n+    CheckpointPolicy,\n+    create_selective_checkpoint_contexts,\n+)\n from torch.utils.cpp_extension import load_inline\n+from torch.utils.flop_counter import FlopCounterMode\n from torch.utils.hooks import RemovableHandle  # noqa: TCH001\n \n \n"
                },
                {
                    "old_start": 13215,
                    "old_length": 6,
                    "new_start": 13222,
                    "new_length": 413,
                    "hunk": "@@ -13215,6 +13222,413 @@ class TestNestedCheckpoint(TestCase):\n         self.assertEqual(counter[0], 1)\n \n \n+class TestSelectiveActivationCheckpoint(TestCase):\n+    @unittest.skipIf(not TEST_CUDA, \"requires CUDA\")\n+    def test_flops_and_mem(self):\n+        # From https://github.com/pytorch/pytorch/pull/126320\n+        def get_act_mem(f):\n+            out = f()\n+            out.backward()\n+            # Why do one forward and backward?\n+            start_mem = torch.cuda.memory_stats()[\"requested_bytes.all.current\"]\n+            out = f()\n+            cur_mem = torch.cuda.memory_stats()[\"requested_bytes.all.current\"]\n+            act_mem = (cur_mem - start_mem) / (1024 * 1024)\n+            out.backward()\n+            return act_mem\n+\n+        def get_bw_flops(f):\n+            # Normalized so that a 512 square matmul returns 1\n+            f().backward()\n+            out = f()\n+            # NB: FlopCounterMode is pushed onto the mode stack before CachedMode, so\n+            # it will be able to observe whether an op is cached or not.\n+            with FlopCounterMode(display=False) as mode:\n+                out.backward()\n+            return mode.get_total_flops() / (512**3 * 2)\n+\n+        x = torch.randn(512, 512, requires_grad=True, device=\"cuda\")\n+        y = torch.randn(512, 512, requires_grad=True, device=\"cuda\")\n+\n+        def fn(x, y):\n+            return torch.mm(x.cos(), y).sin().sum()\n+\n+        def fn_ac(x, y):\n+            return checkpoint(fn, x, y, use_reentrant=False)\n+\n+        def fn_sac(x, y):\n+            context_fn = functools.partial(\n+                create_selective_checkpoint_contexts,\n+                [\n+                    torch.ops.aten.mm.default,\n+                ],\n+            )\n+            out = checkpoint(fn, x, y, use_reentrant=False, context_fn=context_fn)\n+            return out\n+\n+        def policy_fn(ctx, op, *args, **kwargs):\n+            if op == torch.ops.aten.mm.default:\n+                return CheckpointPolicy.MUST_SAVE\n+            else:\n+                return CheckpointPolicy.PREFER_RECOMPUTE\n+\n+        def fn_sac2(x, y):\n+            context_fn = functools.partial(\n+                create_selective_checkpoint_contexts,\n+                policy_fn,\n+            )\n+            out = checkpoint(fn, x, y, use_reentrant=False, context_fn=context_fn)\n+            return out\n+\n+        act_mem_noac = get_act_mem(lambda: fn(x, y))\n+        bw_flops_noac = get_bw_flops(lambda: fn(x, y))\n+\n+        self.assertEqual(act_mem_noac, 2.0)\n+        self.assertEqual(bw_flops_noac, 2.0)\n+\n+        act_mem_ac = get_act_mem(lambda: fn_ac(x, y))\n+        bw_flops_ac = get_bw_flops(lambda: fn_ac(x, y))\n+\n+        self.assertEqual(act_mem_ac, 0.0)\n+        self.assertEqual(bw_flops_ac, 3.0)\n+\n+        act_mem_sac = get_act_mem(lambda: fn_sac(x, y))\n+        bw_flops_sac = get_bw_flops(lambda: fn_sac(x, y))\n+\n+        self.assertEqual(act_mem_sac, 1.0)\n+        self.assertEqual(bw_flops_sac, 2.0)\n+\n+        act_mem_sac2 = get_act_mem(lambda: fn_sac2(x, y))\n+        bw_flops_sac2 = get_bw_flops(lambda: fn_sac2(x, y))\n+\n+        self.assertEqual(act_mem_sac2, 1.0)\n+        self.assertEqual(bw_flops_sac2, 2.0)\n+\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_output_already_has_autograd_meta(self):\n+        # View of tensor of non-differentiable dtype still has AutogradMeta\n+        def fn(x, y):\n+            return x.view(-1), y.sin().cos()\n+\n+        x = torch.tensor([1, 2, 3], dtype=torch.int64)\n+        y = torch.randn(3, requires_grad=True)\n+\n+        context_fn = functools.partial(\n+            create_selective_checkpoint_contexts,\n+            [\n+                torch.ops.aten.view.default,\n+            ],\n+        )\n+        out = checkpoint(fn, x, y, use_reentrant=False, context_fn=context_fn)\n+        out[1].sum().backward()\n+\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_subclass_dispatching_sizes(self):\n+        # Test that we ignore ops that grab metadata like torch.ops.aten.sym_size.default\n+        # Caching such metadata ops can be problematic when the following are satisfied:\n+        #\n+        # 1. size/strides are dispatched upon\n+        # 2. our policy saves sizes\n+        ta = torch.randn(6, 2)\n+\n+        class CustomSizeDynamicShapesTensor(torch.Tensor):\n+            @staticmethod\n+            def __new__(cls, inner):\n+                return torch.Tensor._make_wrapper_subclass(\n+                    # TODO: right now, _make_wrapper_subclass's dynamic shape interaction is not great.\n+                    # Calling the overload that has kwargs causes us to go down the first overload path,\n+                    # which will **always** specialize sizes.\n+                    # We should probably eventually fix this so that the first overload can just handle dynamic shapes.\n+                    cls,\n+                    inner.size(),\n+                    inner.stride(),\n+                    None,\n+                    None,\n+                    inner.dtype,\n+                    inner.layout,\n+                    inner.device,\n+                    False,\n+                    inner.requires_grad,\n+                    \"sizes\",\n+                )\n+\n+            def __init__(self, inner):\n+                self.inner = inner\n+\n+            @classmethod\n+            def __torch_dispatch__(cls, func, types, args, kwargs):\n+                if kwargs is None:\n+                    kwargs = {}\n+                args_inner = torch.utils._pytree.tree_map_only(\n+                    cls, lambda x: x.inner, args\n+                )\n+                out_inner = func(*args_inner, **kwargs)\n+                return torch.utils._pytree.tree_map_only(\n+                    torch.Tensor, lambda x: cls(x), out_inner\n+                )\n+\n+        def policy_fn(ctx, op, *args, **kwargs):\n+            if op is torch.ops.aten.sym_size.default:\n+                # Silently ignored!\n+                return CheckpointPolicy.MUST_SAVE\n+            else:\n+                return CheckpointPolicy.PREFER_RECOMPUTE\n+\n+        def fn(x):\n+            # We avoid the following case\n+            #\n+            # saved     :[4, 3], [], [], [4, 3], [4, 3], [4, 3], [12]\n+            # forward   :sum   ,sum,mul, mul   , mul   ,view   , view\n+            # recompute :sum   ,sum,mul, view  , view\n+            #\n+            # Views save the shape of their input, so we expect the second\n+            # view to save 12, but because during AC packing during forward\n+            # saves the shapes of the input for metadata checks later,\n+            # we would save the wrong shape during the recompute.\n+            view_out = (x * x.sum()).view(-1).view(4, 3)\n+            self.assertEqual(view_out.grad_fn._saved_self_sym_sizes, [12])\n+            return view_out.exp()\n+\n+        x = torch.randn(4, 3, requires_grad=True)\n+        x_wrapper = CustomSizeDynamicShapesTensor(x)\n+        context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n+        out = checkpoint(fn, x_wrapper, use_reentrant=False, context_fn=context_fn)\n+        out.sum().backward()\n+\n+    def test_bad_inputs(self):\n+        bad_op_list1 = [2]\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"Expected op in `op_list` to be an OpOverload\"\n+        ):\n+            create_selective_checkpoint_contexts(bad_op_list1)\n+\n+        bad_op_list2 = [torch.ops.aten.sin]\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"update the OpOverloadPacket to a specific OpOverload\"\n+        ):\n+            create_selective_checkpoint_contexts(bad_op_list2)\n+\n+        with self.assertRaisesRegex(TypeError, \"either a function or a list of ops.\"):\n+            create_selective_checkpoint_contexts(2)\n+\n+    # Dynamo fails for various reasons:\n+    # - some tests using custom op that does not implement Fake\n+    # - dynamo is trying to trace into saved variable hooks unpack hook for some reason\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_policy_with_state(self):\n+        # If I have a stateful callable, state is shared between the original\n+        # forward and the recompute.\n+        counters = []\n+\n+        class Policy:\n+            def __init__(self):\n+                self.counter = [0]\n+                self.recompute_counter = [0]\n+\n+            def __call__(self, ctx, func, *args, **kwargs):\n+                counter = self.recompute_counter if ctx.is_recompute else self.counter\n+                counter[0] += 1\n+                counters.append(counter[0])\n+                if counter == 1 and func is torch.ops.aten.mm.default:\n+                    return CheckpointPolicy.MUST_SAVE\n+                return CheckpointPolicy.PREFER_RECOMPUTE\n+\n+        def fn(x):\n+            return x.sin().sin().sin()\n+\n+        x = torch.randn(3, requires_grad=True)\n+        context_fn = functools.partial(\n+            create_selective_checkpoint_contexts,\n+            Policy(),\n+            allow_cache_entry_mutation=True,\n+        )\n+        out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+        out.sum().backward()\n+        # 1. counter properly reset to 0 for the recompute\n+        # 2. due to early-stop we do not recompute the final op\n+        self.assertEqual(counters, [1, 2, 3, 1, 2])\n+\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_storage_lifetime(self):\n+        from torch.utils._python_dispatch import _get_current_dispatch_mode\n+        from torch.utils.checkpoint import (\n+            _CachedTorchDispatchMode,\n+            _CachingTorchDispatchMode,\n+        )\n+\n+        def policy_fn(ctx, op, *args, **kwargs):\n+            return CheckpointPolicy.MUST_SAVE\n+\n+        ref = None\n+\n+        def fn(x):\n+            nonlocal ref\n+\n+            self.assertIsInstance(\n+                _get_current_dispatch_mode(),\n+                (_CachingTorchDispatchMode, _CachedTorchDispatchMode),\n+            )\n+\n+            out = x.cos().exp()\n+\n+            if isinstance(_get_current_dispatch_mode(), _CachingTorchDispatchMode):\n+                raw_val = (\n+                    _get_current_dispatch_mode()\n+                    .storage[torch.ops.aten.exp.default][0]\n+                    .val\n+                )\n+                # ref should've been detached\n+                # to avoid graph -> the saved variable hooks -> recompute_context -> storage -> graph\n+                self.assertFalse(raw_val.requires_grad)\n+                ref = weakref.ref(raw_val)\n+\n+            # Careful for early-stop\n+            return out.sin()\n+\n+        with disable_gc():\n+            # Case 1: If graph goes away without backward, make sure there's no reference cycle\n+            #         keeping storage alive.\n+            x = torch.randn(3, requires_grad=True)\n+            context_fn = functools.partial(\n+                create_selective_checkpoint_contexts, policy_fn\n+            )\n+            out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+            self.assertIsNotNone(ref())\n+            del out\n+            self.assertIsNone(ref())\n+\n+            # Case 2: After backward, even if retain_graph=True, the storage should go away\n+            x = torch.randn(3, requires_grad=True)\n+            context_fn = functools.partial(\n+                create_selective_checkpoint_contexts, policy_fn\n+            )\n+            out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+            self.assertIsNotNone(ref())\n+            out.sum().backward(retain_graph=True)\n+            # The dispatch mode's storage should still be alive, but the entries should've\n+            # been cleared.\n+            self.assertIsNone(ref())\n+\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_version_counter(self):\n+        def policy_fn(ctx, op, *args, **kwargs):\n+            if op == torch.ops.aten.sin.default:\n+                return CheckpointPolicy.MUST_SAVE\n+            else:\n+                return CheckpointPolicy.PREFER_RECOMPUTE\n+\n+        def fn(x):\n+            return x.sin().mul_(2).cos().exp()\n+\n+        x = torch.randn(3, requires_grad=True)\n+        context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n+        out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+\n+        # 1) Error because the output of sin is saved and mutated by mul_\n+        with self.assertRaisesRegex(RuntimeError, \"has been mutated\"):\n+            out.sum().backward()\n+\n+        x = torch.randn(3, requires_grad=True)\n+        context_fn = functools.partial(\n+            create_selective_checkpoint_contexts,\n+            policy_fn,\n+            allow_cache_entry_mutation=True,\n+        )\n+        out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+\n+        # 2) No longer should be an error because of allow_cache_entry_mutation\n+        out.sum().backward()\n+\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_function_with_more_than_one_output(self):\n+        # maybe there is a more systematic way:\n+        counter = [0]\n+\n+        def policy_fn(ctx, op, *args, **kwargs):\n+            if op == torch.ops.aten.var_mean.correction:\n+                counter[0] += 1\n+                return CheckpointPolicy.MUST_SAVE\n+            else:\n+                return CheckpointPolicy.PREFER_RECOMPUTE\n+\n+        # var_mean has two outputs\n+        def fn(x):\n+            a, b = torch.var_mean(x)\n+            return a * b\n+\n+        x = torch.randn(3, requires_grad=True)\n+        context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n+        out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+        x_grad = torch.autograd.grad(out.sum(), (x,))\n+        x_grad_ref = torch.autograd.grad(fn(x).sum(), (x,))\n+        self.assertEqual(x_grad, x_grad_ref)\n+        self.assertEqual(counter[0], 2)\n+\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_function_with_non_tensor_output(self):\n+        # When SAC is enabled, the op is not computed a second time\n+        with torch.library._scoped_library(\"mylib\", \"FRAGMENT\") as lib:\n+            counter = [0]\n+\n+            @torch.library.custom_op(\"mylib::sin_with_extra\", mutates_args=())\n+            def sin_with_extra(x: torch.Tensor) -> Tuple[torch.Tensor, int]:\n+                counter[0] += 1\n+                return x.sin(), 2\n+\n+            def setup_context(ctx, inputs, output) -> torch.Tensor:\n+                (x,) = inputs\n+                ctx.save_for_backward(x)\n+\n+            def backward(ctx, grad, _unused):\n+                (x,) = ctx.saved_tensors\n+                return grad * x.cos()\n+\n+            torch.library.register_autograd(\n+                \"mylib::sin_with_extra\", backward, setup_context=setup_context\n+            )\n+\n+            x = torch.randn(3, requires_grad=True)\n+\n+            def fn(x):\n+                return (torch.ops.mylib.sin_with_extra(x)[0] * x.sin().exp()).sin()\n+\n+            ops_list = [torch.ops.mylib.sin_with_extra.default]\n+\n+            x = torch.randn(3, requires_grad=True)\n+            context_fn = functools.partial(\n+                create_selective_checkpoint_contexts, ops_list\n+            )\n+            out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+            x_grad = torch.autograd.grad(out.sum(), (x,))\n+            self.assertEqual(counter[0], 1)\n+            x_grad_ref = torch.autograd.grad(fn(x).sum(), (x,))\n+            self.assertEqual(x_grad, x_grad_ref)\n+\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_can_only_trigger_recompute_once(self):\n+        # We don't support this to avoid adding extra complexity for now.\n+        # If there's a need, we could probably do some kind of use_count tracking.\n+        # TODO: have a nice error message here.\n+        def policy_fn(ctx, op, *args, **kwargs):\n+            if op == torch.ops.aten.sin.default:\n+                return CheckpointPolicy.MUST_SAVE\n+            else:\n+                return CheckpointPolicy.PREFER_RECOMPUTE\n+\n+        def fn(x):\n+            return x.sin().cos().exp()\n+\n+        x = torch.randn(3, requires_grad=True)\n+        context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n+        out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+        out.sum().backward(retain_graph=True)\n+\n+        with self.assertRaisesRegex(RuntimeError, \"Trying to backward an extra time\"):\n+            out.sum().backward(retain_graph=True)\n+\n+\n class TestAutogradMultipleDispatch(TestCase):\n     def test_autograd_multiple_dispatch_registrations(self, device):\n         t = torch.randn(3, 3, device=device, requires_grad=True)\n"
                }
            ],
            "whole_deleted": "-from torch.utils.checkpoint import checkpoint, checkpoint_sequential\n",
            "whole_added": "+import functools\n+from torch.utils.checkpoint import (\n+    checkpoint,\n+    checkpoint_sequential,\n+    CheckpointPolicy,\n+    create_selective_checkpoint_contexts,\n+)\n+from torch.utils.flop_counter import FlopCounterMode\n+class TestSelectiveActivationCheckpoint(TestCase):\n+    @unittest.skipIf(not TEST_CUDA, \"requires CUDA\")\n+    def test_flops_and_mem(self):\n+        # From https://github.com/pytorch/pytorch/pull/126320\n+        def get_act_mem(f):\n+            out = f()\n+            out.backward()\n+            # Why do one forward and backward?\n+            start_mem = torch.cuda.memory_stats()[\"requested_bytes.all.current\"]\n+            out = f()\n+            cur_mem = torch.cuda.memory_stats()[\"requested_bytes.all.current\"]\n+            act_mem = (cur_mem - start_mem) / (1024 * 1024)\n+            out.backward()\n+            return act_mem\n+\n+        def get_bw_flops(f):\n+            # Normalized so that a 512 square matmul returns 1\n+            f().backward()\n+            out = f()\n+            # NB: FlopCounterMode is pushed onto the mode stack before CachedMode, so\n+            # it will be able to observe whether an op is cached or not.\n+            with FlopCounterMode(display=False) as mode:\n+                out.backward()\n+            return mode.get_total_flops() / (512**3 * 2)\n+\n+        x = torch.randn(512, 512, requires_grad=True, device=\"cuda\")\n+        y = torch.randn(512, 512, requires_grad=True, device=\"cuda\")\n+\n+        def fn(x, y):\n+            return torch.mm(x.cos(), y).sin().sum()\n+\n+        def fn_ac(x, y):\n+            return checkpoint(fn, x, y, use_reentrant=False)\n+\n+        def fn_sac(x, y):\n+            context_fn = functools.partial(\n+                create_selective_checkpoint_contexts,\n+                [\n+                    torch.ops.aten.mm.default,\n+                ],\n+            )\n+            out = checkpoint(fn, x, y, use_reentrant=False, context_fn=context_fn)\n+            return out\n+\n+        def policy_fn(ctx, op, *args, **kwargs):\n+            if op == torch.ops.aten.mm.default:\n+                return CheckpointPolicy.MUST_SAVE\n+            else:\n+                return CheckpointPolicy.PREFER_RECOMPUTE\n+\n+        def fn_sac2(x, y):\n+            context_fn = functools.partial(\n+                create_selective_checkpoint_contexts,\n+                policy_fn,\n+            )\n+            out = checkpoint(fn, x, y, use_reentrant=False, context_fn=context_fn)\n+            return out\n+\n+        act_mem_noac = get_act_mem(lambda: fn(x, y))\n+        bw_flops_noac = get_bw_flops(lambda: fn(x, y))\n+\n+        self.assertEqual(act_mem_noac, 2.0)\n+        self.assertEqual(bw_flops_noac, 2.0)\n+\n+        act_mem_ac = get_act_mem(lambda: fn_ac(x, y))\n+        bw_flops_ac = get_bw_flops(lambda: fn_ac(x, y))\n+\n+        self.assertEqual(act_mem_ac, 0.0)\n+        self.assertEqual(bw_flops_ac, 3.0)\n+\n+        act_mem_sac = get_act_mem(lambda: fn_sac(x, y))\n+        bw_flops_sac = get_bw_flops(lambda: fn_sac(x, y))\n+\n+        self.assertEqual(act_mem_sac, 1.0)\n+        self.assertEqual(bw_flops_sac, 2.0)\n+\n+        act_mem_sac2 = get_act_mem(lambda: fn_sac2(x, y))\n+        bw_flops_sac2 = get_bw_flops(lambda: fn_sac2(x, y))\n+\n+        self.assertEqual(act_mem_sac2, 1.0)\n+        self.assertEqual(bw_flops_sac2, 2.0)\n+\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_output_already_has_autograd_meta(self):\n+        # View of tensor of non-differentiable dtype still has AutogradMeta\n+        def fn(x, y):\n+            return x.view(-1), y.sin().cos()\n+\n+        x = torch.tensor([1, 2, 3], dtype=torch.int64)\n+        y = torch.randn(3, requires_grad=True)\n+\n+        context_fn = functools.partial(\n+            create_selective_checkpoint_contexts,\n+            [\n+                torch.ops.aten.view.default,\n+            ],\n+        )\n+        out = checkpoint(fn, x, y, use_reentrant=False, context_fn=context_fn)\n+        out[1].sum().backward()\n+\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_subclass_dispatching_sizes(self):\n+        # Test that we ignore ops that grab metadata like torch.ops.aten.sym_size.default\n+        # Caching such metadata ops can be problematic when the following are satisfied:\n+        #\n+        # 1. size/strides are dispatched upon\n+        # 2. our policy saves sizes\n+        ta = torch.randn(6, 2)\n+\n+        class CustomSizeDynamicShapesTensor(torch.Tensor):\n+            @staticmethod\n+            def __new__(cls, inner):\n+                return torch.Tensor._make_wrapper_subclass(\n+                    # TODO: right now, _make_wrapper_subclass's dynamic shape interaction is not great.\n+                    # Calling the overload that has kwargs causes us to go down the first overload path,\n+                    # which will **always** specialize sizes.\n+                    # We should probably eventually fix this so that the first overload can just handle dynamic shapes.\n+                    cls,\n+                    inner.size(),\n+                    inner.stride(),\n+                    None,\n+                    None,\n+                    inner.dtype,\n+                    inner.layout,\n+                    inner.device,\n+                    False,\n+                    inner.requires_grad,\n+                    \"sizes\",\n+                )\n+\n+            def __init__(self, inner):\n+                self.inner = inner\n+\n+            @classmethod\n+            def __torch_dispatch__(cls, func, types, args, kwargs):\n+                if kwargs is None:\n+                    kwargs = {}\n+                args_inner = torch.utils._pytree.tree_map_only(\n+                    cls, lambda x: x.inner, args\n+                )\n+                out_inner = func(*args_inner, **kwargs)\n+                return torch.utils._pytree.tree_map_only(\n+                    torch.Tensor, lambda x: cls(x), out_inner\n+                )\n+\n+        def policy_fn(ctx, op, *args, **kwargs):\n+            if op is torch.ops.aten.sym_size.default:\n+                # Silently ignored!\n+                return CheckpointPolicy.MUST_SAVE\n+            else:\n+                return CheckpointPolicy.PREFER_RECOMPUTE\n+\n+        def fn(x):\n+            # We avoid the following case\n+            #\n+            # saved     :[4, 3], [], [], [4, 3], [4, 3], [4, 3], [12]\n+            # forward   :sum   ,sum,mul, mul   , mul   ,view   , view\n+            # recompute :sum   ,sum,mul, view  , view\n+            #\n+            # Views save the shape of their input, so we expect the second\n+            # view to save 12, but because during AC packing during forward\n+            # saves the shapes of the input for metadata checks later,\n+            # we would save the wrong shape during the recompute.\n+            view_out = (x * x.sum()).view(-1).view(4, 3)\n+            self.assertEqual(view_out.grad_fn._saved_self_sym_sizes, [12])\n+            return view_out.exp()\n+\n+        x = torch.randn(4, 3, requires_grad=True)\n+        x_wrapper = CustomSizeDynamicShapesTensor(x)\n+        context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n+        out = checkpoint(fn, x_wrapper, use_reentrant=False, context_fn=context_fn)\n+        out.sum().backward()\n+\n+    def test_bad_inputs(self):\n+        bad_op_list1 = [2]\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"Expected op in `op_list` to be an OpOverload\"\n+        ):\n+            create_selective_checkpoint_contexts(bad_op_list1)\n+\n+        bad_op_list2 = [torch.ops.aten.sin]\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"update the OpOverloadPacket to a specific OpOverload\"\n+        ):\n+            create_selective_checkpoint_contexts(bad_op_list2)\n+\n+        with self.assertRaisesRegex(TypeError, \"either a function or a list of ops.\"):\n+            create_selective_checkpoint_contexts(2)\n+\n+    # Dynamo fails for various reasons:\n+    # - some tests using custom op that does not implement Fake\n+    # - dynamo is trying to trace into saved variable hooks unpack hook for some reason\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_policy_with_state(self):\n+        # If I have a stateful callable, state is shared between the original\n+        # forward and the recompute.\n+        counters = []\n+\n+        class Policy:\n+            def __init__(self):\n+                self.counter = [0]\n+                self.recompute_counter = [0]\n+\n+            def __call__(self, ctx, func, *args, **kwargs):\n+                counter = self.recompute_counter if ctx.is_recompute else self.counter\n+                counter[0] += 1\n+                counters.append(counter[0])\n+                if counter == 1 and func is torch.ops.aten.mm.default:\n+                    return CheckpointPolicy.MUST_SAVE\n+                return CheckpointPolicy.PREFER_RECOMPUTE\n+\n+        def fn(x):\n+            return x.sin().sin().sin()\n+\n+        x = torch.randn(3, requires_grad=True)\n+        context_fn = functools.partial(\n+            create_selective_checkpoint_contexts,\n+            Policy(),\n+            allow_cache_entry_mutation=True,\n+        )\n+        out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+        out.sum().backward()\n+        # 1. counter properly reset to 0 for the recompute\n+        # 2. due to early-stop we do not recompute the final op\n+        self.assertEqual(counters, [1, 2, 3, 1, 2])\n+\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_storage_lifetime(self):\n+        from torch.utils._python_dispatch import _get_current_dispatch_mode\n+        from torch.utils.checkpoint import (\n+            _CachedTorchDispatchMode,\n+            _CachingTorchDispatchMode,\n+        )\n+\n+        def policy_fn(ctx, op, *args, **kwargs):\n+            return CheckpointPolicy.MUST_SAVE\n+\n+        ref = None\n+\n+        def fn(x):\n+            nonlocal ref\n+\n+            self.assertIsInstance(\n+                _get_current_dispatch_mode(),\n+                (_CachingTorchDispatchMode, _CachedTorchDispatchMode),\n+            )\n+\n+            out = x.cos().exp()\n+\n+            if isinstance(_get_current_dispatch_mode(), _CachingTorchDispatchMode):\n+                raw_val = (\n+                    _get_current_dispatch_mode()\n+                    .storage[torch.ops.aten.exp.default][0]\n+                    .val\n+                )\n+                # ref should've been detached\n+                # to avoid graph -> the saved variable hooks -> recompute_context -> storage -> graph\n+                self.assertFalse(raw_val.requires_grad)\n+                ref = weakref.ref(raw_val)\n+\n+            # Careful for early-stop\n+            return out.sin()\n+\n+        with disable_gc():\n+            # Case 1: If graph goes away without backward, make sure there's no reference cycle\n+            #         keeping storage alive.\n+            x = torch.randn(3, requires_grad=True)\n+            context_fn = functools.partial(\n+                create_selective_checkpoint_contexts, policy_fn\n+            )\n+            out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+            self.assertIsNotNone(ref())\n+            del out\n+            self.assertIsNone(ref())\n+\n+            # Case 2: After backward, even if retain_graph=True, the storage should go away\n+            x = torch.randn(3, requires_grad=True)\n+            context_fn = functools.partial(\n+                create_selective_checkpoint_contexts, policy_fn\n+            )\n+            out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+            self.assertIsNotNone(ref())\n+            out.sum().backward(retain_graph=True)\n+            # The dispatch mode's storage should still be alive, but the entries should've\n+            # been cleared.\n+            self.assertIsNone(ref())\n+\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_version_counter(self):\n+        def policy_fn(ctx, op, *args, **kwargs):\n+            if op == torch.ops.aten.sin.default:\n+                return CheckpointPolicy.MUST_SAVE\n+            else:\n+                return CheckpointPolicy.PREFER_RECOMPUTE\n+\n+        def fn(x):\n+            return x.sin().mul_(2).cos().exp()\n+\n+        x = torch.randn(3, requires_grad=True)\n+        context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n+        out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+\n+        # 1) Error because the output of sin is saved and mutated by mul_\n+        with self.assertRaisesRegex(RuntimeError, \"has been mutated\"):\n+            out.sum().backward()\n+\n+        x = torch.randn(3, requires_grad=True)\n+        context_fn = functools.partial(\n+            create_selective_checkpoint_contexts,\n+            policy_fn,\n+            allow_cache_entry_mutation=True,\n+        )\n+        out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+\n+        # 2) No longer should be an error because of allow_cache_entry_mutation\n+        out.sum().backward()\n+\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_function_with_more_than_one_output(self):\n+        # maybe there is a more systematic way:\n+        counter = [0]\n+\n+        def policy_fn(ctx, op, *args, **kwargs):\n+            if op == torch.ops.aten.var_mean.correction:\n+                counter[0] += 1\n+                return CheckpointPolicy.MUST_SAVE\n+            else:\n+                return CheckpointPolicy.PREFER_RECOMPUTE\n+\n+        # var_mean has two outputs\n+        def fn(x):\n+            a, b = torch.var_mean(x)\n+            return a * b\n+\n+        x = torch.randn(3, requires_grad=True)\n+        context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n+        out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+        x_grad = torch.autograd.grad(out.sum(), (x,))\n+        x_grad_ref = torch.autograd.grad(fn(x).sum(), (x,))\n+        self.assertEqual(x_grad, x_grad_ref)\n+        self.assertEqual(counter[0], 2)\n+\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_function_with_non_tensor_output(self):\n+        # When SAC is enabled, the op is not computed a second time\n+        with torch.library._scoped_library(\"mylib\", \"FRAGMENT\") as lib:\n+            counter = [0]\n+\n+            @torch.library.custom_op(\"mylib::sin_with_extra\", mutates_args=())\n+            def sin_with_extra(x: torch.Tensor) -> Tuple[torch.Tensor, int]:\n+                counter[0] += 1\n+                return x.sin(), 2\n+\n+            def setup_context(ctx, inputs, output) -> torch.Tensor:\n+                (x,) = inputs\n+                ctx.save_for_backward(x)\n+\n+            def backward(ctx, grad, _unused):\n+                (x,) = ctx.saved_tensors\n+                return grad * x.cos()\n+\n+            torch.library.register_autograd(\n+                \"mylib::sin_with_extra\", backward, setup_context=setup_context\n+            )\n+\n+            x = torch.randn(3, requires_grad=True)\n+\n+            def fn(x):\n+                return (torch.ops.mylib.sin_with_extra(x)[0] * x.sin().exp()).sin()\n+\n+            ops_list = [torch.ops.mylib.sin_with_extra.default]\n+\n+            x = torch.randn(3, requires_grad=True)\n+            context_fn = functools.partial(\n+                create_selective_checkpoint_contexts, ops_list\n+            )\n+            out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+            x_grad = torch.autograd.grad(out.sum(), (x,))\n+            self.assertEqual(counter[0], 1)\n+            x_grad_ref = torch.autograd.grad(fn(x).sum(), (x,))\n+            self.assertEqual(x_grad, x_grad_ref)\n+\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_can_only_trigger_recompute_once(self):\n+        # We don't support this to avoid adding extra complexity for now.\n+        # If there's a need, we could probably do some kind of use_count tracking.\n+        # TODO: have a nice error message here.\n+        def policy_fn(ctx, op, *args, **kwargs):\n+            if op == torch.ops.aten.sin.default:\n+                return CheckpointPolicy.MUST_SAVE\n+            else:\n+                return CheckpointPolicy.PREFER_RECOMPUTE\n+\n+        def fn(x):\n+            return x.sin().cos().exp()\n+\n+        x = torch.randn(3, requires_grad=True)\n+        context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n+        out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+        out.sum().backward(retain_graph=True)\n+\n+        with self.assertRaisesRegex(RuntimeError, \"Trying to backward an extra time\"):\n+            out.sum().backward(retain_graph=True)\n+\n+\n",
            "whole_hunk": "@@ -2,6 +2,7 @@\n \n import collections\n import contextlib\n+import functools\n import gc\n import io\n import math\n@@ -79,8 +80,14 @@ from torch.testing._internal.common_utils import (\n )\n from torch.utils._mode_utils import no_dispatch\n from torch.utils._python_dispatch import TorchDispatchMode\n-from torch.utils.checkpoint import checkpoint, checkpoint_sequential\n+from torch.utils.checkpoint import (\n+    checkpoint,\n+    checkpoint_sequential,\n+    CheckpointPolicy,\n+    create_selective_checkpoint_contexts,\n+)\n from torch.utils.cpp_extension import load_inline\n+from torch.utils.flop_counter import FlopCounterMode\n from torch.utils.hooks import RemovableHandle  # noqa: TCH001\n \n \n@@ -13215,6 +13222,413 @@ class TestNestedCheckpoint(TestCase):\n         self.assertEqual(counter[0], 1)\n \n \n+class TestSelectiveActivationCheckpoint(TestCase):\n+    @unittest.skipIf(not TEST_CUDA, \"requires CUDA\")\n+    def test_flops_and_mem(self):\n+        # From https://github.com/pytorch/pytorch/pull/126320\n+        def get_act_mem(f):\n+            out = f()\n+            out.backward()\n+            # Why do one forward and backward?\n+            start_mem = torch.cuda.memory_stats()[\"requested_bytes.all.current\"]\n+            out = f()\n+            cur_mem = torch.cuda.memory_stats()[\"requested_bytes.all.current\"]\n+            act_mem = (cur_mem - start_mem) / (1024 * 1024)\n+            out.backward()\n+            return act_mem\n+\n+        def get_bw_flops(f):\n+            # Normalized so that a 512 square matmul returns 1\n+            f().backward()\n+            out = f()\n+            # NB: FlopCounterMode is pushed onto the mode stack before CachedMode, so\n+            # it will be able to observe whether an op is cached or not.\n+            with FlopCounterMode(display=False) as mode:\n+                out.backward()\n+            return mode.get_total_flops() / (512**3 * 2)\n+\n+        x = torch.randn(512, 512, requires_grad=True, device=\"cuda\")\n+        y = torch.randn(512, 512, requires_grad=True, device=\"cuda\")\n+\n+        def fn(x, y):\n+            return torch.mm(x.cos(), y).sin().sum()\n+\n+        def fn_ac(x, y):\n+            return checkpoint(fn, x, y, use_reentrant=False)\n+\n+        def fn_sac(x, y):\n+            context_fn = functools.partial(\n+                create_selective_checkpoint_contexts,\n+                [\n+                    torch.ops.aten.mm.default,\n+                ],\n+            )\n+            out = checkpoint(fn, x, y, use_reentrant=False, context_fn=context_fn)\n+            return out\n+\n+        def policy_fn(ctx, op, *args, **kwargs):\n+            if op == torch.ops.aten.mm.default:\n+                return CheckpointPolicy.MUST_SAVE\n+            else:\n+                return CheckpointPolicy.PREFER_RECOMPUTE\n+\n+        def fn_sac2(x, y):\n+            context_fn = functools.partial(\n+                create_selective_checkpoint_contexts,\n+                policy_fn,\n+            )\n+            out = checkpoint(fn, x, y, use_reentrant=False, context_fn=context_fn)\n+            return out\n+\n+        act_mem_noac = get_act_mem(lambda: fn(x, y))\n+        bw_flops_noac = get_bw_flops(lambda: fn(x, y))\n+\n+        self.assertEqual(act_mem_noac, 2.0)\n+        self.assertEqual(bw_flops_noac, 2.0)\n+\n+        act_mem_ac = get_act_mem(lambda: fn_ac(x, y))\n+        bw_flops_ac = get_bw_flops(lambda: fn_ac(x, y))\n+\n+        self.assertEqual(act_mem_ac, 0.0)\n+        self.assertEqual(bw_flops_ac, 3.0)\n+\n+        act_mem_sac = get_act_mem(lambda: fn_sac(x, y))\n+        bw_flops_sac = get_bw_flops(lambda: fn_sac(x, y))\n+\n+        self.assertEqual(act_mem_sac, 1.0)\n+        self.assertEqual(bw_flops_sac, 2.0)\n+\n+        act_mem_sac2 = get_act_mem(lambda: fn_sac2(x, y))\n+        bw_flops_sac2 = get_bw_flops(lambda: fn_sac2(x, y))\n+\n+        self.assertEqual(act_mem_sac2, 1.0)\n+        self.assertEqual(bw_flops_sac2, 2.0)\n+\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_output_already_has_autograd_meta(self):\n+        # View of tensor of non-differentiable dtype still has AutogradMeta\n+        def fn(x, y):\n+            return x.view(-1), y.sin().cos()\n+\n+        x = torch.tensor([1, 2, 3], dtype=torch.int64)\n+        y = torch.randn(3, requires_grad=True)\n+\n+        context_fn = functools.partial(\n+            create_selective_checkpoint_contexts,\n+            [\n+                torch.ops.aten.view.default,\n+            ],\n+        )\n+        out = checkpoint(fn, x, y, use_reentrant=False, context_fn=context_fn)\n+        out[1].sum().backward()\n+\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_subclass_dispatching_sizes(self):\n+        # Test that we ignore ops that grab metadata like torch.ops.aten.sym_size.default\n+        # Caching such metadata ops can be problematic when the following are satisfied:\n+        #\n+        # 1. size/strides are dispatched upon\n+        # 2. our policy saves sizes\n+        ta = torch.randn(6, 2)\n+\n+        class CustomSizeDynamicShapesTensor(torch.Tensor):\n+            @staticmethod\n+            def __new__(cls, inner):\n+                return torch.Tensor._make_wrapper_subclass(\n+                    # TODO: right now, _make_wrapper_subclass's dynamic shape interaction is not great.\n+                    # Calling the overload that has kwargs causes us to go down the first overload path,\n+                    # which will **always** specialize sizes.\n+                    # We should probably eventually fix this so that the first overload can just handle dynamic shapes.\n+                    cls,\n+                    inner.size(),\n+                    inner.stride(),\n+                    None,\n+                    None,\n+                    inner.dtype,\n+                    inner.layout,\n+                    inner.device,\n+                    False,\n+                    inner.requires_grad,\n+                    \"sizes\",\n+                )\n+\n+            def __init__(self, inner):\n+                self.inner = inner\n+\n+            @classmethod\n+            def __torch_dispatch__(cls, func, types, args, kwargs):\n+                if kwargs is None:\n+                    kwargs = {}\n+                args_inner = torch.utils._pytree.tree_map_only(\n+                    cls, lambda x: x.inner, args\n+                )\n+                out_inner = func(*args_inner, **kwargs)\n+                return torch.utils._pytree.tree_map_only(\n+                    torch.Tensor, lambda x: cls(x), out_inner\n+                )\n+\n+        def policy_fn(ctx, op, *args, **kwargs):\n+            if op is torch.ops.aten.sym_size.default:\n+                # Silently ignored!\n+                return CheckpointPolicy.MUST_SAVE\n+            else:\n+                return CheckpointPolicy.PREFER_RECOMPUTE\n+\n+        def fn(x):\n+            # We avoid the following case\n+            #\n+            # saved     :[4, 3], [], [], [4, 3], [4, 3], [4, 3], [12]\n+            # forward   :sum   ,sum,mul, mul   , mul   ,view   , view\n+            # recompute :sum   ,sum,mul, view  , view\n+            #\n+            # Views save the shape of their input, so we expect the second\n+            # view to save 12, but because during AC packing during forward\n+            # saves the shapes of the input for metadata checks later,\n+            # we would save the wrong shape during the recompute.\n+            view_out = (x * x.sum()).view(-1).view(4, 3)\n+            self.assertEqual(view_out.grad_fn._saved_self_sym_sizes, [12])\n+            return view_out.exp()\n+\n+        x = torch.randn(4, 3, requires_grad=True)\n+        x_wrapper = CustomSizeDynamicShapesTensor(x)\n+        context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n+        out = checkpoint(fn, x_wrapper, use_reentrant=False, context_fn=context_fn)\n+        out.sum().backward()\n+\n+    def test_bad_inputs(self):\n+        bad_op_list1 = [2]\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"Expected op in `op_list` to be an OpOverload\"\n+        ):\n+            create_selective_checkpoint_contexts(bad_op_list1)\n+\n+        bad_op_list2 = [torch.ops.aten.sin]\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"update the OpOverloadPacket to a specific OpOverload\"\n+        ):\n+            create_selective_checkpoint_contexts(bad_op_list2)\n+\n+        with self.assertRaisesRegex(TypeError, \"either a function or a list of ops.\"):\n+            create_selective_checkpoint_contexts(2)\n+\n+    # Dynamo fails for various reasons:\n+    # - some tests using custom op that does not implement Fake\n+    # - dynamo is trying to trace into saved variable hooks unpack hook for some reason\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_policy_with_state(self):\n+        # If I have a stateful callable, state is shared between the original\n+        # forward and the recompute.\n+        counters = []\n+\n+        class Policy:\n+            def __init__(self):\n+                self.counter = [0]\n+                self.recompute_counter = [0]\n+\n+            def __call__(self, ctx, func, *args, **kwargs):\n+                counter = self.recompute_counter if ctx.is_recompute else self.counter\n+                counter[0] += 1\n+                counters.append(counter[0])\n+                if counter == 1 and func is torch.ops.aten.mm.default:\n+                    return CheckpointPolicy.MUST_SAVE\n+                return CheckpointPolicy.PREFER_RECOMPUTE\n+\n+        def fn(x):\n+            return x.sin().sin().sin()\n+\n+        x = torch.randn(3, requires_grad=True)\n+        context_fn = functools.partial(\n+            create_selective_checkpoint_contexts,\n+            Policy(),\n+            allow_cache_entry_mutation=True,\n+        )\n+        out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+        out.sum().backward()\n+        # 1. counter properly reset to 0 for the recompute\n+        # 2. due to early-stop we do not recompute the final op\n+        self.assertEqual(counters, [1, 2, 3, 1, 2])\n+\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_storage_lifetime(self):\n+        from torch.utils._python_dispatch import _get_current_dispatch_mode\n+        from torch.utils.checkpoint import (\n+            _CachedTorchDispatchMode,\n+            _CachingTorchDispatchMode,\n+        )\n+\n+        def policy_fn(ctx, op, *args, **kwargs):\n+            return CheckpointPolicy.MUST_SAVE\n+\n+        ref = None\n+\n+        def fn(x):\n+            nonlocal ref\n+\n+            self.assertIsInstance(\n+                _get_current_dispatch_mode(),\n+                (_CachingTorchDispatchMode, _CachedTorchDispatchMode),\n+            )\n+\n+            out = x.cos().exp()\n+\n+            if isinstance(_get_current_dispatch_mode(), _CachingTorchDispatchMode):\n+                raw_val = (\n+                    _get_current_dispatch_mode()\n+                    .storage[torch.ops.aten.exp.default][0]\n+                    .val\n+                )\n+                # ref should've been detached\n+                # to avoid graph -> the saved variable hooks -> recompute_context -> storage -> graph\n+                self.assertFalse(raw_val.requires_grad)\n+                ref = weakref.ref(raw_val)\n+\n+            # Careful for early-stop\n+            return out.sin()\n+\n+        with disable_gc():\n+            # Case 1: If graph goes away without backward, make sure there's no reference cycle\n+            #         keeping storage alive.\n+            x = torch.randn(3, requires_grad=True)\n+            context_fn = functools.partial(\n+                create_selective_checkpoint_contexts, policy_fn\n+            )\n+            out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+            self.assertIsNotNone(ref())\n+            del out\n+            self.assertIsNone(ref())\n+\n+            # Case 2: After backward, even if retain_graph=True, the storage should go away\n+            x = torch.randn(3, requires_grad=True)\n+            context_fn = functools.partial(\n+                create_selective_checkpoint_contexts, policy_fn\n+            )\n+            out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+            self.assertIsNotNone(ref())\n+            out.sum().backward(retain_graph=True)\n+            # The dispatch mode's storage should still be alive, but the entries should've\n+            # been cleared.\n+            self.assertIsNone(ref())\n+\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_version_counter(self):\n+        def policy_fn(ctx, op, *args, **kwargs):\n+            if op == torch.ops.aten.sin.default:\n+                return CheckpointPolicy.MUST_SAVE\n+            else:\n+                return CheckpointPolicy.PREFER_RECOMPUTE\n+\n+        def fn(x):\n+            return x.sin().mul_(2).cos().exp()\n+\n+        x = torch.randn(3, requires_grad=True)\n+        context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n+        out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+\n+        # 1) Error because the output of sin is saved and mutated by mul_\n+        with self.assertRaisesRegex(RuntimeError, \"has been mutated\"):\n+            out.sum().backward()\n+\n+        x = torch.randn(3, requires_grad=True)\n+        context_fn = functools.partial(\n+            create_selective_checkpoint_contexts,\n+            policy_fn,\n+            allow_cache_entry_mutation=True,\n+        )\n+        out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+\n+        # 2) No longer should be an error because of allow_cache_entry_mutation\n+        out.sum().backward()\n+\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_function_with_more_than_one_output(self):\n+        # maybe there is a more systematic way:\n+        counter = [0]\n+\n+        def policy_fn(ctx, op, *args, **kwargs):\n+            if op == torch.ops.aten.var_mean.correction:\n+                counter[0] += 1\n+                return CheckpointPolicy.MUST_SAVE\n+            else:\n+                return CheckpointPolicy.PREFER_RECOMPUTE\n+\n+        # var_mean has two outputs\n+        def fn(x):\n+            a, b = torch.var_mean(x)\n+            return a * b\n+\n+        x = torch.randn(3, requires_grad=True)\n+        context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n+        out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+        x_grad = torch.autograd.grad(out.sum(), (x,))\n+        x_grad_ref = torch.autograd.grad(fn(x).sum(), (x,))\n+        self.assertEqual(x_grad, x_grad_ref)\n+        self.assertEqual(counter[0], 2)\n+\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_function_with_non_tensor_output(self):\n+        # When SAC is enabled, the op is not computed a second time\n+        with torch.library._scoped_library(\"mylib\", \"FRAGMENT\") as lib:\n+            counter = [0]\n+\n+            @torch.library.custom_op(\"mylib::sin_with_extra\", mutates_args=())\n+            def sin_with_extra(x: torch.Tensor) -> Tuple[torch.Tensor, int]:\n+                counter[0] += 1\n+                return x.sin(), 2\n+\n+            def setup_context(ctx, inputs, output) -> torch.Tensor:\n+                (x,) = inputs\n+                ctx.save_for_backward(x)\n+\n+            def backward(ctx, grad, _unused):\n+                (x,) = ctx.saved_tensors\n+                return grad * x.cos()\n+\n+            torch.library.register_autograd(\n+                \"mylib::sin_with_extra\", backward, setup_context=setup_context\n+            )\n+\n+            x = torch.randn(3, requires_grad=True)\n+\n+            def fn(x):\n+                return (torch.ops.mylib.sin_with_extra(x)[0] * x.sin().exp()).sin()\n+\n+            ops_list = [torch.ops.mylib.sin_with_extra.default]\n+\n+            x = torch.randn(3, requires_grad=True)\n+            context_fn = functools.partial(\n+                create_selective_checkpoint_contexts, ops_list\n+            )\n+            out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+            x_grad = torch.autograd.grad(out.sum(), (x,))\n+            self.assertEqual(counter[0], 1)\n+            x_grad_ref = torch.autograd.grad(fn(x).sum(), (x,))\n+            self.assertEqual(x_grad, x_grad_ref)\n+\n+    @skipIfTorchDynamo(\"compile tested in test/dynamo/test_activation_checkpointing.py\")\n+    def test_can_only_trigger_recompute_once(self):\n+        # We don't support this to avoid adding extra complexity for now.\n+        # If there's a need, we could probably do some kind of use_count tracking.\n+        # TODO: have a nice error message here.\n+        def policy_fn(ctx, op, *args, **kwargs):\n+            if op == torch.ops.aten.sin.default:\n+                return CheckpointPolicy.MUST_SAVE\n+            else:\n+                return CheckpointPolicy.PREFER_RECOMPUTE\n+\n+        def fn(x):\n+            return x.sin().cos().exp()\n+\n+        x = torch.randn(3, requires_grad=True)\n+        context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n+        out = checkpoint(fn, x, use_reentrant=False, context_fn=context_fn)\n+        out.sum().backward(retain_graph=True)\n+\n+        with self.assertRaisesRegex(RuntimeError, \"Trying to backward an extra time\"):\n+            out.sum().backward(retain_graph=True)\n+\n+\n class TestAutogradMultipleDispatch(TestCase):\n     def test_autograd_multiple_dispatch_registrations(self, device):\n         t = torch.randn(3, 3, device=device, requires_grad=True)\n"
        },
        {
            "name": "wrap.py",
            "path": "torch/_higher_order_ops/wrap.py",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 15,
                    "new_start": 1,
                    "new_length": 17,
                    "hunk": "@@ -1,15 +1,17 @@\n # mypy: allow-untyped-defs\n import inspect\n+import itertools\n import logging\n \n import torch\n from torch._ops import HigherOrderOperator\n-from torch.utils.checkpoint import checkpoint, uid\n+from torch.utils.checkpoint import checkpoint\n+\n import torch._dynamo.config\n \n log = logging.getLogger(__name__)\n \n-\n+uid = itertools.count(1)\n \n # Used for testing the HigherOrderOperator mechanism\n class Wrap(HigherOrderOperator):\n"
                }
            ],
            "whole_deleted": "-from torch.utils.checkpoint import checkpoint, uid\n-\n",
            "whole_added": "+import itertools\n+from torch.utils.checkpoint import checkpoint\n+\n+uid = itertools.count(1)\n",
            "whole_hunk": "@@ -1,15 +1,17 @@\n # mypy: allow-untyped-defs\n import inspect\n+import itertools\n import logging\n \n import torch\n from torch._ops import HigherOrderOperator\n-from torch.utils.checkpoint import checkpoint, uid\n+from torch.utils.checkpoint import checkpoint\n+\n import torch._dynamo.config\n \n log = logging.getLogger(__name__)\n \n-\n+uid = itertools.count(1)\n \n # Used for testing the HigherOrderOperator mechanism\n class Wrap(HigherOrderOperator):\n"
        },
        {
            "name": "checkpoint.py",
            "path": "torch/utils/checkpoint.py",
            "patches": [
                {
                    "old_start": 5,
                    "old_length": 18,
                    "new_start": 5,
                    "new_length": 8,
                    "hunk": "@@ -5,18 +5,8 @@ import uuid\n import warnings\n import weakref\n from collections import defaultdict\n-from itertools import count\n-from typing import (\n-    Any,\n-    Callable,\n-    ContextManager,\n-    DefaultDict,\n-    Dict,\n-    Iterable,\n-    List,\n-    Optional,\n-    Tuple,\n-)\n+from typing import *  # noqa: F403\n+import enum\n from weakref import ReferenceType\n \n import torch\n"
                },
                {
                    "old_start": 39,
                    "old_length": 6,
                    "new_start": 29,
                    "new_length": 10,
                    "hunk": "@@ -39,6 +29,10 @@ __all__ = [\n     \"set_checkpoint_early_stop\",\n     \"DefaultDeviceType\",\n     \"set_checkpoint_debug_enabled\",\n+    \"CheckpointPolicy\",\n+    \"SelectiveCheckpointContext\",\n+    \"create_selective_checkpoint_contexts\",\n+    \"SAC_IGNORED_OPS\",\n ]\n \n _DEFAULT_DETERMINISM_MODE = \"default\"\n"
                },
                {
                    "old_start": 1153,
                    "old_length": 149,
                    "new_start": 1147,
                    "new_length": 247,
                    "hunk": "@@ -1153,149 +1147,247 @@ def _is_compiling(func, args, kwargs):\n     return False\n \n \n-def _detach(x):\n-    if isinstance(x, torch.Tensor):\n-        return x.detach()\n+class _VersionWrapper:\n+    # Check that cached tensors are not mutated.\n+    def __init__(self, val):\n+        self.val: Union[torch.Tensor, Any] = val\n+        self.version: Optional[int] = val._version if isinstance(val, torch.Tensor) else None\n+\n+    def get_val(self, allow_cache_entry_mutation):\n+        if self.version is not None and not allow_cache_entry_mutation:\n+            if self.val._version != self.version:\n+                # Can we give user a stack trace of where the mutation happened?\n+                raise RuntimeError(\n+                    \"Tensor cached during selective activation checkpoint has been mutated\"\n+                )\n+        return self.val\n+\n+\n+def _maybe_detach(x, any_ret_has_alias_info):\n+    # We detach for two separate reasons:\n+    # - For view ops, we need to ensure that when the tensor is returned from\n+    #   CachedDispatchMode, as_view sees that the AutogradMeta is nullptr\n+    # - Avoid reference cycles\n+    # For case 1, it is not enough to check whether x has differentiable dtype\n+    # because non-differentiable dtype can have non-nullptr AutogradMeta, e.g.\n+    # when the tensor is a view.\n+    if isinstance(x, torch.Tensor) and (x.is_floating_point() or x.is_complex() or any_ret_has_alias_info):\n+        with torch._C._SetExcludeDispatchKeyGuard(torch._C.DispatchKey.ADInplaceOrView, False):\n+            # Ensure that view performed beneath autograd properly propagates\n+            # version counter. TODO: Use reentrant_dispatch instead of\n+            # manually manipulating dispatch keys. Using reentrant_dispatch\n+            # would respect inference_mode, though that is not relevant for\n+            # this case.\n+            x = x.detach()\n     return x\n \n \n-uid = count(1)\n+class SelectiveCheckpointContext:\n+    \"\"\"\n+    Context passed to policy function during selective checkpointing.\n \n+    This class is used to pass relevant metadata to the policy function during\n+    selective checkpointing. The metadata includes whether the current invocation\n+    of the policy function is during recomputation or not.\n \n-# NOTE: torch.utils.checkpoint internal logic will call these two functions unknown number of times\n-# (i.e. there could be _CachedTorchDispatchMode calls that doesn't map to a _CachingTorchDispatchMode call),\n-# so we ignore these ops and just always recompute them.\n-_ignored_ops = {\n-    torch.ops.prim.device.default,\n+    Example:\n+        >>> # xdoctest: +SKIP(stub)\n+        >>>\n+        >>> def policy_fn(ctx, op, *args, **kwargs):\n+        >>>    print(ctx.is_recompute)\n+        >>>\n+        >>> context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n+        >>>\n+        >>> out = torch.utils.checkpoint.checkpoint(\n+        >>>     fn, x, y,\n+        >>>     use_reentrant=False,\n+        >>>     context_fn=context_fn,\n+        >>> )\n+    \"\"\"\n+    def __init__(self, *, is_recompute):\n+        self.is_recompute = is_recompute\n+\n+\n+class CheckpointPolicy(enum.Enum):\n+    \"\"\"\n+    Enum for specifying the policy for checkpointing during backpropagation.\n+\n+    The following policies are supported:\n+\n+    - ``{MUST,PREFER}_SAVE``: The operation's output will be saved during the forward\n+      pass and will not be recomputed during the backward pass\n+    - ``{MUST,PREFER}_RECOMPUTE``: The operation's output will not be saved during the\n+      forward pass and will be recomputed during the backward pass\n+\n+    Use ``MUST_*`` over ``PREFER_*`` to indicate that the policy should not be overridden\n+    by other subsystems like `torch.compile`.\n+\n+    .. note::\n+        A policy function that always returns ``PREFER_RECOMPUTE`` is\n+        equivalent to vanilla checkpointing.\n+\n+        A policy function that returns ``PREFER_SAVE`` every op is\n+        NOT equivalent to not using checkpointing. Using such a policy would\n+        save additional tensors not limited to ones that are actually needed for\n+        gradient computation.\n+    \"\"\"\n+    MUST_SAVE = 0\n+    PREFER_SAVE = 1\n+    MUST_RECOMPUTE = 2\n+    PREFER_RECOMPUTE = 3\n+\n+\n+SAC_IGNORED_OPS = {\n+    # AC inserts different number of detach during forward and recompute.\n     torch.ops.aten.detach.default,\n+    # AC's determinism check invokes additional metadata ops during forward.\n+    # With subclasses involved, these metadata ops become dispatchable, this\n+    # can result in incorrectness if these ops are selected cached.\n+    torch.ops.prim.device.default,\n } | set(torch._subclasses.functional_tensor.FunctionalTensor.metadata_fns)\n \n \n class _CachingTorchDispatchMode(TorchDispatchMode):\n-    r\"\"\"\n-    A :class:`TorchDispatchMode` to implement selective activation checkpointing\n-    that's compatible with torch.compile. Used together with _CachedTorchDispatchMode.\n-    \"\"\"\n+    # Used together with _CachedTorchDispatchMode to implement SAC.\n     def __init__(self, policy_fn, storage):\n         self.policy_fn = policy_fn\n         self.storage = storage\n \n-    def push_into_storage(self, out, func, args, kwargs):\n-        out_detached = tree_map(_detach, out)\n-        self.storage[func].append(out_detached)\n+    def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n+        if func in SAC_IGNORED_OPS:\n+            return func(*args, **kwargs)\n \n-    def _handle_compile_in_forward_ctx(self, should_not_recompute, func, args, kwargs):\n-        if should_not_recompute:\n+        kwargs = {} if kwargs is None else kwargs\n+        policy = self.policy_fn(SelectiveCheckpointContext(is_recompute=False),\n+                                func, *args, **kwargs)\n+        is_compiling = _is_compiling(func, args, kwargs)\n+\n+        if is_compiling and policy == CheckpointPolicy.MUST_SAVE:\n             fx_traceback.current_meta[\"recompute\"] = 0\n-        # NOTE: Here we just store and reuse output of all ops, since in torch.compile mode\n-        # we decide and handle recomputation in the partitioner.\n+\n         out = func(*args, **kwargs)\n-        self.push_into_storage(out, func, args, kwargs)\n-        return out\n \n-    def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n-        if kwargs is None:\n-            kwargs = {}\n-        if func in _ignored_ops:\n-            return func(*args, **kwargs)\n-        should_not_recompute = self.policy_fn(\"forward\", func, *args, **kwargs)\n-        if _is_compiling(func, args, kwargs):\n-            return self._handle_compile_in_forward_ctx(should_not_recompute, func, args, kwargs)\n-        else:\n-            if should_not_recompute:\n-                out = func(*args, **kwargs)\n-                self.push_into_storage(out, func, args, kwargs)\n-            else:\n-                out = func(*args, **kwargs)\n-            return out\n+        any_ret_has_alias_info = any(ret.alias_info is not None for ret in func._schema.returns)\n+\n+        if policy in (CheckpointPolicy.MUST_SAVE, CheckpointPolicy.PREFER_SAVE) or is_compiling:\n+            self.storage[func].append(tree_map(lambda x: _VersionWrapper(_maybe_detach(x, any_ret_has_alias_info)), out))\n+        return out\n \n class _CachedTorchDispatchMode(TorchDispatchMode):\n-    r\"\"\"\n-    A :class:`TorchDispatchMode` to implement selective activation checkpointing\n-    that's compatible with torch.compile. Used together with _CachingTorchDispatchMode.\n-    \"\"\"\n-    def __init__(self, policy_fn, storage):\n+    # Used together with _CachedTorchDispatchMode to implement SAC.\n+    def __init__(self, policy_fn, storage, allow_cache_entry_mutation):\n         self.policy_fn = policy_fn\n         self.storage = storage\n-\n-    def pop_from_storage(self, func, args, kwargs):\n-        assert func in self.storage\n-        out = self.storage[func].pop(0)\n-        return out\n-\n-    def _handle_compile_in_recompute_ctx(self, should_not_recompute, func, args, kwargs):\n-        out = self.pop_from_storage(func, args, kwargs)\n-        return out\n+        self.allow_cache_entry_mutation = allow_cache_entry_mutation\n \n     def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n-        if kwargs is None:\n-            kwargs = {}\n-        if func in _ignored_ops:\n+        if func in SAC_IGNORED_OPS:\n             return func(*args, **kwargs)\n-        should_not_recompute = self.policy_fn(\"recompute\", func, *args, **kwargs)\n-        if _is_compiling(func, args, kwargs):\n-            return self._handle_compile_in_recompute_ctx(should_not_recompute, func, args, kwargs)\n+\n+        kwargs = {} if kwargs is None else kwargs\n+        policy = self.policy_fn(SelectiveCheckpointContext(is_recompute=True),\n+                                func, *args, **kwargs)\n+        is_compiling = _is_compiling(func, args, kwargs)\n+\n+        if policy in (CheckpointPolicy.MUST_SAVE, CheckpointPolicy.PREFER_SAVE) or is_compiling:\n+            storage = self.storage.get(func)\n+            if storage is None:\n+                raise RuntimeError(f\"{func} encountered during backward, but not found in storage\")\n+            if len(storage) == 0:\n+                raise RuntimeError(\n+                    \"Trying to backward an extra time. You are only allowed to backward once \"\n+                    \"on any region computed under selective activation checkpoint.\"\n+                )\n+            out = tree_map(lambda x: x.get_val(self.allow_cache_entry_mutation), storage.pop(0))\n         else:\n-            if should_not_recompute:\n-                out = self.pop_from_storage(func, args, kwargs)\n-            else:\n-                out = func(*args, **kwargs)\n-            return out\n+            out = func(*args, **kwargs)\n+        return out\n \n-def _pt2_selective_checkpoint_context_fn_gen(policy_fn):\n+\n+def create_selective_checkpoint_contexts(policy_fn_or_list, allow_cache_entry_mutation=False):\n     \"\"\"\n-    A helper function that generates a pair of contexts to be later passed into\n-    `torch.utils.checkpoint` API to implment selective checkpointing.\n+    Helper to avoid recomputing certain ops during activation checkpointing.\n \n-    .. warning::\n-        This is context_fn is intended for use with torch.compile only.\n+    Use this with `torch.utils.checkpoint.checkpoint` to control which\n+    operations are recomputed during the backward pass.\n \n     Args:\n-        policy_fn (Callable[[Callable, List[Any], Dict[str, Any]], bool]): Policy function\n-            to decide whether a particular op should be recomputed in backward pass or not.\n-            In eager mode:\n-                If policy_fn(...) returns True, the op is guaranteed to NOT be recomputed.\n-                If policy_fn(...) returns False, the op is guaranteed to be recomputed.\n-            In torch.compile mode:\n-                If policy_fn(...) returns True, the op is guaranteed to NOT be recomputed.\n-                If policy_fn(...) returns False, the op may or may not be recomputed\n-                (it's up to the partitioner to decide).\n-\n+        policy_fn_or_list (Callable or List):\n+          - If a policy function is provided, it should accept a\n+            :class:`SelectiveCheckpointContext`, the :class:`OpOverload`, args and\n+            kwargs to the op, and return a :class:`CheckpointPolicy` enum value\n+            indicating whether the execution of the op should be recomputed or not.\n+          - If a list of operations is provided, it is equivalent to a policy\n+            returning `CheckpointPolicy.MUST_SAVE` for the specified\n+            operations and `CheckpointPolicy.PREFER_RECOMPUTE` for all other\n+            operations.\n+        allow_cache_entry_mutation (bool, optional): By default, an error is\n+            raised if any tensors cached by selective activation checkpoint are\n+            mutated in order to ensure correctness. If set to `True`, this check\n+            is disabled.\n     Returns:\n-        A pair of generated contexts.\n+        A tuple of two context managers.\n \n     Example:\n         >>> # xdoctest: +REQUIRES(LINUX)\n+        >>> import functools\n         >>>\n-        >>> def get_custom_policy():\n-        >>>     no_recompute_list = [\n-        >>>         torch.ops.aten.mm.default,\n-        >>>     ]\n-        >>>     def custom_policy(mode, func, *args, **kwargs):\n-        >>>         return func in no_recompute_list\n-        >>>     return custom_policy\n+        >>> x = torch.rand(10, 10, requires_grad=True)\n+        >>> y = torch.rand(10, 10, requires_grad=True)\n         >>>\n-        >>> def selective_checkpointing_context_fn():\n-        >>>     return _pt2_selective_checkpoint_context_fn_gen(get_custom_policy())\n+        >>> ops_to_save = [\n+        >>>    torch.ops.aten.mm.default,\n+        >>> ]\n         >>>\n-        >>> def gn(x, y):\n-        >>>     return torch.sigmoid(torch.matmul(torch.matmul(x, y), y)) * y\n+        >>> def policy_fn(ctx, op, *args, **kwargs):\n+        >>>    if op in ops_to_save:\n+        >>>        return CheckpointPolicy.MUST_SAVE\n+        >>>    else:\n+        >>>        return CheckpointPolicy.PREFER_RECOMPUTE\n         >>>\n-        >>> def fn(x, y):\n-        >>>     return torch.utils.checkpoint.checkpoint(\n-        >>>         gn, x, y,\n-        >>>         use_reentrant=False,\n-        >>>         context_fn=selective_checkpointing_context_fn,\n-        >>>     )\n+        >>> context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n+        >>>\n+        >>> # or equivalently\n+        >>> context_fn = functools.partial(create_selective_checkpoint_contexts, ops_to_save)\n         >>>\n-        >>> x = torch.randn(4, 4, requires_grad=True)\n-        >>> y = torch.randn(4, 4, requires_grad=True)\n+        >>> def fn(x, y):\n+        >>>     return torch.sigmoid(torch.matmul(torch.matmul(x, y), y)) * y\n         >>>\n-        >>> compiled_fn = torch.compile(fn)\n+        >>> out = torch.utils.checkpoint.checkpoint(\n+        >>>     fn, x, y,\n+        >>>     use_reentrant=False,\n+        >>>     context_fn=context_fn,\n+        >>> )\n     \"\"\"\n-    storage: Dict[Any, List[Any]] = defaultdict(list)\n-    return _CachingTorchDispatchMode(policy_fn, storage), _CachedTorchDispatchMode(policy_fn, storage)\n+    # NB: If grad_mode is disabled, checkpoint would not run forward under\n+    #     context_fn anyway, so proceed as usual.\n+    if isinstance(policy_fn_or_list, list):\n+        for op in policy_fn_or_list:\n+            if not isinstance(op, torch._ops.OpOverload):\n+                _extra_msg = (\n+                    \"Please update the OpOverloadPacket to a specific OpOverload.\"\n+                    \"For example, if you have `torch.ops.aten.mm`, change it to `torch.ops.aten.mm.default`.\"\n+                ) if isinstance(op, torch._ops.OpOverloadPacket) else \"\"\n+                raise ValueError(\n+                    f\"Expected op in `op_list` to be an OpOverload but got: {op} \"\n+                    f\"of type {type(op)}. {_extra_msg}\"\n+                )\n \n+        def policy_fn(ctx, op, *args, **kwargs):\n+            if op in policy_fn_or_list:\n+                return CheckpointPolicy.MUST_SAVE\n+            else:\n+                return CheckpointPolicy.PREFER_RECOMPUTE\n+    elif callable(policy_fn_or_list):\n+        policy_fn = policy_fn_or_list\n+    else:\n+        raise TypeError(\"policy_fn_or_list must be either a function or a list of ops.\")\n+\n+    storage: Dict[Any, List[Any]] = defaultdict(list)\n+    return (\n+        _CachingTorchDispatchMode(policy_fn, storage),\n+        _CachedTorchDispatchMode(policy_fn, storage, allow_cache_entry_mutation),\n+    )\n \n # NB: this helper wraps fn before calling checkpoint_impl. kwargs and\n #     saving/restoring of global state is handled here."
                }
            ],
            "whole_deleted": "-from itertools import count\n-from typing import (\n-    Any,\n-    Callable,\n-    ContextManager,\n-    DefaultDict,\n-    Dict,\n-    Iterable,\n-    List,\n-    Optional,\n-    Tuple,\n-)\n-def _detach(x):\n-    if isinstance(x, torch.Tensor):\n-        return x.detach()\n-uid = count(1)\n-# NOTE: torch.utils.checkpoint internal logic will call these two functions unknown number of times\n-# (i.e. there could be _CachedTorchDispatchMode calls that doesn't map to a _CachingTorchDispatchMode call),\n-# so we ignore these ops and just always recompute them.\n-_ignored_ops = {\n-    torch.ops.prim.device.default,\n-    r\"\"\"\n-    A :class:`TorchDispatchMode` to implement selective activation checkpointing\n-    that's compatible with torch.compile. Used together with _CachedTorchDispatchMode.\n-    \"\"\"\n-    def push_into_storage(self, out, func, args, kwargs):\n-        out_detached = tree_map(_detach, out)\n-        self.storage[func].append(out_detached)\n-    def _handle_compile_in_forward_ctx(self, should_not_recompute, func, args, kwargs):\n-        if should_not_recompute:\n-        # NOTE: Here we just store and reuse output of all ops, since in torch.compile mode\n-        # we decide and handle recomputation in the partitioner.\n-        self.push_into_storage(out, func, args, kwargs)\n-        return out\n-    def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n-        if kwargs is None:\n-            kwargs = {}\n-        if func in _ignored_ops:\n-            return func(*args, **kwargs)\n-        should_not_recompute = self.policy_fn(\"forward\", func, *args, **kwargs)\n-        if _is_compiling(func, args, kwargs):\n-            return self._handle_compile_in_forward_ctx(should_not_recompute, func, args, kwargs)\n-        else:\n-            if should_not_recompute:\n-                out = func(*args, **kwargs)\n-                self.push_into_storage(out, func, args, kwargs)\n-            else:\n-                out = func(*args, **kwargs)\n-            return out\n-    r\"\"\"\n-    A :class:`TorchDispatchMode` to implement selective activation checkpointing\n-    that's compatible with torch.compile. Used together with _CachingTorchDispatchMode.\n-    \"\"\"\n-    def __init__(self, policy_fn, storage):\n-\n-    def pop_from_storage(self, func, args, kwargs):\n-        assert func in self.storage\n-        out = self.storage[func].pop(0)\n-        return out\n-\n-    def _handle_compile_in_recompute_ctx(self, should_not_recompute, func, args, kwargs):\n-        out = self.pop_from_storage(func, args, kwargs)\n-        return out\n-        if kwargs is None:\n-            kwargs = {}\n-        if func in _ignored_ops:\n-        should_not_recompute = self.policy_fn(\"recompute\", func, *args, **kwargs)\n-        if _is_compiling(func, args, kwargs):\n-            return self._handle_compile_in_recompute_ctx(should_not_recompute, func, args, kwargs)\n-            if should_not_recompute:\n-                out = self.pop_from_storage(func, args, kwargs)\n-            else:\n-                out = func(*args, **kwargs)\n-            return out\n-def _pt2_selective_checkpoint_context_fn_gen(policy_fn):\n-    A helper function that generates a pair of contexts to be later passed into\n-    `torch.utils.checkpoint` API to implment selective checkpointing.\n-    .. warning::\n-        This is context_fn is intended for use with torch.compile only.\n-        policy_fn (Callable[[Callable, List[Any], Dict[str, Any]], bool]): Policy function\n-            to decide whether a particular op should be recomputed in backward pass or not.\n-            In eager mode:\n-                If policy_fn(...) returns True, the op is guaranteed to NOT be recomputed.\n-                If policy_fn(...) returns False, the op is guaranteed to be recomputed.\n-            In torch.compile mode:\n-                If policy_fn(...) returns True, the op is guaranteed to NOT be recomputed.\n-                If policy_fn(...) returns False, the op may or may not be recomputed\n-                (it's up to the partitioner to decide).\n-\n-        A pair of generated contexts.\n-        >>> def get_custom_policy():\n-        >>>     no_recompute_list = [\n-        >>>         torch.ops.aten.mm.default,\n-        >>>     ]\n-        >>>     def custom_policy(mode, func, *args, **kwargs):\n-        >>>         return func in no_recompute_list\n-        >>>     return custom_policy\n-        >>> def selective_checkpointing_context_fn():\n-        >>>     return _pt2_selective_checkpoint_context_fn_gen(get_custom_policy())\n-        >>> def gn(x, y):\n-        >>>     return torch.sigmoid(torch.matmul(torch.matmul(x, y), y)) * y\n-        >>> def fn(x, y):\n-        >>>     return torch.utils.checkpoint.checkpoint(\n-        >>>         gn, x, y,\n-        >>>         use_reentrant=False,\n-        >>>         context_fn=selective_checkpointing_context_fn,\n-        >>>     )\n-        >>> x = torch.randn(4, 4, requires_grad=True)\n-        >>> y = torch.randn(4, 4, requires_grad=True)\n-        >>> compiled_fn = torch.compile(fn)\n-    storage: Dict[Any, List[Any]] = defaultdict(list)\n-    return _CachingTorchDispatchMode(policy_fn, storage), _CachedTorchDispatchMode(policy_fn, storage)\n",
            "whole_added": "+from typing import *  # noqa: F403\n+import enum\n+    \"CheckpointPolicy\",\n+    \"SelectiveCheckpointContext\",\n+    \"create_selective_checkpoint_contexts\",\n+    \"SAC_IGNORED_OPS\",\n+class _VersionWrapper:\n+    # Check that cached tensors are not mutated.\n+    def __init__(self, val):\n+        self.val: Union[torch.Tensor, Any] = val\n+        self.version: Optional[int] = val._version if isinstance(val, torch.Tensor) else None\n+\n+    def get_val(self, allow_cache_entry_mutation):\n+        if self.version is not None and not allow_cache_entry_mutation:\n+            if self.val._version != self.version:\n+                # Can we give user a stack trace of where the mutation happened?\n+                raise RuntimeError(\n+                    \"Tensor cached during selective activation checkpoint has been mutated\"\n+                )\n+        return self.val\n+\n+\n+def _maybe_detach(x, any_ret_has_alias_info):\n+    # We detach for two separate reasons:\n+    # - For view ops, we need to ensure that when the tensor is returned from\n+    #   CachedDispatchMode, as_view sees that the AutogradMeta is nullptr\n+    # - Avoid reference cycles\n+    # For case 1, it is not enough to check whether x has differentiable dtype\n+    # because non-differentiable dtype can have non-nullptr AutogradMeta, e.g.\n+    # when the tensor is a view.\n+    if isinstance(x, torch.Tensor) and (x.is_floating_point() or x.is_complex() or any_ret_has_alias_info):\n+        with torch._C._SetExcludeDispatchKeyGuard(torch._C.DispatchKey.ADInplaceOrView, False):\n+            # Ensure that view performed beneath autograd properly propagates\n+            # version counter. TODO: Use reentrant_dispatch instead of\n+            # manually manipulating dispatch keys. Using reentrant_dispatch\n+            # would respect inference_mode, though that is not relevant for\n+            # this case.\n+            x = x.detach()\n+class SelectiveCheckpointContext:\n+    \"\"\"\n+    Context passed to policy function during selective checkpointing.\n+    This class is used to pass relevant metadata to the policy function during\n+    selective checkpointing. The metadata includes whether the current invocation\n+    of the policy function is during recomputation or not.\n+    Example:\n+        >>> # xdoctest: +SKIP(stub)\n+        >>>\n+        >>> def policy_fn(ctx, op, *args, **kwargs):\n+        >>>    print(ctx.is_recompute)\n+        >>>\n+        >>> context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n+        >>>\n+        >>> out = torch.utils.checkpoint.checkpoint(\n+        >>>     fn, x, y,\n+        >>>     use_reentrant=False,\n+        >>>     context_fn=context_fn,\n+        >>> )\n+    \"\"\"\n+    def __init__(self, *, is_recompute):\n+        self.is_recompute = is_recompute\n+\n+\n+class CheckpointPolicy(enum.Enum):\n+    \"\"\"\n+    Enum for specifying the policy for checkpointing during backpropagation.\n+\n+    The following policies are supported:\n+\n+    - ``{MUST,PREFER}_SAVE``: The operation's output will be saved during the forward\n+      pass and will not be recomputed during the backward pass\n+    - ``{MUST,PREFER}_RECOMPUTE``: The operation's output will not be saved during the\n+      forward pass and will be recomputed during the backward pass\n+\n+    Use ``MUST_*`` over ``PREFER_*`` to indicate that the policy should not be overridden\n+    by other subsystems like `torch.compile`.\n+\n+    .. note::\n+        A policy function that always returns ``PREFER_RECOMPUTE`` is\n+        equivalent to vanilla checkpointing.\n+\n+        A policy function that returns ``PREFER_SAVE`` every op is\n+        NOT equivalent to not using checkpointing. Using such a policy would\n+        save additional tensors not limited to ones that are actually needed for\n+        gradient computation.\n+    \"\"\"\n+    MUST_SAVE = 0\n+    PREFER_SAVE = 1\n+    MUST_RECOMPUTE = 2\n+    PREFER_RECOMPUTE = 3\n+\n+\n+SAC_IGNORED_OPS = {\n+    # AC inserts different number of detach during forward and recompute.\n+    # AC's determinism check invokes additional metadata ops during forward.\n+    # With subclasses involved, these metadata ops become dispatchable, this\n+    # can result in incorrectness if these ops are selected cached.\n+    torch.ops.prim.device.default,\n+    # Used together with _CachedTorchDispatchMode to implement SAC.\n+    def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n+        if func in SAC_IGNORED_OPS:\n+            return func(*args, **kwargs)\n+        kwargs = {} if kwargs is None else kwargs\n+        policy = self.policy_fn(SelectiveCheckpointContext(is_recompute=False),\n+                                func, *args, **kwargs)\n+        is_compiling = _is_compiling(func, args, kwargs)\n+\n+        if is_compiling and policy == CheckpointPolicy.MUST_SAVE:\n+\n+        any_ret_has_alias_info = any(ret.alias_info is not None for ret in func._schema.returns)\n+\n+        if policy in (CheckpointPolicy.MUST_SAVE, CheckpointPolicy.PREFER_SAVE) or is_compiling:\n+            self.storage[func].append(tree_map(lambda x: _VersionWrapper(_maybe_detach(x, any_ret_has_alias_info)), out))\n+        return out\n+    # Used together with _CachedTorchDispatchMode to implement SAC.\n+    def __init__(self, policy_fn, storage, allow_cache_entry_mutation):\n+        self.allow_cache_entry_mutation = allow_cache_entry_mutation\n+        if func in SAC_IGNORED_OPS:\n+\n+        kwargs = {} if kwargs is None else kwargs\n+        policy = self.policy_fn(SelectiveCheckpointContext(is_recompute=True),\n+                                func, *args, **kwargs)\n+        is_compiling = _is_compiling(func, args, kwargs)\n+\n+        if policy in (CheckpointPolicy.MUST_SAVE, CheckpointPolicy.PREFER_SAVE) or is_compiling:\n+            storage = self.storage.get(func)\n+            if storage is None:\n+                raise RuntimeError(f\"{func} encountered during backward, but not found in storage\")\n+            if len(storage) == 0:\n+                raise RuntimeError(\n+                    \"Trying to backward an extra time. You are only allowed to backward once \"\n+                    \"on any region computed under selective activation checkpoint.\"\n+                )\n+            out = tree_map(lambda x: x.get_val(self.allow_cache_entry_mutation), storage.pop(0))\n+            out = func(*args, **kwargs)\n+        return out\n+\n+def create_selective_checkpoint_contexts(policy_fn_or_list, allow_cache_entry_mutation=False):\n+    Helper to avoid recomputing certain ops during activation checkpointing.\n+    Use this with `torch.utils.checkpoint.checkpoint` to control which\n+    operations are recomputed during the backward pass.\n+        policy_fn_or_list (Callable or List):\n+          - If a policy function is provided, it should accept a\n+            :class:`SelectiveCheckpointContext`, the :class:`OpOverload`, args and\n+            kwargs to the op, and return a :class:`CheckpointPolicy` enum value\n+            indicating whether the execution of the op should be recomputed or not.\n+          - If a list of operations is provided, it is equivalent to a policy\n+            returning `CheckpointPolicy.MUST_SAVE` for the specified\n+            operations and `CheckpointPolicy.PREFER_RECOMPUTE` for all other\n+            operations.\n+        allow_cache_entry_mutation (bool, optional): By default, an error is\n+            raised if any tensors cached by selective activation checkpoint are\n+            mutated in order to ensure correctness. If set to `True`, this check\n+            is disabled.\n+        A tuple of two context managers.\n+        >>> import functools\n+        >>> x = torch.rand(10, 10, requires_grad=True)\n+        >>> y = torch.rand(10, 10, requires_grad=True)\n+        >>> ops_to_save = [\n+        >>>    torch.ops.aten.mm.default,\n+        >>> ]\n+        >>> def policy_fn(ctx, op, *args, **kwargs):\n+        >>>    if op in ops_to_save:\n+        >>>        return CheckpointPolicy.MUST_SAVE\n+        >>>    else:\n+        >>>        return CheckpointPolicy.PREFER_RECOMPUTE\n+        >>> context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n+        >>>\n+        >>> # or equivalently\n+        >>> context_fn = functools.partial(create_selective_checkpoint_contexts, ops_to_save)\n+        >>> def fn(x, y):\n+        >>>     return torch.sigmoid(torch.matmul(torch.matmul(x, y), y)) * y\n+        >>> out = torch.utils.checkpoint.checkpoint(\n+        >>>     fn, x, y,\n+        >>>     use_reentrant=False,\n+        >>>     context_fn=context_fn,\n+        >>> )\n+    # NB: If grad_mode is disabled, checkpoint would not run forward under\n+    #     context_fn anyway, so proceed as usual.\n+    if isinstance(policy_fn_or_list, list):\n+        for op in policy_fn_or_list:\n+            if not isinstance(op, torch._ops.OpOverload):\n+                _extra_msg = (\n+                    \"Please update the OpOverloadPacket to a specific OpOverload.\"\n+                    \"For example, if you have `torch.ops.aten.mm`, change it to `torch.ops.aten.mm.default`.\"\n+                ) if isinstance(op, torch._ops.OpOverloadPacket) else \"\"\n+                raise ValueError(\n+                    f\"Expected op in `op_list` to be an OpOverload but got: {op} \"\n+                    f\"of type {type(op)}. {_extra_msg}\"\n+                )\n+        def policy_fn(ctx, op, *args, **kwargs):\n+            if op in policy_fn_or_list:\n+                return CheckpointPolicy.MUST_SAVE\n+            else:\n+                return CheckpointPolicy.PREFER_RECOMPUTE\n+    elif callable(policy_fn_or_list):\n+        policy_fn = policy_fn_or_list\n+    else:\n+        raise TypeError(\"policy_fn_or_list must be either a function or a list of ops.\")\n+\n+    storage: Dict[Any, List[Any]] = defaultdict(list)\n+    return (\n+        _CachingTorchDispatchMode(policy_fn, storage),\n+        _CachedTorchDispatchMode(policy_fn, storage, allow_cache_entry_mutation),\n+    )\n",
            "whole_hunk": "@@ -5,18 +5,8 @@ import uuid\n import warnings\n import weakref\n from collections import defaultdict\n-from itertools import count\n-from typing import (\n-    Any,\n-    Callable,\n-    ContextManager,\n-    DefaultDict,\n-    Dict,\n-    Iterable,\n-    List,\n-    Optional,\n-    Tuple,\n-)\n+from typing import *  # noqa: F403\n+import enum\n from weakref import ReferenceType\n \n import torch\n@@ -39,6 +29,10 @@ __all__ = [\n     \"set_checkpoint_early_stop\",\n     \"DefaultDeviceType\",\n     \"set_checkpoint_debug_enabled\",\n+    \"CheckpointPolicy\",\n+    \"SelectiveCheckpointContext\",\n+    \"create_selective_checkpoint_contexts\",\n+    \"SAC_IGNORED_OPS\",\n ]\n \n _DEFAULT_DETERMINISM_MODE = \"default\"\n@@ -1153,149 +1147,247 @@ def _is_compiling(func, args, kwargs):\n     return False\n \n \n-def _detach(x):\n-    if isinstance(x, torch.Tensor):\n-        return x.detach()\n+class _VersionWrapper:\n+    # Check that cached tensors are not mutated.\n+    def __init__(self, val):\n+        self.val: Union[torch.Tensor, Any] = val\n+        self.version: Optional[int] = val._version if isinstance(val, torch.Tensor) else None\n+\n+    def get_val(self, allow_cache_entry_mutation):\n+        if self.version is not None and not allow_cache_entry_mutation:\n+            if self.val._version != self.version:\n+                # Can we give user a stack trace of where the mutation happened?\n+                raise RuntimeError(\n+                    \"Tensor cached during selective activation checkpoint has been mutated\"\n+                )\n+        return self.val\n+\n+\n+def _maybe_detach(x, any_ret_has_alias_info):\n+    # We detach for two separate reasons:\n+    # - For view ops, we need to ensure that when the tensor is returned from\n+    #   CachedDispatchMode, as_view sees that the AutogradMeta is nullptr\n+    # - Avoid reference cycles\n+    # For case 1, it is not enough to check whether x has differentiable dtype\n+    # because non-differentiable dtype can have non-nullptr AutogradMeta, e.g.\n+    # when the tensor is a view.\n+    if isinstance(x, torch.Tensor) and (x.is_floating_point() or x.is_complex() or any_ret_has_alias_info):\n+        with torch._C._SetExcludeDispatchKeyGuard(torch._C.DispatchKey.ADInplaceOrView, False):\n+            # Ensure that view performed beneath autograd properly propagates\n+            # version counter. TODO: Use reentrant_dispatch instead of\n+            # manually manipulating dispatch keys. Using reentrant_dispatch\n+            # would respect inference_mode, though that is not relevant for\n+            # this case.\n+            x = x.detach()\n     return x\n \n \n-uid = count(1)\n+class SelectiveCheckpointContext:\n+    \"\"\"\n+    Context passed to policy function during selective checkpointing.\n \n+    This class is used to pass relevant metadata to the policy function during\n+    selective checkpointing. The metadata includes whether the current invocation\n+    of the policy function is during recomputation or not.\n \n-# NOTE: torch.utils.checkpoint internal logic will call these two functions unknown number of times\n-# (i.e. there could be _CachedTorchDispatchMode calls that doesn't map to a _CachingTorchDispatchMode call),\n-# so we ignore these ops and just always recompute them.\n-_ignored_ops = {\n-    torch.ops.prim.device.default,\n+    Example:\n+        >>> # xdoctest: +SKIP(stub)\n+        >>>\n+        >>> def policy_fn(ctx, op, *args, **kwargs):\n+        >>>    print(ctx.is_recompute)\n+        >>>\n+        >>> context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n+        >>>\n+        >>> out = torch.utils.checkpoint.checkpoint(\n+        >>>     fn, x, y,\n+        >>>     use_reentrant=False,\n+        >>>     context_fn=context_fn,\n+        >>> )\n+    \"\"\"\n+    def __init__(self, *, is_recompute):\n+        self.is_recompute = is_recompute\n+\n+\n+class CheckpointPolicy(enum.Enum):\n+    \"\"\"\n+    Enum for specifying the policy for checkpointing during backpropagation.\n+\n+    The following policies are supported:\n+\n+    - ``{MUST,PREFER}_SAVE``: The operation's output will be saved during the forward\n+      pass and will not be recomputed during the backward pass\n+    - ``{MUST,PREFER}_RECOMPUTE``: The operation's output will not be saved during the\n+      forward pass and will be recomputed during the backward pass\n+\n+    Use ``MUST_*`` over ``PREFER_*`` to indicate that the policy should not be overridden\n+    by other subsystems like `torch.compile`.\n+\n+    .. note::\n+        A policy function that always returns ``PREFER_RECOMPUTE`` is\n+        equivalent to vanilla checkpointing.\n+\n+        A policy function that returns ``PREFER_SAVE`` every op is\n+        NOT equivalent to not using checkpointing. Using such a policy would\n+        save additional tensors not limited to ones that are actually needed for\n+        gradient computation.\n+    \"\"\"\n+    MUST_SAVE = 0\n+    PREFER_SAVE = 1\n+    MUST_RECOMPUTE = 2\n+    PREFER_RECOMPUTE = 3\n+\n+\n+SAC_IGNORED_OPS = {\n+    # AC inserts different number of detach during forward and recompute.\n     torch.ops.aten.detach.default,\n+    # AC's determinism check invokes additional metadata ops during forward.\n+    # With subclasses involved, these metadata ops become dispatchable, this\n+    # can result in incorrectness if these ops are selected cached.\n+    torch.ops.prim.device.default,\n } | set(torch._subclasses.functional_tensor.FunctionalTensor.metadata_fns)\n \n \n class _CachingTorchDispatchMode(TorchDispatchMode):\n-    r\"\"\"\n-    A :class:`TorchDispatchMode` to implement selective activation checkpointing\n-    that's compatible with torch.compile. Used together with _CachedTorchDispatchMode.\n-    \"\"\"\n+    # Used together with _CachedTorchDispatchMode to implement SAC.\n     def __init__(self, policy_fn, storage):\n         self.policy_fn = policy_fn\n         self.storage = storage\n \n-    def push_into_storage(self, out, func, args, kwargs):\n-        out_detached = tree_map(_detach, out)\n-        self.storage[func].append(out_detached)\n+    def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n+        if func in SAC_IGNORED_OPS:\n+            return func(*args, **kwargs)\n \n-    def _handle_compile_in_forward_ctx(self, should_not_recompute, func, args, kwargs):\n-        if should_not_recompute:\n+        kwargs = {} if kwargs is None else kwargs\n+        policy = self.policy_fn(SelectiveCheckpointContext(is_recompute=False),\n+                                func, *args, **kwargs)\n+        is_compiling = _is_compiling(func, args, kwargs)\n+\n+        if is_compiling and policy == CheckpointPolicy.MUST_SAVE:\n             fx_traceback.current_meta[\"recompute\"] = 0\n-        # NOTE: Here we just store and reuse output of all ops, since in torch.compile mode\n-        # we decide and handle recomputation in the partitioner.\n+\n         out = func(*args, **kwargs)\n-        self.push_into_storage(out, func, args, kwargs)\n-        return out\n \n-    def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n-        if kwargs is None:\n-            kwargs = {}\n-        if func in _ignored_ops:\n-            return func(*args, **kwargs)\n-        should_not_recompute = self.policy_fn(\"forward\", func, *args, **kwargs)\n-        if _is_compiling(func, args, kwargs):\n-            return self._handle_compile_in_forward_ctx(should_not_recompute, func, args, kwargs)\n-        else:\n-            if should_not_recompute:\n-                out = func(*args, **kwargs)\n-                self.push_into_storage(out, func, args, kwargs)\n-            else:\n-                out = func(*args, **kwargs)\n-            return out\n+        any_ret_has_alias_info = any(ret.alias_info is not None for ret in func._schema.returns)\n+\n+        if policy in (CheckpointPolicy.MUST_SAVE, CheckpointPolicy.PREFER_SAVE) or is_compiling:\n+            self.storage[func].append(tree_map(lambda x: _VersionWrapper(_maybe_detach(x, any_ret_has_alias_info)), out))\n+        return out\n \n class _CachedTorchDispatchMode(TorchDispatchMode):\n-    r\"\"\"\n-    A :class:`TorchDispatchMode` to implement selective activation checkpointing\n-    that's compatible with torch.compile. Used together with _CachingTorchDispatchMode.\n-    \"\"\"\n-    def __init__(self, policy_fn, storage):\n+    # Used together with _CachedTorchDispatchMode to implement SAC.\n+    def __init__(self, policy_fn, storage, allow_cache_entry_mutation):\n         self.policy_fn = policy_fn\n         self.storage = storage\n-\n-    def pop_from_storage(self, func, args, kwargs):\n-        assert func in self.storage\n-        out = self.storage[func].pop(0)\n-        return out\n-\n-    def _handle_compile_in_recompute_ctx(self, should_not_recompute, func, args, kwargs):\n-        out = self.pop_from_storage(func, args, kwargs)\n-        return out\n+        self.allow_cache_entry_mutation = allow_cache_entry_mutation\n \n     def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n-        if kwargs is None:\n-            kwargs = {}\n-        if func in _ignored_ops:\n+        if func in SAC_IGNORED_OPS:\n             return func(*args, **kwargs)\n-        should_not_recompute = self.policy_fn(\"recompute\", func, *args, **kwargs)\n-        if _is_compiling(func, args, kwargs):\n-            return self._handle_compile_in_recompute_ctx(should_not_recompute, func, args, kwargs)\n+\n+        kwargs = {} if kwargs is None else kwargs\n+        policy = self.policy_fn(SelectiveCheckpointContext(is_recompute=True),\n+                                func, *args, **kwargs)\n+        is_compiling = _is_compiling(func, args, kwargs)\n+\n+        if policy in (CheckpointPolicy.MUST_SAVE, CheckpointPolicy.PREFER_SAVE) or is_compiling:\n+            storage = self.storage.get(func)\n+            if storage is None:\n+                raise RuntimeError(f\"{func} encountered during backward, but not found in storage\")\n+            if len(storage) == 0:\n+                raise RuntimeError(\n+                    \"Trying to backward an extra time. You are only allowed to backward once \"\n+                    \"on any region computed under selective activation checkpoint.\"\n+                )\n+            out = tree_map(lambda x: x.get_val(self.allow_cache_entry_mutation), storage.pop(0))\n         else:\n-            if should_not_recompute:\n-                out = self.pop_from_storage(func, args, kwargs)\n-            else:\n-                out = func(*args, **kwargs)\n-            return out\n+            out = func(*args, **kwargs)\n+        return out\n \n-def _pt2_selective_checkpoint_context_fn_gen(policy_fn):\n+\n+def create_selective_checkpoint_contexts(policy_fn_or_list, allow_cache_entry_mutation=False):\n     \"\"\"\n-    A helper function that generates a pair of contexts to be later passed into\n-    `torch.utils.checkpoint` API to implment selective checkpointing.\n+    Helper to avoid recomputing certain ops during activation checkpointing.\n \n-    .. warning::\n-        This is context_fn is intended for use with torch.compile only.\n+    Use this with `torch.utils.checkpoint.checkpoint` to control which\n+    operations are recomputed during the backward pass.\n \n     Args:\n-        policy_fn (Callable[[Callable, List[Any], Dict[str, Any]], bool]): Policy function\n-            to decide whether a particular op should be recomputed in backward pass or not.\n-            In eager mode:\n-                If policy_fn(...) returns True, the op is guaranteed to NOT be recomputed.\n-                If policy_fn(...) returns False, the op is guaranteed to be recomputed.\n-            In torch.compile mode:\n-                If policy_fn(...) returns True, the op is guaranteed to NOT be recomputed.\n-                If policy_fn(...) returns False, the op may or may not be recomputed\n-                (it's up to the partitioner to decide).\n-\n+        policy_fn_or_list (Callable or List):\n+          - If a policy function is provided, it should accept a\n+            :class:`SelectiveCheckpointContext`, the :class:`OpOverload`, args and\n+            kwargs to the op, and return a :class:`CheckpointPolicy` enum value\n+            indicating whether the execution of the op should be recomputed or not.\n+          - If a list of operations is provided, it is equivalent to a policy\n+            returning `CheckpointPolicy.MUST_SAVE` for the specified\n+            operations and `CheckpointPolicy.PREFER_RECOMPUTE` for all other\n+            operations.\n+        allow_cache_entry_mutation (bool, optional): By default, an error is\n+            raised if any tensors cached by selective activation checkpoint are\n+            mutated in order to ensure correctness. If set to `True`, this check\n+            is disabled.\n     Returns:\n-        A pair of generated contexts.\n+        A tuple of two context managers.\n \n     Example:\n         >>> # xdoctest: +REQUIRES(LINUX)\n+        >>> import functools\n         >>>\n-        >>> def get_custom_policy():\n-        >>>     no_recompute_list = [\n-        >>>         torch.ops.aten.mm.default,\n-        >>>     ]\n-        >>>     def custom_policy(mode, func, *args, **kwargs):\n-        >>>         return func in no_recompute_list\n-        >>>     return custom_policy\n+        >>> x = torch.rand(10, 10, requires_grad=True)\n+        >>> y = torch.rand(10, 10, requires_grad=True)\n         >>>\n-        >>> def selective_checkpointing_context_fn():\n-        >>>     return _pt2_selective_checkpoint_context_fn_gen(get_custom_policy())\n+        >>> ops_to_save = [\n+        >>>    torch.ops.aten.mm.default,\n+        >>> ]\n         >>>\n-        >>> def gn(x, y):\n-        >>>     return torch.sigmoid(torch.matmul(torch.matmul(x, y), y)) * y\n+        >>> def policy_fn(ctx, op, *args, **kwargs):\n+        >>>    if op in ops_to_save:\n+        >>>        return CheckpointPolicy.MUST_SAVE\n+        >>>    else:\n+        >>>        return CheckpointPolicy.PREFER_RECOMPUTE\n         >>>\n-        >>> def fn(x, y):\n-        >>>     return torch.utils.checkpoint.checkpoint(\n-        >>>         gn, x, y,\n-        >>>         use_reentrant=False,\n-        >>>         context_fn=selective_checkpointing_context_fn,\n-        >>>     )\n+        >>> context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n+        >>>\n+        >>> # or equivalently\n+        >>> context_fn = functools.partial(create_selective_checkpoint_contexts, ops_to_save)\n         >>>\n-        >>> x = torch.randn(4, 4, requires_grad=True)\n-        >>> y = torch.randn(4, 4, requires_grad=True)\n+        >>> def fn(x, y):\n+        >>>     return torch.sigmoid(torch.matmul(torch.matmul(x, y), y)) * y\n         >>>\n-        >>> compiled_fn = torch.compile(fn)\n+        >>> out = torch.utils.checkpoint.checkpoint(\n+        >>>     fn, x, y,\n+        >>>     use_reentrant=False,\n+        >>>     context_fn=context_fn,\n+        >>> )\n     \"\"\"\n-    storage: Dict[Any, List[Any]] = defaultdict(list)\n-    return _CachingTorchDispatchMode(policy_fn, storage), _CachedTorchDispatchMode(policy_fn, storage)\n+    # NB: If grad_mode is disabled, checkpoint would not run forward under\n+    #     context_fn anyway, so proceed as usual.\n+    if isinstance(policy_fn_or_list, list):\n+        for op in policy_fn_or_list:\n+            if not isinstance(op, torch._ops.OpOverload):\n+                _extra_msg = (\n+                    \"Please update the OpOverloadPacket to a specific OpOverload.\"\n+                    \"For example, if you have `torch.ops.aten.mm`, change it to `torch.ops.aten.mm.default`.\"\n+                ) if isinstance(op, torch._ops.OpOverloadPacket) else \"\"\n+                raise ValueError(\n+                    f\"Expected op in `op_list` to be an OpOverload but got: {op} \"\n+                    f\"of type {type(op)}. {_extra_msg}\"\n+                )\n \n+        def policy_fn(ctx, op, *args, **kwargs):\n+            if op in policy_fn_or_list:\n+                return CheckpointPolicy.MUST_SAVE\n+            else:\n+                return CheckpointPolicy.PREFER_RECOMPUTE\n+    elif callable(policy_fn_or_list):\n+        policy_fn = policy_fn_or_list\n+    else:\n+        raise TypeError(\"policy_fn_or_list must be either a function or a list of ops.\")\n+\n+    storage: Dict[Any, List[Any]] = defaultdict(list)\n+    return (\n+        _CachingTorchDispatchMode(policy_fn, storage),\n+        _CachedTorchDispatchMode(policy_fn, storage, allow_cache_entry_mutation),\n+    )\n \n # NB: this helper wraps fn before calling checkpoint_impl. kwargs and\n #     saving/restoring of global state is handled here."
        }
    ]
},
{
    "Id": 153,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/6db32710074f0944305b2d1e4571bb4ce571bf6a",
    "date": "2024-05-14T00:05:41+00:00",
    "message": "[c10d] Add an option for NAN check on every collective (#125726)\n\nSummary:\nThe NAN CHECK is done through device side assert without copying needed\nfrom GPU to CPU\nTest Plan:\nUnit test for collectives that should experience run time error\n\n(sqzhang_1) [sqzhang@devgpu009.cln1 ~/pytorch (38f5143e)]$  python\ntest/distributed/test_c10d_nccl.py ProcessGroupNCCLTest.test_nan_assert\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [0,0,0] Assertion `!isnan(val)`\nfailed.\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [1,0,0] Assertion `!isnan(val)`\nfailed.\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [2,0,0] Assertion `!isnan(val)`\nfailed.\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [3,0,0] Assertion `!isnan(val)`\nfailed.\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [4,0,0] Assertion `!isnan(val)`\nfailed.\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [5,0,0] Assertion `!isnan(val)`\nfailed.\n[rank0]:[E507 17:31:56.885473996 Utils.cu:30] CUDA error during\ncheckForNan: device-side assert triggered\n\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [0,0,0] Assertion `!isnan(val)`\nfailed.\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [1,0,0] Assertion `!isnan(val)`\nfailed.\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [2,0,0] Assertion `!isnan(val)`\nfailed.\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [3,0,0] Assertion `!isnan(val)`\nfailed.\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [4,0,0] Assertion `!isnan(val)`\nfailed.\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [5,0,0] Assertion `!isnan(val)`\nfailed.\n[rank1]:[E507 17:31:56.128961534 Utils.cu:30] CUDA error during\ncheckForNan: device-side assert triggered\n\n.\n----------------------------------------------------------------------\nRan 1 test in 7.723s\n\nOK\n\nTags:\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125726\nApproved by: https://github.com/kwen2501",
    "label": "YES",
    "changes": [
        {
            "name": "BUILD.bazel",
            "path": "BUILD.bazel",
            "patches": [
                {
                    "old_start": 663,
                    "old_length": 6,
                    "new_start": 663,
                    "new_length": 7,
                    "hunk": "@@ -663,6 +663,7 @@ cu_library(\n     name = \"torch_cuda\",\n     srcs = [\n         \"torch/csrc/distributed/c10d/intra_node_comm.cu\",\n+        \"torch/csrc/distributed/c10d/Utils.cu\",\n         \"torch/csrc/distributed/c10d/quantization/quantization_gpu.cu\",\n     ],\n     copts = torch_cuda_half_options,\n"
                },
                {
                    "old_start": 830,
                    "old_length": 6,
                    "new_start": 831,
                    "new_length": 7,
                    "hunk": "@@ -830,6 +831,7 @@ cc_library(\n             \"torch/csrc/cuda/python_nccl.cpp\",\n             \"torch/csrc/cuda/nccl.cpp\",\n             \"torch/csrc/distributed/c10d/intra_node_comm.cu\",\n+            \"torch/csrc/distributed/c10d/Utils.cu\",\n             \"torch/csrc/distributed/c10d/quantization/quantization_gpu.cu\",\n         ],\n     )) + torch_sources,\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"torch/csrc/distributed/c10d/Utils.cu\",\n+            \"torch/csrc/distributed/c10d/Utils.cu\",\n",
            "whole_hunk": "@@ -663,6 +663,7 @@ cu_library(\n     name = \"torch_cuda\",\n     srcs = [\n         \"torch/csrc/distributed/c10d/intra_node_comm.cu\",\n+        \"torch/csrc/distributed/c10d/Utils.cu\",\n         \"torch/csrc/distributed/c10d/quantization/quantization_gpu.cu\",\n     ],\n     copts = torch_cuda_half_options,\n@@ -830,6 +831,7 @@ cc_library(\n             \"torch/csrc/cuda/python_nccl.cpp\",\n             \"torch/csrc/cuda/nccl.cpp\",\n             \"torch/csrc/distributed/c10d/intra_node_comm.cu\",\n+            \"torch/csrc/distributed/c10d/Utils.cu\",\n             \"torch/csrc/distributed/c10d/quantization/quantization_gpu.cu\",\n         ],\n     )) + torch_sources,\n"
        },
        {
            "name": "build_variables.bzl",
            "path": "build_variables.bzl",
            "patches": [
                {
                    "old_start": 679,
                    "old_length": 6,
                    "new_start": 679,
                    "new_length": 7,
                    "hunk": "@@ -679,6 +679,7 @@ libtorch_cuda_distributed_extra_sources = [\n     \"torch/csrc/distributed/c10d/UCCUtils.cpp\",\n     \"torch/csrc/distributed/c10d/intra_node_comm.cpp\",\n     \"torch/csrc/distributed/c10d/intra_node_comm.cu\",\n+    \"torch/csrc/distributed/c10d/Utils.cu\",\n     \"torch/csrc/distributed/rpc/tensorpipe_cuda.cpp\",\n     \"torch/csrc/distributed/c10d/quantization/quantization_gpu.cu\",\n ]\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    \"torch/csrc/distributed/c10d/Utils.cu\",\n",
            "whole_hunk": "@@ -679,6 +679,7 @@ libtorch_cuda_distributed_extra_sources = [\n     \"torch/csrc/distributed/c10d/UCCUtils.cpp\",\n     \"torch/csrc/distributed/c10d/intra_node_comm.cpp\",\n     \"torch/csrc/distributed/c10d/intra_node_comm.cu\",\n+    \"torch/csrc/distributed/c10d/Utils.cu\",\n     \"torch/csrc/distributed/rpc/tensorpipe_cuda.cpp\",\n     \"torch/csrc/distributed/c10d/quantization/quantization_gpu.cu\",\n ]\n"
        },
        {
            "name": "test_c10d_nccl.py",
            "path": "test/distributed/test_c10d_nccl.py",
            "patches": [
                {
                    "old_start": 325,
                    "old_length": 6,
                    "new_start": 325,
                    "new_length": 26,
                    "hunk": "@@ -325,6 +325,26 @@ class ProcessGroupNCCLGroupTest(MultiProcessTestCase):\n \n         del pg\n \n+    @requires_nccl()\n+    @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"NCCL test requires 2+ GPUs\")\n+    @parametrize(\"type\", [torch.float16, torch.float32, torch.float64])\n+    @parametrize(\"size\", [(10, 10), (1000, 1000)])\n+    def test_nan_assert(self, type, size):\n+        os.environ[\"TORCH_NCCL_NAN_CHECK\"] = \"1\"\n+        store = c10d.FileStore(self.file_name, self.world_size)\n+        pg = self._create_process_group_nccl(store, self.opts())\n+        device = self.rank_to_GPU[self.rank][0]\n+        nan_tensor = torch.full(size, self.rank, dtype=type, device=device)\n+        # randomly pick an nan element\n+        i = random.randint(0, nan_tensor.size(0) - 1)\n+        j = random.randint(0, nan_tensor.size(1) - 1)\n+        nan_tensor[i, j] = float(\"nan\")\n+        with self.assertRaises(RuntimeError):\n+            pg.allreduce(nan_tensor)\n+        dist.destroy_process_group()\n+        # reset env\n+        os.environ[\"TORCH_NCCL_NAN_CHECK\"] = \"0\"\n+\n     @requires_nccl()\n     @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"NCCL test requires 2+ GPUs\")\n     def test_destruct_before_terminate_pg(self):\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    @requires_nccl()\n+    @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"NCCL test requires 2+ GPUs\")\n+    @parametrize(\"type\", [torch.float16, torch.float32, torch.float64])\n+    @parametrize(\"size\", [(10, 10), (1000, 1000)])\n+    def test_nan_assert(self, type, size):\n+        os.environ[\"TORCH_NCCL_NAN_CHECK\"] = \"1\"\n+        store = c10d.FileStore(self.file_name, self.world_size)\n+        pg = self._create_process_group_nccl(store, self.opts())\n+        device = self.rank_to_GPU[self.rank][0]\n+        nan_tensor = torch.full(size, self.rank, dtype=type, device=device)\n+        # randomly pick an nan element\n+        i = random.randint(0, nan_tensor.size(0) - 1)\n+        j = random.randint(0, nan_tensor.size(1) - 1)\n+        nan_tensor[i, j] = float(\"nan\")\n+        with self.assertRaises(RuntimeError):\n+            pg.allreduce(nan_tensor)\n+        dist.destroy_process_group()\n+        # reset env\n+        os.environ[\"TORCH_NCCL_NAN_CHECK\"] = \"0\"\n+\n",
            "whole_hunk": "@@ -325,6 +325,26 @@ class ProcessGroupNCCLGroupTest(MultiProcessTestCase):\n \n         del pg\n \n+    @requires_nccl()\n+    @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"NCCL test requires 2+ GPUs\")\n+    @parametrize(\"type\", [torch.float16, torch.float32, torch.float64])\n+    @parametrize(\"size\", [(10, 10), (1000, 1000)])\n+    def test_nan_assert(self, type, size):\n+        os.environ[\"TORCH_NCCL_NAN_CHECK\"] = \"1\"\n+        store = c10d.FileStore(self.file_name, self.world_size)\n+        pg = self._create_process_group_nccl(store, self.opts())\n+        device = self.rank_to_GPU[self.rank][0]\n+        nan_tensor = torch.full(size, self.rank, dtype=type, device=device)\n+        # randomly pick an nan element\n+        i = random.randint(0, nan_tensor.size(0) - 1)\n+        j = random.randint(0, nan_tensor.size(1) - 1)\n+        nan_tensor[i, j] = float(\"nan\")\n+        with self.assertRaises(RuntimeError):\n+            pg.allreduce(nan_tensor)\n+        dist.destroy_process_group()\n+        # reset env\n+        os.environ[\"TORCH_NCCL_NAN_CHECK\"] = \"0\"\n+\n     @requires_nccl()\n     @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"NCCL test requires 2+ GPUs\")\n     def test_destruct_before_terminate_pg(self):\n"
        },
        {
            "name": "ProcessGroupNCCL.cpp",
            "path": "torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp",
            "patches": [
                {
                    "old_start": 748,
                    "old_length": 6,
                    "new_start": 748,
                    "new_length": 7,
                    "hunk": "@@ -748,6 +748,7 @@ ProcessGroupNCCL::ProcessGroupNCCL(\n   // both timeout and other errors.\n   dumpOnException_ = getCvarBool(TORCH_NCCL_DUMP_ON_TIMEOUT, false) ||\n       (dist_debug_level_ >= DebugLevel::Detail);\n+  enableNanCheck_ = getCvarBool(TORCH_NCCL_NAN_CHECK, false);\n   heartbeat_ = 1ULL;\n   monitorThreadEnabled_.store(getCvarBool(TORCH_NCCL_ENABLE_MONITORING, true));\n   heartbeatTimeoutInSec_ =\n"
                },
                {
                    "old_start": 836,
                    "old_length": 6,
                    "new_start": 837,
                    "new_length": 7,
                    "hunk": "@@ -836,6 +837,7 @@ ProcessGroupNCCL::ProcessGroupNCCL(\n             << \", TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: \" << heartbeatTimeoutInSec_\n             << \", TORCH_NCCL_TRACE_BUFFER_SIZE: \" << ncclTraceBufferSize_\n             << \", TORCH_NCCL_COORD_CHECK_MILSEC: \" << coordCheckIntervalMilSec_\n+            << \", TORCH_NCCL_NAN_CHECK: \" << enableNanCheck_\n             << \", PG Name: \" << options_->group_name;\n \n   if (options_->global_ranks_in_group.empty()) {\n"
                },
                {
                    "old_start": 2424,
                    "old_length": 6,
                    "new_start": 2426,
                    "new_length": 9,
                    "hunk": "@@ -2424,6 +2426,9 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::collective(\n     OpType opType,\n     const char* profilingTitle,\n     bool avoidRecordStreams) {\n+  if (enableNanCheck_) {\n+    checkForNan(input);\n+  }\n   // Environment setting by the user may add onto collective call's option\n   avoidRecordStreams |= avoidRecordStreams_;\n   c10::cuda::CaptureStatus capture_status =\n"
                },
                {
                    "old_start": 2779,
                    "old_length": 6,
                    "new_start": 2784,
                    "new_length": 9,
                    "hunk": "@@ -2779,6 +2784,9 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::pointToPoint(\n     PreProcess pre,\n     PostProcess post,\n     const char* profilingTitle) {\n+  if (enableNanCheck_) {\n+    checkForNan(tensor);\n+  }\n   // avoidRecordStreams_ note:\n   // send, recv, and irecv should be ok with avoidRecordStreams,\n   // However, for isend, I don't think the API requires the user\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  enableNanCheck_ = getCvarBool(TORCH_NCCL_NAN_CHECK, false);\n+            << \", TORCH_NCCL_NAN_CHECK: \" << enableNanCheck_\n+  if (enableNanCheck_) {\n+    checkForNan(input);\n+  }\n+  if (enableNanCheck_) {\n+    checkForNan(tensor);\n+  }\n",
            "whole_hunk": "@@ -748,6 +748,7 @@ ProcessGroupNCCL::ProcessGroupNCCL(\n   // both timeout and other errors.\n   dumpOnException_ = getCvarBool(TORCH_NCCL_DUMP_ON_TIMEOUT, false) ||\n       (dist_debug_level_ >= DebugLevel::Detail);\n+  enableNanCheck_ = getCvarBool(TORCH_NCCL_NAN_CHECK, false);\n   heartbeat_ = 1ULL;\n   monitorThreadEnabled_.store(getCvarBool(TORCH_NCCL_ENABLE_MONITORING, true));\n   heartbeatTimeoutInSec_ =\n@@ -836,6 +837,7 @@ ProcessGroupNCCL::ProcessGroupNCCL(\n             << \", TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: \" << heartbeatTimeoutInSec_\n             << \", TORCH_NCCL_TRACE_BUFFER_SIZE: \" << ncclTraceBufferSize_\n             << \", TORCH_NCCL_COORD_CHECK_MILSEC: \" << coordCheckIntervalMilSec_\n+            << \", TORCH_NCCL_NAN_CHECK: \" << enableNanCheck_\n             << \", PG Name: \" << options_->group_name;\n \n   if (options_->global_ranks_in_group.empty()) {\n@@ -2424,6 +2426,9 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::collective(\n     OpType opType,\n     const char* profilingTitle,\n     bool avoidRecordStreams) {\n+  if (enableNanCheck_) {\n+    checkForNan(input);\n+  }\n   // Environment setting by the user may add onto collective call's option\n   avoidRecordStreams |= avoidRecordStreams_;\n   c10::cuda::CaptureStatus capture_status =\n@@ -2779,6 +2784,9 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::pointToPoint(\n     PreProcess pre,\n     PostProcess post,\n     const char* profilingTitle) {\n+  if (enableNanCheck_) {\n+    checkForNan(tensor);\n+  }\n   // avoidRecordStreams_ note:\n   // send, recv, and irecv should be ok with avoidRecordStreams,\n   // However, for isend, I don't think the API requires the user\n"
        },
        {
            "name": "ProcessGroupNCCL.hpp",
            "path": "torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp",
            "patches": [
                {
                    "old_start": 100,
                    "old_length": 6,
                    "new_start": 100,
                    "new_length": 8,
                    "hunk": "@@ -100,6 +100,8 @@ static std::vector<std::string> TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC = {\n static std::vector<std::string> TORCH_NCCL_COORD_CHECK_MILSEC = {\n     \"TORCH_NCCL_COORD_CHECK_MILSEC\"};\n \n+static std::vector<std::string> TORCH_NCCL_NAN_CHECK = {\"TORCH_NCCL_NAN_CHECK\"};\n+\n constexpr const char* NCCL_BACKEND_NAME = \"nccl\";\n \n constexpr const char* EXCEPTION_DUMP = \"exception_dump\";\n"
                },
                {
                    "old_start": 1024,
                    "old_length": 6,
                    "new_start": 1026,
                    "new_length": 9,
                    "hunk": "@@ -1024,6 +1026,9 @@ class TORCH_API ProcessGroupNCCL : public Backend {\n   // timeout and nccl errors.\n   bool dumpOnException_;\n \n+  // Whether or not to enable nan check for input tensors to collectives.\n+  bool enableNanCheck_;\n+\n   // Whether or not to create start CUDAEvent and enable timing for start\n   // and end events. Note that enableTiming_ is always true if desyncDebug_\n   // is set to true.\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+static std::vector<std::string> TORCH_NCCL_NAN_CHECK = {\"TORCH_NCCL_NAN_CHECK\"};\n+\n+  // Whether or not to enable nan check for input tensors to collectives.\n+  bool enableNanCheck_;\n+\n",
            "whole_hunk": "@@ -100,6 +100,8 @@ static std::vector<std::string> TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC = {\n static std::vector<std::string> TORCH_NCCL_COORD_CHECK_MILSEC = {\n     \"TORCH_NCCL_COORD_CHECK_MILSEC\"};\n \n+static std::vector<std::string> TORCH_NCCL_NAN_CHECK = {\"TORCH_NCCL_NAN_CHECK\"};\n+\n constexpr const char* NCCL_BACKEND_NAME = \"nccl\";\n \n constexpr const char* EXCEPTION_DUMP = \"exception_dump\";\n@@ -1024,6 +1026,9 @@ class TORCH_API ProcessGroupNCCL : public Backend {\n   // timeout and nccl errors.\n   bool dumpOnException_;\n \n+  // Whether or not to enable nan check for input tensors to collectives.\n+  bool enableNanCheck_;\n+\n   // Whether or not to create start CUDAEvent and enable timing for start\n   // and end events. Note that enableTiming_ is always true if desyncDebug_\n   // is set to true.\n"
        },
        {
            "name": "Utils.cu",
            "path": "torch/csrc/distributed/c10d/Utils.cu",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 45,
                    "hunk": "@@ -0,0 +1,45 @@\n+#include <ATen/Dispatch.h>\n+#include <ATen/cuda/CUDAContext.h>\n+#include <c10/cuda/CUDAGuard.h>\n+#include <torch/csrc/distributed/c10d/Utils.hpp>\n+#include <torch/torch.h>\n+#include <algorithm>\n+\n+namespace c10d {\n+\n+// CUDA kernel to check if data has NAN, device side assert\n+// is raised if NAN is found\n+template <typename T>\n+__global__ void checkForNaN(T* data, size_t size) {\n+  size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n+  size_t stride = blockDim.x * gridDim.x;\n+\n+  for (size_t i = tid; i < size; i += stride) {\n+    CUDA_KERNEL_ASSERT(!isnan(data[i]));\n+  }\n+}\n+\n+// CHECK if a Tensor contains NAN in any of its element\n+void checkForNan(const at::Tensor& tensor) {\n+  // skip check for non float types\n+  if (!torch::is_floating_point(tensor)) {\n+    return;\n+  }\n+  const size_t maxNumThreadsPerBlock = 512;\n+  const size_t maxNumBlocks = 24;\n+  const size_t numThreadsPerBlock =\n+      std::min<size_t>(maxNumThreadsPerBlock, tensor.numel());\n+\n+  const size_t numBlocks = std::min<size_t>(\n+      maxNumBlocks,\n+      (tensor.numel() + numThreadsPerBlock - 1) / numThreadsPerBlock);\n+\n+  AT_DISPATCH_FLOATING_TYPES_AND_HALF(tensor.scalar_type(), \"checkForNaN\", [&] {\n+    checkForNaN<scalar_t><<<numBlocks, numThreadsPerBlock>>>(\n+        tensor.data_ptr<scalar_t>(), tensor.numel());\n+    C10_CUDA_KERNEL_LAUNCH_CHECK();\n+  });\n+\n+}\n+\n+} // namespace c10d\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include <ATen/Dispatch.h>\n+#include <ATen/cuda/CUDAContext.h>\n+#include <c10/cuda/CUDAGuard.h>\n+#include <torch/csrc/distributed/c10d/Utils.hpp>\n+#include <torch/torch.h>\n+#include <algorithm>\n+\n+namespace c10d {\n+\n+// CUDA kernel to check if data has NAN, device side assert\n+// is raised if NAN is found\n+template <typename T>\n+__global__ void checkForNaN(T* data, size_t size) {\n+  size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n+  size_t stride = blockDim.x * gridDim.x;\n+\n+  for (size_t i = tid; i < size; i += stride) {\n+    CUDA_KERNEL_ASSERT(!isnan(data[i]));\n+  }\n+}\n+\n+// CHECK if a Tensor contains NAN in any of its element\n+void checkForNan(const at::Tensor& tensor) {\n+  // skip check for non float types\n+  if (!torch::is_floating_point(tensor)) {\n+    return;\n+  }\n+  const size_t maxNumThreadsPerBlock = 512;\n+  const size_t maxNumBlocks = 24;\n+  const size_t numThreadsPerBlock =\n+      std::min<size_t>(maxNumThreadsPerBlock, tensor.numel());\n+\n+  const size_t numBlocks = std::min<size_t>(\n+      maxNumBlocks,\n+      (tensor.numel() + numThreadsPerBlock - 1) / numThreadsPerBlock);\n+\n+  AT_DISPATCH_FLOATING_TYPES_AND_HALF(tensor.scalar_type(), \"checkForNaN\", [&] {\n+    checkForNaN<scalar_t><<<numBlocks, numThreadsPerBlock>>>(\n+        tensor.data_ptr<scalar_t>(), tensor.numel());\n+    C10_CUDA_KERNEL_LAUNCH_CHECK();\n+  });\n+\n+}\n+\n+} // namespace c10d\n",
            "whole_hunk": "@@ -0,0 +1,45 @@\n+#include <ATen/Dispatch.h>\n+#include <ATen/cuda/CUDAContext.h>\n+#include <c10/cuda/CUDAGuard.h>\n+#include <torch/csrc/distributed/c10d/Utils.hpp>\n+#include <torch/torch.h>\n+#include <algorithm>\n+\n+namespace c10d {\n+\n+// CUDA kernel to check if data has NAN, device side assert\n+// is raised if NAN is found\n+template <typename T>\n+__global__ void checkForNaN(T* data, size_t size) {\n+  size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n+  size_t stride = blockDim.x * gridDim.x;\n+\n+  for (size_t i = tid; i < size; i += stride) {\n+    CUDA_KERNEL_ASSERT(!isnan(data[i]));\n+  }\n+}\n+\n+// CHECK if a Tensor contains NAN in any of its element\n+void checkForNan(const at::Tensor& tensor) {\n+  // skip check for non float types\n+  if (!torch::is_floating_point(tensor)) {\n+    return;\n+  }\n+  const size_t maxNumThreadsPerBlock = 512;\n+  const size_t maxNumBlocks = 24;\n+  const size_t numThreadsPerBlock =\n+      std::min<size_t>(maxNumThreadsPerBlock, tensor.numel());\n+\n+  const size_t numBlocks = std::min<size_t>(\n+      maxNumBlocks,\n+      (tensor.numel() + numThreadsPerBlock - 1) / numThreadsPerBlock);\n+\n+  AT_DISPATCH_FLOATING_TYPES_AND_HALF(tensor.scalar_type(), \"checkForNaN\", [&] {\n+    checkForNaN<scalar_t><<<numBlocks, numThreadsPerBlock>>>(\n+        tensor.data_ptr<scalar_t>(), tensor.numel());\n+    C10_CUDA_KERNEL_LAUNCH_CHECK();\n+  });\n+\n+}\n+\n+} // namespace c10d\n"
        },
        {
            "name": "Utils.hpp",
            "path": "torch/csrc/distributed/c10d/Utils.hpp",
            "patches": [
                {
                    "old_start": 612,
                    "old_length": 6,
                    "new_start": 612,
                    "new_length": 8,
                    "hunk": "@@ -612,6 +612,8 @@ using SizeType = uint64_t;\n // Since SOCKET_ERROR = -1 in MSVC, so also leverage SYSCHECK_ERR_RETURN_NEG1\n #define SYSCHECK_ERR_RETURN_NEG1(expr) SYSCHECK(expr, __output != -1)\n \n+void checkForNan(const at::Tensor& tensor);\n+\n namespace tcputil {\n \n // Send and receive"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+void checkForNan(const at::Tensor& tensor);\n+\n",
            "whole_hunk": "@@ -612,6 +612,8 @@ using SizeType = uint64_t;\n // Since SOCKET_ERROR = -1 in MSVC, so also leverage SYSCHECK_ERR_RETURN_NEG1\n #define SYSCHECK_ERR_RETURN_NEG1(expr) SYSCHECK(expr, __output != -1)\n \n+void checkForNan(const at::Tensor& tensor);\n+\n namespace tcputil {\n \n // Send and receive"
        }
    ]
},
{
    "Id": 386,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/ca15671c303b4d6fa37373ec65935f8b448c6155",
    "date": "2023-12-08T11:55:21+00:00",
    "message": "Fix failing test_invalid_input_csr_large (#114940)\n\nThe test introduced in #102530 has a bug:\nConstruction of `crow_indices` raises an exception: \"value cannot be converted to type int32 without overflow\" which is obviously correct.\nThis makes the test fail which is supposed to check for an overflow in nnz.\nFix by making the construction of `crow_indices` pass although with an invalid value which would error later but triggers the correct check.\n\nGiven that I'm not sure it is even worth checking for an overflow in nnz:\n- `crow_indices[..., -1] == nnz` is already enforced\n- this can only hold if `crow_indices` is able to hold `nnz` without overflow\n- `col_indices` has to be of the same type as `crow_indices`\n- Hence the type of `col_indices` has to be able to hold the value of `nnz`\n\nSo in conclusion: The situation being checked for cannot reasonably occur\n\nCC @pearu as the test author for additional insight\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114940\nApproved by: https://github.com/pearu, https://github.com/cpuhrsch",
    "label": "YES",
    "changes": [
        {
            "name": "test_sparse_csr.py",
            "path": "test/test_sparse_csr.py",
            "patches": [
                {
                    "old_start": 866,
                    "old_length": 7,
                    "new_start": 866,
                    "new_length": 11,
                    "hunk": "@@ -866,7 +866,11 @@ class TestSparseCompressed(TestCase):\n \n         nnz = 2 ** 31\n         with self.assertRaisesRegex(RuntimeError, '32-bit integer overflow in nnz'):\n-            torch.sparse_csr_tensor(torch.tensor([0, nnz // 2, nnz], dtype=torch.int32),\n+            # nnz cannot be stored in int32 crow_indices\n+            # but the `crow_indices[..., -1] == nnz`` check happens after the overflow validation\n+            # So we can use `nnz - 1` here to avoid `value cannot be converted to type int32 without overflow`\n+            # during construction of crow_indices\n+            torch.sparse_csr_tensor(torch.tensor([0, nnz // 2, nnz - 1], dtype=torch.int32),\n                                     torch.arange(nnz // 2, dtype=torch.int32).repeat(2),\n                                     torch.ones(nnz, dtype=torch.int8), (2, nnz // 2))\n         torch.sparse_csr_tensor(torch.tensor([0, nnz // 2, nnz], dtype=torch.int64),"
                }
            ],
            "whole_deleted": "-            torch.sparse_csr_tensor(torch.tensor([0, nnz // 2, nnz], dtype=torch.int32),\n",
            "whole_added": "+            # nnz cannot be stored in int32 crow_indices\n+            # but the `crow_indices[..., -1] == nnz`` check happens after the overflow validation\n+            # So we can use `nnz - 1` here to avoid `value cannot be converted to type int32 without overflow`\n+            # during construction of crow_indices\n+            torch.sparse_csr_tensor(torch.tensor([0, nnz // 2, nnz - 1], dtype=torch.int32),\n",
            "whole_hunk": "@@ -866,7 +866,11 @@ class TestSparseCompressed(TestCase):\n \n         nnz = 2 ** 31\n         with self.assertRaisesRegex(RuntimeError, '32-bit integer overflow in nnz'):\n-            torch.sparse_csr_tensor(torch.tensor([0, nnz // 2, nnz], dtype=torch.int32),\n+            # nnz cannot be stored in int32 crow_indices\n+            # but the `crow_indices[..., -1] == nnz`` check happens after the overflow validation\n+            # So we can use `nnz - 1` here to avoid `value cannot be converted to type int32 without overflow`\n+            # during construction of crow_indices\n+            torch.sparse_csr_tensor(torch.tensor([0, nnz // 2, nnz - 1], dtype=torch.int32),\n                                     torch.arange(nnz // 2, dtype=torch.int32).repeat(2),\n                                     torch.ones(nnz, dtype=torch.int8), (2, nnz // 2))\n         torch.sparse_csr_tensor(torch.tensor([0, nnz // 2, nnz], dtype=torch.int64),"
        }
    ]
},
{
    "Id": 520,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/62732bdcdb8b6112e01366d4ad1c2a50e61da1ed",
    "date": "2023-09-14T01:40:49+00:00",
    "message": "[ez][inductor][fx passes] quick fix for invalid nodes (#109234)\n\nSummary: As title.Need to check whether node is valid before fusion\n\nTest Plan: To add test\n\nDifferential Revision: D49241525\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109234\nApproved by: https://github.com/yanboliang",
    "label": "YES",
    "changes": [
        {
            "name": "group_batch_fusion.py",
            "path": "torch/_inductor/fx_passes/group_batch_fusion.py",
            "patches": [
                {
                    "old_start": 214,
                    "old_length": 6,
                    "new_start": 214,
                    "new_length": 8,
                    "hunk": "@@ -214,6 +214,8 @@ def is_linear_node_can_be_fused(node):\n     weight = get_arg_value(node, 1, \"weight\")\n     return (\n         is_node_meta_valid(node)\n+        and is_node_meta_valid(input)\n+        and is_node_meta_valid(weight)\n         and len(input.meta[\"example_value\"].shape) == 2\n         and len(weight.meta[\"example_value\"].shape) == 2\n     )"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        and is_node_meta_valid(input)\n+        and is_node_meta_valid(weight)\n",
            "whole_hunk": "@@ -214,6 +214,8 @@ def is_linear_node_can_be_fused(node):\n     weight = get_arg_value(node, 1, \"weight\")\n     return (\n         is_node_meta_valid(node)\n+        and is_node_meta_valid(input)\n+        and is_node_meta_valid(weight)\n         and len(input.meta[\"example_value\"].shape) == 2\n         and len(weight.meta[\"example_value\"].shape) == 2\n     )"
        }
    ]
},
{
    "Id": 506,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/175ccfc4c8443bcc65c87d9c942272d3ebf16b0b",
    "date": "2023-09-21T23:19:17+00:00",
    "message": "Verify flatbuffer module fields are initialized (#109794)\n\nFixes #109793\n\nAdd validation on flatbuffer module field to prevent segfault\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109794\nApproved by: https://github.com/malfet",
    "label": "YES",
    "changes": [
        {
            "name": "flatbuffer_loader.cpp",
            "path": "torch/csrc/jit/mobile/flatbuffer_loader.cpp",
            "patches": [
                {
                    "old_start": 291,
                    "old_length": 9,
                    "new_start": 291,
                    "new_length": 11,
                    "hunk": "@@ -291,9 +291,11 @@ mobile::Module FlatbufferLoader::parseModule(\n   module_parsed_ = false;\n \n   const auto* ivalues = module->ivalues();\n-  TORCH_CHECK(ivalues != nullptr, \"Corrupted ivalues field\")\n   TORCH_CHECK(\n-      reinterpret_cast<const char*>(ivalues) < end, \"Corrupted ivalues field\")\n+      ivalues && module->object_types(),\n+      \"Parsing flatbuffer module: Corrupted ivalues/object_types field\");\n+  TORCH_CHECK(\n+      reinterpret_cast<const char*>(ivalues) < end, \"Corrupted ivalues field\");\n   all_ivalues_.resize(ivalues->size());\n   all_types_.resize(module->object_types()->size());\n   storages_.resize(module->storage_data_size());"
                }
            ],
            "whole_deleted": "-  TORCH_CHECK(ivalues != nullptr, \"Corrupted ivalues field\")\n-      reinterpret_cast<const char*>(ivalues) < end, \"Corrupted ivalues field\")\n",
            "whole_added": "+      ivalues && module->object_types(),\n+      \"Parsing flatbuffer module: Corrupted ivalues/object_types field\");\n+  TORCH_CHECK(\n+      reinterpret_cast<const char*>(ivalues) < end, \"Corrupted ivalues field\");\n",
            "whole_hunk": "@@ -291,9 +291,11 @@ mobile::Module FlatbufferLoader::parseModule(\n   module_parsed_ = false;\n \n   const auto* ivalues = module->ivalues();\n-  TORCH_CHECK(ivalues != nullptr, \"Corrupted ivalues field\")\n   TORCH_CHECK(\n-      reinterpret_cast<const char*>(ivalues) < end, \"Corrupted ivalues field\")\n+      ivalues && module->object_types(),\n+      \"Parsing flatbuffer module: Corrupted ivalues/object_types field\");\n+  TORCH_CHECK(\n+      reinterpret_cast<const char*>(ivalues) < end, \"Corrupted ivalues field\");\n   all_ivalues_.resize(ivalues->size());\n   all_types_.resize(module->object_types()->size());\n   storages_.resize(module->storage_data_size());"
        }
    ]
},
{
    "Id": 280,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/3e382456c109820c836b104570104c3c5aff5632",
    "date": "2024-02-25T02:41:20+00:00",
    "message": "Fix compiler check (#120492)\n\nFixes #119304\n\n1. Add try catch to handle the compiler version check.\n2. Retry to query compiler version info.\n3. Return False if can't get compiler info twice.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/120492\nApproved by: https://github.com/ezyang",
    "label": "YES",
    "changes": [
        {
            "name": "cpp_extension.py",
            "path": "torch/utils/cpp_extension.py",
            "patches": [
                {
                    "old_start": 1350,
                    "old_length": 7,
                    "new_start": 1350,
                    "new_length": 13,
                    "hunk": "@@ -1350,7 +1350,13 @@ def check_compiler_is_gcc(compiler):\n \n     env = os.environ.copy()\n     env['LC_ALL'] = 'C'  # Don't localize output\n-    version_string = subprocess.check_output([compiler, '-v'], stderr=subprocess.STDOUT, env=env).decode(*SUBPROCESS_DECODE_ARGS)\n+    try:\n+        version_string = subprocess.check_output([compiler, '-v'], stderr=subprocess.STDOUT, env=env).decode(*SUBPROCESS_DECODE_ARGS)\n+    except Exception as e:\n+        try:\n+            version_string = subprocess.check_output([compiler, '--version'], stderr=subprocess.STDOUT, env=env).decode(*SUBPROCESS_DECODE_ARGS)\n+        except Exception as e:\n+            return False\n     # Check for 'gcc' or 'g++' for sccache wrapper\n     pattern = re.compile(\"^COLLECT_GCC=(.*)$\", re.MULTILINE)\n     results = re.findall(pattern, version_string)"
                }
            ],
            "whole_deleted": "-    version_string = subprocess.check_output([compiler, '-v'], stderr=subprocess.STDOUT, env=env).decode(*SUBPROCESS_DECODE_ARGS)\n",
            "whole_added": "+    try:\n+        version_string = subprocess.check_output([compiler, '-v'], stderr=subprocess.STDOUT, env=env).decode(*SUBPROCESS_DECODE_ARGS)\n+    except Exception as e:\n+        try:\n+            version_string = subprocess.check_output([compiler, '--version'], stderr=subprocess.STDOUT, env=env).decode(*SUBPROCESS_DECODE_ARGS)\n+        except Exception as e:\n+            return False\n",
            "whole_hunk": "@@ -1350,7 +1350,13 @@ def check_compiler_is_gcc(compiler):\n \n     env = os.environ.copy()\n     env['LC_ALL'] = 'C'  # Don't localize output\n-    version_string = subprocess.check_output([compiler, '-v'], stderr=subprocess.STDOUT, env=env).decode(*SUBPROCESS_DECODE_ARGS)\n+    try:\n+        version_string = subprocess.check_output([compiler, '-v'], stderr=subprocess.STDOUT, env=env).decode(*SUBPROCESS_DECODE_ARGS)\n+    except Exception as e:\n+        try:\n+            version_string = subprocess.check_output([compiler, '--version'], stderr=subprocess.STDOUT, env=env).decode(*SUBPROCESS_DECODE_ARGS)\n+        except Exception as e:\n+            return False\n     # Check for 'gcc' or 'g++' for sccache wrapper\n     pattern = re.compile(\"^COLLECT_GCC=(.*)$\", re.MULTILINE)\n     results = re.findall(pattern, version_string)"
        }
    ]
},
{
    "Id": 519,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/50a084070fe782e898edc8ba42c9e5f945c9c149",
    "date": "2023-09-14T01:45:25+00:00",
    "message": "[inductor][easy] Enable mypy checking for all inductor files that already pass (#109238)\n\nSummary: Let's just enable if mypy checking already passes. I checked all entries in the exclude list and enabled any that individually pass. Also needed one trivial change to a file already enabled.\n\nTest Plan: `lintrunner torch/_inductor/*.py torch/_inductor/*/*.py`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109238\nApproved by: https://github.com/eellison",
    "label": "NO",
    "changes": [
        {
            "name": ".lintrunner.toml",
            "path": ".lintrunner.toml",
            "patches": [
                {
                    "old_start": 195,
                    "old_length": 35,
                    "new_start": 195,
                    "new_length": 23,
                    "hunk": "@@ -195,35 +195,23 @@ include_patterns = [\n exclude_patterns = [\n     '**/fb/**',\n     'torch/_inductor/index_propagation.py',\n-    'torch/_inductor/coordinate_descent_tuner.py',\n     'torch/_inductor/debug.py',\n-    'torch/_inductor/hooks.py',\n     'torch/_inductor/bounds.py',\n-    'torch/_inductor/config.py',\n     'torch/_inductor/ir.py',\n-    'torch/_inductor/test_operators.py',\n-    'torch/_inductor/inductor_prims.py',\n     'torch/_inductor/scheduler.py',\n     'torch/_inductor/exc.py',\n     'torch/_inductor/sizevars.py',\n-    'torch/_inductor/triton_helpers.py',\n     'torch/_inductor/freezing.py',\n     'torch/_inductor/pattern_matcher.py',\n     'torch/_inductor/fx_utils.py',\n-    'torch/_inductor/cuda_properties.py',\n     'torch/_inductor/codegen/triton_foreach.py',\n-    'torch/_inductor/codegen/__init__.py',\n     'torch/_inductor/codegen/cpp.py',\n     'torch/_inductor/codegen/triton.py',\n     'torch/_inductor/fx_passes/split_cat.py',\n-    'torch/_inductor/fx_passes/binary_folding.py',\n-    'torch/_inductor/fx_passes/replace_random.py',\n     'torch/_inductor/fx_passes/joint_graph.py',\n     'torch/_inductor/fx_passes/pad_mm.py',\n-    'torch/_inductor/fx_passes/__init__.py',\n     'torch/_inductor/fx_passes/group_batch_fusion.py',\n     'torch/_inductor/fx_passes/pre_grad.py',\n-    'torch/_inductor/fx_passes/freezing_patterns.py',\n ]\n command = [\n     'python3',\n"
                }
            ],
            "whole_deleted": "-    'torch/_inductor/coordinate_descent_tuner.py',\n-    'torch/_inductor/hooks.py',\n-    'torch/_inductor/config.py',\n-    'torch/_inductor/test_operators.py',\n-    'torch/_inductor/inductor_prims.py',\n-    'torch/_inductor/triton_helpers.py',\n-    'torch/_inductor/cuda_properties.py',\n-    'torch/_inductor/codegen/__init__.py',\n-    'torch/_inductor/fx_passes/binary_folding.py',\n-    'torch/_inductor/fx_passes/replace_random.py',\n-    'torch/_inductor/fx_passes/__init__.py',\n-    'torch/_inductor/fx_passes/freezing_patterns.py',\n",
            "whole_added": "",
            "whole_hunk": "@@ -195,35 +195,23 @@ include_patterns = [\n exclude_patterns = [\n     '**/fb/**',\n     'torch/_inductor/index_propagation.py',\n-    'torch/_inductor/coordinate_descent_tuner.py',\n     'torch/_inductor/debug.py',\n-    'torch/_inductor/hooks.py',\n     'torch/_inductor/bounds.py',\n-    'torch/_inductor/config.py',\n     'torch/_inductor/ir.py',\n-    'torch/_inductor/test_operators.py',\n-    'torch/_inductor/inductor_prims.py',\n     'torch/_inductor/scheduler.py',\n     'torch/_inductor/exc.py',\n     'torch/_inductor/sizevars.py',\n-    'torch/_inductor/triton_helpers.py',\n     'torch/_inductor/freezing.py',\n     'torch/_inductor/pattern_matcher.py',\n     'torch/_inductor/fx_utils.py',\n-    'torch/_inductor/cuda_properties.py',\n     'torch/_inductor/codegen/triton_foreach.py',\n-    'torch/_inductor/codegen/__init__.py',\n     'torch/_inductor/codegen/cpp.py',\n     'torch/_inductor/codegen/triton.py',\n     'torch/_inductor/fx_passes/split_cat.py',\n-    'torch/_inductor/fx_passes/binary_folding.py',\n-    'torch/_inductor/fx_passes/replace_random.py',\n     'torch/_inductor/fx_passes/joint_graph.py',\n     'torch/_inductor/fx_passes/pad_mm.py',\n-    'torch/_inductor/fx_passes/__init__.py',\n     'torch/_inductor/fx_passes/group_batch_fusion.py',\n     'torch/_inductor/fx_passes/pre_grad.py',\n-    'torch/_inductor/fx_passes/freezing_patterns.py',\n ]\n command = [\n     'python3',\n"
        },
        {
            "name": "codecache.py",
            "path": "torch/_inductor/codecache.py",
            "patches": [
                {
                    "old_start": 1410,
                    "old_length": 7,
                    "new_start": 1410,
                    "new_length": 7,
                    "hunk": "@@ -1410,7 +1410,7 @@ class TritonCodeCache:\n         return getattr(mod, kernel_name)\n \n \n-def _cuda_compiler() -> str:\n+def _cuda_compiler() -> Optional[str]:\n     if cuda_env.nvcc_exist(config.cuda.cuda_cxx):\n         return config.cuda.cuda_cxx\n     if cuda_env.nvcc_exist(os.getenv(\"CUDACXX\")):"
                }
            ],
            "whole_deleted": "-def _cuda_compiler() -> str:\n",
            "whole_added": "+def _cuda_compiler() -> Optional[str]:\n",
            "whole_hunk": "@@ -1410,7 +1410,7 @@ class TritonCodeCache:\n         return getattr(mod, kernel_name)\n \n \n-def _cuda_compiler() -> str:\n+def _cuda_compiler() -> Optional[str]:\n     if cuda_env.nvcc_exist(config.cuda.cuda_cxx):\n         return config.cuda.cuda_cxx\n     if cuda_env.nvcc_exist(os.getenv(\"CUDACXX\")):"
        }
    ]
},
{
    "Id": 393,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/e06bff8bbea84ce672285bc34690b2e45a1b63ab",
    "date": "2023-12-05T15:02:17+00:00",
    "message": "[AOTI] Handle empty input args (#114682)\n\nSummary: When the model takes no inputs, AOTInductor relies on checking weights to figure out which device to compile the model into. Currently recording buffer device type happens too late, and this PR fixes that.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114682\nApproved by: https://github.com/chenyang78",
    "label": "YES",
    "changes": [
        {
            "name": "test_aot_inductor.py",
            "path": "test/inductor/test_aot_inductor.py",
            "patches": [
                {
                    "old_start": 1465,
                    "old_length": 6,
                    "new_start": 1465,
                    "new_length": 20,
                    "hunk": "@@ -1465,6 +1465,20 @@ class AOTInductorTestsTemplate:\n         inputs = (torch.rand(4, 4, 4, 4, device=self.device),)\n         self.check_model(Model(4), inputs)\n \n+    def test_no_args(self):\n+        class Model(torch.nn.Module):\n+            def __init__(self, m, n):\n+                super().__init__()\n+                self.weight = torch.nn.Parameter(\n+                    torch.randn(m, n),\n+                )\n+                self.alpha = torch.nn.Parameter(torch.randn(m, n))\n+\n+            def forward(self):\n+                return self.weight * self.alpha\n+\n+        self.check_model(Model(6, 4), ())\n+\n \n common_utils.instantiate_parametrized_tests(AOTInductorTestsTemplate)\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    def test_no_args(self):\n+        class Model(torch.nn.Module):\n+            def __init__(self, m, n):\n+                super().__init__()\n+                self.weight = torch.nn.Parameter(\n+                    torch.randn(m, n),\n+                )\n+                self.alpha = torch.nn.Parameter(torch.randn(m, n))\n+\n+            def forward(self):\n+                return self.weight * self.alpha\n+\n+        self.check_model(Model(6, 4), ())\n+\n",
            "whole_hunk": "@@ -1465,6 +1465,20 @@ class AOTInductorTestsTemplate:\n         inputs = (torch.rand(4, 4, 4, 4, device=self.device),)\n         self.check_model(Model(4), inputs)\n \n+    def test_no_args(self):\n+        class Model(torch.nn.Module):\n+            def __init__(self, m, n):\n+                super().__init__()\n+                self.weight = torch.nn.Parameter(\n+                    torch.randn(m, n),\n+                )\n+                self.alpha = torch.nn.Parameter(torch.randn(m, n))\n+\n+            def forward(self):\n+                return self.weight * self.alpha\n+\n+        self.check_model(Model(6, 4), ())\n+\n \n common_utils.instantiate_parametrized_tests(AOTInductorTestsTemplate)\n \n"
        },
        {
            "name": "graph.py",
            "path": "torch/_inductor/graph.py",
            "patches": [
                {
                    "old_start": 472,
                    "old_length": 9,
                    "new_start": 472,
                    "new_length": 10,
                    "hunk": "@@ -472,9 +472,10 @@ class GraphLowering(torch.fx.Interpreter):\n             self._warned_fallback.add(name)\n             perf_hint_log.info(\"Using FallbackKernel: %s\", name)\n \n-    def add_device_idx(self, idx: Optional[int]):\n-        if idx is not None:\n-            self.device_idxs.add(idx)\n+    def add_device_info(self, device: torch.device):\n+        self.device_types.add(device.type)\n+        if device.index is not None:\n+            self.device_idxs.add(device.index)\n \n     @property\n     def fake_mode(self):\n"
                },
                {
                    "old_start": 521,
                    "old_length": 6,
                    "new_start": 522,
                    "new_length": 9,
                    "hunk": "@@ -521,6 +522,9 @@ class GraphLowering(torch.fx.Interpreter):\n         name = f\"buf{len(self.buffers)}\"\n         self.buffers.append(buffer)\n         self.name_to_buffer[name] = buffer\n+        # Skip empty CPU tensor so that CUDA graphs can succeed, see https://github.com/pytorch/pytorch/pull/114144\n+        if not isinstance(buffer, ir.ComputedBuffer) or not buffer.is_zero_elements():\n+            self.add_device_info(buffer.get_device())\n         return name\n \n     def register_list(self, buffer_names: List[str]):\n"
                },
                {
                    "old_start": 645,
                    "old_length": 8,
                    "new_start": 649,
                    "new_length": 7,
                    "hunk": "@@ -645,8 +649,7 @@ class GraphLowering(torch.fx.Interpreter):\n         )\n         self.graph_inputs[target] = tensor\n         self.graph_inputs_original[target] = tensor.data.data\n-        self.device_types.add(example.device.type)\n-        self.add_device_idx(example.device.index)\n+        self.add_device_info(example.device)\n         return tensor\n \n     def call_function(self, target, args, kwargs):\n"
                },
                {
                    "old_start": 979,
                    "old_length": 10,
                    "new_start": 982,
                    "new_length": 6,
                    "hunk": "@@ -979,10 +982,6 @@ class GraphLowering(torch.fx.Interpreter):\n             return\n \n         device_types = self.device_types.copy()\n-        # In terms of some operations that don't have input tensors, we need to\n-        # check the device of the buffers.\n-        for buffer in self.buffers:\n-            device_types.add(buffer.get_device().type)\n         device_types.discard(\"cpu\")\n         # TODO(Eikan): Only support mixing cpu and other device now.\n         assert len(device_types) <= 1, \"Does not support mixing {}\".format(\n"
                },
                {
                    "old_start": 1015,
                    "old_length": 7,
                    "new_start": 1014,
                    "new_length": 7,
                    "hunk": "@@ -1015,7 +1014,7 @@ class GraphLowering(torch.fx.Interpreter):\n                 else:\n                     assert isinstance(\n                         x, torch.Tensor\n-                    ), \"Unknown type when creating real inputs\"\n+                    ), \"Unknown type when creating real inputs\" + str(type(x))\n                     return x\n \n             with torch.utils._python_dispatch._disable_current_modes():\n"
                }
            ],
            "whole_deleted": "-    def add_device_idx(self, idx: Optional[int]):\n-        if idx is not None:\n-            self.device_idxs.add(idx)\n-        self.device_types.add(example.device.type)\n-        self.add_device_idx(example.device.index)\n-        # In terms of some operations that don't have input tensors, we need to\n-        # check the device of the buffers.\n-        for buffer in self.buffers:\n-            device_types.add(buffer.get_device().type)\n-                    ), \"Unknown type when creating real inputs\"\n",
            "whole_added": "+    def add_device_info(self, device: torch.device):\n+        self.device_types.add(device.type)\n+        if device.index is not None:\n+            self.device_idxs.add(device.index)\n+        # Skip empty CPU tensor so that CUDA graphs can succeed, see https://github.com/pytorch/pytorch/pull/114144\n+        if not isinstance(buffer, ir.ComputedBuffer) or not buffer.is_zero_elements():\n+            self.add_device_info(buffer.get_device())\n+        self.add_device_info(example.device)\n+                    ), \"Unknown type when creating real inputs\" + str(type(x))\n",
            "whole_hunk": "@@ -472,9 +472,10 @@ class GraphLowering(torch.fx.Interpreter):\n             self._warned_fallback.add(name)\n             perf_hint_log.info(\"Using FallbackKernel: %s\", name)\n \n-    def add_device_idx(self, idx: Optional[int]):\n-        if idx is not None:\n-            self.device_idxs.add(idx)\n+    def add_device_info(self, device: torch.device):\n+        self.device_types.add(device.type)\n+        if device.index is not None:\n+            self.device_idxs.add(device.index)\n \n     @property\n     def fake_mode(self):\n@@ -521,6 +522,9 @@ class GraphLowering(torch.fx.Interpreter):\n         name = f\"buf{len(self.buffers)}\"\n         self.buffers.append(buffer)\n         self.name_to_buffer[name] = buffer\n+        # Skip empty CPU tensor so that CUDA graphs can succeed, see https://github.com/pytorch/pytorch/pull/114144\n+        if not isinstance(buffer, ir.ComputedBuffer) or not buffer.is_zero_elements():\n+            self.add_device_info(buffer.get_device())\n         return name\n \n     def register_list(self, buffer_names: List[str]):\n@@ -645,8 +649,7 @@ class GraphLowering(torch.fx.Interpreter):\n         )\n         self.graph_inputs[target] = tensor\n         self.graph_inputs_original[target] = tensor.data.data\n-        self.device_types.add(example.device.type)\n-        self.add_device_idx(example.device.index)\n+        self.add_device_info(example.device)\n         return tensor\n \n     def call_function(self, target, args, kwargs):\n@@ -979,10 +982,6 @@ class GraphLowering(torch.fx.Interpreter):\n             return\n \n         device_types = self.device_types.copy()\n-        # In terms of some operations that don't have input tensors, we need to\n-        # check the device of the buffers.\n-        for buffer in self.buffers:\n-            device_types.add(buffer.get_device().type)\n         device_types.discard(\"cpu\")\n         # TODO(Eikan): Only support mixing cpu and other device now.\n         assert len(device_types) <= 1, \"Does not support mixing {}\".format(\n@@ -1015,7 +1014,7 @@ class GraphLowering(torch.fx.Interpreter):\n                 else:\n                     assert isinstance(\n                         x, torch.Tensor\n-                    ), \"Unknown type when creating real inputs\"\n+                    ), \"Unknown type when creating real inputs\" + str(type(x))\n                     return x\n \n             with torch.utils._python_dispatch._disable_current_modes():\n"
        },
        {
            "name": "ir.py",
            "path": "torch/_inductor/ir.py",
            "patches": [
                {
                    "old_start": 4160,
                    "old_length": 10,
                    "new_start": 4160,
                    "new_length": 8,
                    "hunk": "@@ -4160,10 +4160,8 @@ class DeviceCopy(ExternKernelOut):\n         ):\n             return x.constant_to_device(device)\n \n-        V.graph.device_types.add(device.type)\n-        V.graph.add_device_idx(device.index)\n-        V.graph.device_types.add(x.get_device().type)\n-        V.graph.add_device_idx(x.get_device().index)\n+        V.graph.add_device_info(device)\n+        V.graph.add_device_info(x.get_device())\n \n         developer_warning(\"DeviceCopy in input program\")\n         return DeviceCopy(\n"
                }
            ],
            "whole_deleted": "-        V.graph.device_types.add(device.type)\n-        V.graph.add_device_idx(device.index)\n-        V.graph.device_types.add(x.get_device().type)\n-        V.graph.add_device_idx(x.get_device().index)\n",
            "whole_added": "+        V.graph.add_device_info(device)\n+        V.graph.add_device_info(x.get_device())\n",
            "whole_hunk": "@@ -4160,10 +4160,8 @@ class DeviceCopy(ExternKernelOut):\n         ):\n             return x.constant_to_device(device)\n \n-        V.graph.device_types.add(device.type)\n-        V.graph.add_device_idx(device.index)\n-        V.graph.device_types.add(x.get_device().type)\n-        V.graph.add_device_idx(x.get_device().index)\n+        V.graph.add_device_info(device)\n+        V.graph.add_device_info(x.get_device())\n \n         developer_warning(\"DeviceCopy in input program\")\n         return DeviceCopy(\n"
        },
        {
            "name": "scheduler.py",
            "path": "torch/_inductor/scheduler.py",
            "patches": [
                {
                    "old_start": 2130,
                    "old_length": 8,
                    "new_start": 2130,
                    "new_length": 7,
                    "hunk": "@@ -2130,8 +2130,7 @@ class Scheduler:\n         assert (\n             device.type != \"cuda\" or device.index is not None\n         ), f\"{device} should have been normalized in lowering\"\n-        V.graph.device_types.add(device.type)\n-        V.graph.add_device_idx(device.index)\n+        V.graph.add_device_info(device)\n \n         device_scheduling = get_scheduling_for_device(device.type)\n         if device_scheduling is None:"
                }
            ],
            "whole_deleted": "-        V.graph.device_types.add(device.type)\n-        V.graph.add_device_idx(device.index)\n",
            "whole_added": "+        V.graph.add_device_info(device)\n",
            "whole_hunk": "@@ -2130,8 +2130,7 @@ class Scheduler:\n         assert (\n             device.type != \"cuda\" or device.index is not None\n         ), f\"{device} should have been normalized in lowering\"\n-        V.graph.device_types.add(device.type)\n-        V.graph.add_device_idx(device.index)\n+        V.graph.add_device_info(device)\n \n         device_scheduling = get_scheduling_for_device(device.type)\n         if device_scheduling is None:"
        }
    ]
},
{
    "Id": 70,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/856541c701f10e075c13cb4be31006ac234fa451",
    "date": "2024-06-23T00:13:23+00:00",
    "message": "[custom_op] support default dtype values (#129189)\n\nThis PR:\n- moves some of the dtype-string utilities into ScalarType.{h, cpp}\n- adds a new utility to get a mapping from dtype name to the C++ dtype\n- the perser now checks if the string is a dtype name; if it is then it\n  pulls the c++ dtype from the mapping.\n\nTest Plan:\n- new tests\nPull Request resolved: https://github.com/pytorch/pytorch/pull/129189\nApproved by: https://github.com/albanD\nghstack dependencies: #129177, #129178, #129179",
    "label": "NO",
    "changes": [
        {
            "name": "ScalarType.cpp",
            "path": "c10/core/ScalarType.cpp",
            "patches": [
                {
                    "old_start": 128,
                    "old_length": 4,
                    "new_start": 128,
                    "new_length": 112,
                    "hunk": "@@ -128,4 +128,112 @@ ScalarType promoteTypes(ScalarType a, ScalarType b) {\n   return _promoteTypesLookup[ix_a][ix_b];\n }\n \n+std::pair<std::string, std::string> getDtypeNames(c10::ScalarType scalarType) {\n+  switch (scalarType) {\n+    case c10::ScalarType::UInt1:\n+      return std::make_pair(\"uint1\", \"bit\");\n+    case c10::ScalarType::UInt2:\n+      return std::make_pair(\"uint2\", \"\");\n+    case c10::ScalarType::UInt3:\n+      return std::make_pair(\"uint3\", \"\");\n+    case c10::ScalarType::UInt4:\n+      return std::make_pair(\"uint4\", \"\");\n+    case c10::ScalarType::UInt5:\n+      return std::make_pair(\"uint5\", \"\");\n+    case c10::ScalarType::UInt6:\n+      return std::make_pair(\"uint6\", \"\");\n+    case c10::ScalarType::UInt7:\n+      return std::make_pair(\"uint7\", \"\");\n+    case c10::ScalarType::Byte:\n+      // no \"byte\" because byte is signed in numpy and we overload\n+      // byte to mean bool often\n+      return std::make_pair(\"uint8\", \"\");\n+    case c10::ScalarType::UInt16:\n+      return std::make_pair(\"uint16\", \"\");\n+    case c10::ScalarType::UInt32:\n+      return std::make_pair(\"uint32\", \"\");\n+    case c10::ScalarType::UInt64:\n+      return std::make_pair(\"uint64\", \"\");\n+    case c10::ScalarType::Char:\n+      // no \"char\" because it is not consistently signed or unsigned; we want\n+      // to move to int8\n+      return std::make_pair(\"int8\", \"\");\n+    case c10::ScalarType::Double:\n+      return std::make_pair(\"float64\", \"double\");\n+    case c10::ScalarType::Float:\n+      return std::make_pair(\"float32\", \"float\");\n+    case c10::ScalarType::Int:\n+      return std::make_pair(\"int32\", \"int\");\n+    case c10::ScalarType::Long:\n+      return std::make_pair(\"int64\", \"long\");\n+    case c10::ScalarType::Short:\n+      return std::make_pair(\"int16\", \"short\");\n+    case c10::ScalarType::Half:\n+      return std::make_pair(\"float16\", \"half\");\n+    case c10::ScalarType::ComplexHalf:\n+      return std::make_pair(\"complex32\", \"chalf\");\n+    case c10::ScalarType::ComplexFloat:\n+      return std::make_pair(\"complex64\", \"cfloat\");\n+    case c10::ScalarType::ComplexDouble:\n+      return std::make_pair(\"complex128\", \"cdouble\");\n+    case c10::ScalarType::Bool:\n+      return std::make_pair(\"bool\", \"\");\n+    case c10::ScalarType::QInt8:\n+      return std::make_pair(\"qint8\", \"\");\n+    case c10::ScalarType::QUInt8:\n+      return std::make_pair(\"quint8\", \"\");\n+    case c10::ScalarType::QInt32:\n+      return std::make_pair(\"qint32\", \"\");\n+    case c10::ScalarType::BFloat16:\n+      return std::make_pair(\"bfloat16\", \"\");\n+    case c10::ScalarType::QUInt4x2:\n+      return std::make_pair(\"quint4x2\", \"\");\n+    case c10::ScalarType::QUInt2x4:\n+      return std::make_pair(\"quint2x4\", \"\");\n+    case c10::ScalarType::Bits1x8:\n+      return std::make_pair(\"bits1x8\", \"\");\n+    case c10::ScalarType::Bits2x4:\n+      return std::make_pair(\"bits2x4\", \"\");\n+    case c10::ScalarType::Bits4x2:\n+      return std::make_pair(\"bits4x2\", \"\");\n+    case c10::ScalarType::Bits8:\n+      return std::make_pair(\"bits8\", \"\");\n+    case c10::ScalarType::Bits16:\n+      return std::make_pair(\"bits16\", \"\");\n+    case c10::ScalarType::Float8_e5m2:\n+      return std::make_pair(\"float8_e5m2\", \"\");\n+    case c10::ScalarType::Float8_e4m3fn:\n+      return std::make_pair(\"float8_e4m3fn\", \"\");\n+    case c10::ScalarType::Float8_e5m2fnuz:\n+      return std::make_pair(\"float8_e5m2fnuz\", \"\");\n+    case c10::ScalarType::Float8_e4m3fnuz:\n+      return std::make_pair(\"float8_e4m3fnuz\", \"\");\n+    default:\n+      throw std::runtime_error(\"Unimplemented scalar type\");\n+  }\n+}\n+\n+const std::unordered_map<std::string, ScalarType>& getStringToDtypeMap() {\n+  static std::unordered_map<std::string, ScalarType> result;\n+  if (!result.empty()) {\n+    return result;\n+  }\n+\n+#define DEFINE_SCALAR_TYPE(_1, n) c10::ScalarType::n,\n+\n+  auto all_scalar_types = {\n+      AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS(DEFINE_SCALAR_TYPE)};\n+\n+#undef DEFINE_SCALAR_TYPE\n+\n+  for (auto scalar_type : all_scalar_types) {\n+    auto names = getDtypeNames(scalar_type);\n+    result[std::get<0>(names)] = scalar_type;\n+    if (!std::get<1>(names).empty()) {\n+      result[std::get<1>(names)] = scalar_type;\n+    }\n+  }\n+  return result;\n+}\n+\n } // namespace c10\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+std::pair<std::string, std::string> getDtypeNames(c10::ScalarType scalarType) {\n+  switch (scalarType) {\n+    case c10::ScalarType::UInt1:\n+      return std::make_pair(\"uint1\", \"bit\");\n+    case c10::ScalarType::UInt2:\n+      return std::make_pair(\"uint2\", \"\");\n+    case c10::ScalarType::UInt3:\n+      return std::make_pair(\"uint3\", \"\");\n+    case c10::ScalarType::UInt4:\n+      return std::make_pair(\"uint4\", \"\");\n+    case c10::ScalarType::UInt5:\n+      return std::make_pair(\"uint5\", \"\");\n+    case c10::ScalarType::UInt6:\n+      return std::make_pair(\"uint6\", \"\");\n+    case c10::ScalarType::UInt7:\n+      return std::make_pair(\"uint7\", \"\");\n+    case c10::ScalarType::Byte:\n+      // no \"byte\" because byte is signed in numpy and we overload\n+      // byte to mean bool often\n+      return std::make_pair(\"uint8\", \"\");\n+    case c10::ScalarType::UInt16:\n+      return std::make_pair(\"uint16\", \"\");\n+    case c10::ScalarType::UInt32:\n+      return std::make_pair(\"uint32\", \"\");\n+    case c10::ScalarType::UInt64:\n+      return std::make_pair(\"uint64\", \"\");\n+    case c10::ScalarType::Char:\n+      // no \"char\" because it is not consistently signed or unsigned; we want\n+      // to move to int8\n+      return std::make_pair(\"int8\", \"\");\n+    case c10::ScalarType::Double:\n+      return std::make_pair(\"float64\", \"double\");\n+    case c10::ScalarType::Float:\n+      return std::make_pair(\"float32\", \"float\");\n+    case c10::ScalarType::Int:\n+      return std::make_pair(\"int32\", \"int\");\n+    case c10::ScalarType::Long:\n+      return std::make_pair(\"int64\", \"long\");\n+    case c10::ScalarType::Short:\n+      return std::make_pair(\"int16\", \"short\");\n+    case c10::ScalarType::Half:\n+      return std::make_pair(\"float16\", \"half\");\n+    case c10::ScalarType::ComplexHalf:\n+      return std::make_pair(\"complex32\", \"chalf\");\n+    case c10::ScalarType::ComplexFloat:\n+      return std::make_pair(\"complex64\", \"cfloat\");\n+    case c10::ScalarType::ComplexDouble:\n+      return std::make_pair(\"complex128\", \"cdouble\");\n+    case c10::ScalarType::Bool:\n+      return std::make_pair(\"bool\", \"\");\n+    case c10::ScalarType::QInt8:\n+      return std::make_pair(\"qint8\", \"\");\n+    case c10::ScalarType::QUInt8:\n+      return std::make_pair(\"quint8\", \"\");\n+    case c10::ScalarType::QInt32:\n+      return std::make_pair(\"qint32\", \"\");\n+    case c10::ScalarType::BFloat16:\n+      return std::make_pair(\"bfloat16\", \"\");\n+    case c10::ScalarType::QUInt4x2:\n+      return std::make_pair(\"quint4x2\", \"\");\n+    case c10::ScalarType::QUInt2x4:\n+      return std::make_pair(\"quint2x4\", \"\");\n+    case c10::ScalarType::Bits1x8:\n+      return std::make_pair(\"bits1x8\", \"\");\n+    case c10::ScalarType::Bits2x4:\n+      return std::make_pair(\"bits2x4\", \"\");\n+    case c10::ScalarType::Bits4x2:\n+      return std::make_pair(\"bits4x2\", \"\");\n+    case c10::ScalarType::Bits8:\n+      return std::make_pair(\"bits8\", \"\");\n+    case c10::ScalarType::Bits16:\n+      return std::make_pair(\"bits16\", \"\");\n+    case c10::ScalarType::Float8_e5m2:\n+      return std::make_pair(\"float8_e5m2\", \"\");\n+    case c10::ScalarType::Float8_e4m3fn:\n+      return std::make_pair(\"float8_e4m3fn\", \"\");\n+    case c10::ScalarType::Float8_e5m2fnuz:\n+      return std::make_pair(\"float8_e5m2fnuz\", \"\");\n+    case c10::ScalarType::Float8_e4m3fnuz:\n+      return std::make_pair(\"float8_e4m3fnuz\", \"\");\n+    default:\n+      throw std::runtime_error(\"Unimplemented scalar type\");\n+  }\n+}\n+\n+const std::unordered_map<std::string, ScalarType>& getStringToDtypeMap() {\n+  static std::unordered_map<std::string, ScalarType> result;\n+  if (!result.empty()) {\n+    return result;\n+  }\n+\n+#define DEFINE_SCALAR_TYPE(_1, n) c10::ScalarType::n,\n+\n+  auto all_scalar_types = {\n+      AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS(DEFINE_SCALAR_TYPE)};\n+\n+#undef DEFINE_SCALAR_TYPE\n+\n+  for (auto scalar_type : all_scalar_types) {\n+    auto names = getDtypeNames(scalar_type);\n+    result[std::get<0>(names)] = scalar_type;\n+    if (!std::get<1>(names).empty()) {\n+      result[std::get<1>(names)] = scalar_type;\n+    }\n+  }\n+  return result;\n+}\n+\n",
            "whole_hunk": "@@ -128,4 +128,112 @@ ScalarType promoteTypes(ScalarType a, ScalarType b) {\n   return _promoteTypesLookup[ix_a][ix_b];\n }\n \n+std::pair<std::string, std::string> getDtypeNames(c10::ScalarType scalarType) {\n+  switch (scalarType) {\n+    case c10::ScalarType::UInt1:\n+      return std::make_pair(\"uint1\", \"bit\");\n+    case c10::ScalarType::UInt2:\n+      return std::make_pair(\"uint2\", \"\");\n+    case c10::ScalarType::UInt3:\n+      return std::make_pair(\"uint3\", \"\");\n+    case c10::ScalarType::UInt4:\n+      return std::make_pair(\"uint4\", \"\");\n+    case c10::ScalarType::UInt5:\n+      return std::make_pair(\"uint5\", \"\");\n+    case c10::ScalarType::UInt6:\n+      return std::make_pair(\"uint6\", \"\");\n+    case c10::ScalarType::UInt7:\n+      return std::make_pair(\"uint7\", \"\");\n+    case c10::ScalarType::Byte:\n+      // no \"byte\" because byte is signed in numpy and we overload\n+      // byte to mean bool often\n+      return std::make_pair(\"uint8\", \"\");\n+    case c10::ScalarType::UInt16:\n+      return std::make_pair(\"uint16\", \"\");\n+    case c10::ScalarType::UInt32:\n+      return std::make_pair(\"uint32\", \"\");\n+    case c10::ScalarType::UInt64:\n+      return std::make_pair(\"uint64\", \"\");\n+    case c10::ScalarType::Char:\n+      // no \"char\" because it is not consistently signed or unsigned; we want\n+      // to move to int8\n+      return std::make_pair(\"int8\", \"\");\n+    case c10::ScalarType::Double:\n+      return std::make_pair(\"float64\", \"double\");\n+    case c10::ScalarType::Float:\n+      return std::make_pair(\"float32\", \"float\");\n+    case c10::ScalarType::Int:\n+      return std::make_pair(\"int32\", \"int\");\n+    case c10::ScalarType::Long:\n+      return std::make_pair(\"int64\", \"long\");\n+    case c10::ScalarType::Short:\n+      return std::make_pair(\"int16\", \"short\");\n+    case c10::ScalarType::Half:\n+      return std::make_pair(\"float16\", \"half\");\n+    case c10::ScalarType::ComplexHalf:\n+      return std::make_pair(\"complex32\", \"chalf\");\n+    case c10::ScalarType::ComplexFloat:\n+      return std::make_pair(\"complex64\", \"cfloat\");\n+    case c10::ScalarType::ComplexDouble:\n+      return std::make_pair(\"complex128\", \"cdouble\");\n+    case c10::ScalarType::Bool:\n+      return std::make_pair(\"bool\", \"\");\n+    case c10::ScalarType::QInt8:\n+      return std::make_pair(\"qint8\", \"\");\n+    case c10::ScalarType::QUInt8:\n+      return std::make_pair(\"quint8\", \"\");\n+    case c10::ScalarType::QInt32:\n+      return std::make_pair(\"qint32\", \"\");\n+    case c10::ScalarType::BFloat16:\n+      return std::make_pair(\"bfloat16\", \"\");\n+    case c10::ScalarType::QUInt4x2:\n+      return std::make_pair(\"quint4x2\", \"\");\n+    case c10::ScalarType::QUInt2x4:\n+      return std::make_pair(\"quint2x4\", \"\");\n+    case c10::ScalarType::Bits1x8:\n+      return std::make_pair(\"bits1x8\", \"\");\n+    case c10::ScalarType::Bits2x4:\n+      return std::make_pair(\"bits2x4\", \"\");\n+    case c10::ScalarType::Bits4x2:\n+      return std::make_pair(\"bits4x2\", \"\");\n+    case c10::ScalarType::Bits8:\n+      return std::make_pair(\"bits8\", \"\");\n+    case c10::ScalarType::Bits16:\n+      return std::make_pair(\"bits16\", \"\");\n+    case c10::ScalarType::Float8_e5m2:\n+      return std::make_pair(\"float8_e5m2\", \"\");\n+    case c10::ScalarType::Float8_e4m3fn:\n+      return std::make_pair(\"float8_e4m3fn\", \"\");\n+    case c10::ScalarType::Float8_e5m2fnuz:\n+      return std::make_pair(\"float8_e5m2fnuz\", \"\");\n+    case c10::ScalarType::Float8_e4m3fnuz:\n+      return std::make_pair(\"float8_e4m3fnuz\", \"\");\n+    default:\n+      throw std::runtime_error(\"Unimplemented scalar type\");\n+  }\n+}\n+\n+const std::unordered_map<std::string, ScalarType>& getStringToDtypeMap() {\n+  static std::unordered_map<std::string, ScalarType> result;\n+  if (!result.empty()) {\n+    return result;\n+  }\n+\n+#define DEFINE_SCALAR_TYPE(_1, n) c10::ScalarType::n,\n+\n+  auto all_scalar_types = {\n+      AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS(DEFINE_SCALAR_TYPE)};\n+\n+#undef DEFINE_SCALAR_TYPE\n+\n+  for (auto scalar_type : all_scalar_types) {\n+    auto names = getDtypeNames(scalar_type);\n+    result[std::get<0>(names)] = scalar_type;\n+    if (!std::get<1>(names).empty()) {\n+      result[std::get<1>(names)] = scalar_type;\n+    }\n+  }\n+  return result;\n+}\n+\n } // namespace c10\n"
        },
        {
            "name": "ScalarType.h",
            "path": "c10/core/ScalarType.h",
            "patches": [
                {
                    "old_start": 22,
                    "old_length": 6,
                    "new_start": 22,
                    "new_length": 7,
                    "hunk": "@@ -22,6 +22,7 @@\n #include <limits>\n #include <ostream>\n #include <type_traits>\n+#include <unordered_map>\n \n namespace c10 {\n \n"
                },
                {
                    "old_start": 561,
                    "old_length": 4,
                    "new_start": 562,
                    "new_length": 12,
                    "hunk": "@@ -561,4 +562,12 @@ inline std::ostream& operator<<(\n   return stream << toString(scalar_type);\n }\n \n+// Returns a pair of strings representing the names for each dtype.\n+// The returned pair is (name, legacy_name_if_applicable)\n+C10_API std::pair<std::string, std::string> getDtypeNames(\n+    c10::ScalarType scalarType);\n+\n+// Returns a map of string name to dtype.\n+C10_API const std::unordered_map<std::string, ScalarType>& getStringToDtypeMap();\n+\n } // namespace c10\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include <unordered_map>\n+// Returns a pair of strings representing the names for each dtype.\n+// The returned pair is (name, legacy_name_if_applicable)\n+C10_API std::pair<std::string, std::string> getDtypeNames(\n+    c10::ScalarType scalarType);\n+\n+// Returns a map of string name to dtype.\n+C10_API const std::unordered_map<std::string, ScalarType>& getStringToDtypeMap();\n+\n",
            "whole_hunk": "@@ -22,6 +22,7 @@\n #include <limits>\n #include <ostream>\n #include <type_traits>\n+#include <unordered_map>\n \n namespace c10 {\n \n@@ -561,4 +562,12 @@ inline std::ostream& operator<<(\n   return stream << toString(scalar_type);\n }\n \n+// Returns a pair of strings representing the names for each dtype.\n+// The returned pair is (name, legacy_name_if_applicable)\n+C10_API std::pair<std::string, std::string> getDtypeNames(\n+    c10::ScalarType scalarType);\n+\n+// Returns a map of string name to dtype.\n+C10_API const std::unordered_map<std::string, ScalarType>& getStringToDtypeMap();\n+\n } // namespace c10\n"
        },
        {
            "name": "test_custom_ops.py",
            "path": "test/test_custom_ops.py",
            "patches": [
                {
                    "old_start": 2412,
                    "old_length": 13,
                    "new_start": 2412,
                    "new_length": 19,
                    "hunk": "@@ -2412,13 +2412,19 @@ class TestCustomOpAPI(TestCase):\n             c: bool = True,\n             d: int = 3,\n             e: str = \"foo\",\n+            f: torch.dtype = torch.float,\n+            g: torch.dtype = torch.float32,\n+            h: torch.dtype = torch.int,\n         ) -> Tensor:\n-            defaults.extend([a, b, c, d, e])\n+            defaults.extend([a, b, c, d, e, f, g, h])\n             return x.clone()\n \n         x = torch.randn(3)\n         f(x)\n-        self.assertEqual(defaults, [None, 3.14, True, 3, \"foo\"])\n+        self.assertEqual(\n+            defaults,\n+            [None, 3.14, True, 3, \"foo\", torch.float, torch.float32, torch.int],\n+        )\n \n     def test_mutated_error(self):\n         with self.assertRaisesRegex(\n"
                }
            ],
            "whole_deleted": "-            defaults.extend([a, b, c, d, e])\n-        self.assertEqual(defaults, [None, 3.14, True, 3, \"foo\"])\n",
            "whole_added": "+            f: torch.dtype = torch.float,\n+            g: torch.dtype = torch.float32,\n+            h: torch.dtype = torch.int,\n+            defaults.extend([a, b, c, d, e, f, g, h])\n+        self.assertEqual(\n+            defaults,\n+            [None, 3.14, True, 3, \"foo\", torch.float, torch.float32, torch.int],\n+        )\n",
            "whole_hunk": "@@ -2412,13 +2412,19 @@ class TestCustomOpAPI(TestCase):\n             c: bool = True,\n             d: int = 3,\n             e: str = \"foo\",\n+            f: torch.dtype = torch.float,\n+            g: torch.dtype = torch.float32,\n+            h: torch.dtype = torch.int,\n         ) -> Tensor:\n-            defaults.extend([a, b, c, d, e])\n+            defaults.extend([a, b, c, d, e, f, g, h])\n             return x.clone()\n \n         x = torch.randn(3)\n         f(x)\n-        self.assertEqual(defaults, [None, 3.14, True, 3, \"foo\"])\n+        self.assertEqual(\n+            defaults,\n+            [None, 3.14, True, 3, \"foo\", torch.float, torch.float32, torch.int],\n+        )\n \n     def test_mutated_error(self):\n         with self.assertRaisesRegex(\n"
        },
        {
            "name": "infer_schema.py",
            "path": "torch/_library/infer_schema.py",
            "patches": [
                {
                    "old_start": 79,
                    "old_length": 6,
                    "new_start": 79,
                    "new_length": 11,
                    "hunk": "@@ -79,6 +79,11 @@ def infer_schema(prototype_function: typing.Callable, mutates_args=()) -> str:\n                 default_repr = str(param.default)\n             elif isinstance(param.default, str):\n                 default_repr = f'\"{param.default}\"'\n+            elif isinstance(param.default, torch.dtype):\n+                dtype_repr = str(param.default)\n+                torch_dot = \"torch.\"\n+                assert dtype_repr.startswith(torch_dot)\n+                default_repr = dtype_repr[len(torch_dot) :]\n             else:\n                 error_fn(\n                     f\"Parameter {name} has an unsupported default value type {type(param.default)}. \"\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+            elif isinstance(param.default, torch.dtype):\n+                dtype_repr = str(param.default)\n+                torch_dot = \"torch.\"\n+                assert dtype_repr.startswith(torch_dot)\n+                default_repr = dtype_repr[len(torch_dot) :]\n",
            "whole_hunk": "@@ -79,6 +79,11 @@ def infer_schema(prototype_function: typing.Callable, mutates_args=()) -> str:\n                 default_repr = str(param.default)\n             elif isinstance(param.default, str):\n                 default_repr = f'\"{param.default}\"'\n+            elif isinstance(param.default, torch.dtype):\n+                dtype_repr = str(param.default)\n+                torch_dot = \"torch.\"\n+                assert dtype_repr.startswith(torch_dot)\n+                default_repr = dtype_repr[len(torch_dot) :]\n             else:\n                 error_fn(\n                     f\"Parameter {name} has an unsupported default value type {type(param.default)}. \"\n"
        },
        {
            "name": "TypeInfo.cpp",
            "path": "torch/csrc/TypeInfo.cpp",
            "patches": [
                {
                    "old_start": 194,
                    "old_length": 7,
                    "new_start": 194,
                    "new_length": 7,
                    "hunk": "@@ -194,7 +194,7 @@ static PyObject* THPIInfo_min(THPIInfo* self, void*) {\n \n static PyObject* THPIInfo_dtype(THPIInfo* self, void*) {\n   HANDLE_TH_ERRORS\n-  auto primary_name = torch::utils::getDtypeNames(self->type).first;\n+  auto primary_name = c10::getDtypeNames(self->type).first;\n   return AT_DISPATCH_IINFO_TYPES(self->type, \"dtype\", [&primary_name] {\n     return PyUnicode_FromString(primary_name.data());\n   });\n"
                },
                {
                    "old_start": 227,
                    "old_length": 7,
                    "new_start": 227,
                    "new_length": 7,
                    "hunk": "@@ -227,7 +227,7 @@ static PyObject* THPFInfo_resolution(THPFInfo* self, void*) {\n \n static PyObject* THPFInfo_dtype(THPFInfo* self, void*) {\n   HANDLE_TH_ERRORS\n-  auto primary_name = torch::utils::getDtypeNames(self->type).first;\n+  auto primary_name = c10::getDtypeNames(self->type).first;\n   return _AT_DISPATCH_FINFO_TYPES(self->type, \"dtype\", [&primary_name] {\n     return PyUnicode_FromString(primary_name.data());\n   });\n"
                }
            ],
            "whole_deleted": "-  auto primary_name = torch::utils::getDtypeNames(self->type).first;\n-  auto primary_name = torch::utils::getDtypeNames(self->type).first;\n",
            "whole_added": "+  auto primary_name = c10::getDtypeNames(self->type).first;\n+  auto primary_name = c10::getDtypeNames(self->type).first;\n",
            "whole_hunk": "@@ -194,7 +194,7 @@ static PyObject* THPIInfo_min(THPIInfo* self, void*) {\n \n static PyObject* THPIInfo_dtype(THPIInfo* self, void*) {\n   HANDLE_TH_ERRORS\n-  auto primary_name = torch::utils::getDtypeNames(self->type).first;\n+  auto primary_name = c10::getDtypeNames(self->type).first;\n   return AT_DISPATCH_IINFO_TYPES(self->type, \"dtype\", [&primary_name] {\n     return PyUnicode_FromString(primary_name.data());\n   });\n@@ -227,7 +227,7 @@ static PyObject* THPFInfo_resolution(THPFInfo* self, void*) {\n \n static PyObject* THPFInfo_dtype(THPFInfo* self, void*) {\n   HANDLE_TH_ERRORS\n-  auto primary_name = torch::utils::getDtypeNames(self->type).first;\n+  auto primary_name = c10::getDtypeNames(self->type).first;\n   return _AT_DISPATCH_FINFO_TYPES(self->type, \"dtype\", [&primary_name] {\n     return PyUnicode_FromString(primary_name.data());\n   });\n"
        },
        {
            "name": "function_schema_parser.cpp",
            "path": "torch/csrc/jit/frontend/function_schema_parser.cpp",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 6,
                    "new_start": 1,
                    "new_length": 7,
                    "hunk": "@@ -1,6 +1,7 @@\n #include <torch/csrc/jit/frontend/function_schema_parser.h>\n \n #include <ATen/core/Reduction.h>\n+#include <ATen/core/jit_type.h>\n #include <ATen/core/type_factory.h>\n #include <c10/util/Optional.h>\n #include <torch/csrc/jit/frontend/lexer.h>\n"
                },
                {
                    "old_start": 185,
                    "old_length": 7,
                    "new_start": 186,
                    "new_length": 8,
                    "hunk": "@@ -185,7 +186,8 @@ struct SchemaParser {\n       name = L.expect(TK_IDENT).text();\n       if (L.nextIf('=')) {\n         // NB: this means we have to unswizzle default too\n-        default_value = parseDefaultValue(*fake_type, fake_type->kind(), N);\n+        default_value =\n+            parseDefaultValue(*fake_type, fake_type->kind(), *real_type, N);\n       }\n     }\n     return Argument(\n"
                },
                {
                    "old_start": 197,
                    "old_length": 11,
                    "new_start": 199,
                    "new_length": 29,
                    "hunk": "@@ -197,11 +199,29 @@ struct SchemaParser {\n         !is_return && kwarg_only,\n         std::move(alias_info));\n   }\n-  IValue parseSingleConstant(const c10::Type& type, TypeKind kind) {\n+\n+  bool isPossiblyOptionalScalarType(const c10::Type& type) {\n+    if (type.kind() == at::ScalarTypeType::Kind) {\n+      return true;\n+    }\n+    if (type.kind() == at::OptionalType::Kind) {\n+      for (const auto& inner : type.containedTypes()) {\n+        if (isPossiblyOptionalScalarType(*inner))\n+          return true;\n+      }\n+    }\n+    return false;\n+  }\n+\n+  IValue parseSingleConstant(\n+      const c10::Type& type,\n+      TypeKind kind,\n+      const c10::Type& real_type) {\n     if (kind == c10::TypeKind::DynamicType) {\n       return parseSingleConstant(\n-          type, type.expectRef<c10::DynamicType>().dynamicKind());\n+          type, type.expectRef<c10::DynamicType>().dynamicKind(), real_type);\n     }\n+    const auto& str2dtype = c10::getStringToDtypeMap();\n     switch (L.cur().kind) {\n       case TK_TRUE:\n         L.next();\n"
                },
                {
                    "old_start": 219,
                    "old_length": 6,
                    "new_start": 239,
                    "new_length": 9,
                    "hunk": "@@ -219,6 +239,9 @@ struct SchemaParser {\n       case TK_IDENT: {\n         auto tok = L.next();\n         auto text = tok.text();\n+        // NB: float/complex/long are here for BC purposes. Other dtypes\n+        // are handled via str2dtype.\n+        // Please don't add more cases to this if-else block.\n         if (\"float\" == text) {\n           return static_cast<int64_t>(at::kFloat);\n         } else if (\"complex\" == text) {\n"
                },
                {
                    "old_start": 231,
                    "old_length": 6,
                    "new_start": 254,
                    "new_length": 10,
                    "hunk": "@@ -231,6 +254,10 @@ struct SchemaParser {\n           return static_cast<int64_t>(at::Reduction::Mean);\n         } else if (\"contiguous_format\" == text) {\n           return static_cast<int64_t>(c10::MemoryFormat::Contiguous);\n+        } else if (\n+            isPossiblyOptionalScalarType(real_type) &&\n+            str2dtype.count(text) > 0) {\n+          return static_cast<int64_t>(str2dtype.at(text));\n         } else {\n           throw ErrorReport(L.cur().range) << \"invalid numeric default value\";\n         }\n"
                },
                {
                    "old_start": 277,
                    "old_length": 12,
                    "new_start": 304,
                    "new_length": 15,
                    "hunk": "@@ -277,12 +304,15 @@ struct SchemaParser {\n             << \"lists are only supported for float, int and complex types\";\n     }\n   }\n-  IValue parseConstantList(const c10::Type& type, TypeKind kind) {\n+  IValue parseConstantList(\n+      const c10::Type& type,\n+      TypeKind kind,\n+      const c10::Type& real_type) {\n     auto tok = L.expect('[');\n     std::vector<IValue> vs;\n     if (L.cur().kind != ']') {\n       do {\n-        vs.push_back(parseSingleConstant(type, kind));\n+        vs.push_back(parseSingleConstant(type, kind, real_type));\n       } while (L.nextIf(','));\n     }\n     L.expect(']');\n"
                },
                {
                    "old_start": 296,
                    "old_length": 6,
                    "new_start": 326,
                    "new_length": 7,
                    "hunk": "@@ -296,6 +326,7 @@ struct SchemaParser {\n   IValue parseDefaultValue(\n       const c10::Type& arg_type,\n       TypeKind kind,\n+      const c10::Type& real_type,\n       std::optional<int32_t> arg_N) {\n     auto range = L.cur().range;\n     switch (kind) {\n"
                },
                {
                    "old_start": 311,
                    "old_length": 7,
                    "new_start": 342,
                    "new_length": 7,
                    "hunk": "@@ -311,7 +342,7 @@ struct SchemaParser {\n       case TypeKind::BoolType:\n       case TypeKind::FloatType:\n       case TypeKind::ComplexType:\n-        return parseSingleConstant(arg_type, kind);\n+        return parseSingleConstant(arg_type, kind, real_type);\n         break;\n       case TypeKind::DeviceObjType: {\n         auto device_text =\n"
                },
                {
                    "old_start": 321,
                    "old_length": 20,
                    "new_start": 352,
                    "new_length": 24,
                    "hunk": "@@ -321,20 +352,24 @@ struct SchemaParser {\n       }\n       case TypeKind::ListType: {\n         auto elem_type = arg_type.containedType(0);\n+        auto real_elem_type = real_type.containedType(0);\n         if (L.cur().kind == TK_IDENT) {\n           return parseTensorDefault(range);\n         } else if (arg_N && L.cur().kind != '[') {\n-          IValue v = parseSingleConstant(*elem_type, elem_type->kind());\n+          IValue v = parseSingleConstant(\n+              *elem_type, elem_type->kind(), *real_elem_type);\n           std::vector<IValue> repeated(*arg_N, v);\n           return convertToList(*elem_type, elem_type->kind(), range, repeated);\n         } else {\n-          return parseConstantList(*elem_type, elem_type->kind());\n+          return parseConstantList(\n+              *elem_type, elem_type->kind(), *real_elem_type);\n         }\n       } break;\n       case TypeKind::DynamicType:\n         return parseDefaultValue(\n             arg_type,\n             arg_type.expectRef<c10::DynamicType>().dynamicKind(),\n+            real_type,\n             arg_N);\n       default:\n         throw ErrorReport(range) << \"unexpected type, file a bug report\";\n"
                }
            ],
            "whole_deleted": "-        default_value = parseDefaultValue(*fake_type, fake_type->kind(), N);\n-  IValue parseSingleConstant(const c10::Type& type, TypeKind kind) {\n-          type, type.expectRef<c10::DynamicType>().dynamicKind());\n-  IValue parseConstantList(const c10::Type& type, TypeKind kind) {\n-        vs.push_back(parseSingleConstant(type, kind));\n-        return parseSingleConstant(arg_type, kind);\n-          IValue v = parseSingleConstant(*elem_type, elem_type->kind());\n-          return parseConstantList(*elem_type, elem_type->kind());\n",
            "whole_added": "+#include <ATen/core/jit_type.h>\n+        default_value =\n+            parseDefaultValue(*fake_type, fake_type->kind(), *real_type, N);\n+\n+  bool isPossiblyOptionalScalarType(const c10::Type& type) {\n+    if (type.kind() == at::ScalarTypeType::Kind) {\n+      return true;\n+    }\n+    if (type.kind() == at::OptionalType::Kind) {\n+      for (const auto& inner : type.containedTypes()) {\n+        if (isPossiblyOptionalScalarType(*inner))\n+          return true;\n+      }\n+    }\n+    return false;\n+  }\n+\n+  IValue parseSingleConstant(\n+      const c10::Type& type,\n+      TypeKind kind,\n+      const c10::Type& real_type) {\n+          type, type.expectRef<c10::DynamicType>().dynamicKind(), real_type);\n+    const auto& str2dtype = c10::getStringToDtypeMap();\n+        // NB: float/complex/long are here for BC purposes. Other dtypes\n+        // are handled via str2dtype.\n+        // Please don't add more cases to this if-else block.\n+        } else if (\n+            isPossiblyOptionalScalarType(real_type) &&\n+            str2dtype.count(text) > 0) {\n+          return static_cast<int64_t>(str2dtype.at(text));\n+  IValue parseConstantList(\n+      const c10::Type& type,\n+      TypeKind kind,\n+      const c10::Type& real_type) {\n+        vs.push_back(parseSingleConstant(type, kind, real_type));\n+      const c10::Type& real_type,\n+        return parseSingleConstant(arg_type, kind, real_type);\n+        auto real_elem_type = real_type.containedType(0);\n+          IValue v = parseSingleConstant(\n+              *elem_type, elem_type->kind(), *real_elem_type);\n+          return parseConstantList(\n+              *elem_type, elem_type->kind(), *real_elem_type);\n+            real_type,\n",
            "whole_hunk": "@@ -1,6 +1,7 @@\n #include <torch/csrc/jit/frontend/function_schema_parser.h>\n \n #include <ATen/core/Reduction.h>\n+#include <ATen/core/jit_type.h>\n #include <ATen/core/type_factory.h>\n #include <c10/util/Optional.h>\n #include <torch/csrc/jit/frontend/lexer.h>\n@@ -185,7 +186,8 @@ struct SchemaParser {\n       name = L.expect(TK_IDENT).text();\n       if (L.nextIf('=')) {\n         // NB: this means we have to unswizzle default too\n-        default_value = parseDefaultValue(*fake_type, fake_type->kind(), N);\n+        default_value =\n+            parseDefaultValue(*fake_type, fake_type->kind(), *real_type, N);\n       }\n     }\n     return Argument(\n@@ -197,11 +199,29 @@ struct SchemaParser {\n         !is_return && kwarg_only,\n         std::move(alias_info));\n   }\n-  IValue parseSingleConstant(const c10::Type& type, TypeKind kind) {\n+\n+  bool isPossiblyOptionalScalarType(const c10::Type& type) {\n+    if (type.kind() == at::ScalarTypeType::Kind) {\n+      return true;\n+    }\n+    if (type.kind() == at::OptionalType::Kind) {\n+      for (const auto& inner : type.containedTypes()) {\n+        if (isPossiblyOptionalScalarType(*inner))\n+          return true;\n+      }\n+    }\n+    return false;\n+  }\n+\n+  IValue parseSingleConstant(\n+      const c10::Type& type,\n+      TypeKind kind,\n+      const c10::Type& real_type) {\n     if (kind == c10::TypeKind::DynamicType) {\n       return parseSingleConstant(\n-          type, type.expectRef<c10::DynamicType>().dynamicKind());\n+          type, type.expectRef<c10::DynamicType>().dynamicKind(), real_type);\n     }\n+    const auto& str2dtype = c10::getStringToDtypeMap();\n     switch (L.cur().kind) {\n       case TK_TRUE:\n         L.next();\n@@ -219,6 +239,9 @@ struct SchemaParser {\n       case TK_IDENT: {\n         auto tok = L.next();\n         auto text = tok.text();\n+        // NB: float/complex/long are here for BC purposes. Other dtypes\n+        // are handled via str2dtype.\n+        // Please don't add more cases to this if-else block.\n         if (\"float\" == text) {\n           return static_cast<int64_t>(at::kFloat);\n         } else if (\"complex\" == text) {\n@@ -231,6 +254,10 @@ struct SchemaParser {\n           return static_cast<int64_t>(at::Reduction::Mean);\n         } else if (\"contiguous_format\" == text) {\n           return static_cast<int64_t>(c10::MemoryFormat::Contiguous);\n+        } else if (\n+            isPossiblyOptionalScalarType(real_type) &&\n+            str2dtype.count(text) > 0) {\n+          return static_cast<int64_t>(str2dtype.at(text));\n         } else {\n           throw ErrorReport(L.cur().range) << \"invalid numeric default value\";\n         }\n@@ -277,12 +304,15 @@ struct SchemaParser {\n             << \"lists are only supported for float, int and complex types\";\n     }\n   }\n-  IValue parseConstantList(const c10::Type& type, TypeKind kind) {\n+  IValue parseConstantList(\n+      const c10::Type& type,\n+      TypeKind kind,\n+      const c10::Type& real_type) {\n     auto tok = L.expect('[');\n     std::vector<IValue> vs;\n     if (L.cur().kind != ']') {\n       do {\n-        vs.push_back(parseSingleConstant(type, kind));\n+        vs.push_back(parseSingleConstant(type, kind, real_type));\n       } while (L.nextIf(','));\n     }\n     L.expect(']');\n@@ -296,6 +326,7 @@ struct SchemaParser {\n   IValue parseDefaultValue(\n       const c10::Type& arg_type,\n       TypeKind kind,\n+      const c10::Type& real_type,\n       std::optional<int32_t> arg_N) {\n     auto range = L.cur().range;\n     switch (kind) {\n@@ -311,7 +342,7 @@ struct SchemaParser {\n       case TypeKind::BoolType:\n       case TypeKind::FloatType:\n       case TypeKind::ComplexType:\n-        return parseSingleConstant(arg_type, kind);\n+        return parseSingleConstant(arg_type, kind, real_type);\n         break;\n       case TypeKind::DeviceObjType: {\n         auto device_text =\n@@ -321,20 +352,24 @@ struct SchemaParser {\n       }\n       case TypeKind::ListType: {\n         auto elem_type = arg_type.containedType(0);\n+        auto real_elem_type = real_type.containedType(0);\n         if (L.cur().kind == TK_IDENT) {\n           return parseTensorDefault(range);\n         } else if (arg_N && L.cur().kind != '[') {\n-          IValue v = parseSingleConstant(*elem_type, elem_type->kind());\n+          IValue v = parseSingleConstant(\n+              *elem_type, elem_type->kind(), *real_elem_type);\n           std::vector<IValue> repeated(*arg_N, v);\n           return convertToList(*elem_type, elem_type->kind(), range, repeated);\n         } else {\n-          return parseConstantList(*elem_type, elem_type->kind());\n+          return parseConstantList(\n+              *elem_type, elem_type->kind(), *real_elem_type);\n         }\n       } break;\n       case TypeKind::DynamicType:\n         return parseDefaultValue(\n             arg_type,\n             arg_type.expectRef<c10::DynamicType>().dynamicKind(),\n+            real_type,\n             arg_N);\n       default:\n         throw ErrorReport(range) << \"unexpected type, file a bug report\";\n"
        },
        {
            "name": "schema_type_parser.cpp",
            "path": "torch/csrc/jit/frontend/schema_type_parser.cpp",
            "patches": [
                {
                    "old_start": 168,
                    "old_length": 6,
                    "new_start": 168,
                    "new_length": 7,
                    "hunk": "@@ -168,6 +168,7 @@ std::optional<at::ScalarType> SchemaTypeParser::parseTensorDType(\n   static std::unordered_map<std::string, at::ScalarType> type_map = {\n       AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS(DEFINE_SCALAR_TYPE)};\n \n+#undef DEFINE_SCALAR_TYPE\n   auto type = type_map.find(dtype);\n   if (type != type_map.end()) {\n     return type->second;\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#undef DEFINE_SCALAR_TYPE\n",
            "whole_hunk": "@@ -168,6 +168,7 @@ std::optional<at::ScalarType> SchemaTypeParser::parseTensorDType(\n   static std::unordered_map<std::string, at::ScalarType> type_map = {\n       AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS(DEFINE_SCALAR_TYPE)};\n \n+#undef DEFINE_SCALAR_TYPE\n   auto type = type_map.find(dtype);\n   if (type != type_map.end()) {\n     return type->second;\n"
        },
        {
            "name": "tensor_dtypes.cpp",
            "path": "torch/csrc/utils/tensor_dtypes.cpp",
            "patches": [
                {
                    "old_start": 7,
                    "old_length": 91,
                    "new_start": 7,
                    "new_length": 6,
                    "hunk": "@@ -7,91 +7,6 @@\n \n namespace torch::utils {\n \n-std::pair<std::string, std::string> getDtypeNames(at::ScalarType scalarType) {\n-  switch (scalarType) {\n-    case at::ScalarType::UInt1:\n-      return std::make_pair(\"uint1\", \"bit\");\n-    case at::ScalarType::UInt2:\n-      return std::make_pair(\"uint2\", \"\");\n-    case at::ScalarType::UInt3:\n-      return std::make_pair(\"uint3\", \"\");\n-    case at::ScalarType::UInt4:\n-      return std::make_pair(\"uint4\", \"\");\n-    case at::ScalarType::UInt5:\n-      return std::make_pair(\"uint5\", \"\");\n-    case at::ScalarType::UInt6:\n-      return std::make_pair(\"uint6\", \"\");\n-    case at::ScalarType::UInt7:\n-      return std::make_pair(\"uint7\", \"\");\n-    case at::ScalarType::Byte:\n-      // no \"byte\" because byte is signed in numpy and we overload\n-      // byte to mean bool often\n-      return std::make_pair(\"uint8\", \"\");\n-    case at::ScalarType::UInt16:\n-      return std::make_pair(\"uint16\", \"\");\n-    case at::ScalarType::UInt32:\n-      return std::make_pair(\"uint32\", \"\");\n-    case at::ScalarType::UInt64:\n-      return std::make_pair(\"uint64\", \"\");\n-    case at::ScalarType::Char:\n-      // no \"char\" because it is not consistently signed or unsigned; we want\n-      // to move to int8\n-      return std::make_pair(\"int8\", \"\");\n-    case at::ScalarType::Double:\n-      return std::make_pair(\"float64\", \"double\");\n-    case at::ScalarType::Float:\n-      return std::make_pair(\"float32\", \"float\");\n-    case at::ScalarType::Int:\n-      return std::make_pair(\"int32\", \"int\");\n-    case at::ScalarType::Long:\n-      return std::make_pair(\"int64\", \"long\");\n-    case at::ScalarType::Short:\n-      return std::make_pair(\"int16\", \"short\");\n-    case at::ScalarType::Half:\n-      return std::make_pair(\"float16\", \"half\");\n-    case at::ScalarType::ComplexHalf:\n-      return std::make_pair(\"complex32\", \"chalf\");\n-    case at::ScalarType::ComplexFloat:\n-      return std::make_pair(\"complex64\", \"cfloat\");\n-    case at::ScalarType::ComplexDouble:\n-      return std::make_pair(\"complex128\", \"cdouble\");\n-    case at::ScalarType::Bool:\n-      return std::make_pair(\"bool\", \"\");\n-    case at::ScalarType::QInt8:\n-      return std::make_pair(\"qint8\", \"\");\n-    case at::ScalarType::QUInt8:\n-      return std::make_pair(\"quint8\", \"\");\n-    case at::ScalarType::QInt32:\n-      return std::make_pair(\"qint32\", \"\");\n-    case at::ScalarType::BFloat16:\n-      return std::make_pair(\"bfloat16\", \"\");\n-    case at::ScalarType::QUInt4x2:\n-      return std::make_pair(\"quint4x2\", \"\");\n-    case at::ScalarType::QUInt2x4:\n-      return std::make_pair(\"quint2x4\", \"\");\n-    case at::ScalarType::Bits1x8:\n-      return std::make_pair(\"bits1x8\", \"\");\n-    case at::ScalarType::Bits2x4:\n-      return std::make_pair(\"bits2x4\", \"\");\n-    case at::ScalarType::Bits4x2:\n-      return std::make_pair(\"bits4x2\", \"\");\n-    case at::ScalarType::Bits8:\n-      return std::make_pair(\"bits8\", \"\");\n-    case at::ScalarType::Bits16:\n-      return std::make_pair(\"bits16\", \"\");\n-    case at::ScalarType::Float8_e5m2:\n-      return std::make_pair(\"float8_e5m2\", \"\");\n-    case at::ScalarType::Float8_e4m3fn:\n-      return std::make_pair(\"float8_e4m3fn\", \"\");\n-    case at::ScalarType::Float8_e5m2fnuz:\n-      return std::make_pair(\"float8_e5m2fnuz\", \"\");\n-    case at::ScalarType::Float8_e4m3fnuz:\n-      return std::make_pair(\"float8_e4m3fnuz\", \"\");\n-    default:\n-      throw std::runtime_error(\"Unimplemented scalar type\");\n-  }\n-}\n-\n void initializeDtypes() {\n   auto torch_module = THPObjectPtr(PyImport_ImportModule(\"torch\"));\n   if (!torch_module)\n"
                },
                {
                    "old_start": 102,
                    "old_length": 8,
                    "new_start": 17,
                    "new_length": 10,
                    "hunk": "@@ -102,8 +17,10 @@ void initializeDtypes() {\n   auto all_scalar_types = {\n       AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS(DEFINE_SCALAR_TYPE)};\n \n+#undef DEFINE_SCALAR_TYPE\n+\n   for (at::ScalarType scalarType : all_scalar_types) {\n-    auto [primary_name, legacy_name] = getDtypeNames(scalarType);\n+    auto [primary_name, legacy_name] = c10::getDtypeNames(scalarType);\n     PyObject* dtype = THPDtype_New(scalarType, primary_name);\n     torch::registerDtypeObject((THPDtype*)dtype, scalarType);\n     Py_INCREF(dtype);"
                }
            ],
            "whole_deleted": "-std::pair<std::string, std::string> getDtypeNames(at::ScalarType scalarType) {\n-  switch (scalarType) {\n-    case at::ScalarType::UInt1:\n-      return std::make_pair(\"uint1\", \"bit\");\n-    case at::ScalarType::UInt2:\n-      return std::make_pair(\"uint2\", \"\");\n-    case at::ScalarType::UInt3:\n-      return std::make_pair(\"uint3\", \"\");\n-    case at::ScalarType::UInt4:\n-      return std::make_pair(\"uint4\", \"\");\n-    case at::ScalarType::UInt5:\n-      return std::make_pair(\"uint5\", \"\");\n-    case at::ScalarType::UInt6:\n-      return std::make_pair(\"uint6\", \"\");\n-    case at::ScalarType::UInt7:\n-      return std::make_pair(\"uint7\", \"\");\n-    case at::ScalarType::Byte:\n-      // no \"byte\" because byte is signed in numpy and we overload\n-      // byte to mean bool often\n-      return std::make_pair(\"uint8\", \"\");\n-    case at::ScalarType::UInt16:\n-      return std::make_pair(\"uint16\", \"\");\n-    case at::ScalarType::UInt32:\n-      return std::make_pair(\"uint32\", \"\");\n-    case at::ScalarType::UInt64:\n-      return std::make_pair(\"uint64\", \"\");\n-    case at::ScalarType::Char:\n-      // no \"char\" because it is not consistently signed or unsigned; we want\n-      // to move to int8\n-      return std::make_pair(\"int8\", \"\");\n-    case at::ScalarType::Double:\n-      return std::make_pair(\"float64\", \"double\");\n-    case at::ScalarType::Float:\n-      return std::make_pair(\"float32\", \"float\");\n-    case at::ScalarType::Int:\n-      return std::make_pair(\"int32\", \"int\");\n-    case at::ScalarType::Long:\n-      return std::make_pair(\"int64\", \"long\");\n-    case at::ScalarType::Short:\n-      return std::make_pair(\"int16\", \"short\");\n-    case at::ScalarType::Half:\n-      return std::make_pair(\"float16\", \"half\");\n-    case at::ScalarType::ComplexHalf:\n-      return std::make_pair(\"complex32\", \"chalf\");\n-    case at::ScalarType::ComplexFloat:\n-      return std::make_pair(\"complex64\", \"cfloat\");\n-    case at::ScalarType::ComplexDouble:\n-      return std::make_pair(\"complex128\", \"cdouble\");\n-    case at::ScalarType::Bool:\n-      return std::make_pair(\"bool\", \"\");\n-    case at::ScalarType::QInt8:\n-      return std::make_pair(\"qint8\", \"\");\n-    case at::ScalarType::QUInt8:\n-      return std::make_pair(\"quint8\", \"\");\n-    case at::ScalarType::QInt32:\n-      return std::make_pair(\"qint32\", \"\");\n-    case at::ScalarType::BFloat16:\n-      return std::make_pair(\"bfloat16\", \"\");\n-    case at::ScalarType::QUInt4x2:\n-      return std::make_pair(\"quint4x2\", \"\");\n-    case at::ScalarType::QUInt2x4:\n-      return std::make_pair(\"quint2x4\", \"\");\n-    case at::ScalarType::Bits1x8:\n-      return std::make_pair(\"bits1x8\", \"\");\n-    case at::ScalarType::Bits2x4:\n-      return std::make_pair(\"bits2x4\", \"\");\n-    case at::ScalarType::Bits4x2:\n-      return std::make_pair(\"bits4x2\", \"\");\n-    case at::ScalarType::Bits8:\n-      return std::make_pair(\"bits8\", \"\");\n-    case at::ScalarType::Bits16:\n-      return std::make_pair(\"bits16\", \"\");\n-    case at::ScalarType::Float8_e5m2:\n-      return std::make_pair(\"float8_e5m2\", \"\");\n-    case at::ScalarType::Float8_e4m3fn:\n-      return std::make_pair(\"float8_e4m3fn\", \"\");\n-    case at::ScalarType::Float8_e5m2fnuz:\n-      return std::make_pair(\"float8_e5m2fnuz\", \"\");\n-    case at::ScalarType::Float8_e4m3fnuz:\n-      return std::make_pair(\"float8_e4m3fnuz\", \"\");\n-    default:\n-      throw std::runtime_error(\"Unimplemented scalar type\");\n-  }\n-}\n-\n-    auto [primary_name, legacy_name] = getDtypeNames(scalarType);\n",
            "whole_added": "+#undef DEFINE_SCALAR_TYPE\n+\n+    auto [primary_name, legacy_name] = c10::getDtypeNames(scalarType);\n",
            "whole_hunk": "@@ -7,91 +7,6 @@\n \n namespace torch::utils {\n \n-std::pair<std::string, std::string> getDtypeNames(at::ScalarType scalarType) {\n-  switch (scalarType) {\n-    case at::ScalarType::UInt1:\n-      return std::make_pair(\"uint1\", \"bit\");\n-    case at::ScalarType::UInt2:\n-      return std::make_pair(\"uint2\", \"\");\n-    case at::ScalarType::UInt3:\n-      return std::make_pair(\"uint3\", \"\");\n-    case at::ScalarType::UInt4:\n-      return std::make_pair(\"uint4\", \"\");\n-    case at::ScalarType::UInt5:\n-      return std::make_pair(\"uint5\", \"\");\n-    case at::ScalarType::UInt6:\n-      return std::make_pair(\"uint6\", \"\");\n-    case at::ScalarType::UInt7:\n-      return std::make_pair(\"uint7\", \"\");\n-    case at::ScalarType::Byte:\n-      // no \"byte\" because byte is signed in numpy and we overload\n-      // byte to mean bool often\n-      return std::make_pair(\"uint8\", \"\");\n-    case at::ScalarType::UInt16:\n-      return std::make_pair(\"uint16\", \"\");\n-    case at::ScalarType::UInt32:\n-      return std::make_pair(\"uint32\", \"\");\n-    case at::ScalarType::UInt64:\n-      return std::make_pair(\"uint64\", \"\");\n-    case at::ScalarType::Char:\n-      // no \"char\" because it is not consistently signed or unsigned; we want\n-      // to move to int8\n-      return std::make_pair(\"int8\", \"\");\n-    case at::ScalarType::Double:\n-      return std::make_pair(\"float64\", \"double\");\n-    case at::ScalarType::Float:\n-      return std::make_pair(\"float32\", \"float\");\n-    case at::ScalarType::Int:\n-      return std::make_pair(\"int32\", \"int\");\n-    case at::ScalarType::Long:\n-      return std::make_pair(\"int64\", \"long\");\n-    case at::ScalarType::Short:\n-      return std::make_pair(\"int16\", \"short\");\n-    case at::ScalarType::Half:\n-      return std::make_pair(\"float16\", \"half\");\n-    case at::ScalarType::ComplexHalf:\n-      return std::make_pair(\"complex32\", \"chalf\");\n-    case at::ScalarType::ComplexFloat:\n-      return std::make_pair(\"complex64\", \"cfloat\");\n-    case at::ScalarType::ComplexDouble:\n-      return std::make_pair(\"complex128\", \"cdouble\");\n-    case at::ScalarType::Bool:\n-      return std::make_pair(\"bool\", \"\");\n-    case at::ScalarType::QInt8:\n-      return std::make_pair(\"qint8\", \"\");\n-    case at::ScalarType::QUInt8:\n-      return std::make_pair(\"quint8\", \"\");\n-    case at::ScalarType::QInt32:\n-      return std::make_pair(\"qint32\", \"\");\n-    case at::ScalarType::BFloat16:\n-      return std::make_pair(\"bfloat16\", \"\");\n-    case at::ScalarType::QUInt4x2:\n-      return std::make_pair(\"quint4x2\", \"\");\n-    case at::ScalarType::QUInt2x4:\n-      return std::make_pair(\"quint2x4\", \"\");\n-    case at::ScalarType::Bits1x8:\n-      return std::make_pair(\"bits1x8\", \"\");\n-    case at::ScalarType::Bits2x4:\n-      return std::make_pair(\"bits2x4\", \"\");\n-    case at::ScalarType::Bits4x2:\n-      return std::make_pair(\"bits4x2\", \"\");\n-    case at::ScalarType::Bits8:\n-      return std::make_pair(\"bits8\", \"\");\n-    case at::ScalarType::Bits16:\n-      return std::make_pair(\"bits16\", \"\");\n-    case at::ScalarType::Float8_e5m2:\n-      return std::make_pair(\"float8_e5m2\", \"\");\n-    case at::ScalarType::Float8_e4m3fn:\n-      return std::make_pair(\"float8_e4m3fn\", \"\");\n-    case at::ScalarType::Float8_e5m2fnuz:\n-      return std::make_pair(\"float8_e5m2fnuz\", \"\");\n-    case at::ScalarType::Float8_e4m3fnuz:\n-      return std::make_pair(\"float8_e4m3fnuz\", \"\");\n-    default:\n-      throw std::runtime_error(\"Unimplemented scalar type\");\n-  }\n-}\n-\n void initializeDtypes() {\n   auto torch_module = THPObjectPtr(PyImport_ImportModule(\"torch\"));\n   if (!torch_module)\n@@ -102,8 +17,10 @@ void initializeDtypes() {\n   auto all_scalar_types = {\n       AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS(DEFINE_SCALAR_TYPE)};\n \n+#undef DEFINE_SCALAR_TYPE\n+\n   for (at::ScalarType scalarType : all_scalar_types) {\n-    auto [primary_name, legacy_name] = getDtypeNames(scalarType);\n+    auto [primary_name, legacy_name] = c10::getDtypeNames(scalarType);\n     PyObject* dtype = THPDtype_New(scalarType, primary_name);\n     torch::registerDtypeObject((THPDtype*)dtype, scalarType);\n     Py_INCREF(dtype);"
        }
    ]
},
{
    "Id": 210,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/ff1e3ff5a503a520c1a310c8e72a383657f9a4bc",
    "date": "2024-04-16T02:20:58+00:00",
    "message": "[reland] `_foreach_copy` with different src/dst dtypes (#123844)\n\nAttempt to reland https://github.com/pytorch/pytorch/pull/121717.\nThe change is the array bounds check.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/123844\nApproved by: https://github.com/janeyx99",
    "label": "YES",
    "changes": [
        {
            "name": "ForeachUtils.h",
            "path": "aten/src/ATen/native/ForeachUtils.h",
            "patches": [
                {
                    "old_start": 102,
                    "old_length": 12,
                    "new_start": 102,
                    "new_length": 13,
                    "hunk": "@@ -102,12 +102,13 @@ inline void check_foreach_api_restrictions(\n // corresponding tensors (aligning in index across the tensorLists) share the\n // same device and dtype.\n inline bool _check_tensors_share_device_and_dtype(\n-    ArrayRef<TensorList> tensorLists) {\n+    ArrayRef<TensorList> tensorLists,\n+    const bool skip_dtype_check = false) {\n   const auto expected_dtype = tensorLists[0][0].dtype();\n   const auto expected_device = tensorLists[0][0].device();\n \n   auto is_tensor_okay = [&](const Tensor& tensor) {\n-    return tensor.dtype() == expected_dtype &&\n+    return (skip_dtype_check || tensor.dtype() == expected_dtype) &&\n         tensor.device() == expected_device && tensor.layout() == at::kStrided &&\n         tensor.is_non_overlapping_and_dense();\n   };\n"
                }
            ],
            "whole_deleted": "-    ArrayRef<TensorList> tensorLists) {\n-    return tensor.dtype() == expected_dtype &&\n",
            "whole_added": "+    ArrayRef<TensorList> tensorLists,\n+    const bool skip_dtype_check = false) {\n+    return (skip_dtype_check || tensor.dtype() == expected_dtype) &&\n",
            "whole_hunk": "@@ -102,12 +102,13 @@ inline void check_foreach_api_restrictions(\n // corresponding tensors (aligning in index across the tensorLists) share the\n // same device and dtype.\n inline bool _check_tensors_share_device_and_dtype(\n-    ArrayRef<TensorList> tensorLists) {\n+    ArrayRef<TensorList> tensorLists,\n+    const bool skip_dtype_check = false) {\n   const auto expected_dtype = tensorLists[0][0].dtype();\n   const auto expected_device = tensorLists[0][0].device();\n \n   auto is_tensor_okay = [&](const Tensor& tensor) {\n-    return tensor.dtype() == expected_dtype &&\n+    return (skip_dtype_check || tensor.dtype() == expected_dtype) &&\n         tensor.device() == expected_device && tensor.layout() == at::kStrided &&\n         tensor.is_non_overlapping_and_dense();\n   };\n"
        },
        {
            "name": "ForeachBinaryOpList.cu",
            "path": "aten/src/ATen/native/cuda/ForeachBinaryOpList.cu",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 9,
                    "new_start": 1,
                    "new_length": 11,
                    "hunk": "@@ -1,9 +1,11 @@\n #define TORCH_ASSERT_ONLY_METHOD_OPERATORS\n #include <ATen/Dispatch.h>\n+#include <ATen/cuda/CUDAContext.h>\n #include <ATen/native/ForeachUtils.h>\n #include <ATen/native/cuda/ForeachFunctors.cuh>\n #include <ATen/native/cuda/ForeachMinMaxFunctors.cuh>\n #include <functional>\n+#include <type_traits>\n \n #ifndef AT_PER_OPERATOR_HEADERS\n #include <ATen/NativeFunctions.h>\n"
                },
                {
                    "old_start": 250,
                    "old_length": 20,
                    "new_start": 252,
                    "new_length": 154,
                    "hunk": "@@ -250,20 +252,154 @@ FOREACH_BINARY_OP_LIST(\n     power_functor,\n     /*division_op*/ true);\n \n-template <typename T>\n-struct Identity {\n-  __device__ __forceinline__ T operator()(const T& x) {\n-    return x;\n+template <typename dst_t, typename src_t = dst_t>\n+struct Copy {\n+  __device__ __forceinline__ dst_t operator()(const src_t& x) {\n+    return static_cast<dst_t>(x);\n   }\n };\n \n+template <typename dst_t>\n+struct Copy<dst_t, c10::complex<double>> {\n+  __device__ __forceinline__ dst_t operator()(const c10::complex<double>& x) {\n+    if constexpr (!(std::is_same_v<dst_t, c10::complex<double>> ||\n+                    std::is_same_v<dst_t, c10::complex<float>>)) {\n+      return static_cast<dst_t>(x.real());\n+    } else {\n+      return static_cast<dst_t>(x);\n+    }\n+  }\n+};\n+\n+template <typename dst_t>\n+struct Copy<dst_t, c10::complex<float>> {\n+  __device__ __forceinline__ dst_t operator()(const c10::complex<float>& x) {\n+    if constexpr (!(std::is_same_v<dst_t, c10::complex<double>> ||\n+                    std::is_same_v<dst_t, c10::complex<float>>)) {\n+      return static_cast<dst_t>(x.real());\n+    } else {\n+      return static_cast<dst_t>(x);\n+    }\n+  }\n+};\n+\n+#define AT_DISPATCH_SOURCE_TYPES(TYPE, NAME, ...)                              \\\n+  AT_DISPATCH_SWITCH(                                                          \\\n+      TYPE,                                                                    \\\n+      NAME,                                                                    \\\n+      AT_PRIVATE_CASE_TYPE_USING_HINT(                                         \\\n+          at::ScalarType::Byte, src_t, __VA_ARGS__)                            \\\n+          AT_PRIVATE_CASE_TYPE_USING_HINT(                                     \\\n+              at::ScalarType::Char, src_t, __VA_ARGS__)                        \\\n+              AT_PRIVATE_CASE_TYPE_USING_HINT(                                 \\\n+                  at::ScalarType::Long, src_t, __VA_ARGS__)                    \\\n+                  AT_PRIVATE_CASE_TYPE_USING_HINT(                             \\\n+                      at::ScalarType::Short, src_t, __VA_ARGS__)               \\\n+                      AT_PRIVATE_CASE_TYPE_USING_HINT(                         \\\n+                          at::ScalarType::Double, src_t, __VA_ARGS__)          \\\n+                          AT_PRIVATE_CASE_TYPE_USING_HINT(                     \\\n+                              at::ScalarType::Float, src_t, __VA_ARGS__)       \\\n+                              AT_PRIVATE_CASE_TYPE_USING_HINT(                 \\\n+                                  at::ScalarType::ComplexDouble,               \\\n+                                  src_t,                                       \\\n+                                  __VA_ARGS__)                                 \\\n+                                  AT_PRIVATE_CASE_TYPE_USING_HINT(             \\\n+                                      at::ScalarType::ComplexFloat,            \\\n+                                      src_t,                                   \\\n+                                      __VA_ARGS__)                             \\\n+                                      AT_PRIVATE_CASE_TYPE_USING_HINT(         \\\n+                                          at::ScalarType::Half,                \\\n+                                          src_t,                               \\\n+                                          __VA_ARGS__)                         \\\n+                                          AT_PRIVATE_CASE_TYPE_USING_HINT(     \\\n+                                              at::ScalarType::BFloat16,        \\\n+                                              src_t,                           \\\n+                                              __VA_ARGS__)                     \\\n+                                              AT_PRIVATE_CASE_TYPE_USING_HINT( \\\n+                                                  at::ScalarType::Bool,        \\\n+                                                  src_t,                       \\\n+                                                  __VA_ARGS__))\n+\n+namespace {\n+\n+template <\n+    typename T,\n+    typename src_t,\n+    int depth,\n+    int r_args_depth,\n+    int res_arg_index>\n+struct CopyFunctor {\n+  static_assert(depth == 2 && r_args_depth == 1 && res_arg_index == 1);\n+  template <typename Op>\n+  __device__ __forceinline__ void operator()(\n+      int chunk_size,\n+      TensorListMetadata<depth>& tl,\n+      Op op) {\n+    const auto tensor_loc = tl.block_to_tensor[blockIdx.x];\n+    const auto chunk_idx = tl.block_to_chunk[blockIdx.x];\n+    auto n = tl.numel_for_tensor[tensor_loc];\n+\n+    src_t* src_ptr = (src_t*)tl.addresses[0][tensor_loc];\n+    src_ptr += chunk_idx * chunk_size;\n+    T* self_ptr = (T*)tl.addresses[1][tensor_loc];\n+    self_ptr += chunk_idx * chunk_size;\n+\n+    const bool all_aligned{is_aligned(src_ptr) && is_aligned(self_ptr)};\n+\n+    n -= chunk_idx * chunk_size;\n+    src_t src_args[kILP];\n+    T r_args[kILP];\n+\n+    // to make things simple, we put aligned case in a different code path\n+    if (n % kILP == 0 && chunk_size % kILP == 0 && all_aligned) {\n+      for (int64_t i_start = threadIdx.x;\n+           i_start * kILP < n && i_start * kILP < chunk_size;\n+           i_start += blockDim.x) {\n+        // load\n+        load_store(src_args, src_ptr, 0, i_start);\n+#pragma unroll\n+        for (int ii = 0; ii < kILP; ii++) {\n+          r_args[ii] = static_cast<T>(op(src_args[ii]));\n+        }\n+        // store\n+        load_store(self_ptr, r_args, i_start, 0);\n+      }\n+    } else {\n+      for (int64_t i_start = 0; i_start < n && i_start < chunk_size;\n+           i_start += blockDim.x * kILP) {\n+#pragma unroll\n+        for (int ii = 0; ii < kILP; ii++) {\n+          const auto i = i_start + threadIdx.x + ii * blockDim.x;\n+          if (i < n && i < chunk_size) {\n+            src_args[ii] = src_ptr[i];\n+          }\n+        }\n+#pragma unroll\n+        for (int ii = 0; ii < kILP; ii++) {\n+          r_args[ii] = static_cast<T>(op(src_args[ii]));\n+        }\n+        store_args(self_ptr, r_args, i_start, chunk_size, n);\n+      }\n+    }\n+  }\n+};\n+\n+} // anonymous namespace\n+\n void foreach_tensor_copy_list_kernel_cuda_(\n     TensorList self,\n     TensorList src,\n     const bool non_blocking) {\n   check_foreach_api_restrictions(self, src);\n-  if (!can_use_fast_route(\n-          self, src, /* does_op_promote_integer_inputs_to_float */ false)) {\n+  if (!(_check_tensors_share_device_and_dtype(\n+            {self, src}, /* skip_dtype_check */ true) &&\n+        std::all_of(\n+            src.cbegin(),\n+            src.cend(),\n+            [&](const auto& t) -> bool {\n+              return t.dtype() == src[0].dtype();\n+            }) &&\n+        _check_tensors_share_sizes_and_strides({self, src}))) {\n     return at::native::foreach_tensor_copy_list_kernel_slow_(\n         self, src, non_blocking);\n   }\n"
                },
                {
                    "old_start": 278,
                    "old_length": 16,
                    "new_start": 414,
                    "new_length": 38,
                    "hunk": "@@ -278,16 +414,38 @@ void foreach_tensor_copy_list_kernel_cuda_(\n       \"foreach_tensor_copy\",\n       [&]() {\n         using opmath_t = at::opmath_type<scalar_t>;\n-        multi_tensor_apply<2>(\n-            tensor_lists,\n-            UnaryOpFunctor<\n-                scalar_t,\n-                /* depth */ 2,\n-                /* r_args_depth */ 1,\n-                /* res_arg_index */ 1>(),\n-            Identity<opmath_t>());\n+        AT_DISPATCH_SOURCE_TYPES(src[0].scalar_type(), \"foreach_tensor_copy\", [&] {\n+          if constexpr (std::is_same_v<scalar_t, src_t>) {\n+            multi_tensor_apply<2>(\n+                tensor_lists,\n+                UnaryOpFunctor<\n+                    scalar_t,\n+                    /* depth */ 2,\n+                    /* r_args_depth */ 1,\n+                    /* res_arg_index */ 1>(),\n+                Copy<opmath_t, opmath_t>());\n+          } else {\n+            // Ref:\n+            // https://github.com/pytorch/pytorch/blob/656134c38f4737d13c3f43fc5c59470bc23c1d2f/aten/src/ATen/native/Copy.cpp#L299-L301\n+            if (!self[0].is_complex() && src[0].is_complex()) {\n+              TORCH_WARN_ONCE(\n+                  \"Casting complex values to real discards the imaginary part\");\n+            }\n+            multi_tensor_apply<2>(\n+                tensor_lists,\n+                CopyFunctor<\n+                    scalar_t,\n+                    src_t,\n+                    /* depth */ 2,\n+                    /* r_args_depth */ 1,\n+                    /* res_arg_index */ 1>(),\n+                Copy<scalar_t, src_t>());\n+          }\n+        });\n       });\n   increment_version(self);\n }\n \n+#undef AT_DISPATCH_SOURCE_TYPES\n+\n } // namespace at::native\n"
                }
            ],
            "whole_deleted": "-template <typename T>\n-struct Identity {\n-  __device__ __forceinline__ T operator()(const T& x) {\n-    return x;\n-  if (!can_use_fast_route(\n-          self, src, /* does_op_promote_integer_inputs_to_float */ false)) {\n-        multi_tensor_apply<2>(\n-            tensor_lists,\n-            UnaryOpFunctor<\n-                scalar_t,\n-                /* depth */ 2,\n-                /* r_args_depth */ 1,\n-                /* res_arg_index */ 1>(),\n-            Identity<opmath_t>());\n",
            "whole_added": "+#include <ATen/cuda/CUDAContext.h>\n+#include <type_traits>\n+template <typename dst_t, typename src_t = dst_t>\n+struct Copy {\n+  __device__ __forceinline__ dst_t operator()(const src_t& x) {\n+    return static_cast<dst_t>(x);\n+template <typename dst_t>\n+struct Copy<dst_t, c10::complex<double>> {\n+  __device__ __forceinline__ dst_t operator()(const c10::complex<double>& x) {\n+    if constexpr (!(std::is_same_v<dst_t, c10::complex<double>> ||\n+                    std::is_same_v<dst_t, c10::complex<float>>)) {\n+      return static_cast<dst_t>(x.real());\n+    } else {\n+      return static_cast<dst_t>(x);\n+    }\n+  }\n+};\n+\n+template <typename dst_t>\n+struct Copy<dst_t, c10::complex<float>> {\n+  __device__ __forceinline__ dst_t operator()(const c10::complex<float>& x) {\n+    if constexpr (!(std::is_same_v<dst_t, c10::complex<double>> ||\n+                    std::is_same_v<dst_t, c10::complex<float>>)) {\n+      return static_cast<dst_t>(x.real());\n+    } else {\n+      return static_cast<dst_t>(x);\n+    }\n+  }\n+};\n+\n+#define AT_DISPATCH_SOURCE_TYPES(TYPE, NAME, ...)                              \\\n+  AT_DISPATCH_SWITCH(                                                          \\\n+      TYPE,                                                                    \\\n+      NAME,                                                                    \\\n+      AT_PRIVATE_CASE_TYPE_USING_HINT(                                         \\\n+          at::ScalarType::Byte, src_t, __VA_ARGS__)                            \\\n+          AT_PRIVATE_CASE_TYPE_USING_HINT(                                     \\\n+              at::ScalarType::Char, src_t, __VA_ARGS__)                        \\\n+              AT_PRIVATE_CASE_TYPE_USING_HINT(                                 \\\n+                  at::ScalarType::Long, src_t, __VA_ARGS__)                    \\\n+                  AT_PRIVATE_CASE_TYPE_USING_HINT(                             \\\n+                      at::ScalarType::Short, src_t, __VA_ARGS__)               \\\n+                      AT_PRIVATE_CASE_TYPE_USING_HINT(                         \\\n+                          at::ScalarType::Double, src_t, __VA_ARGS__)          \\\n+                          AT_PRIVATE_CASE_TYPE_USING_HINT(                     \\\n+                              at::ScalarType::Float, src_t, __VA_ARGS__)       \\\n+                              AT_PRIVATE_CASE_TYPE_USING_HINT(                 \\\n+                                  at::ScalarType::ComplexDouble,               \\\n+                                  src_t,                                       \\\n+                                  __VA_ARGS__)                                 \\\n+                                  AT_PRIVATE_CASE_TYPE_USING_HINT(             \\\n+                                      at::ScalarType::ComplexFloat,            \\\n+                                      src_t,                                   \\\n+                                      __VA_ARGS__)                             \\\n+                                      AT_PRIVATE_CASE_TYPE_USING_HINT(         \\\n+                                          at::ScalarType::Half,                \\\n+                                          src_t,                               \\\n+                                          __VA_ARGS__)                         \\\n+                                          AT_PRIVATE_CASE_TYPE_USING_HINT(     \\\n+                                              at::ScalarType::BFloat16,        \\\n+                                              src_t,                           \\\n+                                              __VA_ARGS__)                     \\\n+                                              AT_PRIVATE_CASE_TYPE_USING_HINT( \\\n+                                                  at::ScalarType::Bool,        \\\n+                                                  src_t,                       \\\n+                                                  __VA_ARGS__))\n+\n+namespace {\n+\n+template <\n+    typename T,\n+    typename src_t,\n+    int depth,\n+    int r_args_depth,\n+    int res_arg_index>\n+struct CopyFunctor {\n+  static_assert(depth == 2 && r_args_depth == 1 && res_arg_index == 1);\n+  template <typename Op>\n+  __device__ __forceinline__ void operator()(\n+      int chunk_size,\n+      TensorListMetadata<depth>& tl,\n+      Op op) {\n+    const auto tensor_loc = tl.block_to_tensor[blockIdx.x];\n+    const auto chunk_idx = tl.block_to_chunk[blockIdx.x];\n+    auto n = tl.numel_for_tensor[tensor_loc];\n+\n+    src_t* src_ptr = (src_t*)tl.addresses[0][tensor_loc];\n+    src_ptr += chunk_idx * chunk_size;\n+    T* self_ptr = (T*)tl.addresses[1][tensor_loc];\n+    self_ptr += chunk_idx * chunk_size;\n+\n+    const bool all_aligned{is_aligned(src_ptr) && is_aligned(self_ptr)};\n+\n+    n -= chunk_idx * chunk_size;\n+    src_t src_args[kILP];\n+    T r_args[kILP];\n+\n+    // to make things simple, we put aligned case in a different code path\n+    if (n % kILP == 0 && chunk_size % kILP == 0 && all_aligned) {\n+      for (int64_t i_start = threadIdx.x;\n+           i_start * kILP < n && i_start * kILP < chunk_size;\n+           i_start += blockDim.x) {\n+        // load\n+        load_store(src_args, src_ptr, 0, i_start);\n+#pragma unroll\n+        for (int ii = 0; ii < kILP; ii++) {\n+          r_args[ii] = static_cast<T>(op(src_args[ii]));\n+        }\n+        // store\n+        load_store(self_ptr, r_args, i_start, 0);\n+      }\n+    } else {\n+      for (int64_t i_start = 0; i_start < n && i_start < chunk_size;\n+           i_start += blockDim.x * kILP) {\n+#pragma unroll\n+        for (int ii = 0; ii < kILP; ii++) {\n+          const auto i = i_start + threadIdx.x + ii * blockDim.x;\n+          if (i < n && i < chunk_size) {\n+            src_args[ii] = src_ptr[i];\n+          }\n+        }\n+#pragma unroll\n+        for (int ii = 0; ii < kILP; ii++) {\n+          r_args[ii] = static_cast<T>(op(src_args[ii]));\n+        }\n+        store_args(self_ptr, r_args, i_start, chunk_size, n);\n+      }\n+    }\n+  }\n+};\n+\n+} // anonymous namespace\n+\n+  if (!(_check_tensors_share_device_and_dtype(\n+            {self, src}, /* skip_dtype_check */ true) &&\n+        std::all_of(\n+            src.cbegin(),\n+            src.cend(),\n+            [&](const auto& t) -> bool {\n+              return t.dtype() == src[0].dtype();\n+            }) &&\n+        _check_tensors_share_sizes_and_strides({self, src}))) {\n+        AT_DISPATCH_SOURCE_TYPES(src[0].scalar_type(), \"foreach_tensor_copy\", [&] {\n+          if constexpr (std::is_same_v<scalar_t, src_t>) {\n+            multi_tensor_apply<2>(\n+                tensor_lists,\n+                UnaryOpFunctor<\n+                    scalar_t,\n+                    /* depth */ 2,\n+                    /* r_args_depth */ 1,\n+                    /* res_arg_index */ 1>(),\n+                Copy<opmath_t, opmath_t>());\n+          } else {\n+            // Ref:\n+            // https://github.com/pytorch/pytorch/blob/656134c38f4737d13c3f43fc5c59470bc23c1d2f/aten/src/ATen/native/Copy.cpp#L299-L301\n+            if (!self[0].is_complex() && src[0].is_complex()) {\n+              TORCH_WARN_ONCE(\n+                  \"Casting complex values to real discards the imaginary part\");\n+            }\n+            multi_tensor_apply<2>(\n+                tensor_lists,\n+                CopyFunctor<\n+                    scalar_t,\n+                    src_t,\n+                    /* depth */ 2,\n+                    /* r_args_depth */ 1,\n+                    /* res_arg_index */ 1>(),\n+                Copy<scalar_t, src_t>());\n+          }\n+        });\n+#undef AT_DISPATCH_SOURCE_TYPES\n+\n",
            "whole_hunk": "@@ -1,9 +1,11 @@\n #define TORCH_ASSERT_ONLY_METHOD_OPERATORS\n #include <ATen/Dispatch.h>\n+#include <ATen/cuda/CUDAContext.h>\n #include <ATen/native/ForeachUtils.h>\n #include <ATen/native/cuda/ForeachFunctors.cuh>\n #include <ATen/native/cuda/ForeachMinMaxFunctors.cuh>\n #include <functional>\n+#include <type_traits>\n \n #ifndef AT_PER_OPERATOR_HEADERS\n #include <ATen/NativeFunctions.h>\n@@ -250,20 +252,154 @@ FOREACH_BINARY_OP_LIST(\n     power_functor,\n     /*division_op*/ true);\n \n-template <typename T>\n-struct Identity {\n-  __device__ __forceinline__ T operator()(const T& x) {\n-    return x;\n+template <typename dst_t, typename src_t = dst_t>\n+struct Copy {\n+  __device__ __forceinline__ dst_t operator()(const src_t& x) {\n+    return static_cast<dst_t>(x);\n   }\n };\n \n+template <typename dst_t>\n+struct Copy<dst_t, c10::complex<double>> {\n+  __device__ __forceinline__ dst_t operator()(const c10::complex<double>& x) {\n+    if constexpr (!(std::is_same_v<dst_t, c10::complex<double>> ||\n+                    std::is_same_v<dst_t, c10::complex<float>>)) {\n+      return static_cast<dst_t>(x.real());\n+    } else {\n+      return static_cast<dst_t>(x);\n+    }\n+  }\n+};\n+\n+template <typename dst_t>\n+struct Copy<dst_t, c10::complex<float>> {\n+  __device__ __forceinline__ dst_t operator()(const c10::complex<float>& x) {\n+    if constexpr (!(std::is_same_v<dst_t, c10::complex<double>> ||\n+                    std::is_same_v<dst_t, c10::complex<float>>)) {\n+      return static_cast<dst_t>(x.real());\n+    } else {\n+      return static_cast<dst_t>(x);\n+    }\n+  }\n+};\n+\n+#define AT_DISPATCH_SOURCE_TYPES(TYPE, NAME, ...)                              \\\n+  AT_DISPATCH_SWITCH(                                                          \\\n+      TYPE,                                                                    \\\n+      NAME,                                                                    \\\n+      AT_PRIVATE_CASE_TYPE_USING_HINT(                                         \\\n+          at::ScalarType::Byte, src_t, __VA_ARGS__)                            \\\n+          AT_PRIVATE_CASE_TYPE_USING_HINT(                                     \\\n+              at::ScalarType::Char, src_t, __VA_ARGS__)                        \\\n+              AT_PRIVATE_CASE_TYPE_USING_HINT(                                 \\\n+                  at::ScalarType::Long, src_t, __VA_ARGS__)                    \\\n+                  AT_PRIVATE_CASE_TYPE_USING_HINT(                             \\\n+                      at::ScalarType::Short, src_t, __VA_ARGS__)               \\\n+                      AT_PRIVATE_CASE_TYPE_USING_HINT(                         \\\n+                          at::ScalarType::Double, src_t, __VA_ARGS__)          \\\n+                          AT_PRIVATE_CASE_TYPE_USING_HINT(                     \\\n+                              at::ScalarType::Float, src_t, __VA_ARGS__)       \\\n+                              AT_PRIVATE_CASE_TYPE_USING_HINT(                 \\\n+                                  at::ScalarType::ComplexDouble,               \\\n+                                  src_t,                                       \\\n+                                  __VA_ARGS__)                                 \\\n+                                  AT_PRIVATE_CASE_TYPE_USING_HINT(             \\\n+                                      at::ScalarType::ComplexFloat,            \\\n+                                      src_t,                                   \\\n+                                      __VA_ARGS__)                             \\\n+                                      AT_PRIVATE_CASE_TYPE_USING_HINT(         \\\n+                                          at::ScalarType::Half,                \\\n+                                          src_t,                               \\\n+                                          __VA_ARGS__)                         \\\n+                                          AT_PRIVATE_CASE_TYPE_USING_HINT(     \\\n+                                              at::ScalarType::BFloat16,        \\\n+                                              src_t,                           \\\n+                                              __VA_ARGS__)                     \\\n+                                              AT_PRIVATE_CASE_TYPE_USING_HINT( \\\n+                                                  at::ScalarType::Bool,        \\\n+                                                  src_t,                       \\\n+                                                  __VA_ARGS__))\n+\n+namespace {\n+\n+template <\n+    typename T,\n+    typename src_t,\n+    int depth,\n+    int r_args_depth,\n+    int res_arg_index>\n+struct CopyFunctor {\n+  static_assert(depth == 2 && r_args_depth == 1 && res_arg_index == 1);\n+  template <typename Op>\n+  __device__ __forceinline__ void operator()(\n+      int chunk_size,\n+      TensorListMetadata<depth>& tl,\n+      Op op) {\n+    const auto tensor_loc = tl.block_to_tensor[blockIdx.x];\n+    const auto chunk_idx = tl.block_to_chunk[blockIdx.x];\n+    auto n = tl.numel_for_tensor[tensor_loc];\n+\n+    src_t* src_ptr = (src_t*)tl.addresses[0][tensor_loc];\n+    src_ptr += chunk_idx * chunk_size;\n+    T* self_ptr = (T*)tl.addresses[1][tensor_loc];\n+    self_ptr += chunk_idx * chunk_size;\n+\n+    const bool all_aligned{is_aligned(src_ptr) && is_aligned(self_ptr)};\n+\n+    n -= chunk_idx * chunk_size;\n+    src_t src_args[kILP];\n+    T r_args[kILP];\n+\n+    // to make things simple, we put aligned case in a different code path\n+    if (n % kILP == 0 && chunk_size % kILP == 0 && all_aligned) {\n+      for (int64_t i_start = threadIdx.x;\n+           i_start * kILP < n && i_start * kILP < chunk_size;\n+           i_start += blockDim.x) {\n+        // load\n+        load_store(src_args, src_ptr, 0, i_start);\n+#pragma unroll\n+        for (int ii = 0; ii < kILP; ii++) {\n+          r_args[ii] = static_cast<T>(op(src_args[ii]));\n+        }\n+        // store\n+        load_store(self_ptr, r_args, i_start, 0);\n+      }\n+    } else {\n+      for (int64_t i_start = 0; i_start < n && i_start < chunk_size;\n+           i_start += blockDim.x * kILP) {\n+#pragma unroll\n+        for (int ii = 0; ii < kILP; ii++) {\n+          const auto i = i_start + threadIdx.x + ii * blockDim.x;\n+          if (i < n && i < chunk_size) {\n+            src_args[ii] = src_ptr[i];\n+          }\n+        }\n+#pragma unroll\n+        for (int ii = 0; ii < kILP; ii++) {\n+          r_args[ii] = static_cast<T>(op(src_args[ii]));\n+        }\n+        store_args(self_ptr, r_args, i_start, chunk_size, n);\n+      }\n+    }\n+  }\n+};\n+\n+} // anonymous namespace\n+\n void foreach_tensor_copy_list_kernel_cuda_(\n     TensorList self,\n     TensorList src,\n     const bool non_blocking) {\n   check_foreach_api_restrictions(self, src);\n-  if (!can_use_fast_route(\n-          self, src, /* does_op_promote_integer_inputs_to_float */ false)) {\n+  if (!(_check_tensors_share_device_and_dtype(\n+            {self, src}, /* skip_dtype_check */ true) &&\n+        std::all_of(\n+            src.cbegin(),\n+            src.cend(),\n+            [&](const auto& t) -> bool {\n+              return t.dtype() == src[0].dtype();\n+            }) &&\n+        _check_tensors_share_sizes_and_strides({self, src}))) {\n     return at::native::foreach_tensor_copy_list_kernel_slow_(\n         self, src, non_blocking);\n   }\n@@ -278,16 +414,38 @@ void foreach_tensor_copy_list_kernel_cuda_(\n       \"foreach_tensor_copy\",\n       [&]() {\n         using opmath_t = at::opmath_type<scalar_t>;\n-        multi_tensor_apply<2>(\n-            tensor_lists,\n-            UnaryOpFunctor<\n-                scalar_t,\n-                /* depth */ 2,\n-                /* r_args_depth */ 1,\n-                /* res_arg_index */ 1>(),\n-            Identity<opmath_t>());\n+        AT_DISPATCH_SOURCE_TYPES(src[0].scalar_type(), \"foreach_tensor_copy\", [&] {\n+          if constexpr (std::is_same_v<scalar_t, src_t>) {\n+            multi_tensor_apply<2>(\n+                tensor_lists,\n+                UnaryOpFunctor<\n+                    scalar_t,\n+                    /* depth */ 2,\n+                    /* r_args_depth */ 1,\n+                    /* res_arg_index */ 1>(),\n+                Copy<opmath_t, opmath_t>());\n+          } else {\n+            // Ref:\n+            // https://github.com/pytorch/pytorch/blob/656134c38f4737d13c3f43fc5c59470bc23c1d2f/aten/src/ATen/native/Copy.cpp#L299-L301\n+            if (!self[0].is_complex() && src[0].is_complex()) {\n+              TORCH_WARN_ONCE(\n+                  \"Casting complex values to real discards the imaginary part\");\n+            }\n+            multi_tensor_apply<2>(\n+                tensor_lists,\n+                CopyFunctor<\n+                    scalar_t,\n+                    src_t,\n+                    /* depth */ 2,\n+                    /* r_args_depth */ 1,\n+                    /* res_arg_index */ 1>(),\n+                Copy<scalar_t, src_t>());\n+          }\n+        });\n       });\n   increment_version(self);\n }\n \n+#undef AT_DISPATCH_SOURCE_TYPES\n+\n } // namespace at::native\n"
        },
        {
            "name": "test_foreach.py",
            "path": "test/test_foreach.py",
            "patches": [
                {
                    "old_start": 1206,
                    "old_length": 6,
                    "new_start": 1206,
                    "new_length": 28,
                    "hunk": "@@ -1206,6 +1206,28 @@ class TestForeach(TestCase):\n                         copy_(t, s, non_blocking)\n                     self.assertEqual(ref_input, sample.input)\n \n+    @onlyCUDA\n+    @ops(filter(lambda op: op.name == \"_foreach_copy\", foreach_binary_op_db))\n+    def test_foreach_copy_with_multi_dtypes(self, device, dtype, op):\n+        # check (a) multi_tensor_apply is called and (b) numerical parity with for-loop and Tensor.copy_\n+        foreach_copy_ = ForeachFuncWrapper(op.inplace_variant)\n+        for sample in op.sample_inputs(device, dtype, noncontiguous=False):\n+            for src_dtype in floating_types_and(torch.half, torch.bfloat16):\n+                if src_dtype == dtype:\n+                    continue\n+                self_tensors = [t.clone() for t in sample.input]\n+                src_tensors = [t.to(src_dtype) for t in self_tensors]\n+                out = foreach_copy_(\n+                    (self_tensors, src_tensors), is_cuda=True, expect_fastpath=True\n+                )\n+                self.assertEqual(\n+                    out,\n+                    [\n+                        torch.empty_like(t).copy_(s)\n+                        for t, s in zip(self_tensors, src_tensors)\n+                    ],\n+                )\n+\n     # Test reverse-mode & forward-mode AD if supported.\n     @onlyCUDA\n     @ops("
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    @onlyCUDA\n+    @ops(filter(lambda op: op.name == \"_foreach_copy\", foreach_binary_op_db))\n+    def test_foreach_copy_with_multi_dtypes(self, device, dtype, op):\n+        # check (a) multi_tensor_apply is called and (b) numerical parity with for-loop and Tensor.copy_\n+        foreach_copy_ = ForeachFuncWrapper(op.inplace_variant)\n+        for sample in op.sample_inputs(device, dtype, noncontiguous=False):\n+            for src_dtype in floating_types_and(torch.half, torch.bfloat16):\n+                if src_dtype == dtype:\n+                    continue\n+                self_tensors = [t.clone() for t in sample.input]\n+                src_tensors = [t.to(src_dtype) for t in self_tensors]\n+                out = foreach_copy_(\n+                    (self_tensors, src_tensors), is_cuda=True, expect_fastpath=True\n+                )\n+                self.assertEqual(\n+                    out,\n+                    [\n+                        torch.empty_like(t).copy_(s)\n+                        for t, s in zip(self_tensors, src_tensors)\n+                    ],\n+                )\n+\n",
            "whole_hunk": "@@ -1206,6 +1206,28 @@ class TestForeach(TestCase):\n                         copy_(t, s, non_blocking)\n                     self.assertEqual(ref_input, sample.input)\n \n+    @onlyCUDA\n+    @ops(filter(lambda op: op.name == \"_foreach_copy\", foreach_binary_op_db))\n+    def test_foreach_copy_with_multi_dtypes(self, device, dtype, op):\n+        # check (a) multi_tensor_apply is called and (b) numerical parity with for-loop and Tensor.copy_\n+        foreach_copy_ = ForeachFuncWrapper(op.inplace_variant)\n+        for sample in op.sample_inputs(device, dtype, noncontiguous=False):\n+            for src_dtype in floating_types_and(torch.half, torch.bfloat16):\n+                if src_dtype == dtype:\n+                    continue\n+                self_tensors = [t.clone() for t in sample.input]\n+                src_tensors = [t.to(src_dtype) for t in self_tensors]\n+                out = foreach_copy_(\n+                    (self_tensors, src_tensors), is_cuda=True, expect_fastpath=True\n+                )\n+                self.assertEqual(\n+                    out,\n+                    [\n+                        torch.empty_like(t).copy_(s)\n+                        for t, s in zip(self_tensors, src_tensors)\n+                    ],\n+                )\n+\n     # Test reverse-mode & forward-mode AD if supported.\n     @onlyCUDA\n     @ops("
        }
    ]
},
{
    "Id": 370,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/16e539e0e68a9a7c100e6b4d7d4ba20d67fad2eb",
    "date": "2023-12-20T15:40:57+00:00",
    "message": "Fix index range check (#116062)\n\nFixes incorrect range check when index is `std::numeric_limits<int64_t>::min()`, as result of unary minus operations for such values is undefined, but in practice is equal to self, see https://godbolt.org/z/Wxhh44ocr\n\nLower bound check was `size >= -index`, which was incorrect if `index` is `INT64_MIN`, with `-1 - index`, which for all int64_t values returns result that also fits into int64_t range. `- (index + 1)` is more readable and results in the identical optimized assembly, see https://godbolt.org/z/3vcnMYf9a , but its intermediate result for `INT64_MAX` is  outside of `int64_t` range, which leads to a similar problems as with `int64_min` in original example.\n\nAdded regression test.\n\nFixes https://github.com/pytorch/pytorch/issues/115415\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116062\nApproved by: https://github.com/Skylion007, https://github.com/albanD",
    "label": "YES",
    "changes": [
        {
            "name": "TensorIndexing.h",
            "path": "aten/src/ATen/TensorIndexing.h",
            "patches": [
                {
                    "old_start": 246,
                    "old_length": 8,
                    "new_start": 246,
                    "new_length": 13,
                    "hunk": "@@ -246,8 +246,13 @@ static inline Tensor applySelect(\n     }\n \n     auto size = (*self_sizes)[dim];\n+    // Note: `size >= -index` is not equivalent to `size > -1 - index` if index\n+    // is INT64_MIN For std::numeric_limits<int64_t>::min() result of unary\n+    // minus is undefined by the standard but in practice is equal to self. On\n+    // the other hand, indexing wraping is valid for all negative int64_t\n+    // values, as x[INT64_MIN] is the same as x[INT64_MAX]\n     TORCH_CHECK_INDEX(\n-        size >= -index && size > index,\n+        size > -1 - index && size > index,\n         \"index \",\n         index,\n         \" is out of bounds for dimension \",\n"
                }
            ],
            "whole_deleted": "-        size >= -index && size > index,\n",
            "whole_added": "+    // Note: `size >= -index` is not equivalent to `size > -1 - index` if index\n+    // is INT64_MIN For std::numeric_limits<int64_t>::min() result of unary\n+    // minus is undefined by the standard but in practice is equal to self. On\n+    // the other hand, indexing wraping is valid for all negative int64_t\n+    // values, as x[INT64_MIN] is the same as x[INT64_MAX]\n+        size > -1 - index && size > index,\n",
            "whole_hunk": "@@ -246,8 +246,13 @@ static inline Tensor applySelect(\n     }\n \n     auto size = (*self_sizes)[dim];\n+    // Note: `size >= -index` is not equivalent to `size > -1 - index` if index\n+    // is INT64_MIN For std::numeric_limits<int64_t>::min() result of unary\n+    // minus is undefined by the standard but in practice is equal to self. On\n+    // the other hand, indexing wraping is valid for all negative int64_t\n+    // values, as x[INT64_MIN] is the same as x[INT64_MAX]\n     TORCH_CHECK_INDEX(\n-        size >= -index && size > index,\n+        size > -1 - index && size > index,\n         \"index \",\n         index,\n         \" is out of bounds for dimension \",\n"
        },
        {
            "name": "TensorShape.cpp",
            "path": "aten/src/ATen/native/TensorShape.cpp",
            "patches": [
                {
                    "old_start": 1828,
                    "old_length": 7,
                    "new_start": 1828,
                    "new_length": 11,
                    "hunk": "@@ -1828,7 +1828,11 @@ Tensor select_symint(const Tensor& self, int64_t dim, c10::SymInt index) {\n   }\n   dim = maybe_wrap_dim(dim, ndim);\n   auto size = self.sym_sizes()[dim];\n-  if (size < -index || size <= index) {\n+  // Note: `size < -index` is not equivalent to `size <= -1 - index` if index is INT64_MIN\n+  // For std::numeric_limits<int64_t>::min() result of unary minus is undefined by the standard\n+  // but in practice is equal to self. On the other hand, indexing wraping is valid for all\n+  // negative int64_t values, as x[INT64_MIN] is the same as x[INT64_MAX]\n+  if (size <= -1 - index || size <= index) {\n     if (self.has_names() && self.names()[dim] != Dimname::wildcard()) {\n       TORCH_CHECK_INDEX(false, \"select(): index \", index, \" out of range for tensor of size \",\n                      self.sizes(), \" at dimension \", self.names()[dim]);\n"
                }
            ],
            "whole_deleted": "-  if (size < -index || size <= index) {\n",
            "whole_added": "+  // Note: `size < -index` is not equivalent to `size <= -1 - index` if index is INT64_MIN\n+  // For std::numeric_limits<int64_t>::min() result of unary minus is undefined by the standard\n+  // but in practice is equal to self. On the other hand, indexing wraping is valid for all\n+  // negative int64_t values, as x[INT64_MIN] is the same as x[INT64_MAX]\n+  if (size <= -1 - index || size <= index) {\n",
            "whole_hunk": "@@ -1828,7 +1828,11 @@ Tensor select_symint(const Tensor& self, int64_t dim, c10::SymInt index) {\n   }\n   dim = maybe_wrap_dim(dim, ndim);\n   auto size = self.sym_sizes()[dim];\n-  if (size < -index || size <= index) {\n+  // Note: `size < -index` is not equivalent to `size <= -1 - index` if index is INT64_MIN\n+  // For std::numeric_limits<int64_t>::min() result of unary minus is undefined by the standard\n+  // but in practice is equal to self. On the other hand, indexing wraping is valid for all\n+  // negative int64_t values, as x[INT64_MIN] is the same as x[INT64_MAX]\n+  if (size <= -1 - index || size <= index) {\n     if (self.has_names() && self.names()[dim] != Dimname::wildcard()) {\n       TORCH_CHECK_INDEX(false, \"select(): index \", index, \" out of range for tensor of size \",\n                      self.sizes(), \" at dimension \", self.names()[dim]);\n"
        },
        {
            "name": "test_indexing.py",
            "path": "test/test_indexing.py",
            "patches": [
                {
                    "old_start": 1395,
                    "old_length": 6,
                    "new_start": 1395,
                    "new_length": 15,
                    "hunk": "@@ -1395,6 +1395,15 @@ class TestIndexing(TestCase):\n             tensor_b[6] = 1.0\n             self.assertEqual(tensor_a, tensor_b.cpu(), atol=0, rtol=0)\n \n+    def test_index_limits(self, device):\n+        #  Regression test for https://github.com/pytorch/pytorch/issues/115415\n+        t = torch.tensor([], device=device)\n+        idx_min = torch.iinfo(torch.int64).min\n+        idx_max = torch.iinfo(torch.int64).max\n+        self.assertRaises(IndexError, lambda: t[idx_min])\n+        self.assertRaises(IndexError, lambda: t[idx_max])\n+\n+\n \n # The tests below are from NumPy test_indexing.py with some modifications to\n # make them compatible with PyTorch. It's licensed under the BDS license below:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    def test_index_limits(self, device):\n+        #  Regression test for https://github.com/pytorch/pytorch/issues/115415\n+        t = torch.tensor([], device=device)\n+        idx_min = torch.iinfo(torch.int64).min\n+        idx_max = torch.iinfo(torch.int64).max\n+        self.assertRaises(IndexError, lambda: t[idx_min])\n+        self.assertRaises(IndexError, lambda: t[idx_max])\n+\n+\n",
            "whole_hunk": "@@ -1395,6 +1395,15 @@ class TestIndexing(TestCase):\n             tensor_b[6] = 1.0\n             self.assertEqual(tensor_a, tensor_b.cpu(), atol=0, rtol=0)\n \n+    def test_index_limits(self, device):\n+        #  Regression test for https://github.com/pytorch/pytorch/issues/115415\n+        t = torch.tensor([], device=device)\n+        idx_min = torch.iinfo(torch.int64).min\n+        idx_max = torch.iinfo(torch.int64).max\n+        self.assertRaises(IndexError, lambda: t[idx_min])\n+        self.assertRaises(IndexError, lambda: t[idx_max])\n+\n+\n \n # The tests below are from NumPy test_indexing.py with some modifications to\n # make them compatible with PyTorch. It's licensed under the BDS license below:"
        }
    ]
},
{
    "Id": 319,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/41902a6ebc1806e7f4d6ce1da604cc9921c6515e",
    "date": "2024-01-29T08:31:26+00:00",
    "message": "[dynamo] Optimize is_tracing checks (#118474)\n\nbenchmarks/dynamo/microbenchmarks/overheads.py\n- before: 10.4us\n- after: 9.9us\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/118474\nApproved by: https://github.com/yanboliang",
    "label": "NO",
    "changes": [
        {
            "name": "eval_frame.py",
            "path": "torch/_dynamo/eval_frame.py",
            "patches": [
                {
                    "old_start": 275,
                    "old_length": 6,
                    "new_start": 275,
                    "new_length": 10,
                    "hunk": "@@ -275,6 +275,10 @@ def nothing():\n     pass\n \n \n+def always_false():\n+    return False\n+\n+\n def innermost_fn(fn):\n     \"\"\"\n     In case of nesting of _TorchDynamoContext calls, find the innermost\n"
                },
                {
                    "old_start": 415,
                    "old_length": 11,
                    "new_start": 419,
                    "new_length": 16,
                    "hunk": "@@ -415,11 +419,16 @@ class _TorchDynamoContext:\n \n         callback = self.callback\n \n+        if isinstance(self, DisableContext):\n+            is_jit_tracing = always_false\n+            is_fx_tracing = always_false\n+        else:\n+            is_jit_tracing = torch._C._is_tracing\n+            is_fx_tracing = torch.fx._symbolic_trace.is_fx_tracing\n+\n         @functools.wraps(fn)\n         def _fn(*args, **kwargs):\n-            if torch.fx._symbolic_trace.is_fx_tracing() and not isinstance(\n-                self, DisableContext\n-            ):\n+            if is_fx_tracing():\n                 if config.error_on_nested_fx_trace:\n                     raise RuntimeError(\n                         \"Detected that you are using FX to symbolically trace \"\n"
                },
                {
                    "old_start": 428,
                    "old_length": 7,
                    "new_start": 437,
                    "new_length": 7,
                    "hunk": "@@ -428,7 +437,7 @@ class _TorchDynamoContext:\n                 else:\n                     return fn(*args, **kwargs)\n \n-            if torch.jit.is_tracing():\n+            if is_jit_tracing():\n                 if config.error_on_nested_jit_trace:\n                     raise RuntimeError(\n                         \"Detected that you are using FX to torch.jit.trace \""
                }
            ],
            "whole_deleted": "-            if torch.fx._symbolic_trace.is_fx_tracing() and not isinstance(\n-                self, DisableContext\n-            ):\n-            if torch.jit.is_tracing():\n",
            "whole_added": "+def always_false():\n+    return False\n+\n+\n+        if isinstance(self, DisableContext):\n+            is_jit_tracing = always_false\n+            is_fx_tracing = always_false\n+        else:\n+            is_jit_tracing = torch._C._is_tracing\n+            is_fx_tracing = torch.fx._symbolic_trace.is_fx_tracing\n+\n+            if is_fx_tracing():\n+            if is_jit_tracing():\n",
            "whole_hunk": "@@ -275,6 +275,10 @@ def nothing():\n     pass\n \n \n+def always_false():\n+    return False\n+\n+\n def innermost_fn(fn):\n     \"\"\"\n     In case of nesting of _TorchDynamoContext calls, find the innermost\n@@ -415,11 +419,16 @@ class _TorchDynamoContext:\n \n         callback = self.callback\n \n+        if isinstance(self, DisableContext):\n+            is_jit_tracing = always_false\n+            is_fx_tracing = always_false\n+        else:\n+            is_jit_tracing = torch._C._is_tracing\n+            is_fx_tracing = torch.fx._symbolic_trace.is_fx_tracing\n+\n         @functools.wraps(fn)\n         def _fn(*args, **kwargs):\n-            if torch.fx._symbolic_trace.is_fx_tracing() and not isinstance(\n-                self, DisableContext\n-            ):\n+            if is_fx_tracing():\n                 if config.error_on_nested_fx_trace:\n                     raise RuntimeError(\n                         \"Detected that you are using FX to symbolically trace \"\n@@ -428,7 +437,7 @@ class _TorchDynamoContext:\n                 else:\n                     return fn(*args, **kwargs)\n \n-            if torch.jit.is_tracing():\n+            if is_jit_tracing():\n                 if config.error_on_nested_jit_trace:\n                     raise RuntimeError(\n                         \"Detected that you are using FX to torch.jit.trace \""
        }
    ]
},
{
    "Id": 55,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/e385bf8ef8f84968c0ff6e47ca5e06d4948e8e62",
    "date": "2024-07-01T14:44:35+00:00",
    "message": "Revert \"[halide-backend] Disable split reductions for Halide (#129320)\"\n\nThis reverts commit a18eb651d352e45860a96869abaf9fb7b215eac6.\n\nReverted https://github.com/pytorch/pytorch/pull/129320 on behalf of https://github.com/jeanschmidt due to This PR is breaking internal builds, please check comments on it D59204360 ([comment](https://github.com/pytorch/pytorch/pull/129320#issuecomment-2200351678))",
    "label": "YES",
    "changes": [
        {
            "name": "common.py",
            "path": "torch/_inductor/codegen/common.py",
            "patches": [
                {
                    "old_start": 154,
                    "old_length": 7,
                    "new_start": 154,
                    "new_length": 6,
                    "hunk": "@@ -154,7 +154,6 @@ class BackendFeature(Enum):\n     TUPLE_REDUCTION = auto()\n     PREFER_STORE_LOOP_ORDER = auto()\n     TRITON_TEMPLATES = auto()\n-    REDUCE_TO_SINGLE_ELEMENT = auto()\n \n \n def get_backend_features(device: Union[torch.device, str]):\n"
                }
            ],
            "whole_deleted": "-    REDUCE_TO_SINGLE_ELEMENT = auto()\n",
            "whole_added": "",
            "whole_hunk": "@@ -154,7 +154,6 @@ class BackendFeature(Enum):\n     TUPLE_REDUCTION = auto()\n     PREFER_STORE_LOOP_ORDER = auto()\n     TRITON_TEMPLATES = auto()\n-    REDUCE_TO_SINGLE_ELEMENT = auto()\n \n \n def get_backend_features(device: Union[torch.device, str]):\n"
        },
        {
            "name": "cpp.py",
            "path": "torch/_inductor/codegen/cpp.py",
            "patches": [
                {
                    "old_start": 3546,
                    "old_length": 7,
                    "new_start": 3546,
                    "new_length": 6,
                    "hunk": "@@ -3546,7 +3546,6 @@ class CppScheduling(BaseScheduling):\n     backend_features = dict.fromkeys(\n         [\n             BackendFeature.INPLACE_BUFFERS,\n-            BackendFeature.REDUCE_TO_SINGLE_ELEMENT,\n         ]\n     )\n \n"
                }
            ],
            "whole_deleted": "-            BackendFeature.REDUCE_TO_SINGLE_ELEMENT,\n",
            "whole_added": "",
            "whole_hunk": "@@ -3546,7 +3546,6 @@ class CppScheduling(BaseScheduling):\n     backend_features = dict.fromkeys(\n         [\n             BackendFeature.INPLACE_BUFFERS,\n-            BackendFeature.REDUCE_TO_SINGLE_ELEMENT,\n         ]\n     )\n \n"
        },
        {
            "name": "halide.py",
            "path": "torch/_inductor/codegen/halide.py",
            "patches": [
                {
                    "old_start": 1541,
                    "old_length": 7,
                    "new_start": 1541,
                    "new_length": 6,
                    "hunk": "@@ -1541,7 +1541,6 @@ class HalideScheduling(SIMDScheduling):\n             [\n                 BackendFeature.TUPLE_REDUCTION,\n                 BackendFeature.PREFER_STORE_LOOP_ORDER,\n-                BackendFeature.REDUCE_TO_SINGLE_ELEMENT,\n             ]\n         )\n         return result\n"
                }
            ],
            "whole_deleted": "-                BackendFeature.REDUCE_TO_SINGLE_ELEMENT,\n",
            "whole_added": "",
            "whole_hunk": "@@ -1541,7 +1541,6 @@ class HalideScheduling(SIMDScheduling):\n             [\n                 BackendFeature.TUPLE_REDUCTION,\n                 BackendFeature.PREFER_STORE_LOOP_ORDER,\n-                BackendFeature.REDUCE_TO_SINGLE_ELEMENT,\n             ]\n         )\n         return result\n"
        },
        {
            "name": "ir.py",
            "path": "torch/_inductor/ir.py",
            "patches": [
                {
                    "old_start": 696,
                    "old_length": 7,
                    "new_start": 696,
                    "new_length": 7,
                    "hunk": "@@ -696,7 +696,7 @@ class Reduction(Loops):\n         numel_hint = V.graph.sizevars.symbolic_hint(sympy_product(ranges))\n \n         should_split = (\n-            not V.graph.has_feature(device, BackendFeature.REDUCE_TO_SINGLE_ELEMENT)\n+            is_gpu(get_device_type(device))\n             and reduction_type\n             not in {\n                 \"argmax\","
                }
            ],
            "whole_deleted": "-            not V.graph.has_feature(device, BackendFeature.REDUCE_TO_SINGLE_ELEMENT)\n",
            "whole_added": "+            is_gpu(get_device_type(device))\n",
            "whole_hunk": "@@ -696,7 +696,7 @@ class Reduction(Loops):\n         numel_hint = V.graph.sizevars.symbolic_hint(sympy_product(ranges))\n \n         should_split = (\n-            not V.graph.has_feature(device, BackendFeature.REDUCE_TO_SINGLE_ELEMENT)\n+            is_gpu(get_device_type(device))\n             and reduction_type\n             not in {\n                 \"argmax\","
        }
    ]
},
{
    "Id": 535,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/e31038d574712d383fdc4c2f1bb63fc82f256ed0",
    "date": "2023-08-30T14:55:18+00:00",
    "message": "Check results dtype in index_out (#108167)\n\nThis logic exists for index_put and index_add, but for some reason not for `index.out`\nSkip testing, as this function is not technically exposed on the Python level.\n\n<!--\ncopilot:poem\n-->\n### <samp>\ud83e\udd16 Generated by Copilot at c688cfd</samp>\n\n> _`index_out` checks types_\n> _avoiding errors in autumn_\n> _complex tensors work_\n\nFixes https://github.com/pytorch/pytorch/issues/107698\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/108167\nApproved by: https://github.com/albanD",
    "label": "YES",
    "changes": [
        {
            "name": "TensorAdvancedIndexing.cpp",
            "path": "aten/src/ATen/native/TensorAdvancedIndexing.cpp",
            "patches": [
                {
                    "old_start": 443,
                    "old_length": 6,
                    "new_start": 443,
                    "new_length": 9,
                    "hunk": "@@ -443,6 +443,9 @@ TORCH_PRECOMPUTE_META_FUNC2(index, Tensor)\n   const auto& result = maybe_get_output();\n \n   if (result.defined()) {\n+    TORCH_CHECK(self.scalar_type() == result.scalar_type(),\n+                \"index_out: self (\", self.scalar_type(), \") and result (\", result.scalar_type(),\n+                \") must have the same scalar type\");\n     at::assert_no_internal_overlap(result);\n     at::assert_no_overlap(result, self);\n     for (const at::OptionalTensorRef& index : materialized) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    TORCH_CHECK(self.scalar_type() == result.scalar_type(),\n+                \"index_out: self (\", self.scalar_type(), \") and result (\", result.scalar_type(),\n+                \") must have the same scalar type\");\n",
            "whole_hunk": "@@ -443,6 +443,9 @@ TORCH_PRECOMPUTE_META_FUNC2(index, Tensor)\n   const auto& result = maybe_get_output();\n \n   if (result.defined()) {\n+    TORCH_CHECK(self.scalar_type() == result.scalar_type(),\n+                \"index_out: self (\", self.scalar_type(), \") and result (\", result.scalar_type(),\n+                \") must have the same scalar type\");\n     at::assert_no_internal_overlap(result);\n     at::assert_no_overlap(result, self);\n     for (const at::OptionalTensorRef& index : materialized) {"
        }
    ]
},
{
    "Id": 457,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/3db0095ea2f351b9935de036a03f29e619093e16",
    "date": "2023-10-31T17:33:24+00:00",
    "message": "[reland][quant][pt2e][be] Cleanup observer insertion logic (#111828) (#112453)\n\nSummary: att, after SharedQuantizationSpec bug fix we are doing some checks before hand, this can simplify the logic when we insert observers\n\nTest Plan:\ncontbuild & OSS CI, see https://hud.pytorch.org/commit/pytorch/pytorch/bf998a2c5d549cf4856c7becfca4a169bf68b709\n\nTest plan from GitHub:\npython test/test_quantization.py TestQuantizePT2E\n\nCIs\n\nDifferential Revision: D50816224\n\nPulled By: jerryzh168\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/112453\nApproved by: https://github.com/andrewor14",
    "label": "NO",
    "changes": [
        {
            "name": "test_xnnpack_quantizer.py",
            "path": "test/quantization/pt2e/test_xnnpack_quantizer.py",
            "patches": [
                {
                    "old_start": 365,
                    "old_length": 12,
                    "new_start": 365,
                    "new_length": 16,
                    "hunk": "@@ -365,12 +365,16 @@ class TestXNNPACKQuantizer(PT2EQuantizationTestCase):\n \n         m = prepare_pt2e(m, quantizer)\n         m(*example_inputs)\n-        self.assertEqual(\n-            id(m.activation_post_process_2), id(m.activation_post_process_3)\n-        )\n-        self.assertEqual(\n-            id(m.activation_post_process_3), id(m.activation_post_process_4)\n-        )\n+        act_post_processes_pairs = []\n+        for n in m.graph.nodes:\n+            if n.target in [\n+                torch.ops.aten.view.default,\n+                torch.ops.aten.hardtanh.default,\n+            ]:\n+                input_act = getattr(m, n.args[0].target)\n+                output_act = getattr(m, list(n.users)[0].target)\n+                self.assertTrue(input_act is output_act)\n+\n         m = convert_pt2e(m, fold_quantize=True)\n         node_occurrence = {\n             # input and output are using quantize_per_tensor and weight is using quantize_per_channel\n"
                }
            ],
            "whole_deleted": "-        self.assertEqual(\n-            id(m.activation_post_process_2), id(m.activation_post_process_3)\n-        )\n-        self.assertEqual(\n-            id(m.activation_post_process_3), id(m.activation_post_process_4)\n-        )\n",
            "whole_added": "+        act_post_processes_pairs = []\n+        for n in m.graph.nodes:\n+            if n.target in [\n+                torch.ops.aten.view.default,\n+                torch.ops.aten.hardtanh.default,\n+            ]:\n+                input_act = getattr(m, n.args[0].target)\n+                output_act = getattr(m, list(n.users)[0].target)\n+                self.assertTrue(input_act is output_act)\n+\n",
            "whole_hunk": "@@ -365,12 +365,16 @@ class TestXNNPACKQuantizer(PT2EQuantizationTestCase):\n \n         m = prepare_pt2e(m, quantizer)\n         m(*example_inputs)\n-        self.assertEqual(\n-            id(m.activation_post_process_2), id(m.activation_post_process_3)\n-        )\n-        self.assertEqual(\n-            id(m.activation_post_process_3), id(m.activation_post_process_4)\n-        )\n+        act_post_processes_pairs = []\n+        for n in m.graph.nodes:\n+            if n.target in [\n+                torch.ops.aten.view.default,\n+                torch.ops.aten.hardtanh.default,\n+            ]:\n+                input_act = getattr(m, n.args[0].target)\n+                output_act = getattr(m, list(n.users)[0].target)\n+                self.assertTrue(input_act is output_act)\n+\n         m = convert_pt2e(m, fold_quantize=True)\n         node_occurrence = {\n             # input and output are using quantize_per_tensor and weight is using quantize_per_channel\n"
        },
        {
            "name": "prepare.py",
            "path": "torch/ao/quantization/pt2e/prepare.py",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 9,
                    "new_start": 1,
                    "new_length": 6,
                    "hunk": "@@ -1,9 +1,6 @@\n import torch\n from torch._subclasses import FakeTensor\n from torch.ao.quantization.fx.prepare import (\n-    _get_arg_as_input_act_obs_or_fq,\n-    _get_output_act_obs_or_fq,\n-    _get_dtype_and_is_dynamic,\n     _insert_obs_or_fq,\n     _save_state,\n     _is_activation_post_process_node,\n"
                },
                {
                    "old_start": 21,
                    "old_length": 7,
                    "new_start": 18,
                    "new_length": 6,
                    "hunk": "@@ -21,7 +18,6 @@ from torch.ao.quantization.qconfig import QConfigAny\n from torch.ao.quantization.fx.custom_config import PrepareCustomConfig\n from typing import Dict, Tuple, Union, Any, Optional\n from torch.ao.quantization.quantizer import (\n-    QuantizationAnnotation,\n     EdgeOrNode,\n     SharedQuantizationSpec,\n     QuantizationSpecBase,\n"
                },
                {
                    "old_start": 260,
                    "old_length": 70,
                    "new_start": 256,
                    "new_length": 56,
                    "hunk": "@@ -260,70 +256,56 @@ def _maybe_insert_input_observer_for_arg_or_kwarg(\n     # default (no observer)\n     new_arg = arg\n \n-    quantization_annotation = node.meta.get(\"quantization_annotation\", QuantizationAnnotation())\n-    arg_as_input_act_obs_or_fq = _get_arg_as_input_act_obs_or_fq(arg, node, named_modules, obs_or_fq_map, is_qat)\n-    arg_as_input_target_dtype, arg_as_input_target_is_dynamic = _get_dtype_and_is_dynamic(arg_as_input_act_obs_or_fq)\n-\n-    arg_as_output_act_obs_or_fq = _get_output_act_obs_or_fq(arg, named_modules, obs_or_fq_map, is_qat)\n-    arg_as_output_target_dtype, arg_as_output_target_is_dynamic = _get_dtype_and_is_dynamic(arg_as_output_act_obs_or_fq)\n-\n-    if arg_as_input_target_is_dynamic or arg_as_input_target_dtype not in [torch.float, None]:\n-        if arg_as_input_target_dtype == arg_as_output_target_dtype and \\\n-           arg_as_input_target_is_dynamic == arg_as_output_target_is_dynamic:\n-            assert _is_activation_post_process_node(arg, named_modules)\n-            assert arg_as_input_act_obs_or_fq is not None\n-            observed_arg = arg.args[0]\n-            assert isinstance(observed_arg, Node), f\"expect observed argument to be a Node, but got: {type(observed_arg)}\"\n-            assert observed_arg in obs_or_fq_map, \\\n-                f\"can't find a sharing group for node: {observed_arg}\"\n-            # reuse the existing obs/fq\n-            arg_as_input_act_obs_or_fq = obs_or_fq_map[observed_arg]\n-            # we don't need to insert new observer node\n-            new_arg = arg\n-        else:\n-            # skip inserting new observers if there is an observer inserted for the arg before\n-            # that has the same dtype that we want to insert here\n-            # alternatively we could have a dedup pass after we insert all observers to deduplicate\n-            # observers\n-            # Example:\n-            # arg -> existing_obs -> conv1\n-            #    \\ -> conv2\n-            #\n-            # instead of inserting new observers we will have:\n-            # arg -> existing_obs -> conv1\n-            #                   \\ -> conv2\n-            existing_obs_node = None\n-            for maybe_obs_node in arg.users.keys():\n-                if maybe_obs_node.op == 'call_module':\n-                    maybe_obs_mod = named_modules[maybe_obs_node.target]  # type: ignore[index]\n-                    if (\n-                        type(maybe_obs_mod) == type(arg_as_input_act_obs_or_fq) and\n-                        maybe_obs_mod.dtype == arg_as_input_target_dtype\n-                    ):\n-                        arg_as_input_act_obs_or_fq = maybe_obs_mod  # type: ignore[assignment]\n-                        existing_obs_node = maybe_obs_node\n-                        break\n-\n-            assert arg_as_input_act_obs_or_fq is not None\n-            if existing_obs_node is None:\n-                maybe_observed_arg = arg\n-                # When quantizing two layers with different configs we can have\n-                # conv2d (int8) -> avgpool(uint8)\n-                # In this case observer insertion for avgpool will come here but the input\n-                # to avgpool will be output observer of conv2d\n-                # Now the obs map that we update must correspond to the original input of\n-                # avgpool and not the output obs of conv2d\n-                # This is because when referring to the edge, quantizer would refer to\n-                # original input and not the observed one.\n-                while _is_activation_post_process_node(arg, named_modules):\n-                    arg = arg.args[0]  # type: ignore[assignment]\n-                arg_as_input_act_obs_or_fq = obs_or_fq_map[(arg, node)]\n-                new_obs_node = _insert_obs_or_fq(\n-                    maybe_observed_arg, arg_as_input_act_obs_or_fq, model, named_modules, model.graph)\n-                # override this arg to be the observed arg\n-                new_arg = new_obs_node\n-            else:\n-                new_arg = existing_obs_node\n+    # find the original `arg` node to the current node, skipping inserted observer/fake_quant nodes\n+    original_arg = arg\n+    while _is_activation_post_process_node(original_arg, named_modules):\n+        original_arg = original_arg.args[0]  # type: ignore[assignment]\n+    assert isinstance(original_arg, Node), f\"expect original argument to be a Node, but got: {type(original_arg)}\"\n+\n+    input_edge = (original_arg, node)\n+    if input_edge not in obs_or_fq_map:\n+        return new_arg\n+    # input_edge needs to be observed\n+    input_edge_obs_or_fq = obs_or_fq_map[input_edge]\n+    if input_edge_obs_or_fq is None:\n+        return new_arg\n+\n+    arg_as_output_obs_or_fq = obs_or_fq_map.get(original_arg, None)\n+    # the arg is observed as the output and is using the same instance as the input_edge\n+    # we'll reuse the inserted observer/fake_quant\n+    if arg_as_output_obs_or_fq is not None and id(arg_as_output_obs_or_fq) == id(input_edge_obs_or_fq):\n+        return new_arg\n+\n+    # otherwise, we'll insert a new observer/fake_quant node\n+\n+    existing_obs_node = None\n+    # skip inserting new observers if there is an observer inserted for the arg before\n+    # that has the same dtype that we want to insert here\n+    # alternatively we could have a dedup pass after we insert all observers to deduplicate\n+    # observers\n+    # Example:\n+    # conv1 -> obs1 -> existing_obs -> conv2\n+    #             \\ -> conv3\n+    #\n+    # instead of inserting new observers we will have:\n+    # conv1 -> obs1 -> existing_obs -> conv2\n+    #                            \\ -> conv3\n+    for maybe_obs_node in arg.users.keys():\n+        if not _is_activation_post_process_node(maybe_obs_node, named_modules):\n+            continue\n+        maybe_obs_mod = named_modules[maybe_obs_node.target]  # type: ignore[index]\n+        if (\n+            type(maybe_obs_mod) == type(input_edge_obs_or_fq) and\n+            maybe_obs_mod.dtype == input_edge_obs_or_fq.dtype\n+        ):\n+            input_edge_obs_or_fq = maybe_obs_mod  # type: ignore[assignment]\n+            existing_obs_node = maybe_obs_node\n+            break\n+\n+    if existing_obs_node is None:\n+        new_arg = _insert_obs_or_fq(arg, input_edge_obs_or_fq, model, named_modules, model.graph)\n+    else:\n+        new_arg = existing_obs_node\n \n     return new_arg\n \n"
                }
            ],
            "whole_deleted": "-    _get_arg_as_input_act_obs_or_fq,\n-    _get_output_act_obs_or_fq,\n-    _get_dtype_and_is_dynamic,\n-    QuantizationAnnotation,\n-    quantization_annotation = node.meta.get(\"quantization_annotation\", QuantizationAnnotation())\n-    arg_as_input_act_obs_or_fq = _get_arg_as_input_act_obs_or_fq(arg, node, named_modules, obs_or_fq_map, is_qat)\n-    arg_as_input_target_dtype, arg_as_input_target_is_dynamic = _get_dtype_and_is_dynamic(arg_as_input_act_obs_or_fq)\n-\n-    arg_as_output_act_obs_or_fq = _get_output_act_obs_or_fq(arg, named_modules, obs_or_fq_map, is_qat)\n-    arg_as_output_target_dtype, arg_as_output_target_is_dynamic = _get_dtype_and_is_dynamic(arg_as_output_act_obs_or_fq)\n-\n-    if arg_as_input_target_is_dynamic or arg_as_input_target_dtype not in [torch.float, None]:\n-        if arg_as_input_target_dtype == arg_as_output_target_dtype and \\\n-           arg_as_input_target_is_dynamic == arg_as_output_target_is_dynamic:\n-            assert _is_activation_post_process_node(arg, named_modules)\n-            assert arg_as_input_act_obs_or_fq is not None\n-            observed_arg = arg.args[0]\n-            assert isinstance(observed_arg, Node), f\"expect observed argument to be a Node, but got: {type(observed_arg)}\"\n-            assert observed_arg in obs_or_fq_map, \\\n-                f\"can't find a sharing group for node: {observed_arg}\"\n-            # reuse the existing obs/fq\n-            arg_as_input_act_obs_or_fq = obs_or_fq_map[observed_arg]\n-            # we don't need to insert new observer node\n-            new_arg = arg\n-        else:\n-            # skip inserting new observers if there is an observer inserted for the arg before\n-            # that has the same dtype that we want to insert here\n-            # alternatively we could have a dedup pass after we insert all observers to deduplicate\n-            # observers\n-            # Example:\n-            # arg -> existing_obs -> conv1\n-            #    \\ -> conv2\n-            #\n-            # instead of inserting new observers we will have:\n-            # arg -> existing_obs -> conv1\n-            #                   \\ -> conv2\n-            existing_obs_node = None\n-            for maybe_obs_node in arg.users.keys():\n-                if maybe_obs_node.op == 'call_module':\n-                    maybe_obs_mod = named_modules[maybe_obs_node.target]  # type: ignore[index]\n-                    if (\n-                        type(maybe_obs_mod) == type(arg_as_input_act_obs_or_fq) and\n-                        maybe_obs_mod.dtype == arg_as_input_target_dtype\n-                    ):\n-                        arg_as_input_act_obs_or_fq = maybe_obs_mod  # type: ignore[assignment]\n-                        existing_obs_node = maybe_obs_node\n-                        break\n-\n-            assert arg_as_input_act_obs_or_fq is not None\n-            if existing_obs_node is None:\n-                maybe_observed_arg = arg\n-                # When quantizing two layers with different configs we can have\n-                # conv2d (int8) -> avgpool(uint8)\n-                # In this case observer insertion for avgpool will come here but the input\n-                # to avgpool will be output observer of conv2d\n-                # Now the obs map that we update must correspond to the original input of\n-                # avgpool and not the output obs of conv2d\n-                # This is because when referring to the edge, quantizer would refer to\n-                # original input and not the observed one.\n-                while _is_activation_post_process_node(arg, named_modules):\n-                    arg = arg.args[0]  # type: ignore[assignment]\n-                arg_as_input_act_obs_or_fq = obs_or_fq_map[(arg, node)]\n-                new_obs_node = _insert_obs_or_fq(\n-                    maybe_observed_arg, arg_as_input_act_obs_or_fq, model, named_modules, model.graph)\n-                # override this arg to be the observed arg\n-                new_arg = new_obs_node\n-            else:\n-                new_arg = existing_obs_node\n",
            "whole_added": "+    # find the original `arg` node to the current node, skipping inserted observer/fake_quant nodes\n+    original_arg = arg\n+    while _is_activation_post_process_node(original_arg, named_modules):\n+        original_arg = original_arg.args[0]  # type: ignore[assignment]\n+    assert isinstance(original_arg, Node), f\"expect original argument to be a Node, but got: {type(original_arg)}\"\n+\n+    input_edge = (original_arg, node)\n+    if input_edge not in obs_or_fq_map:\n+        return new_arg\n+    # input_edge needs to be observed\n+    input_edge_obs_or_fq = obs_or_fq_map[input_edge]\n+    if input_edge_obs_or_fq is None:\n+        return new_arg\n+\n+    arg_as_output_obs_or_fq = obs_or_fq_map.get(original_arg, None)\n+    # the arg is observed as the output and is using the same instance as the input_edge\n+    # we'll reuse the inserted observer/fake_quant\n+    if arg_as_output_obs_or_fq is not None and id(arg_as_output_obs_or_fq) == id(input_edge_obs_or_fq):\n+        return new_arg\n+\n+    # otherwise, we'll insert a new observer/fake_quant node\n+\n+    existing_obs_node = None\n+    # skip inserting new observers if there is an observer inserted for the arg before\n+    # that has the same dtype that we want to insert here\n+    # alternatively we could have a dedup pass after we insert all observers to deduplicate\n+    # observers\n+    # Example:\n+    # conv1 -> obs1 -> existing_obs -> conv2\n+    #             \\ -> conv3\n+    #\n+    # instead of inserting new observers we will have:\n+    # conv1 -> obs1 -> existing_obs -> conv2\n+    #                            \\ -> conv3\n+    for maybe_obs_node in arg.users.keys():\n+        if not _is_activation_post_process_node(maybe_obs_node, named_modules):\n+            continue\n+        maybe_obs_mod = named_modules[maybe_obs_node.target]  # type: ignore[index]\n+        if (\n+            type(maybe_obs_mod) == type(input_edge_obs_or_fq) and\n+            maybe_obs_mod.dtype == input_edge_obs_or_fq.dtype\n+        ):\n+            input_edge_obs_or_fq = maybe_obs_mod  # type: ignore[assignment]\n+            existing_obs_node = maybe_obs_node\n+            break\n+\n+    if existing_obs_node is None:\n+        new_arg = _insert_obs_or_fq(arg, input_edge_obs_or_fq, model, named_modules, model.graph)\n+    else:\n+        new_arg = existing_obs_node\n",
            "whole_hunk": "@@ -1,9 +1,6 @@\n import torch\n from torch._subclasses import FakeTensor\n from torch.ao.quantization.fx.prepare import (\n-    _get_arg_as_input_act_obs_or_fq,\n-    _get_output_act_obs_or_fq,\n-    _get_dtype_and_is_dynamic,\n     _insert_obs_or_fq,\n     _save_state,\n     _is_activation_post_process_node,\n@@ -21,7 +18,6 @@ from torch.ao.quantization.qconfig import QConfigAny\n from torch.ao.quantization.fx.custom_config import PrepareCustomConfig\n from typing import Dict, Tuple, Union, Any, Optional\n from torch.ao.quantization.quantizer import (\n-    QuantizationAnnotation,\n     EdgeOrNode,\n     SharedQuantizationSpec,\n     QuantizationSpecBase,\n@@ -260,70 +256,56 @@ def _maybe_insert_input_observer_for_arg_or_kwarg(\n     # default (no observer)\n     new_arg = arg\n \n-    quantization_annotation = node.meta.get(\"quantization_annotation\", QuantizationAnnotation())\n-    arg_as_input_act_obs_or_fq = _get_arg_as_input_act_obs_or_fq(arg, node, named_modules, obs_or_fq_map, is_qat)\n-    arg_as_input_target_dtype, arg_as_input_target_is_dynamic = _get_dtype_and_is_dynamic(arg_as_input_act_obs_or_fq)\n-\n-    arg_as_output_act_obs_or_fq = _get_output_act_obs_or_fq(arg, named_modules, obs_or_fq_map, is_qat)\n-    arg_as_output_target_dtype, arg_as_output_target_is_dynamic = _get_dtype_and_is_dynamic(arg_as_output_act_obs_or_fq)\n-\n-    if arg_as_input_target_is_dynamic or arg_as_input_target_dtype not in [torch.float, None]:\n-        if arg_as_input_target_dtype == arg_as_output_target_dtype and \\\n-           arg_as_input_target_is_dynamic == arg_as_output_target_is_dynamic:\n-            assert _is_activation_post_process_node(arg, named_modules)\n-            assert arg_as_input_act_obs_or_fq is not None\n-            observed_arg = arg.args[0]\n-            assert isinstance(observed_arg, Node), f\"expect observed argument to be a Node, but got: {type(observed_arg)}\"\n-            assert observed_arg in obs_or_fq_map, \\\n-                f\"can't find a sharing group for node: {observed_arg}\"\n-            # reuse the existing obs/fq\n-            arg_as_input_act_obs_or_fq = obs_or_fq_map[observed_arg]\n-            # we don't need to insert new observer node\n-            new_arg = arg\n-        else:\n-            # skip inserting new observers if there is an observer inserted for the arg before\n-            # that has the same dtype that we want to insert here\n-            # alternatively we could have a dedup pass after we insert all observers to deduplicate\n-            # observers\n-            # Example:\n-            # arg -> existing_obs -> conv1\n-            #    \\ -> conv2\n-            #\n-            # instead of inserting new observers we will have:\n-            # arg -> existing_obs -> conv1\n-            #                   \\ -> conv2\n-            existing_obs_node = None\n-            for maybe_obs_node in arg.users.keys():\n-                if maybe_obs_node.op == 'call_module':\n-                    maybe_obs_mod = named_modules[maybe_obs_node.target]  # type: ignore[index]\n-                    if (\n-                        type(maybe_obs_mod) == type(arg_as_input_act_obs_or_fq) and\n-                        maybe_obs_mod.dtype == arg_as_input_target_dtype\n-                    ):\n-                        arg_as_input_act_obs_or_fq = maybe_obs_mod  # type: ignore[assignment]\n-                        existing_obs_node = maybe_obs_node\n-                        break\n-\n-            assert arg_as_input_act_obs_or_fq is not None\n-            if existing_obs_node is None:\n-                maybe_observed_arg = arg\n-                # When quantizing two layers with different configs we can have\n-                # conv2d (int8) -> avgpool(uint8)\n-                # In this case observer insertion for avgpool will come here but the input\n-                # to avgpool will be output observer of conv2d\n-                # Now the obs map that we update must correspond to the original input of\n-                # avgpool and not the output obs of conv2d\n-                # This is because when referring to the edge, quantizer would refer to\n-                # original input and not the observed one.\n-                while _is_activation_post_process_node(arg, named_modules):\n-                    arg = arg.args[0]  # type: ignore[assignment]\n-                arg_as_input_act_obs_or_fq = obs_or_fq_map[(arg, node)]\n-                new_obs_node = _insert_obs_or_fq(\n-                    maybe_observed_arg, arg_as_input_act_obs_or_fq, model, named_modules, model.graph)\n-                # override this arg to be the observed arg\n-                new_arg = new_obs_node\n-            else:\n-                new_arg = existing_obs_node\n+    # find the original `arg` node to the current node, skipping inserted observer/fake_quant nodes\n+    original_arg = arg\n+    while _is_activation_post_process_node(original_arg, named_modules):\n+        original_arg = original_arg.args[0]  # type: ignore[assignment]\n+    assert isinstance(original_arg, Node), f\"expect original argument to be a Node, but got: {type(original_arg)}\"\n+\n+    input_edge = (original_arg, node)\n+    if input_edge not in obs_or_fq_map:\n+        return new_arg\n+    # input_edge needs to be observed\n+    input_edge_obs_or_fq = obs_or_fq_map[input_edge]\n+    if input_edge_obs_or_fq is None:\n+        return new_arg\n+\n+    arg_as_output_obs_or_fq = obs_or_fq_map.get(original_arg, None)\n+    # the arg is observed as the output and is using the same instance as the input_edge\n+    # we'll reuse the inserted observer/fake_quant\n+    if arg_as_output_obs_or_fq is not None and id(arg_as_output_obs_or_fq) == id(input_edge_obs_or_fq):\n+        return new_arg\n+\n+    # otherwise, we'll insert a new observer/fake_quant node\n+\n+    existing_obs_node = None\n+    # skip inserting new observers if there is an observer inserted for the arg before\n+    # that has the same dtype that we want to insert here\n+    # alternatively we could have a dedup pass after we insert all observers to deduplicate\n+    # observers\n+    # Example:\n+    # conv1 -> obs1 -> existing_obs -> conv2\n+    #             \\ -> conv3\n+    #\n+    # instead of inserting new observers we will have:\n+    # conv1 -> obs1 -> existing_obs -> conv2\n+    #                            \\ -> conv3\n+    for maybe_obs_node in arg.users.keys():\n+        if not _is_activation_post_process_node(maybe_obs_node, named_modules):\n+            continue\n+        maybe_obs_mod = named_modules[maybe_obs_node.target]  # type: ignore[index]\n+        if (\n+            type(maybe_obs_mod) == type(input_edge_obs_or_fq) and\n+            maybe_obs_mod.dtype == input_edge_obs_or_fq.dtype\n+        ):\n+            input_edge_obs_or_fq = maybe_obs_mod  # type: ignore[assignment]\n+            existing_obs_node = maybe_obs_node\n+            break\n+\n+    if existing_obs_node is None:\n+        new_arg = _insert_obs_or_fq(arg, input_edge_obs_or_fq, model, named_modules, model.graph)\n+    else:\n+        new_arg = existing_obs_node\n \n     return new_arg\n \n"
        },
        {
            "name": "quantizer.py",
            "path": "torch/ao/quantization/quantizer/quantizer.py",
            "patches": [
                {
                    "old_start": 138,
                    "old_length": 7,
                    "new_start": 138,
                    "new_length": 9,
                    "hunk": "@@ -138,7 +138,9 @@ class QuantizationAnnotation:\n     \"\"\"\n \n     # a map from torch.fx.Node to a type of QuantizationSpecBase\n-    input_qspec_map: Dict[Node, QuantizationSpecBase] = field(default_factory=dict)\n+    input_qspec_map: Dict[Node, Optional[QuantizationSpecBase]] = field(\n+        default_factory=dict\n+    )\n \n     # How the output of this node is quantized, expressed as QuantizationSpec\n     # TODO: change the value to QuantizationSpec in a separate PR\n"
                }
            ],
            "whole_deleted": "-    input_qspec_map: Dict[Node, QuantizationSpecBase] = field(default_factory=dict)\n",
            "whole_added": "+    input_qspec_map: Dict[Node, Optional[QuantizationSpecBase]] = field(\n+        default_factory=dict\n+    )\n",
            "whole_hunk": "@@ -138,7 +138,9 @@ class QuantizationAnnotation:\n     \"\"\"\n \n     # a map from torch.fx.Node to a type of QuantizationSpecBase\n-    input_qspec_map: Dict[Node, QuantizationSpecBase] = field(default_factory=dict)\n+    input_qspec_map: Dict[Node, Optional[QuantizationSpecBase]] = field(\n+        default_factory=dict\n+    )\n \n     # How the output of this node is quantized, expressed as QuantizationSpec\n     # TODO: change the value to QuantizationSpec in a separate PR\n"
        },
        {
            "name": "xnnpack_quantizer.py",
            "path": "torch/ao/quantization/quantizer/xnnpack_quantizer.py",
            "patches": [
                {
                    "old_start": 154,
                    "old_length": 12,
                    "new_start": 154,
                    "new_length": 7,
                    "hunk": "@@ -154,12 +154,7 @@ def get_symmetric_quantization_config(\n         ),\n     )\n \n-    bias_observer_or_fake_quant_ctr: _ObserverOrFakeQuantizeConstructor = (\n-        PlaceholderObserver\n-    )\n-    bias_quantization_spec = QuantizationSpec(\n-        dtype=torch.float, observer_or_fake_quant_ctr=bias_observer_or_fake_quant_ctr\n-    )\n+    bias_quantization_spec = None\n     if is_dynamic:\n         quantization_config = QuantizationConfig(\n             act_quantization_spec,"
                }
            ],
            "whole_deleted": "-    bias_observer_or_fake_quant_ctr: _ObserverOrFakeQuantizeConstructor = (\n-        PlaceholderObserver\n-    )\n-    bias_quantization_spec = QuantizationSpec(\n-        dtype=torch.float, observer_or_fake_quant_ctr=bias_observer_or_fake_quant_ctr\n-    )\n",
            "whole_added": "+    bias_quantization_spec = None\n",
            "whole_hunk": "@@ -154,12 +154,7 @@ def get_symmetric_quantization_config(\n         ),\n     )\n \n-    bias_observer_or_fake_quant_ctr: _ObserverOrFakeQuantizeConstructor = (\n-        PlaceholderObserver\n-    )\n-    bias_quantization_spec = QuantizationSpec(\n-        dtype=torch.float, observer_or_fake_quant_ctr=bias_observer_or_fake_quant_ctr\n-    )\n+    bias_quantization_spec = None\n     if is_dynamic:\n         quantization_config = QuantizationConfig(\n             act_quantization_spec,"
        }
    ]
},
{
    "Id": 86,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/8629939a51813def63363ff3bdfe1a6e56c69e18",
    "date": "2024-06-14T16:01:12+00:00",
    "message": "[torch/c10] Add C10_UBSAN_ENABLED macro and use it to disable SymInt_\u2026 (#127967)\n\nAdds `C10_UBSAN_ENABLED` macro and use it to disable `SymIntTest::Overflows` (fails under `signed-integer-overflow` UBSAN check).\n\nAlso cleans up UBSAN guard in `jit/test_misc.cpp` to use `C10_UBSAN_ENABLED`  and the existing `C10_ASAN_ENABLED` instead of locally defining `HAS_ASANUBSAN`.\n\n> NOTE: This should fix `SymIntTest::Overflows` failing under ubsan in fbcode too...\nPull Request resolved: https://github.com/pytorch/pytorch/pull/127967\nApproved by: https://github.com/atalman, https://github.com/d4l3k, https://github.com/malfet",
    "label": "NO",
    "changes": [
        {
            "name": "Macros.h",
            "path": "c10/macros/Macros.h",
            "patches": [
                {
                    "old_start": 64,
                    "old_length": 6,
                    "new_start": 64,
                    "new_length": 25,
                    "hunk": "@@ -64,6 +64,25 @@\n #define C10_ASAN_ENABLED 0\n #endif\n \n+// Detect undefined-behavior sanitizer (UBSAN)\n+#undef C10_UBSAN_ENABLED\n+\n+// for clang or gcc >= 14\n+// NB: gcc 14 adds support for Clang's __has_feature\n+//   https://gcc.gnu.org/gcc-14/changes.html\n+//   gcc < 14 doesn't have a macro for UBSAN\n+//   (e.g. __SANITIZE_UNDEFINED__ does not exist in gcc)\n+//   https://github.com/google/sanitizers/issues/765\n+#if defined(__has_feature)\n+#if ((__has_feature(undefined_behavior_sanitizer)))\n+#define C10_UBSAN_ENABLED 1\n+#endif\n+#endif\n+\n+#if !defined(C10_UBSAN_ENABLED)\n+#define C10_UBSAN_ENABLED 0\n+#endif\n+\n // Disable the copy and assignment operator for a class. Note that this will\n // disable the usage of the class in std containers.\n #define C10_DISABLE_COPY_AND_ASSIGN(classname) \\\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+// Detect undefined-behavior sanitizer (UBSAN)\n+#undef C10_UBSAN_ENABLED\n+\n+// for clang or gcc >= 14\n+// NB: gcc 14 adds support for Clang's __has_feature\n+//   https://gcc.gnu.org/gcc-14/changes.html\n+//   gcc < 14 doesn't have a macro for UBSAN\n+//   (e.g. __SANITIZE_UNDEFINED__ does not exist in gcc)\n+//   https://github.com/google/sanitizers/issues/765\n+#if defined(__has_feature)\n+#if ((__has_feature(undefined_behavior_sanitizer)))\n+#define C10_UBSAN_ENABLED 1\n+#endif\n+#endif\n+\n+#if !defined(C10_UBSAN_ENABLED)\n+#define C10_UBSAN_ENABLED 0\n+#endif\n+\n",
            "whole_hunk": "@@ -64,6 +64,25 @@\n #define C10_ASAN_ENABLED 0\n #endif\n \n+// Detect undefined-behavior sanitizer (UBSAN)\n+#undef C10_UBSAN_ENABLED\n+\n+// for clang or gcc >= 14\n+// NB: gcc 14 adds support for Clang's __has_feature\n+//   https://gcc.gnu.org/gcc-14/changes.html\n+//   gcc < 14 doesn't have a macro for UBSAN\n+//   (e.g. __SANITIZE_UNDEFINED__ does not exist in gcc)\n+//   https://github.com/google/sanitizers/issues/765\n+#if defined(__has_feature)\n+#if ((__has_feature(undefined_behavior_sanitizer)))\n+#define C10_UBSAN_ENABLED 1\n+#endif\n+#endif\n+\n+#if !defined(C10_UBSAN_ENABLED)\n+#define C10_UBSAN_ENABLED 0\n+#endif\n+\n // Disable the copy and assignment operator for a class. Note that this will\n // disable the usage of the class in std containers.\n #define C10_DISABLE_COPY_AND_ASSIGN(classname) \\\n"
        },
        {
            "name": "SymInt_test.cpp",
            "path": "c10/test/core/SymInt_test.cpp",
            "patches": [
                {
                    "old_start": 2,
                    "old_length": 6,
                    "new_start": 2,
                    "new_length": 7,
                    "hunk": "@@ -2,6 +2,7 @@\n \n #include <c10/core/SymInt.h>\n #include <c10/core/SymNodeImpl.h>\n+#include <c10/macros/Macros.h>\n \n using namespace c10;\n #ifndef C10_MOBILE\n"
                },
                {
                    "old_start": 22,
                    "old_length": 6,
                    "new_start": 23,
                    "new_length": 8,
                    "hunk": "@@ -22,6 +23,8 @@ TEST(SymIntTest, CheckRange) {\n   EXPECT_FALSE(SymInt::check_range(INT64_MIN));\n }\n \n+#if !C10_UBSAN_ENABLED\n+// This test fails signed-integer-overflow UBSAN check\n TEST(SymIntTest, Overflows) {\n   const auto x = SymInt(INT64_MAX);\n   EXPECT_NE(-(x + 1), 0);\n"
                },
                {
                    "old_start": 30,
                    "old_length": 5,
                    "new_start": 33,
                    "new_length": 6,
                    "hunk": "@@ -30,5 +33,6 @@ TEST(SymIntTest, Overflows) {\n   EXPECT_NE(-y, 0);\n   EXPECT_NE(0 - y, 0);\n }\n+#endif\n \n #endif\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include <c10/macros/Macros.h>\n+#if !C10_UBSAN_ENABLED\n+// This test fails signed-integer-overflow UBSAN check\n+#endif\n",
            "whole_hunk": "@@ -2,6 +2,7 @@\n \n #include <c10/core/SymInt.h>\n #include <c10/core/SymNodeImpl.h>\n+#include <c10/macros/Macros.h>\n \n using namespace c10;\n #ifndef C10_MOBILE\n@@ -22,6 +23,8 @@ TEST(SymIntTest, CheckRange) {\n   EXPECT_FALSE(SymInt::check_range(INT64_MIN));\n }\n \n+#if !C10_UBSAN_ENABLED\n+// This test fails signed-integer-overflow UBSAN check\n TEST(SymIntTest, Overflows) {\n   const auto x = SymInt(INT64_MAX);\n   EXPECT_NE(-(x + 1), 0);\n@@ -30,5 +33,6 @@ TEST(SymIntTest, Overflows) {\n   EXPECT_NE(-y, 0);\n   EXPECT_NE(0 - y, 0);\n }\n+#endif\n \n #endif\n"
        },
        {
            "name": "test_misc.cpp",
            "path": "test/cpp/jit/test_misc.cpp",
            "patches": [
                {
                    "old_start": 6,
                    "old_length": 6,
                    "new_start": 6,
                    "new_length": 7,
                    "hunk": "@@ -6,6 +6,7 @@\n #include <ATen/core/interned_strings.h>\n #include <ATen/core/ivalue.h>\n #include <ATen/core/jit_type_base.h>\n+#include <c10/macros/Macros.h>\n #include <test/cpp/jit/test_utils.h>\n #include <torch/csrc/jit/passes/remove_mutation.h>\n #include <torch/csrc/jit/passes/tensorexpr_fuser.h>\n"
                },
                {
                    "old_start": 491,
                    "old_length": 13,
                    "new_start": 492,
                    "new_length": 7,
                    "hunk": "@@ -491,13 +492,7 @@ TEST(ControlFlowTest, Basic) {\n   ASSERT_EQ(256, run_binary(\"while_test\", 2, 0));\n }\n \n-#if defined(__has_feature)\n-#if __has_feature(address_sanitizer)\n-#define HAS_ASANUBSAN 1\n-#endif\n-#endif\n-\n-#ifndef HAS_ASANUBSAN\n+#if !(C10_ASAN_ENABLED || C10_UBSAN_ENABLED)\n // This test fails vptr UBSAN checks\n \n TEST(ProtoTest, Basic) {"
                }
            ],
            "whole_deleted": "-#if defined(__has_feature)\n-#if __has_feature(address_sanitizer)\n-#define HAS_ASANUBSAN 1\n-#endif\n-#endif\n-\n-#ifndef HAS_ASANUBSAN\n",
            "whole_added": "+#include <c10/macros/Macros.h>\n+#if !(C10_ASAN_ENABLED || C10_UBSAN_ENABLED)\n",
            "whole_hunk": "@@ -6,6 +6,7 @@\n #include <ATen/core/interned_strings.h>\n #include <ATen/core/ivalue.h>\n #include <ATen/core/jit_type_base.h>\n+#include <c10/macros/Macros.h>\n #include <test/cpp/jit/test_utils.h>\n #include <torch/csrc/jit/passes/remove_mutation.h>\n #include <torch/csrc/jit/passes/tensorexpr_fuser.h>\n@@ -491,13 +492,7 @@ TEST(ControlFlowTest, Basic) {\n   ASSERT_EQ(256, run_binary(\"while_test\", 2, 0));\n }\n \n-#if defined(__has_feature)\n-#if __has_feature(address_sanitizer)\n-#define HAS_ASANUBSAN 1\n-#endif\n-#endif\n-\n-#ifndef HAS_ASANUBSAN\n+#if !(C10_ASAN_ENABLED || C10_UBSAN_ENABLED)\n // This test fails vptr UBSAN checks\n \n TEST(ProtoTest, Basic) {"
        }
    ]
},
{
    "Id": 452,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/db7a3cc436df016cf4d1b9edf7557621f5cdbf47",
    "date": "2023-11-02T21:28:05+00:00",
    "message": "fix missing nvml in c10/cuda/driver_api.cpp issue (#112121)\n\nSince https://github.com/pytorch/pytorch/pull/99699 introduced a dependency on nvml for oom reporting in `c10/cuda/driver_api.h`, `c10/cuda/driver_api.cpp`, and `reportProcessMemoryInfo` from `c10/cuda/CUDACachingAllocator.cpp`, we've seen failures regarding cuda expandable segments and oom reporting in NVIDIA's internal CI, specifically on Jetson devices which don't have nvml support as it is incompatible with Jetson. Example failures using the latest upstream on Orin AGX node:\n\n`python test/test_cuda.py -k test_notifies_oom` generates\n\n```\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/pytorch/pytorch/test/test_cuda.py\", line 1643, in _worker\n    results[t] = torch.nn.functional.conv2d(results[t], weight, padding=0)\nRuntimeError: CUDA driver error: out of memory\n```\n\n`python test/test_cuda_expandable_segments.py` generates\n\n```\nTraceback (most recent call last):\n  File \"/opt/pytorch/pytorch/test/test_cuda_expandable_segments.py\", line 12, in <module>\n    exec(compile(open(filepath).read(), filepath, mode='exec'))\n  File \"/opt/pytorch/pytorch/test/test_cuda.py\", line 66, in <module>\n    class TestCuda(TestCase):\n  File \"/opt/pytorch/pytorch/test/test_cuda.py\", line 1609, in TestCuda\n    @unittest.skipIf(not TEST_CUDNN, 'CUDNN not available')\n  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_utils.py\", line 4628, in wrapped\n    self._value = self._cb()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_cuda.py\", line 20, in <lambda>\n    TEST_CUDNN = LazyVal(lambda: TEST_CUDA and torch.backends.cudnn.is_acceptable(torch.tensor(1., device=CUDA_DEVICE)))\nRuntimeError: handle_0 INTERNAL ASSERT FAILED at \"/opt/pytorch/pytorch/c10/cuda/driver_api.cpp\":15, please report a bug to PyTorch.\n```\n\nThis PR intends to fix this issue by adding various dlopen checks to make sure nvml actually exists, and safely fall back to using the older libcuda based features of cuda expandable segments and oom reporting if nvml is not found.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/112121\nApproved by: https://github.com/eqy, https://github.com/ngimel, https://github.com/albanD",
    "label": "YES",
    "changes": [
        {
            "name": "CUDACachingAllocator.cpp",
            "path": "c10/cuda/CUDACachingAllocator.cpp",
            "patches": [
                {
                    "old_start": 790,
                    "old_length": 6,
                    "new_start": 790,
                    "new_length": 10,
                    "hunk": "@@ -790,6 +790,10 @@ cudaError_t cudaMallocMaybeCapturing(void** p, size_t size) {\n \n static std::string reportProcessMemoryInfo(int device) {\n #ifdef PYTORCH_C10_DRIVER_API_SUPPORTED\n+  void* nvml_handle = DriverAPI::get_nvml_handle();\n+  if (!nvml_handle) {\n+    return \"\";\n+  }\n   static c10::once_flag nvml_init;\n   c10::call_once(nvml_init, [] {\n     TORCH_INTERNAL_ASSERT(NVML_SUCCESS == DriverAPI::get()->nvmlInit_v2_());\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  void* nvml_handle = DriverAPI::get_nvml_handle();\n+  if (!nvml_handle) {\n+    return \"\";\n+  }\n",
            "whole_hunk": "@@ -790,6 +790,10 @@ cudaError_t cudaMallocMaybeCapturing(void** p, size_t size) {\n \n static std::string reportProcessMemoryInfo(int device) {\n #ifdef PYTORCH_C10_DRIVER_API_SUPPORTED\n+  void* nvml_handle = DriverAPI::get_nvml_handle();\n+  if (!nvml_handle) {\n+    return \"\";\n+  }\n   static c10::once_flag nvml_init;\n   c10::call_once(nvml_init, [] {\n     TORCH_INTERNAL_ASSERT(NVML_SUCCESS == DriverAPI::get()->nvmlInit_v2_());\n"
        },
        {
            "name": "driver_api.cpp",
            "path": "c10/cuda/driver_api.cpp",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 5,
                    "new_start": 1,
                    "new_length": 6,
                    "hunk": "@@ -1,5 +1,6 @@\n #if !defined(USE_ROCM) && defined(PYTORCH_C10_DRIVER_API_SUPPORTED)\n #include <c10/cuda/driver_api.h>\n+#include <c10/util/CallOnce.h>\n #include <c10/util/Exception.h>\n #include <dlfcn.h>\n #include <iostream>\n"
                },
                {
                    "old_start": 7,
                    "old_length": 24,
                    "new_start": 8,
                    "new_length": 38,
                    "hunk": "@@ -7,24 +8,38 @@ namespace c10 {\n namespace cuda {\n \n namespace {\n-DriverAPI create_driver_api() {\n-#define OPEN_LIBRARIES(name, n)               \\\n-  void* handle_##n = dlopen(name, RTLD_LAZY); \\\n-  TORCH_INTERNAL_ASSERT(handle_##n);\n \n-  C10_FORALL_DRIVER_LIBRARIES(OPEN_LIBRARIES)\n-#undef OPEN_LIBRARIES\n+DriverAPI create_driver_api() {\n+  void* handle_0 = dlopen(\"libcuda.so\", RTLD_LAZY | RTLD_NOLOAD);\n+  TORCH_INTERNAL_ASSERT(handle_0);\n+  void* handle_1 = DriverAPI::get_nvml_handle();\n   DriverAPI r{};\n \n-#define LOOKUP_ENTRY(name, n)                              \\\n-  r.name##_ = ((decltype(&name))dlsym(handle_##n, #name)); \\\n+#define LOOKUP_LIBCUDA_ENTRY(name)                       \\\n+  r.name##_ = ((decltype(&name))dlsym(handle_0, #name)); \\\n+  TORCH_INTERNAL_ASSERT(r.name##_)\n+  C10_LIBCUDA_DRIVER_API(LOOKUP_LIBCUDA_ENTRY)\n+#undef LOOKUP_LIBCUDA_ENTRY\n+\n+  if (handle_1) {\n+#define LOOKUP_NVML_ENTRY(name)                          \\\n+  r.name##_ = ((decltype(&name))dlsym(handle_1, #name)); \\\n   TORCH_INTERNAL_ASSERT(r.name##_)\n-  C10_FORALL_DRIVER_API(LOOKUP_ENTRY)\n-#undef LOOKUP_ENTRY\n+    C10_NVML_DRIVER_API(LOOKUP_NVML_ENTRY)\n+#undef LOOKUP_NVML_ENTRY\n+  }\n   return r;\n }\n } // namespace\n \n+void* DriverAPI::get_nvml_handle() {\n+  static c10::once_flag once;\n+  static void* handle_1;\n+  c10::call_once(\n+      once, [] { handle_1 = dlopen(\"libnvidia-ml.so.1\", RTLD_LAZY); });\n+  return handle_1;\n+}\n+\n DriverAPI* DriverAPI::get() {\n   static DriverAPI singleton = create_driver_api();\n   return &singleton;\n"
                }
            ],
            "whole_deleted": "-DriverAPI create_driver_api() {\n-#define OPEN_LIBRARIES(name, n)               \\\n-  void* handle_##n = dlopen(name, RTLD_LAZY); \\\n-  TORCH_INTERNAL_ASSERT(handle_##n);\n-  C10_FORALL_DRIVER_LIBRARIES(OPEN_LIBRARIES)\n-#undef OPEN_LIBRARIES\n-#define LOOKUP_ENTRY(name, n)                              \\\n-  r.name##_ = ((decltype(&name))dlsym(handle_##n, #name)); \\\n-  C10_FORALL_DRIVER_API(LOOKUP_ENTRY)\n-#undef LOOKUP_ENTRY\n",
            "whole_added": "+#include <c10/util/CallOnce.h>\n+DriverAPI create_driver_api() {\n+  void* handle_0 = dlopen(\"libcuda.so\", RTLD_LAZY | RTLD_NOLOAD);\n+  TORCH_INTERNAL_ASSERT(handle_0);\n+  void* handle_1 = DriverAPI::get_nvml_handle();\n+#define LOOKUP_LIBCUDA_ENTRY(name)                       \\\n+  r.name##_ = ((decltype(&name))dlsym(handle_0, #name)); \\\n+  TORCH_INTERNAL_ASSERT(r.name##_)\n+  C10_LIBCUDA_DRIVER_API(LOOKUP_LIBCUDA_ENTRY)\n+#undef LOOKUP_LIBCUDA_ENTRY\n+\n+  if (handle_1) {\n+#define LOOKUP_NVML_ENTRY(name)                          \\\n+  r.name##_ = ((decltype(&name))dlsym(handle_1, #name)); \\\n+    C10_NVML_DRIVER_API(LOOKUP_NVML_ENTRY)\n+#undef LOOKUP_NVML_ENTRY\n+  }\n+void* DriverAPI::get_nvml_handle() {\n+  static c10::once_flag once;\n+  static void* handle_1;\n+  c10::call_once(\n+      once, [] { handle_1 = dlopen(\"libnvidia-ml.so.1\", RTLD_LAZY); });\n+  return handle_1;\n+}\n+\n",
            "whole_hunk": "@@ -1,5 +1,6 @@\n #if !defined(USE_ROCM) && defined(PYTORCH_C10_DRIVER_API_SUPPORTED)\n #include <c10/cuda/driver_api.h>\n+#include <c10/util/CallOnce.h>\n #include <c10/util/Exception.h>\n #include <dlfcn.h>\n #include <iostream>\n@@ -7,24 +8,38 @@ namespace c10 {\n namespace cuda {\n \n namespace {\n-DriverAPI create_driver_api() {\n-#define OPEN_LIBRARIES(name, n)               \\\n-  void* handle_##n = dlopen(name, RTLD_LAZY); \\\n-  TORCH_INTERNAL_ASSERT(handle_##n);\n \n-  C10_FORALL_DRIVER_LIBRARIES(OPEN_LIBRARIES)\n-#undef OPEN_LIBRARIES\n+DriverAPI create_driver_api() {\n+  void* handle_0 = dlopen(\"libcuda.so\", RTLD_LAZY | RTLD_NOLOAD);\n+  TORCH_INTERNAL_ASSERT(handle_0);\n+  void* handle_1 = DriverAPI::get_nvml_handle();\n   DriverAPI r{};\n \n-#define LOOKUP_ENTRY(name, n)                              \\\n-  r.name##_ = ((decltype(&name))dlsym(handle_##n, #name)); \\\n+#define LOOKUP_LIBCUDA_ENTRY(name)                       \\\n+  r.name##_ = ((decltype(&name))dlsym(handle_0, #name)); \\\n+  TORCH_INTERNAL_ASSERT(r.name##_)\n+  C10_LIBCUDA_DRIVER_API(LOOKUP_LIBCUDA_ENTRY)\n+#undef LOOKUP_LIBCUDA_ENTRY\n+\n+  if (handle_1) {\n+#define LOOKUP_NVML_ENTRY(name)                          \\\n+  r.name##_ = ((decltype(&name))dlsym(handle_1, #name)); \\\n   TORCH_INTERNAL_ASSERT(r.name##_)\n-  C10_FORALL_DRIVER_API(LOOKUP_ENTRY)\n-#undef LOOKUP_ENTRY\n+    C10_NVML_DRIVER_API(LOOKUP_NVML_ENTRY)\n+#undef LOOKUP_NVML_ENTRY\n+  }\n   return r;\n }\n } // namespace\n \n+void* DriverAPI::get_nvml_handle() {\n+  static c10::once_flag once;\n+  static void* handle_1;\n+  c10::call_once(\n+      once, [] { handle_1 = dlopen(\"libnvidia-ml.so.1\", RTLD_LAZY); });\n+  return handle_1;\n+}\n+\n DriverAPI* DriverAPI::get() {\n   static DriverAPI singleton = create_driver_api();\n   return &singleton;\n"
        },
        {
            "name": "driver_api.h",
            "path": "c10/cuda/driver_api.h",
            "patches": [
                {
                    "old_start": 18,
                    "old_length": 31,
                    "new_start": 18,
                    "new_length": 31,
                    "hunk": "@@ -18,31 +18,31 @@\n     }                                                                      \\\n   } while (0)\n \n-#define C10_FORALL_DRIVER_LIBRARIES(_) \\\n-  _(\"libcuda.so\", 0)                   \\\n-  _(\"libnvidia-ml.so.1\", 1)\n+#define C10_LIBCUDA_DRIVER_API(_) \\\n+  _(cuMemAddressReserve)          \\\n+  _(cuMemRelease)                 \\\n+  _(cuMemMap)                     \\\n+  _(cuMemAddressFree)             \\\n+  _(cuMemSetAccess)               \\\n+  _(cuMemUnmap)                   \\\n+  _(cuMemCreate)                  \\\n+  _(cuGetErrorString)\n \n-#define C10_FORALL_DRIVER_API(_)         \\\n-  _(cuMemAddressReserve, 0)              \\\n-  _(cuMemRelease, 0)                     \\\n-  _(cuMemMap, 0)                         \\\n-  _(cuMemAddressFree, 0)                 \\\n-  _(cuMemSetAccess, 0)                   \\\n-  _(cuMemUnmap, 0)                       \\\n-  _(cuMemCreate, 0)                      \\\n-  _(cuGetErrorString, 0)                 \\\n-  _(nvmlInit_v2, 1)                      \\\n-  _(nvmlDeviceGetHandleByPciBusId_v2, 1) \\\n-  _(nvmlDeviceGetComputeRunningProcesses, 1)\n+#define C10_NVML_DRIVER_API(_)        \\\n+  _(nvmlInit_v2)                      \\\n+  _(nvmlDeviceGetHandleByPciBusId_v2) \\\n+  _(nvmlDeviceGetComputeRunningProcesses)\n \n namespace c10 {\n namespace cuda {\n \n struct DriverAPI {\n-#define CREATE_MEMBER(name, n) decltype(&name) name##_;\n-  C10_FORALL_DRIVER_API(CREATE_MEMBER)\n+#define CREATE_MEMBER(name) decltype(&name) name##_;\n+  C10_LIBCUDA_DRIVER_API(CREATE_MEMBER)\n+  C10_NVML_DRIVER_API(CREATE_MEMBER)\n #undef CREATE_MEMBER\n   static DriverAPI* get();\n+  static void* get_nvml_handle();\n };\n \n } // namespace cuda"
                }
            ],
            "whole_deleted": "-#define C10_FORALL_DRIVER_LIBRARIES(_) \\\n-  _(\"libcuda.so\", 0)                   \\\n-  _(\"libnvidia-ml.so.1\", 1)\n-#define C10_FORALL_DRIVER_API(_)         \\\n-  _(cuMemAddressReserve, 0)              \\\n-  _(cuMemRelease, 0)                     \\\n-  _(cuMemMap, 0)                         \\\n-  _(cuMemAddressFree, 0)                 \\\n-  _(cuMemSetAccess, 0)                   \\\n-  _(cuMemUnmap, 0)                       \\\n-  _(cuMemCreate, 0)                      \\\n-  _(cuGetErrorString, 0)                 \\\n-  _(nvmlInit_v2, 1)                      \\\n-  _(nvmlDeviceGetHandleByPciBusId_v2, 1) \\\n-  _(nvmlDeviceGetComputeRunningProcesses, 1)\n-#define CREATE_MEMBER(name, n) decltype(&name) name##_;\n-  C10_FORALL_DRIVER_API(CREATE_MEMBER)\n",
            "whole_added": "+#define C10_LIBCUDA_DRIVER_API(_) \\\n+  _(cuMemAddressReserve)          \\\n+  _(cuMemRelease)                 \\\n+  _(cuMemMap)                     \\\n+  _(cuMemAddressFree)             \\\n+  _(cuMemSetAccess)               \\\n+  _(cuMemUnmap)                   \\\n+  _(cuMemCreate)                  \\\n+  _(cuGetErrorString)\n+#define C10_NVML_DRIVER_API(_)        \\\n+  _(nvmlInit_v2)                      \\\n+  _(nvmlDeviceGetHandleByPciBusId_v2) \\\n+  _(nvmlDeviceGetComputeRunningProcesses)\n+#define CREATE_MEMBER(name) decltype(&name) name##_;\n+  C10_LIBCUDA_DRIVER_API(CREATE_MEMBER)\n+  C10_NVML_DRIVER_API(CREATE_MEMBER)\n+  static void* get_nvml_handle();\n",
            "whole_hunk": "@@ -18,31 +18,31 @@\n     }                                                                      \\\n   } while (0)\n \n-#define C10_FORALL_DRIVER_LIBRARIES(_) \\\n-  _(\"libcuda.so\", 0)                   \\\n-  _(\"libnvidia-ml.so.1\", 1)\n+#define C10_LIBCUDA_DRIVER_API(_) \\\n+  _(cuMemAddressReserve)          \\\n+  _(cuMemRelease)                 \\\n+  _(cuMemMap)                     \\\n+  _(cuMemAddressFree)             \\\n+  _(cuMemSetAccess)               \\\n+  _(cuMemUnmap)                   \\\n+  _(cuMemCreate)                  \\\n+  _(cuGetErrorString)\n \n-#define C10_FORALL_DRIVER_API(_)         \\\n-  _(cuMemAddressReserve, 0)              \\\n-  _(cuMemRelease, 0)                     \\\n-  _(cuMemMap, 0)                         \\\n-  _(cuMemAddressFree, 0)                 \\\n-  _(cuMemSetAccess, 0)                   \\\n-  _(cuMemUnmap, 0)                       \\\n-  _(cuMemCreate, 0)                      \\\n-  _(cuGetErrorString, 0)                 \\\n-  _(nvmlInit_v2, 1)                      \\\n-  _(nvmlDeviceGetHandleByPciBusId_v2, 1) \\\n-  _(nvmlDeviceGetComputeRunningProcesses, 1)\n+#define C10_NVML_DRIVER_API(_)        \\\n+  _(nvmlInit_v2)                      \\\n+  _(nvmlDeviceGetHandleByPciBusId_v2) \\\n+  _(nvmlDeviceGetComputeRunningProcesses)\n \n namespace c10 {\n namespace cuda {\n \n struct DriverAPI {\n-#define CREATE_MEMBER(name, n) decltype(&name) name##_;\n-  C10_FORALL_DRIVER_API(CREATE_MEMBER)\n+#define CREATE_MEMBER(name) decltype(&name) name##_;\n+  C10_LIBCUDA_DRIVER_API(CREATE_MEMBER)\n+  C10_NVML_DRIVER_API(CREATE_MEMBER)\n #undef CREATE_MEMBER\n   static DriverAPI* get();\n+  static void* get_nvml_handle();\n };\n \n } // namespace cuda"
        }
    ]
},
{
    "Id": 478,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/bb89a9e48c869b3056a42d026f963c2ce4fb41a4",
    "date": "2023-10-14T00:10:01+00:00",
    "message": "Skipped CUDA Flags if C++ Extension Name includes \"arch\" Substring (#111211)\n\nThe CUDA architecture flags from TORCH_CUDA_ARCH_LIST will be skipped if the TORCH_EXTENSION_NAME includes the substring \"arch\". A C++ Extension should be allowed to have any name. I just manually skip the TORCH_EXTENSION_NAME flag when checking if one of the flags is \"arch\". There is probably a better fix, but I'll leave this to experts.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111211\nApproved by: https://github.com/ezyang",
    "label": "NO",
    "changes": [
        {
            "name": "cpp_extension.py",
            "path": "torch/utils/cpp_extension.py",
            "patches": [
                {
                    "old_start": 1928,
                    "old_length": 6,
                    "new_start": 1928,
                    "new_length": 8,
                    "hunk": "@@ -1928,6 +1928,8 @@ def _get_cuda_arch_flags(cflags: Optional[List[str]] = None) -> List[str]:\n     # (from `extra_compile_args`)\n     if cflags is not None:\n         for flag in cflags:\n+            if 'TORCH_EXTENSION_NAME' in flag:\n+                continue\n             if 'arch' in flag:\n                 return []\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "+            if 'TORCH_EXTENSION_NAME' in flag:\n+                continue\n",
            "whole_hunk": "@@ -1928,6 +1928,8 @@ def _get_cuda_arch_flags(cflags: Optional[List[str]] = None) -> List[str]:\n     # (from `extra_compile_args`)\n     if cflags is not None:\n         for flag in cflags:\n+            if 'TORCH_EXTENSION_NAME' in flag:\n+                continue\n             if 'arch' in flag:\n                 return []\n "
        }
    ]
},
{
    "Id": 238,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/eb1d6ed9f9d9731401b04382f526a64e6d27b6e6",
    "date": "2024-03-20T09:22:51+00:00",
    "message": "[Inductor] fix addmm fusion check (#121953)\n\nFixes #121253.\n\nTo avoid functional issue, disable pattern match for `addmm` when `beta!=1 or 0` or `alpha!=1`, as either `mkl_linear` or `mkldnn_linear` doesn't accept `beta` or `alpha` as parameters.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/121953\nApproved by: https://github.com/jgong5, https://github.com/leslie-fang-intel, https://github.com/jansel",
    "label": "YES",
    "changes": [
        {
            "name": "test_mkldnn_pattern_matcher.py",
            "path": "test/inductor/test_mkldnn_pattern_matcher.py",
            "patches": [
                {
                    "old_start": 2131,
                    "old_length": 6,
                    "new_start": 2131,
                    "new_length": 43,
                    "hunk": "@@ -2131,6 +2131,43 @@ class TestPatternMatcher(TestPatternMatcherBase):\n             om(*example_inputs)\n             om(*example_inputs)\n \n+    def test_reproduce_121253_issue(self):\n+        class Mod(torch.nn.Module):\n+            def __init__(self, weight, bias, beta, alpha):\n+                super().__init__()\n+                self.weight = weight\n+                self.bias = bias\n+                self.beta = beta\n+                self.alpha = alpha\n+\n+            def forward(self, x):\n+                return torch.addmm(\n+                    self.bias, x, self.weight, beta=self.beta, alpha=self.alpha\n+                )\n+\n+        dtypes = [torch.float32]\n+        if torch.ops.mkldnn._is_mkldnn_bf16_supported():\n+            dtypes.append(torch.bfloat16)\n+        for dtype in dtypes:\n+            linear_op = (\n+                \"mkl._mkl_linear\"\n+                if dtype == torch.float32\n+                else \"mkldnn._linear_pointwise\"\n+            )\n+            for beta, alpha in zip([1.0, 0.1, 0.0], [1.0, 0.1, 1.0]):\n+                weight = torch.randn(64, 64, dtype=dtype)\n+                bias = torch.randn(64, dtype=dtype)\n+                mod = Mod(weight, bias, beta, alpha).to(dtype).eval()\n+                with torch.no_grad():\n+                    x = torch.randn(1, 64, dtype=dtype)\n+                    include_ops = []\n+                    exclude_ops = []\n+                    if (beta != 1.0 and beta != 0.0) or alpha != 1.0:\n+                        exclude_ops = [linear_op]\n+                    else:\n+                        include_ops = [linear_op]\n+                    self._test_code_common(mod, (x,), include_ops, exclude_ops)\n+\n     @skipIfNoDynamoSupport\n     @skipIfRocm\n     def test_woq_int8(self):\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    def test_reproduce_121253_issue(self):\n+        class Mod(torch.nn.Module):\n+            def __init__(self, weight, bias, beta, alpha):\n+                super().__init__()\n+                self.weight = weight\n+                self.bias = bias\n+                self.beta = beta\n+                self.alpha = alpha\n+\n+            def forward(self, x):\n+                return torch.addmm(\n+                    self.bias, x, self.weight, beta=self.beta, alpha=self.alpha\n+                )\n+\n+        dtypes = [torch.float32]\n+        if torch.ops.mkldnn._is_mkldnn_bf16_supported():\n+            dtypes.append(torch.bfloat16)\n+        for dtype in dtypes:\n+            linear_op = (\n+                \"mkl._mkl_linear\"\n+                if dtype == torch.float32\n+                else \"mkldnn._linear_pointwise\"\n+            )\n+            for beta, alpha in zip([1.0, 0.1, 0.0], [1.0, 0.1, 1.0]):\n+                weight = torch.randn(64, 64, dtype=dtype)\n+                bias = torch.randn(64, dtype=dtype)\n+                mod = Mod(weight, bias, beta, alpha).to(dtype).eval()\n+                with torch.no_grad():\n+                    x = torch.randn(1, 64, dtype=dtype)\n+                    include_ops = []\n+                    exclude_ops = []\n+                    if (beta != 1.0 and beta != 0.0) or alpha != 1.0:\n+                        exclude_ops = [linear_op]\n+                    else:\n+                        include_ops = [linear_op]\n+                    self._test_code_common(mod, (x,), include_ops, exclude_ops)\n+\n",
            "whole_hunk": "@@ -2131,6 +2131,43 @@ class TestPatternMatcher(TestPatternMatcherBase):\n             om(*example_inputs)\n             om(*example_inputs)\n \n+    def test_reproduce_121253_issue(self):\n+        class Mod(torch.nn.Module):\n+            def __init__(self, weight, bias, beta, alpha):\n+                super().__init__()\n+                self.weight = weight\n+                self.bias = bias\n+                self.beta = beta\n+                self.alpha = alpha\n+\n+            def forward(self, x):\n+                return torch.addmm(\n+                    self.bias, x, self.weight, beta=self.beta, alpha=self.alpha\n+                )\n+\n+        dtypes = [torch.float32]\n+        if torch.ops.mkldnn._is_mkldnn_bf16_supported():\n+            dtypes.append(torch.bfloat16)\n+        for dtype in dtypes:\n+            linear_op = (\n+                \"mkl._mkl_linear\"\n+                if dtype == torch.float32\n+                else \"mkldnn._linear_pointwise\"\n+            )\n+            for beta, alpha in zip([1.0, 0.1, 0.0], [1.0, 0.1, 1.0]):\n+                weight = torch.randn(64, 64, dtype=dtype)\n+                bias = torch.randn(64, dtype=dtype)\n+                mod = Mod(weight, bias, beta, alpha).to(dtype).eval()\n+                with torch.no_grad():\n+                    x = torch.randn(1, 64, dtype=dtype)\n+                    include_ops = []\n+                    exclude_ops = []\n+                    if (beta != 1.0 and beta != 0.0) or alpha != 1.0:\n+                        exclude_ops = [linear_op]\n+                    else:\n+                        include_ops = [linear_op]\n+                    self._test_code_common(mod, (x,), include_ops, exclude_ops)\n+\n     @skipIfNoDynamoSupport\n     @skipIfRocm\n     def test_woq_int8(self):\n"
        },
        {
            "name": "mkldnn_fusion.py",
            "path": "torch/_inductor/fx_passes/mkldnn_fusion.py",
            "patches": [
                {
                    "old_start": 911,
                    "old_length": 6,
                    "new_start": 911,
                    "new_length": 12,
                    "hunk": "@@ -911,6 +911,12 @@ if torch._C._has_mkldnn:\n         Check if the node is supported for MKLDNN linear.\n         \"\"\"\n         linear_node = match.output_node()\n+        # mkldnn linear only supports beta=1or0 and alpha=1\n+        if linear_node.target == aten.addmm.default:\n+            alpha = linear_node.kwargs.get(\"alpha\", 1.0)\n+            beta = linear_node.kwargs.get(\"beta\", 1.0)\n+            if (beta != 0.0 and beta != 1.0) or alpha != 1.0:\n+                return False\n         # weight_idx is 1 for aten.mm and is 2 for aten.addmm\n         weight_idx = 2 if linear_node.target == aten.addmm.default else 1\n         if linear_node.args[weight_idx].op != \"get_attr\":\n"
                },
                {
                    "old_start": 1093,
                    "old_length": 7,
                    "new_start": 1099,
                    "new_length": 14,
                    "hunk": "@@ -1093,7 +1099,14 @@ if torch._C._has_mkldnn:\n                 graph.erase_node(lstm_node)\n \n         @register_freezing_graph_pattern(\n-            CallFunction(aten.addmm.default, Arg(), Arg(), Arg()),\n+            CallFunction(\n+                aten.addmm.default,\n+                Arg(),\n+                Arg(),\n+                Arg(),\n+                beta=KeywordArg(\"beta\"),\n+                alpha=KeywordArg(\"alpha\"),\n+            ),\n             extra_check=_is_packable_linear,\n         )\n         @register_freezing_graph_pattern(\n"
                },
                {
                    "old_start": 1104,
                    "old_length": 7,
                    "new_start": 1117,
                    "new_length": 15,
                    "hunk": "@@ -1104,7 +1117,15 @@ if torch._C._has_mkldnn:\n             graph = match.graph\n             linear_node = match.output_node()\n             input = args[0] if linear_node.target == aten.mm.default else args[1]\n-            bias = None if linear_node.target == aten.mm.default else args[0]\n+            bias = (\n+                None\n+                if linear_node.target == aten.mm.default\n+                or (\n+                    linear_node.target == aten.addmm.default\n+                    and linear_node.kwargs.get(\"beta\", 1.0) == 0.0\n+                )\n+                else args[0]\n+            )\n             weight = args[1] if linear_node.target == aten.mm.default else args[2]\n             with graph.inserting_before(linear_node):\n                 transpose_weight_node = graph.create_node("
                }
            ],
            "whole_deleted": "-            CallFunction(aten.addmm.default, Arg(), Arg(), Arg()),\n-            bias = None if linear_node.target == aten.mm.default else args[0]\n",
            "whole_added": "+        # mkldnn linear only supports beta=1or0 and alpha=1\n+        if linear_node.target == aten.addmm.default:\n+            alpha = linear_node.kwargs.get(\"alpha\", 1.0)\n+            beta = linear_node.kwargs.get(\"beta\", 1.0)\n+            if (beta != 0.0 and beta != 1.0) or alpha != 1.0:\n+                return False\n+            CallFunction(\n+                aten.addmm.default,\n+                Arg(),\n+                Arg(),\n+                Arg(),\n+                beta=KeywordArg(\"beta\"),\n+                alpha=KeywordArg(\"alpha\"),\n+            ),\n+            bias = (\n+                None\n+                if linear_node.target == aten.mm.default\n+                or (\n+                    linear_node.target == aten.addmm.default\n+                    and linear_node.kwargs.get(\"beta\", 1.0) == 0.0\n+                )\n+                else args[0]\n+            )\n",
            "whole_hunk": "@@ -911,6 +911,12 @@ if torch._C._has_mkldnn:\n         Check if the node is supported for MKLDNN linear.\n         \"\"\"\n         linear_node = match.output_node()\n+        # mkldnn linear only supports beta=1or0 and alpha=1\n+        if linear_node.target == aten.addmm.default:\n+            alpha = linear_node.kwargs.get(\"alpha\", 1.0)\n+            beta = linear_node.kwargs.get(\"beta\", 1.0)\n+            if (beta != 0.0 and beta != 1.0) or alpha != 1.0:\n+                return False\n         # weight_idx is 1 for aten.mm and is 2 for aten.addmm\n         weight_idx = 2 if linear_node.target == aten.addmm.default else 1\n         if linear_node.args[weight_idx].op != \"get_attr\":\n@@ -1093,7 +1099,14 @@ if torch._C._has_mkldnn:\n                 graph.erase_node(lstm_node)\n \n         @register_freezing_graph_pattern(\n-            CallFunction(aten.addmm.default, Arg(), Arg(), Arg()),\n+            CallFunction(\n+                aten.addmm.default,\n+                Arg(),\n+                Arg(),\n+                Arg(),\n+                beta=KeywordArg(\"beta\"),\n+                alpha=KeywordArg(\"alpha\"),\n+            ),\n             extra_check=_is_packable_linear,\n         )\n         @register_freezing_graph_pattern(\n@@ -1104,7 +1117,15 @@ if torch._C._has_mkldnn:\n             graph = match.graph\n             linear_node = match.output_node()\n             input = args[0] if linear_node.target == aten.mm.default else args[1]\n-            bias = None if linear_node.target == aten.mm.default else args[0]\n+            bias = (\n+                None\n+                if linear_node.target == aten.mm.default\n+                or (\n+                    linear_node.target == aten.addmm.default\n+                    and linear_node.kwargs.get(\"beta\", 1.0) == 0.0\n+                )\n+                else args[0]\n+            )\n             weight = args[1] if linear_node.target == aten.mm.default else args[2]\n             with graph.inserting_before(linear_node):\n                 transpose_weight_node = graph.create_node("
        }
    ]
},
{
    "Id": 30,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/fe3e6878c4bb2a6001045c179fd7fa9838242558",
    "date": "2024-07-10T16:44:27+00:00",
    "message": "[cond] inlining into one of the branches when pred is a python constant (#128709)\n\nWhen the input predicate is a python constant, we specialize into one of the branches and warn users that torch.cond is not preserving the dynamism. The previous behavior is that we baked in True/False in the cond operator. This can be confusing. In this PR, we change it to be specializing into one of the branches when the inputs are constants.\n\nWe additionally change the naming of cond operator to default one without overriding its name. This allows better testing on de-serialized graph.\n\nTest Plan:\nThe predicate in some existing tests is the result of a shape comparison. When no dynamic shape is involved, the predicate is a python bool. To fix them, we either change the predicate to be some data-dependent tensor or change the test to check cond is specialized as one of the branches,\n\nDifferential Revision: [D59589709](https://our.internmc.facebook.com/intern/diff/D59589709)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128709\nApproved by: https://github.com/zou3519",
    "label": "YES",
    "changes": [
        {
            "name": "test_export.py",
            "path": "test/dynamo/test_export.py",
            "patches": [
                {
                    "old_start": 1912,
                    "old_length": 13,
                    "new_start": 1912,
                    "new_length": 10,
                    "hunk": "@@ -1912,13 +1912,10 @@ def forward(self, l_x_):\n             ):\n                 # True branch and false branch return tensors of different shape\n                 torch._dynamo.export(mod)(torch.randn(3, 2))\n-            with self.assertRaisesRegex(\n-                torch._dynamo.exc.UncapturedHigherOrderOpError,\n-                \"Cond doesn't work unless it is captured completely with torch.compile\",\n-            ):\n-                # True branch and false branch return tensors of different shape\n-                test_x = torch.randn(3, 2)\n-                mod(test_x)\n+\n+            # We specialize into one of the branches since predicate is a python boolean.\n+            test_x = torch.randn(3, 2)\n+            mod(test_x)\n \n     def test_export_with_map_cond(self):\n         from functorch.experimental.control_flow import cond, map\n"
                }
            ],
            "whole_deleted": "-            with self.assertRaisesRegex(\n-                torch._dynamo.exc.UncapturedHigherOrderOpError,\n-                \"Cond doesn't work unless it is captured completely with torch.compile\",\n-            ):\n-                # True branch and false branch return tensors of different shape\n-                test_x = torch.randn(3, 2)\n-                mod(test_x)\n",
            "whole_added": "+\n+            # We specialize into one of the branches since predicate is a python boolean.\n+            test_x = torch.randn(3, 2)\n+            mod(test_x)\n",
            "whole_hunk": "@@ -1912,13 +1912,10 @@ def forward(self, l_x_):\n             ):\n                 # True branch and false branch return tensors of different shape\n                 torch._dynamo.export(mod)(torch.randn(3, 2))\n-            with self.assertRaisesRegex(\n-                torch._dynamo.exc.UncapturedHigherOrderOpError,\n-                \"Cond doesn't work unless it is captured completely with torch.compile\",\n-            ):\n-                # True branch and false branch return tensors of different shape\n-                test_x = torch.randn(3, 2)\n-                mod(test_x)\n+\n+            # We specialize into one of the branches since predicate is a python boolean.\n+            test_x = torch.randn(3, 2)\n+            mod(test_x)\n \n     def test_export_with_map_cond(self):\n         from functorch.experimental.control_flow import cond, map\n"
        },
        {
            "name": "test_higher_order_ops.py",
            "path": "test/dynamo/test_higher_order_ops.py",
            "patches": [
                {
                    "old_start": 1408,
                    "old_length": 7,
                    "new_start": 1408,
                    "new_length": 7,
                    "hunk": "@@ -1408,7 +1408,7 @@ def forward(self, child, const_unused):\n                 def false_fn(x):\n                     return (x - 1).sum()\n \n-                return control_flow.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n+                return control_flow.cond(x.sum() > 4, true_fn, false_fn, [x])\n \n         mod_for_compile = torch.compile(Foo(), backend=cnt, dynamic=True)\n         mod_for_eager = Foo()\n"
                },
                {
                    "old_start": 6147,
                    "old_length": 12,
                    "new_start": 6147,
                    "new_length": 16,
                    "hunk": "@@ -6147,12 +6147,16 @@ class ActivationCheckpointingTests(torch._dynamo.test_case.TestCase):\n             return cond_op(pred=pred, true_fn=true_fn, false_fn=false_fn, operands=[x])\n \n         cnt = CompileCounter()\n-        opt_test = torch.compile(test, backend=cnt)\n+        opt_test = torch.compile(test, backend=cnt, fullgraph=True)\n         inp = torch.ones(3, 3)\n-        self.assertTrue(torch.allclose(test(True, inp), opt_test(True, inp)))\n+        true_pred = torch.Tensor([True])\n+        false_pred = torch.Tensor([False])\n+        self.assertTrue(torch.allclose(test(true_pred, inp), opt_test(true_pred, inp)))\n+        self.assertEqual(cnt.frame_count, 1)\n+        self.assertTrue(\n+            torch.allclose(test(false_pred, inp), opt_test(false_pred, inp))\n+        )\n         self.assertEqual(cnt.frame_count, 1)\n-        self.assertTrue(torch.allclose(test(False, inp), opt_test(False, inp)))\n-        self.assertEqual(cnt.frame_count, 2)\n \n     def test_cond_with_invalid_kwargs(self):\n         from torch._higher_order_ops.cond import cond_op\n"
                }
            ],
            "whole_deleted": "-                return control_flow.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n-        opt_test = torch.compile(test, backend=cnt)\n-        self.assertTrue(torch.allclose(test(True, inp), opt_test(True, inp)))\n-        self.assertTrue(torch.allclose(test(False, inp), opt_test(False, inp)))\n-        self.assertEqual(cnt.frame_count, 2)\n",
            "whole_added": "+                return control_flow.cond(x.sum() > 4, true_fn, false_fn, [x])\n+        opt_test = torch.compile(test, backend=cnt, fullgraph=True)\n+        true_pred = torch.Tensor([True])\n+        false_pred = torch.Tensor([False])\n+        self.assertTrue(torch.allclose(test(true_pred, inp), opt_test(true_pred, inp)))\n+        self.assertEqual(cnt.frame_count, 1)\n+        self.assertTrue(\n+            torch.allclose(test(false_pred, inp), opt_test(false_pred, inp))\n+        )\n",
            "whole_hunk": "@@ -1408,7 +1408,7 @@ def forward(self, child, const_unused):\n                 def false_fn(x):\n                     return (x - 1).sum()\n \n-                return control_flow.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n+                return control_flow.cond(x.sum() > 4, true_fn, false_fn, [x])\n \n         mod_for_compile = torch.compile(Foo(), backend=cnt, dynamic=True)\n         mod_for_eager = Foo()\n@@ -6147,12 +6147,16 @@ class ActivationCheckpointingTests(torch._dynamo.test_case.TestCase):\n             return cond_op(pred=pred, true_fn=true_fn, false_fn=false_fn, operands=[x])\n \n         cnt = CompileCounter()\n-        opt_test = torch.compile(test, backend=cnt)\n+        opt_test = torch.compile(test, backend=cnt, fullgraph=True)\n         inp = torch.ones(3, 3)\n-        self.assertTrue(torch.allclose(test(True, inp), opt_test(True, inp)))\n+        true_pred = torch.Tensor([True])\n+        false_pred = torch.Tensor([False])\n+        self.assertTrue(torch.allclose(test(true_pred, inp), opt_test(true_pred, inp)))\n+        self.assertEqual(cnt.frame_count, 1)\n+        self.assertTrue(\n+            torch.allclose(test(false_pred, inp), opt_test(false_pred, inp))\n+        )\n         self.assertEqual(cnt.frame_count, 1)\n-        self.assertTrue(torch.allclose(test(False, inp), opt_test(False, inp)))\n-        self.assertEqual(cnt.frame_count, 2)\n \n     def test_cond_with_invalid_kwargs(self):\n         from torch._higher_order_ops.cond import cond_op\n"
        },
        {
            "name": "test_export.py",
            "path": "test/export/test_export.py",
            "patches": [
                {
                    "old_start": 801,
                    "old_length": 7,
                    "new_start": 801,
                    "new_length": 7,
                    "hunk": "@@ -801,7 +801,7 @@ def forward(self, p_linear_weight, p_linear_bias, x):\n                 return x.sin()\n \n             def forward(self, x):\n-                return cond(x.shape[0] <= 2, self.subm.forward, self.bar, [x])\n+                return cond(x.sum() <= 2, self.subm.forward, self.bar, [x])\n \n         example_inputs = (torch.randn(1, 3, 3, 3),)\n         m = CondBranchClassMethod()\n"
                },
                {
                    "old_start": 3603,
                    "old_length": 7,
                    "new_start": 3603,
                    "new_length": 7,
                    "hunk": "@@ -3603,7 +3603,7 @@ def forward(self, x):\n \n     # https://github.com/pytorch/pytorch/issues/129939\n     @testing.expectedFailureNonStrict\n-    def test_export_cond(self):\n+    def test_export_cond_symbool_pred(self):\n         class A(torch.nn.Module):\n             def __init__(self):\n                 super().__init__()\n"
                },
                {
                    "old_start": 3626,
                    "old_length": 10,
                    "new_start": 3626,
                    "new_length": 20,
                    "hunk": "@@ -3626,10 +3626,20 @@ def forward(self, x):\n \n                 return cond(x.shape[0] > 4, true_fn, false_fn, [x])\n \n+        dim0 = torch.export.Dim(\"dim0\", min=3)\n         inp = torch.ones(6, 4)\n-        ep = export(\n-            Foo(),\n-            (inp,),\n+        ep = export(Foo(), (inp,), dynamic_shapes={\"x\": {0: dim0}})\n+        self.assertExpectedInline(\n+            ep.graph_module.code.strip(),\n+            \"\"\"\\\n+def forward(self, b_a_buffer, x):\n+    sym_size_int_1 = torch.ops.aten.sym_size.int(x, 0)\n+    gt = sym_size_int_1 > 4;  sym_size_int_1 = None\n+    true_graph_0 = self.true_graph_0\n+    false_graph_0 = self.false_graph_0\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [x, b_a_buffer]);  gt = true_graph_0 = false_graph_0 = x = b_a_buffer = None\n+    getitem = cond[0];  cond = None\n+    return (getitem,)\"\"\",\n         )\n         self.assertTrue(\n             torch.allclose(ep.module()(torch.ones(6, 4)), Foo()(torch.ones(6, 4)))\n"
                },
                {
                    "old_start": 5007,
                    "old_length": 7,
                    "new_start": 5017,
                    "new_length": 7,
                    "hunk": "@@ -5007,7 +5017,7 @@ graph():\n                 def false_fn(x):\n                     return self.linear(x).sin()\n \n-                return torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n+                return torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n \n         class CondExport(torch.nn.Module):\n             def __init__(self):\n"
                },
                {
                    "old_start": 5024,
                    "old_length": 10,
                    "new_start": 5034,
                    "new_length": 12,
                    "hunk": "@@ -5024,10 +5034,12 @@ graph():\n             \"\"\"\\\n def forward(self, p_bar_linear_weight, p_bar_linear_bias, x):\n     cos = torch.ops.aten.cos.default(x)\n+    sum_1 = torch.ops.aten.sum.default(x)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [p_bar_linear_bias, p_bar_linear_weight, x]);  true_graph_0 = false_graph_0 = p_bar_linear_bias = p_bar_linear_weight = x = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [p_bar_linear_bias, p_bar_linear_weight, x]);  gt = true_graph_0 = false_graph_0 = p_bar_linear_bias = p_bar_linear_weight = x = None\n+    getitem = cond[0];  cond = None\n     add = torch.ops.aten.add.Tensor(cos, getitem);  cos = getitem = None\n     return (add,)\"\"\",\n         )\n"
                },
                {
                    "old_start": 5122,
                    "old_length": 8,
                    "new_start": 5134,
                    "new_length": 8,
                    "hunk": "@@ -5122,8 +5134,8 @@ def forward(self, p_bar_linear_weight, p_bar_linear_bias, x):\n def forward(self, b_pred, b_t, x, y):\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(b_pred, true_graph_0, false_graph_0, [b_t, x, y]);  b_pred = true_graph_0 = false_graph_0 = b_t = x = y = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(b_pred, true_graph_0, false_graph_0, [b_t, x, y]);  b_pred = true_graph_0 = false_graph_0 = b_t = x = y = None\n+    getitem = cond[0];  cond = None\n     return (getitem,)\"\"\",\n         )  # noqa: B950\n \n"
                }
            ],
            "whole_deleted": "-                return cond(x.shape[0] <= 2, self.subm.forward, self.bar, [x])\n-    def test_export_cond(self):\n-        ep = export(\n-            Foo(),\n-            (inp,),\n-                return torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [p_bar_linear_bias, p_bar_linear_weight, x]);  true_graph_0 = false_graph_0 = p_bar_linear_bias = p_bar_linear_weight = x = None\n-    getitem = conditional[0];  conditional = None\n-    conditional = torch.ops.higher_order.cond(b_pred, true_graph_0, false_graph_0, [b_t, x, y]);  b_pred = true_graph_0 = false_graph_0 = b_t = x = y = None\n-    getitem = conditional[0];  conditional = None\n",
            "whole_added": "+                return cond(x.sum() <= 2, self.subm.forward, self.bar, [x])\n+    def test_export_cond_symbool_pred(self):\n+        dim0 = torch.export.Dim(\"dim0\", min=3)\n+        ep = export(Foo(), (inp,), dynamic_shapes={\"x\": {0: dim0}})\n+        self.assertExpectedInline(\n+            ep.graph_module.code.strip(),\n+            \"\"\"\\\n+def forward(self, b_a_buffer, x):\n+    sym_size_int_1 = torch.ops.aten.sym_size.int(x, 0)\n+    gt = sym_size_int_1 > 4;  sym_size_int_1 = None\n+    true_graph_0 = self.true_graph_0\n+    false_graph_0 = self.false_graph_0\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [x, b_a_buffer]);  gt = true_graph_0 = false_graph_0 = x = b_a_buffer = None\n+    getitem = cond[0];  cond = None\n+    return (getitem,)\"\"\",\n+                return torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n+    sum_1 = torch.ops.aten.sum.default(x)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [p_bar_linear_bias, p_bar_linear_weight, x]);  gt = true_graph_0 = false_graph_0 = p_bar_linear_bias = p_bar_linear_weight = x = None\n+    getitem = cond[0];  cond = None\n+    cond = torch.ops.higher_order.cond(b_pred, true_graph_0, false_graph_0, [b_t, x, y]);  b_pred = true_graph_0 = false_graph_0 = b_t = x = y = None\n+    getitem = cond[0];  cond = None\n",
            "whole_hunk": "@@ -801,7 +801,7 @@ def forward(self, p_linear_weight, p_linear_bias, x):\n                 return x.sin()\n \n             def forward(self, x):\n-                return cond(x.shape[0] <= 2, self.subm.forward, self.bar, [x])\n+                return cond(x.sum() <= 2, self.subm.forward, self.bar, [x])\n \n         example_inputs = (torch.randn(1, 3, 3, 3),)\n         m = CondBranchClassMethod()\n@@ -3603,7 +3603,7 @@ def forward(self, x):\n \n     # https://github.com/pytorch/pytorch/issues/129939\n     @testing.expectedFailureNonStrict\n-    def test_export_cond(self):\n+    def test_export_cond_symbool_pred(self):\n         class A(torch.nn.Module):\n             def __init__(self):\n                 super().__init__()\n@@ -3626,10 +3626,20 @@ def forward(self, x):\n \n                 return cond(x.shape[0] > 4, true_fn, false_fn, [x])\n \n+        dim0 = torch.export.Dim(\"dim0\", min=3)\n         inp = torch.ones(6, 4)\n-        ep = export(\n-            Foo(),\n-            (inp,),\n+        ep = export(Foo(), (inp,), dynamic_shapes={\"x\": {0: dim0}})\n+        self.assertExpectedInline(\n+            ep.graph_module.code.strip(),\n+            \"\"\"\\\n+def forward(self, b_a_buffer, x):\n+    sym_size_int_1 = torch.ops.aten.sym_size.int(x, 0)\n+    gt = sym_size_int_1 > 4;  sym_size_int_1 = None\n+    true_graph_0 = self.true_graph_0\n+    false_graph_0 = self.false_graph_0\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [x, b_a_buffer]);  gt = true_graph_0 = false_graph_0 = x = b_a_buffer = None\n+    getitem = cond[0];  cond = None\n+    return (getitem,)\"\"\",\n         )\n         self.assertTrue(\n             torch.allclose(ep.module()(torch.ones(6, 4)), Foo()(torch.ones(6, 4)))\n@@ -5007,7 +5017,7 @@ graph():\n                 def false_fn(x):\n                     return self.linear(x).sin()\n \n-                return torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n+                return torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n \n         class CondExport(torch.nn.Module):\n             def __init__(self):\n@@ -5024,10 +5034,12 @@ graph():\n             \"\"\"\\\n def forward(self, p_bar_linear_weight, p_bar_linear_bias, x):\n     cos = torch.ops.aten.cos.default(x)\n+    sum_1 = torch.ops.aten.sum.default(x)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [p_bar_linear_bias, p_bar_linear_weight, x]);  true_graph_0 = false_graph_0 = p_bar_linear_bias = p_bar_linear_weight = x = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [p_bar_linear_bias, p_bar_linear_weight, x]);  gt = true_graph_0 = false_graph_0 = p_bar_linear_bias = p_bar_linear_weight = x = None\n+    getitem = cond[0];  cond = None\n     add = torch.ops.aten.add.Tensor(cos, getitem);  cos = getitem = None\n     return (add,)\"\"\",\n         )\n@@ -5122,8 +5134,8 @@ def forward(self, p_bar_linear_weight, p_bar_linear_bias, x):\n def forward(self, b_pred, b_t, x, y):\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(b_pred, true_graph_0, false_graph_0, [b_t, x, y]);  b_pred = true_graph_0 = false_graph_0 = b_t = x = y = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(b_pred, true_graph_0, false_graph_0, [b_t, x, y]);  b_pred = true_graph_0 = false_graph_0 = b_t = x = y = None\n+    getitem = cond[0];  cond = None\n     return (getitem,)\"\"\",\n         )  # noqa: B950\n \n"
        },
        {
            "name": "test_verifier.py",
            "path": "test/export/test_verifier.py",
            "patches": [
                {
                    "old_start": 68,
                    "old_length": 7,
                    "new_start": 68,
                    "new_length": 7,
                    "hunk": "@@ -68,7 +68,7 @@ class TestVerifier(TestCase):\n                 def false_fn(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n                     return x - y\n \n-                return control_flow.cond(x.shape[0] > 2, true_fn, false_fn, [x, y])\n+                return control_flow.cond(x.sum() > 2, true_fn, false_fn, [x, y])\n \n         f = Foo()\n \n"
                },
                {
                    "old_start": 87,
                    "old_length": 7,
                    "new_start": 87,
                    "new_length": 7,
                    "hunk": "@@ -87,7 +87,7 @@ class TestVerifier(TestCase):\n                 def false_fn(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n                     return x - y\n \n-                return control_flow.cond(x.shape[0] > 2, true_fn, false_fn, [x, y])\n+                return control_flow.cond(x.sum() > 2, true_fn, false_fn, [x, y])\n \n         f = Foo()\n \n"
                }
            ],
            "whole_deleted": "-                return control_flow.cond(x.shape[0] > 2, true_fn, false_fn, [x, y])\n-                return control_flow.cond(x.shape[0] > 2, true_fn, false_fn, [x, y])\n",
            "whole_added": "+                return control_flow.cond(x.sum() > 2, true_fn, false_fn, [x, y])\n+                return control_flow.cond(x.sum() > 2, true_fn, false_fn, [x, y])\n",
            "whole_hunk": "@@ -68,7 +68,7 @@ class TestVerifier(TestCase):\n                 def false_fn(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n                     return x - y\n \n-                return control_flow.cond(x.shape[0] > 2, true_fn, false_fn, [x, y])\n+                return control_flow.cond(x.sum() > 2, true_fn, false_fn, [x, y])\n \n         f = Foo()\n \n@@ -87,7 +87,7 @@ class TestVerifier(TestCase):\n                 def false_fn(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n                     return x - y\n \n-                return control_flow.cond(x.shape[0] > 2, true_fn, false_fn, [x, y])\n+                return control_flow.cond(x.sum() > 2, true_fn, false_fn, [x, y])\n \n         f = Foo()\n \n"
        },
        {
            "name": "test_aotdispatch.py",
            "path": "test/functorch/test_aotdispatch.py",
            "patches": [
                {
                    "old_start": 4237,
                    "old_length": 7,
                    "new_start": 4237,
                    "new_length": 7,
                    "hunk": "@@ -4237,7 +4237,7 @@ def forward(self, arg0_1, arg1_1):\n                         return x.cos()\n \n                     return torch.cond(\n-                        y.cos().shape[0] > 5, true_true_fn, true_false_fn, [y.cos()]\n+                        y.cos().sum() > 5, true_true_fn, true_false_fn, [y.cos()]\n                     )\n \n                 def false_fn(x):\n"
                },
                {
                    "old_start": 4245,
                    "old_length": 7,
                    "new_start": 4245,
                    "new_length": 7,
                    "hunk": "@@ -4245,7 +4245,7 @@ def forward(self, arg0_1, arg1_1):\n                     z.add_(6)\n                     return z.sin()\n \n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n                 return (a + 3, a + 4)\n \n         inp = torch.randn(2, 2)\n"
                },
                {
                    "old_start": 4254,
                    "old_length": 10,
                    "new_start": 4254,
                    "new_length": 12,
                    "hunk": "@@ -4254,10 +4254,12 @@ def forward(self, arg0_1, arg1_1):\n             str(gm.code).strip(),\n             \"\"\"\\\n def forward(self, arg0_1):\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1]);  true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n     add = torch.ops.aten.add.Tensor(getitem, 3)\n     add_1 = torch.ops.aten.add.Tensor(getitem, 4);  getitem = None\n     return (add, add_1)\"\"\",  # noqa: B950\n"
                },
                {
                    "old_start": 4270,
                    "old_length": 11,
                    "new_start": 4272,
                    "new_length": 13,
                    "hunk": "@@ -4270,11 +4272,13 @@ def forward(self, arg0_1):\n     sin = torch.ops.aten.sin.default(arg0_1);  arg0_1 = None\n     add = torch.ops.aten.add.Tensor(sin, 5);  sin = None\n     cos = torch.ops.aten.cos.default(add)\n+    sum_1 = torch.ops.aten.sum.default(cos);  cos = None\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 5);  sum_1 = None\n     cos_1 = torch.ops.aten.cos.default(add);  add = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [cos_1]);  true_graph_0 = false_graph_0 = cos_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [cos_1]);  gt = true_graph_0 = false_graph_0 = cos_1 = None\n+    getitem = cond[0];  cond = None\n     return (getitem,)\"\"\",  # noqa: B950\n         )\n \n"
                },
                {
                    "old_start": 4317,
                    "old_length": 7,
                    "new_start": 4321,
                    "new_length": 7,
                    "hunk": "@@ -4317,7 +4321,7 @@ def forward(self, arg0_1):\n                         + control_flow.map(f, z, r).sum()\n                     )\n \n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x, y])\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x, y])\n                 return (a + 3, a + 4)\n \n         inps = [torch.randn(2, 2), torch.ones(2)]\n"
                },
                {
                    "old_start": 4326,
                    "old_length": 10,
                    "new_start": 4330,
                    "new_length": 12,
                    "hunk": "@@ -4326,10 +4330,12 @@ def forward(self, arg0_1):\n             str(gm.code).strip(),\n             \"\"\"\\\n def forward(self, arg0_1, arg1_1):\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1, arg1_1]);  true_graph_0 = false_graph_0 = arg0_1 = arg1_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1, arg1_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = arg1_1 = None\n+    getitem = cond[0];  cond = None\n     add = torch.ops.aten.add.Tensor(getitem, 3)\n     add_1 = torch.ops.aten.add.Tensor(getitem, 4);  getitem = None\n     return (add, add_1)\"\"\",  # noqa: B950\n"
                },
                {
                    "old_start": 4434,
                    "old_length": 7,
                    "new_start": 4440,
                    "new_length": 7,
                    "hunk": "@@ -4434,7 +4440,7 @@ def forward(self, arg0_1, arg1_1):\n                     z.add_(6)\n                     return z.sin()\n \n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n                 return (a + 3, a + 4)\n \n         inp = torch.randn(2, 2)\n"
                },
                {
                    "old_start": 4443,
                    "old_length": 10,
                    "new_start": 4449,
                    "new_length": 12,
                    "hunk": "@@ -4443,10 +4449,12 @@ def forward(self, arg0_1, arg1_1):\n             str(gm.code).strip(),\n             \"\"\"\\\n def forward(self, arg0_1):\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1]);  true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n     add = torch.ops.aten.add.Tensor(getitem, 3)\n     add_1 = torch.ops.aten.add.Tensor(getitem, 4);  getitem = None\n     return (add, add_1)\"\"\",  # noqa: B950\n"
                },
                {
                    "old_start": 4867,
                    "old_length": 7,
                    "new_start": 4875,
                    "new_length": 7,
                    "hunk": "@@ -4867,7 +4875,7 @@ def forward(self, arg0_1, arg1_1, arg2_1):\n                     y.add_(6)\n                     return x.sin()\n \n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n                 return (a + 3, a + 4)\n \n         inp = torch.randn(3, 4)\n"
                },
                {
                    "old_start": 4876,
                    "old_length": 10,
                    "new_start": 4884,
                    "new_length": 12,
                    "hunk": "@@ -4876,10 +4884,12 @@ def forward(self, arg0_1, arg1_1, arg2_1):\n             gm.code.strip(),\n             \"\"\"\\\n def forward(self, arg0_1):\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1]);  true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n     add = torch.ops.aten.add.Tensor(getitem, 3)\n     add_1 = torch.ops.aten.add.Tensor(getitem, 4);  getitem = None\n     return (add, add_1)\"\"\",  # noqa: B950\n"
                }
            ],
            "whole_deleted": "-                        y.cos().shape[0] > 5, true_true_fn, true_false_fn, [y.cos()]\n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1]);  true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [cos_1]);  true_graph_0 = false_graph_0 = cos_1 = None\n-    getitem = conditional[0];  conditional = None\n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x, y])\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1, arg1_1]);  true_graph_0 = false_graph_0 = arg0_1 = arg1_1 = None\n-    getitem = conditional[0];  conditional = None\n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1]);  true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1]);  true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n",
            "whole_added": "+                        y.cos().sum() > 5, true_true_fn, true_false_fn, [y.cos()]\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n+    sum_1 = torch.ops.aten.sum.default(cos);  cos = None\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 5);  sum_1 = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [cos_1]);  gt = true_graph_0 = false_graph_0 = cos_1 = None\n+    getitem = cond[0];  cond = None\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x, y])\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1, arg1_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = arg1_1 = None\n+    getitem = cond[0];  cond = None\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n",
            "whole_hunk": "@@ -4237,7 +4237,7 @@ def forward(self, arg0_1, arg1_1):\n                         return x.cos()\n \n                     return torch.cond(\n-                        y.cos().shape[0] > 5, true_true_fn, true_false_fn, [y.cos()]\n+                        y.cos().sum() > 5, true_true_fn, true_false_fn, [y.cos()]\n                     )\n \n                 def false_fn(x):\n@@ -4245,7 +4245,7 @@ def forward(self, arg0_1, arg1_1):\n                     z.add_(6)\n                     return z.sin()\n \n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n                 return (a + 3, a + 4)\n \n         inp = torch.randn(2, 2)\n@@ -4254,10 +4254,12 @@ def forward(self, arg0_1, arg1_1):\n             str(gm.code).strip(),\n             \"\"\"\\\n def forward(self, arg0_1):\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1]);  true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n     add = torch.ops.aten.add.Tensor(getitem, 3)\n     add_1 = torch.ops.aten.add.Tensor(getitem, 4);  getitem = None\n     return (add, add_1)\"\"\",  # noqa: B950\n@@ -4270,11 +4272,13 @@ def forward(self, arg0_1):\n     sin = torch.ops.aten.sin.default(arg0_1);  arg0_1 = None\n     add = torch.ops.aten.add.Tensor(sin, 5);  sin = None\n     cos = torch.ops.aten.cos.default(add)\n+    sum_1 = torch.ops.aten.sum.default(cos);  cos = None\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 5);  sum_1 = None\n     cos_1 = torch.ops.aten.cos.default(add);  add = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [cos_1]);  true_graph_0 = false_graph_0 = cos_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [cos_1]);  gt = true_graph_0 = false_graph_0 = cos_1 = None\n+    getitem = cond[0];  cond = None\n     return (getitem,)\"\"\",  # noqa: B950\n         )\n \n@@ -4317,7 +4321,7 @@ def forward(self, arg0_1):\n                         + control_flow.map(f, z, r).sum()\n                     )\n \n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x, y])\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x, y])\n                 return (a + 3, a + 4)\n \n         inps = [torch.randn(2, 2), torch.ones(2)]\n@@ -4326,10 +4330,12 @@ def forward(self, arg0_1):\n             str(gm.code).strip(),\n             \"\"\"\\\n def forward(self, arg0_1, arg1_1):\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1, arg1_1]);  true_graph_0 = false_graph_0 = arg0_1 = arg1_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1, arg1_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = arg1_1 = None\n+    getitem = cond[0];  cond = None\n     add = torch.ops.aten.add.Tensor(getitem, 3)\n     add_1 = torch.ops.aten.add.Tensor(getitem, 4);  getitem = None\n     return (add, add_1)\"\"\",  # noqa: B950\n@@ -4434,7 +4440,7 @@ def forward(self, arg0_1, arg1_1):\n                     z.add_(6)\n                     return z.sin()\n \n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n                 return (a + 3, a + 4)\n \n         inp = torch.randn(2, 2)\n@@ -4443,10 +4449,12 @@ def forward(self, arg0_1, arg1_1):\n             str(gm.code).strip(),\n             \"\"\"\\\n def forward(self, arg0_1):\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1]);  true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n     add = torch.ops.aten.add.Tensor(getitem, 3)\n     add_1 = torch.ops.aten.add.Tensor(getitem, 4);  getitem = None\n     return (add, add_1)\"\"\",  # noqa: B950\n@@ -4867,7 +4875,7 @@ def forward(self, arg0_1, arg1_1, arg2_1):\n                     y.add_(6)\n                     return x.sin()\n \n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n                 return (a + 3, a + 4)\n \n         inp = torch.randn(3, 4)\n@@ -4876,10 +4884,12 @@ def forward(self, arg0_1, arg1_1, arg2_1):\n             gm.code.strip(),\n             \"\"\"\\\n def forward(self, arg0_1):\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1]);  true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n     add = torch.ops.aten.add.Tensor(getitem, 3)\n     add_1 = torch.ops.aten.add.Tensor(getitem, 4);  getitem = None\n     return (add, add_1)\"\"\",  # noqa: B950\n"
        },
        {
            "name": "test_control_flow.py",
            "path": "test/functorch/test_control_flow.py",
            "patches": [
                {
                    "old_start": 877,
                    "old_length": 7,
                    "new_start": 877,
                    "new_length": 7,
                    "hunk": "@@ -877,7 +877,7 @@ def forward(self, arg0_1, arg1_1, arg2_1, arg3_1):\n             f(x, torch.tensor(True), torch.tensor(True)),\n         )\n \n-    def test_cond_functionalized_hah(self):\n+    def test_cond_functionalized(self):\n         def true_fn(x):\n             y = x.sin()\n             y.add_(4)\n"
                },
                {
                    "old_start": 894,
                    "old_length": 7,
                    "new_start": 894,
                    "new_length": 9,
                    "hunk": "@@ -894,7 +894,9 @@ def forward(self, arg0_1, arg1_1, arg2_1, arg3_1):\n         functional_f = torch.func.functionalize(f)\n         self.assertEqual(functional_f(*example_inputs), f(*example_inputs))\n \n-        graph_module = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        graph_module = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+            *example_inputs\n+        )\n         self.assertEqual(graph_module(*example_inputs), f(*example_inputs))\n \n         all_ops_in_true_branch = []\n"
                },
                {
                    "old_start": 904,
                    "old_length": 9,
                    "new_start": 906,
                    "new_length": 6,
                    "hunk": "@@ -904,9 +906,6 @@ def forward(self, arg0_1, arg1_1, arg2_1, arg3_1):\n \n         self.assertFalse(any(op._schema.is_mutable for op in all_ops_in_true_branch))\n \n-        graph_module = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n-            *example_inputs\n-        )\n         self.assertEqual(graph_module(*example_inputs), f(*example_inputs))\n \n     def test_cond_accepts_torch_function_as_inputs(self):\n"
                },
                {
                    "old_start": 925,
                    "old_length": 8,
                    "new_start": 924,
                    "new_length": 8,
                    "hunk": "@@ -925,8 +924,8 @@ def forward(self, a_1, b_1):\n     gt = torch.ops.aten.gt.Scalar(sum_1, 0);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [a_1, b_1]);  gt = true_graph_0 = false_graph_0 = a_1 = b_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [a_1, b_1]);  gt = true_graph_0 = false_graph_0 = a_1 = b_1 = None\n+    getitem = cond[0];  cond = None\n     return getitem\"\"\",  # noqa: B950\n         )\n         self.assertExpectedInline(\n"
                },
                {
                    "old_start": 973,
                    "old_length": 9,
                    "new_start": 972,
                    "new_length": 9,
                    "hunk": "@@ -973,9 +972,9 @@ def forward(self, arg0_1, arg1_1):\n             z = torch.add(y, y)\n             return z\n \n-        symbolic_traced_graph = self._check_tracing(f, (torch.ones(4), True))[\n-            \"symbolic\"\n-        ]\n+        symbolic_traced_graph = self._check_tracing(\n+            f, (torch.ones(4), torch.Tensor([True]))\n+        )[\"symbolic\"]\n         graph_shape_env = symbolic_traced_graph.shape_env\n \n         def _node_shape_env_iter(gm):\n"
                },
                {
                    "old_start": 1021,
                    "old_length": 15,
                    "new_start": 1020,
                    "new_length": 14,
                    "hunk": "@@ -1021,15 +1020,14 @@ def forward(self, arg0_1, arg1_1):\n         functional_f = torch.func.functionalize(f)\n         self.assertEqual(functional_f(*example_inputs), f(*example_inputs))\n \n-        graph_module = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        graph_module = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+            *example_inputs\n+        )\n         self.assertEqual(graph_module(*example_inputs), f(*example_inputs))\n \n         gm_true_true_branch = graph_module.true_graph_0.true_graph_0\n \n-        graph_module1 = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n-            *example_inputs\n-        )\n-        self.assertEqual(graph_module1(*example_inputs), f(*example_inputs))\n+        self.assertEqual(graph_module(*example_inputs), f(*example_inputs))\n \n         all_ops = []\n         for node in gm_true_true_branch.graph.nodes:\n"
                },
                {
                    "old_start": 1057,
                    "old_length": 8,
                    "new_start": 1055,
                    "new_length": 7,
                    "hunk": "@@ -1057,8 +1055,7 @@ def forward(self, arg0_1, arg1_1):\n         self.assertEqual(graph_module(*example_inputs), f(*example_inputs))\n \n     # https://github.com/pytorch/pytorch/issues/126988\n-    @xfailIfTorchDynamo\n-    def test_cond_functionalized_input_mutation_on_true_branch(self):\n+    def test_cond_functionalized_input_mutation_on_true_brancte(self):\n         def true_fn(x):\n             view_x = x.view(x.shape)\n             view_x.add_(1)\n"
                },
                {
                    "old_start": 1072,
                    "old_length": 19,
                    "new_start": 1069,
                    "new_length": 33,
                    "hunk": "@@ -1072,19 +1069,33 @@ def forward(self, arg0_1, arg1_1):\n             return cond(pred, true_fn, false_fn, [x])\n \n         example_inputs = (torch.ones(4, 5),)\n-        functional_f = torch.func.functionalize(f)\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-        ):\n-            functional_f(*example_inputs)\n+        # torch.cond inlines into one of the branches because the predicate\n+        # is a constant.\n+        gm = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        self.assertExpectedInline(\n+            gm.code.strip(),\n+            \"\"\"\\\n+def forward(self, x_1):\n+    view = torch.ops.aten.view.default(x_1, [4, 5])\n+    add = torch.ops.aten.add.Tensor(view, 1);  view = None\n+    view_1 = torch.ops.aten.view.default(add, [4, 5]);  add = None\n+    view_2 = torch.ops.aten.view.default(view_1, [4, 5])\n+    sin = torch.ops.aten.sin.default(view_2);  view_2 = None\n+    sum_1 = torch.ops.aten.sum.default(sin);  sin = None\n+    copy_ = torch.ops.aten.copy_.default(x_1, view_1);  x_1 = view_1 = None\n+    return sum_1\"\"\",\n+        )\n \n+        # torch.cond triggers the check of the branches because the predicate\n+        # is a SymBool.\n         with self.assertRaisesRegex(\n             UnsupportedAliasMutationException, \"One of torch.cond branch\"\n         ):\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n \n     # https://github.com/pytorch/pytorch/issues/126988\n-    @xfailIfTorchDynamo\n     def test_cond_functionalized_input_mutation_on_false_branch(self):\n         def true_fn(x):\n             return x.sin().sum()\n"
                },
                {
                    "old_start": 1099,
                    "old_length": 19,
                    "new_start": 1110,
                    "new_length": 33,
                    "hunk": "@@ -1099,19 +1110,33 @@ def forward(self, arg0_1, arg1_1):\n             return cond(pred, true_fn, false_fn, [x])\n \n         example_inputs = (torch.ones(5, 5),)\n-        functional_f = torch.func.functionalize(f)\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-        ):\n-            functional_f(*example_inputs)\n+        gm = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        # torch.cond inlines into one of the branches because the predicate\n+        # is a constant.\n+        self.assertExpectedInline(\n+            gm.code.strip(),\n+            \"\"\"\\\n+def forward(self, x_1):\n+    view = torch.ops.aten.view.default(x_1, [5, 5])\n+    add = torch.ops.aten.add.Tensor(view, 1);  view = None\n+    view_1 = torch.ops.aten.view.default(add, [5, 5]);  add = None\n+    view_2 = torch.ops.aten.view.default(view_1, [5, 5])\n+    cos = torch.ops.aten.cos.default(view_2);  view_2 = None\n+    sum_1 = torch.ops.aten.sum.default(cos);  cos = None\n+    copy_ = torch.ops.aten.copy_.default(x_1, view_1);  x_1 = view_1 = None\n+    return sum_1\"\"\",\n+        )\n \n+        # torch.cond triggers the check of the branches because the predicate\n+        # is a SymBool.\n         with self.assertRaisesRegex(\n             UnsupportedAliasMutationException, \"One of torch.cond branch\"\n         ):\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n \n     # https://github.com/pytorch/pytorch/issues/126988\n-    @xfailIfTorchDynamo\n     def test_cond_functionalized_output_alias_input(self):\n         def true_fn(x):\n             return x\n"
                },
                {
                    "old_start": 1125,
                    "old_length": 22,
                    "new_start": 1150,
                    "new_length": 27,
                    "hunk": "@@ -1125,22 +1150,27 @@ def forward(self, arg0_1, arg1_1):\n             return cond(pred, true_fn, false_fn, [x])\n \n         example_inputs = (torch.ones(5, 5),)\n-        functional_f = torch.func.functionalize(f)\n-\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException,\n-            \"One of torch.cond branch might be aliasing\",\n-        ):\n-            functional_f(*example_inputs)\n+        gm = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        # torch.cond inlines into one of the branches because the predicate\n+        # is a constant.\n+        self.assertExpectedInline(\n+            gm.code.strip(),\n+            \"\"\"\\\n+def forward(self, x_1):\n+    view = torch.ops.aten.view.default(x_1, [5, 5]);  x_1 = None\n+    return view\"\"\",\n+        )\n \n+        # torch.cond triggers the check of the branches because the predicate\n+        # is a SymBool.\n         with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException,\n-            \"One of torch.cond branch might be aliasing\",\n+            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n         ):\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n \n     # https://github.com/pytorch/pytorch/issues/126988\n-    @xfailIfTorchDynamo\n     def test_cond_functionalized_nested_input_mutation(self):\n         def true_true_fn(x):\n             x.add_(4)\n"
                },
                {
                    "old_start": 1161,
                    "old_length": 19,
                    "new_start": 1191,
                    "new_length": 14,
                    "hunk": "@@ -1161,19 +1191,14 @@ def forward(self, arg0_1, arg1_1):\n             return cond(pred, true_fn, false_fn, [x])\n \n         example_inputs = (torch.ones(4, 5),)\n-        functional_f = torch.func.functionalize(f)\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-        ):\n-            functional_f(*example_inputs)\n-\n         with self.assertRaisesRegex(\n             UnsupportedAliasMutationException, \"One of torch.cond branch\"\n         ):\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n \n     # https://github.com/pytorch/pytorch/issues/126988\n-    @xfailIfTorchDynamo\n     def test_cond_functionalized_nested_input_mutation_with_aot_func(self):\n         def true_true_fn(x):\n             x.add_(4)\n"
                },
                {
                    "old_start": 1197,
                    "old_length": 15,
                    "new_start": 1222,
                    "new_length": 12,
                    "hunk": "@@ -1197,15 +1222,12 @@ def forward(self, arg0_1, arg1_1):\n         try:\n             example_input_func = to_fun_old(example_input)\n             torch._enable_functionalization(reapply_views=False)\n-            with self.assertRaisesRegex(\n-                UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-            ):\n-                f(example_input_func)\n+            f(example_input_func)\n \n             with self.assertRaisesRegex(\n                 UnsupportedAliasMutationException, \"One of torch.cond branch\"\n             ):\n-                make_fx(f)(example_input_func)\n+                make_fx(f, tracing_mode=\"symbolic\")(example_input_func)\n         finally:\n             torch._disable_functionalization()\n \n"
                },
                {
                    "old_start": 1223,
                    "old_length": 7,
                    "new_start": 1245,
                    "new_length": 7,
                    "hunk": "@@ -1223,7 +1245,7 @@ def forward(self, arg0_1, arg1_1):\n         with self.assertRaisesRegex(\n             UnsupportedAliasMutationException, \"One of torch.cond branch\"\n         ):\n-            make_fx(f_wrapper(f))(example_input_func)\n+            make_fx(f_wrapper(f), tracing_mode=\"symbolic\")(example_input_func)\n \n     # https://github.com/pytorch/pytorch/issues/126988\n     @xfailIfTorchDynamo\n"
                },
                {
                    "old_start": 1236,
                    "old_length": 7,
                    "new_start": 1258,
                    "new_length": 7,
                    "hunk": "@@ -1236,7 +1258,7 @@ def forward(self, arg0_1, arg1_1):\n             return view_x\n \n         def f(x):\n-            pred = x.shape[0] == 4\n+            pred = x.sum() > 0\n             return cond(pred, true_fn, false_fn, [x])\n \n         example_input = torch.ones(5, 5)\n"
                },
                {
                    "old_start": 1278,
                    "old_length": 7,
                    "new_start": 1300,
                    "new_length": 7,
                    "hunk": "@@ -1278,7 +1300,7 @@ def forward(self, arg0_1, arg1_1):\n             UnsupportedAliasMutationException,\n             \"One of torch.cond branch might be aliasing\",\n         ):\n-            make_fx(f_wrapper(f))(example_input)\n+            make_fx(f_wrapper(f), tracing_mode=\"symbolic\")(example_input)\n \n     def test_cond_functionalized_aot_func_check_functional(self):\n         def true_fn(x):\n"
                },
                {
                    "old_start": 1316,
                    "old_length": 7,
                    "new_start": 1338,
                    "new_length": 7,
                    "hunk": "@@ -1316,7 +1338,7 @@ def forward(self, arg0_1, arg1_1):\n \n             return wrapper\n \n-        result_gm = make_fx(f_wrapper(f))(example_input)\n+        result_gm = make_fx(f_wrapper(f), tracing_mode=\"symbolic\")(example_input)\n         for node in result_gm.true_graph_0.graph.nodes:\n             if node.op == \"call_function\":\n                 self.assertTrue(not node.target._schema.is_mutable)\n"
                },
                {
                    "old_start": 1382,
                    "old_length": 12,
                    "new_start": 1404,
                    "new_length": 12,
                    "hunk": "@@ -1382,12 +1404,12 @@ def forward(self, arg0_1, arg1_1):\n def forward(self, x_1, pred_1, pred2_1):\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n+    getitem = cond[0];  cond = None\n     true_graph_1 = self.true_graph_1\n     false_graph_1 = self.false_graph_1\n-    conditional_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n-    getitem_1 = conditional_1[0];  conditional_1 = None\n+    cond_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n+    getitem_1 = cond_1[0];  cond_1 = None\n     add = torch.ops.aten.add.Tensor(getitem, getitem_1);  getitem = getitem_1 = None\n     return add\"\"\",  # noqa: B950\n         )\n"
                },
                {
                    "old_start": 1555,
                    "old_length": 12,
                    "new_start": 1577,
                    "new_length": 12,
                    "hunk": "@@ -1555,12 +1577,12 @@ def forward(self, arg0_1):\n def forward(self, x_1, pred_1, pred2_1):\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n+    getitem = cond[0];  cond = None\n     true_graph_1 = self.true_graph_1\n     false_graph_1 = self.false_graph_1\n-    conditional_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n-    getitem_1 = conditional_1[0];  conditional_1 = None\n+    cond_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n+    getitem_1 = cond_1[0];  cond_1 = None\n     add = torch.ops.aten.add.Tensor(getitem, getitem_1);  getitem = getitem_1 = None\n     return add\"\"\",  # noqa: B950\n         )\n"
                },
                {
                    "old_start": 1891,
                    "old_length": 7,
                    "new_start": 1913,
                    "new_length": 7,
                    "hunk": "@@ -1891,7 +1913,7 @@ def forward(self, arg0_1):\n         ):\n             functional_f(*example_inputs)\n \n-    def test_cond_autograd_fail(self):\n+    def test_cond_autograd_succeed_when_pred_is_constant(self):\n         def true_fn(x):\n             return x.cos()\n \n"
                },
                {
                    "old_start": 1901,
                    "old_length": 6,
                    "new_start": 1923,
                    "new_length": 27,
                    "hunk": "@@ -1901,6 +1923,27 @@ def forward(self, arg0_1):\n         def f(x, y):\n             return control_flow.cond(x.shape[0] > 4, true_fn, false_fn, [y])\n \n+        example_inputs = (\n+            torch.ones(3, 2, 4, requires_grad=True),\n+            torch.ones(4, requires_grad=True),\n+        )\n+        # Due to x.shape[0] can be statically evaluated to be False, we can evaluate\n+        # the backward.\n+        f(*example_inputs).sum().backward()\n+\n+        # Ensure no error is thrown when not running backward\n+        f(*example_inputs)\n+\n+    def test_cond_autograd_fail(self):\n+        def true_fn(x):\n+            return x.cos()\n+\n+        def false_fn(x):\n+            return x.sin()\n+\n+        def f(x, y):\n+            return control_flow.cond(x.sum() > 4, true_fn, false_fn, [y])\n+\n         example_inputs = (\n             torch.ones(3, 2, 4, requires_grad=True),\n             torch.ones(4, requires_grad=True),\n"
                },
                {
                    "old_start": 2029,
                    "old_length": 8,
                    "new_start": 2072,
                    "new_length": 8,
                    "hunk": "@@ -2029,8 +2072,8 @@ def forward(self, x_1):\n     eq = sym_size_int == 4;  sym_size_int = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n+    getitem = cond[0];  cond = None\n     return getitem\"\"\",  # noqa: B950\n         )\n \n"
                },
                {
                    "old_start": 2102,
                    "old_length": 18,
                    "new_start": 2145,
                    "new_length": 20,
                    "hunk": "@@ -2102,18 +2145,20 @@ def forward(self, x_1):\n         # expected branches takes [x, a, b] as input\n         inp = torch.randn(2, 3)\n \n-        gm = make_fx(foo)(inp)\n+        gm = make_fx(foo, tracing_mode=\"symbolic\", _allow_non_fake_inputs=True)(inp)\n \n         self.assertExpectedInline(\n             gm.code.strip(),\n             \"\"\"\\\n def forward(self, x_1):\n+    sym_size_int = torch.ops.aten.sym_size.int(x_1, 0)\n+    eq = sym_size_int == 4;  sym_size_int = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n     _tensor_constant0 = self._tensor_constant0\n     _tensor_constant1 = self._tensor_constant1\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [x_1, _tensor_constant0, _tensor_constant1]);  true_graph_0 = false_graph_0 = x_1 = _tensor_constant0 = _tensor_constant1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1, _tensor_constant0, _tensor_constant1]);  eq = true_graph_0 = false_graph_0 = x_1 = _tensor_constant0 = _tensor_constant1 = None\n+    getitem = cond[0];  cond = None\n     return getitem\"\"\",  # noqa: B950\n         )\n         self.assertExpectedInline(\n"
                },
                {
                    "old_start": 2263,
                    "old_length": 8,
                    "new_start": 2308,
                    "new_length": 8,
                    "hunk": "@@ -2263,8 +2308,8 @@ def forward(self, pred_1, x_1):\n def forward(self, arg0_1, arg1_1):\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(arg1_1, true_graph_0, false_graph_0, [arg0_1]);  arg1_1 = true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(arg1_1, true_graph_0, false_graph_0, [arg0_1]);  arg1_1 = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n     return [getitem]\"\"\",  # noqa: B950\n         )\n \n"
                },
                {
                    "old_start": 2305,
                    "old_length": 7,
                    "new_start": 2350,
                    "new_length": 7,
                    "hunk": "@@ -2305,7 +2350,7 @@ def forward(self, arg0_1, arg1_1):\n         counters.clear()\n \n         def foo(x, true_fn, false_fn):\n-            return cond(x.shape[0] == 4, true_fn, false_fn, (x,))\n+            return cond(x.sum() < 0, true_fn, false_fn, (x,))\n \n         inp = torch.ones(3, 4)\n         exp_out = inp.sin()\n"
                },
                {
                    "old_start": 2347,
                    "old_length": 8,
                    "new_start": 2392,
                    "new_length": 8,
                    "hunk": "@@ -2347,8 +2392,8 @@ def forward(self, x_1):\n     eq = sym_size_int == 4;  sym_size_int = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n+    getitem = cond[0];  cond = None\n     return getitem\"\"\",  # noqa: B950\n             )\n \n"
                }
            ],
            "whole_deleted": "-    def test_cond_functionalized_hah(self):\n-        graph_module = make_fx(torch.func.functionalize(f))(*example_inputs)\n-        graph_module = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n-            *example_inputs\n-        )\n-    conditional = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [a_1, b_1]);  gt = true_graph_0 = false_graph_0 = a_1 = b_1 = None\n-    getitem = conditional[0];  conditional = None\n-        symbolic_traced_graph = self._check_tracing(f, (torch.ones(4), True))[\n-            \"symbolic\"\n-        ]\n-        graph_module = make_fx(torch.func.functionalize(f))(*example_inputs)\n-        graph_module1 = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n-            *example_inputs\n-        )\n-        self.assertEqual(graph_module1(*example_inputs), f(*example_inputs))\n-    @xfailIfTorchDynamo\n-    def test_cond_functionalized_input_mutation_on_true_branch(self):\n-        functional_f = torch.func.functionalize(f)\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-        ):\n-            functional_f(*example_inputs)\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n-    @xfailIfTorchDynamo\n-        functional_f = torch.func.functionalize(f)\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-        ):\n-            functional_f(*example_inputs)\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n-    @xfailIfTorchDynamo\n-        functional_f = torch.func.functionalize(f)\n-\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException,\n-            \"One of torch.cond branch might be aliasing\",\n-        ):\n-            functional_f(*example_inputs)\n-            UnsupportedAliasMutationException,\n-            \"One of torch.cond branch might be aliasing\",\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n-    @xfailIfTorchDynamo\n-        functional_f = torch.func.functionalize(f)\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-        ):\n-            functional_f(*example_inputs)\n-\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n-    @xfailIfTorchDynamo\n-            with self.assertRaisesRegex(\n-                UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-            ):\n-                f(example_input_func)\n-                make_fx(f)(example_input_func)\n-            make_fx(f_wrapper(f))(example_input_func)\n-            pred = x.shape[0] == 4\n-            make_fx(f_wrapper(f))(example_input)\n-        result_gm = make_fx(f_wrapper(f))(example_input)\n-    conditional = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n-    getitem = conditional[0];  conditional = None\n-    conditional_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n-    getitem_1 = conditional_1[0];  conditional_1 = None\n-    conditional = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n-    getitem = conditional[0];  conditional = None\n-    conditional_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n-    getitem_1 = conditional_1[0];  conditional_1 = None\n-    def test_cond_autograd_fail(self):\n-    conditional = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n-    getitem = conditional[0];  conditional = None\n-        gm = make_fx(foo)(inp)\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [x_1, _tensor_constant0, _tensor_constant1]);  true_graph_0 = false_graph_0 = x_1 = _tensor_constant0 = _tensor_constant1 = None\n-    getitem = conditional[0];  conditional = None\n-    conditional = torch.ops.higher_order.cond(arg1_1, true_graph_0, false_graph_0, [arg0_1]);  arg1_1 = true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n-            return cond(x.shape[0] == 4, true_fn, false_fn, (x,))\n-    conditional = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n-    getitem = conditional[0];  conditional = None\n",
            "whole_added": "+    def test_cond_functionalized(self):\n+        graph_module = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+            *example_inputs\n+        )\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [a_1, b_1]);  gt = true_graph_0 = false_graph_0 = a_1 = b_1 = None\n+    getitem = cond[0];  cond = None\n+        symbolic_traced_graph = self._check_tracing(\n+            f, (torch.ones(4), torch.Tensor([True]))\n+        )[\"symbolic\"]\n+        graph_module = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+            *example_inputs\n+        )\n+        self.assertEqual(graph_module(*example_inputs), f(*example_inputs))\n+    def test_cond_functionalized_input_mutation_on_true_brancte(self):\n+        # torch.cond inlines into one of the branches because the predicate\n+        # is a constant.\n+        gm = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        self.assertExpectedInline(\n+            gm.code.strip(),\n+            \"\"\"\\\n+def forward(self, x_1):\n+    view = torch.ops.aten.view.default(x_1, [4, 5])\n+    add = torch.ops.aten.add.Tensor(view, 1);  view = None\n+    view_1 = torch.ops.aten.view.default(add, [4, 5]);  add = None\n+    view_2 = torch.ops.aten.view.default(view_1, [4, 5])\n+    sin = torch.ops.aten.sin.default(view_2);  view_2 = None\n+    sum_1 = torch.ops.aten.sum.default(sin);  sin = None\n+    copy_ = torch.ops.aten.copy_.default(x_1, view_1);  x_1 = view_1 = None\n+    return sum_1\"\"\",\n+        )\n+        # torch.cond triggers the check of the branches because the predicate\n+        # is a SymBool.\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n+        gm = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        # torch.cond inlines into one of the branches because the predicate\n+        # is a constant.\n+        self.assertExpectedInline(\n+            gm.code.strip(),\n+            \"\"\"\\\n+def forward(self, x_1):\n+    view = torch.ops.aten.view.default(x_1, [5, 5])\n+    add = torch.ops.aten.add.Tensor(view, 1);  view = None\n+    view_1 = torch.ops.aten.view.default(add, [5, 5]);  add = None\n+    view_2 = torch.ops.aten.view.default(view_1, [5, 5])\n+    cos = torch.ops.aten.cos.default(view_2);  view_2 = None\n+    sum_1 = torch.ops.aten.sum.default(cos);  cos = None\n+    copy_ = torch.ops.aten.copy_.default(x_1, view_1);  x_1 = view_1 = None\n+    return sum_1\"\"\",\n+        )\n+        # torch.cond triggers the check of the branches because the predicate\n+        # is a SymBool.\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n+        gm = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        # torch.cond inlines into one of the branches because the predicate\n+        # is a constant.\n+        self.assertExpectedInline(\n+            gm.code.strip(),\n+            \"\"\"\\\n+def forward(self, x_1):\n+    view = torch.ops.aten.view.default(x_1, [5, 5]);  x_1 = None\n+    return view\"\"\",\n+        )\n+        # torch.cond triggers the check of the branches because the predicate\n+        # is a SymBool.\n+            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n+            f(example_input_func)\n+                make_fx(f, tracing_mode=\"symbolic\")(example_input_func)\n+            make_fx(f_wrapper(f), tracing_mode=\"symbolic\")(example_input_func)\n+            pred = x.sum() > 0\n+            make_fx(f_wrapper(f), tracing_mode=\"symbolic\")(example_input)\n+        result_gm = make_fx(f_wrapper(f), tracing_mode=\"symbolic\")(example_input)\n+    cond = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n+    getitem = cond[0];  cond = None\n+    cond_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n+    getitem_1 = cond_1[0];  cond_1 = None\n+    cond = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n+    getitem = cond[0];  cond = None\n+    cond_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n+    getitem_1 = cond_1[0];  cond_1 = None\n+    def test_cond_autograd_succeed_when_pred_is_constant(self):\n+        example_inputs = (\n+            torch.ones(3, 2, 4, requires_grad=True),\n+            torch.ones(4, requires_grad=True),\n+        )\n+        # Due to x.shape[0] can be statically evaluated to be False, we can evaluate\n+        # the backward.\n+        f(*example_inputs).sum().backward()\n+\n+        # Ensure no error is thrown when not running backward\n+        f(*example_inputs)\n+\n+    def test_cond_autograd_fail(self):\n+        def true_fn(x):\n+            return x.cos()\n+\n+        def false_fn(x):\n+            return x.sin()\n+\n+        def f(x, y):\n+            return control_flow.cond(x.sum() > 4, true_fn, false_fn, [y])\n+\n+    cond = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n+    getitem = cond[0];  cond = None\n+        gm = make_fx(foo, tracing_mode=\"symbolic\", _allow_non_fake_inputs=True)(inp)\n+    sym_size_int = torch.ops.aten.sym_size.int(x_1, 0)\n+    eq = sym_size_int == 4;  sym_size_int = None\n+    cond = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1, _tensor_constant0, _tensor_constant1]);  eq = true_graph_0 = false_graph_0 = x_1 = _tensor_constant0 = _tensor_constant1 = None\n+    getitem = cond[0];  cond = None\n+    cond = torch.ops.higher_order.cond(arg1_1, true_graph_0, false_graph_0, [arg0_1]);  arg1_1 = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n+            return cond(x.sum() < 0, true_fn, false_fn, (x,))\n+    cond = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n+    getitem = cond[0];  cond = None\n",
            "whole_hunk": "@@ -877,7 +877,7 @@ def forward(self, arg0_1, arg1_1, arg2_1, arg3_1):\n             f(x, torch.tensor(True), torch.tensor(True)),\n         )\n \n-    def test_cond_functionalized_hah(self):\n+    def test_cond_functionalized(self):\n         def true_fn(x):\n             y = x.sin()\n             y.add_(4)\n@@ -894,7 +894,9 @@ def forward(self, arg0_1, arg1_1, arg2_1, arg3_1):\n         functional_f = torch.func.functionalize(f)\n         self.assertEqual(functional_f(*example_inputs), f(*example_inputs))\n \n-        graph_module = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        graph_module = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+            *example_inputs\n+        )\n         self.assertEqual(graph_module(*example_inputs), f(*example_inputs))\n \n         all_ops_in_true_branch = []\n@@ -904,9 +906,6 @@ def forward(self, arg0_1, arg1_1, arg2_1, arg3_1):\n \n         self.assertFalse(any(op._schema.is_mutable for op in all_ops_in_true_branch))\n \n-        graph_module = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n-            *example_inputs\n-        )\n         self.assertEqual(graph_module(*example_inputs), f(*example_inputs))\n \n     def test_cond_accepts_torch_function_as_inputs(self):\n@@ -925,8 +924,8 @@ def forward(self, a_1, b_1):\n     gt = torch.ops.aten.gt.Scalar(sum_1, 0);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [a_1, b_1]);  gt = true_graph_0 = false_graph_0 = a_1 = b_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [a_1, b_1]);  gt = true_graph_0 = false_graph_0 = a_1 = b_1 = None\n+    getitem = cond[0];  cond = None\n     return getitem\"\"\",  # noqa: B950\n         )\n         self.assertExpectedInline(\n@@ -973,9 +972,9 @@ def forward(self, arg0_1, arg1_1):\n             z = torch.add(y, y)\n             return z\n \n-        symbolic_traced_graph = self._check_tracing(f, (torch.ones(4), True))[\n-            \"symbolic\"\n-        ]\n+        symbolic_traced_graph = self._check_tracing(\n+            f, (torch.ones(4), torch.Tensor([True]))\n+        )[\"symbolic\"]\n         graph_shape_env = symbolic_traced_graph.shape_env\n \n         def _node_shape_env_iter(gm):\n@@ -1021,15 +1020,14 @@ def forward(self, arg0_1, arg1_1):\n         functional_f = torch.func.functionalize(f)\n         self.assertEqual(functional_f(*example_inputs), f(*example_inputs))\n \n-        graph_module = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        graph_module = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+            *example_inputs\n+        )\n         self.assertEqual(graph_module(*example_inputs), f(*example_inputs))\n \n         gm_true_true_branch = graph_module.true_graph_0.true_graph_0\n \n-        graph_module1 = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n-            *example_inputs\n-        )\n-        self.assertEqual(graph_module1(*example_inputs), f(*example_inputs))\n+        self.assertEqual(graph_module(*example_inputs), f(*example_inputs))\n \n         all_ops = []\n         for node in gm_true_true_branch.graph.nodes:\n@@ -1057,8 +1055,7 @@ def forward(self, arg0_1, arg1_1):\n         self.assertEqual(graph_module(*example_inputs), f(*example_inputs))\n \n     # https://github.com/pytorch/pytorch/issues/126988\n-    @xfailIfTorchDynamo\n-    def test_cond_functionalized_input_mutation_on_true_branch(self):\n+    def test_cond_functionalized_input_mutation_on_true_brancte(self):\n         def true_fn(x):\n             view_x = x.view(x.shape)\n             view_x.add_(1)\n@@ -1072,19 +1069,33 @@ def forward(self, arg0_1, arg1_1):\n             return cond(pred, true_fn, false_fn, [x])\n \n         example_inputs = (torch.ones(4, 5),)\n-        functional_f = torch.func.functionalize(f)\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-        ):\n-            functional_f(*example_inputs)\n+        # torch.cond inlines into one of the branches because the predicate\n+        # is a constant.\n+        gm = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        self.assertExpectedInline(\n+            gm.code.strip(),\n+            \"\"\"\\\n+def forward(self, x_1):\n+    view = torch.ops.aten.view.default(x_1, [4, 5])\n+    add = torch.ops.aten.add.Tensor(view, 1);  view = None\n+    view_1 = torch.ops.aten.view.default(add, [4, 5]);  add = None\n+    view_2 = torch.ops.aten.view.default(view_1, [4, 5])\n+    sin = torch.ops.aten.sin.default(view_2);  view_2 = None\n+    sum_1 = torch.ops.aten.sum.default(sin);  sin = None\n+    copy_ = torch.ops.aten.copy_.default(x_1, view_1);  x_1 = view_1 = None\n+    return sum_1\"\"\",\n+        )\n \n+        # torch.cond triggers the check of the branches because the predicate\n+        # is a SymBool.\n         with self.assertRaisesRegex(\n             UnsupportedAliasMutationException, \"One of torch.cond branch\"\n         ):\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n \n     # https://github.com/pytorch/pytorch/issues/126988\n-    @xfailIfTorchDynamo\n     def test_cond_functionalized_input_mutation_on_false_branch(self):\n         def true_fn(x):\n             return x.sin().sum()\n@@ -1099,19 +1110,33 @@ def forward(self, arg0_1, arg1_1):\n             return cond(pred, true_fn, false_fn, [x])\n \n         example_inputs = (torch.ones(5, 5),)\n-        functional_f = torch.func.functionalize(f)\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-        ):\n-            functional_f(*example_inputs)\n+        gm = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        # torch.cond inlines into one of the branches because the predicate\n+        # is a constant.\n+        self.assertExpectedInline(\n+            gm.code.strip(),\n+            \"\"\"\\\n+def forward(self, x_1):\n+    view = torch.ops.aten.view.default(x_1, [5, 5])\n+    add = torch.ops.aten.add.Tensor(view, 1);  view = None\n+    view_1 = torch.ops.aten.view.default(add, [5, 5]);  add = None\n+    view_2 = torch.ops.aten.view.default(view_1, [5, 5])\n+    cos = torch.ops.aten.cos.default(view_2);  view_2 = None\n+    sum_1 = torch.ops.aten.sum.default(cos);  cos = None\n+    copy_ = torch.ops.aten.copy_.default(x_1, view_1);  x_1 = view_1 = None\n+    return sum_1\"\"\",\n+        )\n \n+        # torch.cond triggers the check of the branches because the predicate\n+        # is a SymBool.\n         with self.assertRaisesRegex(\n             UnsupportedAliasMutationException, \"One of torch.cond branch\"\n         ):\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n \n     # https://github.com/pytorch/pytorch/issues/126988\n-    @xfailIfTorchDynamo\n     def test_cond_functionalized_output_alias_input(self):\n         def true_fn(x):\n             return x\n@@ -1125,22 +1150,27 @@ def forward(self, arg0_1, arg1_1):\n             return cond(pred, true_fn, false_fn, [x])\n \n         example_inputs = (torch.ones(5, 5),)\n-        functional_f = torch.func.functionalize(f)\n-\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException,\n-            \"One of torch.cond branch might be aliasing\",\n-        ):\n-            functional_f(*example_inputs)\n+        gm = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        # torch.cond inlines into one of the branches because the predicate\n+        # is a constant.\n+        self.assertExpectedInline(\n+            gm.code.strip(),\n+            \"\"\"\\\n+def forward(self, x_1):\n+    view = torch.ops.aten.view.default(x_1, [5, 5]);  x_1 = None\n+    return view\"\"\",\n+        )\n \n+        # torch.cond triggers the check of the branches because the predicate\n+        # is a SymBool.\n         with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException,\n-            \"One of torch.cond branch might be aliasing\",\n+            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n         ):\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n \n     # https://github.com/pytorch/pytorch/issues/126988\n-    @xfailIfTorchDynamo\n     def test_cond_functionalized_nested_input_mutation(self):\n         def true_true_fn(x):\n             x.add_(4)\n@@ -1161,19 +1191,14 @@ def forward(self, arg0_1, arg1_1):\n             return cond(pred, true_fn, false_fn, [x])\n \n         example_inputs = (torch.ones(4, 5),)\n-        functional_f = torch.func.functionalize(f)\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-        ):\n-            functional_f(*example_inputs)\n-\n         with self.assertRaisesRegex(\n             UnsupportedAliasMutationException, \"One of torch.cond branch\"\n         ):\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n \n     # https://github.com/pytorch/pytorch/issues/126988\n-    @xfailIfTorchDynamo\n     def test_cond_functionalized_nested_input_mutation_with_aot_func(self):\n         def true_true_fn(x):\n             x.add_(4)\n@@ -1197,15 +1222,12 @@ def forward(self, arg0_1, arg1_1):\n         try:\n             example_input_func = to_fun_old(example_input)\n             torch._enable_functionalization(reapply_views=False)\n-            with self.assertRaisesRegex(\n-                UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-            ):\n-                f(example_input_func)\n+            f(example_input_func)\n \n             with self.assertRaisesRegex(\n                 UnsupportedAliasMutationException, \"One of torch.cond branch\"\n             ):\n-                make_fx(f)(example_input_func)\n+                make_fx(f, tracing_mode=\"symbolic\")(example_input_func)\n         finally:\n             torch._disable_functionalization()\n \n@@ -1223,7 +1245,7 @@ def forward(self, arg0_1, arg1_1):\n         with self.assertRaisesRegex(\n             UnsupportedAliasMutationException, \"One of torch.cond branch\"\n         ):\n-            make_fx(f_wrapper(f))(example_input_func)\n+            make_fx(f_wrapper(f), tracing_mode=\"symbolic\")(example_input_func)\n \n     # https://github.com/pytorch/pytorch/issues/126988\n     @xfailIfTorchDynamo\n@@ -1236,7 +1258,7 @@ def forward(self, arg0_1, arg1_1):\n             return view_x\n \n         def f(x):\n-            pred = x.shape[0] == 4\n+            pred = x.sum() > 0\n             return cond(pred, true_fn, false_fn, [x])\n \n         example_input = torch.ones(5, 5)\n@@ -1278,7 +1300,7 @@ def forward(self, arg0_1, arg1_1):\n             UnsupportedAliasMutationException,\n             \"One of torch.cond branch might be aliasing\",\n         ):\n-            make_fx(f_wrapper(f))(example_input)\n+            make_fx(f_wrapper(f), tracing_mode=\"symbolic\")(example_input)\n \n     def test_cond_functionalized_aot_func_check_functional(self):\n         def true_fn(x):\n@@ -1316,7 +1338,7 @@ def forward(self, arg0_1, arg1_1):\n \n             return wrapper\n \n-        result_gm = make_fx(f_wrapper(f))(example_input)\n+        result_gm = make_fx(f_wrapper(f), tracing_mode=\"symbolic\")(example_input)\n         for node in result_gm.true_graph_0.graph.nodes:\n             if node.op == \"call_function\":\n                 self.assertTrue(not node.target._schema.is_mutable)\n@@ -1382,12 +1404,12 @@ def forward(self, arg0_1, arg1_1):\n def forward(self, x_1, pred_1, pred2_1):\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n+    getitem = cond[0];  cond = None\n     true_graph_1 = self.true_graph_1\n     false_graph_1 = self.false_graph_1\n-    conditional_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n-    getitem_1 = conditional_1[0];  conditional_1 = None\n+    cond_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n+    getitem_1 = cond_1[0];  cond_1 = None\n     add = torch.ops.aten.add.Tensor(getitem, getitem_1);  getitem = getitem_1 = None\n     return add\"\"\",  # noqa: B950\n         )\n@@ -1555,12 +1577,12 @@ def forward(self, arg0_1):\n def forward(self, x_1, pred_1, pred2_1):\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n+    getitem = cond[0];  cond = None\n     true_graph_1 = self.true_graph_1\n     false_graph_1 = self.false_graph_1\n-    conditional_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n-    getitem_1 = conditional_1[0];  conditional_1 = None\n+    cond_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n+    getitem_1 = cond_1[0];  cond_1 = None\n     add = torch.ops.aten.add.Tensor(getitem, getitem_1);  getitem = getitem_1 = None\n     return add\"\"\",  # noqa: B950\n         )\n@@ -1891,7 +1913,7 @@ def forward(self, arg0_1):\n         ):\n             functional_f(*example_inputs)\n \n-    def test_cond_autograd_fail(self):\n+    def test_cond_autograd_succeed_when_pred_is_constant(self):\n         def true_fn(x):\n             return x.cos()\n \n@@ -1901,6 +1923,27 @@ def forward(self, arg0_1):\n         def f(x, y):\n             return control_flow.cond(x.shape[0] > 4, true_fn, false_fn, [y])\n \n+        example_inputs = (\n+            torch.ones(3, 2, 4, requires_grad=True),\n+            torch.ones(4, requires_grad=True),\n+        )\n+        # Due to x.shape[0] can be statically evaluated to be False, we can evaluate\n+        # the backward.\n+        f(*example_inputs).sum().backward()\n+\n+        # Ensure no error is thrown when not running backward\n+        f(*example_inputs)\n+\n+    def test_cond_autograd_fail(self):\n+        def true_fn(x):\n+            return x.cos()\n+\n+        def false_fn(x):\n+            return x.sin()\n+\n+        def f(x, y):\n+            return control_flow.cond(x.sum() > 4, true_fn, false_fn, [y])\n+\n         example_inputs = (\n             torch.ones(3, 2, 4, requires_grad=True),\n             torch.ones(4, requires_grad=True),\n@@ -2029,8 +2072,8 @@ def forward(self, x_1):\n     eq = sym_size_int == 4;  sym_size_int = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n+    getitem = cond[0];  cond = None\n     return getitem\"\"\",  # noqa: B950\n         )\n \n@@ -2102,18 +2145,20 @@ def forward(self, x_1):\n         # expected branches takes [x, a, b] as input\n         inp = torch.randn(2, 3)\n \n-        gm = make_fx(foo)(inp)\n+        gm = make_fx(foo, tracing_mode=\"symbolic\", _allow_non_fake_inputs=True)(inp)\n \n         self.assertExpectedInline(\n             gm.code.strip(),\n             \"\"\"\\\n def forward(self, x_1):\n+    sym_size_int = torch.ops.aten.sym_size.int(x_1, 0)\n+    eq = sym_size_int == 4;  sym_size_int = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n     _tensor_constant0 = self._tensor_constant0\n     _tensor_constant1 = self._tensor_constant1\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [x_1, _tensor_constant0, _tensor_constant1]);  true_graph_0 = false_graph_0 = x_1 = _tensor_constant0 = _tensor_constant1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1, _tensor_constant0, _tensor_constant1]);  eq = true_graph_0 = false_graph_0 = x_1 = _tensor_constant0 = _tensor_constant1 = None\n+    getitem = cond[0];  cond = None\n     return getitem\"\"\",  # noqa: B950\n         )\n         self.assertExpectedInline(\n@@ -2263,8 +2308,8 @@ def forward(self, pred_1, x_1):\n def forward(self, arg0_1, arg1_1):\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(arg1_1, true_graph_0, false_graph_0, [arg0_1]);  arg1_1 = true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(arg1_1, true_graph_0, false_graph_0, [arg0_1]);  arg1_1 = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n     return [getitem]\"\"\",  # noqa: B950\n         )\n \n@@ -2305,7 +2350,7 @@ def forward(self, arg0_1, arg1_1):\n         counters.clear()\n \n         def foo(x, true_fn, false_fn):\n-            return cond(x.shape[0] == 4, true_fn, false_fn, (x,))\n+            return cond(x.sum() < 0, true_fn, false_fn, (x,))\n \n         inp = torch.ones(3, 4)\n         exp_out = inp.sin()\n@@ -2347,8 +2392,8 @@ def forward(self, x_1):\n     eq = sym_size_int == 4;  sym_size_int = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n+    getitem = cond[0];  cond = None\n     return getitem\"\"\",  # noqa: B950\n             )\n \n"
        },
        {
            "name": "higher_order_ops.py",
            "path": "torch/_dynamo/variables/higher_order_ops.py",
            "patches": [
                {
                    "old_start": 632,
                    "old_length": 6,
                    "new_start": 632,
                    "new_length": 18,
                    "hunk": "@@ -632,6 +632,18 @@ class CondHigherOrderVariable(TorchHigherOrderOperatorVariable):\n                 f\"Expected 4 arguments but got {len(args)}.\\n\"\n                 f\"Usage: cond(pred, true_fn, false_fn, operands)\",\n             )\n+\n+        # Specialize into one of the branches since pred is constant\n+        if type(args[0]) is ConstantVariable:\n+            log.warning(\n+                \"Pred is a Python constant. When used with torch.cond, it executes only one of the branches.\"\n+                \" If you want torch.cond to perserve two branches, please make the predicate a boolean tensor or a SymBool.\"\n+            )\n+            if args[0].as_python_constant():\n+                return args[1].call_function(tx, args[3].unpack_var_sequence(tx), {})\n+            else:\n+                return args[2].call_function(tx, args[3].unpack_var_sequence(tx), {})\n+\n         # predicate\n         if type(args[0]) not in (ConstantVariable, TensorVariable, SymNodeVariable):\n             unimplemented(\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+\n+        # Specialize into one of the branches since pred is constant\n+        if type(args[0]) is ConstantVariable:\n+            log.warning(\n+                \"Pred is a Python constant. When used with torch.cond, it executes only one of the branches.\"\n+                \" If you want torch.cond to perserve two branches, please make the predicate a boolean tensor or a SymBool.\"\n+            )\n+            if args[0].as_python_constant():\n+                return args[1].call_function(tx, args[3].unpack_var_sequence(tx), {})\n+            else:\n+                return args[2].call_function(tx, args[3].unpack_var_sequence(tx), {})\n+\n",
            "whole_hunk": "@@ -632,6 +632,18 @@ class CondHigherOrderVariable(TorchHigherOrderOperatorVariable):\n                 f\"Expected 4 arguments but got {len(args)}.\\n\"\n                 f\"Usage: cond(pred, true_fn, false_fn, operands)\",\n             )\n+\n+        # Specialize into one of the branches since pred is constant\n+        if type(args[0]) is ConstantVariable:\n+            log.warning(\n+                \"Pred is a Python constant. When used with torch.cond, it executes only one of the branches.\"\n+                \" If you want torch.cond to perserve two branches, please make the predicate a boolean tensor or a SymBool.\"\n+            )\n+            if args[0].as_python_constant():\n+                return args[1].call_function(tx, args[3].unpack_var_sequence(tx), {})\n+            else:\n+                return args[2].call_function(tx, args[3].unpack_var_sequence(tx), {})\n+\n         # predicate\n         if type(args[0]) not in (ConstantVariable, TensorVariable, SymNodeVariable):\n             unimplemented(\n"
        },
        {
            "name": "cond.py",
            "path": "torch/_higher_order_ops/cond.py",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 6,
                    "new_start": 1,
                    "new_length": 8,
                    "hunk": "@@ -1,6 +1,8 @@\n # mypy: allow-untyped-defs\n import contextlib\n \n+import logging\n+\n import torch\n import torch._subclasses.functional_tensor\n import torch.utils._pytree as pytree\n"
                },
                {
                    "old_start": 32,
                    "old_length": 6,
                    "new_start": 34,
                    "new_length": 8,
                    "hunk": "@@ -32,6 +34,8 @@ from torch.fx.experimental.proxy_tensor import (\n from torch.fx.passes.shape_prop import _extract_tensor_metadata\n from torch.utils._python_dispatch import _get_current_dispatch_mode\n \n+log = logging.getLogger(__name__)\n+\n \n @exposed_in(\"torch\")\n def cond(pred, true_fn, false_fn, operands):\n"
                },
                {
                    "old_start": 107,
                    "old_length": 6,
                    "new_start": 110,
                    "new_length": 16,
                    "hunk": "@@ -107,6 +110,16 @@ def cond(pred, true_fn, false_fn, operands):\n     if torch.compiler.is_dynamo_compiling():\n         return cond_op(pred, true_fn, false_fn, operands)\n \n+    if isinstance(pred, (bool, int, float)):\n+        log.warning(\n+            \"Pred is a Python constant. When used with torch.cond, it executes only one of the branches.\"\n+            \" If you want torch.cond to perserve two branches, please make the predicate a boolean tensor or a SymBool.\"\n+        )\n+        if pred:\n+            return true_fn(*operands)\n+        else:\n+            return false_fn(*operands)\n+\n     def _validate_input(pred, true_fn, false_fn, operands):\n         if not isinstance(pred, (bool, torch.Tensor, torch.SymBool)):\n             raise RuntimeError(f\"Expected pred to be bool or tensor, but got {pred}.\")\n"
                },
                {
                    "old_start": 200,
                    "old_length": 7,
                    "new_start": 213,
                    "new_length": 7,
                    "hunk": "@@ -200,7 +213,7 @@ def trace_cond(proxy_mode, func_overload, pred, true_fn, false_fn, operands):\n     proxy_args = pytree.tree_map(proxy_mode.tracer.unwrap_proxy, args)\n \n     out_proxy = proxy_mode.tracer.create_proxy(\n-        \"call_function\", func_overload, proxy_args, {}, name=\"conditional\"\n+        \"call_function\", func_overload, proxy_args, {}\n     )\n \n     # At this point, we're *guaranteed* that whether an output came from the"
                }
            ],
            "whole_deleted": "-        \"call_function\", func_overload, proxy_args, {}, name=\"conditional\"\n",
            "whole_added": "+import logging\n+\n+log = logging.getLogger(__name__)\n+\n+    if isinstance(pred, (bool, int, float)):\n+        log.warning(\n+            \"Pred is a Python constant. When used with torch.cond, it executes only one of the branches.\"\n+            \" If you want torch.cond to perserve two branches, please make the predicate a boolean tensor or a SymBool.\"\n+        )\n+        if pred:\n+            return true_fn(*operands)\n+        else:\n+            return false_fn(*operands)\n+\n+        \"call_function\", func_overload, proxy_args, {}\n",
            "whole_hunk": "@@ -1,6 +1,8 @@\n # mypy: allow-untyped-defs\n import contextlib\n \n+import logging\n+\n import torch\n import torch._subclasses.functional_tensor\n import torch.utils._pytree as pytree\n@@ -32,6 +34,8 @@ from torch.fx.experimental.proxy_tensor import (\n from torch.fx.passes.shape_prop import _extract_tensor_metadata\n from torch.utils._python_dispatch import _get_current_dispatch_mode\n \n+log = logging.getLogger(__name__)\n+\n \n @exposed_in(\"torch\")\n def cond(pred, true_fn, false_fn, operands):\n@@ -107,6 +110,16 @@ def cond(pred, true_fn, false_fn, operands):\n     if torch.compiler.is_dynamo_compiling():\n         return cond_op(pred, true_fn, false_fn, operands)\n \n+    if isinstance(pred, (bool, int, float)):\n+        log.warning(\n+            \"Pred is a Python constant. When used with torch.cond, it executes only one of the branches.\"\n+            \" If you want torch.cond to perserve two branches, please make the predicate a boolean tensor or a SymBool.\"\n+        )\n+        if pred:\n+            return true_fn(*operands)\n+        else:\n+            return false_fn(*operands)\n+\n     def _validate_input(pred, true_fn, false_fn, operands):\n         if not isinstance(pred, (bool, torch.Tensor, torch.SymBool)):\n             raise RuntimeError(f\"Expected pred to be bool or tensor, but got {pred}.\")\n@@ -200,7 +213,7 @@ def trace_cond(proxy_mode, func_overload, pred, true_fn, false_fn, operands):\n     proxy_args = pytree.tree_map(proxy_mode.tracer.unwrap_proxy, args)\n \n     out_proxy = proxy_mode.tracer.create_proxy(\n-        \"call_function\", func_overload, proxy_args, {}, name=\"conditional\"\n+        \"call_function\", func_overload, proxy_args, {}\n     )\n \n     # At this point, we're *guaranteed* that whether an output came from the"
        }
    ]
},
{
    "Id": 177,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/506eda538b85c9fec7a35a4c8684cdef05fc5e9d",
    "date": "2024-05-01T22:06:47+00:00",
    "message": "Fix windows build error not propagating (#125306)\n\n* Fixes https://github.com/pytorch/pytorch/issues/124886\n* Kind of similar to https://github.com/pytorch/pytorch/pull/109393\n\nI think what happens is `exit` and `exit /b` propagate the errorlevel correctly, but `exit /b` only exists the currently running batch script and not the entire cmd.exe (or whatever program is running the batch script), so `exit /b` exits with errorlevel 1, but the the parent cmd exits with 0, and bash sees cmd's 0\n\nI think `goto fail` and `exit` are the same thing when the batch script is run from a bash script so either would work in this case?  But the `goto fail` method might be better if someone happens to run the script on cmdline\n\nI assumed that anywhere anyone was exiting after checking the error code, they did want to exit completely, and I'm pretty sure that being inside a parenthesis counts as being a different script, so I changed everything to goto fail just in case, this might be too aggressive?\n\nLogs after this change for a build failure on cuda:\nhttps://github.com/pytorch/pytorch/actions/runs/8912185834/job/24475087535?pr=125306\n```\n2 errors detected in the compilation of \"C:/actions-runner/_work/pytorch/pytorch/aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu\".\nAdaptiveMaxPooling3d.cu\n[7599/8420] Linking CXX shared library bin\\torch_cpu.dll\nninja: build stopped: subcommand failed.\n-- Building version 2.4.0a0+git3171c11\ncmake -GNinja -DBUILD_ENVIRONMENT=win-vs2019-cuda11.8-py3 -DBUILD_PYTHON=True -DBUILD_TEST=True -DBUILD_TYPE=release -DBUILD_WHEEL=1 -DCMAKE_BUILD_TYPE=Release -DCMAKE_CUDA_COMPILER=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.8/bin/nvcc.exe -DCMAKE_CUDA_COMPILER_LAUNCHER=C:/actions-runner/_work/pytorch/pytorch/build/win_tmp/bin/randomtemp.exe;C:/actions-runner/_work/pytorch/pytorch/build/win_tmp\\bin\\sccache.exe -DCMAKE_CXX_COMPILER_LAUNCHER=sccache -DCMAKE_C_COMPILER_LAUNCHER=sccache -DCMAKE_GENERATOR=Ninja -DCMAKE_INSTALL_PREFIX=C:\\actions-runner\\_work\\pytorch\\pytorch\\torch -DCMAKE_PREFIX_PATH=C:\\Jenkins\\Miniconda3\\Lib\\site-packages -DCUDA_NVCC_EXECUTABLE=C:/actions-runner/_work/pytorch/pytorch/build/win_tmp/bin/nvcc.bat -DCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\lib\\x64 -DNUMPY_INCLUDE_DIR=C:\\Jenkins\\Miniconda3\\lib\\site-packages\\numpy\\core\\include -DPYTHON_EXECUTABLE=C:\\Jenkins\\Miniconda3\\python.exe -DPYTHON_INCLUDE_DIR=C:\\Jenkins\\Miniconda3\\Include -DPYTHON_LIBRARY=C:\\Jenkins\\Miniconda3/libs/python39.lib -DTORCH_BUILD_VERSION=2.4.0a0+git3171c11 -DTORCH_CUDA_ARCH_LIST=8.6 -DUSE_CUDA=1 -DUSE_NUMPY=True C:\\actions-runner\\_work\\pytorch\\pytorch\ncmake --build . --target install --config Release -- -j 8\n\n(base) C:\\actions-runner\\_work\\pytorch\\pytorch>if errorlevel 1 goto fail\n\n(base) C:\\actions-runner\\_work\\pytorch\\pytorch>exit /b 1\nError: Process completed with exit code 1.\n```\n\nvs original\nhttps://github.com/pytorch/pytorch/actions/runs/8910674030/job/24470387612\n```\n2 errors detected in the compilation of \"C:/actions-runner/_work/pytorch/pytorch/aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu\".\nAdaptiveMaxPooling3d.cu\n[7604/8420] Linking CXX shared library bin\\torch_cpu.dll\nninja: build stopped: subcommand failed.\n-- Building version 2.4.0a0+gite09f98c\ncmake -GNinja -DBUILD_ENVIRONMENT=win-vs2019-cuda11.8-py3 -DBUILD_PYTHON=True -DBUILD_TEST=True -DBUILD_TYPE=release -DBUILD_WHEEL=1 -DCMAKE_BUILD_TYPE=Release -DCMAKE_CUDA_COMPILER=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.8/bin/nvcc.exe -DCMAKE_CUDA_COMPILER_LAUNCHER=C:/actions-runner/_work/pytorch/pytorch/build/win_tmp/bin/randomtemp.exe;C:/actions-runner/_work/pytorch/pytorch/build/win_tmp\\bin\\sccache.exe -DCMAKE_CXX_COMPILER_LAUNCHER=sccache -DCMAKE_C_COMPILER_LAUNCHER=sccache -DCMAKE_GENERATOR=Ninja -DCMAKE_INSTALL_PREFIX=C:\\actions-runner\\_work\\pytorch\\pytorch\\torch -DCMAKE_PREFIX_PATH=C:\\Jenkins\\Miniconda3\\Lib\\site-packages -DCUDA_NVCC_EXECUTABLE=C:/actions-runner/_work/pytorch/pytorch/build/win_tmp/bin/nvcc.bat -DCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\lib\\x64 -DNUMPY_INCLUDE_DIR=C:\\Jenkins\\Miniconda3\\lib\\site-packages\\numpy\\core\\include -DPYTHON_EXECUTABLE=C:\\Jenkins\\Miniconda3\\python.exe -DPYTHON_INCLUDE_DIR=C:\\Jenkins\\Miniconda3\\Include -DPYTHON_LIBRARY=C:\\Jenkins\\Miniconda3/libs/python39.lib -DTORCH_BUILD_VERSION=2.4.0a0+gite09f98c -DTORCH_CUDA_ARCH_LIST=8.6 -DUSE_CUDA=1 -DUSE_NUMPY=True C:\\actions-runner\\_work\\pytorch\\pytorch\ncmake --build . --target install --config Release -- -j 8\n\n(base) C:\\actions-runner\\_work\\pytorch\\pytorch>if errorlevel 1 exit /b\n+ assert_git_not_dirty\n+ [[ win-vs2019-cuda11.8-py3 != *rocm* ]]\n+ [[ win-vs2019-cuda11.8-py3 != *xla* ]]\n++ git status --porcelain\n++ grep -v '?? third_party'\n++ true\n+ git_status=\n+ [[ -n '' ]]\n+ echo 'BUILD PASSED'\nBUILD PASSED\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125306\nApproved by: https://github.com/ZainRizvi, https://github.com/huydhn, https://github.com/atalman",
    "label": "YES",
    "changes": [
        {
            "name": "build_pytorch.bat",
            "path": ".ci/pytorch/win-test-helpers/build_pytorch.bat",
            "patches": [
                {
                    "old_start": 17,
                    "old_length": 22,
                    "new_start": 17,
                    "new_length": 22,
                    "hunk": "@@ -17,22 +17,22 @@ set PATH=C:\\Program Files\\CMake\\bin;C:\\Program Files\\7-Zip;C:\\ProgramData\\chocol\n set INSTALLER_DIR=%SCRIPT_HELPERS_DIR%\\installation-helpers\n \n call %INSTALLER_DIR%\\install_magma.bat\n-if errorlevel 1 exit /b\n-if not errorlevel 0 exit /b\n+if errorlevel 1 goto fail\n+if not errorlevel 0 goto fail\n \n call %INSTALLER_DIR%\\install_sccache.bat\n-if errorlevel 1 exit /b\n-if not errorlevel 0 exit /b\n+if errorlevel 1 goto fail\n+if not errorlevel 0 goto fail\n \n :: Miniconda has been installed as part of the Windows AMI with all the dependencies.\n :: We just need to activate it here\n call %INSTALLER_DIR%\\activate_miniconda3.bat\n-if errorlevel 1 exit /b\n-if not errorlevel 0 exit /b\n+if errorlevel 1 goto fail\n+if not errorlevel 0 goto fail\n \n call pip install mkl-include==2021.4.0 mkl-devel==2021.4.0\n-if errorlevel 1 exit /b\n-if not errorlevel 0 exit /b\n+if errorlevel 1 goto fail\n+if not errorlevel 0 goto fail\n \n :: Override VS env here\n pushd .\n"
                },
                {
                    "old_start": 41,
                    "old_length": 8,
                    "new_start": 41,
                    "new_length": 8,
                    "hunk": "@@ -41,8 +41,8 @@ if \"%VC_VERSION%\" == \"\" (\n ) else (\n     call \"C:\\Program Files (x86)\\Microsoft Visual Studio\\%VC_YEAR%\\%VC_PRODUCT%\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64 -vcvars_ver=%VC_VERSION%\n )\n-if errorlevel 1 exit /b\n-if not errorlevel 0 exit /b\n+if errorlevel 1 goto fail\n+if not errorlevel 0 goto fail\n @echo on\n popd\n \n"
                },
                {
                    "old_start": 52,
                    "old_length": 12,
                    "new_start": 52,
                    "new_length": 12,
                    "hunk": "@@ -52,12 +52,12 @@ set CUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v%CUDA_VERSION%\n \n if x%CUDA_VERSION:.=%==x%CUDA_VERSION% (\n     echo CUDA version %CUDA_VERSION% format isn't correct, which doesn't contain '.'\n-    exit /b 1\n+    goto fail\n )\n rem version transformer, for example 10.1 to 10_1.\n if x%CUDA_VERSION:.=%==x%CUDA_VERSION% (\n     echo CUDA version %CUDA_VERSION% format isn't correct, which doesn't contain '.'\n-    exit /b 1\n+    goto fail\n )\n set VERSION_SUFFIX=%CUDA_VERSION:.=_%\n set CUDA_PATH_V%VERSION_SUFFIX%=%CUDA_PATH%\n"
                },
                {
                    "old_start": 101,
                    "old_length": 8,
                    "new_start": 101,
                    "new_length": 8,
                    "hunk": "@@ -101,8 +101,8 @@ if \"%USE_CUDA%\"==\"1\" (\n   :: CMake requires a single command as CUDA_NVCC_EXECUTABLE, so we push the wrappers\n   :: randomtemp.exe and sccache.exe into a batch file which CMake invokes.\n   curl -kL https://github.com/peterjc123/randomtemp-rust/releases/download/v0.4/randomtemp.exe --output %TMP_DIR_WIN%\\bin\\randomtemp.exe\n-  if errorlevel 1 exit /b\n-  if not errorlevel 0 exit /b\n+  if errorlevel 1 goto fail\n+  if not errorlevel 0 goto fail\n   echo @\"%TMP_DIR_WIN%\\bin\\randomtemp.exe\" \"%TMP_DIR_WIN%\\bin\\sccache.exe\" \"%CUDA_PATH%\\bin\\nvcc.exe\" %%* > \"%TMP_DIR%/bin/nvcc.bat\"\n   cat %TMP_DIR%/bin/nvcc.bat\n   set CUDA_NVCC_EXECUTABLE=%TMP_DIR%/bin/nvcc.bat\n"
                },
                {
                    "old_start": 114,
                    "old_length": 8,
                    "new_start": 114,
                    "new_length": 8,
                    "hunk": "@@ -114,8 +114,8 @@ if \"%USE_CUDA%\"==\"1\" (\n set\n \n python setup.py bdist_wheel\n-if errorlevel 1 exit /b\n-if not errorlevel 0 exit /b\n+if errorlevel 1 goto fail\n+if not errorlevel 0 goto fail\n sccache --show-stats\n python -c \"import os, glob; os.system('python -mpip install --no-index --no-deps ' + glob.glob('dist/*.whl')[0])\"\n (\n"
                },
                {
                    "old_start": 135,
                    "old_length": 3,
                    "new_start": 135,
                    "new_length": 8,
                    "hunk": "@@ -135,3 +135,8 @@ python -c \"import os, glob; os.system('python -mpip install --no-index --no-deps\n \n sccache --show-stats --stats-format json | jq .stats > sccache-stats-%BUILD_ENVIRONMENT%-%OUR_GITHUB_JOB_ID%.json\n sccache --stop-server\n+\n+exit /b 0\n+\n+:fail\n+exit /b 1"
                }
            ],
            "whole_deleted": "-if errorlevel 1 exit /b\n-if not errorlevel 0 exit /b\n-if errorlevel 1 exit /b\n-if not errorlevel 0 exit /b\n-if errorlevel 1 exit /b\n-if not errorlevel 0 exit /b\n-if errorlevel 1 exit /b\n-if not errorlevel 0 exit /b\n-if errorlevel 1 exit /b\n-if not errorlevel 0 exit /b\n-    exit /b 1\n-    exit /b 1\n-  if errorlevel 1 exit /b\n-  if not errorlevel 0 exit /b\n-if errorlevel 1 exit /b\n-if not errorlevel 0 exit /b\n",
            "whole_added": "+if errorlevel 1 goto fail\n+if not errorlevel 0 goto fail\n+if errorlevel 1 goto fail\n+if not errorlevel 0 goto fail\n+if errorlevel 1 goto fail\n+if not errorlevel 0 goto fail\n+if errorlevel 1 goto fail\n+if not errorlevel 0 goto fail\n+if errorlevel 1 goto fail\n+if not errorlevel 0 goto fail\n+    goto fail\n+    goto fail\n+  if errorlevel 1 goto fail\n+  if not errorlevel 0 goto fail\n+if errorlevel 1 goto fail\n+if not errorlevel 0 goto fail\n+\n+exit /b 0\n+\n+:fail\n+exit /b 1\n",
            "whole_hunk": "@@ -17,22 +17,22 @@ set PATH=C:\\Program Files\\CMake\\bin;C:\\Program Files\\7-Zip;C:\\ProgramData\\chocol\n set INSTALLER_DIR=%SCRIPT_HELPERS_DIR%\\installation-helpers\n \n call %INSTALLER_DIR%\\install_magma.bat\n-if errorlevel 1 exit /b\n-if not errorlevel 0 exit /b\n+if errorlevel 1 goto fail\n+if not errorlevel 0 goto fail\n \n call %INSTALLER_DIR%\\install_sccache.bat\n-if errorlevel 1 exit /b\n-if not errorlevel 0 exit /b\n+if errorlevel 1 goto fail\n+if not errorlevel 0 goto fail\n \n :: Miniconda has been installed as part of the Windows AMI with all the dependencies.\n :: We just need to activate it here\n call %INSTALLER_DIR%\\activate_miniconda3.bat\n-if errorlevel 1 exit /b\n-if not errorlevel 0 exit /b\n+if errorlevel 1 goto fail\n+if not errorlevel 0 goto fail\n \n call pip install mkl-include==2021.4.0 mkl-devel==2021.4.0\n-if errorlevel 1 exit /b\n-if not errorlevel 0 exit /b\n+if errorlevel 1 goto fail\n+if not errorlevel 0 goto fail\n \n :: Override VS env here\n pushd .\n@@ -41,8 +41,8 @@ if \"%VC_VERSION%\" == \"\" (\n ) else (\n     call \"C:\\Program Files (x86)\\Microsoft Visual Studio\\%VC_YEAR%\\%VC_PRODUCT%\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64 -vcvars_ver=%VC_VERSION%\n )\n-if errorlevel 1 exit /b\n-if not errorlevel 0 exit /b\n+if errorlevel 1 goto fail\n+if not errorlevel 0 goto fail\n @echo on\n popd\n \n@@ -52,12 +52,12 @@ set CUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v%CUDA_VERSION%\n \n if x%CUDA_VERSION:.=%==x%CUDA_VERSION% (\n     echo CUDA version %CUDA_VERSION% format isn't correct, which doesn't contain '.'\n-    exit /b 1\n+    goto fail\n )\n rem version transformer, for example 10.1 to 10_1.\n if x%CUDA_VERSION:.=%==x%CUDA_VERSION% (\n     echo CUDA version %CUDA_VERSION% format isn't correct, which doesn't contain '.'\n-    exit /b 1\n+    goto fail\n )\n set VERSION_SUFFIX=%CUDA_VERSION:.=_%\n set CUDA_PATH_V%VERSION_SUFFIX%=%CUDA_PATH%\n@@ -101,8 +101,8 @@ if \"%USE_CUDA%\"==\"1\" (\n   :: CMake requires a single command as CUDA_NVCC_EXECUTABLE, so we push the wrappers\n   :: randomtemp.exe and sccache.exe into a batch file which CMake invokes.\n   curl -kL https://github.com/peterjc123/randomtemp-rust/releases/download/v0.4/randomtemp.exe --output %TMP_DIR_WIN%\\bin\\randomtemp.exe\n-  if errorlevel 1 exit /b\n-  if not errorlevel 0 exit /b\n+  if errorlevel 1 goto fail\n+  if not errorlevel 0 goto fail\n   echo @\"%TMP_DIR_WIN%\\bin\\randomtemp.exe\" \"%TMP_DIR_WIN%\\bin\\sccache.exe\" \"%CUDA_PATH%\\bin\\nvcc.exe\" %%* > \"%TMP_DIR%/bin/nvcc.bat\"\n   cat %TMP_DIR%/bin/nvcc.bat\n   set CUDA_NVCC_EXECUTABLE=%TMP_DIR%/bin/nvcc.bat\n@@ -114,8 +114,8 @@ if \"%USE_CUDA%\"==\"1\" (\n set\n \n python setup.py bdist_wheel\n-if errorlevel 1 exit /b\n-if not errorlevel 0 exit /b\n+if errorlevel 1 goto fail\n+if not errorlevel 0 goto fail\n sccache --show-stats\n python -c \"import os, glob; os.system('python -mpip install --no-index --no-deps ' + glob.glob('dist/*.whl')[0])\"\n (\n@@ -135,3 +135,8 @@ python -c \"import os, glob; os.system('python -mpip install --no-index --no-deps\n \n sccache --show-stats --stats-format json | jq .stats > sccache-stats-%BUILD_ENVIRONMENT%-%OUR_GITHUB_JOB_ID%.json\n sccache --stop-server\n+\n+exit /b 0\n+\n+:fail\n+exit /b 1"
        }
    ]
},
{
    "Id": 104,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/fba21edf5b9aa14babb9c0bc860dc9c597eb8010",
    "date": "2024-06-06T18:23:52+00:00",
    "message": "[CI] Ensure inductor/test_cpu_cpp_wrapper is actually run in inductor_cpp_wrapper_abi_compatible (#126717)\n\n`inductor/test_cpu_cpp_wrapper` is not actually being run in `inductor_cpp_wrapper_abi_compatible` test config\n\nThe cpu device type gets removed in https://github.com/pytorch/pytorch/blob/d28868c7e8bcd41c9219f099aa5f7a5332c912fd/torch/testing/_internal/common_device_type.py#L733\n\nso https://github.com/pytorch/pytorch/blob/d28868c7e8bcd41c9219f099aa5f7a5332c912fd/test/inductor/test_cpu_cpp_wrapper.py#L396 returns false.\n\nFeel free to make a PR with a different way to do this (a better RUN_CPU check?)\n\nAdd a skip for a failing test.  I am not equipped to fix it\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/126717\nApproved by: https://github.com/ZainRizvi",
    "label": "YES",
    "changes": [
        {
            "name": "test.sh",
            "path": ".ci/pytorch/test.sh",
            "patches": [
                {
                    "old_start": 368,
                    "old_length": 7,
                    "new_start": 368,
                    "new_length": 7,
                    "hunk": "@@ -368,7 +368,7 @@ test_inductor_cpp_wrapper_abi_compatible() {\n \n   echo \"Testing Inductor cpp wrapper mode with TORCHINDUCTOR_ABI_COMPATIBLE=1\"\n   # cpu stack allocation causes segfault and needs more investigation\n-  python test/run_test.py --include inductor/test_cpu_cpp_wrapper\n+  PYTORCH_TESTING_DEVICE_ONLY_FOR=\"\" python test/run_test.py --include inductor/test_cpu_cpp_wrapper\n   python test/run_test.py --include inductor/test_cuda_cpp_wrapper\n \n   TORCHINDUCTOR_CPP_WRAPPER=1 python benchmarks/dynamo/timm_models.py --device cuda --accuracy --amp \\\n"
                }
            ],
            "whole_deleted": "-  python test/run_test.py --include inductor/test_cpu_cpp_wrapper\n",
            "whole_added": "+  PYTORCH_TESTING_DEVICE_ONLY_FOR=\"\" python test/run_test.py --include inductor/test_cpu_cpp_wrapper\n",
            "whole_hunk": "@@ -368,7 +368,7 @@ test_inductor_cpp_wrapper_abi_compatible() {\n \n   echo \"Testing Inductor cpp wrapper mode with TORCHINDUCTOR_ABI_COMPATIBLE=1\"\n   # cpu stack allocation causes segfault and needs more investigation\n-  python test/run_test.py --include inductor/test_cpu_cpp_wrapper\n+  PYTORCH_TESTING_DEVICE_ONLY_FOR=\"\" python test/run_test.py --include inductor/test_cpu_cpp_wrapper\n   python test/run_test.py --include inductor/test_cuda_cpp_wrapper\n \n   TORCHINDUCTOR_CPP_WRAPPER=1 python benchmarks/dynamo/timm_models.py --device cuda --accuracy --amp \\\n"
        },
        {
            "name": "test_cpu_cpp_wrapper.py",
            "path": "test/inductor/test_cpu_cpp_wrapper.py",
            "patches": [
                {
                    "old_start": 115,
                    "old_length": 6,
                    "new_start": 115,
                    "new_length": 7,
                    "hunk": "@@ -115,6 +115,7 @@ def make_test_case(\n     slow=False,\n     func_inputs=None,\n     code_string_count=None,\n+    skip=None,\n ):\n     test_name = f\"{name}_{device}\" if device else name\n     if code_string_count is None:\n"
                },
                {
                    "old_start": 123,
                    "old_length": 6,
                    "new_start": 124,
                    "new_length": 8,
                    "hunk": "@@ -123,6 +124,8 @@ def make_test_case(\n     func = getattr(tests, test_name)\n     assert callable(func), \"not a callable\"\n     func = slowTest(func) if slow else func\n+    if skip:\n+        func = unittest.skip(skip)(func)\n \n     @config.patch(cpp_wrapper=True, search_autotune_cache=False)\n     def fn(self):\n"
                },
                {
                    "old_start": 170,
                    "old_length": 6,
                    "new_start": 173,
                    "new_length": 7,
                    "hunk": "@@ -170,6 +173,7 @@ if RUN_CPU:\n         slow: bool = False\n         func_inputs: list = None\n         code_string_count: dict = {}\n+        skip: str = None\n \n     for item in [\n         BaseTest(\"test_add_complex\"),\n"
                },
                {
                    "old_start": 228,
                    "old_length": 7,
                    "new_start": 232,
                    "new_length": 9,
                    "hunk": "@@ -228,7 +232,9 @@ if RUN_CPU:\n             torch.backends.mkldnn.is_available()\n             and torch.ops.mkldnn._is_mkldnn_bf16_supported(),\n         ),\n-        BaseTest(\"test_linear_packed\", \"\", test_cpu_repro.CPUReproTests()),\n+        BaseTest(\n+            \"test_linear_packed\", \"\", test_cpu_repro.CPUReproTests(), skip=\"Failing\"\n+        ),\n         BaseTest(\n             \"test_lstm_packed_change_input_sizes\",\n             \"cpu\",\n"
                },
                {
                    "old_start": 302,
                    "old_length": 18,
                    "new_start": 308,
                    "new_length": 21,
                    "hunk": "@@ -302,18 +308,21 @@ if RUN_CPU:\n             \"cpu\",\n             test_mkldnn_pattern_matcher.TestPatternMatcher(),\n             condition=torch.backends.mkldnn.is_available(),\n+            skip=\"Failing\",\n         ),\n         BaseTest(\n             \"test_qlinear_add\",\n             \"cpu\",\n             test_mkldnn_pattern_matcher.TestPatternMatcher(),\n             condition=torch.backends.mkldnn.is_available(),\n+            skip=\"Failing\",\n         ),\n         BaseTest(\n             \"test_qlinear_add_relu\",\n             \"cpu\",\n             test_mkldnn_pattern_matcher.TestPatternMatcher(),\n             condition=torch.backends.mkldnn.is_available(),\n+            skip=\"Failing\",\n         ),\n         BaseTest(\n             \"test_qlinear_dequant_promotion\",\n"
                },
                {
                    "old_start": 369,
                    "old_length": 6,
                    "new_start": 378,
                    "new_length": 7,
                    "hunk": "@@ -369,6 +378,7 @@ if RUN_CPU:\n             item.slow,\n             item.func_inputs,\n             item.code_string_count,\n+            skip=item.skip,\n         )\n \n     test_torchinductor.copy_tests("
                }
            ],
            "whole_deleted": "-        BaseTest(\"test_linear_packed\", \"\", test_cpu_repro.CPUReproTests()),\n",
            "whole_added": "+    skip=None,\n+    if skip:\n+        func = unittest.skip(skip)(func)\n+        skip: str = None\n+        BaseTest(\n+            \"test_linear_packed\", \"\", test_cpu_repro.CPUReproTests(), skip=\"Failing\"\n+        ),\n+            skip=\"Failing\",\n+            skip=\"Failing\",\n+            skip=\"Failing\",\n+            skip=item.skip,\n",
            "whole_hunk": "@@ -115,6 +115,7 @@ def make_test_case(\n     slow=False,\n     func_inputs=None,\n     code_string_count=None,\n+    skip=None,\n ):\n     test_name = f\"{name}_{device}\" if device else name\n     if code_string_count is None:\n@@ -123,6 +124,8 @@ def make_test_case(\n     func = getattr(tests, test_name)\n     assert callable(func), \"not a callable\"\n     func = slowTest(func) if slow else func\n+    if skip:\n+        func = unittest.skip(skip)(func)\n \n     @config.patch(cpp_wrapper=True, search_autotune_cache=False)\n     def fn(self):\n@@ -170,6 +173,7 @@ if RUN_CPU:\n         slow: bool = False\n         func_inputs: list = None\n         code_string_count: dict = {}\n+        skip: str = None\n \n     for item in [\n         BaseTest(\"test_add_complex\"),\n@@ -228,7 +232,9 @@ if RUN_CPU:\n             torch.backends.mkldnn.is_available()\n             and torch.ops.mkldnn._is_mkldnn_bf16_supported(),\n         ),\n-        BaseTest(\"test_linear_packed\", \"\", test_cpu_repro.CPUReproTests()),\n+        BaseTest(\n+            \"test_linear_packed\", \"\", test_cpu_repro.CPUReproTests(), skip=\"Failing\"\n+        ),\n         BaseTest(\n             \"test_lstm_packed_change_input_sizes\",\n             \"cpu\",\n@@ -302,18 +308,21 @@ if RUN_CPU:\n             \"cpu\",\n             test_mkldnn_pattern_matcher.TestPatternMatcher(),\n             condition=torch.backends.mkldnn.is_available(),\n+            skip=\"Failing\",\n         ),\n         BaseTest(\n             \"test_qlinear_add\",\n             \"cpu\",\n             test_mkldnn_pattern_matcher.TestPatternMatcher(),\n             condition=torch.backends.mkldnn.is_available(),\n+            skip=\"Failing\",\n         ),\n         BaseTest(\n             \"test_qlinear_add_relu\",\n             \"cpu\",\n             test_mkldnn_pattern_matcher.TestPatternMatcher(),\n             condition=torch.backends.mkldnn.is_available(),\n+            skip=\"Failing\",\n         ),\n         BaseTest(\n             \"test_qlinear_dequant_promotion\",\n@@ -369,6 +378,7 @@ if RUN_CPU:\n             item.slow,\n             item.func_inputs,\n             item.code_string_count,\n+            skip=item.skip,\n         )\n \n     test_torchinductor.copy_tests("
        }
    ]
},
{
    "Id": 145,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/490d72e4e6eb07ed7c65ee6f7c48bb393483c01c",
    "date": "2024-05-15T17:18:22+00:00",
    "message": "CMake: Improve check and report of Magma (#117858)\n\n- Only search for magma if it is used (GPU builds)\n- Don't report it was not found when it isn't searched for\n- Don't report if magma is disabled (currently: \"MAGMA not found. Compiling without MAGMA support\" is reported)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/117858\nApproved by: https://github.com/malfet",
    "label": "NO",
    "changes": [
        {
            "name": "Dependencies.cmake",
            "path": "cmake/Dependencies.cmake",
            "patches": [
                {
                    "old_start": 1597,
                    "old_length": 23,
                    "new_start": 1597,
                    "new_length": 24,
                    "hunk": "@@ -1597,23 +1597,24 @@ if(NOT INTERN_BUILD_MOBILE)\n \n   set(CUDA_ATTACH_VS_BUILD_RULE_TO_CUDA_FILE OFF)\n \n-  if(USE_MAGMA)\n-    find_package(MAGMA)\n-  endif()\n-  if((USE_CUDA OR USE_ROCM) AND MAGMA_FOUND)\n-    set(USE_MAGMA 1)\n-    message(STATUS \"Compiling with MAGMA support\")\n-    message(STATUS \"MAGMA INCLUDE DIRECTORIES: ${MAGMA_INCLUDE_DIR}\")\n-    message(STATUS \"MAGMA LIBRARIES: ${MAGMA_LIBRARIES}\")\n-    message(STATUS \"MAGMA V2 check: ${MAGMA_V2}\")\n+  if(USE_CUDA OR USE_ROCM)\n+    if(USE_MAGMA)\n+      find_package(MAGMA)\n+      if(MAGMA_FOUND)\n+        message(STATUS \"Compiling with MAGMA support\")\n+        message(STATUS \"MAGMA INCLUDE DIRECTORIES: ${MAGMA_INCLUDE_DIR}\")\n+        message(STATUS \"MAGMA LIBRARIES: ${MAGMA_LIBRARIES}\")\n+        message(STATUS \"MAGMA V2 check: ${MAGMA_V2}\")\n+      else()\n+        message(STATUS \"MAGMA not found. Compiling without MAGMA support\")\n+        caffe2_update_option(USE_MAGMA OFF)\n+      endif()\n+    endif()\n   elseif(USE_MAGMA)\n     message(WARNING\n       \"Not compiling with MAGMA. Suppress this warning with \"\n       \"-DUSE_MAGMA=OFF.\")\n     caffe2_update_option(USE_MAGMA OFF)\n-  else()\n-    message(STATUS \"MAGMA not found. Compiling without MAGMA support\")\n-    caffe2_update_option(USE_MAGMA OFF)\n   endif()\n \n   # ARM specific flags"
                }
            ],
            "whole_deleted": "-  if(USE_MAGMA)\n-    find_package(MAGMA)\n-  endif()\n-  if((USE_CUDA OR USE_ROCM) AND MAGMA_FOUND)\n-    set(USE_MAGMA 1)\n-    message(STATUS \"Compiling with MAGMA support\")\n-    message(STATUS \"MAGMA INCLUDE DIRECTORIES: ${MAGMA_INCLUDE_DIR}\")\n-    message(STATUS \"MAGMA LIBRARIES: ${MAGMA_LIBRARIES}\")\n-    message(STATUS \"MAGMA V2 check: ${MAGMA_V2}\")\n-  else()\n-    message(STATUS \"MAGMA not found. Compiling without MAGMA support\")\n-    caffe2_update_option(USE_MAGMA OFF)\n",
            "whole_added": "+  if(USE_CUDA OR USE_ROCM)\n+    if(USE_MAGMA)\n+      find_package(MAGMA)\n+      if(MAGMA_FOUND)\n+        message(STATUS \"Compiling with MAGMA support\")\n+        message(STATUS \"MAGMA INCLUDE DIRECTORIES: ${MAGMA_INCLUDE_DIR}\")\n+        message(STATUS \"MAGMA LIBRARIES: ${MAGMA_LIBRARIES}\")\n+        message(STATUS \"MAGMA V2 check: ${MAGMA_V2}\")\n+      else()\n+        message(STATUS \"MAGMA not found. Compiling without MAGMA support\")\n+        caffe2_update_option(USE_MAGMA OFF)\n+      endif()\n+    endif()\n",
            "whole_hunk": "@@ -1597,23 +1597,24 @@ if(NOT INTERN_BUILD_MOBILE)\n \n   set(CUDA_ATTACH_VS_BUILD_RULE_TO_CUDA_FILE OFF)\n \n-  if(USE_MAGMA)\n-    find_package(MAGMA)\n-  endif()\n-  if((USE_CUDA OR USE_ROCM) AND MAGMA_FOUND)\n-    set(USE_MAGMA 1)\n-    message(STATUS \"Compiling with MAGMA support\")\n-    message(STATUS \"MAGMA INCLUDE DIRECTORIES: ${MAGMA_INCLUDE_DIR}\")\n-    message(STATUS \"MAGMA LIBRARIES: ${MAGMA_LIBRARIES}\")\n-    message(STATUS \"MAGMA V2 check: ${MAGMA_V2}\")\n+  if(USE_CUDA OR USE_ROCM)\n+    if(USE_MAGMA)\n+      find_package(MAGMA)\n+      if(MAGMA_FOUND)\n+        message(STATUS \"Compiling with MAGMA support\")\n+        message(STATUS \"MAGMA INCLUDE DIRECTORIES: ${MAGMA_INCLUDE_DIR}\")\n+        message(STATUS \"MAGMA LIBRARIES: ${MAGMA_LIBRARIES}\")\n+        message(STATUS \"MAGMA V2 check: ${MAGMA_V2}\")\n+      else()\n+        message(STATUS \"MAGMA not found. Compiling without MAGMA support\")\n+        caffe2_update_option(USE_MAGMA OFF)\n+      endif()\n+    endif()\n   elseif(USE_MAGMA)\n     message(WARNING\n       \"Not compiling with MAGMA. Suppress this warning with \"\n       \"-DUSE_MAGMA=OFF.\")\n     caffe2_update_option(USE_MAGMA OFF)\n-  else()\n-    message(STATUS \"MAGMA not found. Compiling without MAGMA support\")\n-    caffe2_update_option(USE_MAGMA OFF)\n   endif()\n \n   # ARM specific flags"
        }
    ]
},
{
    "Id": 384,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/937d616e825e70b8d786c1f514ae9cec9c8d4ee9",
    "date": "2023-12-09T11:07:54+00:00",
    "message": "Re-enable type checking for distributed_c10d.py (#115223)\n\nRe-enable type checking for distributed_c10d.py\n\nType checking for distributed_c10d.py was inadvertently turned off in issues that have accumulated since.\n\nNote: the backwards compatibility linter does not like some of these changes.  But they were incorrect before.  This needs human verification, however.\n\n#suppress-api-compatibility-check\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115223\nApproved by: https://github.com/wconstab",
    "label": "YES",
    "changes": [
        {
            "name": ".lintrunner.toml",
            "path": ".lintrunner.toml",
            "patches": [
                {
                    "old_start": 134,
                    "old_length": 7,
                    "new_start": 134,
                    "new_length": 6,
                    "hunk": "@@ -134,7 +134,6 @@ exclude_patterns = [\n     'torch/distributed/elastic/agent/server/api.py',\n     'torch/testing/_internal/**',\n     'torch/distributed/fsdp/fully_sharded_data_parallel.py',\n-    'torch/distributed/distributed_c10d.py',\n     # TODO(suo): these exclusions were added just to get lint clean on master.\n     # Follow up to do more target suppressions and remove them.\n     'torch/ao/quantization/fx/convert.py',\n"
                }
            ],
            "whole_deleted": "-    'torch/distributed/distributed_c10d.py',\n",
            "whole_added": "",
            "whole_hunk": "@@ -134,7 +134,6 @@ exclude_patterns = [\n     'torch/distributed/elastic/agent/server/api.py',\n     'torch/testing/_internal/**',\n     'torch/distributed/fsdp/fully_sharded_data_parallel.py',\n-    'torch/distributed/distributed_c10d.py',\n     # TODO(suo): these exclusions were added just to get lint clean on master.\n     # Follow up to do more target suppressions and remove them.\n     'torch/ao/quantization/fx/convert.py',\n"
        },
        {
            "name": "_distributed_c10d.pyi",
            "path": "torch/_C/_distributed_c10d.pyi",
            "patches": [
                {
                    "old_start": 3,
                    "old_length": 6,
                    "new_start": 3,
                    "new_length": 7,
                    "hunk": "@@ -3,6 +3,7 @@ from datetime import timedelta\n from enum import Enum\n from typing import Any, Dict, List, Optional, overload, Tuple, Union\n \n+import torch\n from torch import Tensor\n from torch._C import ScriptObject\n from torch.futures import Future\n"
                },
                {
                    "old_start": 23,
                    "old_length": 6,
                    "new_start": 24,
                    "new_length": 7,
                    "hunk": "@@ -23,6 +24,7 @@ def _register_builtin_comm_hook(\n     reducer: Reducer,\n     comm_hook_type: BuiltinCommHookType,\n ): ...\n+def _set_global_rank(rank: int) -> None: ...\n \n class GradBucket:\n     def index(self) -> int: ...\n"
                },
                {
                    "old_start": 154,
                    "old_length": 6,
                    "new_start": 156,
                    "new_length": 7,
                    "hunk": "@@ -154,6 +156,7 @@ class ReduceScatterOptions:\n \n class BarrierOptions:\n     device_ids: List[int]\n+    device: torch.device\n     timeout: timedelta\n \n class AllToAllOptions:\n"
                },
                {
                    "old_start": 206,
                    "old_length": 11,
                    "new_start": 209,
                    "new_length": 39,
                    "hunk": "@@ -206,11 +209,39 @@ class PrefixStore(Store):\n     @property\n     def underlying_store(self) -> Store: ...\n \n+class _DistributedBackendOptions:\n+    def __init__(self): ...\n+    @property\n+    def store(self) -> Store: ...\n+    @store.setter\n+    def store(self, store: Store) -> None: ...\n+    @property\n+    def group_rank(self) -> int: ...\n+    @group_rank.setter\n+    def group_rank(self, rank: int) -> None: ...\n+    @property\n+    def group_size(self) -> int: ...\n+    @group_size.setter\n+    def group_size(self, size: int) -> None: ...\n+    @property\n+    def timeout(self) -> timedelta: ...\n+    @timeout.setter\n+    def timeout(self, timeout: timedelta) -> None: ...\n+    @property\n+    def group_id(self) -> str: ...\n+    @group_id.setter\n+    def group_id(self, group_id: str) -> None: ...\n+    @property\n+    def global_ranks_in_group(self) -> List[int]: ...\n+    @global_ranks_in_group.setter\n+    def global_ranks_in_group(self, ranks: List[int]) -> None: ...\n+\n class Work:\n     def is_completed(self) -> bool: ...\n     def is_success(self) -> bool: ...\n     def exception(self) -> Any: ...\n     def wait(self, timeout: timedelta = ...) -> bool: ...\n+    def get_future(self) -> Future: ...\n     def source_rank(self) -> int: ...\n     def _source_rank(self) -> int: ...\n     def result(self) -> List[Tensor]: ...\n"
                },
                {
                    "old_start": 220,
                    "old_length": 9,
                    "new_start": 251,
                    "new_length": 23,
                    "hunk": "@@ -220,9 +251,23 @@ class Work:\n     def unbox(obj: ScriptObject) -> Work: ...\n \n class ProcessGroup:\n-    class Options: ...\n-\n-    def __init__(self): ...\n+    class Options:\n+        def __init__(self, backend: str, timeout: timedelta = ...): ...\n+        @property\n+        def backend(self) -> str: ...\n+        @property\n+        def _timeout(self) -> timedelta: ...\n+        @timeout.setter\n+        def _timeout(self, val: timedelta) -> None: ...\n+\n+    class BackendType(Enum):\n+        UNDEFINED = ...\n+        GLOO = ...\n+        NCCL = ...\n+        UCC = ...\n+        MPI = ...\n+        CUSTOM = ...\n+    def __init__(self, store: Store, rank: int, size: int, options: Options): ...\n     def rank(self) -> int: ...\n     def size(self) -> int: ...\n     @overload\n"
                },
                {
                    "old_start": 260,
                    "old_length": 6,
                    "new_start": 305,
                    "new_length": 12,
                    "hunk": "@@ -260,6 +305,12 @@ class ProcessGroup:\n         tensors: List[Tensor],\n         opts=...,\n     ) -> Work: ...\n+    def reduce_scatter_tensor_coalesced(\n+        self,\n+        outputTensors: List[Tensor],\n+        inputTensors: List[Tensor],\n+        opts: Optional[ReduceScatterOptions] = None,\n+    ) -> Work: ...\n     @overload\n     def reduce(\n         self,\n"
                },
                {
                    "old_start": 298,
                    "old_length": 6,
                    "new_start": 349,
                    "new_length": 12,
                    "hunk": "@@ -298,6 +349,12 @@ class ProcessGroup:\n         input_list: List[Tensor],\n         opts=...,\n     ) -> Work: ...\n+    def allgather_into_tensor_coalesced(\n+        self,\n+        output_lists: List[Tensor],\n+        input_list: List[Tensor],\n+        opts=...,\n+    ) -> Work: ...\n     @overload\n     def gather(\n         self,\n"
                },
                {
                    "old_start": 343,
                    "old_length": 6,
                    "new_start": 400,
                    "new_length": 7,
                    "hunk": "@@ -343,6 +400,7 @@ class ProcessGroup:\n         self,\n         outputTensor: Tensor,\n         inputTensor: Tensor,\n+        opts: Optional[ReduceScatterOptions],\n     ) -> Work: ...\n     @overload\n     def alltoall_base(\n"
                },
                {
                    "old_start": 391,
                    "old_length": 6,
                    "new_start": 449,
                    "new_length": 28,
                    "hunk": "@@ -391,6 +449,28 @@ class ProcessGroup:\n     def boxed(self) -> ScriptObject: ...\n     @staticmethod\n     def unbox(obj: ScriptObject) -> ProcessGroup: ...\n+    def _start_coalescing(self, device: torch.device) -> None: ...\n+    def _end_coalescing(self, device: torch.device) -> Work: ...\n+    def _get_backend_name(self) -> str: ...\n+    def _backend_id(self, backend_type: BackendType) -> int: ...\n+    @property\n+    def _device_types(self) -> List[torch.device]: ...\n+    def _get_backend(self, device: torch.device) -> Backend: ...\n+    def _register_backend(\n+        self,\n+        device: torch.device,\n+        backend_type: BackendType,\n+        backend: Optional[ProcessGroup],\n+    ) -> None: ...\n+    def _set_group_name(self, name: str) -> None: ...\n+    def name(self) -> str: ...\n+    def _has_hooks(self) -> bool: ...\n+    def _wait_for_pending_works(self) -> None: ...\n+    def _set_sequence_number_for_group(self) -> None: ...\n+    @property\n+    def bound_device_id(self) -> Optional[torch.device]: ...\n+    @bound_device_id.setter\n+    def bound_device_id(self, device: Optional[torch.device]) -> None: ...\n \n class ProcessGroupRoundRobin(ProcessGroup): ...\n \n"
                },
                {
                    "old_start": 416,
                    "old_length": 10,
                    "new_start": 496,
                    "new_length": 21,
                    "hunk": "@@ -416,10 +496,21 @@ class ProcessGroupGloo(ProcessGroup):\n \n class _ProcessGroupWrapper(ProcessGroup):\n     def __init__(self, pg: ProcessGroup, gloo_pg: ProcessGroupGloo): ...\n-    wrapped_pg: ProcessGroup\n+    wrapped_pg: Backend\n \n class ProcessGroupNCCL(ProcessGroup):\n-    class Options: ...\n+    class Options:\n+        def __init__(self, timeout: Optional[timedelta] = None): ...\n+        @property\n+        def backend(self) -> str: ...\n+        @property\n+        def _timeout(self) -> timedelta: ...\n+        @timeout.setter\n+        def _timeout(self, val: timedelta) -> None: ...\n+        @property\n+        def _is_high_priority_stream(self) -> bool: ...\n+        @is_high_priority_stream.setter\n+        def _is_high_priority_stream(self, val: bool) -> None: ...\n \n     def __init__(\n         self,\n"
                },
                {
                    "old_start": 476,
                    "old_length": 3,
                    "new_start": 567,
                    "new_length": 6,
                    "hunk": "@@ -476,3 +567,6 @@ class Backend:\n         rank: int,\n         size: int,\n     ): ...\n+    @property\n+    def supports_splitting(self) -> bool: ...\n+    def eager_connect_single_device(self, device: Optional[torch.device]) -> None: ...\n"
                }
            ],
            "whole_deleted": "-    class Options: ...\n-\n-    def __init__(self): ...\n-    wrapped_pg: ProcessGroup\n-    class Options: ...\n",
            "whole_added": "+import torch\n+def _set_global_rank(rank: int) -> None: ...\n+    device: torch.device\n+class _DistributedBackendOptions:\n+    def __init__(self): ...\n+    @property\n+    def store(self) -> Store: ...\n+    @store.setter\n+    def store(self, store: Store) -> None: ...\n+    @property\n+    def group_rank(self) -> int: ...\n+    @group_rank.setter\n+    def group_rank(self, rank: int) -> None: ...\n+    @property\n+    def group_size(self) -> int: ...\n+    @group_size.setter\n+    def group_size(self, size: int) -> None: ...\n+    @property\n+    def timeout(self) -> timedelta: ...\n+    @timeout.setter\n+    def timeout(self, timeout: timedelta) -> None: ...\n+    @property\n+    def group_id(self) -> str: ...\n+    @group_id.setter\n+    def group_id(self, group_id: str) -> None: ...\n+    @property\n+    def global_ranks_in_group(self) -> List[int]: ...\n+    @global_ranks_in_group.setter\n+    def global_ranks_in_group(self, ranks: List[int]) -> None: ...\n+\n+    def get_future(self) -> Future: ...\n+    class Options:\n+        def __init__(self, backend: str, timeout: timedelta = ...): ...\n+        @property\n+        def backend(self) -> str: ...\n+        @property\n+        def _timeout(self) -> timedelta: ...\n+        @timeout.setter\n+        def _timeout(self, val: timedelta) -> None: ...\n+\n+    class BackendType(Enum):\n+        UNDEFINED = ...\n+        GLOO = ...\n+        NCCL = ...\n+        UCC = ...\n+        MPI = ...\n+        CUSTOM = ...\n+    def __init__(self, store: Store, rank: int, size: int, options: Options): ...\n+    def reduce_scatter_tensor_coalesced(\n+        self,\n+        outputTensors: List[Tensor],\n+        inputTensors: List[Tensor],\n+        opts: Optional[ReduceScatterOptions] = None,\n+    ) -> Work: ...\n+    def allgather_into_tensor_coalesced(\n+        self,\n+        output_lists: List[Tensor],\n+        input_list: List[Tensor],\n+        opts=...,\n+    ) -> Work: ...\n+        opts: Optional[ReduceScatterOptions],\n+    def _start_coalescing(self, device: torch.device) -> None: ...\n+    def _end_coalescing(self, device: torch.device) -> Work: ...\n+    def _get_backend_name(self) -> str: ...\n+    def _backend_id(self, backend_type: BackendType) -> int: ...\n+    @property\n+    def _device_types(self) -> List[torch.device]: ...\n+    def _get_backend(self, device: torch.device) -> Backend: ...\n+    def _register_backend(\n+        self,\n+        device: torch.device,\n+        backend_type: BackendType,\n+        backend: Optional[ProcessGroup],\n+    ) -> None: ...\n+    def _set_group_name(self, name: str) -> None: ...\n+    def name(self) -> str: ...\n+    def _has_hooks(self) -> bool: ...\n+    def _wait_for_pending_works(self) -> None: ...\n+    def _set_sequence_number_for_group(self) -> None: ...\n+    @property\n+    def bound_device_id(self) -> Optional[torch.device]: ...\n+    @bound_device_id.setter\n+    def bound_device_id(self, device: Optional[torch.device]) -> None: ...\n+    wrapped_pg: Backend\n+    class Options:\n+        def __init__(self, timeout: Optional[timedelta] = None): ...\n+        @property\n+        def backend(self) -> str: ...\n+        @property\n+        def _timeout(self) -> timedelta: ...\n+        @timeout.setter\n+        def _timeout(self, val: timedelta) -> None: ...\n+        @property\n+        def _is_high_priority_stream(self) -> bool: ...\n+        @is_high_priority_stream.setter\n+        def _is_high_priority_stream(self, val: bool) -> None: ...\n+    @property\n+    def supports_splitting(self) -> bool: ...\n+    def eager_connect_single_device(self, device: Optional[torch.device]) -> None: ...\n",
            "whole_hunk": "@@ -3,6 +3,7 @@ from datetime import timedelta\n from enum import Enum\n from typing import Any, Dict, List, Optional, overload, Tuple, Union\n \n+import torch\n from torch import Tensor\n from torch._C import ScriptObject\n from torch.futures import Future\n@@ -23,6 +24,7 @@ def _register_builtin_comm_hook(\n     reducer: Reducer,\n     comm_hook_type: BuiltinCommHookType,\n ): ...\n+def _set_global_rank(rank: int) -> None: ...\n \n class GradBucket:\n     def index(self) -> int: ...\n@@ -154,6 +156,7 @@ class ReduceScatterOptions:\n \n class BarrierOptions:\n     device_ids: List[int]\n+    device: torch.device\n     timeout: timedelta\n \n class AllToAllOptions:\n@@ -206,11 +209,39 @@ class PrefixStore(Store):\n     @property\n     def underlying_store(self) -> Store: ...\n \n+class _DistributedBackendOptions:\n+    def __init__(self): ...\n+    @property\n+    def store(self) -> Store: ...\n+    @store.setter\n+    def store(self, store: Store) -> None: ...\n+    @property\n+    def group_rank(self) -> int: ...\n+    @group_rank.setter\n+    def group_rank(self, rank: int) -> None: ...\n+    @property\n+    def group_size(self) -> int: ...\n+    @group_size.setter\n+    def group_size(self, size: int) -> None: ...\n+    @property\n+    def timeout(self) -> timedelta: ...\n+    @timeout.setter\n+    def timeout(self, timeout: timedelta) -> None: ...\n+    @property\n+    def group_id(self) -> str: ...\n+    @group_id.setter\n+    def group_id(self, group_id: str) -> None: ...\n+    @property\n+    def global_ranks_in_group(self) -> List[int]: ...\n+    @global_ranks_in_group.setter\n+    def global_ranks_in_group(self, ranks: List[int]) -> None: ...\n+\n class Work:\n     def is_completed(self) -> bool: ...\n     def is_success(self) -> bool: ...\n     def exception(self) -> Any: ...\n     def wait(self, timeout: timedelta = ...) -> bool: ...\n+    def get_future(self) -> Future: ...\n     def source_rank(self) -> int: ...\n     def _source_rank(self) -> int: ...\n     def result(self) -> List[Tensor]: ...\n@@ -220,9 +251,23 @@ class Work:\n     def unbox(obj: ScriptObject) -> Work: ...\n \n class ProcessGroup:\n-    class Options: ...\n-\n-    def __init__(self): ...\n+    class Options:\n+        def __init__(self, backend: str, timeout: timedelta = ...): ...\n+        @property\n+        def backend(self) -> str: ...\n+        @property\n+        def _timeout(self) -> timedelta: ...\n+        @timeout.setter\n+        def _timeout(self, val: timedelta) -> None: ...\n+\n+    class BackendType(Enum):\n+        UNDEFINED = ...\n+        GLOO = ...\n+        NCCL = ...\n+        UCC = ...\n+        MPI = ...\n+        CUSTOM = ...\n+    def __init__(self, store: Store, rank: int, size: int, options: Options): ...\n     def rank(self) -> int: ...\n     def size(self) -> int: ...\n     @overload\n@@ -260,6 +305,12 @@ class ProcessGroup:\n         tensors: List[Tensor],\n         opts=...,\n     ) -> Work: ...\n+    def reduce_scatter_tensor_coalesced(\n+        self,\n+        outputTensors: List[Tensor],\n+        inputTensors: List[Tensor],\n+        opts: Optional[ReduceScatterOptions] = None,\n+    ) -> Work: ...\n     @overload\n     def reduce(\n         self,\n@@ -298,6 +349,12 @@ class ProcessGroup:\n         input_list: List[Tensor],\n         opts=...,\n     ) -> Work: ...\n+    def allgather_into_tensor_coalesced(\n+        self,\n+        output_lists: List[Tensor],\n+        input_list: List[Tensor],\n+        opts=...,\n+    ) -> Work: ...\n     @overload\n     def gather(\n         self,\n@@ -343,6 +400,7 @@ class ProcessGroup:\n         self,\n         outputTensor: Tensor,\n         inputTensor: Tensor,\n+        opts: Optional[ReduceScatterOptions],\n     ) -> Work: ...\n     @overload\n     def alltoall_base(\n@@ -391,6 +449,28 @@ class ProcessGroup:\n     def boxed(self) -> ScriptObject: ...\n     @staticmethod\n     def unbox(obj: ScriptObject) -> ProcessGroup: ...\n+    def _start_coalescing(self, device: torch.device) -> None: ...\n+    def _end_coalescing(self, device: torch.device) -> Work: ...\n+    def _get_backend_name(self) -> str: ...\n+    def _backend_id(self, backend_type: BackendType) -> int: ...\n+    @property\n+    def _device_types(self) -> List[torch.device]: ...\n+    def _get_backend(self, device: torch.device) -> Backend: ...\n+    def _register_backend(\n+        self,\n+        device: torch.device,\n+        backend_type: BackendType,\n+        backend: Optional[ProcessGroup],\n+    ) -> None: ...\n+    def _set_group_name(self, name: str) -> None: ...\n+    def name(self) -> str: ...\n+    def _has_hooks(self) -> bool: ...\n+    def _wait_for_pending_works(self) -> None: ...\n+    def _set_sequence_number_for_group(self) -> None: ...\n+    @property\n+    def bound_device_id(self) -> Optional[torch.device]: ...\n+    @bound_device_id.setter\n+    def bound_device_id(self, device: Optional[torch.device]) -> None: ...\n \n class ProcessGroupRoundRobin(ProcessGroup): ...\n \n@@ -416,10 +496,21 @@ class ProcessGroupGloo(ProcessGroup):\n \n class _ProcessGroupWrapper(ProcessGroup):\n     def __init__(self, pg: ProcessGroup, gloo_pg: ProcessGroupGloo): ...\n-    wrapped_pg: ProcessGroup\n+    wrapped_pg: Backend\n \n class ProcessGroupNCCL(ProcessGroup):\n-    class Options: ...\n+    class Options:\n+        def __init__(self, timeout: Optional[timedelta] = None): ...\n+        @property\n+        def backend(self) -> str: ...\n+        @property\n+        def _timeout(self) -> timedelta: ...\n+        @timeout.setter\n+        def _timeout(self, val: timedelta) -> None: ...\n+        @property\n+        def _is_high_priority_stream(self) -> bool: ...\n+        @is_high_priority_stream.setter\n+        def _is_high_priority_stream(self, val: bool) -> None: ...\n \n     def __init__(\n         self,\n@@ -476,3 +567,6 @@ class Backend:\n         rank: int,\n         size: int,\n     ): ...\n+    @property\n+    def supports_splitting(self) -> bool: ...\n+    def eager_connect_single_device(self, device: Optional[torch.device]) -> None: ...\n"
        },
        {
            "name": "c10d_logger.py",
            "path": "torch/distributed/c10d_logger.py",
            "patches": [
                {
                    "old_start": 9,
                    "old_length": 7,
                    "new_start": 9,
                    "new_length": 8,
                    "hunk": "@@ -9,7 +9,8 @@\n import functools\n import logging\n import time\n-from typing import Any, Dict, List, Tuple\n+from typing import Any, Callable, Dict, List, Tuple, TypeVar\n+from typing_extensions import ParamSpec\n \n import torch\n import torch.distributed as dist\n"
                },
                {
                    "old_start": 64,
                    "old_length": 10,
                    "new_start": 65,
                    "new_length": 12,
                    "hunk": "@@ -64,10 +65,12 @@ def _get_msg_dict(func_name, *args, **kwargs) -> Dict[str, Any]:\n         }\n     return msg_dict\n \n+_T = TypeVar('_T')\n+_P = ParamSpec('_P')\n \n-def _exception_logger(func):\n+def _exception_logger(func: Callable[_P, _T]) -> Callable[_P, _T]:\n     @functools.wraps(func)\n-    def wrapper(*args, **kwargs):\n+    def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> _T:\n         try:\n             return func(*args, **kwargs)\n         except Exception as error:\n"
                },
                {
                    "old_start": 79,
                    "old_length": 9,
                    "new_start": 82,
                    "new_length": 9,
                    "hunk": "@@ -79,9 +82,9 @@ def _exception_logger(func):\n     return wrapper\n \n \n-def _time_logger(func):\n+def _time_logger(func: Callable[_P, _T]) -> Callable[_P, _T]:\n     @functools.wraps(func)\n-    def wrapper(*args, **kwargs):\n+    def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> _T:\n         t1 = time.time_ns()\n         func_return = func(*args, **kwargs)\n         time_spent = time.time_ns() - t1\n"
                }
            ],
            "whole_deleted": "-from typing import Any, Dict, List, Tuple\n-def _exception_logger(func):\n-    def wrapper(*args, **kwargs):\n-def _time_logger(func):\n-    def wrapper(*args, **kwargs):\n",
            "whole_added": "+from typing import Any, Callable, Dict, List, Tuple, TypeVar\n+from typing_extensions import ParamSpec\n+_T = TypeVar('_T')\n+_P = ParamSpec('_P')\n+def _exception_logger(func: Callable[_P, _T]) -> Callable[_P, _T]:\n+    def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> _T:\n+def _time_logger(func: Callable[_P, _T]) -> Callable[_P, _T]:\n+    def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> _T:\n",
            "whole_hunk": "@@ -9,7 +9,8 @@\n import functools\n import logging\n import time\n-from typing import Any, Dict, List, Tuple\n+from typing import Any, Callable, Dict, List, Tuple, TypeVar\n+from typing_extensions import ParamSpec\n \n import torch\n import torch.distributed as dist\n@@ -64,10 +65,12 @@ def _get_msg_dict(func_name, *args, **kwargs) -> Dict[str, Any]:\n         }\n     return msg_dict\n \n+_T = TypeVar('_T')\n+_P = ParamSpec('_P')\n \n-def _exception_logger(func):\n+def _exception_logger(func: Callable[_P, _T]) -> Callable[_P, _T]:\n     @functools.wraps(func)\n-    def wrapper(*args, **kwargs):\n+    def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> _T:\n         try:\n             return func(*args, **kwargs)\n         except Exception as error:\n@@ -79,9 +82,9 @@ def _exception_logger(func):\n     return wrapper\n \n \n-def _time_logger(func):\n+def _time_logger(func: Callable[_P, _T]) -> Callable[_P, _T]:\n     @functools.wraps(func)\n-    def wrapper(*args, **kwargs):\n+    def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> _T:\n         t1 = time.time_ns()\n         func_return = func(*args, **kwargs)\n         time_spent = time.time_ns() - t1\n"
        },
        {
            "name": "device_mesh.py",
            "path": "torch/distributed/device_mesh.py",
            "patches": [
                {
                    "old_start": 6,
                    "old_length": 6,
                    "new_start": 6,
                    "new_length": 7,
                    "hunk": "@@ -6,6 +6,7 @@ from typing import Dict, List, Optional, Tuple, TYPE_CHECKING, Union\n import torch\n \n from torch.distributed import is_available\n+from ..utils._typing_utils import not_none\n \n __all__ = [\"init_device_mesh\", \"DeviceMesh\"]\n \n"
                },
                {
                    "old_start": 369,
                    "old_length": 17,
                    "new_start": 370,
                    "new_length": 21,
                    "hunk": "@@ -369,17 +370,21 @@ else:\n                 raise RuntimeError(\"DeviceMesh process groups not initialized!\")\n \n             if self.mesh.ndim == 1:\n-                return _find_pg_by_ranks_and_tag(*self._dim_group_infos[0])\n+                return not_none(_find_pg_by_ranks_and_tag(*self._dim_group_infos[0]))\n \n             if mesh_dim is not None:\n                 if isinstance(mesh_dim, str):\n                     mesh_dim = self._get_mesh_dim_by_name(mesh_dim)\n-                return _find_pg_by_ranks_and_tag(*self._dim_group_infos[mesh_dim])\n+                return not_none(\n+                    _find_pg_by_ranks_and_tag(*self._dim_group_infos[mesh_dim])\n+                )\n             else:\n                 dim_groups = []\n                 for ith_dim in range(self.mesh.ndim):\n                     dim_groups.append(\n-                        _find_pg_by_ranks_and_tag(*self._dim_group_infos[ith_dim])\n+                        not_none(\n+                            _find_pg_by_ranks_and_tag(*self._dim_group_infos[ith_dim])\n+                        )\n                     )\n                 return dim_groups\n \n"
                }
            ],
            "whole_deleted": "-                return _find_pg_by_ranks_and_tag(*self._dim_group_infos[0])\n-                return _find_pg_by_ranks_and_tag(*self._dim_group_infos[mesh_dim])\n-                        _find_pg_by_ranks_and_tag(*self._dim_group_infos[ith_dim])\n",
            "whole_added": "+from ..utils._typing_utils import not_none\n+                return not_none(_find_pg_by_ranks_and_tag(*self._dim_group_infos[0]))\n+                return not_none(\n+                    _find_pg_by_ranks_and_tag(*self._dim_group_infos[mesh_dim])\n+                )\n+                        not_none(\n+                            _find_pg_by_ranks_and_tag(*self._dim_group_infos[ith_dim])\n+                        )\n",
            "whole_hunk": "@@ -6,6 +6,7 @@ from typing import Dict, List, Optional, Tuple, TYPE_CHECKING, Union\n import torch\n \n from torch.distributed import is_available\n+from ..utils._typing_utils import not_none\n \n __all__ = [\"init_device_mesh\", \"DeviceMesh\"]\n \n@@ -369,17 +370,21 @@ else:\n                 raise RuntimeError(\"DeviceMesh process groups not initialized!\")\n \n             if self.mesh.ndim == 1:\n-                return _find_pg_by_ranks_and_tag(*self._dim_group_infos[0])\n+                return not_none(_find_pg_by_ranks_and_tag(*self._dim_group_infos[0]))\n \n             if mesh_dim is not None:\n                 if isinstance(mesh_dim, str):\n                     mesh_dim = self._get_mesh_dim_by_name(mesh_dim)\n-                return _find_pg_by_ranks_and_tag(*self._dim_group_infos[mesh_dim])\n+                return not_none(\n+                    _find_pg_by_ranks_and_tag(*self._dim_group_infos[mesh_dim])\n+                )\n             else:\n                 dim_groups = []\n                 for ith_dim in range(self.mesh.ndim):\n                     dim_groups.append(\n-                        _find_pg_by_ranks_and_tag(*self._dim_group_infos[ith_dim])\n+                        not_none(\n+                            _find_pg_by_ranks_and_tag(*self._dim_group_infos[ith_dim])\n+                        )\n                     )\n                 return dim_groups\n \n"
        },
        {
            "name": "distributed_c10d.py",
            "path": "torch/distributed/distributed_c10d.py",
            "patches": [
                {
                    "old_start": 39,
                    "old_length": 6,
                    "new_start": 39,
                    "new_length": 7,
                    "hunk": "@@ -39,6 +39,7 @@ from torch._C._distributed_c10d import (\n from .constants import default_pg_timeout, default_pg_nccl_timeout\n from .c10d_logger import _exception_logger, _time_logger\n from .rendezvous import register_rendezvous_handler, rendezvous  # noqa: F401\n+from ..utils._typing_utils import not_none\n DistStoreError = torch._C._DistStoreError\n \n __all__ = [\n"
                },
                {
                    "old_start": 72,
                    "old_length": 7,
                    "new_start": 73,
                    "new_length": 7,
                    "hunk": "@@ -72,7 +73,7 @@ _pickler = pickle.Pickler\n _unpickler = pickle.Unpickler\n \n # Change __module__ of all imported types from torch._C._distributed_c10d that are public\n-def _export_c_types():\n+def _export_c_types() -> None:\n     _public_types_to_change_module = [\n         AllreduceCoalescedOptions,\n         AllreduceOptions,\n"
                },
                {
                    "old_start": 149,
                    "old_length": 7,
                    "new_start": 150,
                    "new_length": 7,
                    "hunk": "@@ -149,7 +150,7 @@ def supports_complex(reduceOp: ReduceOp) -> bool:\n     return reduceOp not in denyList\n \n \n-class Backend:\n+class Backend(str):\n     \"\"\"\n     An enum-like class for backends.\n \n"
                },
                {
                    "old_start": 202,
                    "old_length": 7,
                    "new_start": 203,
                    "new_length": 7,
                    "hunk": "@@ -202,7 +203,7 @@ class Backend:\n     def __new__(cls, name: str):\n         \"\"\"Create and return a new instance of the class.\"\"\"\n         if not isinstance(name, str):\n-            raise ValueError(f\"Backend name must be a string, but got: {name}\")\n+            raise ValueError(\"Backend constructor parameter must be string-ish\")\n         value = getattr(Backend, name.upper(), Backend.UNDEFINED)\n \n         if value == Backend.UNDEFINED:\n"
                },
                {
                    "old_start": 210,
                    "old_length": 7,
                    "new_start": 211,
                    "new_length": 7,
                    "hunk": "@@ -210,7 +211,7 @@ class Backend:\n         return value\n \n     @classmethod\n-    def register_backend(cls, name, func, extended_api=False, devices: Optional[Union[str, List[str]]] = None):\n+    def register_backend(cls, name, func, extended_api=False, devices: Optional[Union[str, List[str]]] = None) -> None:\n         \"\"\"\n         Register a new backend with the given name and instantiating function.\n \n"
                },
                {
                    "old_start": 274,
                    "old_length": 16,
                    "new_start": 275,
                    "new_length": 17,
                    "hunk": "@@ -274,16 +275,17 @@ class Backend:\n class BackendConfig:\n     \"\"\"Backend configuration class.\"\"\"\n \n-    def __init__(self, backend: Union[str, Backend]):\n+    def __init__(self, backend: Backend):\n         \"\"\"Init.\"\"\"\n-        self.device_backend_map: Dict[torch.device, Backend] = {}\n+        self.device_backend_map: Dict[str, Backend] = {}\n+        backend = str(backend)\n \n         if backend == Backend.UNDEFINED:\n             # default config when backend is not specified\n             # supported since PyTorch 2.0\n             for device in Backend.default_device_backend_map:\n                 if is_backend_available(Backend.default_device_backend_map[device]):\n-                    self.device_backend_map[device] = Backend.default_device_backend_map[device]\n+                    self.device_backend_map[device] = Backend(Backend.default_device_backend_map[device])\n         elif backend.lower() in Backend.backend_list:\n             # Cases for when backend is a single string (without device types)\n             # e.g. \"nccl\", \"gloo\", \"ucc\", \"mpi\"\n"
                },
                {
                    "old_start": 336,
                    "old_length": 7,
                    "new_start": 338,
                    "new_length": 7,
                    "hunk": "@@ -336,7 +338,7 @@ class BackendConfig:\n         \"\"\"Return all the device:backend pairs separated by commas.\"\"\"\n         return \",\".join(f\"{device}:{backend}\" for device, backend in self.device_backend_map.items())\n \n-    def get_device_backend_map(self):\n+    def get_device_backend_map(self) -> Dict[str, Backend]:\n         \"\"\"Return backend map of the device.\"\"\"\n         return self.device_backend_map\n \n"
                },
                {
                    "old_start": 425,
                    "old_length": 7,
                    "new_start": 427,
                    "new_length": 7,
                    "hunk": "@@ -425,7 +427,7 @@ class _CollOp:\n \n # DO NOT USE THESE FIELDS DIRECTLY.\n # Use them through the _world object to make sure the _world override mechanism\n-_pg_map: Dict[ProcessGroup, Tuple[str, Optional[Store]]] = {}\n+_pg_map: Dict[ProcessGroup, Tuple[str, Store]] = {}\n _pg_names: Dict[ProcessGroup, str] = {}\n _pg_group_ranks: Dict[ProcessGroup, Dict[int, int]] = {}\n # For a pg, it is a map from ProcessGroup to BackendConfig\n"
                },
                {
                    "old_start": 433,
                    "old_length": 7,
                    "new_start": 435,
                    "new_length": 7,
                    "hunk": "@@ -433,7 +435,7 @@ _pg_backend_config: Dict[ProcessGroup, str] = {}\n _group_count = 0\n _tags_to_pg: Dict[str, List[ProcessGroup]] = {}\n _pg_to_tag: Dict[ProcessGroup, str] = {}\n-\n+_backend: Optional[str] = None\n \n class _World:\n     \"\"\"\n"
                },
                {
                    "old_start": 447,
                    "old_length": 11,
                    "new_start": 449,
                    "new_length": 11,
                    "hunk": "@@ -447,11 +449,11 @@ class _World:\n \n     def __init__(self):\n         self._default_pg = None\n-        self._pg_coalesce_state: Dict[ProcessGroup, List[Union[_CollOp, P2POp]]] = {}\n+        self._pg_coalesce_state: Dict[ProcessGroup, List[_CollOp]] = {}\n         self._pg_default_device: Dict[ProcessGroup, torch.device] = {}\n \n     @property\n-    def default_pg(self):\n+    def default_pg(self) -> Optional[ProcessGroup]:\n         \"\"\"\n         Process group that includes all ranks of the cluster.\n \n"
                },
                {
                    "old_start": 461,
                    "old_length": 11,
                    "new_start": 463,
                    "new_length": 11,
                    "hunk": "@@ -461,11 +463,11 @@ class _World:\n         return self._default_pg\n \n     @default_pg.setter\n-    def default_pg(self, value):\n+    def default_pg(self, value) -> None:\n         self._default_pg = value\n \n     @property\n-    def pg_map(self) -> Dict[ProcessGroup, Tuple[str, Optional[Store]]]:\n+    def pg_map(self) -> Dict[ProcessGroup, Tuple[str, Store]]:\n         \"\"\"\n         Provide Mapping from ProcessGroup to backend name and store.\n \n"
                },
                {
                    "old_start": 518,
                    "old_length": 7,
                    "new_start": 520,
                    "new_length": 7,
                    "hunk": "@@ -518,7 +520,7 @@ class _World:\n         return _group_count\n \n     @group_count.setter\n-    def group_count(self, value):\n+    def group_count(self, value: int) -> None:\n         \"\"\"Use to compute the name of ProcessGroups when using global synchronization.\"\"\"\n         global _group_count\n         _group_count = value\n"
                },
                {
                    "old_start": 534,
                    "old_length": 7,
                    "new_start": 536,
                    "new_length": 7,
                    "hunk": "@@ -534,7 +536,7 @@ class _World:\n         return _pg_to_tag\n \n     @property\n-    def pg_coalesce_state(self) -> Dict[ProcessGroup, List[Union[_CollOp, P2POp]]]:\n+    def pg_coalesce_state(self) -> Dict[ProcessGroup, List[_CollOp]]:\n         return self._pg_coalesce_state\n \n     @property\n"
                },
                {
                    "old_start": 542,
                    "old_length": 13,
                    "new_start": 544,
                    "new_length": 13,
                    "hunk": "@@ -542,13 +544,13 @@ class _World:\n         return self._pg_default_device\n \n     @property\n-    def pg_config_info(self) -> List[Dict[str, Union[int, str]]]:\n+    def pg_config_info(self) -> List[Dict[str, Union[int, str, List[int]]]]:\n         \"\"\"\n         Return a list of dict with process groups and backends.\n \n         Along with their unique IDs and configurations (types and ranks).\n         \"\"\"\n-        config_info = []\n+        config_info: List[Dict[str, Union[int, str, List[int]]]] = []\n         default_pg_size = _get_group_size(None)\n         for pg, backend in self.pg_map.items():\n             # backend is a tuple with the first element being the backend type (\"nccl\", etc.)\n"
                },
                {
                    "old_start": 691,
                    "old_length": 7,
                    "new_start": 693,
                    "new_length": 7,
                    "hunk": "@@ -691,7 +693,7 @@ def _get_pg_default_device(group: Optional[ProcessGroup] = None) -> torch.device\n \n \n @_time_logger\n-def _store_based_barrier(rank, store, group_name, rendezvous_count, timeout, logging_interval=timedelta(seconds=10)):\n+def _store_based_barrier(rank, store, group_name, rendezvous_count, timeout, logging_interval=timedelta(seconds=10)) -> None:\n     \"\"\"\n     Store based barrier for synchronizing processes.\n \n"
                },
                {
                    "old_start": 744,
                    "old_length": 14,
                    "new_start": 746,
                    "new_length": 14,
                    "hunk": "@@ -744,14 +746,14 @@ def _store_based_barrier(rank, store, group_name, rendezvous_count, timeout, log\n     )\n \n \n-def _rank_not_in_group(group: ProcessGroup):\n+def _rank_not_in_group(group: Optional[ProcessGroup]) -> bool:\n     \"\"\"Check if the current process's rank is not in a given group.\"\"\"\n     if group is None:\n         return False\n     return group == GroupMember.NON_GROUP_MEMBER\n \n \n-def _warn_not_in_group(op_name):\n+def _warn_not_in_group(op_name) -> None:\n     global_rank = -1 if GroupMember.WORLD is None else GroupMember.WORLD.rank()\n     warnings.warn(\n         f\"Running {op_name} on global rank {global_rank} which does not \"\n"
                },
                {
                    "old_start": 809,
                    "old_length": 7,
                    "new_start": 811,
                    "new_length": 7,
                    "hunk": "@@ -809,7 +811,7 @@ def get_global_rank(group: ProcessGroup, group_rank: int) -> int:\n     raise ValueError(f\"Group rank {group_rank} is not part of group {group}\")\n \n # TODO: remove this once the ecosystem moves away from it.\n-def _get_global_rank(group, rank):\n+def _get_global_rank(group, rank) -> int:\n     \"\"\"Use get_global_rank as this method is deprecated.\"\"\"\n     warnings.warn(\n         \"torch.distributed.distributed_c10d._get_global_rank is deprecated \"\n"
                },
                {
                    "old_start": 818,
                    "old_length": 7,
                    "new_start": 820,
                    "new_length": 7,
                    "hunk": "@@ -818,7 +820,7 @@ def _get_global_rank(group, rank):\n     return get_global_rank(group, rank)\n \n \n-def get_process_group_ranks(group: ProcessGroup):\n+def get_process_group_ranks(group: ProcessGroup) -> List[int]:\n     \"\"\"\n     Get all ranks associated with ``group``.\n \n"
                },
                {
                    "old_start": 830,
                    "old_length": 7,
                    "new_start": 832,
                    "new_length": 7,
                    "hunk": "@@ -830,7 +832,7 @@ def get_process_group_ranks(group: ProcessGroup):\n     \"\"\"\n     return list(_world.pg_group_ranks[group].keys())\n \n-def _get_group_size(group):\n+def _get_group_size(group) -> int:\n     \"\"\"Get a given group's world size.\"\"\"\n     if group is GroupMember.WORLD or group is None:\n         default_pg = _get_default_group()\n"
                },
                {
                    "old_start": 838,
                    "old_length": 7,
                    "new_start": 840,
                    "new_length": 7,
                    "hunk": "@@ -838,7 +840,7 @@ def _get_group_size(group):\n     return group.size()\n \n \n-def _check_single_tensor(param, param_name):\n+def _check_single_tensor(param, param_name) -> None:\n     \"\"\"Check that the parameter ``param_name`` is a single tensor.\"\"\"\n     if not isinstance(param, torch.Tensor):\n         raise TypeError(\n"
                },
                {
                    "old_start": 846,
                    "old_length": 7,
                    "new_start": 848,
                    "new_length": 7,
                    "hunk": "@@ -846,7 +848,7 @@ def _check_single_tensor(param, param_name):\n         )\n \n \n-def _check_tensor_list(param, param_name):\n+def _check_tensor_list(param, param_name) -> None:\n     \"\"\"Check that the parameter ``param_name`` is a list of tensors.\"\"\"\n     if not isinstance(param, list) or not all(\n         isinstance(p, torch.Tensor) for p in param\n"
                },
                {
                    "old_start": 876,
                    "old_length": 7,
                    "new_start": 878,
                    "new_length": 7,
                    "hunk": "@@ -876,7 +878,7 @@ def _ensure_all_tensors_same_dtype(*tensors) -> None:\n                 )\n \n \n-def _check_op(op):\n+def _check_op(op) -> None:\n     \"\"\"Check that the ``op`` is either isend or irecv.\"\"\"\n     if op not in [isend, irecv]:\n         raise ValueError(\n"
                },
                {
                    "old_start": 886,
                    "old_length": 7,
                    "new_start": 888,
                    "new_length": 7,
                    "hunk": "@@ -886,7 +888,7 @@ def _check_op(op):\n         )\n \n \n-def _check_p2p_op_list(p2p_op_list):\n+def _check_p2p_op_list(p2p_op_list) -> None:\n     \"\"\"\n     Check that the ``p2p_op_list`` is a list of P2POp instances.\n \n"
                },
                {
                    "old_start": 971,
                    "old_length": 17,
                    "new_start": 973,
                    "new_length": 17,
                    "hunk": "@@ -971,17 +973,17 @@ def _is_barrier_after_init() -> int:\n     return int(os.getenv(\"TORCH_DIST_INIT_BARRIER\", \"0\"))\n \n \n-def _get_default_group():\n+def _get_default_group() -> ProcessGroup:\n     \"\"\"Get the default process group created by init_process_group.\"\"\"\n     if not is_initialized():\n         raise ValueError(\n             \"Default process group has not been initialized, \"\n             \"please make sure to call init_process_group.\"\n         )\n-    return GroupMember.WORLD\n+    return not_none(GroupMember.WORLD)\n \n \n-def _get_default_store():\n+def _get_default_store() -> Store:\n     \"\"\"Get the default store created by init_process_group.\"\"\"\n     if not is_initialized():\n         raise ValueError(\n"
                },
                {
                    "old_start": 993,
                    "old_length": 7,
                    "new_start": 995,
                    "new_length": 7,
                    "hunk": "@@ -993,7 +995,7 @@ def _get_default_store():\n     return default_store\n \n \n-def _update_default_pg(pg):\n+def _update_default_pg(pg) -> None:\n     _world.default_pg = pg\n     rank = pg.rank() if pg is not None and pg != GroupMember.NON_GROUP_MEMBER else -1\n     torch._C._distributed_c10d._set_global_rank(rank)\n"
                },
                {
                    "old_start": 1018,
                    "old_length": 10,
                    "new_start": 1020,
                    "new_length": 9,
                    "hunk": "@@ -1018,10 +1020,9 @@ def get_backend_config(group: Optional[ProcessGroup] = None) -> str:\n     if _rank_not_in_group(pg):\n         raise ValueError(\"Invalid process group specified\")\n     backend_config = _world.pg_backend_config.get(pg)\n-    assert backend_config is not None\n-    return str(backend_config)\n+    return str(not_none(backend_config))\n \n-def get_backend(group: Optional[ProcessGroup] = None) -> str:\n+def get_backend(group: Optional[ProcessGroup] = None) -> Backend:\n     \"\"\"\n     Return the backend of the given process group.\n \n"
                },
                {
                    "old_start": 1041,
                    "old_length": 14,
                    "new_start": 1042,
                    "new_length": 12,
                    "hunk": "@@ -1041,14 +1042,12 @@ def get_backend(group: Optional[ProcessGroup] = None) -> str:\n     if _rank_not_in_group(pg):\n         raise ValueError(\"Invalid process group specified\")\n     pg_store = _world.pg_map[pg] if pg in _world.pg_map else None\n-    assert pg_store is not None\n-    return pg_store[0]\n-\n+    return Backend(not_none(pg_store)[0])\n \n-_exception_logger\n+@_exception_logger\n @_time_logger\n def init_process_group(\n-    backend: Union[str, Backend] = None,\n+    backend: Optional[str] = None,\n     init_method: Optional[str] = None,\n     timeout: Optional[timedelta] = None,\n     world_size: int = -1,\n"
                },
                {
                    "old_start": 1057,
                    "old_length": 7,
                    "new_start": 1056,
                    "new_length": 7,
                    "hunk": "@@ -1057,7 +1056,7 @@ def init_process_group(\n     group_name: str = \"\",\n     pg_options: Optional[Any] = None,\n     device_id: Optional[torch.device] = None,\n-):\n+) -> None:\n     \"\"\"\n     Initialize the default distributed process group.\n \n"
                },
                {
                    "old_start": 1173,
                    "old_length": 7,
                    "new_start": 1172,
                    "new_length": 7,
                    "hunk": "@@ -1173,7 +1172,7 @@ def init_process_group(\n         # backward compatible API\n         if store is None:\n             rendezvous_iterator = rendezvous(\n-                init_method, rank, world_size, timeout=timeout\n+                not_none(init_method), rank, world_size, timeout=timeout\n             )\n             store, rank, world_size = next(rendezvous_iterator)\n             store.set_timeout(timeout)\n"
                },
                {
                    "old_start": 1196,
                    "old_length": 7,
                    "new_start": 1195,
                    "new_length": 7,
                    "hunk": "@@ -1196,7 +1195,7 @@ def init_process_group(\n         _update_default_pg(default_pg)\n \n     _world.pg_group_ranks[GroupMember.WORLD] = {i: i for i in range(GroupMember.WORLD.size())}  # type: ignore[attr-defined, index]\n-    _backend = _world.pg_map[GroupMember.WORLD][0]  # type: ignore[index]\n+    _backend = _world.pg_map[not_none(GroupMember.WORLD)][0]\n     _default_pg_init_method = init_method\n \n     if _is_barrier_after_init() == 1:\n"
                },
                {
                    "old_start": 1296,
                    "old_length": 8,
                    "new_start": 1295,
                    "new_length": 8,
                    "hunk": "@@ -1296,8 +1295,8 @@ def _new_process_group_helper(\n     # entire world or if we have bound a device id to the world (which\n     # causes early connection initialization).\n     if (is_initialized() and\n-            (len(global_ranks_in_group) == _world.default_pg.size() or _world.default_pg.bound_device_id)):\n-        split_from = _get_split_source(_world.default_pg)\n+            (len(global_ranks_in_group) == _get_default_group().size() or _get_default_group().bound_device_id)):\n+        split_from = _get_split_source(_get_default_group())\n     else:\n         split_from = None\n \n"
                },
                {
                    "old_start": 1313,
                    "old_length": 7,
                    "new_start": 1312,
                    "new_length": 7,
                    "hunk": "@@ -1313,7 +1312,7 @@ def _new_process_group_helper(\n             # a requirement of the NCCL API as otherwise we would get\n             # out of sync.\n             if split_from:\n-                split_from.perform_nocolor_split(_world.default_pg.bound_device_id)\n+                split_from.perform_nocolor_split(_get_default_group().bound_device_id)\n             return GroupMember.NON_GROUP_MEMBER, None\n \n     prefix_store = PrefixStore(f\"{group_name}/\", store)\n"
                },
                {
                    "old_start": 1323,
                    "old_length": 6,
                    "new_start": 1322,
                    "new_length": 7,
                    "hunk": "@@ -1323,6 +1322,7 @@ def _new_process_group_helper(\n     if device_id:\n         pg.bound_device_id = device_id\n     backend_config = BackendConfig(backend)\n+    backend_class: ProcessGroup\n     for device, backend_str in backend_config.get_device_backend_map().items():\n         # Use the group name as prefix in the default store, such that\n         # a single store can be reused by multiple groups.\n"
                },
                {
                    "old_start": 1338,
                    "old_length": 7,
                    "new_start": 1338,
                    "new_length": 7,
                    "hunk": "@@ -1338,7 +1338,7 @@ def _new_process_group_helper(\n             backend_class = ProcessGroupMPI.create(global_ranks_in_group)\n             backend_type = ProcessGroup.BackendType.MPI\n             if not backend_class:\n-                return GroupMember.NON_GROUP_MEMBER\n+                return GroupMember.NON_GROUP_MEMBER, None\n             # create new process group with accurate rank and size\n             if pg.rank() == -1 and pg.size() == -1:\n                 pg = ProcessGroup(backend_prefix_store, backend_class.rank(), backend_class.size(), base_pg_options)\n"
                },
                {
                    "old_start": 1403,
                    "old_length": 8,
                    "new_start": 1403,
                    "new_length": 13,
                    "hunk": "@@ -1403,8 +1403,13 @@ def _new_process_group_helper(\n                 backend_class = creator_fn(dist_backend_opts, pg_options)\n \n         # Set sequence numbers for gloo and nccl backends.\n-        if backend_str in [Backend.GLOO, Backend.NCCL]:\n+        if backend_str == Backend.GLOO:\n+            assert isinstance(backend_class, ProcessGroupGloo)\n             backend_class._set_sequence_number_for_group()\n+        elif backend_str == Backend.NCCL:\n+            assert isinstance(backend_class, ProcessGroupNCCL)\n+            backend_class._set_sequence_number_for_group()\n+\n         # If the type is a subclass of ProcessGroup then return this process group immediately\n         # TODO: This defaults to the old behavior for PythonProcessGroups which overwrites the\n         # ProcessGroup instance\n"
                },
                {
                    "old_start": 1592,
                    "old_length": 7,
                    "new_start": 1597,
                    "new_length": 7,
                    "hunk": "@@ -1592,7 +1597,7 @@ def get_world_size(group: Optional[ProcessGroup] = None) -> int:\n     return _get_group_size(group)\n \n \n-def isend(tensor: torch.Tensor, dst: int, group: Optional[ProcessGroup] = None, tag: int = 0) -> Work:\n+def isend(tensor: torch.Tensor, dst: int, group: Optional[ProcessGroup] = None, tag: int = 0) -> Optional[Work]:\n     \"\"\"\n     Send a tensor asynchronously.\n \n"
                },
                {
                    "old_start": 1618,
                    "old_length": 17,
                    "new_start": 1623,
                    "new_length": 17,
                    "hunk": "@@ -1618,17 +1623,17 @@ def isend(tensor: torch.Tensor, dst: int, group: Optional[ProcessGroup] = None,\n     _check_single_tensor(tensor, \"tensor\")\n     if _rank_not_in_group(group):\n         _warn_not_in_group(\"isend\")\n-        return\n+        return None\n \n     if group is None or group is GroupMember.WORLD:\n-        default_pg = _get_default_group()\n-        return default_pg.send([tensor], dst, tag)\n+        pg = _get_default_group()\n     else:\n-        group_dst_rank = get_group_rank(group, dst)\n-        return group.send([tensor], group_dst_rank, tag)\n+        pg = group\n+        dst = get_group_rank(pg, dst)\n \n+    return pg.send([tensor], dst, tag)\n \n-def irecv(tensor: torch.Tensor, src: Optional[int] = None, group: Optional[ProcessGroup] = None, tag: int = 0) -> Work:\n+def irecv(tensor: torch.Tensor, src: Optional[int] = None, group: Optional[ProcessGroup] = None, tag: int = 0) -> Optional[Work]:\n     \"\"\"\n     Receives a tensor asynchronously.\n \n"
                },
                {
                    "old_start": 1651,
                    "old_length": 7,
                    "new_start": 1656,
                    "new_length": 7,
                    "hunk": "@@ -1651,7 +1656,7 @@ def irecv(tensor: torch.Tensor, src: Optional[int] = None, group: Optional[Proce\n     _check_single_tensor(tensor, \"tensor\")\n     if _rank_not_in_group(group):\n         _warn_not_in_group(\"irecv\")\n-        return\n+        return None\n \n     if group is None or group is GroupMember.WORLD:\n         pg = _get_default_group()\n"
                },
                {
                    "old_start": 1690,
                    "old_length": 7,
                    "new_start": 1695,
                    "new_length": 7,
                    "hunk": "@@ -1690,7 +1695,7 @@ def send(tensor: torch.Tensor, dst: int, group: Optional[ProcessGroup] = None, t\n     _check_single_tensor(tensor, \"tensor\")\n     if _rank_not_in_group(group):\n         _warn_not_in_group(\"send\")\n-        return\n+        return None\n \n     if group is None or group is GroupMember.WORLD:\n         default_pg = _get_default_group()\n"
                },
                {
                    "old_start": 1817,
                    "old_length": 25,
                    "new_start": 1822,
                    "new_length": 25,
                    "hunk": "@@ -1817,25 +1822,25 @@ def _coalescing_manager(\n             tensors = []\n             for op in op_list:\n                 tensors.append(op.tensor)\n-            opts = AllreduceCoalescedOptions()\n-            opts.reduceOp = op_list[0].redop\n-            work = group.allreduce_coalesced(tensors, opts)\n+            all_reduce_opts = AllreduceCoalescedOptions()\n+            all_reduce_opts.reduceOp = not_none(op_list[0].redop)\n+            work = group.allreduce_coalesced(tensors, all_reduce_opts)\n         elif op0 == all_gather_into_tensor:\n             inputs = []\n             outputs = []\n             for op in op_list:\n                 inputs.append(op.tensor)\n-                outputs.append(op.dst_tensor)\n+                outputs.append(not_none(op.dst_tensor))\n             work = group.allgather_into_tensor_coalesced(outputs, inputs)\n         elif op0 == reduce_scatter_tensor:\n             inputs = []\n             outputs = []\n             for op in op_list:\n                 inputs.append(op.tensor)\n-                outputs.append(op.dst_tensor)\n-                opts = ReduceScatterOptions()\n-                opts.reduceOp = op_list[0].redop\n-            work = group.reduce_scatter_tensor_coalesced(outputs, inputs, opts)\n+                outputs.append(not_none(op.dst_tensor))\n+            reduce_opts = ReduceScatterOptions()\n+            reduce_opts.reduceOp = not_none(op_list[0].redop)\n+            work = group.reduce_scatter_tensor_coalesced(outputs, inputs, reduce_opts)\n         else:\n             raise AssertionError(\n                 f\"Coalescing manager does not support fast-path coalescing of {op0}, \"\n"
                },
                {
                    "old_start": 3978,
                    "old_length": 7,
                    "new_start": 3983,
                    "new_length": 7,
                    "hunk": "@@ -3978,7 +3983,7 @@ def new_subgroups_by_enumeration(\n     return cur_subgroup, subgroups\n \n \n-def _find_pg_by_ranks_and_tag(tag: str, ranks: List[int]) -> ProcessGroup:\n+def _find_pg_by_ranks_and_tag(tag: str, ranks: List[int]) -> Optional[ProcessGroup]:\n     if len(tag) > 0 and not tag.startswith(\"ptd:\") and not tag.startswith(\"user:\"):\n         tag = f\"user:{tag}\"\n \n"
                }
            ],
            "whole_deleted": "-def _export_c_types():\n-class Backend:\n-            raise ValueError(f\"Backend name must be a string, but got: {name}\")\n-    def register_backend(cls, name, func, extended_api=False, devices: Optional[Union[str, List[str]]] = None):\n-    def __init__(self, backend: Union[str, Backend]):\n-        self.device_backend_map: Dict[torch.device, Backend] = {}\n-                    self.device_backend_map[device] = Backend.default_device_backend_map[device]\n-    def get_device_backend_map(self):\n-_pg_map: Dict[ProcessGroup, Tuple[str, Optional[Store]]] = {}\n-\n-        self._pg_coalesce_state: Dict[ProcessGroup, List[Union[_CollOp, P2POp]]] = {}\n-    def default_pg(self):\n-    def default_pg(self, value):\n-    def pg_map(self) -> Dict[ProcessGroup, Tuple[str, Optional[Store]]]:\n-    def group_count(self, value):\n-    def pg_coalesce_state(self) -> Dict[ProcessGroup, List[Union[_CollOp, P2POp]]]:\n-    def pg_config_info(self) -> List[Dict[str, Union[int, str]]]:\n-        config_info = []\n-def _store_based_barrier(rank, store, group_name, rendezvous_count, timeout, logging_interval=timedelta(seconds=10)):\n-def _rank_not_in_group(group: ProcessGroup):\n-def _warn_not_in_group(op_name):\n-def _get_global_rank(group, rank):\n-def get_process_group_ranks(group: ProcessGroup):\n-def _get_group_size(group):\n-def _check_single_tensor(param, param_name):\n-def _check_tensor_list(param, param_name):\n-def _check_op(op):\n-def _check_p2p_op_list(p2p_op_list):\n-def _get_default_group():\n-    return GroupMember.WORLD\n-def _get_default_store():\n-def _update_default_pg(pg):\n-    assert backend_config is not None\n-    return str(backend_config)\n-def get_backend(group: Optional[ProcessGroup] = None) -> str:\n-    assert pg_store is not None\n-    return pg_store[0]\n-\n-_exception_logger\n-    backend: Union[str, Backend] = None,\n-):\n-                init_method, rank, world_size, timeout=timeout\n-    _backend = _world.pg_map[GroupMember.WORLD][0]  # type: ignore[index]\n-            (len(global_ranks_in_group) == _world.default_pg.size() or _world.default_pg.bound_device_id)):\n-        split_from = _get_split_source(_world.default_pg)\n-                split_from.perform_nocolor_split(_world.default_pg.bound_device_id)\n-                return GroupMember.NON_GROUP_MEMBER\n-        if backend_str in [Backend.GLOO, Backend.NCCL]:\n-def isend(tensor: torch.Tensor, dst: int, group: Optional[ProcessGroup] = None, tag: int = 0) -> Work:\n-        return\n-        default_pg = _get_default_group()\n-        return default_pg.send([tensor], dst, tag)\n-        group_dst_rank = get_group_rank(group, dst)\n-        return group.send([tensor], group_dst_rank, tag)\n-def irecv(tensor: torch.Tensor, src: Optional[int] = None, group: Optional[ProcessGroup] = None, tag: int = 0) -> Work:\n-        return\n-        return\n-            opts = AllreduceCoalescedOptions()\n-            opts.reduceOp = op_list[0].redop\n-            work = group.allreduce_coalesced(tensors, opts)\n-                outputs.append(op.dst_tensor)\n-                outputs.append(op.dst_tensor)\n-                opts = ReduceScatterOptions()\n-                opts.reduceOp = op_list[0].redop\n-            work = group.reduce_scatter_tensor_coalesced(outputs, inputs, opts)\n-def _find_pg_by_ranks_and_tag(tag: str, ranks: List[int]) -> ProcessGroup:\n",
            "whole_added": "+from ..utils._typing_utils import not_none\n+def _export_c_types() -> None:\n+class Backend(str):\n+            raise ValueError(\"Backend constructor parameter must be string-ish\")\n+    def register_backend(cls, name, func, extended_api=False, devices: Optional[Union[str, List[str]]] = None) -> None:\n+    def __init__(self, backend: Backend):\n+        self.device_backend_map: Dict[str, Backend] = {}\n+        backend = str(backend)\n+                    self.device_backend_map[device] = Backend(Backend.default_device_backend_map[device])\n+    def get_device_backend_map(self) -> Dict[str, Backend]:\n+_pg_map: Dict[ProcessGroup, Tuple[str, Store]] = {}\n+_backend: Optional[str] = None\n+        self._pg_coalesce_state: Dict[ProcessGroup, List[_CollOp]] = {}\n+    def default_pg(self) -> Optional[ProcessGroup]:\n+    def default_pg(self, value) -> None:\n+    def pg_map(self) -> Dict[ProcessGroup, Tuple[str, Store]]:\n+    def group_count(self, value: int) -> None:\n+    def pg_coalesce_state(self) -> Dict[ProcessGroup, List[_CollOp]]:\n+    def pg_config_info(self) -> List[Dict[str, Union[int, str, List[int]]]]:\n+        config_info: List[Dict[str, Union[int, str, List[int]]]] = []\n+def _store_based_barrier(rank, store, group_name, rendezvous_count, timeout, logging_interval=timedelta(seconds=10)) -> None:\n+def _rank_not_in_group(group: Optional[ProcessGroup]) -> bool:\n+def _warn_not_in_group(op_name) -> None:\n+def _get_global_rank(group, rank) -> int:\n+def get_process_group_ranks(group: ProcessGroup) -> List[int]:\n+def _get_group_size(group) -> int:\n+def _check_single_tensor(param, param_name) -> None:\n+def _check_tensor_list(param, param_name) -> None:\n+def _check_op(op) -> None:\n+def _check_p2p_op_list(p2p_op_list) -> None:\n+def _get_default_group() -> ProcessGroup:\n+    return not_none(GroupMember.WORLD)\n+def _get_default_store() -> Store:\n+def _update_default_pg(pg) -> None:\n+    return str(not_none(backend_config))\n+def get_backend(group: Optional[ProcessGroup] = None) -> Backend:\n+    return Backend(not_none(pg_store)[0])\n+@_exception_logger\n+    backend: Optional[str] = None,\n+) -> None:\n+                not_none(init_method), rank, world_size, timeout=timeout\n+    _backend = _world.pg_map[not_none(GroupMember.WORLD)][0]\n+            (len(global_ranks_in_group) == _get_default_group().size() or _get_default_group().bound_device_id)):\n+        split_from = _get_split_source(_get_default_group())\n+                split_from.perform_nocolor_split(_get_default_group().bound_device_id)\n+    backend_class: ProcessGroup\n+                return GroupMember.NON_GROUP_MEMBER, None\n+        if backend_str == Backend.GLOO:\n+            assert isinstance(backend_class, ProcessGroupGloo)\n+        elif backend_str == Backend.NCCL:\n+            assert isinstance(backend_class, ProcessGroupNCCL)\n+            backend_class._set_sequence_number_for_group()\n+\n+def isend(tensor: torch.Tensor, dst: int, group: Optional[ProcessGroup] = None, tag: int = 0) -> Optional[Work]:\n+        return None\n+        pg = _get_default_group()\n+        pg = group\n+        dst = get_group_rank(pg, dst)\n+    return pg.send([tensor], dst, tag)\n+def irecv(tensor: torch.Tensor, src: Optional[int] = None, group: Optional[ProcessGroup] = None, tag: int = 0) -> Optional[Work]:\n+        return None\n+        return None\n+            all_reduce_opts = AllreduceCoalescedOptions()\n+            all_reduce_opts.reduceOp = not_none(op_list[0].redop)\n+            work = group.allreduce_coalesced(tensors, all_reduce_opts)\n+                outputs.append(not_none(op.dst_tensor))\n+                outputs.append(not_none(op.dst_tensor))\n+            reduce_opts = ReduceScatterOptions()\n+            reduce_opts.reduceOp = not_none(op_list[0].redop)\n+            work = group.reduce_scatter_tensor_coalesced(outputs, inputs, reduce_opts)\n+def _find_pg_by_ranks_and_tag(tag: str, ranks: List[int]) -> Optional[ProcessGroup]:\n",
            "whole_hunk": "@@ -39,6 +39,7 @@ from torch._C._distributed_c10d import (\n from .constants import default_pg_timeout, default_pg_nccl_timeout\n from .c10d_logger import _exception_logger, _time_logger\n from .rendezvous import register_rendezvous_handler, rendezvous  # noqa: F401\n+from ..utils._typing_utils import not_none\n DistStoreError = torch._C._DistStoreError\n \n __all__ = [\n@@ -72,7 +73,7 @@ _pickler = pickle.Pickler\n _unpickler = pickle.Unpickler\n \n # Change __module__ of all imported types from torch._C._distributed_c10d that are public\n-def _export_c_types():\n+def _export_c_types() -> None:\n     _public_types_to_change_module = [\n         AllreduceCoalescedOptions,\n         AllreduceOptions,\n@@ -149,7 +150,7 @@ def supports_complex(reduceOp: ReduceOp) -> bool:\n     return reduceOp not in denyList\n \n \n-class Backend:\n+class Backend(str):\n     \"\"\"\n     An enum-like class for backends.\n \n@@ -202,7 +203,7 @@ class Backend:\n     def __new__(cls, name: str):\n         \"\"\"Create and return a new instance of the class.\"\"\"\n         if not isinstance(name, str):\n-            raise ValueError(f\"Backend name must be a string, but got: {name}\")\n+            raise ValueError(\"Backend constructor parameter must be string-ish\")\n         value = getattr(Backend, name.upper(), Backend.UNDEFINED)\n \n         if value == Backend.UNDEFINED:\n@@ -210,7 +211,7 @@ class Backend:\n         return value\n \n     @classmethod\n-    def register_backend(cls, name, func, extended_api=False, devices: Optional[Union[str, List[str]]] = None):\n+    def register_backend(cls, name, func, extended_api=False, devices: Optional[Union[str, List[str]]] = None) -> None:\n         \"\"\"\n         Register a new backend with the given name and instantiating function.\n \n@@ -274,16 +275,17 @@ class Backend:\n class BackendConfig:\n     \"\"\"Backend configuration class.\"\"\"\n \n-    def __init__(self, backend: Union[str, Backend]):\n+    def __init__(self, backend: Backend):\n         \"\"\"Init.\"\"\"\n-        self.device_backend_map: Dict[torch.device, Backend] = {}\n+        self.device_backend_map: Dict[str, Backend] = {}\n+        backend = str(backend)\n \n         if backend == Backend.UNDEFINED:\n             # default config when backend is not specified\n             # supported since PyTorch 2.0\n             for device in Backend.default_device_backend_map:\n                 if is_backend_available(Backend.default_device_backend_map[device]):\n-                    self.device_backend_map[device] = Backend.default_device_backend_map[device]\n+                    self.device_backend_map[device] = Backend(Backend.default_device_backend_map[device])\n         elif backend.lower() in Backend.backend_list:\n             # Cases for when backend is a single string (without device types)\n             # e.g. \"nccl\", \"gloo\", \"ucc\", \"mpi\"\n@@ -336,7 +338,7 @@ class BackendConfig:\n         \"\"\"Return all the device:backend pairs separated by commas.\"\"\"\n         return \",\".join(f\"{device}:{backend}\" for device, backend in self.device_backend_map.items())\n \n-    def get_device_backend_map(self):\n+    def get_device_backend_map(self) -> Dict[str, Backend]:\n         \"\"\"Return backend map of the device.\"\"\"\n         return self.device_backend_map\n \n@@ -425,7 +427,7 @@ class _CollOp:\n \n # DO NOT USE THESE FIELDS DIRECTLY.\n # Use them through the _world object to make sure the _world override mechanism\n-_pg_map: Dict[ProcessGroup, Tuple[str, Optional[Store]]] = {}\n+_pg_map: Dict[ProcessGroup, Tuple[str, Store]] = {}\n _pg_names: Dict[ProcessGroup, str] = {}\n _pg_group_ranks: Dict[ProcessGroup, Dict[int, int]] = {}\n # For a pg, it is a map from ProcessGroup to BackendConfig\n@@ -433,7 +435,7 @@ _pg_backend_config: Dict[ProcessGroup, str] = {}\n _group_count = 0\n _tags_to_pg: Dict[str, List[ProcessGroup]] = {}\n _pg_to_tag: Dict[ProcessGroup, str] = {}\n-\n+_backend: Optional[str] = None\n \n class _World:\n     \"\"\"\n@@ -447,11 +449,11 @@ class _World:\n \n     def __init__(self):\n         self._default_pg = None\n-        self._pg_coalesce_state: Dict[ProcessGroup, List[Union[_CollOp, P2POp]]] = {}\n+        self._pg_coalesce_state: Dict[ProcessGroup, List[_CollOp]] = {}\n         self._pg_default_device: Dict[ProcessGroup, torch.device] = {}\n \n     @property\n-    def default_pg(self):\n+    def default_pg(self) -> Optional[ProcessGroup]:\n         \"\"\"\n         Process group that includes all ranks of the cluster.\n \n@@ -461,11 +463,11 @@ class _World:\n         return self._default_pg\n \n     @default_pg.setter\n-    def default_pg(self, value):\n+    def default_pg(self, value) -> None:\n         self._default_pg = value\n \n     @property\n-    def pg_map(self) -> Dict[ProcessGroup, Tuple[str, Optional[Store]]]:\n+    def pg_map(self) -> Dict[ProcessGroup, Tuple[str, Store]]:\n         \"\"\"\n         Provide Mapping from ProcessGroup to backend name and store.\n \n@@ -518,7 +520,7 @@ class _World:\n         return _group_count\n \n     @group_count.setter\n-    def group_count(self, value):\n+    def group_count(self, value: int) -> None:\n         \"\"\"Use to compute the name of ProcessGroups when using global synchronization.\"\"\"\n         global _group_count\n         _group_count = value\n@@ -534,7 +536,7 @@ class _World:\n         return _pg_to_tag\n \n     @property\n-    def pg_coalesce_state(self) -> Dict[ProcessGroup, List[Union[_CollOp, P2POp]]]:\n+    def pg_coalesce_state(self) -> Dict[ProcessGroup, List[_CollOp]]:\n         return self._pg_coalesce_state\n \n     @property\n@@ -542,13 +544,13 @@ class _World:\n         return self._pg_default_device\n \n     @property\n-    def pg_config_info(self) -> List[Dict[str, Union[int, str]]]:\n+    def pg_config_info(self) -> List[Dict[str, Union[int, str, List[int]]]]:\n         \"\"\"\n         Return a list of dict with process groups and backends.\n \n         Along with their unique IDs and configurations (types and ranks).\n         \"\"\"\n-        config_info = []\n+        config_info: List[Dict[str, Union[int, str, List[int]]]] = []\n         default_pg_size = _get_group_size(None)\n         for pg, backend in self.pg_map.items():\n             # backend is a tuple with the first element being the backend type (\"nccl\", etc.)\n@@ -691,7 +693,7 @@ def _get_pg_default_device(group: Optional[ProcessGroup] = None) -> torch.device\n \n \n @_time_logger\n-def _store_based_barrier(rank, store, group_name, rendezvous_count, timeout, logging_interval=timedelta(seconds=10)):\n+def _store_based_barrier(rank, store, group_name, rendezvous_count, timeout, logging_interval=timedelta(seconds=10)) -> None:\n     \"\"\"\n     Store based barrier for synchronizing processes.\n \n@@ -744,14 +746,14 @@ def _store_based_barrier(rank, store, group_name, rendezvous_count, timeout, log\n     )\n \n \n-def _rank_not_in_group(group: ProcessGroup):\n+def _rank_not_in_group(group: Optional[ProcessGroup]) -> bool:\n     \"\"\"Check if the current process's rank is not in a given group.\"\"\"\n     if group is None:\n         return False\n     return group == GroupMember.NON_GROUP_MEMBER\n \n \n-def _warn_not_in_group(op_name):\n+def _warn_not_in_group(op_name) -> None:\n     global_rank = -1 if GroupMember.WORLD is None else GroupMember.WORLD.rank()\n     warnings.warn(\n         f\"Running {op_name} on global rank {global_rank} which does not \"\n@@ -809,7 +811,7 @@ def get_global_rank(group: ProcessGroup, group_rank: int) -> int:\n     raise ValueError(f\"Group rank {group_rank} is not part of group {group}\")\n \n # TODO: remove this once the ecosystem moves away from it.\n-def _get_global_rank(group, rank):\n+def _get_global_rank(group, rank) -> int:\n     \"\"\"Use get_global_rank as this method is deprecated.\"\"\"\n     warnings.warn(\n         \"torch.distributed.distributed_c10d._get_global_rank is deprecated \"\n@@ -818,7 +820,7 @@ def _get_global_rank(group, rank):\n     return get_global_rank(group, rank)\n \n \n-def get_process_group_ranks(group: ProcessGroup):\n+def get_process_group_ranks(group: ProcessGroup) -> List[int]:\n     \"\"\"\n     Get all ranks associated with ``group``.\n \n@@ -830,7 +832,7 @@ def get_process_group_ranks(group: ProcessGroup):\n     \"\"\"\n     return list(_world.pg_group_ranks[group].keys())\n \n-def _get_group_size(group):\n+def _get_group_size(group) -> int:\n     \"\"\"Get a given group's world size.\"\"\"\n     if group is GroupMember.WORLD or group is None:\n         default_pg = _get_default_group()\n@@ -838,7 +840,7 @@ def _get_group_size(group):\n     return group.size()\n \n \n-def _check_single_tensor(param, param_name):\n+def _check_single_tensor(param, param_name) -> None:\n     \"\"\"Check that the parameter ``param_name`` is a single tensor.\"\"\"\n     if not isinstance(param, torch.Tensor):\n         raise TypeError(\n@@ -846,7 +848,7 @@ def _check_single_tensor(param, param_name):\n         )\n \n \n-def _check_tensor_list(param, param_name):\n+def _check_tensor_list(param, param_name) -> None:\n     \"\"\"Check that the parameter ``param_name`` is a list of tensors.\"\"\"\n     if not isinstance(param, list) or not all(\n         isinstance(p, torch.Tensor) for p in param\n@@ -876,7 +878,7 @@ def _ensure_all_tensors_same_dtype(*tensors) -> None:\n                 )\n \n \n-def _check_op(op):\n+def _check_op(op) -> None:\n     \"\"\"Check that the ``op`` is either isend or irecv.\"\"\"\n     if op not in [isend, irecv]:\n         raise ValueError(\n@@ -886,7 +888,7 @@ def _check_op(op):\n         )\n \n \n-def _check_p2p_op_list(p2p_op_list):\n+def _check_p2p_op_list(p2p_op_list) -> None:\n     \"\"\"\n     Check that the ``p2p_op_list`` is a list of P2POp instances.\n \n@@ -971,17 +973,17 @@ def _is_barrier_after_init() -> int:\n     return int(os.getenv(\"TORCH_DIST_INIT_BARRIER\", \"0\"))\n \n \n-def _get_default_group():\n+def _get_default_group() -> ProcessGroup:\n     \"\"\"Get the default process group created by init_process_group.\"\"\"\n     if not is_initialized():\n         raise ValueError(\n             \"Default process group has not been initialized, \"\n             \"please make sure to call init_process_group.\"\n         )\n-    return GroupMember.WORLD\n+    return not_none(GroupMember.WORLD)\n \n \n-def _get_default_store():\n+def _get_default_store() -> Store:\n     \"\"\"Get the default store created by init_process_group.\"\"\"\n     if not is_initialized():\n         raise ValueError(\n@@ -993,7 +995,7 @@ def _get_default_store():\n     return default_store\n \n \n-def _update_default_pg(pg):\n+def _update_default_pg(pg) -> None:\n     _world.default_pg = pg\n     rank = pg.rank() if pg is not None and pg != GroupMember.NON_GROUP_MEMBER else -1\n     torch._C._distributed_c10d._set_global_rank(rank)\n@@ -1018,10 +1020,9 @@ def get_backend_config(group: Optional[ProcessGroup] = None) -> str:\n     if _rank_not_in_group(pg):\n         raise ValueError(\"Invalid process group specified\")\n     backend_config = _world.pg_backend_config.get(pg)\n-    assert backend_config is not None\n-    return str(backend_config)\n+    return str(not_none(backend_config))\n \n-def get_backend(group: Optional[ProcessGroup] = None) -> str:\n+def get_backend(group: Optional[ProcessGroup] = None) -> Backend:\n     \"\"\"\n     Return the backend of the given process group.\n \n@@ -1041,14 +1042,12 @@ def get_backend(group: Optional[ProcessGroup] = None) -> str:\n     if _rank_not_in_group(pg):\n         raise ValueError(\"Invalid process group specified\")\n     pg_store = _world.pg_map[pg] if pg in _world.pg_map else None\n-    assert pg_store is not None\n-    return pg_store[0]\n-\n+    return Backend(not_none(pg_store)[0])\n \n-_exception_logger\n+@_exception_logger\n @_time_logger\n def init_process_group(\n-    backend: Union[str, Backend] = None,\n+    backend: Optional[str] = None,\n     init_method: Optional[str] = None,\n     timeout: Optional[timedelta] = None,\n     world_size: int = -1,\n@@ -1057,7 +1056,7 @@ def init_process_group(\n     group_name: str = \"\",\n     pg_options: Optional[Any] = None,\n     device_id: Optional[torch.device] = None,\n-):\n+) -> None:\n     \"\"\"\n     Initialize the default distributed process group.\n \n@@ -1173,7 +1172,7 @@ def init_process_group(\n         # backward compatible API\n         if store is None:\n             rendezvous_iterator = rendezvous(\n-                init_method, rank, world_size, timeout=timeout\n+                not_none(init_method), rank, world_size, timeout=timeout\n             )\n             store, rank, world_size = next(rendezvous_iterator)\n             store.set_timeout(timeout)\n@@ -1196,7 +1195,7 @@ def init_process_group(\n         _update_default_pg(default_pg)\n \n     _world.pg_group_ranks[GroupMember.WORLD] = {i: i for i in range(GroupMember.WORLD.size())}  # type: ignore[attr-defined, index]\n-    _backend = _world.pg_map[GroupMember.WORLD][0]  # type: ignore[index]\n+    _backend = _world.pg_map[not_none(GroupMember.WORLD)][0]\n     _default_pg_init_method = init_method\n \n     if _is_barrier_after_init() == 1:\n@@ -1296,8 +1295,8 @@ def _new_process_group_helper(\n     # entire world or if we have bound a device id to the world (which\n     # causes early connection initialization).\n     if (is_initialized() and\n-            (len(global_ranks_in_group) == _world.default_pg.size() or _world.default_pg.bound_device_id)):\n-        split_from = _get_split_source(_world.default_pg)\n+            (len(global_ranks_in_group) == _get_default_group().size() or _get_default_group().bound_device_id)):\n+        split_from = _get_split_source(_get_default_group())\n     else:\n         split_from = None\n \n@@ -1313,7 +1312,7 @@ def _new_process_group_helper(\n             # a requirement of the NCCL API as otherwise we would get\n             # out of sync.\n             if split_from:\n-                split_from.perform_nocolor_split(_world.default_pg.bound_device_id)\n+                split_from.perform_nocolor_split(_get_default_group().bound_device_id)\n             return GroupMember.NON_GROUP_MEMBER, None\n \n     prefix_store = PrefixStore(f\"{group_name}/\", store)\n@@ -1323,6 +1322,7 @@ def _new_process_group_helper(\n     if device_id:\n         pg.bound_device_id = device_id\n     backend_config = BackendConfig(backend)\n+    backend_class: ProcessGroup\n     for device, backend_str in backend_config.get_device_backend_map().items():\n         # Use the group name as prefix in the default store, such that\n         # a single store can be reused by multiple groups.\n@@ -1338,7 +1338,7 @@ def _new_process_group_helper(\n             backend_class = ProcessGroupMPI.create(global_ranks_in_group)\n             backend_type = ProcessGroup.BackendType.MPI\n             if not backend_class:\n-                return GroupMember.NON_GROUP_MEMBER\n+                return GroupMember.NON_GROUP_MEMBER, None\n             # create new process group with accurate rank and size\n             if pg.rank() == -1 and pg.size() == -1:\n                 pg = ProcessGroup(backend_prefix_store, backend_class.rank(), backend_class.size(), base_pg_options)\n@@ -1403,8 +1403,13 @@ def _new_process_group_helper(\n                 backend_class = creator_fn(dist_backend_opts, pg_options)\n \n         # Set sequence numbers for gloo and nccl backends.\n-        if backend_str in [Backend.GLOO, Backend.NCCL]:\n+        if backend_str == Backend.GLOO:\n+            assert isinstance(backend_class, ProcessGroupGloo)\n             backend_class._set_sequence_number_for_group()\n+        elif backend_str == Backend.NCCL:\n+            assert isinstance(backend_class, ProcessGroupNCCL)\n+            backend_class._set_sequence_number_for_group()\n+\n         # If the type is a subclass of ProcessGroup then return this process group immediately\n         # TODO: This defaults to the old behavior for PythonProcessGroups which overwrites the\n         # ProcessGroup instance\n@@ -1592,7 +1597,7 @@ def get_world_size(group: Optional[ProcessGroup] = None) -> int:\n     return _get_group_size(group)\n \n \n-def isend(tensor: torch.Tensor, dst: int, group: Optional[ProcessGroup] = None, tag: int = 0) -> Work:\n+def isend(tensor: torch.Tensor, dst: int, group: Optional[ProcessGroup] = None, tag: int = 0) -> Optional[Work]:\n     \"\"\"\n     Send a tensor asynchronously.\n \n@@ -1618,17 +1623,17 @@ def isend(tensor: torch.Tensor, dst: int, group: Optional[ProcessGroup] = None,\n     _check_single_tensor(tensor, \"tensor\")\n     if _rank_not_in_group(group):\n         _warn_not_in_group(\"isend\")\n-        return\n+        return None\n \n     if group is None or group is GroupMember.WORLD:\n-        default_pg = _get_default_group()\n-        return default_pg.send([tensor], dst, tag)\n+        pg = _get_default_group()\n     else:\n-        group_dst_rank = get_group_rank(group, dst)\n-        return group.send([tensor], group_dst_rank, tag)\n+        pg = group\n+        dst = get_group_rank(pg, dst)\n \n+    return pg.send([tensor], dst, tag)\n \n-def irecv(tensor: torch.Tensor, src: Optional[int] = None, group: Optional[ProcessGroup] = None, tag: int = 0) -> Work:\n+def irecv(tensor: torch.Tensor, src: Optional[int] = None, group: Optional[ProcessGroup] = None, tag: int = 0) -> Optional[Work]:\n     \"\"\"\n     Receives a tensor asynchronously.\n \n@@ -1651,7 +1656,7 @@ def irecv(tensor: torch.Tensor, src: Optional[int] = None, group: Optional[Proce\n     _check_single_tensor(tensor, \"tensor\")\n     if _rank_not_in_group(group):\n         _warn_not_in_group(\"irecv\")\n-        return\n+        return None\n \n     if group is None or group is GroupMember.WORLD:\n         pg = _get_default_group()\n@@ -1690,7 +1695,7 @@ def send(tensor: torch.Tensor, dst: int, group: Optional[ProcessGroup] = None, t\n     _check_single_tensor(tensor, \"tensor\")\n     if _rank_not_in_group(group):\n         _warn_not_in_group(\"send\")\n-        return\n+        return None\n \n     if group is None or group is GroupMember.WORLD:\n         default_pg = _get_default_group()\n@@ -1817,25 +1822,25 @@ def _coalescing_manager(\n             tensors = []\n             for op in op_list:\n                 tensors.append(op.tensor)\n-            opts = AllreduceCoalescedOptions()\n-            opts.reduceOp = op_list[0].redop\n-            work = group.allreduce_coalesced(tensors, opts)\n+            all_reduce_opts = AllreduceCoalescedOptions()\n+            all_reduce_opts.reduceOp = not_none(op_list[0].redop)\n+            work = group.allreduce_coalesced(tensors, all_reduce_opts)\n         elif op0 == all_gather_into_tensor:\n             inputs = []\n             outputs = []\n             for op in op_list:\n                 inputs.append(op.tensor)\n-                outputs.append(op.dst_tensor)\n+                outputs.append(not_none(op.dst_tensor))\n             work = group.allgather_into_tensor_coalesced(outputs, inputs)\n         elif op0 == reduce_scatter_tensor:\n             inputs = []\n             outputs = []\n             for op in op_list:\n                 inputs.append(op.tensor)\n-                outputs.append(op.dst_tensor)\n-                opts = ReduceScatterOptions()\n-                opts.reduceOp = op_list[0].redop\n-            work = group.reduce_scatter_tensor_coalesced(outputs, inputs, opts)\n+                outputs.append(not_none(op.dst_tensor))\n+            reduce_opts = ReduceScatterOptions()\n+            reduce_opts.reduceOp = not_none(op_list[0].redop)\n+            work = group.reduce_scatter_tensor_coalesced(outputs, inputs, reduce_opts)\n         else:\n             raise AssertionError(\n                 f\"Coalescing manager does not support fast-path coalescing of {op0}, \"\n@@ -3978,7 +3983,7 @@ def new_subgroups_by_enumeration(\n     return cur_subgroup, subgroups\n \n \n-def _find_pg_by_ranks_and_tag(tag: str, ranks: List[int]) -> ProcessGroup:\n+def _find_pg_by_ranks_and_tag(tag: str, ranks: List[int]) -> Optional[ProcessGroup]:\n     if len(tag) > 0 and not tag.startswith(\"ptd:\") and not tag.startswith(\"user:\"):\n         tag = f\"user:{tag}\"\n \n"
        },
        {
            "name": "_typing_utils.py",
            "path": "torch/utils/_typing_utils.py",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 13,
                    "hunk": "@@ -0,0 +1,13 @@\n+\"\"\"Miscellaneous utilities to aid with typing.\"\"\"\n+\n+from typing import Optional, TypeVar\n+\n+# Helper to turn Optional[T] into T when we know None either isn't\n+# possible or should trigger an exception.\n+T = TypeVar(\"T\")\n+\n+\n+def not_none(obj: Optional[T]) -> T:\n+    if obj is None:\n+        raise TypeError(\"Invariant encountered: value was None when it should not be\")\n+    return obj\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+\"\"\"Miscellaneous utilities to aid with typing.\"\"\"\n+\n+from typing import Optional, TypeVar\n+\n+# Helper to turn Optional[T] into T when we know None either isn't\n+# possible or should trigger an exception.\n+T = TypeVar(\"T\")\n+\n+\n+def not_none(obj: Optional[T]) -> T:\n+    if obj is None:\n+        raise TypeError(\"Invariant encountered: value was None when it should not be\")\n+    return obj\n",
            "whole_hunk": "@@ -0,0 +1,13 @@\n+\"\"\"Miscellaneous utilities to aid with typing.\"\"\"\n+\n+from typing import Optional, TypeVar\n+\n+# Helper to turn Optional[T] into T when we know None either isn't\n+# possible or should trigger an exception.\n+T = TypeVar(\"T\")\n+\n+\n+def not_none(obj: Optional[T]) -> T:\n+    if obj is None:\n+        raise TypeError(\"Invariant encountered: value was None when it should not be\")\n+    return obj\n"
        },
        {
            "name": "datapipe.pyi.in",
            "path": "torch/utils/data/datapipes/datapipe.pyi.in",
            "patches": [
                {
                    "old_start": 65,
                    "old_length": 6,
                    "new_start": 65,
                    "new_length": 7,
                    "hunk": "@@ -65,6 +65,7 @@ class IterDataPipe(IterableDataset[T_co], metaclass=_IterDataPipeMeta):\n \n class DFIterDataPipe(IterDataPipe):\n     def _is_dfpipe(self): ...\n+    def __iter__(self): ...\n \n class _DataPipeSerializationWrapper:\n     def __init__(self, datapipe): ..."
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    def __iter__(self): ...\n",
            "whole_hunk": "@@ -65,6 +65,7 @@ class IterDataPipe(IterableDataset[T_co], metaclass=_IterDataPipeMeta):\n \n class DFIterDataPipe(IterDataPipe):\n     def _is_dfpipe(self): ...\n+    def __iter__(self): ...\n \n class _DataPipeSerializationWrapper:\n     def __init__(self, datapipe): ..."
        }
    ]
},
{
    "Id": 412,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/7b442c2b0ae0d9c944a777d7352135f370837c15",
    "date": "2023-11-18T03:55:52+00:00",
    "message": "limit fused kernel num args. (#113131)\n\nFixes #97361\n\nWhen fused kernel more than 1024 parameters, it should throw error from ctypes.\nLimit args number is should be a mechanism to protect stack memory. As we known, CPP is passing args via stack memory, and stack memory has size limitation.\n\nCode change:\n\n1. cpp backend will check the fused nodes' args number, if it is reach the limitation. It will status flush status to ready.\n2. scheduler will check `ready_to_flush` API and help backend flush codegen.\n3. Add `ready_to_flush` API to `BaseScheduling`, Triton backend will return False due to not support it yet.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113131\nApproved by: https://github.com/jgong5, https://github.com/mlazos",
    "label": "YES",
    "changes": [
        {
            "name": "test_torchinductor.py",
            "path": "test/inductor/test_torchinductor.py",
            "patches": [
                {
                    "old_start": 7712,
                    "old_length": 6,
                    "new_start": 7712,
                    "new_length": 24,
                    "hunk": "@@ -7712,6 +7712,24 @@ class CommonTemplate:\n         b = torch.randn(65, 2**24, device=self.device)\n         fn(a, b)\n \n+    def test_fuse_large_params(self):\n+        def pt2_optimizer_step(optimizer):\n+            @torch.compile()\n+            def f():\n+                optimizer.step()\n+\n+            f()\n+\n+        params = [\n+            torch.rand(10, 10, dtype=torch.float32, device=self.device)\n+            for _ in range(194)\n+        ]\n+        for p in params:\n+            p.grad = torch.rand_like(p)\n+\n+        o = torch.optim.AdamW(params)\n+        pt2_optimizer_step(o)\n+\n     def test_adaptive_avg_pool1d_argmax(self):\n         # https://github.com/pytorch/pytorch/issues/113013\n         def fn(x):\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    def test_fuse_large_params(self):\n+        def pt2_optimizer_step(optimizer):\n+            @torch.compile()\n+            def f():\n+                optimizer.step()\n+\n+            f()\n+\n+        params = [\n+            torch.rand(10, 10, dtype=torch.float32, device=self.device)\n+            for _ in range(194)\n+        ]\n+        for p in params:\n+            p.grad = torch.rand_like(p)\n+\n+        o = torch.optim.AdamW(params)\n+        pt2_optimizer_step(o)\n+\n",
            "whole_hunk": "@@ -7712,6 +7712,24 @@ class CommonTemplate:\n         b = torch.randn(65, 2**24, device=self.device)\n         fn(a, b)\n \n+    def test_fuse_large_params(self):\n+        def pt2_optimizer_step(optimizer):\n+            @torch.compile()\n+            def f():\n+                optimizer.step()\n+\n+            f()\n+\n+        params = [\n+            torch.rand(10, 10, dtype=torch.float32, device=self.device)\n+            for _ in range(194)\n+        ]\n+        for p in params:\n+            p.grad = torch.rand_like(p)\n+\n+        o = torch.optim.AdamW(params)\n+        pt2_optimizer_step(o)\n+\n     def test_adaptive_avg_pool1d_argmax(self):\n         # https://github.com/pytorch/pytorch/issues/113013\n         def fn(x):\n"
        },
        {
            "name": "cpp.py",
            "path": "torch/_inductor/codegen/cpp.py",
            "patches": [
                {
                    "old_start": 2809,
                    "old_length": 9,
                    "new_start": 2809,
                    "new_length": 18,
                    "hunk": "@@ -2809,9 +2809,18 @@ class CppKernelProxy(CppKernel):\n \n \n class CppScheduling(BaseScheduling):\n+    # ctypes limits the number of args to 1024, refer to:\n+    # https://github.com/python/cpython/commit/a285af7e626d1b81cf09f8b2bf7656f100bc1237\n+    # We set a conservative threshold here.\n+    MAX_FUSED_KERNEL_ARGS_NUM = 500\n+\n     def __init__(self, scheduler):\n         self.scheduler = scheduler\n         self.get_kernel_group()\n+        self._ready_to_flush = False\n+\n+    def _set_flush_status(self, status: bool):\n+        self._ready_to_flush = status\n \n     def group_fn(self, sizes):\n         return tuple(tuple(map(V.graph.sizevars.simplify, s)) for s in sizes)\n"
                },
                {
                    "old_start": 2858,
                    "old_length": 12,
                    "new_start": 2867,
                    "new_length": 23,
                    "hunk": "@@ -2858,12 +2867,23 @@ class CppScheduling(BaseScheduling):\n \n         kernel_group.finalize_kernel(cpp_kernel_proxy, nodes)\n \n+        args_num = self._get_scheduled_num_args()\n+        if args_num > CppScheduling.MAX_FUSED_KERNEL_ARGS_NUM:\n+            self._set_flush_status(True)\n+\n+    def _get_scheduled_num_args(self):\n+        return self.kernel_group.get_num_args()\n+\n+    def ready_to_flush(self):\n+        return self._ready_to_flush\n+\n     def codegen_sync(self):\n         pass\n \n     def flush(self):\n         self.kernel_group.codegen_define_and_call(V.graph.wrapper_code)\n         self.get_kernel_group()\n+        self._set_flush_status(False)\n \n \n class KernelGroup:\n"
                },
                {
                    "old_start": 2885,
                    "old_length": 6,
                    "new_start": 2905,
                    "new_length": 11,
                    "hunk": "@@ -2885,6 +2905,11 @@ class KernelGroup:\n         ws = self.ws\n         new_kernel.codegen_loops(code, ws)\n \n+    def get_num_args(self):\n+        arg_defs, call_args, arg_types = self.args.cpp_argdefs()\n+        args_num = len(arg_defs)\n+        return args_num\n+\n     def codegen_define_and_call(self, wrapper):\n         self.stack.close()\n         if not self.scheduled_nodes:\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    # ctypes limits the number of args to 1024, refer to:\n+    # https://github.com/python/cpython/commit/a285af7e626d1b81cf09f8b2bf7656f100bc1237\n+    # We set a conservative threshold here.\n+    MAX_FUSED_KERNEL_ARGS_NUM = 500\n+\n+        self._ready_to_flush = False\n+\n+    def _set_flush_status(self, status: bool):\n+        self._ready_to_flush = status\n+        args_num = self._get_scheduled_num_args()\n+        if args_num > CppScheduling.MAX_FUSED_KERNEL_ARGS_NUM:\n+            self._set_flush_status(True)\n+\n+    def _get_scheduled_num_args(self):\n+        return self.kernel_group.get_num_args()\n+\n+    def ready_to_flush(self):\n+        return self._ready_to_flush\n+\n+        self._set_flush_status(False)\n+    def get_num_args(self):\n+        arg_defs, call_args, arg_types = self.args.cpp_argdefs()\n+        args_num = len(arg_defs)\n+        return args_num\n+\n",
            "whole_hunk": "@@ -2809,9 +2809,18 @@ class CppKernelProxy(CppKernel):\n \n \n class CppScheduling(BaseScheduling):\n+    # ctypes limits the number of args to 1024, refer to:\n+    # https://github.com/python/cpython/commit/a285af7e626d1b81cf09f8b2bf7656f100bc1237\n+    # We set a conservative threshold here.\n+    MAX_FUSED_KERNEL_ARGS_NUM = 500\n+\n     def __init__(self, scheduler):\n         self.scheduler = scheduler\n         self.get_kernel_group()\n+        self._ready_to_flush = False\n+\n+    def _set_flush_status(self, status: bool):\n+        self._ready_to_flush = status\n \n     def group_fn(self, sizes):\n         return tuple(tuple(map(V.graph.sizevars.simplify, s)) for s in sizes)\n@@ -2858,12 +2867,23 @@ class CppScheduling(BaseScheduling):\n \n         kernel_group.finalize_kernel(cpp_kernel_proxy, nodes)\n \n+        args_num = self._get_scheduled_num_args()\n+        if args_num > CppScheduling.MAX_FUSED_KERNEL_ARGS_NUM:\n+            self._set_flush_status(True)\n+\n+    def _get_scheduled_num_args(self):\n+        return self.kernel_group.get_num_args()\n+\n+    def ready_to_flush(self):\n+        return self._ready_to_flush\n+\n     def codegen_sync(self):\n         pass\n \n     def flush(self):\n         self.kernel_group.codegen_define_and_call(V.graph.wrapper_code)\n         self.get_kernel_group()\n+        self._set_flush_status(False)\n \n \n class KernelGroup:\n@@ -2885,6 +2905,11 @@ class KernelGroup:\n         ws = self.ws\n         new_kernel.codegen_loops(code, ws)\n \n+    def get_num_args(self):\n+        arg_defs, call_args, arg_types = self.args.cpp_argdefs()\n+        args_num = len(arg_defs)\n+        return args_num\n+\n     def codegen_define_and_call(self, wrapper):\n         self.stack.close()\n         if not self.scheduled_nodes:\n"
        },
        {
            "name": "triton.py",
            "path": "torch/_inductor/codegen/triton.py",
            "patches": [
                {
                    "old_start": 2850,
                    "old_length": 6,
                    "new_start": 2850,
                    "new_length": 9,
                    "hunk": "@@ -2850,6 +2850,9 @@ class TritonScheduling(BaseScheduling):\n     def flush(self):\n         pass\n \n+    def ready_to_flush(self) -> bool:\n+        return False\n+\n     def benchmark_fused_nodes(self, nodes):\n         _, (numel, rnumel) = max(nodes, key=lambda x: int(x.is_reduction())).group\n         node_schedule = self.generate_node_schedule(nodes, numel, rnumel)\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    def ready_to_flush(self) -> bool:\n+        return False\n+\n",
            "whole_hunk": "@@ -2850,6 +2850,9 @@ class TritonScheduling(BaseScheduling):\n     def flush(self):\n         pass\n \n+    def ready_to_flush(self) -> bool:\n+        return False\n+\n     def benchmark_fused_nodes(self, nodes):\n         _, (numel, rnumel) = max(nodes, key=lambda x: int(x.is_reduction())).group\n         node_schedule = self.generate_node_schedule(nodes, numel, rnumel)\n"
        },
        {
            "name": "scheduler.py",
            "path": "torch/_inductor/scheduler.py",
            "patches": [
                {
                    "old_start": 2134,
                    "old_length": 6,
                    "new_start": 2134,
                    "new_length": 7,
                    "hunk": "@@ -2134,6 +2134,7 @@ class Scheduler:\n     @dynamo_timed\n     def codegen(self):\n         for node in self.nodes:\n+            device = None\n             try:\n                 log.debug(\n                     \"Generating code for node %s with estimated runtime %f\",\n"
                },
                {
                    "old_start": 2189,
                    "old_length": 6,
                    "new_start": 2190,
                    "new_length": 9,
                    "hunk": "@@ -2189,6 +2190,9 @@ class Scheduler:\n \n             self.available_buffer_names.update(node.get_names())\n \n+            if device is not None and self.get_backend(device).ready_to_flush():\n+                self.flush()\n+\n         self.flush()\n \n     def is_unaligned_buffer(self, buf_name):\n"
                },
                {
                    "old_start": 2245,
                    "old_length": 6,
                    "new_start": 2249,
                    "new_length": 13,
                    "hunk": "@@ -2245,6 +2249,13 @@ class BaseScheduling:\n         \"\"\"\n         raise NotImplementedError()\n \n+    def ready_to_flush(self) -> bool:\n+        \"\"\"\n+        Check whether the backend is request scheduler flush the generated kernel.\n+        If not support, please return False.\n+        \"\"\"\n+        return False\n+\n     def flush(self):\n         \"\"\"\n         Flush the generated kernel and python wrapper code to the source code file."
                }
            ],
            "whole_deleted": "",
            "whole_added": "+            device = None\n+            if device is not None and self.get_backend(device).ready_to_flush():\n+                self.flush()\n+\n+    def ready_to_flush(self) -> bool:\n+        \"\"\"\n+        Check whether the backend is request scheduler flush the generated kernel.\n+        If not support, please return False.\n+        \"\"\"\n+        return False\n+\n",
            "whole_hunk": "@@ -2134,6 +2134,7 @@ class Scheduler:\n     @dynamo_timed\n     def codegen(self):\n         for node in self.nodes:\n+            device = None\n             try:\n                 log.debug(\n                     \"Generating code for node %s with estimated runtime %f\",\n@@ -2189,6 +2190,9 @@ class Scheduler:\n \n             self.available_buffer_names.update(node.get_names())\n \n+            if device is not None and self.get_backend(device).ready_to_flush():\n+                self.flush()\n+\n         self.flush()\n \n     def is_unaligned_buffer(self, buf_name):\n@@ -2245,6 +2249,13 @@ class BaseScheduling:\n         \"\"\"\n         raise NotImplementedError()\n \n+    def ready_to_flush(self) -> bool:\n+        \"\"\"\n+        Check whether the backend is request scheduler flush the generated kernel.\n+        If not support, please return False.\n+        \"\"\"\n+        return False\n+\n     def flush(self):\n         \"\"\"\n         Flush the generated kernel and python wrapper code to the source code file."
        }
    ]
},
{
    "Id": 90,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/ede74940a150632bf28ed9d1728e929ee06d6d94",
    "date": "2024-06-13T01:06:34+00:00",
    "message": "optimize vec isa check dispatch logical. (#128320)\n\nOptimize cpu vec isa check dispatch by archecture, it makes code easy to read and maintaince.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128320\nApproved by: https://github.com/jgong5, https://github.com/desertfire",
    "label": "NO",
    "changes": [
        {
            "name": "codecache.py",
            "path": "torch/_inductor/codecache.py",
            "patches": [
                {
                    "old_start": 1479,
                    "old_length": 11,
                    "new_start": 1479,
                    "new_length": 13,
                    "hunk": "@@ -1479,11 +1479,13 @@ def valid_vec_isa_list() -> List[VecISA]:\n     if sys.platform == \"darwin\" and platform.processor() == \"arm\":\n         return [VecNEON()]\n \n+    isa_list: List[VecISA] = []\n     cur_os = sys.platform\n     if cur_os != \"linux\" and cur_os != \"win32\":\n-        return []\n+        return isa_list\n \n-    if platform.machine() == \"s390x\":\n+    arch = platform.machine()\n+    if arch == \"s390x\":\n         with open(\"/proc/cpuinfo\") as _cpu_info:\n             while True:\n                 line = _cpu_info.readline()\n"
                },
                {
                    "old_start": 1494,
                    "old_length": 14,
                    "new_start": 1496,
                    "new_length": 20,
                    "hunk": "@@ -1494,14 +1496,20 @@ def valid_vec_isa_list() -> List[VecISA]:\n                 if featuresmatch:\n                     for group in featuresmatch.groups():\n                         if re.search(r\"[\\^ ]+vxe[\\$ ]+\", group):\n-                            return [VecZVECTOR()]\n-        return []\n-\n-    isa_list = []\n-    _cpu_supported_isa = x86_isa_checker()\n-    for isa in supported_vec_isa_list:\n-        if str(isa) in _cpu_supported_isa and isa:\n-            isa_list.append(isa)\n+                            isa_list.append(VecZVECTOR())\n+                            break\n+        return isa_list\n+\n+    if arch == \"x86_64\" or arch == \"AMD64\":\n+        \"\"\"\n+        arch value is x86_64 on Linux, and the value is AMD64 on Windows.\n+        \"\"\"\n+        _cpu_supported_x86_isa = x86_isa_checker()\n+        for isa in supported_vec_isa_list:\n+            if str(isa) in _cpu_supported_x86_isa and isa:\n+                isa_list.append(isa)\n+        return isa_list\n+\n     return isa_list\n \n "
                }
            ],
            "whole_deleted": "-        return []\n-    if platform.machine() == \"s390x\":\n-                            return [VecZVECTOR()]\n-        return []\n-\n-    isa_list = []\n-    _cpu_supported_isa = x86_isa_checker()\n-    for isa in supported_vec_isa_list:\n-        if str(isa) in _cpu_supported_isa and isa:\n-            isa_list.append(isa)\n",
            "whole_added": "+    isa_list: List[VecISA] = []\n+        return isa_list\n+    arch = platform.machine()\n+    if arch == \"s390x\":\n+                            isa_list.append(VecZVECTOR())\n+                            break\n+        return isa_list\n+\n+    if arch == \"x86_64\" or arch == \"AMD64\":\n+        \"\"\"\n+        arch value is x86_64 on Linux, and the value is AMD64 on Windows.\n+        \"\"\"\n+        _cpu_supported_x86_isa = x86_isa_checker()\n+        for isa in supported_vec_isa_list:\n+            if str(isa) in _cpu_supported_x86_isa and isa:\n+                isa_list.append(isa)\n+        return isa_list\n+\n",
            "whole_hunk": "@@ -1479,11 +1479,13 @@ def valid_vec_isa_list() -> List[VecISA]:\n     if sys.platform == \"darwin\" and platform.processor() == \"arm\":\n         return [VecNEON()]\n \n+    isa_list: List[VecISA] = []\n     cur_os = sys.platform\n     if cur_os != \"linux\" and cur_os != \"win32\":\n-        return []\n+        return isa_list\n \n-    if platform.machine() == \"s390x\":\n+    arch = platform.machine()\n+    if arch == \"s390x\":\n         with open(\"/proc/cpuinfo\") as _cpu_info:\n             while True:\n                 line = _cpu_info.readline()\n@@ -1494,14 +1496,20 @@ def valid_vec_isa_list() -> List[VecISA]:\n                 if featuresmatch:\n                     for group in featuresmatch.groups():\n                         if re.search(r\"[\\^ ]+vxe[\\$ ]+\", group):\n-                            return [VecZVECTOR()]\n-        return []\n-\n-    isa_list = []\n-    _cpu_supported_isa = x86_isa_checker()\n-    for isa in supported_vec_isa_list:\n-        if str(isa) in _cpu_supported_isa and isa:\n-            isa_list.append(isa)\n+                            isa_list.append(VecZVECTOR())\n+                            break\n+        return isa_list\n+\n+    if arch == \"x86_64\" or arch == \"AMD64\":\n+        \"\"\"\n+        arch value is x86_64 on Linux, and the value is AMD64 on Windows.\n+        \"\"\"\n+        _cpu_supported_x86_isa = x86_isa_checker()\n+        for isa in supported_vec_isa_list:\n+            if str(isa) in _cpu_supported_x86_isa and isa:\n+                isa_list.append(isa)\n+        return isa_list\n+\n     return isa_list\n \n "
        }
    ]
},
{
    "Id": 78,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/17abbafdfc6935bcc133e5f43ba32d44914fe316",
    "date": "2024-06-18T03:25:20+00:00",
    "message": "[inductor] Fix some windows cpp builder issue (#128765)\n\n1. fix some Windows build args.\n2. fix c++20 likely issue on Windows, reference: https://github.com/pytorch/pytorch/pull/124997.\n3. remove compiler return value check, different compilers return variant value, let's check exception to catch error.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128765\nApproved by: https://github.com/jgong5, https://github.com/jansel",
    "label": "YES",
    "changes": [
        {
            "name": "codecache.py",
            "path": "torch/_inductor/codecache.py",
            "patches": [
                {
                    "old_start": 1376,
                    "old_length": 8,
                    "new_start": 1376,
                    "new_length": 6,
                    "hunk": "@@ -1376,8 +1376,6 @@ cdll.LoadLibrary(\"__lib_path__\")\n                 output_path = x86_isa_help_builder.get_target_file_path()\n                 if not os.path.isfile(output_path):\n                     status, target_file = x86_isa_help_builder.build()\n-                    if status:\n-                        return False\n \n                 # Check build result\n                 subprocess.check_call(\n"
                },
                {
                    "old_start": 2573,
                    "old_length": 11,
                    "new_start": 2571,
                    "new_length": 14,
                    "hunk": "@@ -2573,11 +2571,14 @@ class CppPythonBindingsCodeCache(CppCodeCache):\n \n         #ifndef _MSC_VER\n         #if __cplusplus < 202002L\n-        // C++20 earlier code\n+        // C++20 (earlier) code\n         // https://en.cppreference.com/w/cpp/language/attributes/likely\n         #define likely(x)       __builtin_expect(!!(x), 1)\n         #define unlikely(x)     __builtin_expect(!!(x), 0)\n         #endif\n+        #else\n+        #define likely(x) (x)\n+        #define unlikely(x) (x)\n         #endif\n \n         // This is defined in guards.cpp so we don't need to import PyTorch headers that are slooow.\n"
                }
            ],
            "whole_deleted": "-                    if status:\n-                        return False\n-        // C++20 earlier code\n",
            "whole_added": "+        // C++20 (earlier) code\n+        #else\n+        #define likely(x) (x)\n+        #define unlikely(x) (x)\n",
            "whole_hunk": "@@ -1376,8 +1376,6 @@ cdll.LoadLibrary(\"__lib_path__\")\n                 output_path = x86_isa_help_builder.get_target_file_path()\n                 if not os.path.isfile(output_path):\n                     status, target_file = x86_isa_help_builder.build()\n-                    if status:\n-                        return False\n \n                 # Check build result\n                 subprocess.check_call(\n@@ -2573,11 +2571,14 @@ class CppPythonBindingsCodeCache(CppCodeCache):\n \n         #ifndef _MSC_VER\n         #if __cplusplus < 202002L\n-        // C++20 earlier code\n+        // C++20 (earlier) code\n         // https://en.cppreference.com/w/cpp/language/attributes/likely\n         #define likely(x)       __builtin_expect(!!(x), 1)\n         #define unlikely(x)     __builtin_expect(!!(x), 0)\n         #endif\n+        #else\n+        #define likely(x) (x)\n+        #define unlikely(x) (x)\n         #endif\n \n         // This is defined in guards.cpp so we don't need to import PyTorch headers that are slooow.\n"
        },
        {
            "name": "cpp_builder.py",
            "path": "torch/_inductor/cpp_builder.py",
            "patches": [
                {
                    "old_start": 345,
                    "old_length": 7,
                    "new_start": 345,
                    "new_length": 11,
                    "hunk": "@@ -345,7 +345,11 @@ def _get_optimization_cflags() -> List[str]:\n \n def _get_shared_cflag(compile_only: bool) -> List[str]:\n     if _IS_WINDOWS:\n-        SHARED_FLAG = [\"DLL\"]\n+        \"\"\"\n+        MSVC `/MD` using python `ucrtbase.dll` lib as runtime.\n+        https://learn.microsoft.com/en-us/cpp/c-runtime-library/crt-library-features?view=msvc-170\n+        \"\"\"\n+        SHARED_FLAG = [\"DLL\", \"MD\"]\n     else:\n         if compile_only:\n             return [\"fPIC\"]\n"
                },
                {
                    "old_start": 567,
                    "old_length": 7,
                    "new_start": 571,
                    "new_length": 7,
                    "hunk": "@@ -567,7 +571,7 @@ def _get_torch_related_args(include_pytorch: bool, aot_mode: bool):\n     ]\n     libraries_dirs = [TORCH_LIB_PATH]\n     libraries = []\n-    if sys.platform == \"linux\" and not config.is_fbcode():\n+    if sys.platform != \"darwin\" and not config.is_fbcode():\n         libraries = [\"torch\", \"torch_cpu\"]\n         if not aot_mode:\n             libraries.append(\"torch_python\")\n"
                },
                {
                    "old_start": 663,
                    "old_length": 6,
                    "new_start": 667,
                    "new_length": 7,
                    "hunk": "@@ -663,6 +667,7 @@ def _get_openmp_args(cpp_compiler):\n         # msvc openmp: https://learn.microsoft.com/zh-cn/cpp/build/reference/openmp-enable-openmp-2-0-support?view=msvc-170\n \n         cflags.append(\"openmp\")\n+        cflags.append(\"openmp:experimental\")  # MSVC CL\n         libs = []\n     else:\n         if config.is_fbcode():"
                }
            ],
            "whole_deleted": "-        SHARED_FLAG = [\"DLL\"]\n-    if sys.platform == \"linux\" and not config.is_fbcode():\n",
            "whole_added": "+        \"\"\"\n+        MSVC `/MD` using python `ucrtbase.dll` lib as runtime.\n+        https://learn.microsoft.com/en-us/cpp/c-runtime-library/crt-library-features?view=msvc-170\n+        \"\"\"\n+        SHARED_FLAG = [\"DLL\", \"MD\"]\n+    if sys.platform != \"darwin\" and not config.is_fbcode():\n+        cflags.append(\"openmp:experimental\")  # MSVC CL\n",
            "whole_hunk": "@@ -345,7 +345,11 @@ def _get_optimization_cflags() -> List[str]:\n \n def _get_shared_cflag(compile_only: bool) -> List[str]:\n     if _IS_WINDOWS:\n-        SHARED_FLAG = [\"DLL\"]\n+        \"\"\"\n+        MSVC `/MD` using python `ucrtbase.dll` lib as runtime.\n+        https://learn.microsoft.com/en-us/cpp/c-runtime-library/crt-library-features?view=msvc-170\n+        \"\"\"\n+        SHARED_FLAG = [\"DLL\", \"MD\"]\n     else:\n         if compile_only:\n             return [\"fPIC\"]\n@@ -567,7 +571,7 @@ def _get_torch_related_args(include_pytorch: bool, aot_mode: bool):\n     ]\n     libraries_dirs = [TORCH_LIB_PATH]\n     libraries = []\n-    if sys.platform == \"linux\" and not config.is_fbcode():\n+    if sys.platform != \"darwin\" and not config.is_fbcode():\n         libraries = [\"torch\", \"torch_cpu\"]\n         if not aot_mode:\n             libraries.append(\"torch_python\")\n@@ -663,6 +667,7 @@ def _get_openmp_args(cpp_compiler):\n         # msvc openmp: https://learn.microsoft.com/zh-cn/cpp/build/reference/openmp-enable-openmp-2-0-support?view=msvc-170\n \n         cflags.append(\"openmp\")\n+        cflags.append(\"openmp:experimental\")  # MSVC CL\n         libs = []\n     else:\n         if config.is_fbcode():"
        }
    ]
},
{
    "Id": 503,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/d0c8e8240dc04ca85dd071d99be49454a9138de6",
    "date": "2023-09-22T15:11:27+00:00",
    "message": "Revert \"When doing typed typecheck, also check signature with symint removed (#109727)\"\n\nThis reverts commit 56ef200c2dc8a1f1e269861b7a6e02e99d3b56a1.\n\nReverted https://github.com/pytorch/pytorch/pull/109727 on behalf of https://github.com/ezyang due to yolov3 problem ([comment](https://github.com/pytorch/pytorch/pull/109727#issuecomment-1731585002))",
    "label": "YES",
    "changes": [
        {
            "name": "KernelFunction.h",
            "path": "aten/src/ATen/core/boxing/KernelFunction.h",
            "patches": [
                {
                    "old_start": 18,
                    "old_length": 10,
                    "new_start": 18,
                    "new_length": 10,
                    "hunk": "@@ -18,10 +18,10 @@ class KernelFunction;\n template <typename T>\n using has_symint =\n   guts::disjunction<\n-    std::is_same<c10::SymInt, T>,\n-    std::is_same<c10::SymIntArrayRef, T>,\n-    std::is_same<at::OptionalSymIntArrayRef, T>,\n-    std::is_same<c10::optional<c10::SymInt>, T>\n+    std::is_same<c10::SymInt, std::decay_t<T>>,\n+    std::is_same<c10::SymIntArrayRef, std::decay_t<T>>,\n+    std::is_same<at::OptionalSymIntArrayRef, std::decay_t<T>>,\n+    std::is_same<c10::optional<c10::SymInt>, std::decay_t<T>>\n   >;\n \n template <typename T>\n"
                },
                {
                    "old_start": 65,
                    "old_length": 14,
                    "new_start": 65,
                    "new_length": 6,
                    "hunk": "@@ -65,14 +65,6 @@ using fn_has_symint = typename guts::typelist::true_for_any_type<\n   typename guts::infer_function_traits<T>::type::parameter_types\n >;\n \n-template <typename T>\n-struct fn_remove_symint;\n-\n-template <typename Ret, typename... Args>\n-struct fn_remove_symint<Ret(Args...)> {\n-  using type = Ret(typename remove_symint<Args>::type...);\n-};\n-\n /**\n  * KernelFunction is similar to std::function but stores a kernel function.\n  * You can create a KernelFunction from a boxed or unboxed function/functor/lambda\n"
                }
            ],
            "whole_deleted": "-    std::is_same<c10::SymInt, T>,\n-    std::is_same<c10::SymIntArrayRef, T>,\n-    std::is_same<at::OptionalSymIntArrayRef, T>,\n-    std::is_same<c10::optional<c10::SymInt>, T>\n-template <typename T>\n-struct fn_remove_symint;\n-\n-template <typename Ret, typename... Args>\n-struct fn_remove_symint<Ret(Args...)> {\n-  using type = Ret(typename remove_symint<Args>::type...);\n-};\n-\n",
            "whole_added": "+    std::is_same<c10::SymInt, std::decay_t<T>>,\n+    std::is_same<c10::SymIntArrayRef, std::decay_t<T>>,\n+    std::is_same<at::OptionalSymIntArrayRef, std::decay_t<T>>,\n+    std::is_same<c10::optional<c10::SymInt>, std::decay_t<T>>\n",
            "whole_hunk": "@@ -18,10 +18,10 @@ class KernelFunction;\n template <typename T>\n using has_symint =\n   guts::disjunction<\n-    std::is_same<c10::SymInt, T>,\n-    std::is_same<c10::SymIntArrayRef, T>,\n-    std::is_same<at::OptionalSymIntArrayRef, T>,\n-    std::is_same<c10::optional<c10::SymInt>, T>\n+    std::is_same<c10::SymInt, std::decay_t<T>>,\n+    std::is_same<c10::SymIntArrayRef, std::decay_t<T>>,\n+    std::is_same<at::OptionalSymIntArrayRef, std::decay_t<T>>,\n+    std::is_same<c10::optional<c10::SymInt>, std::decay_t<T>>\n   >;\n \n template <typename T>\n@@ -65,14 +65,6 @@ using fn_has_symint = typename guts::typelist::true_for_any_type<\n   typename guts::infer_function_traits<T>::type::parameter_types\n >;\n \n-template <typename T>\n-struct fn_remove_symint;\n-\n-template <typename Ret, typename... Args>\n-struct fn_remove_symint<Ret(Args...)> {\n-  using type = Ret(typename remove_symint<Args>::type...);\n-};\n-\n /**\n  * KernelFunction is similar to std::function but stores a kernel function.\n  * You can create a KernelFunction from a boxed or unboxed function/functor/lambda\n"
        },
        {
            "name": "make_boxed_from_unboxed_functor.h",
            "path": "aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h",
            "patches": [
                {
                    "old_start": 179,
                    "old_length": 6,
                    "new_start": 179,
                    "new_length": 10,
                    "hunk": "@@ -179,6 +179,10 @@ namespace impl {\n       \"You tried to register a kernel with an unsupported input type: std::array<Scalar, N>. Please use std::array<int64_t, N> instead.\");\n   };\n \n+  // The following specialisations of assert_is_valid_input_type are technically not\n+  // necessary since we would hit the base case and show an error message\n+  // there if they didn't exist, but we can show a better error message\n+  // in some common error scenarios.\n   template<class T, bool AllowDeprecatedTypes>\n   struct assert_is_valid_input_type<T, AllowDeprecatedTypes, std::enable_if_t<std::is_same<float, T>::value>> {\n     // There is no reason to support float when we have double. Keep the API lean.\n"
                },
                {
                    "old_start": 200,
                    "old_length": 14,
                    "new_start": 204,
                    "new_length": 6,
                    "hunk": "@@ -200,14 +204,6 @@ namespace impl {\n     static_assert(guts::false_t<T>::value,\n       \"You tried to register a kernel with an unsupported integral input type. Please use int64_t instead.\");\n   };\n-  template<class T, bool AllowDeprecatedTypes>\n-  struct assert_is_valid_input_type<T, AllowDeprecatedTypes, std::enable_if_t<std::is_same<const c10::SymInt&, T>::value>> {\n-    static_assert(guts::false_t<T>::value,\n-      \"You tried to register a kernel taking c10::SymInt by reference. Please accept it by value instead.\");\n-  };\n-\n-  // TODO: it probably would be good to tighten this up quite a bit more with\n-  // an explicit list for everything\n \n   //\n   // assert_is_valid_output_type\n"
                }
            ],
            "whole_deleted": "-  template<class T, bool AllowDeprecatedTypes>\n-  struct assert_is_valid_input_type<T, AllowDeprecatedTypes, std::enable_if_t<std::is_same<const c10::SymInt&, T>::value>> {\n-    static_assert(guts::false_t<T>::value,\n-      \"You tried to register a kernel taking c10::SymInt by reference. Please accept it by value instead.\");\n-  };\n-\n-  // TODO: it probably would be good to tighten this up quite a bit more with\n-  // an explicit list for everything\n",
            "whole_added": "+  // The following specialisations of assert_is_valid_input_type are technically not\n+  // necessary since we would hit the base case and show an error message\n+  // there if they didn't exist, but we can show a better error message\n+  // in some common error scenarios.\n",
            "whole_hunk": "@@ -179,6 +179,10 @@ namespace impl {\n       \"You tried to register a kernel with an unsupported input type: std::array<Scalar, N>. Please use std::array<int64_t, N> instead.\");\n   };\n \n+  // The following specialisations of assert_is_valid_input_type are technically not\n+  // necessary since we would hit the base case and show an error message\n+  // there if they didn't exist, but we can show a better error message\n+  // in some common error scenarios.\n   template<class T, bool AllowDeprecatedTypes>\n   struct assert_is_valid_input_type<T, AllowDeprecatedTypes, std::enable_if_t<std::is_same<float, T>::value>> {\n     // There is no reason to support float when we have double. Keep the API lean.\n@@ -200,14 +204,6 @@ namespace impl {\n     static_assert(guts::false_t<T>::value,\n       \"You tried to register a kernel with an unsupported integral input type. Please use int64_t instead.\");\n   };\n-  template<class T, bool AllowDeprecatedTypes>\n-  struct assert_is_valid_input_type<T, AllowDeprecatedTypes, std::enable_if_t<std::is_same<const c10::SymInt&, T>::value>> {\n-    static_assert(guts::false_t<T>::value,\n-      \"You tried to register a kernel taking c10::SymInt by reference. Please accept it by value instead.\");\n-  };\n-\n-  // TODO: it probably would be good to tighten this up quite a bit more with\n-  // an explicit list for everything\n \n   //\n   // assert_is_valid_output_type\n"
        },
        {
            "name": "OperatorEntry.h",
            "path": "aten/src/ATen/core/dispatch/OperatorEntry.h",
            "patches": [
                {
                    "old_start": 166,
                    "old_length": 10,
                    "new_start": 166,
                    "new_length": 6,
                    "hunk": "@@ -166,10 +166,6 @@ public:\n   template<class FuncType>\n   inline void assertSignatureIsCorrect() {\n     assertSignatureIsCorrect(CppSignature::make<FuncType>(), fn_has_symint<FuncType>::value);\n-    if (fn_has_symint<FuncType>::value) {\n-      static_assert(!fn_has_symint<typename fn_remove_symint<FuncType>::type>::value);\n-      assertSignatureIsCorrect<typename fn_remove_symint<FuncType>::type>();\n-    }\n   }\n \n   void assertSignatureIsCorrect(const CppSignature& call_signature, bool has_symint) const;\n"
                }
            ],
            "whole_deleted": "-    if (fn_has_symint<FuncType>::value) {\n-      static_assert(!fn_has_symint<typename fn_remove_symint<FuncType>::type>::value);\n-      assertSignatureIsCorrect<typename fn_remove_symint<FuncType>::type>();\n-    }\n",
            "whole_added": "",
            "whole_hunk": "@@ -166,10 +166,6 @@ public:\n   template<class FuncType>\n   inline void assertSignatureIsCorrect() {\n     assertSignatureIsCorrect(CppSignature::make<FuncType>(), fn_has_symint<FuncType>::value);\n-    if (fn_has_symint<FuncType>::value) {\n-      static_assert(!fn_has_symint<typename fn_remove_symint<FuncType>::type>::value);\n-      assertSignatureIsCorrect<typename fn_remove_symint<FuncType>::type>();\n-    }\n   }\n \n   void assertSignatureIsCorrect(const CppSignature& call_signature, bool has_symint) const;\n"
        },
        {
            "name": "op_registration_test.cpp",
            "path": "aten/src/ATen/core/op_registration/op_registration_test.cpp",
            "patches": [
                {
                    "old_start": 2157,
                    "old_length": 70,
                    "new_start": 2157,
                    "new_length": 6,
                    "hunk": "@@ -2157,70 +2157,6 @@ TEST(OperatorRegistrationTest, getRegistrationsForDispatchKey) {\n   ASSERT_TRUE(std::includes(all_ops.begin(), all_ops.end(), cpu_ops.begin(), cpu_ops.end(), cmp_lambda));\n }\n \n-Tensor symint_op(const Tensor& self, int64_t length) {\n-  return self.clone();\n-}\n-\n-TEST(OperatorRegistrationTest, TestSymNonSymCompatibility) {\n-  auto m = MAKE_TORCH_LIBRARY(_test);\n-  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n-  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n-  m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op));\n-\n-  auto opHandle = c10::Dispatcher::singleton().findSchemaOrThrow(\n-      \"_test::symint_op\", \"\");\n-\n-  opHandle.typed<Tensor(const Tensor&, int64_t)>().call(dummyTensor(c10::DispatchKey::CPU), 4);\n-  opHandle.typed<Tensor(const Tensor&, c10::SymInt)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n-\n-  expectThrows<c10::Error>([&] {\n-    opHandle.typed<Tensor(const Tensor&, const c10::SymInt&)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n-  }, \"Tried to access or call an operator with a wrong signature\");\n-}\n-\n-Tensor symint_op2(const Tensor& self, c10::SymInt length) {\n-  return self.clone();\n-}\n-\n-TEST(OperatorRegistrationTest, TestSymSymCompatibility) {\n-  auto m = MAKE_TORCH_LIBRARY(_test);\n-  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n-  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n-  m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op2));\n-\n-  auto opHandle = c10::Dispatcher::singleton().findSchemaOrThrow(\n-      \"_test::symint_op\", \"\");\n-\n-  opHandle.typed<Tensor(const Tensor&, int64_t)>().call(dummyTensor(c10::DispatchKey::CPU), 4);\n-  opHandle.typed<Tensor(const Tensor&, c10::SymInt)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n-  // TODO: We should reject this on principle, but today it accidentally works\n-  // due to going through the boxed calling convention.\n-  //\n-  // First, we attempt to test if const SymInt& has SymInt. It does not,\n-  // because we only accept something as SymInt if it has exactly SymInt in\n-  // its signature. So we check if there is a non-symint kernel. But there is\n-  // no non-SymInt kernel, because we only registered a real SymInt kernel.\n-  // When this occurs, we fall back to the boxed calling convention.  And the\n-  // boxed calling convention can deal with const SymInt& fine, as during\n-  // boxing it will just create a SymInt to push onto the argument stack and\n-  // everything is fine.\n-  opHandle.typed<Tensor(const Tensor&, const c10::SymInt&)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n-}\n-\n-Tensor symint_op3(const Tensor& self, const c10::SymInt& length) {\n-  return self.clone();\n-}\n-\n-TEST(OperatorRegistrationTest, TestSymSymRefCompatibility) {\n-  auto m = MAKE_TORCH_LIBRARY(_test);\n-  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n-  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n-\n-  expectThrows<c10::Error>([&] {\n-    m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op3));\n-  }, \"doesn't match the expected function schema\");\n-}\n-\n }\n \n #pragma GCC diagnostic pop"
                }
            ],
            "whole_deleted": "-Tensor symint_op(const Tensor& self, int64_t length) {\n-  return self.clone();\n-}\n-\n-TEST(OperatorRegistrationTest, TestSymNonSymCompatibility) {\n-  auto m = MAKE_TORCH_LIBRARY(_test);\n-  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n-  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n-  m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op));\n-\n-  auto opHandle = c10::Dispatcher::singleton().findSchemaOrThrow(\n-      \"_test::symint_op\", \"\");\n-\n-  opHandle.typed<Tensor(const Tensor&, int64_t)>().call(dummyTensor(c10::DispatchKey::CPU), 4);\n-  opHandle.typed<Tensor(const Tensor&, c10::SymInt)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n-\n-  expectThrows<c10::Error>([&] {\n-    opHandle.typed<Tensor(const Tensor&, const c10::SymInt&)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n-  }, \"Tried to access or call an operator with a wrong signature\");\n-}\n-\n-Tensor symint_op2(const Tensor& self, c10::SymInt length) {\n-  return self.clone();\n-}\n-\n-TEST(OperatorRegistrationTest, TestSymSymCompatibility) {\n-  auto m = MAKE_TORCH_LIBRARY(_test);\n-  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n-  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n-  m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op2));\n-\n-  auto opHandle = c10::Dispatcher::singleton().findSchemaOrThrow(\n-      \"_test::symint_op\", \"\");\n-\n-  opHandle.typed<Tensor(const Tensor&, int64_t)>().call(dummyTensor(c10::DispatchKey::CPU), 4);\n-  opHandle.typed<Tensor(const Tensor&, c10::SymInt)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n-  // TODO: We should reject this on principle, but today it accidentally works\n-  // due to going through the boxed calling convention.\n-  //\n-  // First, we attempt to test if const SymInt& has SymInt. It does not,\n-  // because we only accept something as SymInt if it has exactly SymInt in\n-  // its signature. So we check if there is a non-symint kernel. But there is\n-  // no non-SymInt kernel, because we only registered a real SymInt kernel.\n-  // When this occurs, we fall back to the boxed calling convention.  And the\n-  // boxed calling convention can deal with const SymInt& fine, as during\n-  // boxing it will just create a SymInt to push onto the argument stack and\n-  // everything is fine.\n-  opHandle.typed<Tensor(const Tensor&, const c10::SymInt&)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n-}\n-\n-Tensor symint_op3(const Tensor& self, const c10::SymInt& length) {\n-  return self.clone();\n-}\n-\n-TEST(OperatorRegistrationTest, TestSymSymRefCompatibility) {\n-  auto m = MAKE_TORCH_LIBRARY(_test);\n-  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n-  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n-\n-  expectThrows<c10::Error>([&] {\n-    m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op3));\n-  }, \"doesn't match the expected function schema\");\n-}\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -2157,70 +2157,6 @@ TEST(OperatorRegistrationTest, getRegistrationsForDispatchKey) {\n   ASSERT_TRUE(std::includes(all_ops.begin(), all_ops.end(), cpu_ops.begin(), cpu_ops.end(), cmp_lambda));\n }\n \n-Tensor symint_op(const Tensor& self, int64_t length) {\n-  return self.clone();\n-}\n-\n-TEST(OperatorRegistrationTest, TestSymNonSymCompatibility) {\n-  auto m = MAKE_TORCH_LIBRARY(_test);\n-  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n-  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n-  m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op));\n-\n-  auto opHandle = c10::Dispatcher::singleton().findSchemaOrThrow(\n-      \"_test::symint_op\", \"\");\n-\n-  opHandle.typed<Tensor(const Tensor&, int64_t)>().call(dummyTensor(c10::DispatchKey::CPU), 4);\n-  opHandle.typed<Tensor(const Tensor&, c10::SymInt)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n-\n-  expectThrows<c10::Error>([&] {\n-    opHandle.typed<Tensor(const Tensor&, const c10::SymInt&)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n-  }, \"Tried to access or call an operator with a wrong signature\");\n-}\n-\n-Tensor symint_op2(const Tensor& self, c10::SymInt length) {\n-  return self.clone();\n-}\n-\n-TEST(OperatorRegistrationTest, TestSymSymCompatibility) {\n-  auto m = MAKE_TORCH_LIBRARY(_test);\n-  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n-  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n-  m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op2));\n-\n-  auto opHandle = c10::Dispatcher::singleton().findSchemaOrThrow(\n-      \"_test::symint_op\", \"\");\n-\n-  opHandle.typed<Tensor(const Tensor&, int64_t)>().call(dummyTensor(c10::DispatchKey::CPU), 4);\n-  opHandle.typed<Tensor(const Tensor&, c10::SymInt)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n-  // TODO: We should reject this on principle, but today it accidentally works\n-  // due to going through the boxed calling convention.\n-  //\n-  // First, we attempt to test if const SymInt& has SymInt. It does not,\n-  // because we only accept something as SymInt if it has exactly SymInt in\n-  // its signature. So we check if there is a non-symint kernel. But there is\n-  // no non-SymInt kernel, because we only registered a real SymInt kernel.\n-  // When this occurs, we fall back to the boxed calling convention.  And the\n-  // boxed calling convention can deal with const SymInt& fine, as during\n-  // boxing it will just create a SymInt to push onto the argument stack and\n-  // everything is fine.\n-  opHandle.typed<Tensor(const Tensor&, const c10::SymInt&)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n-}\n-\n-Tensor symint_op3(const Tensor& self, const c10::SymInt& length) {\n-  return self.clone();\n-}\n-\n-TEST(OperatorRegistrationTest, TestSymSymRefCompatibility) {\n-  auto m = MAKE_TORCH_LIBRARY(_test);\n-  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n-  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n-\n-  expectThrows<c10::Error>([&] {\n-    m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op3));\n-  }, \"doesn't match the expected function schema\");\n-}\n-\n }\n \n #pragma GCC diagnostic pop"
        }
    ]
},
{
    "Id": 311,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/05ac295177ba885165108b03f7a35155ceee6904",
    "date": "2024-02-02T09:02:04+00:00",
    "message": "[export] Fix bug with user input mutations (#118942)\n\nWe hit an edge case where the graph exporting contains placeholder nodes whose names conflict with names from aot_export, we don't update the user_inputs_to_mutate in the graph signature correctly.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/118942\nApproved by: https://github.com/tugsbayasgalan, https://github.com/zhxchen17",
    "label": "YES",
    "changes": [
        {
            "name": "test_export.py",
            "path": "test/export/test_export.py",
            "patches": [
                {
                    "old_start": 2540,
                    "old_length": 6,
                    "new_start": 2540,
                    "new_length": 28,
                    "hunk": "@@ -2540,6 +2540,28 @@ def forward(self, l_x_):\n         self.assertEqual(inputs[0][0] * 2.0, inputs_model[0][0])\n         self.assertEqual(inputs[0][0] * 2.0, inputs_export[0][0])\n \n+    def test_export_input_mutation_bug(self):\n+        class M(torch.nn.Module):\n+            def forward(self, x):\n+                x[:, :2, :] = x[:,:2,:] + 1\n+                return x\n+\n+        inputs = (torch.ones(4,4,4),)\n+        ep = torch.export.export(M(), inputs)\n+        m = ep.module()\n+\n+        # Make the name conflict with a placeholder name that we get from\n+        # aot_export\n+        for i, node in enumerate(m.graph.nodes):\n+            if node.op == \"placeholder\":\n+                node.name = f\"arg0_{i + 1}\"\n+        m.recompile()\n+\n+        ep = torch.export.export(m, inputs)\n+\n+        inputs = (torch.randn(4,4,4),)\n+        self.assertEqual(ep.module()(*copy.deepcopy(inputs)), M()(*copy.deepcopy(inputs)))\n+\n     def test__scaled_dot_product_flash_attention(self):\n         class Module(torch.nn.Module):\n             def forward(self, q, k, v):\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    def test_export_input_mutation_bug(self):\n+        class M(torch.nn.Module):\n+            def forward(self, x):\n+                x[:, :2, :] = x[:,:2,:] + 1\n+                return x\n+\n+        inputs = (torch.ones(4,4,4),)\n+        ep = torch.export.export(M(), inputs)\n+        m = ep.module()\n+\n+        # Make the name conflict with a placeholder name that we get from\n+        # aot_export\n+        for i, node in enumerate(m.graph.nodes):\n+            if node.op == \"placeholder\":\n+                node.name = f\"arg0_{i + 1}\"\n+        m.recompile()\n+\n+        ep = torch.export.export(m, inputs)\n+\n+        inputs = (torch.randn(4,4,4),)\n+        self.assertEqual(ep.module()(*copy.deepcopy(inputs)), M()(*copy.deepcopy(inputs)))\n+\n",
            "whole_hunk": "@@ -2540,6 +2540,28 @@ def forward(self, l_x_):\n         self.assertEqual(inputs[0][0] * 2.0, inputs_model[0][0])\n         self.assertEqual(inputs[0][0] * 2.0, inputs_export[0][0])\n \n+    def test_export_input_mutation_bug(self):\n+        class M(torch.nn.Module):\n+            def forward(self, x):\n+                x[:, :2, :] = x[:,:2,:] + 1\n+                return x\n+\n+        inputs = (torch.ones(4,4,4),)\n+        ep = torch.export.export(M(), inputs)\n+        m = ep.module()\n+\n+        # Make the name conflict with a placeholder name that we get from\n+        # aot_export\n+        for i, node in enumerate(m.graph.nodes):\n+            if node.op == \"placeholder\":\n+                node.name = f\"arg0_{i + 1}\"\n+        m.recompile()\n+\n+        ep = torch.export.export(m, inputs)\n+\n+        inputs = (torch.randn(4,4,4),)\n+        self.assertEqual(ep.module()(*copy.deepcopy(inputs)), M()(*copy.deepcopy(inputs)))\n+\n     def test__scaled_dot_product_flash_attention(self):\n         class Module(torch.nn.Module):\n             def forward(self, q, k, v):\n"
        },
        {
            "name": "_trace.py",
            "path": "torch/export/_trace.py",
            "patches": [
                {
                    "old_start": 368,
                    "old_length": 7,
                    "new_start": 368,
                    "new_length": 9,
                    "hunk": "@@ -368,7 +368,9 @@ def _lift_buffers_to_user_inputs(\n         i: b for i, b in graph_signature.inputs_to_buffers.items() if b not in names\n     }\n     user_inputs_to_mutate = {\n-        o: b for o, b in graph_signature.buffers_to_mutate.items() if b in names\n+        o: new_node_names[b]\n+        for o, b in graph_signature.buffers_to_mutate.items()\n+        if b in names\n     }\n     graph_signature.buffers_to_mutate = {\n         o: b for o, b in graph_signature.buffers_to_mutate.items() if b not in names"
                }
            ],
            "whole_deleted": "-        o: b for o, b in graph_signature.buffers_to_mutate.items() if b in names\n",
            "whole_added": "+        o: new_node_names[b]\n+        for o, b in graph_signature.buffers_to_mutate.items()\n+        if b in names\n",
            "whole_hunk": "@@ -368,7 +368,9 @@ def _lift_buffers_to_user_inputs(\n         i: b for i, b in graph_signature.inputs_to_buffers.items() if b not in names\n     }\n     user_inputs_to_mutate = {\n-        o: b for o, b in graph_signature.buffers_to_mutate.items() if b in names\n+        o: new_node_names[b]\n+        for o, b in graph_signature.buffers_to_mutate.items()\n+        if b in names\n     }\n     graph_signature.buffers_to_mutate = {\n         o: b for o, b in graph_signature.buffers_to_mutate.items() if b not in names"
        }
    ]
},
{
    "Id": 117,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/cdeb242fc977210e211fd77b217320205c9f4042",
    "date": "2024-05-30T12:29:36+00:00",
    "message": "[inductor] fix mkldnn linear binary fusion check ut (#127296)\n\nIn this PR:\n\n\uff081\uff09Fix the unary fusion for bf16 conv/linear.\n    Previously we registered same fusion pattern for `bf16. fp16`. And we do not check the dtype while matching the pattern. This results the `fp16` case matched the `bf16` pattern but in later replacement, we found that we have a float16 here which is not expected, so we do not fuse them.  We fix it by checking dtypes to avoid `fp16` case matched `bf16` pattern.\n\n```\n  def _is_valid_computation_unary_fusion(computation_op, lowp_dtype=None):\n      def fn(match):\n          matched = _is_single_computation_op(computation_op, **lowp_dtype**)(match) # previously we do not check lowp_dtype here\n\n```\n\nIt is not exposed before because we only check the match count, and the match count is anyway correct because we matched the pattern. To address this, we add check on number of `generated_kernel`. If it is not fused, there will be an additional kernel to compute the post op.\n\n\uff082\uff09Previous the ut\n```\npython test/inductor/test_mkldnn_pattern_matcher.py -k test_linear_binary\n```\ndose not check the fusion status, fix it in this PR.\n\n\uff083\uff09Extend `test_conv_binary` to test with lp.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/127296\nApproved by: https://github.com/leslie-fang-intel, https://github.com/jgong5, https://github.com/jansel",
    "label": "YES",
    "changes": [
        {
            "name": "test_mkldnn_pattern_matcher.py",
            "path": "test/inductor/test_mkldnn_pattern_matcher.py",
            "patches": [
                {
                    "old_start": 10,
                    "old_length": 7,
                    "new_start": 10,
                    "new_length": 7,
                    "hunk": "@@ -10,7 +10,7 @@ import torch.ao.quantization.quantizer.x86_inductor_quantizer as xiq\n from torch._dynamo import config as dynamo_config\n from torch._dynamo.utils import counters\n from torch._export import capture_pre_autograd_graph\n-from torch._inductor import config\n+from torch._inductor import config, metrics\n from torch._inductor.test_case import run_tests, TestCase\n from torch._inductor.utils import run_and_get_code\n from torch.ao.quantization.quantize_pt2e import (\n"
                },
                {
                    "old_start": 264,
                    "old_length": 6,
                    "new_start": 264,
                    "new_length": 7,
                    "hunk": "@@ -264,6 +264,7 @@ class TestPatternMatcher(TestPatternMatcherBase):\n             memory_format,\n             dtype,\n         ) in options:\n+            metrics.reset()\n             if dim == 4:\n                 x_shape = (1, 3, 56, 56)\n             else:\n"
                },
                {
                    "old_start": 284,
                    "old_length": 6,
                    "new_start": 285,
                    "new_length": 18,
                    "hunk": "@@ -284,6 +285,18 @@ class TestPatternMatcher(TestPatternMatcherBase):\n                 # Has extra dtype conversion nodes for autocast.\n                 match_nodes += 2\n             self._test_common(mod, (v,), 2, match_nodes, check_autocast=dtype)\n+            generated_kernel_count = 0\n+            if dtype != torch.float32:\n+                # \"to_dtype\" for input\n+                generated_kernel_count = 1\n+            if memory_format == torch.contiguous_format:\n+                # \"to_dtype + to_channel_last\" for input, \"to_contiguous\" for output\n+                generated_kernel_count = 2\n+            if memory_format == torch.channels_last_3d:\n+                # for float conv3d, the output for eager is channel last, we will generate \"to_contiguous\" for output\n+                # for lp conv3d, the output for eager is channel last too, we will only generate \"to_dtype\"\n+                generated_kernel_count = 1\n+            self.assertEqual(metrics.generated_kernel_count, generated_kernel_count)\n \n     def test_conv2d_unary_cpu(self):\n         self._test_conv_unary_cpu_base(dim=4)\n"
                },
                {
                    "old_start": 321,
                    "old_length": 6,
                    "new_start": 334,
                    "new_length": 7,
                    "hunk": "@@ -321,6 +334,7 @@ class TestPatternMatcher(TestPatternMatcherBase):\n             dtypes.append(torch.float16)\n         options = itertools.product(unary_list, [True, False], dtypes)\n         for unary_fn, bias, dtype in options:\n+            metrics.reset()\n             mod = M(unary_fn, 10, 30, bias=bias).eval()\n             # only fuse for linear when the dtype is bf16\n             mod = mod\n"
                },
                {
                    "old_start": 335,
                    "old_length": 6,
                    "new_start": 349,
                    "new_length": 8,
                    "hunk": "@@ -335,6 +349,8 @@ class TestPatternMatcher(TestPatternMatcherBase):\n             self._test_common(\n                 mod, (v,), matcher_count, matcher_nodes, check_autocast=dtype\n             )\n+            # only generated 1 kernel for \"to\"\n+            self.assertEqual(metrics.generated_kernel_count, 1)\n \n     @unittest.skipIf(not TEST_MKL, \"Test requires MKL\")\n     def test_linear_fp32(self):\n"
                },
                {
                    "old_start": 386,
                    "old_length": 6,
                    "new_start": 402,
                    "new_length": 7,
                    "hunk": "@@ -386,6 +402,7 @@ class TestPatternMatcher(TestPatternMatcherBase):\n         )\n \n         for unary_fn, memory_format, dtype in options:\n+            metrics.reset()\n             x_shape = (1, 3, 28, 28)\n             mod = M(unary_fn).eval()\n \n"
                },
                {
                    "old_start": 401,
                    "old_length": 6,
                    "new_start": 418,
                    "new_length": 14,
                    "hunk": "@@ -401,6 +418,14 @@ class TestPatternMatcher(TestPatternMatcherBase):\n                 # Has extra dtype conversion nodes for autocast.\n                 match_nodes += 2\n             self._test_common(mod, (v,), 2, match_nodes, check_autocast=dtype)\n+            generated_kernel_count = 0\n+            if dtype != torch.float32:\n+                # \"to\" for input\n+                generated_kernel_count = 1\n+            if memory_format == torch.contiguous_format:\n+                # \"to_dtype + to_channel_last\" for input, \"to_contiguous\" for output\n+                generated_kernel_count = 2\n+            self.assertEqual(metrics.generated_kernel_count, generated_kernel_count)\n \n     def _test_conv_binary_base(self, dim=4):\n         assert dim == 4 or dim == 5\n"
                },
                {
                    "old_start": 430,
                    "old_length": 19,
                    "new_start": 455,
                    "new_length": 29,
                    "hunk": "@@ -430,19 +455,29 @@ class TestPatternMatcher(TestPatternMatcherBase):\n                 else:\n                     return self.binary_fn(x1, x2)\n \n+        dtypes = [\n+            torch.float,\n+        ]\n+        if torch.ops.mkldnn._is_mkldnn_bf16_supported():\n+            dtypes.append(torch.bfloat16)\n+        if torch.ops.mkldnn._is_mkldnn_fp16_supported():\n+            dtypes.append(torch.float16)\n         cl_format = torch.channels_last if dim == 4 else torch.channels_last_3d\n         test_memory_format = [torch.contiguous_format, cl_format]\n         options = itertools.product(\n             binary_list,\n             [True, False],\n             test_memory_format,\n+            dtypes,\n         )\n \n         for (\n             binary_fn,\n             has_relu,\n             memory_format,\n+            dtype,\n         ) in options:\n+            metrics.reset()\n             if dim == 4:\n                 x_shape = (1, 3, 56, 56)\n             else:\n"
                },
                {
                    "old_start": 457,
                    "old_length": 7,
                    "new_start": 492,
                    "new_length": 19,
                    "hunk": "@@ -457,7 +492,19 @@ class TestPatternMatcher(TestPatternMatcherBase):\n             match_nodes = binary_list[binary_fn][1]\n             if has_relu:\n                 match_nodes += 1\n-            self._test_common(mod, (v,), match_count, match_nodes + 2)\n+            self._test_common(\n+                mod, (v,), match_count, match_nodes + 2, check_autocast=dtype\n+            )\n+            generated_kernel_count = 0\n+            if dtype != torch.float32:\n+                # \"to_dtype\" for input\n+                generated_kernel_count = 1\n+            if memory_format == torch.contiguous_format:\n+                # \"to_dtype + to_channel_last\" for input, \"to_contiguous\" for output\n+                generated_kernel_count = 2\n+            elif memory_format == torch.channels_last_3d:\n+                generated_kernel_count = 1\n+            self.assertEqual(metrics.generated_kernel_count, generated_kernel_count)\n \n     def test_conv2d_binary(self):\n         self._test_conv_binary_base(dim=4)\n"
                },
                {
                    "old_start": 489,
                    "old_length": 7,
                    "new_start": 536,
                    "new_length": 7,
                    "hunk": "@@ -489,7 +536,7 @@ class TestPatternMatcher(TestPatternMatcherBase):\n         )\n         out_feature = 30\n         for binary_fn, input_shape, bias, dtype in options:\n-            torch._dynamo.reset()\n+            metrics.reset()\n             # addmm(mm) + (linear+add)\n             match_count = 2\n             match_nodes = 3\n"
                },
                {
                    "old_start": 498,
                    "old_length": 13,
                    "new_start": 545,
                    "new_length": 20,
                    "hunk": "@@ -498,13 +545,20 @@ class TestPatternMatcher(TestPatternMatcherBase):\n                 # view + linear + view(joint_graph+freeze pass)\n                 match_count = match_count + 5 if is_inplace else match_count + 3\n                 match_nodes = match_nodes + 7 if is_inplace else match_nodes + 5\n-            mod = M(binary_fn, input_shape[-1], out_feature, bias).to(dtype).eval()\n-            v = torch.randn(input_shape).to(dtype)\n+            mod = M(binary_fn, input_shape[-1], out_feature, bias).eval()\n+            v = torch.randn(input_shape)\n             other = torch.randn(input_shape[:-1] + [out_feature]).to(dtype)\n-            mod_c = torch.compile(mod)\n-            out, code = run_and_get_code(mod_c, v, other)\n-            self.assertEqual(out, mod(v, other), rtol=1e-2, atol=1e-2)\n-            # TODO - assert fusions work code\n+            self._test_common(\n+                mod,\n+                (\n+                    v,\n+                    other,\n+                ),\n+                match_count,\n+                match_nodes,\n+                check_autocast=dtype,\n+            )\n+            self.assertEqual(metrics.generated_kernel_count, 1)\n \n     def test_multi_linear_share_same_input(self):\n         # llama pattern.\n"
                }
            ],
            "whole_deleted": "-from torch._inductor import config\n-            self._test_common(mod, (v,), match_count, match_nodes + 2)\n-            torch._dynamo.reset()\n-            mod = M(binary_fn, input_shape[-1], out_feature, bias).to(dtype).eval()\n-            v = torch.randn(input_shape).to(dtype)\n-            mod_c = torch.compile(mod)\n-            out, code = run_and_get_code(mod_c, v, other)\n-            self.assertEqual(out, mod(v, other), rtol=1e-2, atol=1e-2)\n-            # TODO - assert fusions work code\n",
            "whole_added": "+from torch._inductor import config, metrics\n+            metrics.reset()\n+            generated_kernel_count = 0\n+            if dtype != torch.float32:\n+                # \"to_dtype\" for input\n+                generated_kernel_count = 1\n+            if memory_format == torch.contiguous_format:\n+                # \"to_dtype + to_channel_last\" for input, \"to_contiguous\" for output\n+                generated_kernel_count = 2\n+            if memory_format == torch.channels_last_3d:\n+                # for float conv3d, the output for eager is channel last, we will generate \"to_contiguous\" for output\n+                # for lp conv3d, the output for eager is channel last too, we will only generate \"to_dtype\"\n+                generated_kernel_count = 1\n+            self.assertEqual(metrics.generated_kernel_count, generated_kernel_count)\n+            metrics.reset()\n+            # only generated 1 kernel for \"to\"\n+            self.assertEqual(metrics.generated_kernel_count, 1)\n+            metrics.reset()\n+            generated_kernel_count = 0\n+            if dtype != torch.float32:\n+                # \"to\" for input\n+                generated_kernel_count = 1\n+            if memory_format == torch.contiguous_format:\n+                # \"to_dtype + to_channel_last\" for input, \"to_contiguous\" for output\n+                generated_kernel_count = 2\n+            self.assertEqual(metrics.generated_kernel_count, generated_kernel_count)\n+        dtypes = [\n+            torch.float,\n+        ]\n+        if torch.ops.mkldnn._is_mkldnn_bf16_supported():\n+            dtypes.append(torch.bfloat16)\n+        if torch.ops.mkldnn._is_mkldnn_fp16_supported():\n+            dtypes.append(torch.float16)\n+            dtypes,\n+            dtype,\n+            metrics.reset()\n+            self._test_common(\n+                mod, (v,), match_count, match_nodes + 2, check_autocast=dtype\n+            )\n+            generated_kernel_count = 0\n+            if dtype != torch.float32:\n+                # \"to_dtype\" for input\n+                generated_kernel_count = 1\n+            if memory_format == torch.contiguous_format:\n+                # \"to_dtype + to_channel_last\" for input, \"to_contiguous\" for output\n+                generated_kernel_count = 2\n+            elif memory_format == torch.channels_last_3d:\n+                generated_kernel_count = 1\n+            self.assertEqual(metrics.generated_kernel_count, generated_kernel_count)\n+            metrics.reset()\n+            mod = M(binary_fn, input_shape[-1], out_feature, bias).eval()\n+            v = torch.randn(input_shape)\n+            self._test_common(\n+                mod,\n+                (\n+                    v,\n+                    other,\n+                ),\n+                match_count,\n+                match_nodes,\n+                check_autocast=dtype,\n+            )\n+            self.assertEqual(metrics.generated_kernel_count, 1)\n",
            "whole_hunk": "@@ -10,7 +10,7 @@ import torch.ao.quantization.quantizer.x86_inductor_quantizer as xiq\n from torch._dynamo import config as dynamo_config\n from torch._dynamo.utils import counters\n from torch._export import capture_pre_autograd_graph\n-from torch._inductor import config\n+from torch._inductor import config, metrics\n from torch._inductor.test_case import run_tests, TestCase\n from torch._inductor.utils import run_and_get_code\n from torch.ao.quantization.quantize_pt2e import (\n@@ -264,6 +264,7 @@ class TestPatternMatcher(TestPatternMatcherBase):\n             memory_format,\n             dtype,\n         ) in options:\n+            metrics.reset()\n             if dim == 4:\n                 x_shape = (1, 3, 56, 56)\n             else:\n@@ -284,6 +285,18 @@ class TestPatternMatcher(TestPatternMatcherBase):\n                 # Has extra dtype conversion nodes for autocast.\n                 match_nodes += 2\n             self._test_common(mod, (v,), 2, match_nodes, check_autocast=dtype)\n+            generated_kernel_count = 0\n+            if dtype != torch.float32:\n+                # \"to_dtype\" for input\n+                generated_kernel_count = 1\n+            if memory_format == torch.contiguous_format:\n+                # \"to_dtype + to_channel_last\" for input, \"to_contiguous\" for output\n+                generated_kernel_count = 2\n+            if memory_format == torch.channels_last_3d:\n+                # for float conv3d, the output for eager is channel last, we will generate \"to_contiguous\" for output\n+                # for lp conv3d, the output for eager is channel last too, we will only generate \"to_dtype\"\n+                generated_kernel_count = 1\n+            self.assertEqual(metrics.generated_kernel_count, generated_kernel_count)\n \n     def test_conv2d_unary_cpu(self):\n         self._test_conv_unary_cpu_base(dim=4)\n@@ -321,6 +334,7 @@ class TestPatternMatcher(TestPatternMatcherBase):\n             dtypes.append(torch.float16)\n         options = itertools.product(unary_list, [True, False], dtypes)\n         for unary_fn, bias, dtype in options:\n+            metrics.reset()\n             mod = M(unary_fn, 10, 30, bias=bias).eval()\n             # only fuse for linear when the dtype is bf16\n             mod = mod\n@@ -335,6 +349,8 @@ class TestPatternMatcher(TestPatternMatcherBase):\n             self._test_common(\n                 mod, (v,), matcher_count, matcher_nodes, check_autocast=dtype\n             )\n+            # only generated 1 kernel for \"to\"\n+            self.assertEqual(metrics.generated_kernel_count, 1)\n \n     @unittest.skipIf(not TEST_MKL, \"Test requires MKL\")\n     def test_linear_fp32(self):\n@@ -386,6 +402,7 @@ class TestPatternMatcher(TestPatternMatcherBase):\n         )\n \n         for unary_fn, memory_format, dtype in options:\n+            metrics.reset()\n             x_shape = (1, 3, 28, 28)\n             mod = M(unary_fn).eval()\n \n@@ -401,6 +418,14 @@ class TestPatternMatcher(TestPatternMatcherBase):\n                 # Has extra dtype conversion nodes for autocast.\n                 match_nodes += 2\n             self._test_common(mod, (v,), 2, match_nodes, check_autocast=dtype)\n+            generated_kernel_count = 0\n+            if dtype != torch.float32:\n+                # \"to\" for input\n+                generated_kernel_count = 1\n+            if memory_format == torch.contiguous_format:\n+                # \"to_dtype + to_channel_last\" for input, \"to_contiguous\" for output\n+                generated_kernel_count = 2\n+            self.assertEqual(metrics.generated_kernel_count, generated_kernel_count)\n \n     def _test_conv_binary_base(self, dim=4):\n         assert dim == 4 or dim == 5\n@@ -430,19 +455,29 @@ class TestPatternMatcher(TestPatternMatcherBase):\n                 else:\n                     return self.binary_fn(x1, x2)\n \n+        dtypes = [\n+            torch.float,\n+        ]\n+        if torch.ops.mkldnn._is_mkldnn_bf16_supported():\n+            dtypes.append(torch.bfloat16)\n+        if torch.ops.mkldnn._is_mkldnn_fp16_supported():\n+            dtypes.append(torch.float16)\n         cl_format = torch.channels_last if dim == 4 else torch.channels_last_3d\n         test_memory_format = [torch.contiguous_format, cl_format]\n         options = itertools.product(\n             binary_list,\n             [True, False],\n             test_memory_format,\n+            dtypes,\n         )\n \n         for (\n             binary_fn,\n             has_relu,\n             memory_format,\n+            dtype,\n         ) in options:\n+            metrics.reset()\n             if dim == 4:\n                 x_shape = (1, 3, 56, 56)\n             else:\n@@ -457,7 +492,19 @@ class TestPatternMatcher(TestPatternMatcherBase):\n             match_nodes = binary_list[binary_fn][1]\n             if has_relu:\n                 match_nodes += 1\n-            self._test_common(mod, (v,), match_count, match_nodes + 2)\n+            self._test_common(\n+                mod, (v,), match_count, match_nodes + 2, check_autocast=dtype\n+            )\n+            generated_kernel_count = 0\n+            if dtype != torch.float32:\n+                # \"to_dtype\" for input\n+                generated_kernel_count = 1\n+            if memory_format == torch.contiguous_format:\n+                # \"to_dtype + to_channel_last\" for input, \"to_contiguous\" for output\n+                generated_kernel_count = 2\n+            elif memory_format == torch.channels_last_3d:\n+                generated_kernel_count = 1\n+            self.assertEqual(metrics.generated_kernel_count, generated_kernel_count)\n \n     def test_conv2d_binary(self):\n         self._test_conv_binary_base(dim=4)\n@@ -489,7 +536,7 @@ class TestPatternMatcher(TestPatternMatcherBase):\n         )\n         out_feature = 30\n         for binary_fn, input_shape, bias, dtype in options:\n-            torch._dynamo.reset()\n+            metrics.reset()\n             # addmm(mm) + (linear+add)\n             match_count = 2\n             match_nodes = 3\n@@ -498,13 +545,20 @@ class TestPatternMatcher(TestPatternMatcherBase):\n                 # view + linear + view(joint_graph+freeze pass)\n                 match_count = match_count + 5 if is_inplace else match_count + 3\n                 match_nodes = match_nodes + 7 if is_inplace else match_nodes + 5\n-            mod = M(binary_fn, input_shape[-1], out_feature, bias).to(dtype).eval()\n-            v = torch.randn(input_shape).to(dtype)\n+            mod = M(binary_fn, input_shape[-1], out_feature, bias).eval()\n+            v = torch.randn(input_shape)\n             other = torch.randn(input_shape[:-1] + [out_feature]).to(dtype)\n-            mod_c = torch.compile(mod)\n-            out, code = run_and_get_code(mod_c, v, other)\n-            self.assertEqual(out, mod(v, other), rtol=1e-2, atol=1e-2)\n-            # TODO - assert fusions work code\n+            self._test_common(\n+                mod,\n+                (\n+                    v,\n+                    other,\n+                ),\n+                match_count,\n+                match_nodes,\n+                check_autocast=dtype,\n+            )\n+            self.assertEqual(metrics.generated_kernel_count, 1)\n \n     def test_multi_linear_share_same_input(self):\n         # llama pattern.\n"
        },
        {
            "name": "mkldnn_fusion.py",
            "path": "torch/_inductor/fx_passes/mkldnn_fusion.py",
            "patches": [
                {
                    "old_start": 197,
                    "old_length": 9,
                    "new_start": 197,
                    "new_length": 15,
                    "hunk": "@@ -197,9 +197,15 @@ if torch._C._has_mkldnn:\n     def _binary_fusion_v2(computation_call, binary_fn):\n         return CallFunction(binary_fn, computation_call, KeywordArg(\"other\"))\n \n-    def _is_single_computation_op(computation_op):\n+    def _is_single_computation_op(computation_op, lowp_dtype=None):\n         def fn(match):\n             computation_nodes = filter_nodes(match.nodes, computation_op)\n+\n+            if lowp_dtype:\n+                output_node_meta = match.output_node().meta.get(\"val\")\n+                if output_node_meta.dtype != lowp_dtype:\n+                    return False\n+\n             if len(computation_nodes) < 1:\n                 return False\n             if any(n.args[-3] != \"none\" for n in computation_nodes):\n"
                },
                {
                    "old_start": 210,
                    "old_length": 7,
                    "new_start": 216,
                    "new_length": 7,
                    "hunk": "@@ -210,7 +216,7 @@ if torch._C._has_mkldnn:\n \n     def _is_valid_computation_unary_fusion(computation_op, lowp_dtype=None):\n         def fn(match):\n-            matched = _is_single_computation_op(computation_op)(match)\n+            matched = _is_single_computation_op(computation_op, lowp_dtype)(match)\n             computation_node = filter_nodes(match.nodes, computation_op)[0]\n             if lowp_dtype:\n                 conversion_dtype_nodes = filter_nodes(\n"
                },
                {
                    "old_start": 249,
                    "old_length": 7,
                    "new_start": 255,
                    "new_length": 7,
                    "hunk": "@@ -249,7 +255,7 @@ if torch._C._has_mkldnn:\n \n     def _register_leaky_relu_fusion_lowering(pattern, computation_op, lowp_dtype=None):\n         @register_lowering_pattern(\n-            pattern, extra_check=_is_single_computation_op(computation_op)\n+            pattern, extra_check=_is_single_computation_op(computation_op, lowp_dtype)\n         )\n         def fn(match, *args, **kwargs):\n             negative_slope = kwargs.get(\"negative_slope\")\n"
                },
                {
                    "old_start": 291,
                    "old_length": 7,
                    "new_start": 297,
                    "new_length": 7,
                    "hunk": "@@ -291,7 +297,7 @@ if torch._C._has_mkldnn:\n \n     def _register_hardtanh_fusion_lowering(pattern, computation_op, lowp_dtype=None):\n         @register_lowering_pattern(\n-            pattern, extra_check=_is_single_computation_op(computation_op)\n+            pattern, extra_check=_is_single_computation_op(computation_op, lowp_dtype)\n         )\n         def fn(match, *args, **kwargs):\n             min_value = kwargs.get(\"min_value\")"
                }
            ],
            "whole_deleted": "-    def _is_single_computation_op(computation_op):\n-            matched = _is_single_computation_op(computation_op)(match)\n-            pattern, extra_check=_is_single_computation_op(computation_op)\n-            pattern, extra_check=_is_single_computation_op(computation_op)\n",
            "whole_added": "+    def _is_single_computation_op(computation_op, lowp_dtype=None):\n+\n+            if lowp_dtype:\n+                output_node_meta = match.output_node().meta.get(\"val\")\n+                if output_node_meta.dtype != lowp_dtype:\n+                    return False\n+\n+            matched = _is_single_computation_op(computation_op, lowp_dtype)(match)\n+            pattern, extra_check=_is_single_computation_op(computation_op, lowp_dtype)\n+            pattern, extra_check=_is_single_computation_op(computation_op, lowp_dtype)\n",
            "whole_hunk": "@@ -197,9 +197,15 @@ if torch._C._has_mkldnn:\n     def _binary_fusion_v2(computation_call, binary_fn):\n         return CallFunction(binary_fn, computation_call, KeywordArg(\"other\"))\n \n-    def _is_single_computation_op(computation_op):\n+    def _is_single_computation_op(computation_op, lowp_dtype=None):\n         def fn(match):\n             computation_nodes = filter_nodes(match.nodes, computation_op)\n+\n+            if lowp_dtype:\n+                output_node_meta = match.output_node().meta.get(\"val\")\n+                if output_node_meta.dtype != lowp_dtype:\n+                    return False\n+\n             if len(computation_nodes) < 1:\n                 return False\n             if any(n.args[-3] != \"none\" for n in computation_nodes):\n@@ -210,7 +216,7 @@ if torch._C._has_mkldnn:\n \n     def _is_valid_computation_unary_fusion(computation_op, lowp_dtype=None):\n         def fn(match):\n-            matched = _is_single_computation_op(computation_op)(match)\n+            matched = _is_single_computation_op(computation_op, lowp_dtype)(match)\n             computation_node = filter_nodes(match.nodes, computation_op)[0]\n             if lowp_dtype:\n                 conversion_dtype_nodes = filter_nodes(\n@@ -249,7 +255,7 @@ if torch._C._has_mkldnn:\n \n     def _register_leaky_relu_fusion_lowering(pattern, computation_op, lowp_dtype=None):\n         @register_lowering_pattern(\n-            pattern, extra_check=_is_single_computation_op(computation_op)\n+            pattern, extra_check=_is_single_computation_op(computation_op, lowp_dtype)\n         )\n         def fn(match, *args, **kwargs):\n             negative_slope = kwargs.get(\"negative_slope\")\n@@ -291,7 +297,7 @@ if torch._C._has_mkldnn:\n \n     def _register_hardtanh_fusion_lowering(pattern, computation_op, lowp_dtype=None):\n         @register_lowering_pattern(\n-            pattern, extra_check=_is_single_computation_op(computation_op)\n+            pattern, extra_check=_is_single_computation_op(computation_op, lowp_dtype)\n         )\n         def fn(match, *args, **kwargs):\n             min_value = kwargs.get(\"min_value\")"
        }
    ]
},
{
    "Id": 83,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/f1ee3589a1277bdc9c580612868a92e7ba27e39a",
    "date": "2024-06-16T07:35:57+00:00",
    "message": "[Inductor] Emit strided block pointer from ModularIndexing and FloorDiv (#127342)\n\n**Summary**\n\nInductor currently uses modulo and division to compute indices into certain multi-dimensional tensors, such as those arising from row padding. This PR matches on that indexing pattern, replacing it with an N-D block pointer. This should be more efficient than computing indices with division and modulo, and it can easily map to DMAs on non-GPU hardware targets.\n\nBecause the 1D block size needs to map to an integer block shape in ND, we need to know that the ND block size evenly divides the size of the iteration range. This PR only generates ND block pointers when it can guarantee that the iteration order and number of elements loaded are unchanged. This means that the number of elements in a slice of the iteration range must either be:\n  - Powers of 2. Since Triton block sizes are powers of 2, any integer power of 2 either divides the block size, or is greater than the block size. In the latter case, `CielDiv(x, y)` rounds up to 1.\n  - Multiples of the maximum block size. Since block sizes are powers of 2, the maximum block size is a multiple of every possible block size.\n\nNote that a *slice* of the iteration range does not include the leading dimension. Thus we can support arbitrary leading dimensions like `(5,8)`.\n\nFeature proposal and discussion: https://github.com/pytorch/pytorch/issues/125077\n\nExample kernel:\n```\ntriton.jit\ndef triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 4096\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    tmp0 = tl.reshape(tl.load(tl.make_block_ptr(in_ptr0, shape=[32, 16, 8], strides=[1024, 32, 1], block_shape=[32 * (32 <= ((127 + XBLOCK) // 128)) + ((127 + XBLOCK) // 128) * (((127 + XBLOCK) // 128) < 32), 16 * (16 <= ((7 + XBLOCK) // 8)) + ((7 + XBLOCK) // 8) * (((7 + XBLOCK) // 8) < 16), 8 * (8 <= XBLOCK) + XBLOCK * (XBLOCK < 8)], order=[0, 1, 2], offsets=[(xoffset // 128), (xoffset // 8) % 16, xoffset % 8]), boundary_check=[0, 1, 2]), [XBLOCK])\n    tmp1 = tmp0 + tmp0\n    tl.store(tl.make_block_ptr(out_ptr0, shape=[4096], strides=[1], block_shape=[XBLOCK], order=[0], offsets=[xoffset]), tl.broadcast_to(tmp1, [XBLOCK]).to(tl.float32))\n''', device_str='cuda')\n```\n\n**Test Plan**\n\nThis PR adds a new CI test script to cover this feature. The tests can be grouped into a few main categories:\n  - Can we generate strided block pointers for the appropriate shapes?\n     - Powers of 2\n     - Non-power of 2, but multiple of the maximum block size\n     - Arbitrary leading dimensions, with power of 2 inner dimensions\n     - Weird strides and offsets\n     - Reductions\n     - Symbolic shapes that are multiples of the maximum block size (wasn't able to trace this through dynamo)\n     - Broadcasts (some variables are missing from the indexing expression)\n  - Do we still compile other cases correctly, even if we don't expect to be able to generate block pointers?\n     - Unsupported static shapes\n     - Unsupported symbolic shapes\n  - Mixing and matching these cases:\n     - Pointwise and reduction in the same kernel\n  - Sanity check the test harness\n     - Do we raise an exception if the expected number of block pointers and the actual number are different?\n\n**Follow-ups**\n\nThere are a few important cases which this PR can't handle. I'm hoping these can be deferred to follow-up PRs:\n  - Handle non-divisible shapes\n      - Change the tiling algorithm to generate a 2D (X,Y) blocking, if doing so enables block pointers to be emitted.\n      - Pad unsupported loads up to the nearest divisible size, then mask/slice out the extra elements? This is probably the best solution, but I'm not yet sure how to go about it in triton.\n - Take advantage of this analysis when `triton.use_block_ptr=False`. I'm guessing we can still avoid `%` and `/` without requiring block pointers. Maybe we could compute block indices with arange and broadcast instead?\n\nDifferential Revision: D56739375\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/127342\nApproved by: https://github.com/jansel, https://github.com/shunting314",
    "label": "NO",
    "changes": [
        {
            "name": "test_codecache.py",
            "path": "test/inductor/test_codecache.py",
            "patches": [
                {
                    "old_start": 36,
                    "old_length": 6,
                    "new_start": 36,
                    "new_length": 7,
                    "hunk": "@@ -36,6 +36,7 @@ from torch.testing._internal.inductor_utils import (\n     HAS_CUDA,\n     HAS_GPU,\n     HAS_MULTIGPU,\n+    requires_gpu,\n )\n from torch.utils._triton import has_triton\n \n"
                },
                {
                    "old_start": 46,
                    "old_length": 7,
                    "new_start": 47,
                    "new_length": 6,
                    "hunk": "@@ -46,7 +47,6 @@ if HAS_TRITON:\n \n     from torch.testing._internal.triton_utils import add_kernel\n \n-requires_gpu = functools.partial(unittest.skipIf, not HAS_GPU, \"requires gpu\")\n requires_triton = functools.partial(unittest.skipIf, not HAS_TRITON, \"requires triton\")\n \n torch._dynamo.config.fake_tensor_cache_enabled = True\n"
                }
            ],
            "whole_deleted": "-requires_gpu = functools.partial(unittest.skipIf, not HAS_GPU, \"requires gpu\")\n",
            "whole_added": "+    requires_gpu,\n",
            "whole_hunk": "@@ -36,6 +36,7 @@ from torch.testing._internal.inductor_utils import (\n     HAS_CUDA,\n     HAS_GPU,\n     HAS_MULTIGPU,\n+    requires_gpu,\n )\n from torch.utils._triton import has_triton\n \n@@ -46,7 +47,6 @@ if HAS_TRITON:\n \n     from torch.testing._internal.triton_utils import add_kernel\n \n-requires_gpu = functools.partial(unittest.skipIf, not HAS_GPU, \"requires gpu\")\n requires_triton = functools.partial(unittest.skipIf, not HAS_TRITON, \"requires triton\")\n \n torch._dynamo.config.fake_tensor_cache_enabled = True\n"
        },
        {
            "name": "test_indexing.py",
            "path": "test/inductor/test_indexing.py",
            "patches": [
                {
                    "old_start": 336,
                    "old_length": 19,
                    "new_start": 336,
                    "new_length": 21,
                    "hunk": "@@ -336,19 +336,21 @@ class ExprPrinterTests(InductorTestCase):\n \n     def test_print_Min_Max(self):\n         cases = (\n-            (sympy.Min, \"min\"),\n-            (sympy.Max, \"max\"),\n+            (sympy.Min, \"min\", \"<\"),\n+            (sympy.Max, \"max\", \">\"),\n         )\n-        for f, s in cases:\n+        for f, s, cmp in cases:\n             x = sympy.Symbol(\"x\", integer=True)\n             expr = f(-2, x)\n-            self.assertEqual(texpr(expr), f\"tl.{s}imum(-2, x)\")\n+            self.assertEqual(\n+                texpr(expr), f\"((-2) * ((-2) {cmp}= (x)) + (x) * ((x) {cmp} (-2)))\"\n+            )\n             self.assertEqual(cexpr(expr), f\"std::{s}(-2L, x)\")\n \n             expr = f(x, 2 * x, 3 * x)\n             self.assertEqual(\n                 texpr(expr),\n-                f\"tl.{s}imum(x, tl.{s}imum(2*x, 3*x))\",\n+                f\"((x) * ((x) {cmp}= (((2*x) * ((2*x) {cmp}= (3*x)) + (3*x) * ((3*x) {cmp} (2*x))))) + (((2*x) * ((2*x) {cmp}= (3*x)) + (3*x) * ((3*x) {cmp} (2*x)))) * ((((2*x) * ((2*x) {cmp}= (3*x)) + (3*x) * ((3*x) {cmp} (2*x)))) {cmp} (x)))\",  # noqa: B950 line too long\n             )\n             self.assertEqual(cexpr(expr), f\"std::{s}({{x, 2L*x, 3L*x}})\")\n \n"
                }
            ],
            "whole_deleted": "-            (sympy.Min, \"min\"),\n-            (sympy.Max, \"max\"),\n-        for f, s in cases:\n-            self.assertEqual(texpr(expr), f\"tl.{s}imum(-2, x)\")\n-                f\"tl.{s}imum(x, tl.{s}imum(2*x, 3*x))\",\n",
            "whole_added": "+            (sympy.Min, \"min\", \"<\"),\n+            (sympy.Max, \"max\", \">\"),\n+        for f, s, cmp in cases:\n+            self.assertEqual(\n+                texpr(expr), f\"((-2) * ((-2) {cmp}= (x)) + (x) * ((x) {cmp} (-2)))\"\n+            )\n+                f\"((x) * ((x) {cmp}= (((2*x) * ((2*x) {cmp}= (3*x)) + (3*x) * ((3*x) {cmp} (2*x))))) + (((2*x) * ((2*x) {cmp}= (3*x)) + (3*x) * ((3*x) {cmp} (2*x)))) * ((((2*x) * ((2*x) {cmp}= (3*x)) + (3*x) * ((3*x) {cmp} (2*x)))) {cmp} (x)))\",  # noqa: B950 line too long\n",
            "whole_hunk": "@@ -336,19 +336,21 @@ class ExprPrinterTests(InductorTestCase):\n \n     def test_print_Min_Max(self):\n         cases = (\n-            (sympy.Min, \"min\"),\n-            (sympy.Max, \"max\"),\n+            (sympy.Min, \"min\", \"<\"),\n+            (sympy.Max, \"max\", \">\"),\n         )\n-        for f, s in cases:\n+        for f, s, cmp in cases:\n             x = sympy.Symbol(\"x\", integer=True)\n             expr = f(-2, x)\n-            self.assertEqual(texpr(expr), f\"tl.{s}imum(-2, x)\")\n+            self.assertEqual(\n+                texpr(expr), f\"((-2) * ((-2) {cmp}= (x)) + (x) * ((x) {cmp} (-2)))\"\n+            )\n             self.assertEqual(cexpr(expr), f\"std::{s}(-2L, x)\")\n \n             expr = f(x, 2 * x, 3 * x)\n             self.assertEqual(\n                 texpr(expr),\n-                f\"tl.{s}imum(x, tl.{s}imum(2*x, 3*x))\",\n+                f\"((x) * ((x) {cmp}= (((2*x) * ((2*x) {cmp}= (3*x)) + (3*x) * ((3*x) {cmp} (2*x))))) + (((2*x) * ((2*x) {cmp}= (3*x)) + (3*x) * ((3*x) {cmp} (2*x)))) * ((((2*x) * ((2*x) {cmp}= (3*x)) + (3*x) * ((3*x) {cmp} (2*x)))) {cmp} (x)))\",  # noqa: B950 line too long\n             )\n             self.assertEqual(cexpr(expr), f\"std::{s}({{x, 2L*x, 3L*x}})\")\n \n"
        },
        {
            "name": "test_torchinductor.py",
            "path": "test/inductor/test_torchinductor.py",
            "patches": [
                {
                    "old_start": 123,
                    "old_length": 6,
                    "new_start": 123,
                    "new_length": 7,
                    "hunk": "@@ -123,6 +123,7 @@ from torch.testing._internal.inductor_utils import (\n     HAS_CPU,\n     HAS_GPU,\n     HAS_MULTIGPU,\n+    requires_gpu,\n     skipCPUIf,\n     skipCUDAIf,\n )\n"
                },
                {
                    "old_start": 130,
                    "old_length": 7,
                    "new_start": 131,
                    "new_length": 6,
                    "hunk": "@@ -130,7 +131,6 @@ from torch.testing._internal.inductor_utils import (\n HAS_AVX2 = \"fbgemm\" in torch.backends.quantized.supported_engines\n \n aten = torch.ops.aten\n-requires_gpu = functools.partial(unittest.skipIf, not HAS_GPU, \"requires gpu\")\n \n requires_multigpu = functools.partial(\n     unittest.skipIf, not HAS_MULTIGPU, f\"requires multiple {GPU_TYPE} devices\"\n"
                },
                {
                    "old_start": 9030,
                    "old_length": 12,
                    "new_start": 9030,
                    "new_length": 13,
                    "hunk": "@@ -9030,12 +9030,13 @@ class CommonTemplate:\n         assertGeneratedKernelCountEqual(self, 0)\n \n     @requires_gpu()\n+    @parametrize(\"use_block_ptr\", (False, True))\n     @unittest.skipIf(\n         not PLATFORM_SUPPORTS_FLASH_ATTENTION,\n         \"Does not support SDPA or pre-SM80 hardware\",\n     )\n     @skipIfRocm\n-    def test_sdpa(self):\n+    def test_sdpa(self, use_block_ptr):\n         def foo(arg0_1, arg1_1, arg2_1, arg3_1, arg4_1):\n             view = torch.ops.aten.view.default(arg3_1, [23760, 128])\n             arg3_1 = None\n"
                },
                {
                    "old_start": 9067,
                    "old_length": 6,
                    "new_start": 9068,
                    "new_length": 9,
                    "hunk": "@@ -9067,6 +9068,9 @@ class CommonTemplate:\n             _scaled_dot_product_efficient_attention = None\n             return (getitem,)\n \n+        if self.device == \"cpu\":\n+            raise unittest.SkipTest(f\"requires {GPU_TYPE}\")\n+\n         DEVICE = torch.device(f\"{GPU_TYPE}:0\")\n         DTYPE = torch.float16\n         B = 3\n"
                },
                {
                    "old_start": 9082,
                    "old_length": 13,
                    "new_start": 9086,
                    "new_length": 22,
                    "hunk": "@@ -9082,13 +9086,22 @@ class CommonTemplate:\n         value = torch.randn((B, H, K, D), device=DEVICE, dtype=DTYPE)\n         bias = torch.randn((B, Q, K, C_bias), device=DEVICE, dtype=DTYPE)\n         weights = torch.randn((C_bias, H), device=DEVICE, dtype=DTYPE)\n+        inps = (query, key, value, bias, weights)\n \n-        self.common(\n-            foo,\n-            (query, key, value, bias, weights),\n-            atol=0.02,\n-            rtol=1e4,\n-        )\n+        with config.patch(\"triton.use_block_ptr\", use_block_ptr):\n+            # Check accuracy\n+            self.common(\n+                foo,\n+                inps,\n+                atol=0.02,\n+                rtol=1e4,\n+            )\n+\n+            # Check code for block pointers\n+            foo_opt = torch._dynamo.optimize(\"inductor\")(foo)\n+            code = run_and_get_triton_code(foo_opt, *inps)\n+            have_block_ptr = code.count(\"tl.make_block_ptr\") > 0\n+            self.assertEqual(have_block_ptr, use_block_ptr)\n \n     @requires_gpu()\n     @unittest.skipIf(\n"
                },
                {
                    "old_start": 11045,
                    "old_length": 8,
                    "new_start": 11058,
                    "new_length": 8,
                    "hunk": "@@ -11045,8 +11058,8 @@ if HAS_GPU and not TEST_WITH_ASAN:\n                 self.assertExpectedInline(\n                     \"\\n\".join(lines),\n                     \"\"\"\\\n-        tmp0 = tl.load(in_ptr0 + (x1 + (512*x0) + (262144*r2)), rmask, eviction_policy='evict_last', other=0.0)\n-        tmp1 = tl.load(block_ptr0, boundary_check=[1], padding_option='zero', eviction_policy='evict_first')\"\"\",\n+        tmp0 = tl.reshape(tl.load(block_ptr0, boundary_check=[3], padding_option='zero', eviction_policy='evict_last'), [XBLOCK, RBLOCK])\n+        tmp1 = tl.load(block_ptr1, boundary_check=[1], padding_option='zero', eviction_policy='evict_first')\"\"\",  # noqa: B950 line too long\n                 )\n \n         # Disable index propagation, so the indirect indexing isn't optimized away\n"
                }
            ],
            "whole_deleted": "-requires_gpu = functools.partial(unittest.skipIf, not HAS_GPU, \"requires gpu\")\n-    def test_sdpa(self):\n-        self.common(\n-            foo,\n-            (query, key, value, bias, weights),\n-            atol=0.02,\n-            rtol=1e4,\n-        )\n-        tmp0 = tl.load(in_ptr0 + (x1 + (512*x0) + (262144*r2)), rmask, eviction_policy='evict_last', other=0.0)\n-        tmp1 = tl.load(block_ptr0, boundary_check=[1], padding_option='zero', eviction_policy='evict_first')\"\"\",\n",
            "whole_added": "+    requires_gpu,\n+    @parametrize(\"use_block_ptr\", (False, True))\n+    def test_sdpa(self, use_block_ptr):\n+        if self.device == \"cpu\":\n+            raise unittest.SkipTest(f\"requires {GPU_TYPE}\")\n+\n+        inps = (query, key, value, bias, weights)\n+        with config.patch(\"triton.use_block_ptr\", use_block_ptr):\n+            # Check accuracy\n+            self.common(\n+                foo,\n+                inps,\n+                atol=0.02,\n+                rtol=1e4,\n+            )\n+\n+            # Check code for block pointers\n+            foo_opt = torch._dynamo.optimize(\"inductor\")(foo)\n+            code = run_and_get_triton_code(foo_opt, *inps)\n+            have_block_ptr = code.count(\"tl.make_block_ptr\") > 0\n+            self.assertEqual(have_block_ptr, use_block_ptr)\n+        tmp0 = tl.reshape(tl.load(block_ptr0, boundary_check=[3], padding_option='zero', eviction_policy='evict_last'), [XBLOCK, RBLOCK])\n+        tmp1 = tl.load(block_ptr1, boundary_check=[1], padding_option='zero', eviction_policy='evict_first')\"\"\",  # noqa: B950 line too long\n",
            "whole_hunk": "@@ -123,6 +123,7 @@ from torch.testing._internal.inductor_utils import (\n     HAS_CPU,\n     HAS_GPU,\n     HAS_MULTIGPU,\n+    requires_gpu,\n     skipCPUIf,\n     skipCUDAIf,\n )\n@@ -130,7 +131,6 @@ from torch.testing._internal.inductor_utils import (\n HAS_AVX2 = \"fbgemm\" in torch.backends.quantized.supported_engines\n \n aten = torch.ops.aten\n-requires_gpu = functools.partial(unittest.skipIf, not HAS_GPU, \"requires gpu\")\n \n requires_multigpu = functools.partial(\n     unittest.skipIf, not HAS_MULTIGPU, f\"requires multiple {GPU_TYPE} devices\"\n@@ -9030,12 +9030,13 @@ class CommonTemplate:\n         assertGeneratedKernelCountEqual(self, 0)\n \n     @requires_gpu()\n+    @parametrize(\"use_block_ptr\", (False, True))\n     @unittest.skipIf(\n         not PLATFORM_SUPPORTS_FLASH_ATTENTION,\n         \"Does not support SDPA or pre-SM80 hardware\",\n     )\n     @skipIfRocm\n-    def test_sdpa(self):\n+    def test_sdpa(self, use_block_ptr):\n         def foo(arg0_1, arg1_1, arg2_1, arg3_1, arg4_1):\n             view = torch.ops.aten.view.default(arg3_1, [23760, 128])\n             arg3_1 = None\n@@ -9067,6 +9068,9 @@ class CommonTemplate:\n             _scaled_dot_product_efficient_attention = None\n             return (getitem,)\n \n+        if self.device == \"cpu\":\n+            raise unittest.SkipTest(f\"requires {GPU_TYPE}\")\n+\n         DEVICE = torch.device(f\"{GPU_TYPE}:0\")\n         DTYPE = torch.float16\n         B = 3\n@@ -9082,13 +9086,22 @@ class CommonTemplate:\n         value = torch.randn((B, H, K, D), device=DEVICE, dtype=DTYPE)\n         bias = torch.randn((B, Q, K, C_bias), device=DEVICE, dtype=DTYPE)\n         weights = torch.randn((C_bias, H), device=DEVICE, dtype=DTYPE)\n+        inps = (query, key, value, bias, weights)\n \n-        self.common(\n-            foo,\n-            (query, key, value, bias, weights),\n-            atol=0.02,\n-            rtol=1e4,\n-        )\n+        with config.patch(\"triton.use_block_ptr\", use_block_ptr):\n+            # Check accuracy\n+            self.common(\n+                foo,\n+                inps,\n+                atol=0.02,\n+                rtol=1e4,\n+            )\n+\n+            # Check code for block pointers\n+            foo_opt = torch._dynamo.optimize(\"inductor\")(foo)\n+            code = run_and_get_triton_code(foo_opt, *inps)\n+            have_block_ptr = code.count(\"tl.make_block_ptr\") > 0\n+            self.assertEqual(have_block_ptr, use_block_ptr)\n \n     @requires_gpu()\n     @unittest.skipIf(\n@@ -11045,8 +11058,8 @@ if HAS_GPU and not TEST_WITH_ASAN:\n                 self.assertExpectedInline(\n                     \"\\n\".join(lines),\n                     \"\"\"\\\n-        tmp0 = tl.load(in_ptr0 + (x1 + (512*x0) + (262144*r2)), rmask, eviction_policy='evict_last', other=0.0)\n-        tmp1 = tl.load(block_ptr0, boundary_check=[1], padding_option='zero', eviction_policy='evict_first')\"\"\",\n+        tmp0 = tl.reshape(tl.load(block_ptr0, boundary_check=[3], padding_option='zero', eviction_policy='evict_last'), [XBLOCK, RBLOCK])\n+        tmp1 = tl.load(block_ptr1, boundary_check=[1], padding_option='zero', eviction_policy='evict_first')\"\"\",  # noqa: B950 line too long\n                 )\n \n         # Disable index propagation, so the indirect indexing isn't optimized away\n"
        },
        {
            "name": "test_torchinductor_strided_blocks.py",
            "path": "test/inductor/test_torchinductor_strided_blocks.py",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 361,
                    "hunk": "@@ -0,0 +1,361 @@\n+# Owner(s): [\"module: inductor\"]\n+import contextlib\n+import importlib\n+import unittest\n+from typing import Any, Callable, Optional, Tuple\n+\n+import torch\n+import torch.utils._pytree as pytree\n+from torch._inductor import config\n+from torch._inductor.runtime.hints import TRITON_MAX_BLOCK\n+from torch._inductor.runtime.runtime_utils import is_power_of_2\n+from torch._inductor.test_case import TestCase as InductorTestCase\n+from torch._inductor.utils import run_and_get_code\n+from torch.testing._internal.common_utils import (\n+    instantiate_parametrized_tests,\n+    parametrize,\n+)\n+from torch.testing._internal.inductor_utils import (\n+    GPU_TYPE,\n+    HAS_GPU,\n+    requires_gpu,\n+    skip_windows_ci,\n+)\n+\n+\n+skip_windows_ci(__name__, __file__)\n+\n+importlib.import_module(\"filelock\")\n+\n+max_block: int = TRITON_MAX_BLOCK[\"X\"]\n+\n+\n+@requires_gpu()\n+@config.patch(\"triton.use_block_ptr\", True)\n+@instantiate_parametrized_tests\n+class TritonBlockPointerTest(InductorTestCase):\n+    def run_and_compare(\n+        self,\n+        func: Callable[..., Any],\n+        *args,\n+        compile_kwargs: Optional[dict] = None,\n+        expected_num_block_pointers: Optional[int] = None,\n+        expected_num_programs: int = 1,\n+        expected_num_triton_kernels: int = 1,\n+    ):\n+        \"\"\"\n+        Runs the module through Inductor, comparing to eager reference.\n+        \"\"\"\n+        if compile_kwargs is None:\n+            compile_kwargs = {}\n+\n+        def flatten_tensors(tensors):\n+            flat, spec = pytree.tree_flatten(tensors)\n+            return flat\n+\n+        compiled = torch.compile(func, backend=\"inductor\", **compile_kwargs)\n+        result, code = run_and_get_code(compiled, *args)\n+\n+        # Check numerical accuracy\n+        ref_tensors = flatten_tensors(func(*args))\n+        actual_tensors = flatten_tensors(result)\n+        for ref, actual in zip(ref_tensors, actual_tensors):\n+            self.assertTrue(torch.allclose(ref, actual))\n+\n+        def count_code(substr: str, expected: Optional[int]):\n+            count = sum(prog.count(substr) for prog in code)\n+            if expected is not None:\n+                self.assertEqual(count, expected)\n+\n+        # Check the code\n+        self.assertEqual(len(code), expected_num_programs)\n+        count_code(\"@triton.jit\", expected_num_triton_kernels)\n+        count_code(\"tl.make_block_ptr\", expected_num_block_pointers)\n+\n+        return result, code\n+\n+    @parametrize(\n+        \"expected_num_block_pointers,raises\",\n+        [\n+            (3, False),  # This should pass\n+            (9, True),  # This should fail\n+        ],\n+    )\n+    def test_expected_num_block_pointers(\n+        self, expected_num_block_pointers: int, raises: bool\n+    ):\n+        \"\"\"\n+        Checks that the test harness verifies the number of block pointers correctly.\n+        \"\"\"\n+\n+        def foo(x, y):\n+            return x + y\n+\n+        device = torch.device(GPU_TYPE)\n+        inputs = [torch.randn(8).to(device) for arg_idx in range(2)]\n+\n+        # Expect failure for bad inputs\n+        with self.assertRaises(AssertionError) if raises else contextlib.nullcontext():\n+            # Expect 3 block pointers: 2 inputs 1 output\n+            self.run_and_compare(\n+                foo, *inputs, expected_num_block_pointers=expected_num_block_pointers\n+            )\n+\n+    @parametrize(\n+        \"full_size,view_size,stride,offset,require_block_ptr\",\n+        [\n+            ((64, 32, 32), (32, 16, 8), None, None, True),\n+            ((16, 8, 8, 8), (8, 8, 4, 2), None, None, True),\n+            ((8, 8, 8, 8), (4, 4, 4, 4), None, None, True),\n+            ((8, 8), (4, 4), None, 10, True),  # Storage offset\n+            ((8, 8), (4, 4), (16, 2), None, True),  # Non-default strides\n+            ((8, 8), (4, 4), (1, 8), None, True),  # Transposed strides\n+            (\n+                (5, 9),\n+                (5, 8),\n+                None,\n+                None,\n+                True,\n+            ),  # Non-power-of-2 leading dim: block ptr\n+            (\n+                (15, 9),\n+                (15, 3),\n+                None,\n+                None,\n+                False,\n+            ),  # Non-power-of-2 inner dims: non-block ptr\n+            ((1, 1, 1), (1, 1, 1), None, None, False),  # Scalar: non-block ptr\n+            (\n+                (2, 4 * max_block),\n+                (2, 3 * max_block),\n+                None,\n+                None,\n+                True,\n+            ),  # Inner dim multiple of max_block\n+        ],\n+    )\n+    def test_pointwise(\n+        self,\n+        full_size: Tuple[int],\n+        view_size: Tuple[int],\n+        stride: Optional[Tuple[int]],\n+        offset: Optional[int],\n+        require_block_ptr: bool,\n+    ):\n+        \"\"\"\n+        Test generating strided ND block pointers for a pointwise kernel.\n+\n+        If require_block_ptr is True, the generated code must contain block\n+        pointers. However, ND block pointers are not supported for all shapes. So\n+        we also test some odd shapes with require_block_ptr set to False, to ensure that\n+        block pointer analysis does not break these cases.\n+        \"\"\"\n+\n+        def get_input() -> torch.Tensor:\n+            device = torch.device(GPU_TYPE)\n+            full = torch.randn(full_size).to(device)\n+\n+            # Use the original tensor's stride by default\n+            view_stride = full.stride() if stride is None else stride\n+\n+            return torch.as_strided(full, view_size, view_stride, storage_offset=offset)\n+\n+        args = [get_input() for arg_idx in range(2)]\n+\n+        # Expect 3 block pointers: 2 inputs 1 output\n+        self.run_and_compare(\n+            torch.add,\n+            *args,\n+            expected_num_block_pointers=3 if require_block_ptr else None,\n+        )\n+\n+    @parametrize(\n+        \"x_size,y_size\",\n+        [\n+            ((8, 8), (8, 1)),\n+            ((8, 8), (1, 8)),\n+            (\n+                (4, 1, 4),\n+                (1, 4, 1),\n+            ),  # Very important case: index variables are disjoint!\n+            (\n+                (1, 1, 1, 4),\n+                (4, 4, 4, 4),\n+            ),  # Unmatched dims for first operand.\n+        ],\n+    )\n+    def test_broadcast(self, x_size: Tuple[int], y_size: Tuple[int]):\n+        \"\"\"\n+        Test that we can generate strided block pointers when inputs have different\n+        shapes, and they are broadcast together.\n+        \"\"\"\n+\n+        def foo(x, y):\n+            a = x + 1\n+            b = y * 2\n+            return a + b\n+\n+        def get_input(view_size: Tuple[int]) -> torch.Tensor:\n+            device = torch.device(GPU_TYPE)\n+            full_size = tuple(2 * dim for dim in view_size)\n+            full = torch.randn(full_size).to(device)\n+            view = torch.as_strided(full, view_size, full.stride())\n+            return view\n+\n+        x, y = (get_input(size) for size in (x_size, y_size))\n+\n+        # Check that input sizes are not the same\n+        self.assertNotEqual(x.shape, y.shape)\n+\n+        # Check that at least one dimension is a singleton\n+        all_dims = x.shape + y.shape\n+        self.assertIn(1, all_dims)\n+\n+        # Expect 3 block pointers: 2 inputs one output\n+        self.run_and_compare(foo, x, y, expected_num_block_pointers=3)\n+\n+    @parametrize(\n+        \"view_size,num_block_pointers,num_triton_kernels\",\n+        [\n+            ((4, 4), 1, 1),\n+            ((4, 4, 4), 1, 1),\n+            ((8, 8, 8), 1, 1),\n+            ((15, 15), 0, 1),  # Non-power of 2\n+            ((3 * max_block, 2), 3, 2),  # Multiple of max block. Uses loops.\n+            (\n+                (2, 3 * max_block),\n+                3,\n+                2,\n+            ),  # Multiple of max block. Uses loops.\n+            ((128, 128), 3, 2),  # Test a large size, with loops.\n+        ],\n+    )\n+    def test_reduction(\n+        self, view_size: Tuple[int], num_block_pointers: int, num_triton_kernels: int\n+    ):\n+        \"\"\"\n+        Tests a reduction kernel.\n+        \"\"\"\n+\n+        device = torch.device(GPU_TYPE)\n+        full_size = tuple(2 * dim for dim in view_size)\n+        full = torch.randn(full_size).to(device)\n+        view = torch.as_strided(full, view_size, full.stride())\n+\n+        # Expect at least 1 block pointer for the input.\n+        # Add 2 more if we generate 2 kernels.\n+        result, (code,) = self.run_and_compare(\n+            torch.sum,\n+            view,\n+            expected_num_block_pointers=num_block_pointers,\n+            expected_num_triton_kernels=num_triton_kernels,\n+        )\n+\n+    @parametrize(\n+        \"view_size,num_block_pointers,num_triton_kernels\",\n+        [\n+            ((8, 8), 2, 1),  # No loops. Should be supported.\n+            (\n+                (128, 128),\n+                None,\n+                None,\n+            ),  # Looped reduction. Block pointers not yet supported.\n+        ],\n+    )\n+    def test_mixed_pointwise_reduction(\n+        self, view_size: Tuple[int], num_block_pointers: int, num_triton_kernels: int\n+    ):\n+        \"\"\"\n+        Tests mixing pointwise with reduction ops.\n+        \"\"\"\n+\n+        def foo(x, y):\n+            return torch.sum(x + y)\n+\n+        device = torch.device(GPU_TYPE)\n+        full_size = tuple(2 * dim for dim in view_size)\n+\n+        def get_input() -> torch.Tensor:\n+            full = torch.randn(full_size).to(device)\n+            view = torch.as_strided(full, view_size, full.stride())\n+            return view\n+\n+        inputs = [get_input() for input_idx in range(2)]\n+\n+        # Expect 2 block pointers: inputs\n+        result, (code,) = self.run_and_compare(\n+            foo,\n+            *inputs,\n+            expected_num_block_pointers=num_block_pointers,\n+            expected_num_triton_kernels=num_triton_kernels,\n+        )\n+\n+    def test_multiple_max_block_non_power_of_2(self):\n+        \"\"\"\n+        Check that we support dims of size n * MAX_BLOCK, where n is any positive integer, not\n+        necessarily a power of 2.\n+        \"\"\"\n+\n+        def foo(x):\n+            return x - 1\n+\n+        device = torch.device(GPU_TYPE)\n+        full_size = (3 * max_block, 3)\n+        view_size = (3 * max_block, 2)\n+        full = torch.randn(full_size).to(device)\n+        view = torch.as_strided(full, view_size, full.stride())\n+\n+        # Check that we're using dims that aren't all powers of 2\n+        have_np2_dim = not all(is_power_of_2(dim) for dim in view_size)\n+        self.assertTrue(have_np2_dim)\n+\n+        # Check that we need more than one stride to represent the tensor\n+        nontrivial_dims = [dim for dim in view_size if dim > 1]\n+        self.assertTrue(len(nontrivial_dims) > 1)\n+\n+        # Expect 2 block pointers: input and output\n+        self.run_and_compare(foo, view, expected_num_block_pointers=2)\n+\n+    def test_dynamic_shapes_generic(self):\n+        \"\"\"\n+        Test a generic strided block with dynamic shapes. Block pointers are not\n+        expected. This only checks that the analysis doesn't break this case.\n+        \"\"\"\n+\n+        device = torch.device(GPU_TYPE)\n+        full_size = (8, 8)\n+        view_size = (4, 4)\n+        full = torch.randn(full_size).to(device)\n+        view = torch.as_strided(full, view_size, full.stride())\n+\n+        self.run_and_compare(torch.div, view, view, compile_kwargs={\"dynamic\": True})\n+\n+    @unittest.skip(reason=\"Dynamo tracing error\")\n+    def test_dynamic_shapes_multiple_max_block(self):\n+        \"\"\"\n+        Test dynamic shapes, where we know the shape is a multiple of the max block\n+        size. We should be able to generate a block pointer for this case.\n+        \"\"\"\n+\n+        def foo(x):\n+            tile_dims = (3 * max_block * x.shape[0], 3 * x.shape[1])\n+            view_size = (3 * max_block * x.shape[0], 2 * x.shape[1])\n+            full = x.tile(tile_dims)\n+            view = torch.as_strided(full, view_size, full.stride())\n+            return view + view\n+\n+        device = torch.device(GPU_TYPE)\n+        x_size = (1, 1)\n+        x = torch.randn(x_size).to(device)\n+\n+        # Expect 2 block pointers: input and output\n+        self.run_and_compare(\n+            x, compile_kwargs={\"dynamic\": True}, expected_num_block_pointers=2\n+        )\n+\n+\n+if __name__ == \"__main__\":\n+    from torch._inductor.test_case import run_tests\n+\n+    if HAS_GPU:\n+        run_tests(needs=\"filelock\")\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+# Owner(s): [\"module: inductor\"]\n+import contextlib\n+import importlib\n+import unittest\n+from typing import Any, Callable, Optional, Tuple\n+\n+import torch\n+import torch.utils._pytree as pytree\n+from torch._inductor import config\n+from torch._inductor.runtime.hints import TRITON_MAX_BLOCK\n+from torch._inductor.runtime.runtime_utils import is_power_of_2\n+from torch._inductor.test_case import TestCase as InductorTestCase\n+from torch._inductor.utils import run_and_get_code\n+from torch.testing._internal.common_utils import (\n+    instantiate_parametrized_tests,\n+    parametrize,\n+)\n+from torch.testing._internal.inductor_utils import (\n+    GPU_TYPE,\n+    HAS_GPU,\n+    requires_gpu,\n+    skip_windows_ci,\n+)\n+\n+\n+skip_windows_ci(__name__, __file__)\n+\n+importlib.import_module(\"filelock\")\n+\n+max_block: int = TRITON_MAX_BLOCK[\"X\"]\n+\n+\n+@requires_gpu()\n+@config.patch(\"triton.use_block_ptr\", True)\n+@instantiate_parametrized_tests\n+class TritonBlockPointerTest(InductorTestCase):\n+    def run_and_compare(\n+        self,\n+        func: Callable[..., Any],\n+        *args,\n+        compile_kwargs: Optional[dict] = None,\n+        expected_num_block_pointers: Optional[int] = None,\n+        expected_num_programs: int = 1,\n+        expected_num_triton_kernels: int = 1,\n+    ):\n+        \"\"\"\n+        Runs the module through Inductor, comparing to eager reference.\n+        \"\"\"\n+        if compile_kwargs is None:\n+            compile_kwargs = {}\n+\n+        def flatten_tensors(tensors):\n+            flat, spec = pytree.tree_flatten(tensors)\n+            return flat\n+\n+        compiled = torch.compile(func, backend=\"inductor\", **compile_kwargs)\n+        result, code = run_and_get_code(compiled, *args)\n+\n+        # Check numerical accuracy\n+        ref_tensors = flatten_tensors(func(*args))\n+        actual_tensors = flatten_tensors(result)\n+        for ref, actual in zip(ref_tensors, actual_tensors):\n+            self.assertTrue(torch.allclose(ref, actual))\n+\n+        def count_code(substr: str, expected: Optional[int]):\n+            count = sum(prog.count(substr) for prog in code)\n+            if expected is not None:\n+                self.assertEqual(count, expected)\n+\n+        # Check the code\n+        self.assertEqual(len(code), expected_num_programs)\n+        count_code(\"@triton.jit\", expected_num_triton_kernels)\n+        count_code(\"tl.make_block_ptr\", expected_num_block_pointers)\n+\n+        return result, code\n+\n+    @parametrize(\n+        \"expected_num_block_pointers,raises\",\n+        [\n+            (3, False),  # This should pass\n+            (9, True),  # This should fail\n+        ],\n+    )\n+    def test_expected_num_block_pointers(\n+        self, expected_num_block_pointers: int, raises: bool\n+    ):\n+        \"\"\"\n+        Checks that the test harness verifies the number of block pointers correctly.\n+        \"\"\"\n+\n+        def foo(x, y):\n+            return x + y\n+\n+        device = torch.device(GPU_TYPE)\n+        inputs = [torch.randn(8).to(device) for arg_idx in range(2)]\n+\n+        # Expect failure for bad inputs\n+        with self.assertRaises(AssertionError) if raises else contextlib.nullcontext():\n+            # Expect 3 block pointers: 2 inputs 1 output\n+            self.run_and_compare(\n+                foo, *inputs, expected_num_block_pointers=expected_num_block_pointers\n+            )\n+\n+    @parametrize(\n+        \"full_size,view_size,stride,offset,require_block_ptr\",\n+        [\n+            ((64, 32, 32), (32, 16, 8), None, None, True),\n+            ((16, 8, 8, 8), (8, 8, 4, 2), None, None, True),\n+            ((8, 8, 8, 8), (4, 4, 4, 4), None, None, True),\n+            ((8, 8), (4, 4), None, 10, True),  # Storage offset\n+            ((8, 8), (4, 4), (16, 2), None, True),  # Non-default strides\n+            ((8, 8), (4, 4), (1, 8), None, True),  # Transposed strides\n+            (\n+                (5, 9),\n+                (5, 8),\n+                None,\n+                None,\n+                True,\n+            ),  # Non-power-of-2 leading dim: block ptr\n+            (\n+                (15, 9),\n+                (15, 3),\n+                None,\n+                None,\n+                False,\n+            ),  # Non-power-of-2 inner dims: non-block ptr\n+            ((1, 1, 1), (1, 1, 1), None, None, False),  # Scalar: non-block ptr\n+            (\n+                (2, 4 * max_block),\n+                (2, 3 * max_block),\n+                None,\n+                None,\n+                True,\n+            ),  # Inner dim multiple of max_block\n+        ],\n+    )\n+    def test_pointwise(\n+        self,\n+        full_size: Tuple[int],\n+        view_size: Tuple[int],\n+        stride: Optional[Tuple[int]],\n+        offset: Optional[int],\n+        require_block_ptr: bool,\n+    ):\n+        \"\"\"\n+        Test generating strided ND block pointers for a pointwise kernel.\n+\n+        If require_block_ptr is True, the generated code must contain block\n+        pointers. However, ND block pointers are not supported for all shapes. So\n+        we also test some odd shapes with require_block_ptr set to False, to ensure that\n+        block pointer analysis does not break these cases.\n+        \"\"\"\n+\n+        def get_input() -> torch.Tensor:\n+            device = torch.device(GPU_TYPE)\n+            full = torch.randn(full_size).to(device)\n+\n+            # Use the original tensor's stride by default\n+            view_stride = full.stride() if stride is None else stride\n+\n+            return torch.as_strided(full, view_size, view_stride, storage_offset=offset)\n+\n+        args = [get_input() for arg_idx in range(2)]\n+\n+        # Expect 3 block pointers: 2 inputs 1 output\n+        self.run_and_compare(\n+            torch.add,\n+            *args,\n+            expected_num_block_pointers=3 if require_block_ptr else None,\n+        )\n+\n+    @parametrize(\n+        \"x_size,y_size\",\n+        [\n+            ((8, 8), (8, 1)),\n+            ((8, 8), (1, 8)),\n+            (\n+                (4, 1, 4),\n+                (1, 4, 1),\n+            ),  # Very important case: index variables are disjoint!\n+            (\n+                (1, 1, 1, 4),\n+                (4, 4, 4, 4),\n+            ),  # Unmatched dims for first operand.\n+        ],\n+    )\n+    def test_broadcast(self, x_size: Tuple[int], y_size: Tuple[int]):\n+        \"\"\"\n+        Test that we can generate strided block pointers when inputs have different\n+        shapes, and they are broadcast together.\n+        \"\"\"\n+\n+        def foo(x, y):\n+            a = x + 1\n+            b = y * 2\n+            return a + b\n+\n+        def get_input(view_size: Tuple[int]) -> torch.Tensor:\n+            device = torch.device(GPU_TYPE)\n+            full_size = tuple(2 * dim for dim in view_size)\n+            full = torch.randn(full_size).to(device)\n+            view = torch.as_strided(full, view_size, full.stride())\n+            return view\n+\n+        x, y = (get_input(size) for size in (x_size, y_size))\n+\n+        # Check that input sizes are not the same\n+        self.assertNotEqual(x.shape, y.shape)\n+\n+        # Check that at least one dimension is a singleton\n+        all_dims = x.shape + y.shape\n+        self.assertIn(1, all_dims)\n+\n+        # Expect 3 block pointers: 2 inputs one output\n+        self.run_and_compare(foo, x, y, expected_num_block_pointers=3)\n+\n+    @parametrize(\n+        \"view_size,num_block_pointers,num_triton_kernels\",\n+        [\n+            ((4, 4), 1, 1),\n+            ((4, 4, 4), 1, 1),\n+            ((8, 8, 8), 1, 1),\n+            ((15, 15), 0, 1),  # Non-power of 2\n+            ((3 * max_block, 2), 3, 2),  # Multiple of max block. Uses loops.\n+            (\n+                (2, 3 * max_block),\n+                3,\n+                2,\n+            ),  # Multiple of max block. Uses loops.\n+            ((128, 128), 3, 2),  # Test a large size, with loops.\n+        ],\n+    )\n+    def test_reduction(\n+        self, view_size: Tuple[int], num_block_pointers: int, num_triton_kernels: int\n+    ):\n+        \"\"\"\n+        Tests a reduction kernel.\n+        \"\"\"\n+\n+        device = torch.device(GPU_TYPE)\n+        full_size = tuple(2 * dim for dim in view_size)\n+        full = torch.randn(full_size).to(device)\n+        view = torch.as_strided(full, view_size, full.stride())\n+\n+        # Expect at least 1 block pointer for the input.\n+        # Add 2 more if we generate 2 kernels.\n+        result, (code,) = self.run_and_compare(\n+            torch.sum,\n+            view,\n+            expected_num_block_pointers=num_block_pointers,\n+            expected_num_triton_kernels=num_triton_kernels,\n+        )\n+\n+    @parametrize(\n+        \"view_size,num_block_pointers,num_triton_kernels\",\n+        [\n+            ((8, 8), 2, 1),  # No loops. Should be supported.\n+            (\n+                (128, 128),\n+                None,\n+                None,\n+            ),  # Looped reduction. Block pointers not yet supported.\n+        ],\n+    )\n+    def test_mixed_pointwise_reduction(\n+        self, view_size: Tuple[int], num_block_pointers: int, num_triton_kernels: int\n+    ):\n+        \"\"\"\n+        Tests mixing pointwise with reduction ops.\n+        \"\"\"\n+\n+        def foo(x, y):\n+            return torch.sum(x + y)\n+\n+        device = torch.device(GPU_TYPE)\n+        full_size = tuple(2 * dim for dim in view_size)\n+\n+        def get_input() -> torch.Tensor:\n+            full = torch.randn(full_size).to(device)\n+            view = torch.as_strided(full, view_size, full.stride())\n+            return view\n+\n+        inputs = [get_input() for input_idx in range(2)]\n+\n+        # Expect 2 block pointers: inputs\n+        result, (code,) = self.run_and_compare(\n+            foo,\n+            *inputs,\n+            expected_num_block_pointers=num_block_pointers,\n+            expected_num_triton_kernels=num_triton_kernels,\n+        )\n+\n+    def test_multiple_max_block_non_power_of_2(self):\n+        \"\"\"\n+        Check that we support dims of size n * MAX_BLOCK, where n is any positive integer, not\n+        necessarily a power of 2.\n+        \"\"\"\n+\n+        def foo(x):\n+            return x - 1\n+\n+        device = torch.device(GPU_TYPE)\n+        full_size = (3 * max_block, 3)\n+        view_size = (3 * max_block, 2)\n+        full = torch.randn(full_size).to(device)\n+        view = torch.as_strided(full, view_size, full.stride())\n+\n+        # Check that we're using dims that aren't all powers of 2\n+        have_np2_dim = not all(is_power_of_2(dim) for dim in view_size)\n+        self.assertTrue(have_np2_dim)\n+\n+        # Check that we need more than one stride to represent the tensor\n+        nontrivial_dims = [dim for dim in view_size if dim > 1]\n+        self.assertTrue(len(nontrivial_dims) > 1)\n+\n+        # Expect 2 block pointers: input and output\n+        self.run_and_compare(foo, view, expected_num_block_pointers=2)\n+\n+    def test_dynamic_shapes_generic(self):\n+        \"\"\"\n+        Test a generic strided block with dynamic shapes. Block pointers are not\n+        expected. This only checks that the analysis doesn't break this case.\n+        \"\"\"\n+\n+        device = torch.device(GPU_TYPE)\n+        full_size = (8, 8)\n+        view_size = (4, 4)\n+        full = torch.randn(full_size).to(device)\n+        view = torch.as_strided(full, view_size, full.stride())\n+\n+        self.run_and_compare(torch.div, view, view, compile_kwargs={\"dynamic\": True})\n+\n+    @unittest.skip(reason=\"Dynamo tracing error\")\n+    def test_dynamic_shapes_multiple_max_block(self):\n+        \"\"\"\n+        Test dynamic shapes, where we know the shape is a multiple of the max block\n+        size. We should be able to generate a block pointer for this case.\n+        \"\"\"\n+\n+        def foo(x):\n+            tile_dims = (3 * max_block * x.shape[0], 3 * x.shape[1])\n+            view_size = (3 * max_block * x.shape[0], 2 * x.shape[1])\n+            full = x.tile(tile_dims)\n+            view = torch.as_strided(full, view_size, full.stride())\n+            return view + view\n+\n+        device = torch.device(GPU_TYPE)\n+        x_size = (1, 1)\n+        x = torch.randn(x_size).to(device)\n+\n+        # Expect 2 block pointers: input and output\n+        self.run_and_compare(\n+            x, compile_kwargs={\"dynamic\": True}, expected_num_block_pointers=2\n+        )\n+\n+\n+if __name__ == \"__main__\":\n+    from torch._inductor.test_case import run_tests\n+\n+    if HAS_GPU:\n+        run_tests(needs=\"filelock\")\n",
            "whole_hunk": "@@ -0,0 +1,361 @@\n+# Owner(s): [\"module: inductor\"]\n+import contextlib\n+import importlib\n+import unittest\n+from typing import Any, Callable, Optional, Tuple\n+\n+import torch\n+import torch.utils._pytree as pytree\n+from torch._inductor import config\n+from torch._inductor.runtime.hints import TRITON_MAX_BLOCK\n+from torch._inductor.runtime.runtime_utils import is_power_of_2\n+from torch._inductor.test_case import TestCase as InductorTestCase\n+from torch._inductor.utils import run_and_get_code\n+from torch.testing._internal.common_utils import (\n+    instantiate_parametrized_tests,\n+    parametrize,\n+)\n+from torch.testing._internal.inductor_utils import (\n+    GPU_TYPE,\n+    HAS_GPU,\n+    requires_gpu,\n+    skip_windows_ci,\n+)\n+\n+\n+skip_windows_ci(__name__, __file__)\n+\n+importlib.import_module(\"filelock\")\n+\n+max_block: int = TRITON_MAX_BLOCK[\"X\"]\n+\n+\n+@requires_gpu()\n+@config.patch(\"triton.use_block_ptr\", True)\n+@instantiate_parametrized_tests\n+class TritonBlockPointerTest(InductorTestCase):\n+    def run_and_compare(\n+        self,\n+        func: Callable[..., Any],\n+        *args,\n+        compile_kwargs: Optional[dict] = None,\n+        expected_num_block_pointers: Optional[int] = None,\n+        expected_num_programs: int = 1,\n+        expected_num_triton_kernels: int = 1,\n+    ):\n+        \"\"\"\n+        Runs the module through Inductor, comparing to eager reference.\n+        \"\"\"\n+        if compile_kwargs is None:\n+            compile_kwargs = {}\n+\n+        def flatten_tensors(tensors):\n+            flat, spec = pytree.tree_flatten(tensors)\n+            return flat\n+\n+        compiled = torch.compile(func, backend=\"inductor\", **compile_kwargs)\n+        result, code = run_and_get_code(compiled, *args)\n+\n+        # Check numerical accuracy\n+        ref_tensors = flatten_tensors(func(*args))\n+        actual_tensors = flatten_tensors(result)\n+        for ref, actual in zip(ref_tensors, actual_tensors):\n+            self.assertTrue(torch.allclose(ref, actual))\n+\n+        def count_code(substr: str, expected: Optional[int]):\n+            count = sum(prog.count(substr) for prog in code)\n+            if expected is not None:\n+                self.assertEqual(count, expected)\n+\n+        # Check the code\n+        self.assertEqual(len(code), expected_num_programs)\n+        count_code(\"@triton.jit\", expected_num_triton_kernels)\n+        count_code(\"tl.make_block_ptr\", expected_num_block_pointers)\n+\n+        return result, code\n+\n+    @parametrize(\n+        \"expected_num_block_pointers,raises\",\n+        [\n+            (3, False),  # This should pass\n+            (9, True),  # This should fail\n+        ],\n+    )\n+    def test_expected_num_block_pointers(\n+        self, expected_num_block_pointers: int, raises: bool\n+    ):\n+        \"\"\"\n+        Checks that the test harness verifies the number of block pointers correctly.\n+        \"\"\"\n+\n+        def foo(x, y):\n+            return x + y\n+\n+        device = torch.device(GPU_TYPE)\n+        inputs = [torch.randn(8).to(device) for arg_idx in range(2)]\n+\n+        # Expect failure for bad inputs\n+        with self.assertRaises(AssertionError) if raises else contextlib.nullcontext():\n+            # Expect 3 block pointers: 2 inputs 1 output\n+            self.run_and_compare(\n+                foo, *inputs, expected_num_block_pointers=expected_num_block_pointers\n+            )\n+\n+    @parametrize(\n+        \"full_size,view_size,stride,offset,require_block_ptr\",\n+        [\n+            ((64, 32, 32), (32, 16, 8), None, None, True),\n+            ((16, 8, 8, 8), (8, 8, 4, 2), None, None, True),\n+            ((8, 8, 8, 8), (4, 4, 4, 4), None, None, True),\n+            ((8, 8), (4, 4), None, 10, True),  # Storage offset\n+            ((8, 8), (4, 4), (16, 2), None, True),  # Non-default strides\n+            ((8, 8), (4, 4), (1, 8), None, True),  # Transposed strides\n+            (\n+                (5, 9),\n+                (5, 8),\n+                None,\n+                None,\n+                True,\n+            ),  # Non-power-of-2 leading dim: block ptr\n+            (\n+                (15, 9),\n+                (15, 3),\n+                None,\n+                None,\n+                False,\n+            ),  # Non-power-of-2 inner dims: non-block ptr\n+            ((1, 1, 1), (1, 1, 1), None, None, False),  # Scalar: non-block ptr\n+            (\n+                (2, 4 * max_block),\n+                (2, 3 * max_block),\n+                None,\n+                None,\n+                True,\n+            ),  # Inner dim multiple of max_block\n+        ],\n+    )\n+    def test_pointwise(\n+        self,\n+        full_size: Tuple[int],\n+        view_size: Tuple[int],\n+        stride: Optional[Tuple[int]],\n+        offset: Optional[int],\n+        require_block_ptr: bool,\n+    ):\n+        \"\"\"\n+        Test generating strided ND block pointers for a pointwise kernel.\n+\n+        If require_block_ptr is True, the generated code must contain block\n+        pointers. However, ND block pointers are not supported for all shapes. So\n+        we also test some odd shapes with require_block_ptr set to False, to ensure that\n+        block pointer analysis does not break these cases.\n+        \"\"\"\n+\n+        def get_input() -> torch.Tensor:\n+            device = torch.device(GPU_TYPE)\n+            full = torch.randn(full_size).to(device)\n+\n+            # Use the original tensor's stride by default\n+            view_stride = full.stride() if stride is None else stride\n+\n+            return torch.as_strided(full, view_size, view_stride, storage_offset=offset)\n+\n+        args = [get_input() for arg_idx in range(2)]\n+\n+        # Expect 3 block pointers: 2 inputs 1 output\n+        self.run_and_compare(\n+            torch.add,\n+            *args,\n+            expected_num_block_pointers=3 if require_block_ptr else None,\n+        )\n+\n+    @parametrize(\n+        \"x_size,y_size\",\n+        [\n+            ((8, 8), (8, 1)),\n+            ((8, 8), (1, 8)),\n+            (\n+                (4, 1, 4),\n+                (1, 4, 1),\n+            ),  # Very important case: index variables are disjoint!\n+            (\n+                (1, 1, 1, 4),\n+                (4, 4, 4, 4),\n+            ),  # Unmatched dims for first operand.\n+        ],\n+    )\n+    def test_broadcast(self, x_size: Tuple[int], y_size: Tuple[int]):\n+        \"\"\"\n+        Test that we can generate strided block pointers when inputs have different\n+        shapes, and they are broadcast together.\n+        \"\"\"\n+\n+        def foo(x, y):\n+            a = x + 1\n+            b = y * 2\n+            return a + b\n+\n+        def get_input(view_size: Tuple[int]) -> torch.Tensor:\n+            device = torch.device(GPU_TYPE)\n+            full_size = tuple(2 * dim for dim in view_size)\n+            full = torch.randn(full_size).to(device)\n+            view = torch.as_strided(full, view_size, full.stride())\n+            return view\n+\n+        x, y = (get_input(size) for size in (x_size, y_size))\n+\n+        # Check that input sizes are not the same\n+        self.assertNotEqual(x.shape, y.shape)\n+\n+        # Check that at least one dimension is a singleton\n+        all_dims = x.shape + y.shape\n+        self.assertIn(1, all_dims)\n+\n+        # Expect 3 block pointers: 2 inputs one output\n+        self.run_and_compare(foo, x, y, expected_num_block_pointers=3)\n+\n+    @parametrize(\n+        \"view_size,num_block_pointers,num_triton_kernels\",\n+        [\n+            ((4, 4), 1, 1),\n+            ((4, 4, 4), 1, 1),\n+            ((8, 8, 8), 1, 1),\n+            ((15, 15), 0, 1),  # Non-power of 2\n+            ((3 * max_block, 2), 3, 2),  # Multiple of max block. Uses loops.\n+            (\n+                (2, 3 * max_block),\n+                3,\n+                2,\n+            ),  # Multiple of max block. Uses loops.\n+            ((128, 128), 3, 2),  # Test a large size, with loops.\n+        ],\n+    )\n+    def test_reduction(\n+        self, view_size: Tuple[int], num_block_pointers: int, num_triton_kernels: int\n+    ):\n+        \"\"\"\n+        Tests a reduction kernel.\n+        \"\"\"\n+\n+        device = torch.device(GPU_TYPE)\n+        full_size = tuple(2 * dim for dim in view_size)\n+        full = torch.randn(full_size).to(device)\n+        view = torch.as_strided(full, view_size, full.stride())\n+\n+        # Expect at least 1 block pointer for the input.\n+        # Add 2 more if we generate 2 kernels.\n+        result, (code,) = self.run_and_compare(\n+            torch.sum,\n+            view,\n+            expected_num_block_pointers=num_block_pointers,\n+            expected_num_triton_kernels=num_triton_kernels,\n+        )\n+\n+    @parametrize(\n+        \"view_size,num_block_pointers,num_triton_kernels\",\n+        [\n+            ((8, 8), 2, 1),  # No loops. Should be supported.\n+            (\n+                (128, 128),\n+                None,\n+                None,\n+            ),  # Looped reduction. Block pointers not yet supported.\n+        ],\n+    )\n+    def test_mixed_pointwise_reduction(\n+        self, view_size: Tuple[int], num_block_pointers: int, num_triton_kernels: int\n+    ):\n+        \"\"\"\n+        Tests mixing pointwise with reduction ops.\n+        \"\"\"\n+\n+        def foo(x, y):\n+            return torch.sum(x + y)\n+\n+        device = torch.device(GPU_TYPE)\n+        full_size = tuple(2 * dim for dim in view_size)\n+\n+        def get_input() -> torch.Tensor:\n+            full = torch.randn(full_size).to(device)\n+            view = torch.as_strided(full, view_size, full.stride())\n+            return view\n+\n+        inputs = [get_input() for input_idx in range(2)]\n+\n+        # Expect 2 block pointers: inputs\n+        result, (code,) = self.run_and_compare(\n+            foo,\n+            *inputs,\n+            expected_num_block_pointers=num_block_pointers,\n+            expected_num_triton_kernels=num_triton_kernels,\n+        )\n+\n+    def test_multiple_max_block_non_power_of_2(self):\n+        \"\"\"\n+        Check that we support dims of size n * MAX_BLOCK, where n is any positive integer, not\n+        necessarily a power of 2.\n+        \"\"\"\n+\n+        def foo(x):\n+            return x - 1\n+\n+        device = torch.device(GPU_TYPE)\n+        full_size = (3 * max_block, 3)\n+        view_size = (3 * max_block, 2)\n+        full = torch.randn(full_size).to(device)\n+        view = torch.as_strided(full, view_size, full.stride())\n+\n+        # Check that we're using dims that aren't all powers of 2\n+        have_np2_dim = not all(is_power_of_2(dim) for dim in view_size)\n+        self.assertTrue(have_np2_dim)\n+\n+        # Check that we need more than one stride to represent the tensor\n+        nontrivial_dims = [dim for dim in view_size if dim > 1]\n+        self.assertTrue(len(nontrivial_dims) > 1)\n+\n+        # Expect 2 block pointers: input and output\n+        self.run_and_compare(foo, view, expected_num_block_pointers=2)\n+\n+    def test_dynamic_shapes_generic(self):\n+        \"\"\"\n+        Test a generic strided block with dynamic shapes. Block pointers are not\n+        expected. This only checks that the analysis doesn't break this case.\n+        \"\"\"\n+\n+        device = torch.device(GPU_TYPE)\n+        full_size = (8, 8)\n+        view_size = (4, 4)\n+        full = torch.randn(full_size).to(device)\n+        view = torch.as_strided(full, view_size, full.stride())\n+\n+        self.run_and_compare(torch.div, view, view, compile_kwargs={\"dynamic\": True})\n+\n+    @unittest.skip(reason=\"Dynamo tracing error\")\n+    def test_dynamic_shapes_multiple_max_block(self):\n+        \"\"\"\n+        Test dynamic shapes, where we know the shape is a multiple of the max block\n+        size. We should be able to generate a block pointer for this case.\n+        \"\"\"\n+\n+        def foo(x):\n+            tile_dims = (3 * max_block * x.shape[0], 3 * x.shape[1])\n+            view_size = (3 * max_block * x.shape[0], 2 * x.shape[1])\n+            full = x.tile(tile_dims)\n+            view = torch.as_strided(full, view_size, full.stride())\n+            return view + view\n+\n+        device = torch.device(GPU_TYPE)\n+        x_size = (1, 1)\n+        x = torch.randn(x_size).to(device)\n+\n+        # Expect 2 block pointers: input and output\n+        self.run_and_compare(\n+            x, compile_kwargs={\"dynamic\": True}, expected_num_block_pointers=2\n+        )\n+\n+\n+if __name__ == \"__main__\":\n+    from torch._inductor.test_case import run_tests\n+\n+    if HAS_GPU:\n+        run_tests(needs=\"filelock\")\n"
        },
        {
            "name": "triton.py",
            "path": "torch/_inductor/codegen/triton.py",
            "patches": [
                {
                    "old_start": 13,
                    "old_length": 6,
                    "new_start": 13,
                    "new_length": 7,
                    "hunk": "@@ -13,6 +13,7 @@ from typing import (\n     Callable,\n     cast,\n     Dict,\n+    Iterable,\n     List,\n     Optional,\n     Set,\n"
                },
                {
                    "old_start": 29,
                    "old_length": 8,
                    "new_start": 30,
                    "new_length": 9,
                    "hunk": "@@ -29,8 +30,9 @@ from torch._dynamo.utils import preserve_rng_state\n \n from torch._inductor.runtime.hints import AutotuneHint, DeviceProperties\n from torch._prims_common import is_integer_dtype\n+from torch.utils._sympy.functions import CeilDiv, FloorDiv, ModularIndexing\n from torch.utils._triton import has_triton_package\n-from ...utils._sympy.symbol import free_symbol_is_type, symbol_is_type, SymT\n+from ...utils._sympy.symbol import free_symbol_is_type, prefix_str, symbol_is_type, SymT\n from ...utils._sympy.value_ranges import ValueRanges\n \n from .. import config, ir\n"
                },
                {
                    "old_start": 119,
                    "old_length": 10,
                    "new_start": 121,
                    "new_length": 21,
                    "hunk": "@@ -119,10 +121,21 @@ def gen_common_triton_imports():\n     return imports.getvalue()\n \n \n+block_offsets = {\n+    symt: sympy.Symbol(f\"{prefix_str[symt]}offset\", integer=True)\n+    for symt in [SymT.XBLOCK, SymT.YBLOCK, SymT.RINDEX]\n+}\n+\n+block_sizes = {\n+    symt: sympy.Symbol(f\"{prefix_str[symt].upper()}BLOCK\", integer=True, nonzero=True)\n+    for symt in [SymT.XBLOCK, SymT.YBLOCK, SymT.RINDEX]\n+}\n+\n+\n @dataclasses.dataclass\n class IndexingOptions:\n     index_str: str\n-    mask_vars: Set[sympy.Symbol]\n+    mask_vars: Set[str]\n     mask_str: str\n     expand_str: Optional[str]\n     _has_rindex: bool\n"
                },
                {
                    "old_start": 146,
                    "old_length": 29,
                    "new_start": 159,
                    "new_length": 46,
                    "hunk": "@@ -146,29 +159,46 @@ class IndexingOptions:\n \n @dataclasses.dataclass\n class BlockPtrOptions:\n+    params: BlockParameters\n     constant_offset: sympy.Expr\n-    shape: List[sympy.Expr]\n-    strides: List[sympy.Expr]\n-    block_shape: List[str]\n     order: List[int]\n-    offsets: List[str]\n-    mask_vars: Set[sympy.Symbol]\n+    mask_vars: Set[str]\n     reshape_suffix: List[str]\n \n+    @property\n+    def shape(self) -> List[sympy.Expr]:\n+        return self.params.shape\n+\n+    @property\n+    def block_shape(self) -> List[sympy.Expr]:\n+        return self.params.block_shape\n+\n+    @property\n+    def strides(self) -> List[sympy.Expr]:\n+        return self.params.strides\n+\n+    @property\n+    def offsets(self) -> List[sympy.Expr]:\n+        return self.params.offsets\n+\n     @staticmethod\n     def create(\n-        strides: List[sympy.Expr],\n+        *,\n+        params: BlockParameters,\n         constant_offset: sympy.Expr,\n         range_trees: List[IterationRangesEntry],\n-        mask_vars: Set[sympy.Symbol],\n+        mask_vars: Set[str],\n     ) -> BlockPtrOptions:\n         \"\"\"Helper to create a  BlockPtrOptions instance\"\"\"\n-        block_shape = [f\"{t.prefix.upper()}BLOCK\" for t in range_trees]\n-        reshape_suffix = [*block_shape]\n+        reshape_suffix = [f\"{t.prefix.upper()}BLOCK\" for t in range_trees]\n+\n+        # Only drop broadcast dims if the output has the same\n+        # rank as the block. Otherwise, we will get shape errors.\n+        drop_broadcasts = len(reshape_suffix) == len(params.strides)\n \n-        broadcasting_dim = [s == 0 for s in strides]\n+        broadcasting_dim = [s == 0 for s in params.strides]\n         for i, is_broadcasting in enumerate(broadcasting_dim):\n-            if is_broadcasting:\n+            if is_broadcasting and drop_broadcasts:\n                 # drop any stride==0 dimensions for performance\n                 reshape_suffix[i] = \"1\"\n \n"
                },
                {
                    "old_start": 178,
                    "old_length": 7,
                    "new_start": 208,
                    "new_length": 7,
                    "hunk": "@@ -178,7 +208,7 @@ class BlockPtrOptions:\n \n         if (\n             not V.kernel.inside_reduction\n-            and len(strides) == len(V.kernel.numels) - 1\n+            and len(params.strides) == len(V.kernel.numels) - 1\n             and V.kernel.numels[-1] != 1\n         ):\n             # Need to expand rank by 1 to match rank when self.inside_reduction=True\n"
                },
                {
                    "old_start": 190,
                    "old_length": 23,
                    "new_start": 220,
                    "new_length": 36,
                    "hunk": "@@ -190,23 +220,36 @@ class BlockPtrOptions:\n             return [\n                 item\n                 for item, is_broadcasting in zip(it, broadcasting_dim)\n-                if not is_broadcasting\n+                if not is_broadcasting or not drop_broadcasts\n             ]\n \n+        # Drop broadcasting dimensions from the input.\n+        params = BlockParameters(\n+            **{key: filter(val) for key, val in dataclasses.asdict(params).items()}\n+        )\n+\n+        def lookup_size(exprs: Iterable[sympy.Expr]) -> List[sympy.Expr]:\n+            return [V.graph.sizevars.lookup_precomputed_size(expr) for expr in exprs]\n+\n+        # Look up precomputed sizes\n+        params.shape = lookup_size(params.shape)\n+        params.strides = lookup_size(params.strides)\n+\n         return BlockPtrOptions(\n+            params=params,\n             constant_offset=V.graph.sizevars.lookup_precomputed_size(constant_offset),\n-            shape=[\n-                V.graph.sizevars.lookup_precomputed_size(t.numel)\n-                for t in filter(range_trees)\n-            ],\n-            strides=[*map(V.graph.sizevars.lookup_precomputed_size, filter(strides))],\n-            block_shape=filter(block_shape),\n-            order=V.graph.sizevars.guarded_order(filter(strides)),\n-            offsets=filter([f\"{t.prefix}offset\" for t in range_trees]),\n+            order=list(reversed(range(len(params.shape)))),\n             mask_vars=mask_vars,\n             reshape_suffix=reshape_suffix,\n         )\n \n+    def replace_roffset(self, expr: sympy.Expr, replacement: sympy.Expr) -> sympy.Expr:\n+        \"\"\"\n+        Replaces instances of roffset with the new expression.\n+        \"\"\"\n+        roffset = block_offsets[SymT.RINDEX]\n+        return sympy_subs(expr, {roffset: replacement})\n+\n     def format(self, name: str, roffset=True) -> str:\n         \"\"\"\n         Codegen a call to tl.make_block_ptr()\n"
                },
                {
                    "old_start": 221,
                    "old_length": 7,
                    "new_start": 264,
                    "new_length": 9,
                    "hunk": "@@ -221,7 +264,9 @@ class BlockPtrOptions:\n         f = V.kernel.index_to_str\n         offsets = [*self.offsets]\n         if not roffset:\n-            offsets[offsets.index(\"roffset\")] = \"0\"\n+            offsets = [\n+                self.replace_roffset(offset, sympy.Integer(0)) for offset in offsets\n+            ]\n         args = [\n             f\"{name} + ({f(self.constant_offset)})\"\n             if self.constant_offset != 0\n"
                },
                {
                    "old_start": 237,
                    "old_length": 31,
                    "new_start": 282,
                    "new_length": 59,
                    "hunk": "@@ -237,31 +282,59 @@ class BlockPtrOptions:\n     @cache_on_self\n     def boundary_check(self) -> List[int]:\n         \"\"\"List of indices to pass to tl.load(boundary_check=...)\"\"\"\n-        check = []\n-        for i in range(len(self.shape)):\n+        sizevars = V.graph.sizevars\n+\n+        # Substitute maximum block sizes in shape expressions.\n+        # This works in multiple_of checks because block sizes are powers of 2.\n+        block_to_max: Dict[sympy.Expr, Any] = {\n+            block_size: TRITON_MAX_BLOCK[prefix_str[symt].upper()]\n+            for symt, block_size in block_sizes.items()\n+        }\n+\n+        return [\n+            idx\n+            for idx in range(len(self.shape))\n             if (\n-                self.block_shape[i] != \"1\"\n-                and not V.graph.sizevars.statically_known_equals(self.strides[i], 0)  # type: ignore[arg-type]\n-                and not V.graph.sizevars.statically_known_multiple_of(\n-                    self.shape[i],\n-                    TRITON_MAX_BLOCK[self.block_shape[i][0]],  # type: ignore[arg-type]\n+                not sizevars.statically_known_equals(\n+                    self.strides[idx], sympy.Integer(0)\n                 )\n-                and not (V.kernel.no_x_dim and self.block_shape[i] == \"XBLOCK\")\n-            ):\n-                check.append(i)\n-        return check\n+                and not sizevars.statically_known_multiple_of(\n+                    self.shape[idx], self.block_shape[idx]\n+                )\n+                and not sizevars.statically_known_multiple_of(\n+                    self.shape[idx], sympy_subs(self.block_shape[idx], block_to_max)\n+                )\n+                and not (\n+                    V.kernel.no_x_dim\n+                    and self.block_shape[idx] == block_sizes[SymT.XBLOCK]\n+                )\n+            )\n+        ]\n \n     def advance_roffset(self):\n-        \"\"\"Codegen string to pass to tl.advance(name, ...)\"\"\"\n-        advance = [\"0\"] * len(self.shape)\n-        advance[self.offsets.index(\"roffset\")] = \"RBLOCK\"\n+        \"\"\"\n+        Codegen string to pass to tl.advance(name, ...).\n+\n+        Advance is the difference between offsets in each loop iteration.\n+        To compute it, we replace roffset with multiples of RBLOCK.\n+        Since we expect roffset to vary in range(0, rnumel, RBLOCK), the first\n+        iteration has roffset=0, while the second has roffset=RBLOCK.\n+        \"\"\"\n+        rblock = block_sizes[SymT.RINDEX]\n+        advance = [\n+            (\n+                self.replace_roffset(offset, rblock)\n+                - self.replace_roffset(offset, sympy.Integer(0))\n+            )\n+            for offset in self.offsets\n+        ]\n         return V.kernel.index_to_str(advance)\n \n     def has_indirect(self):\n         return False  # block_ptr can't do indirect indexing\n \n-    def has_rindex(self):\n-        return \"RBLOCK\" in self.block_shape\n+    def has_rindex(self) -> bool:\n+        return any(free_symbol_is_type(expr, SymT.RINDEX) for expr in self.block_shape)\n \n     def has_rmask(self):\n         return self.has_rindex()\n"
                },
                {
                    "old_start": 365,
                    "old_length": 26,
                    "new_start": 438,
                    "new_length": 31,
                    "hunk": "@@ -365,26 +438,31 @@ class TritonPrinter(PythonPrinter):\n         q = self.doprint(expr.args[2])\n         return f\"tl.where({c}, {p}, {q})\"\n \n-    def _print_Min(self, expr):\n+    def _print_min_max_helper(self, expr: sympy.Expr, cmp: str) -> str:\n+        \"\"\"\n+        Helper for max/min code genereration.\n+        cmp: > or <\n+        \"\"\"\n         nargs = len(expr.args)\n         if len(expr.args) == 1:\n             return self._print(expr.args[0])\n \n         mid = len(expr.args) // 2\n-        a = self._print(sympy.Min(*expr.args[:mid]))\n-        b = self._print(sympy.Min(*expr.args[mid:]))\n-        return f\"tl.minimum({a}, {b})\"\n+        cls = type(expr)\n+        a = self._print(cls(*expr.args[:mid]))\n+        b = self._print(cls(*expr.args[mid:]))\n \n-    def _print_Max(self, expr):\n-        nargs = len(expr.args)\n-        if len(expr.args) == 1:\n-            return self._print(expr.args[0])\n+        # Use a macro so we can propagate constexprs.\n+        # https://github.com/triton-lang/triton/issues/3815\n+        a, b = tuple(f\"({x})\" for x in (a, b))\n+        assert cmp in {\">\", \"<\"}, f\"Unexpected comparator: '{cmp}'\"\n+        return f\"({a} * ({a} {cmp}= {b}) + {b} * ({b} {cmp} {a}))\"\n \n-        mid = len(expr.args) // 2\n-        a = self._print(sympy.Max(*expr.args[:mid]))\n-        b = self._print(sympy.Max(*expr.args[mid:]))\n+    def _print_Min(self, expr):\n+        return self._print_min_max_helper(expr, \"<\")\n \n-        return f\"tl.maximum({a}, {b})\"\n+    def _print_Max(self, expr):\n+        return self._print_min_max_helper(expr, \">\")\n \n     def _print_Abs(self, expr):\n         assert len(expr.args) == 1\n"
                },
                {
                    "old_start": 1023,
                    "old_length": 6,
                    "new_start": 1101,
                    "new_length": 26,
                    "hunk": "@@ -1023,6 +1101,26 @@ class HelperFunctions:\n         return self.finalized_helpers[idx]\n \n \n+@dataclasses.dataclass\n+class BlockParameters:\n+    \"\"\"\n+    Class representing ND block dimensions, for block pointer analysis.\n+    \"\"\"\n+\n+    shape: List[sympy.Expr] = dataclasses.field(default_factory=list)\n+    block_shape: List[sympy.Expr] = dataclasses.field(default_factory=list)\n+    strides: List[sympy.Expr] = dataclasses.field(default_factory=list)\n+    offsets: List[sympy.Expr] = dataclasses.field(default_factory=list)\n+\n+    def __add__(self, other: BlockParameters) -> BlockParameters:\n+        \"\"\"\n+        Concatenates block parameters.\n+        \"\"\"\n+        cls = type(self)\n+        a, b = tuple(dataclasses.asdict(x) for x in (self, other))\n+        return cls(**{key: a[key] + b[key] for key in a})\n+\n+\n class TritonKernel(SIMDKernel):\n     overrides = TritonKernelOverrides  # type: ignore[assignment]\n     helper_functions: HelperFunctions\n"
                },
                {
                    "old_start": 1059,
                    "old_length": 6,
                    "new_start": 1157,
                    "new_length": 19,
                    "hunk": "@@ -1059,6 +1157,19 @@ class TritonKernel(SIMDKernel):\n \n         self.codegen_range_tree()\n \n+    def _get_symt(self, tree: IterationRangesEntry) -> SymT:\n+        prefix_to_symt = {prefix: symt for symt, prefix in prefix_str.items()}\n+        return prefix_to_symt[tree.prefix]\n+\n+    def _get_block_size(self, tree: IterationRangesEntry) -> sympy.Symbol:\n+        return block_sizes[self._get_symt(tree)]\n+\n+    def _get_block_offset(self, tree: IterationRangesEntry) -> sympy.Symbol:\n+        return block_offsets[self._get_symt(tree)]\n+\n+    def _max_block_size(self, tree: IterationRangesEntry) -> int:\n+        return TRITON_MAX_BLOCK[tree.prefix.upper()]\n+\n     def codegen_range_tree(self):\n         for tree in self.range_trees:\n             # reduction indexing goes inside a loop\n"
                },
                {
                    "old_start": 1187,
                    "old_length": 27,
                    "new_start": 1298,
                    "new_length": 241,
                    "hunk": "@@ -1187,27 +1298,241 @@ class TritonKernel(SIMDKernel):\n             # workaround https://github.com/openai/triton/issues/2821\n             and self.index_dtype == \"tl.int32\"\n         ):\n-            index_relative_to_xyr_index = sympy_subs(\n-                index, {v: t.expr for v, t in self.range_tree_nodes.items()}\n-            )\n-            range_trees = self.active_range_trees(reorder=True)\n-            symbols = [t.symbol() for t in range_trees]\n-            strides = [sympy.Wild(f\"stride_{s}\", exclude=symbols) for s in symbols]\n-            offset = sympy.Wild(\"_offset\", exclude=symbols)\n-            m = index_relative_to_xyr_index.match(sympy_dot(symbols, strides) + offset)\n-            # TODO(jansel): it is sometimes possible to do higher dimensional block_ptrs with\n-            #               a tl.reshape the correct block.  We will miss these cases today.\n-            if m:\n-                self.filter_masks(mask_vars)\n-                from .triton import BlockPtrOptions\n \n+            def match_strided_block(\n+                index: sympy.Expr, range_tree: IterationRangesEntry\n+            ) -> Optional[BlockParameters]:\n+                \"\"\"\n+                Matches expressions of the form:\n+                    idx = s * xindex\n+\n+                This implies stride (s,), and shape (XBLOCK,).\n+                \"\"\"\n+                symbol = range_tree.symbol()\n+                stride = sympy.Wild(\"stride\", exclude=[symbol])\n+                m = index.match(symbol * stride)\n+                if m is None:\n+                    return None\n+\n+                return BlockParameters(\n+                    shape=[range_tree.numel],\n+                    block_shape=[self._get_block_size(range_tree)],\n+                    strides=[m[stride]],\n+                    offsets=[self._get_block_offset(range_tree)],\n+                )\n+\n+            def match_mod_div_block(\n+                index: sympy.Expr, range_tree: IterationRangesEntry\n+            ) -> Optional[BlockParameters]:\n+                \"\"\"\n+                Matches higher-dimensional blocks coming from FloorDiv and ModularIndexing.\n+\n+                Example expression to match:\n+                   sN * ((rindex//(d1 * ... * d(N-1))))\n+                       + s1 * ModularIndexing(rindex, 1, d1)\n+                       + ...\n+                       + s(N-1) * ModularIndexing(rindex, d1 * ... * d(N-2), d(N-1))\n+\n+                This iterates over a block of shape (dN, ..., d1) and stride\n+                (sN, ..., s1). (d1,...,d(N-1)) and (s1,...,sN) are\n+                wildcards that we match.\n+\n+                Note that dN does not appear in the expression, but we solve for it\n+                using range tree numels and the other dims.\n+                \"\"\"\n+                # Bound the possible number of dims. We use the following heuristics:\n+                # - At least one dim for each range tree node.\n+                # - At least one dim for every FloorDiv or ModularIndexing op.\n+                # - At least 2 dims to pattern match.\n+                num_dims = max(\n+                    2,\n+                    len(self.range_tree_nodes),\n+                    (index.count(FloorDiv) + index.count(ModularIndexing)),\n+                )\n+\n+                # Pattern match to find the strides and offset.\n+                index_var = range_tree.symbol()\n+                wild = functools.partial(sympy.Wild, exclude=[index_var])\n+                dims: List[sympy.Expr] = [\n+                    wild(f\"dim_mod{idx}\") for idx in range(num_dims)\n+                ]\n+                strides: List[sympy.Expr] = [\n+                    wild(f\"stride_mod{idx}\") for idx in range(num_dims)\n+                ]\n+\n+                def get_slice_numels(dims: List[Any]) -> List[Any]:\n+                    \"\"\"\n+                    Compute the cumulative size of each dimension's slice.\n+                    This proceeds from the last dim up to the second.\n+                    \"\"\"\n+                    numels = [sympy.Integer(1)]\n+                    for dim in dims[:0:-1]:\n+                        numel = dim * numels[0]\n+                        numels.insert(0, numel)\n+                    return numels\n+\n+                # The first dimension's index is computed by division.\n+                # The remaining are computed by modulo.\n+                slice_numels = get_slice_numels(dims[:num_dims])\n+                block_index_exprs = [FloorDiv(index_var, slice_numels[0])] + [\n+                    ModularIndexing(index_var, numel, dim)\n+                    for dim, numel in zip(dims[1:], slice_numels[1:])\n+                ]\n+\n+                # Calculate a linear index from block indices.\n+                match_expr = sympy_dot(strides, block_index_exprs)\n+\n+                # Pattern match.\n+                match = index.match(match_expr)\n+                if match is None:\n+                    return None\n+\n+                # Provide default values for unmatched dims and strides.\n+                for dim in dims[1:]:\n+                    if dim not in match:\n+                        match[dim] = sympy.Integer(1)\n+                for stride in strides[1:]:\n+                    if stride not in match:\n+                        match[stride] = sympy.Integer(0)\n+\n+                sizevars = V.graph.sizevars\n+\n+                def get_match(expr: sympy.Expr) -> sympy.Expr:\n+                    return sizevars.lookup_precomputed_size(match[expr])\n+\n+                # Replace wildcards with matched expressions.\n+                dims = [dims[0]] + [get_match(dim) for dim in dims[1:]]\n+                strides = [get_match(stride) for stride in strides]\n+                slice_numels = get_slice_numels(dims)\n+                block_index_exprs = [\n+                    sympy_subs(expr, match) for expr in block_index_exprs\n+                ]\n+\n+                # The leading dimension is not directly matched in our expression.\n+                # We solve for it by dividing the range tree numel by the product of\n+                # all other dimensions. We quit if they are not known to be divisible.\n+                assert (\n+                    dims[0] not in match\n+                ), \"Expected not to match the leading dimension!\"\n+                if not sizevars.statically_known_multiple_of(\n+                    range_tree.numel, slice_numels[0]\n+                ):\n+                    return None\n+                dims[0] = range_tree.numel / slice_numels[0]\n+\n+                # Check for applicable iteration range sizes.\n+                # When mapping a 1D block into an ND one, we need to know that\n+                # the number of elements is not changed. This means the slice numels of\n+                # the ND iteration range must evenly divide the length of the 1D block.\n+                # There are two cases where we can guarantee this:\n+                #  1. Numels are powers of 2. If numel == 2 ** n, and we know XBLOCK == 2 ** m,\n+                #     with n and m integers, then either numel is a multiple of XBLOCK, or numel\n+                #     is less than XBLOCK. (If numel is less than XBLOCK, we round up to 1 below.)\n+                #  2. Numels are multiples of the maximum possible block size.\n+                max_block = self._max_block_size(range_tree)\n+                if any(\n+                    not sizevars.statically_known_multiple_of(numel, max_block)\n+                    and not sizevars.statically_known_power_of_2(numel)\n+                    for numel in slice_numels\n+                ):\n+                    return None\n+\n+                def identity(expr: sympy.Expr) -> sympy.Expr:\n+                    return expr\n+\n+                # Compute the ND block shape from the linear block size.\n+                # Use CielDiv to round leading dimensions up to 1.\n+                # Non-leading dimensions are clamped to the size of the iteration range,\n+                # while the leading dimension can exceed this to accomodate a larger\n+                # block size.\n+                linear_block_size = self._get_block_size(range_tree)\n+                block_shape: List[sympy.Expr] = [\n+                    CeilDiv(linear_block_size, slice_numels[0])\n+                ] + [\n+                    sympy.Min(CeilDiv(linear_block_size, numel), dim)\n+                    for numel, dim in zip(slice_numels[1:], dims[1:])\n+                ]\n+\n+                # Compute block offsets from {xyzr}offset and the matched expressions.\n+                block_offsets: List[sympy.Expr] = [\n+                    sympy_subs(expr, {index_var: self._get_block_offset(range_tree)})\n+                    for expr in block_index_exprs\n+                ]\n+\n+                return BlockParameters(\n+                    shape=dims,\n+                    block_shape=block_shape,\n+                    strides=strides,\n+                    offsets=block_offsets,\n+                )\n+\n+            def match_block_pointer_subexpr(\n+                expr: sympy.Expr, range_tree: IterationRangesEntry\n+            ) -> Optional[BlockParameters]:\n+                \"\"\"\n+                Match a block indexing subexpression involving a single range tree.\n+                \"\"\"\n+                for match_func in (\n+                    match_strided_block,\n+                    match_mod_div_block,\n+                ):\n+                    match = match_func(expr, range_tree)\n+                    if match is not None:\n+                        return match\n+\n+                return None\n+\n+            def match_block_pointer() -> Optional[BlockPtrOptions]:\n+                index_relative_to_xyr_index = sympy_subs(\n+                    index, {v: t.expr for v, t in self.range_tree_nodes.items()}\n+                )\n+                range_trees = self.active_range_trees(reorder=True)\n+\n+                # Match each range tree separately.\n+                range_symbols = {tree.symbol() for tree in range_trees}\n+                index_terms = sympy.Add.make_args(index_relative_to_xyr_index)\n+                block_params = BlockParameters()\n+                for tree in range_trees:\n+                    # Partition the index into subexpressions pertaining to each range tree.\n+                    # For example xindex * 5 + rindex * 3 is partitioned to\n+                    # (xindex * 5, rindex * 3).\n+                    symbol = tree.symbol()\n+                    subexpr = sympy.Integer(0) + sum(\n+                        expr for expr in index_terms if symbol in expr.free_symbols\n+                    )\n+\n+                    # Reject mixed terms, e.g. xindex * rindex.\n+                    # NB: the zero expression is allowed, for broadcasting.\n+                    if len(range_symbols.intersection(subexpr.free_symbols)) > 1:\n+                        return None\n+\n+                    # Match the subexpression for this range tree.\n+                    params = match_block_pointer_subexpr(subexpr, tree)\n+                    if params is None:\n+                        return None\n+                    block_params += params\n+\n+                # Collect leftover terms as a constant offset.\n+                offset = sum(\n+                    expr\n+                    for expr in index_terms\n+                    if not range_symbols.intersection(expr.free_symbols)\n+                )\n+\n+                # Form the block pointer.\n+                self.filter_masks(mask_vars)\n                 return BlockPtrOptions.create(\n-                    [m[s] for s in strides],\n-                    m[offset],\n-                    range_trees,\n-                    mask_vars,  # type: ignore[arg-type]\n+                    params=block_params,\n+                    constant_offset=offset,\n+                    range_trees=range_trees,\n+                    mask_vars=mask_vars,\n                 )\n \n+            # Return a block pointer, if indexing matches the pattern.\n+            options = match_block_pointer()\n+            if options is not None:\n+                return options\n+\n         expand_str = None\n         index_str = self.index_to_str(index)\n         if isinstance(index, sympy.Integer):\n"
                },
                {
                    "old_start": 1274,
                    "old_length": 7,
                    "new_start": 1599,
                    "new_length": 8,
                    "hunk": "@@ -1274,7 +1599,8 @@ class TritonKernel(SIMDKernel):\n             f\"tl.broadcast_to({value}, {self.index_to_str(indexing.reshape_suffix)})\"\n         )\n         # drop any extra size=1 dimensions\n-        value = triton_reshape(value, indexing.reshape_suffix, indexing.block_shape)\n+        block_shape = [V.kernel.index_to_str(expr) for expr in indexing.block_shape]\n+        value = triton_reshape(value, indexing.reshape_suffix, block_shape)\n         # workaround https://github.com/openai/triton/issues/2814\n         value = f\"{value}.to({triton_store_type(V.graph.get_dtype(name))})\"\n         return f\"tl.store({block_ptr}, {value}{other})\"\n"
                },
                {
                    "old_start": 1381,
                    "old_length": 9,
                    "new_start": 1707,
                    "new_length": 8,
                    "hunk": "@@ -1381,9 +1707,8 @@ class TritonKernel(SIMDKernel):\n                 )\n                 line = f\"tl.load({block_ptr}{other}{ep})\"\n                 # add needed size=1 dimensions\n-                line = triton_reshape(\n-                    line, indexing.block_shape, indexing.reshape_suffix\n-                )\n+                block_shape = [str(dim) for dim in indexing.block_shape]\n+                line = triton_reshape(line, block_shape, indexing.reshape_suffix)\n             elif isinstance(original_index, sympy.Integer):\n                 line = f\"tl.load({var} + ({original_index}))\"\n                 append_broadcast = indexing.expand_str\n"
                }
            ],
            "whole_deleted": "-from ...utils._sympy.symbol import free_symbol_is_type, symbol_is_type, SymT\n-    mask_vars: Set[sympy.Symbol]\n-    shape: List[sympy.Expr]\n-    strides: List[sympy.Expr]\n-    block_shape: List[str]\n-    offsets: List[str]\n-    mask_vars: Set[sympy.Symbol]\n-        strides: List[sympy.Expr],\n-        mask_vars: Set[sympy.Symbol],\n-        block_shape = [f\"{t.prefix.upper()}BLOCK\" for t in range_trees]\n-        reshape_suffix = [*block_shape]\n-        broadcasting_dim = [s == 0 for s in strides]\n-            if is_broadcasting:\n-            and len(strides) == len(V.kernel.numels) - 1\n-                if not is_broadcasting\n-            shape=[\n-                V.graph.sizevars.lookup_precomputed_size(t.numel)\n-                for t in filter(range_trees)\n-            ],\n-            strides=[*map(V.graph.sizevars.lookup_precomputed_size, filter(strides))],\n-            block_shape=filter(block_shape),\n-            order=V.graph.sizevars.guarded_order(filter(strides)),\n-            offsets=filter([f\"{t.prefix}offset\" for t in range_trees]),\n-            offsets[offsets.index(\"roffset\")] = \"0\"\n-        check = []\n-        for i in range(len(self.shape)):\n-                self.block_shape[i] != \"1\"\n-                and not V.graph.sizevars.statically_known_equals(self.strides[i], 0)  # type: ignore[arg-type]\n-                and not V.graph.sizevars.statically_known_multiple_of(\n-                    self.shape[i],\n-                    TRITON_MAX_BLOCK[self.block_shape[i][0]],  # type: ignore[arg-type]\n-                and not (V.kernel.no_x_dim and self.block_shape[i] == \"XBLOCK\")\n-            ):\n-                check.append(i)\n-        return check\n-        \"\"\"Codegen string to pass to tl.advance(name, ...)\"\"\"\n-        advance = [\"0\"] * len(self.shape)\n-        advance[self.offsets.index(\"roffset\")] = \"RBLOCK\"\n-    def has_rindex(self):\n-        return \"RBLOCK\" in self.block_shape\n-    def _print_Min(self, expr):\n-        a = self._print(sympy.Min(*expr.args[:mid]))\n-        b = self._print(sympy.Min(*expr.args[mid:]))\n-        return f\"tl.minimum({a}, {b})\"\n-    def _print_Max(self, expr):\n-        nargs = len(expr.args)\n-        if len(expr.args) == 1:\n-            return self._print(expr.args[0])\n-        mid = len(expr.args) // 2\n-        a = self._print(sympy.Max(*expr.args[:mid]))\n-        b = self._print(sympy.Max(*expr.args[mid:]))\n-        return f\"tl.maximum({a}, {b})\"\n-            index_relative_to_xyr_index = sympy_subs(\n-                index, {v: t.expr for v, t in self.range_tree_nodes.items()}\n-            )\n-            range_trees = self.active_range_trees(reorder=True)\n-            symbols = [t.symbol() for t in range_trees]\n-            strides = [sympy.Wild(f\"stride_{s}\", exclude=symbols) for s in symbols]\n-            offset = sympy.Wild(\"_offset\", exclude=symbols)\n-            m = index_relative_to_xyr_index.match(sympy_dot(symbols, strides) + offset)\n-            # TODO(jansel): it is sometimes possible to do higher dimensional block_ptrs with\n-            #               a tl.reshape the correct block.  We will miss these cases today.\n-            if m:\n-                self.filter_masks(mask_vars)\n-                from .triton import BlockPtrOptions\n-                    [m[s] for s in strides],\n-                    m[offset],\n-                    range_trees,\n-                    mask_vars,  # type: ignore[arg-type]\n-        value = triton_reshape(value, indexing.reshape_suffix, indexing.block_shape)\n-                line = triton_reshape(\n-                    line, indexing.block_shape, indexing.reshape_suffix\n-                )\n",
            "whole_added": "+    Iterable,\n+from torch.utils._sympy.functions import CeilDiv, FloorDiv, ModularIndexing\n+from ...utils._sympy.symbol import free_symbol_is_type, prefix_str, symbol_is_type, SymT\n+block_offsets = {\n+    symt: sympy.Symbol(f\"{prefix_str[symt]}offset\", integer=True)\n+    for symt in [SymT.XBLOCK, SymT.YBLOCK, SymT.RINDEX]\n+}\n+\n+block_sizes = {\n+    symt: sympy.Symbol(f\"{prefix_str[symt].upper()}BLOCK\", integer=True, nonzero=True)\n+    for symt in [SymT.XBLOCK, SymT.YBLOCK, SymT.RINDEX]\n+}\n+\n+\n+    mask_vars: Set[str]\n+    params: BlockParameters\n+    mask_vars: Set[str]\n+    @property\n+    def shape(self) -> List[sympy.Expr]:\n+        return self.params.shape\n+\n+    @property\n+    def block_shape(self) -> List[sympy.Expr]:\n+        return self.params.block_shape\n+\n+    @property\n+    def strides(self) -> List[sympy.Expr]:\n+        return self.params.strides\n+\n+    @property\n+    def offsets(self) -> List[sympy.Expr]:\n+        return self.params.offsets\n+\n+        *,\n+        params: BlockParameters,\n+        mask_vars: Set[str],\n+        reshape_suffix = [f\"{t.prefix.upper()}BLOCK\" for t in range_trees]\n+\n+        # Only drop broadcast dims if the output has the same\n+        # rank as the block. Otherwise, we will get shape errors.\n+        drop_broadcasts = len(reshape_suffix) == len(params.strides)\n+        broadcasting_dim = [s == 0 for s in params.strides]\n+            if is_broadcasting and drop_broadcasts:\n+            and len(params.strides) == len(V.kernel.numels) - 1\n+                if not is_broadcasting or not drop_broadcasts\n+        # Drop broadcasting dimensions from the input.\n+        params = BlockParameters(\n+            **{key: filter(val) for key, val in dataclasses.asdict(params).items()}\n+        )\n+\n+        def lookup_size(exprs: Iterable[sympy.Expr]) -> List[sympy.Expr]:\n+            return [V.graph.sizevars.lookup_precomputed_size(expr) for expr in exprs]\n+\n+        # Look up precomputed sizes\n+        params.shape = lookup_size(params.shape)\n+        params.strides = lookup_size(params.strides)\n+\n+            params=params,\n+            order=list(reversed(range(len(params.shape)))),\n+    def replace_roffset(self, expr: sympy.Expr, replacement: sympy.Expr) -> sympy.Expr:\n+        \"\"\"\n+        Replaces instances of roffset with the new expression.\n+        \"\"\"\n+        roffset = block_offsets[SymT.RINDEX]\n+        return sympy_subs(expr, {roffset: replacement})\n+\n+            offsets = [\n+                self.replace_roffset(offset, sympy.Integer(0)) for offset in offsets\n+            ]\n+        sizevars = V.graph.sizevars\n+\n+        # Substitute maximum block sizes in shape expressions.\n+        # This works in multiple_of checks because block sizes are powers of 2.\n+        block_to_max: Dict[sympy.Expr, Any] = {\n+            block_size: TRITON_MAX_BLOCK[prefix_str[symt].upper()]\n+            for symt, block_size in block_sizes.items()\n+        }\n+\n+        return [\n+            idx\n+            for idx in range(len(self.shape))\n+                not sizevars.statically_known_equals(\n+                    self.strides[idx], sympy.Integer(0)\n+                and not sizevars.statically_known_multiple_of(\n+                    self.shape[idx], self.block_shape[idx]\n+                )\n+                and not sizevars.statically_known_multiple_of(\n+                    self.shape[idx], sympy_subs(self.block_shape[idx], block_to_max)\n+                )\n+                and not (\n+                    V.kernel.no_x_dim\n+                    and self.block_shape[idx] == block_sizes[SymT.XBLOCK]\n+                )\n+            )\n+        ]\n+        \"\"\"\n+        Codegen string to pass to tl.advance(name, ...).\n+\n+        Advance is the difference between offsets in each loop iteration.\n+        To compute it, we replace roffset with multiples of RBLOCK.\n+        Since we expect roffset to vary in range(0, rnumel, RBLOCK), the first\n+        iteration has roffset=0, while the second has roffset=RBLOCK.\n+        \"\"\"\n+        rblock = block_sizes[SymT.RINDEX]\n+        advance = [\n+            (\n+                self.replace_roffset(offset, rblock)\n+                - self.replace_roffset(offset, sympy.Integer(0))\n+            )\n+            for offset in self.offsets\n+        ]\n+    def has_rindex(self) -> bool:\n+        return any(free_symbol_is_type(expr, SymT.RINDEX) for expr in self.block_shape)\n+    def _print_min_max_helper(self, expr: sympy.Expr, cmp: str) -> str:\n+        \"\"\"\n+        Helper for max/min code genereration.\n+        cmp: > or <\n+        \"\"\"\n+        cls = type(expr)\n+        a = self._print(cls(*expr.args[:mid]))\n+        b = self._print(cls(*expr.args[mid:]))\n+        # Use a macro so we can propagate constexprs.\n+        # https://github.com/triton-lang/triton/issues/3815\n+        a, b = tuple(f\"({x})\" for x in (a, b))\n+        assert cmp in {\">\", \"<\"}, f\"Unexpected comparator: '{cmp}'\"\n+        return f\"({a} * ({a} {cmp}= {b}) + {b} * ({b} {cmp} {a}))\"\n+    def _print_Min(self, expr):\n+        return self._print_min_max_helper(expr, \"<\")\n+    def _print_Max(self, expr):\n+        return self._print_min_max_helper(expr, \">\")\n+@dataclasses.dataclass\n+class BlockParameters:\n+    \"\"\"\n+    Class representing ND block dimensions, for block pointer analysis.\n+    \"\"\"\n+\n+    shape: List[sympy.Expr] = dataclasses.field(default_factory=list)\n+    block_shape: List[sympy.Expr] = dataclasses.field(default_factory=list)\n+    strides: List[sympy.Expr] = dataclasses.field(default_factory=list)\n+    offsets: List[sympy.Expr] = dataclasses.field(default_factory=list)\n+\n+    def __add__(self, other: BlockParameters) -> BlockParameters:\n+        \"\"\"\n+        Concatenates block parameters.\n+        \"\"\"\n+        cls = type(self)\n+        a, b = tuple(dataclasses.asdict(x) for x in (self, other))\n+        return cls(**{key: a[key] + b[key] for key in a})\n+\n+\n+    def _get_symt(self, tree: IterationRangesEntry) -> SymT:\n+        prefix_to_symt = {prefix: symt for symt, prefix in prefix_str.items()}\n+        return prefix_to_symt[tree.prefix]\n+\n+    def _get_block_size(self, tree: IterationRangesEntry) -> sympy.Symbol:\n+        return block_sizes[self._get_symt(tree)]\n+\n+    def _get_block_offset(self, tree: IterationRangesEntry) -> sympy.Symbol:\n+        return block_offsets[self._get_symt(tree)]\n+\n+    def _max_block_size(self, tree: IterationRangesEntry) -> int:\n+        return TRITON_MAX_BLOCK[tree.prefix.upper()]\n+\n+            def match_strided_block(\n+                index: sympy.Expr, range_tree: IterationRangesEntry\n+            ) -> Optional[BlockParameters]:\n+                \"\"\"\n+                Matches expressions of the form:\n+                    idx = s * xindex\n+\n+                This implies stride (s,), and shape (XBLOCK,).\n+                \"\"\"\n+                symbol = range_tree.symbol()\n+                stride = sympy.Wild(\"stride\", exclude=[symbol])\n+                m = index.match(symbol * stride)\n+                if m is None:\n+                    return None\n+\n+                return BlockParameters(\n+                    shape=[range_tree.numel],\n+                    block_shape=[self._get_block_size(range_tree)],\n+                    strides=[m[stride]],\n+                    offsets=[self._get_block_offset(range_tree)],\n+                )\n+\n+            def match_mod_div_block(\n+                index: sympy.Expr, range_tree: IterationRangesEntry\n+            ) -> Optional[BlockParameters]:\n+                \"\"\"\n+                Matches higher-dimensional blocks coming from FloorDiv and ModularIndexing.\n+\n+                Example expression to match:\n+                   sN * ((rindex//(d1 * ... * d(N-1))))\n+                       + s1 * ModularIndexing(rindex, 1, d1)\n+                       + ...\n+                       + s(N-1) * ModularIndexing(rindex, d1 * ... * d(N-2), d(N-1))\n+\n+                This iterates over a block of shape (dN, ..., d1) and stride\n+                (sN, ..., s1). (d1,...,d(N-1)) and (s1,...,sN) are\n+                wildcards that we match.\n+\n+                Note that dN does not appear in the expression, but we solve for it\n+                using range tree numels and the other dims.\n+                \"\"\"\n+                # Bound the possible number of dims. We use the following heuristics:\n+                # - At least one dim for each range tree node.\n+                # - At least one dim for every FloorDiv or ModularIndexing op.\n+                # - At least 2 dims to pattern match.\n+                num_dims = max(\n+                    2,\n+                    len(self.range_tree_nodes),\n+                    (index.count(FloorDiv) + index.count(ModularIndexing)),\n+                )\n+\n+                # Pattern match to find the strides and offset.\n+                index_var = range_tree.symbol()\n+                wild = functools.partial(sympy.Wild, exclude=[index_var])\n+                dims: List[sympy.Expr] = [\n+                    wild(f\"dim_mod{idx}\") for idx in range(num_dims)\n+                ]\n+                strides: List[sympy.Expr] = [\n+                    wild(f\"stride_mod{idx}\") for idx in range(num_dims)\n+                ]\n+\n+                def get_slice_numels(dims: List[Any]) -> List[Any]:\n+                    \"\"\"\n+                    Compute the cumulative size of each dimension's slice.\n+                    This proceeds from the last dim up to the second.\n+                    \"\"\"\n+                    numels = [sympy.Integer(1)]\n+                    for dim in dims[:0:-1]:\n+                        numel = dim * numels[0]\n+                        numels.insert(0, numel)\n+                    return numels\n+\n+                # The first dimension's index is computed by division.\n+                # The remaining are computed by modulo.\n+                slice_numels = get_slice_numels(dims[:num_dims])\n+                block_index_exprs = [FloorDiv(index_var, slice_numels[0])] + [\n+                    ModularIndexing(index_var, numel, dim)\n+                    for dim, numel in zip(dims[1:], slice_numels[1:])\n+                ]\n+\n+                # Calculate a linear index from block indices.\n+                match_expr = sympy_dot(strides, block_index_exprs)\n+\n+                # Pattern match.\n+                match = index.match(match_expr)\n+                if match is None:\n+                    return None\n+\n+                # Provide default values for unmatched dims and strides.\n+                for dim in dims[1:]:\n+                    if dim not in match:\n+                        match[dim] = sympy.Integer(1)\n+                for stride in strides[1:]:\n+                    if stride not in match:\n+                        match[stride] = sympy.Integer(0)\n+\n+                sizevars = V.graph.sizevars\n+\n+                def get_match(expr: sympy.Expr) -> sympy.Expr:\n+                    return sizevars.lookup_precomputed_size(match[expr])\n+\n+                # Replace wildcards with matched expressions.\n+                dims = [dims[0]] + [get_match(dim) for dim in dims[1:]]\n+                strides = [get_match(stride) for stride in strides]\n+                slice_numels = get_slice_numels(dims)\n+                block_index_exprs = [\n+                    sympy_subs(expr, match) for expr in block_index_exprs\n+                ]\n+\n+                # The leading dimension is not directly matched in our expression.\n+                # We solve for it by dividing the range tree numel by the product of\n+                # all other dimensions. We quit if they are not known to be divisible.\n+                assert (\n+                    dims[0] not in match\n+                ), \"Expected not to match the leading dimension!\"\n+                if not sizevars.statically_known_multiple_of(\n+                    range_tree.numel, slice_numels[0]\n+                ):\n+                    return None\n+                dims[0] = range_tree.numel / slice_numels[0]\n+\n+                # Check for applicable iteration range sizes.\n+                # When mapping a 1D block into an ND one, we need to know that\n+                # the number of elements is not changed. This means the slice numels of\n+                # the ND iteration range must evenly divide the length of the 1D block.\n+                # There are two cases where we can guarantee this:\n+                #  1. Numels are powers of 2. If numel == 2 ** n, and we know XBLOCK == 2 ** m,\n+                #     with n and m integers, then either numel is a multiple of XBLOCK, or numel\n+                #     is less than XBLOCK. (If numel is less than XBLOCK, we round up to 1 below.)\n+                #  2. Numels are multiples of the maximum possible block size.\n+                max_block = self._max_block_size(range_tree)\n+                if any(\n+                    not sizevars.statically_known_multiple_of(numel, max_block)\n+                    and not sizevars.statically_known_power_of_2(numel)\n+                    for numel in slice_numels\n+                ):\n+                    return None\n+\n+                def identity(expr: sympy.Expr) -> sympy.Expr:\n+                    return expr\n+\n+                # Compute the ND block shape from the linear block size.\n+                # Use CielDiv to round leading dimensions up to 1.\n+                # Non-leading dimensions are clamped to the size of the iteration range,\n+                # while the leading dimension can exceed this to accomodate a larger\n+                # block size.\n+                linear_block_size = self._get_block_size(range_tree)\n+                block_shape: List[sympy.Expr] = [\n+                    CeilDiv(linear_block_size, slice_numels[0])\n+                ] + [\n+                    sympy.Min(CeilDiv(linear_block_size, numel), dim)\n+                    for numel, dim in zip(slice_numels[1:], dims[1:])\n+                ]\n+\n+                # Compute block offsets from {xyzr}offset and the matched expressions.\n+                block_offsets: List[sympy.Expr] = [\n+                    sympy_subs(expr, {index_var: self._get_block_offset(range_tree)})\n+                    for expr in block_index_exprs\n+                ]\n+\n+                return BlockParameters(\n+                    shape=dims,\n+                    block_shape=block_shape,\n+                    strides=strides,\n+                    offsets=block_offsets,\n+                )\n+\n+            def match_block_pointer_subexpr(\n+                expr: sympy.Expr, range_tree: IterationRangesEntry\n+            ) -> Optional[BlockParameters]:\n+                \"\"\"\n+                Match a block indexing subexpression involving a single range tree.\n+                \"\"\"\n+                for match_func in (\n+                    match_strided_block,\n+                    match_mod_div_block,\n+                ):\n+                    match = match_func(expr, range_tree)\n+                    if match is not None:\n+                        return match\n+\n+                return None\n+\n+            def match_block_pointer() -> Optional[BlockPtrOptions]:\n+                index_relative_to_xyr_index = sympy_subs(\n+                    index, {v: t.expr for v, t in self.range_tree_nodes.items()}\n+                )\n+                range_trees = self.active_range_trees(reorder=True)\n+\n+                # Match each range tree separately.\n+                range_symbols = {tree.symbol() for tree in range_trees}\n+                index_terms = sympy.Add.make_args(index_relative_to_xyr_index)\n+                block_params = BlockParameters()\n+                for tree in range_trees:\n+                    # Partition the index into subexpressions pertaining to each range tree.\n+                    # For example xindex * 5 + rindex * 3 is partitioned to\n+                    # (xindex * 5, rindex * 3).\n+                    symbol = tree.symbol()\n+                    subexpr = sympy.Integer(0) + sum(\n+                        expr for expr in index_terms if symbol in expr.free_symbols\n+                    )\n+\n+                    # Reject mixed terms, e.g. xindex * rindex.\n+                    # NB: the zero expression is allowed, for broadcasting.\n+                    if len(range_symbols.intersection(subexpr.free_symbols)) > 1:\n+                        return None\n+\n+                    # Match the subexpression for this range tree.\n+                    params = match_block_pointer_subexpr(subexpr, tree)\n+                    if params is None:\n+                        return None\n+                    block_params += params\n+\n+                # Collect leftover terms as a constant offset.\n+                offset = sum(\n+                    expr\n+                    for expr in index_terms\n+                    if not range_symbols.intersection(expr.free_symbols)\n+                )\n+\n+                # Form the block pointer.\n+                self.filter_masks(mask_vars)\n+                    params=block_params,\n+                    constant_offset=offset,\n+                    range_trees=range_trees,\n+                    mask_vars=mask_vars,\n+            # Return a block pointer, if indexing matches the pattern.\n+            options = match_block_pointer()\n+            if options is not None:\n+                return options\n+\n+        block_shape = [V.kernel.index_to_str(expr) for expr in indexing.block_shape]\n+        value = triton_reshape(value, indexing.reshape_suffix, block_shape)\n+                block_shape = [str(dim) for dim in indexing.block_shape]\n+                line = triton_reshape(line, block_shape, indexing.reshape_suffix)\n",
            "whole_hunk": "@@ -13,6 +13,7 @@ from typing import (\n     Callable,\n     cast,\n     Dict,\n+    Iterable,\n     List,\n     Optional,\n     Set,\n@@ -29,8 +30,9 @@ from torch._dynamo.utils import preserve_rng_state\n \n from torch._inductor.runtime.hints import AutotuneHint, DeviceProperties\n from torch._prims_common import is_integer_dtype\n+from torch.utils._sympy.functions import CeilDiv, FloorDiv, ModularIndexing\n from torch.utils._triton import has_triton_package\n-from ...utils._sympy.symbol import free_symbol_is_type, symbol_is_type, SymT\n+from ...utils._sympy.symbol import free_symbol_is_type, prefix_str, symbol_is_type, SymT\n from ...utils._sympy.value_ranges import ValueRanges\n \n from .. import config, ir\n@@ -119,10 +121,21 @@ def gen_common_triton_imports():\n     return imports.getvalue()\n \n \n+block_offsets = {\n+    symt: sympy.Symbol(f\"{prefix_str[symt]}offset\", integer=True)\n+    for symt in [SymT.XBLOCK, SymT.YBLOCK, SymT.RINDEX]\n+}\n+\n+block_sizes = {\n+    symt: sympy.Symbol(f\"{prefix_str[symt].upper()}BLOCK\", integer=True, nonzero=True)\n+    for symt in [SymT.XBLOCK, SymT.YBLOCK, SymT.RINDEX]\n+}\n+\n+\n @dataclasses.dataclass\n class IndexingOptions:\n     index_str: str\n-    mask_vars: Set[sympy.Symbol]\n+    mask_vars: Set[str]\n     mask_str: str\n     expand_str: Optional[str]\n     _has_rindex: bool\n@@ -146,29 +159,46 @@ class IndexingOptions:\n \n @dataclasses.dataclass\n class BlockPtrOptions:\n+    params: BlockParameters\n     constant_offset: sympy.Expr\n-    shape: List[sympy.Expr]\n-    strides: List[sympy.Expr]\n-    block_shape: List[str]\n     order: List[int]\n-    offsets: List[str]\n-    mask_vars: Set[sympy.Symbol]\n+    mask_vars: Set[str]\n     reshape_suffix: List[str]\n \n+    @property\n+    def shape(self) -> List[sympy.Expr]:\n+        return self.params.shape\n+\n+    @property\n+    def block_shape(self) -> List[sympy.Expr]:\n+        return self.params.block_shape\n+\n+    @property\n+    def strides(self) -> List[sympy.Expr]:\n+        return self.params.strides\n+\n+    @property\n+    def offsets(self) -> List[sympy.Expr]:\n+        return self.params.offsets\n+\n     @staticmethod\n     def create(\n-        strides: List[sympy.Expr],\n+        *,\n+        params: BlockParameters,\n         constant_offset: sympy.Expr,\n         range_trees: List[IterationRangesEntry],\n-        mask_vars: Set[sympy.Symbol],\n+        mask_vars: Set[str],\n     ) -> BlockPtrOptions:\n         \"\"\"Helper to create a  BlockPtrOptions instance\"\"\"\n-        block_shape = [f\"{t.prefix.upper()}BLOCK\" for t in range_trees]\n-        reshape_suffix = [*block_shape]\n+        reshape_suffix = [f\"{t.prefix.upper()}BLOCK\" for t in range_trees]\n+\n+        # Only drop broadcast dims if the output has the same\n+        # rank as the block. Otherwise, we will get shape errors.\n+        drop_broadcasts = len(reshape_suffix) == len(params.strides)\n \n-        broadcasting_dim = [s == 0 for s in strides]\n+        broadcasting_dim = [s == 0 for s in params.strides]\n         for i, is_broadcasting in enumerate(broadcasting_dim):\n-            if is_broadcasting:\n+            if is_broadcasting and drop_broadcasts:\n                 # drop any stride==0 dimensions for performance\n                 reshape_suffix[i] = \"1\"\n \n@@ -178,7 +208,7 @@ class BlockPtrOptions:\n \n         if (\n             not V.kernel.inside_reduction\n-            and len(strides) == len(V.kernel.numels) - 1\n+            and len(params.strides) == len(V.kernel.numels) - 1\n             and V.kernel.numels[-1] != 1\n         ):\n             # Need to expand rank by 1 to match rank when self.inside_reduction=True\n@@ -190,23 +220,36 @@ class BlockPtrOptions:\n             return [\n                 item\n                 for item, is_broadcasting in zip(it, broadcasting_dim)\n-                if not is_broadcasting\n+                if not is_broadcasting or not drop_broadcasts\n             ]\n \n+        # Drop broadcasting dimensions from the input.\n+        params = BlockParameters(\n+            **{key: filter(val) for key, val in dataclasses.asdict(params).items()}\n+        )\n+\n+        def lookup_size(exprs: Iterable[sympy.Expr]) -> List[sympy.Expr]:\n+            return [V.graph.sizevars.lookup_precomputed_size(expr) for expr in exprs]\n+\n+        # Look up precomputed sizes\n+        params.shape = lookup_size(params.shape)\n+        params.strides = lookup_size(params.strides)\n+\n         return BlockPtrOptions(\n+            params=params,\n             constant_offset=V.graph.sizevars.lookup_precomputed_size(constant_offset),\n-            shape=[\n-                V.graph.sizevars.lookup_precomputed_size(t.numel)\n-                for t in filter(range_trees)\n-            ],\n-            strides=[*map(V.graph.sizevars.lookup_precomputed_size, filter(strides))],\n-            block_shape=filter(block_shape),\n-            order=V.graph.sizevars.guarded_order(filter(strides)),\n-            offsets=filter([f\"{t.prefix}offset\" for t in range_trees]),\n+            order=list(reversed(range(len(params.shape)))),\n             mask_vars=mask_vars,\n             reshape_suffix=reshape_suffix,\n         )\n \n+    def replace_roffset(self, expr: sympy.Expr, replacement: sympy.Expr) -> sympy.Expr:\n+        \"\"\"\n+        Replaces instances of roffset with the new expression.\n+        \"\"\"\n+        roffset = block_offsets[SymT.RINDEX]\n+        return sympy_subs(expr, {roffset: replacement})\n+\n     def format(self, name: str, roffset=True) -> str:\n         \"\"\"\n         Codegen a call to tl.make_block_ptr()\n@@ -221,7 +264,9 @@ class BlockPtrOptions:\n         f = V.kernel.index_to_str\n         offsets = [*self.offsets]\n         if not roffset:\n-            offsets[offsets.index(\"roffset\")] = \"0\"\n+            offsets = [\n+                self.replace_roffset(offset, sympy.Integer(0)) for offset in offsets\n+            ]\n         args = [\n             f\"{name} + ({f(self.constant_offset)})\"\n             if self.constant_offset != 0\n@@ -237,31 +282,59 @@ class BlockPtrOptions:\n     @cache_on_self\n     def boundary_check(self) -> List[int]:\n         \"\"\"List of indices to pass to tl.load(boundary_check=...)\"\"\"\n-        check = []\n-        for i in range(len(self.shape)):\n+        sizevars = V.graph.sizevars\n+\n+        # Substitute maximum block sizes in shape expressions.\n+        # This works in multiple_of checks because block sizes are powers of 2.\n+        block_to_max: Dict[sympy.Expr, Any] = {\n+            block_size: TRITON_MAX_BLOCK[prefix_str[symt].upper()]\n+            for symt, block_size in block_sizes.items()\n+        }\n+\n+        return [\n+            idx\n+            for idx in range(len(self.shape))\n             if (\n-                self.block_shape[i] != \"1\"\n-                and not V.graph.sizevars.statically_known_equals(self.strides[i], 0)  # type: ignore[arg-type]\n-                and not V.graph.sizevars.statically_known_multiple_of(\n-                    self.shape[i],\n-                    TRITON_MAX_BLOCK[self.block_shape[i][0]],  # type: ignore[arg-type]\n+                not sizevars.statically_known_equals(\n+                    self.strides[idx], sympy.Integer(0)\n                 )\n-                and not (V.kernel.no_x_dim and self.block_shape[i] == \"XBLOCK\")\n-            ):\n-                check.append(i)\n-        return check\n+                and not sizevars.statically_known_multiple_of(\n+                    self.shape[idx], self.block_shape[idx]\n+                )\n+                and not sizevars.statically_known_multiple_of(\n+                    self.shape[idx], sympy_subs(self.block_shape[idx], block_to_max)\n+                )\n+                and not (\n+                    V.kernel.no_x_dim\n+                    and self.block_shape[idx] == block_sizes[SymT.XBLOCK]\n+                )\n+            )\n+        ]\n \n     def advance_roffset(self):\n-        \"\"\"Codegen string to pass to tl.advance(name, ...)\"\"\"\n-        advance = [\"0\"] * len(self.shape)\n-        advance[self.offsets.index(\"roffset\")] = \"RBLOCK\"\n+        \"\"\"\n+        Codegen string to pass to tl.advance(name, ...).\n+\n+        Advance is the difference between offsets in each loop iteration.\n+        To compute it, we replace roffset with multiples of RBLOCK.\n+        Since we expect roffset to vary in range(0, rnumel, RBLOCK), the first\n+        iteration has roffset=0, while the second has roffset=RBLOCK.\n+        \"\"\"\n+        rblock = block_sizes[SymT.RINDEX]\n+        advance = [\n+            (\n+                self.replace_roffset(offset, rblock)\n+                - self.replace_roffset(offset, sympy.Integer(0))\n+            )\n+            for offset in self.offsets\n+        ]\n         return V.kernel.index_to_str(advance)\n \n     def has_indirect(self):\n         return False  # block_ptr can't do indirect indexing\n \n-    def has_rindex(self):\n-        return \"RBLOCK\" in self.block_shape\n+    def has_rindex(self) -> bool:\n+        return any(free_symbol_is_type(expr, SymT.RINDEX) for expr in self.block_shape)\n \n     def has_rmask(self):\n         return self.has_rindex()\n@@ -365,26 +438,31 @@ class TritonPrinter(PythonPrinter):\n         q = self.doprint(expr.args[2])\n         return f\"tl.where({c}, {p}, {q})\"\n \n-    def _print_Min(self, expr):\n+    def _print_min_max_helper(self, expr: sympy.Expr, cmp: str) -> str:\n+        \"\"\"\n+        Helper for max/min code genereration.\n+        cmp: > or <\n+        \"\"\"\n         nargs = len(expr.args)\n         if len(expr.args) == 1:\n             return self._print(expr.args[0])\n \n         mid = len(expr.args) // 2\n-        a = self._print(sympy.Min(*expr.args[:mid]))\n-        b = self._print(sympy.Min(*expr.args[mid:]))\n-        return f\"tl.minimum({a}, {b})\"\n+        cls = type(expr)\n+        a = self._print(cls(*expr.args[:mid]))\n+        b = self._print(cls(*expr.args[mid:]))\n \n-    def _print_Max(self, expr):\n-        nargs = len(expr.args)\n-        if len(expr.args) == 1:\n-            return self._print(expr.args[0])\n+        # Use a macro so we can propagate constexprs.\n+        # https://github.com/triton-lang/triton/issues/3815\n+        a, b = tuple(f\"({x})\" for x in (a, b))\n+        assert cmp in {\">\", \"<\"}, f\"Unexpected comparator: '{cmp}'\"\n+        return f\"({a} * ({a} {cmp}= {b}) + {b} * ({b} {cmp} {a}))\"\n \n-        mid = len(expr.args) // 2\n-        a = self._print(sympy.Max(*expr.args[:mid]))\n-        b = self._print(sympy.Max(*expr.args[mid:]))\n+    def _print_Min(self, expr):\n+        return self._print_min_max_helper(expr, \"<\")\n \n-        return f\"tl.maximum({a}, {b})\"\n+    def _print_Max(self, expr):\n+        return self._print_min_max_helper(expr, \">\")\n \n     def _print_Abs(self, expr):\n         assert len(expr.args) == 1\n@@ -1023,6 +1101,26 @@ class HelperFunctions:\n         return self.finalized_helpers[idx]\n \n \n+@dataclasses.dataclass\n+class BlockParameters:\n+    \"\"\"\n+    Class representing ND block dimensions, for block pointer analysis.\n+    \"\"\"\n+\n+    shape: List[sympy.Expr] = dataclasses.field(default_factory=list)\n+    block_shape: List[sympy.Expr] = dataclasses.field(default_factory=list)\n+    strides: List[sympy.Expr] = dataclasses.field(default_factory=list)\n+    offsets: List[sympy.Expr] = dataclasses.field(default_factory=list)\n+\n+    def __add__(self, other: BlockParameters) -> BlockParameters:\n+        \"\"\"\n+        Concatenates block parameters.\n+        \"\"\"\n+        cls = type(self)\n+        a, b = tuple(dataclasses.asdict(x) for x in (self, other))\n+        return cls(**{key: a[key] + b[key] for key in a})\n+\n+\n class TritonKernel(SIMDKernel):\n     overrides = TritonKernelOverrides  # type: ignore[assignment]\n     helper_functions: HelperFunctions\n@@ -1059,6 +1157,19 @@ class TritonKernel(SIMDKernel):\n \n         self.codegen_range_tree()\n \n+    def _get_symt(self, tree: IterationRangesEntry) -> SymT:\n+        prefix_to_symt = {prefix: symt for symt, prefix in prefix_str.items()}\n+        return prefix_to_symt[tree.prefix]\n+\n+    def _get_block_size(self, tree: IterationRangesEntry) -> sympy.Symbol:\n+        return block_sizes[self._get_symt(tree)]\n+\n+    def _get_block_offset(self, tree: IterationRangesEntry) -> sympy.Symbol:\n+        return block_offsets[self._get_symt(tree)]\n+\n+    def _max_block_size(self, tree: IterationRangesEntry) -> int:\n+        return TRITON_MAX_BLOCK[tree.prefix.upper()]\n+\n     def codegen_range_tree(self):\n         for tree in self.range_trees:\n             # reduction indexing goes inside a loop\n@@ -1187,27 +1298,241 @@ class TritonKernel(SIMDKernel):\n             # workaround https://github.com/openai/triton/issues/2821\n             and self.index_dtype == \"tl.int32\"\n         ):\n-            index_relative_to_xyr_index = sympy_subs(\n-                index, {v: t.expr for v, t in self.range_tree_nodes.items()}\n-            )\n-            range_trees = self.active_range_trees(reorder=True)\n-            symbols = [t.symbol() for t in range_trees]\n-            strides = [sympy.Wild(f\"stride_{s}\", exclude=symbols) for s in symbols]\n-            offset = sympy.Wild(\"_offset\", exclude=symbols)\n-            m = index_relative_to_xyr_index.match(sympy_dot(symbols, strides) + offset)\n-            # TODO(jansel): it is sometimes possible to do higher dimensional block_ptrs with\n-            #               a tl.reshape the correct block.  We will miss these cases today.\n-            if m:\n-                self.filter_masks(mask_vars)\n-                from .triton import BlockPtrOptions\n \n+            def match_strided_block(\n+                index: sympy.Expr, range_tree: IterationRangesEntry\n+            ) -> Optional[BlockParameters]:\n+                \"\"\"\n+                Matches expressions of the form:\n+                    idx = s * xindex\n+\n+                This implies stride (s,), and shape (XBLOCK,).\n+                \"\"\"\n+                symbol = range_tree.symbol()\n+                stride = sympy.Wild(\"stride\", exclude=[symbol])\n+                m = index.match(symbol * stride)\n+                if m is None:\n+                    return None\n+\n+                return BlockParameters(\n+                    shape=[range_tree.numel],\n+                    block_shape=[self._get_block_size(range_tree)],\n+                    strides=[m[stride]],\n+                    offsets=[self._get_block_offset(range_tree)],\n+                )\n+\n+            def match_mod_div_block(\n+                index: sympy.Expr, range_tree: IterationRangesEntry\n+            ) -> Optional[BlockParameters]:\n+                \"\"\"\n+                Matches higher-dimensional blocks coming from FloorDiv and ModularIndexing.\n+\n+                Example expression to match:\n+                   sN * ((rindex//(d1 * ... * d(N-1))))\n+                       + s1 * ModularIndexing(rindex, 1, d1)\n+                       + ...\n+                       + s(N-1) * ModularIndexing(rindex, d1 * ... * d(N-2), d(N-1))\n+\n+                This iterates over a block of shape (dN, ..., d1) and stride\n+                (sN, ..., s1). (d1,...,d(N-1)) and (s1,...,sN) are\n+                wildcards that we match.\n+\n+                Note that dN does not appear in the expression, but we solve for it\n+                using range tree numels and the other dims.\n+                \"\"\"\n+                # Bound the possible number of dims. We use the following heuristics:\n+                # - At least one dim for each range tree node.\n+                # - At least one dim for every FloorDiv or ModularIndexing op.\n+                # - At least 2 dims to pattern match.\n+                num_dims = max(\n+                    2,\n+                    len(self.range_tree_nodes),\n+                    (index.count(FloorDiv) + index.count(ModularIndexing)),\n+                )\n+\n+                # Pattern match to find the strides and offset.\n+                index_var = range_tree.symbol()\n+                wild = functools.partial(sympy.Wild, exclude=[index_var])\n+                dims: List[sympy.Expr] = [\n+                    wild(f\"dim_mod{idx}\") for idx in range(num_dims)\n+                ]\n+                strides: List[sympy.Expr] = [\n+                    wild(f\"stride_mod{idx}\") for idx in range(num_dims)\n+                ]\n+\n+                def get_slice_numels(dims: List[Any]) -> List[Any]:\n+                    \"\"\"\n+                    Compute the cumulative size of each dimension's slice.\n+                    This proceeds from the last dim up to the second.\n+                    \"\"\"\n+                    numels = [sympy.Integer(1)]\n+                    for dim in dims[:0:-1]:\n+                        numel = dim * numels[0]\n+                        numels.insert(0, numel)\n+                    return numels\n+\n+                # The first dimension's index is computed by division.\n+                # The remaining are computed by modulo.\n+                slice_numels = get_slice_numels(dims[:num_dims])\n+                block_index_exprs = [FloorDiv(index_var, slice_numels[0])] + [\n+                    ModularIndexing(index_var, numel, dim)\n+                    for dim, numel in zip(dims[1:], slice_numels[1:])\n+                ]\n+\n+                # Calculate a linear index from block indices.\n+                match_expr = sympy_dot(strides, block_index_exprs)\n+\n+                # Pattern match.\n+                match = index.match(match_expr)\n+                if match is None:\n+                    return None\n+\n+                # Provide default values for unmatched dims and strides.\n+                for dim in dims[1:]:\n+                    if dim not in match:\n+                        match[dim] = sympy.Integer(1)\n+                for stride in strides[1:]:\n+                    if stride not in match:\n+                        match[stride] = sympy.Integer(0)\n+\n+                sizevars = V.graph.sizevars\n+\n+                def get_match(expr: sympy.Expr) -> sympy.Expr:\n+                    return sizevars.lookup_precomputed_size(match[expr])\n+\n+                # Replace wildcards with matched expressions.\n+                dims = [dims[0]] + [get_match(dim) for dim in dims[1:]]\n+                strides = [get_match(stride) for stride in strides]\n+                slice_numels = get_slice_numels(dims)\n+                block_index_exprs = [\n+                    sympy_subs(expr, match) for expr in block_index_exprs\n+                ]\n+\n+                # The leading dimension is not directly matched in our expression.\n+                # We solve for it by dividing the range tree numel by the product of\n+                # all other dimensions. We quit if they are not known to be divisible.\n+                assert (\n+                    dims[0] not in match\n+                ), \"Expected not to match the leading dimension!\"\n+                if not sizevars.statically_known_multiple_of(\n+                    range_tree.numel, slice_numels[0]\n+                ):\n+                    return None\n+                dims[0] = range_tree.numel / slice_numels[0]\n+\n+                # Check for applicable iteration range sizes.\n+                # When mapping a 1D block into an ND one, we need to know that\n+                # the number of elements is not changed. This means the slice numels of\n+                # the ND iteration range must evenly divide the length of the 1D block.\n+                # There are two cases where we can guarantee this:\n+                #  1. Numels are powers of 2. If numel == 2 ** n, and we know XBLOCK == 2 ** m,\n+                #     with n and m integers, then either numel is a multiple of XBLOCK, or numel\n+                #     is less than XBLOCK. (If numel is less than XBLOCK, we round up to 1 below.)\n+                #  2. Numels are multiples of the maximum possible block size.\n+                max_block = self._max_block_size(range_tree)\n+                if any(\n+                    not sizevars.statically_known_multiple_of(numel, max_block)\n+                    and not sizevars.statically_known_power_of_2(numel)\n+                    for numel in slice_numels\n+                ):\n+                    return None\n+\n+                def identity(expr: sympy.Expr) -> sympy.Expr:\n+                    return expr\n+\n+                # Compute the ND block shape from the linear block size.\n+                # Use CielDiv to round leading dimensions up to 1.\n+                # Non-leading dimensions are clamped to the size of the iteration range,\n+                # while the leading dimension can exceed this to accomodate a larger\n+                # block size.\n+                linear_block_size = self._get_block_size(range_tree)\n+                block_shape: List[sympy.Expr] = [\n+                    CeilDiv(linear_block_size, slice_numels[0])\n+                ] + [\n+                    sympy.Min(CeilDiv(linear_block_size, numel), dim)\n+                    for numel, dim in zip(slice_numels[1:], dims[1:])\n+                ]\n+\n+                # Compute block offsets from {xyzr}offset and the matched expressions.\n+                block_offsets: List[sympy.Expr] = [\n+                    sympy_subs(expr, {index_var: self._get_block_offset(range_tree)})\n+                    for expr in block_index_exprs\n+                ]\n+\n+                return BlockParameters(\n+                    shape=dims,\n+                    block_shape=block_shape,\n+                    strides=strides,\n+                    offsets=block_offsets,\n+                )\n+\n+            def match_block_pointer_subexpr(\n+                expr: sympy.Expr, range_tree: IterationRangesEntry\n+            ) -> Optional[BlockParameters]:\n+                \"\"\"\n+                Match a block indexing subexpression involving a single range tree.\n+                \"\"\"\n+                for match_func in (\n+                    match_strided_block,\n+                    match_mod_div_block,\n+                ):\n+                    match = match_func(expr, range_tree)\n+                    if match is not None:\n+                        return match\n+\n+                return None\n+\n+            def match_block_pointer() -> Optional[BlockPtrOptions]:\n+                index_relative_to_xyr_index = sympy_subs(\n+                    index, {v: t.expr for v, t in self.range_tree_nodes.items()}\n+                )\n+                range_trees = self.active_range_trees(reorder=True)\n+\n+                # Match each range tree separately.\n+                range_symbols = {tree.symbol() for tree in range_trees}\n+                index_terms = sympy.Add.make_args(index_relative_to_xyr_index)\n+                block_params = BlockParameters()\n+                for tree in range_trees:\n+                    # Partition the index into subexpressions pertaining to each range tree.\n+                    # For example xindex * 5 + rindex * 3 is partitioned to\n+                    # (xindex * 5, rindex * 3).\n+                    symbol = tree.symbol()\n+                    subexpr = sympy.Integer(0) + sum(\n+                        expr for expr in index_terms if symbol in expr.free_symbols\n+                    )\n+\n+                    # Reject mixed terms, e.g. xindex * rindex.\n+                    # NB: the zero expression is allowed, for broadcasting.\n+                    if len(range_symbols.intersection(subexpr.free_symbols)) > 1:\n+                        return None\n+\n+                    # Match the subexpression for this range tree.\n+                    params = match_block_pointer_subexpr(subexpr, tree)\n+                    if params is None:\n+                        return None\n+                    block_params += params\n+\n+                # Collect leftover terms as a constant offset.\n+                offset = sum(\n+                    expr\n+                    for expr in index_terms\n+                    if not range_symbols.intersection(expr.free_symbols)\n+                )\n+\n+                # Form the block pointer.\n+                self.filter_masks(mask_vars)\n                 return BlockPtrOptions.create(\n-                    [m[s] for s in strides],\n-                    m[offset],\n-                    range_trees,\n-                    mask_vars,  # type: ignore[arg-type]\n+                    params=block_params,\n+                    constant_offset=offset,\n+                    range_trees=range_trees,\n+                    mask_vars=mask_vars,\n                 )\n \n+            # Return a block pointer, if indexing matches the pattern.\n+            options = match_block_pointer()\n+            if options is not None:\n+                return options\n+\n         expand_str = None\n         index_str = self.index_to_str(index)\n         if isinstance(index, sympy.Integer):\n@@ -1274,7 +1599,8 @@ class TritonKernel(SIMDKernel):\n             f\"tl.broadcast_to({value}, {self.index_to_str(indexing.reshape_suffix)})\"\n         )\n         # drop any extra size=1 dimensions\n-        value = triton_reshape(value, indexing.reshape_suffix, indexing.block_shape)\n+        block_shape = [V.kernel.index_to_str(expr) for expr in indexing.block_shape]\n+        value = triton_reshape(value, indexing.reshape_suffix, block_shape)\n         # workaround https://github.com/openai/triton/issues/2814\n         value = f\"{value}.to({triton_store_type(V.graph.get_dtype(name))})\"\n         return f\"tl.store({block_ptr}, {value}{other})\"\n@@ -1381,9 +1707,8 @@ class TritonKernel(SIMDKernel):\n                 )\n                 line = f\"tl.load({block_ptr}{other}{ep})\"\n                 # add needed size=1 dimensions\n-                line = triton_reshape(\n-                    line, indexing.block_shape, indexing.reshape_suffix\n-                )\n+                block_shape = [str(dim) for dim in indexing.block_shape]\n+                line = triton_reshape(line, block_shape, indexing.reshape_suffix)\n             elif isinstance(original_index, sympy.Integer):\n                 line = f\"tl.load({var} + ({original_index}))\"\n                 append_broadcast = indexing.expand_str\n"
        },
        {
            "name": "runtime_utils.py",
            "path": "torch/_inductor/runtime/runtime_utils.py",
            "patches": [
                {
                    "old_start": 21,
                    "old_length": 6,
                    "new_start": 21,
                    "new_length": 11,
                    "hunk": "@@ -21,6 +21,11 @@ def ceildiv(numer: int, denom: int) -> int:\n     return -(numer // -denom)\n \n \n+def is_power_of_2(n: int) -> bool:\n+    \"\"\"Returns whether n = 2 ** m for some integer m.\"\"\"\n+    return n > 0 and n & n - 1 == 0\n+\n+\n def next_power_of_2(n: int) -> int:\n     \"\"\"Return the smallest power of 2 greater than or equal to n\"\"\"\n     n -= 1\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+def is_power_of_2(n: int) -> bool:\n+    \"\"\"Returns whether n = 2 ** m for some integer m.\"\"\"\n+    return n > 0 and n & n - 1 == 0\n+\n+\n",
            "whole_hunk": "@@ -21,6 +21,11 @@ def ceildiv(numer: int, denom: int) -> int:\n     return -(numer // -denom)\n \n \n+def is_power_of_2(n: int) -> bool:\n+    \"\"\"Returns whether n = 2 ** m for some integer m.\"\"\"\n+    return n > 0 and n & n - 1 == 0\n+\n+\n def next_power_of_2(n: int) -> int:\n     \"\"\"Return the smallest power of 2 greater than or equal to n\"\"\"\n     n -= 1\n"
        },
        {
            "name": "sizevars.py",
            "path": "torch/_inductor/sizevars.py",
            "patches": [
                {
                    "old_start": 23,
                    "old_length": 6,
                    "new_start": 23,
                    "new_length": 7,
                    "hunk": "@@ -23,6 +23,7 @@ from torch.utils._sympy.functions import FloorDiv, ModularIndexing\n from torch.utils._sympy.symbol import symbol_is_type, SymT\n from torch.utils._sympy.value_ranges import bound_sympy\n \n+from .runtime.runtime_utils import is_power_of_2\n from .utils import (\n     sympy_index_symbol,\n     sympy_index_symbol_with_prefix,\n"
                },
                {
                    "old_start": 354,
                    "old_length": 6,
                    "new_start": 355,
                    "new_length": 13,
                    "hunk": "@@ -354,6 +355,13 @@ class SizeVarAllocator:\n         expr = sympy.Eq(numerator % denominator, 0)\n         return self.is_expr_static_and_true(expr)  # type: ignore[arg-type]\n \n+    # See Note - [On Statically Known]\n+    def statically_known_power_of_2(self, expr: Expr) -> bool:\n+        \"\"\"\n+        Returns a bool indicating if x is known to be a power of 2.\n+        \"\"\"\n+        return isinstance(expr, sympy.Integer) and is_power_of_2(int(expr))\n+\n     # The guard functions require you to ALREADY KNOW that a particular\n     # condition holds.  If you don't know (you want to guard on an expression\n     # being a particular value, and then get access to that value), use\n"
                },
                {
                    "old_start": 376,
                    "old_length": 7,
                    "new_start": 384,
                    "new_length": 6,
                    "hunk": "@@ -376,7 +384,6 @@ class SizeVarAllocator:\n     def guarded_order(self, seq):\n         \"\"\"\n         Return the order of a sequence as a permutation of range(len(seq)) and guard on that order not changing.\n-        Used for generating block_ptrs.\n         \"\"\"\n         seq = [*map(self.remove_precomputed_replacements, seq)]\n         seq = [(self.size_hint(var), orig_idx, var) for orig_idx, var in enumerate(seq)]\n"
                }
            ],
            "whole_deleted": "-        Used for generating block_ptrs.\n",
            "whole_added": "+from .runtime.runtime_utils import is_power_of_2\n+    # See Note - [On Statically Known]\n+    def statically_known_power_of_2(self, expr: Expr) -> bool:\n+        \"\"\"\n+        Returns a bool indicating if x is known to be a power of 2.\n+        \"\"\"\n+        return isinstance(expr, sympy.Integer) and is_power_of_2(int(expr))\n+\n",
            "whole_hunk": "@@ -23,6 +23,7 @@ from torch.utils._sympy.functions import FloorDiv, ModularIndexing\n from torch.utils._sympy.symbol import symbol_is_type, SymT\n from torch.utils._sympy.value_ranges import bound_sympy\n \n+from .runtime.runtime_utils import is_power_of_2\n from .utils import (\n     sympy_index_symbol,\n     sympy_index_symbol_with_prefix,\n@@ -354,6 +355,13 @@ class SizeVarAllocator:\n         expr = sympy.Eq(numerator % denominator, 0)\n         return self.is_expr_static_and_true(expr)  # type: ignore[arg-type]\n \n+    # See Note - [On Statically Known]\n+    def statically_known_power_of_2(self, expr: Expr) -> bool:\n+        \"\"\"\n+        Returns a bool indicating if x is known to be a power of 2.\n+        \"\"\"\n+        return isinstance(expr, sympy.Integer) and is_power_of_2(int(expr))\n+\n     # The guard functions require you to ALREADY KNOW that a particular\n     # condition holds.  If you don't know (you want to guard on an expression\n     # being a particular value, and then get access to that value), use\n@@ -376,7 +384,6 @@ class SizeVarAllocator:\n     def guarded_order(self, seq):\n         \"\"\"\n         Return the order of a sequence as a permutation of range(len(seq)) and guard on that order not changing.\n-        Used for generating block_ptrs.\n         \"\"\"\n         seq = [*map(self.remove_precomputed_replacements, seq)]\n         seq = [(self.size_hint(var), orig_idx, var) for orig_idx, var in enumerate(seq)]\n"
        },
        {
            "name": "inductor_utils.py",
            "path": "torch/testing/_internal/inductor_utils.py",
            "patches": [
                {
                    "old_start": 4,
                    "old_length": 7,
                    "new_start": 4,
                    "new_length": 9,
                    "hunk": "@@ -4,7 +4,9 @@ import torch\n import re\n import unittest\n import functools\n+import os\n from subprocess import CalledProcessError\n+import sys\n import torch._inductor.async_compile  # noqa: F401 required to warm up AsyncCompile pools\n from torch._inductor.codecache import CppCodeCache\n from torch.utils._triton import has_triton\n"
                },
                {
                    "old_start": 12,
                    "old_length": 7,
                    "new_start": 14,
                    "new_length": 11,
                    "hunk": "@@ -12,7 +14,11 @@ from torch.testing._internal.common_utils import (\n     LazyVal,\n     IS_FBCODE,\n )\n-from torch.testing._internal.common_utils import TestCase\n+from torch.testing._internal.common_utils import (\n+    TestCase,\n+    IS_CI,\n+    IS_WINDOWS,\n+)\n \n def test_cpu():\n     try:\n"
                },
                {
                    "old_start": 79,
                    "old_length": 6,
                    "new_start": 85,
                    "new_length": 18,
                    "hunk": "@@ -79,6 +85,18 @@ def skipDeviceIf(cond, msg, *, device):\n \n     return decorate_fn\n \n+def skip_windows_ci(name: str, file: str) -> None:\n+    if IS_WINDOWS and IS_CI:\n+        module = os.path.basename(file).strip(\".py\")\n+        sys.stderr.write(\n+            f\"Windows CI does not have necessary dependencies for {module} tests yet\\n\"\n+        )\n+        if name == \"__main__\":\n+            sys.exit(0)\n+        raise unittest.SkipTest(\"requires sympy/functorch/filelock\")\n+\n+requires_gpu = functools.partial(unittest.skipIf, not HAS_GPU, \"requires gpu\")\n+\n skipCUDAIf = functools.partial(skipDeviceIf, device=\"cuda\")\n skipXPUIf = functools.partial(skipDeviceIf, device=\"xpu\")\n skipCPUIf = functools.partial(skipDeviceIf, device=\"cpu\")\n"
                }
            ],
            "whole_deleted": "-from torch.testing._internal.common_utils import TestCase\n",
            "whole_added": "+import os\n+import sys\n+from torch.testing._internal.common_utils import (\n+    TestCase,\n+    IS_CI,\n+    IS_WINDOWS,\n+)\n+def skip_windows_ci(name: str, file: str) -> None:\n+    if IS_WINDOWS and IS_CI:\n+        module = os.path.basename(file).strip(\".py\")\n+        sys.stderr.write(\n+            f\"Windows CI does not have necessary dependencies for {module} tests yet\\n\"\n+        )\n+        if name == \"__main__\":\n+            sys.exit(0)\n+        raise unittest.SkipTest(\"requires sympy/functorch/filelock\")\n+\n+requires_gpu = functools.partial(unittest.skipIf, not HAS_GPU, \"requires gpu\")\n+\n",
            "whole_hunk": "@@ -4,7 +4,9 @@ import torch\n import re\n import unittest\n import functools\n+import os\n from subprocess import CalledProcessError\n+import sys\n import torch._inductor.async_compile  # noqa: F401 required to warm up AsyncCompile pools\n from torch._inductor.codecache import CppCodeCache\n from torch.utils._triton import has_triton\n@@ -12,7 +14,11 @@ from torch.testing._internal.common_utils import (\n     LazyVal,\n     IS_FBCODE,\n )\n-from torch.testing._internal.common_utils import TestCase\n+from torch.testing._internal.common_utils import (\n+    TestCase,\n+    IS_CI,\n+    IS_WINDOWS,\n+)\n \n def test_cpu():\n     try:\n@@ -79,6 +85,18 @@ def skipDeviceIf(cond, msg, *, device):\n \n     return decorate_fn\n \n+def skip_windows_ci(name: str, file: str) -> None:\n+    if IS_WINDOWS and IS_CI:\n+        module = os.path.basename(file).strip(\".py\")\n+        sys.stderr.write(\n+            f\"Windows CI does not have necessary dependencies for {module} tests yet\\n\"\n+        )\n+        if name == \"__main__\":\n+            sys.exit(0)\n+        raise unittest.SkipTest(\"requires sympy/functorch/filelock\")\n+\n+requires_gpu = functools.partial(unittest.skipIf, not HAS_GPU, \"requires gpu\")\n+\n skipCUDAIf = functools.partial(skipDeviceIf, device=\"cuda\")\n skipXPUIf = functools.partial(skipDeviceIf, device=\"xpu\")\n skipCPUIf = functools.partial(skipDeviceIf, device=\"cpu\")\n"
        },
        {
            "name": "functions.py",
            "path": "torch/utils/_sympy/functions.py",
            "patches": [
                {
                    "old_start": 143,
                    "old_length": 15,
                    "new_start": 143,
                    "new_length": 15,
                    "hunk": "@@ -143,15 +143,15 @@ class FloorDiv(sympy.Function):\n         if isinstance(base, FloorDiv):\n             return FloorDiv(base.args[0], base.args[1] * divisor)\n \n-        # gcd in sympy is over polynomials, so you'll end up with rationals if\n-        # you do this.  Don't.\n-        \"\"\"\n-        if isinstance(base, sympy.Add):\n-            for a in base.args:\n-                gcd = sympy.gcd(a, divisor)\n-                if gcd == divisor:\n-                    return FloorDiv(base - a, divisor) + a / gcd\n-        \"\"\"\n+        # Expands (x + y) // b into x // b + y // b.\n+        # This only works if floor is an identity, i.e. x / b is an integer.\n+        for term in sympy.Add.make_args(base):\n+            quotient = term / divisor\n+            if quotient.is_integer and isinstance(divisor, sympy.Integer):\n+                # NB: this is correct even if the divisor is not an integer, but it\n+                # creates rational expressions that cause problems with dynamic\n+                # shapes.\n+                return FloorDiv(base - term, divisor) + quotient\n \n         try:\n             gcd = sympy.gcd(base, divisor)\n"
                }
            ],
            "whole_deleted": "-        # gcd in sympy is over polynomials, so you'll end up with rationals if\n-        # you do this.  Don't.\n-        \"\"\"\n-        if isinstance(base, sympy.Add):\n-            for a in base.args:\n-                gcd = sympy.gcd(a, divisor)\n-                if gcd == divisor:\n-                    return FloorDiv(base - a, divisor) + a / gcd\n-        \"\"\"\n",
            "whole_added": "+        # Expands (x + y) // b into x // b + y // b.\n+        # This only works if floor is an identity, i.e. x / b is an integer.\n+        for term in sympy.Add.make_args(base):\n+            quotient = term / divisor\n+            if quotient.is_integer and isinstance(divisor, sympy.Integer):\n+                # NB: this is correct even if the divisor is not an integer, but it\n+                # creates rational expressions that cause problems with dynamic\n+                # shapes.\n+                return FloorDiv(base - term, divisor) + quotient\n",
            "whole_hunk": "@@ -143,15 +143,15 @@ class FloorDiv(sympy.Function):\n         if isinstance(base, FloorDiv):\n             return FloorDiv(base.args[0], base.args[1] * divisor)\n \n-        # gcd in sympy is over polynomials, so you'll end up with rationals if\n-        # you do this.  Don't.\n-        \"\"\"\n-        if isinstance(base, sympy.Add):\n-            for a in base.args:\n-                gcd = sympy.gcd(a, divisor)\n-                if gcd == divisor:\n-                    return FloorDiv(base - a, divisor) + a / gcd\n-        \"\"\"\n+        # Expands (x + y) // b into x // b + y // b.\n+        # This only works if floor is an identity, i.e. x / b is an integer.\n+        for term in sympy.Add.make_args(base):\n+            quotient = term / divisor\n+            if quotient.is_integer and isinstance(divisor, sympy.Integer):\n+                # NB: this is correct even if the divisor is not an integer, but it\n+                # creates rational expressions that cause problems with dynamic\n+                # shapes.\n+                return FloorDiv(base - term, divisor) + quotient\n \n         try:\n             gcd = sympy.gcd(base, divisor)\n"
        },
        {
            "name": "symbol.py",
            "path": "torch/utils/_sympy/symbol.py",
            "patches": [
                {
                    "old_start": 82,
                    "old_length": 10,
                    "new_start": 82,
                    "new_length": 11,
                    "hunk": "@@ -82,10 +82,11 @@ def make_symbol(prefix: SymT, idx: int, **kwargs) -> sympy.Symbol:\n # that it contains Basic, rather than Symbol\n def symbol_is_type(sym: sympy.Basic, prefix: Union[SymT, Sequence[SymT]]) -> bool:\n     assert isinstance(sym, sympy.Symbol)\n+    name_str = sym.name.lower()  # Match capitalized names like XBLOCK, RBLOCK\n     if isinstance(prefix, SymT):\n-        return sym.name.startswith(prefix_str[prefix])\n+        return name_str.startswith(prefix_str[prefix])\n     else:\n-        return sym.name.startswith(tuple(prefix_str[p] for p in prefix))\n+        return name_str.startswith(tuple(prefix_str[p] for p in prefix))\n \n \n def free_symbol_is_type(e: sympy.Expr, prefix: SymT) -> bool:"
                }
            ],
            "whole_deleted": "-        return sym.name.startswith(prefix_str[prefix])\n-        return sym.name.startswith(tuple(prefix_str[p] for p in prefix))\n",
            "whole_added": "+    name_str = sym.name.lower()  # Match capitalized names like XBLOCK, RBLOCK\n+        return name_str.startswith(prefix_str[prefix])\n+        return name_str.startswith(tuple(prefix_str[p] for p in prefix))\n",
            "whole_hunk": "@@ -82,10 +82,11 @@ def make_symbol(prefix: SymT, idx: int, **kwargs) -> sympy.Symbol:\n # that it contains Basic, rather than Symbol\n def symbol_is_type(sym: sympy.Basic, prefix: Union[SymT, Sequence[SymT]]) -> bool:\n     assert isinstance(sym, sympy.Symbol)\n+    name_str = sym.name.lower()  # Match capitalized names like XBLOCK, RBLOCK\n     if isinstance(prefix, SymT):\n-        return sym.name.startswith(prefix_str[prefix])\n+        return name_str.startswith(prefix_str[prefix])\n     else:\n-        return sym.name.startswith(tuple(prefix_str[p] for p in prefix))\n+        return name_str.startswith(tuple(prefix_str[p] for p in prefix))\n \n \n def free_symbol_is_type(e: sympy.Expr, prefix: SymT) -> bool:"
        }
    ]
},
{
    "Id": 381,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/1becd2c314f45bded8d3fbec91d785e7190b4afe",
    "date": "2023-12-12T22:20:20+00:00",
    "message": "Align checks in `_use_cudnn_ctc_loss` with those in `_cudnn_ctc_loss` (#115617)\n\nThis PR is intended to fix the following problem:\n\nWhen using `CTCLoss`, there is a cudnn path gated by a call to [`_use_cudnn_ctc_loss`](\nhttps://github.com/pytorch/pytorch/blob/e91846137795e2d976b9e0ba2e1d886f34fcfb7a/aten/src/ATen/native/cudnn/LossCTC.cpp#L73-L101) which checks some conditions\n\nhttps://github.com/pytorch/pytorch/blob/e91846137795e2d976b9e0ba2e1d886f34fcfb7a/aten/src/ATen/native/LossCTC.cpp#L486-L496\n\nHowever, there are more checks in `_cudnn_ctc_loss`\nhttps://github.com/pytorch/pytorch/blob/e91846137795e2d976b9e0ba2e1d886f34fcfb7a/aten/src/ATen/native/cudnn/LossCTC.cpp#L122-L130\n\nsome of which are not present in `_use_cudnn_ctc_loss` (e.g. the check that `targets` is on CPU which will cause a RuntimeError after dispatching to `_cudnn_ctc_loss`). Instead, these checks should be in `_use_cudnn_ctc_loss` so that the normal `_ctc_loss` path will be used if the checks are not met)\n\ne.g. Before this PR\n\n```python\n>>> import torch\n>>> ctcloss = torch.nn.CTCLoss()\n>>> log_probs = torch.randn((50, 3, 15), device='cuda').log_softmax(2)\n>>> target = torch.randint(1, 15, (30 + 25 + 20,), dtype = torch.int)\n>>> input_lengths = torch.tensor((50, 50, 50), device='cuda')\n>>> target_lengths = torch.tensor((30, 25, 20), device='cuda')\n>>> ctcloss(log_probs, target, input_lengths, target_lengths)\ntensor(4.1172, device='cuda:0')\n>>> target = target.to('cuda')\n>>> ctcloss(log_probs, target, input_lengths, target_lengths)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/data/users/mg1998/pytorch/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/data/users/mg1998/pytorch/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/data/users/mg1998/pytorch/torch/nn/modules/loss.py\", line 1779, in forward\n    return F.ctc_loss(log_probs, targets, input_lengths, target_lengths, self.blank, self.reduction,\n  File \"/data/users/mg1998/pytorch/torch/nn/functional.py\", line 2660, in ctc_loss\n    return torch.ctc_loss(\nRuntimeError: Expected tensor to have CPU Backend, but got tensor with CUDA Backend (while checking arguments for cudnn_ctc_loss)\n```\n\nAfter this PR the above snippet runs without error.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115617\nApproved by: https://github.com/janeyx99",
    "label": "YES",
    "changes": [
        {
            "name": "LossCTC.cpp",
            "path": "aten/src/ATen/native/cudnn/LossCTC.cpp",
            "patches": [
                {
                    "old_start": 81,
                    "old_length": 7,
                    "new_start": 81,
                    "new_length": 10,
                    "hunk": "@@ -81,7 +81,10 @@ bool _use_cudnn_ctc_loss(\n   bool use_cudnn = ctx.userEnabledCuDNN() && (BLANK == 0) &&\n       (targets.dim() == 1) && (log_probs.scalar_type() == at::kFloat) &&\n       (targets.scalar_type() == at::kInt) &&\n-      (log_probs.device().type() == at::kCUDA);\n+      (log_probs.device().type() == at::kCUDA) &&\n+      (targets.device().type() == at::kCPU) &&\n+      (targets.is_contiguous()) &&\n+      (log_probs.dim() == 3);\n \n   if (use_cudnn) {\n     // we don't know that input_lengths and target_lengths have the same size"
                }
            ],
            "whole_deleted": "-      (log_probs.device().type() == at::kCUDA);\n",
            "whole_added": "+      (log_probs.device().type() == at::kCUDA) &&\n+      (targets.device().type() == at::kCPU) &&\n+      (targets.is_contiguous()) &&\n+      (log_probs.dim() == 3);\n",
            "whole_hunk": "@@ -81,7 +81,10 @@ bool _use_cudnn_ctc_loss(\n   bool use_cudnn = ctx.userEnabledCuDNN() && (BLANK == 0) &&\n       (targets.dim() == 1) && (log_probs.scalar_type() == at::kFloat) &&\n       (targets.scalar_type() == at::kInt) &&\n-      (log_probs.device().type() == at::kCUDA);\n+      (log_probs.device().type() == at::kCUDA) &&\n+      (targets.device().type() == at::kCPU) &&\n+      (targets.is_contiguous()) &&\n+      (log_probs.dim() == 3);\n \n   if (use_cudnn) {\n     // we don't know that input_lengths and target_lengths have the same size"
        }
    ]
},
{
    "Id": 149,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/20aa7cc6788ff10dee2d927057b10a81af638a32",
    "date": "2024-05-14T16:26:34+00:00",
    "message": "Revert \"[c10d] Add an option for NAN check on every collective (#125726)\"\n\nThis reverts commit 6db32710074f0944305b2d1e4571bb4ce571bf6a.\n\nReverted https://github.com/pytorch/pytorch/pull/125726 on behalf of https://github.com/huydhn due to Sorry for reverting your change, but the new test is failing on both multigpu and rocm distributed, i.e. https://hud.pytorch.org/pytorch/pytorch/commit/c712b0f8a3e72feda9a90e22e9f36bd102b7d25e ([comment](https://github.com/pytorch/pytorch/pull/125726#issuecomment-2110646075))",
    "label": "YES",
    "changes": [
        {
            "name": "BUILD.bazel",
            "path": "BUILD.bazel",
            "patches": [
                {
                    "old_start": 663,
                    "old_length": 7,
                    "new_start": 663,
                    "new_length": 6,
                    "hunk": "@@ -663,7 +663,6 @@ cu_library(\n     name = \"torch_cuda\",\n     srcs = [\n         \"torch/csrc/distributed/c10d/intra_node_comm.cu\",\n-        \"torch/csrc/distributed/c10d/Utils.cu\",\n         \"torch/csrc/distributed/c10d/quantization/quantization_gpu.cu\",\n     ],\n     copts = torch_cuda_half_options,\n"
                },
                {
                    "old_start": 831,
                    "old_length": 7,
                    "new_start": 830,
                    "new_length": 6,
                    "hunk": "@@ -831,7 +830,6 @@ cc_library(\n             \"torch/csrc/cuda/python_nccl.cpp\",\n             \"torch/csrc/cuda/nccl.cpp\",\n             \"torch/csrc/distributed/c10d/intra_node_comm.cu\",\n-            \"torch/csrc/distributed/c10d/Utils.cu\",\n             \"torch/csrc/distributed/c10d/quantization/quantization_gpu.cu\",\n         ],\n     )) + torch_sources,\n"
                }
            ],
            "whole_deleted": "-        \"torch/csrc/distributed/c10d/Utils.cu\",\n-            \"torch/csrc/distributed/c10d/Utils.cu\",\n",
            "whole_added": "",
            "whole_hunk": "@@ -663,7 +663,6 @@ cu_library(\n     name = \"torch_cuda\",\n     srcs = [\n         \"torch/csrc/distributed/c10d/intra_node_comm.cu\",\n-        \"torch/csrc/distributed/c10d/Utils.cu\",\n         \"torch/csrc/distributed/c10d/quantization/quantization_gpu.cu\",\n     ],\n     copts = torch_cuda_half_options,\n@@ -831,7 +830,6 @@ cc_library(\n             \"torch/csrc/cuda/python_nccl.cpp\",\n             \"torch/csrc/cuda/nccl.cpp\",\n             \"torch/csrc/distributed/c10d/intra_node_comm.cu\",\n-            \"torch/csrc/distributed/c10d/Utils.cu\",\n             \"torch/csrc/distributed/c10d/quantization/quantization_gpu.cu\",\n         ],\n     )) + torch_sources,\n"
        },
        {
            "name": "build_variables.bzl",
            "path": "build_variables.bzl",
            "patches": [
                {
                    "old_start": 679,
                    "old_length": 7,
                    "new_start": 679,
                    "new_length": 6,
                    "hunk": "@@ -679,7 +679,6 @@ libtorch_cuda_distributed_extra_sources = [\n     \"torch/csrc/distributed/c10d/UCCUtils.cpp\",\n     \"torch/csrc/distributed/c10d/intra_node_comm.cpp\",\n     \"torch/csrc/distributed/c10d/intra_node_comm.cu\",\n-    \"torch/csrc/distributed/c10d/Utils.cu\",\n     \"torch/csrc/distributed/rpc/tensorpipe_cuda.cpp\",\n     \"torch/csrc/distributed/c10d/quantization/quantization_gpu.cu\",\n ]\n"
                }
            ],
            "whole_deleted": "-    \"torch/csrc/distributed/c10d/Utils.cu\",\n",
            "whole_added": "",
            "whole_hunk": "@@ -679,7 +679,6 @@ libtorch_cuda_distributed_extra_sources = [\n     \"torch/csrc/distributed/c10d/UCCUtils.cpp\",\n     \"torch/csrc/distributed/c10d/intra_node_comm.cpp\",\n     \"torch/csrc/distributed/c10d/intra_node_comm.cu\",\n-    \"torch/csrc/distributed/c10d/Utils.cu\",\n     \"torch/csrc/distributed/rpc/tensorpipe_cuda.cpp\",\n     \"torch/csrc/distributed/c10d/quantization/quantization_gpu.cu\",\n ]\n"
        },
        {
            "name": "test_c10d_nccl.py",
            "path": "test/distributed/test_c10d_nccl.py",
            "patches": [
                {
                    "old_start": 325,
                    "old_length": 26,
                    "new_start": 325,
                    "new_length": 6,
                    "hunk": "@@ -325,26 +325,6 @@ class ProcessGroupNCCLGroupTest(MultiProcessTestCase):\n \n         del pg\n \n-    @requires_nccl()\n-    @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"NCCL test requires 2+ GPUs\")\n-    @parametrize(\"type\", [torch.float16, torch.float32, torch.float64])\n-    @parametrize(\"size\", [(10, 10), (1000, 1000)])\n-    def test_nan_assert(self, type, size):\n-        os.environ[\"TORCH_NCCL_NAN_CHECK\"] = \"1\"\n-        store = c10d.FileStore(self.file_name, self.world_size)\n-        pg = self._create_process_group_nccl(store, self.opts())\n-        device = self.rank_to_GPU[self.rank][0]\n-        nan_tensor = torch.full(size, self.rank, dtype=type, device=device)\n-        # randomly pick an nan element\n-        i = random.randint(0, nan_tensor.size(0) - 1)\n-        j = random.randint(0, nan_tensor.size(1) - 1)\n-        nan_tensor[i, j] = float(\"nan\")\n-        with self.assertRaises(RuntimeError):\n-            pg.allreduce(nan_tensor)\n-        dist.destroy_process_group()\n-        # reset env\n-        os.environ[\"TORCH_NCCL_NAN_CHECK\"] = \"0\"\n-\n     @requires_nccl()\n     @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"NCCL test requires 2+ GPUs\")\n     def test_destruct_before_terminate_pg(self):\n"
                }
            ],
            "whole_deleted": "-    @requires_nccl()\n-    @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"NCCL test requires 2+ GPUs\")\n-    @parametrize(\"type\", [torch.float16, torch.float32, torch.float64])\n-    @parametrize(\"size\", [(10, 10), (1000, 1000)])\n-    def test_nan_assert(self, type, size):\n-        os.environ[\"TORCH_NCCL_NAN_CHECK\"] = \"1\"\n-        store = c10d.FileStore(self.file_name, self.world_size)\n-        pg = self._create_process_group_nccl(store, self.opts())\n-        device = self.rank_to_GPU[self.rank][0]\n-        nan_tensor = torch.full(size, self.rank, dtype=type, device=device)\n-        # randomly pick an nan element\n-        i = random.randint(0, nan_tensor.size(0) - 1)\n-        j = random.randint(0, nan_tensor.size(1) - 1)\n-        nan_tensor[i, j] = float(\"nan\")\n-        with self.assertRaises(RuntimeError):\n-            pg.allreduce(nan_tensor)\n-        dist.destroy_process_group()\n-        # reset env\n-        os.environ[\"TORCH_NCCL_NAN_CHECK\"] = \"0\"\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -325,26 +325,6 @@ class ProcessGroupNCCLGroupTest(MultiProcessTestCase):\n \n         del pg\n \n-    @requires_nccl()\n-    @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"NCCL test requires 2+ GPUs\")\n-    @parametrize(\"type\", [torch.float16, torch.float32, torch.float64])\n-    @parametrize(\"size\", [(10, 10), (1000, 1000)])\n-    def test_nan_assert(self, type, size):\n-        os.environ[\"TORCH_NCCL_NAN_CHECK\"] = \"1\"\n-        store = c10d.FileStore(self.file_name, self.world_size)\n-        pg = self._create_process_group_nccl(store, self.opts())\n-        device = self.rank_to_GPU[self.rank][0]\n-        nan_tensor = torch.full(size, self.rank, dtype=type, device=device)\n-        # randomly pick an nan element\n-        i = random.randint(0, nan_tensor.size(0) - 1)\n-        j = random.randint(0, nan_tensor.size(1) - 1)\n-        nan_tensor[i, j] = float(\"nan\")\n-        with self.assertRaises(RuntimeError):\n-            pg.allreduce(nan_tensor)\n-        dist.destroy_process_group()\n-        # reset env\n-        os.environ[\"TORCH_NCCL_NAN_CHECK\"] = \"0\"\n-\n     @requires_nccl()\n     @skip_but_pass_in_sandcastle_if(not TEST_MULTIGPU, \"NCCL test requires 2+ GPUs\")\n     def test_destruct_before_terminate_pg(self):\n"
        },
        {
            "name": "ProcessGroupNCCL.cpp",
            "path": "torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp",
            "patches": [
                {
                    "old_start": 748,
                    "old_length": 7,
                    "new_start": 748,
                    "new_length": 6,
                    "hunk": "@@ -748,7 +748,6 @@ ProcessGroupNCCL::ProcessGroupNCCL(\n   // both timeout and other errors.\n   dumpOnException_ = getCvarBool(TORCH_NCCL_DUMP_ON_TIMEOUT, false) ||\n       (dist_debug_level_ >= DebugLevel::Detail);\n-  enableNanCheck_ = getCvarBool(TORCH_NCCL_NAN_CHECK, false);\n   heartbeat_ = 1ULL;\n   monitorThreadEnabled_.store(getCvarBool(TORCH_NCCL_ENABLE_MONITORING, true));\n   heartbeatTimeoutInSec_ =\n"
                },
                {
                    "old_start": 837,
                    "old_length": 7,
                    "new_start": 836,
                    "new_length": 6,
                    "hunk": "@@ -837,7 +836,6 @@ ProcessGroupNCCL::ProcessGroupNCCL(\n             << \", TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: \" << heartbeatTimeoutInSec_\n             << \", TORCH_NCCL_TRACE_BUFFER_SIZE: \" << ncclTraceBufferSize_\n             << \", TORCH_NCCL_COORD_CHECK_MILSEC: \" << coordCheckIntervalMilSec_\n-            << \", TORCH_NCCL_NAN_CHECK: \" << enableNanCheck_\n             << \", PG Name: \" << options_->group_name;\n \n   if (options_->global_ranks_in_group.empty()) {\n"
                },
                {
                    "old_start": 2426,
                    "old_length": 9,
                    "new_start": 2424,
                    "new_length": 6,
                    "hunk": "@@ -2426,9 +2424,6 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::collective(\n     OpType opType,\n     const char* profilingTitle,\n     bool avoidRecordStreams) {\n-  if (enableNanCheck_) {\n-    checkForNan(input);\n-  }\n   // Environment setting by the user may add onto collective call's option\n   avoidRecordStreams |= avoidRecordStreams_;\n   c10::cuda::CaptureStatus capture_status =\n"
                },
                {
                    "old_start": 2784,
                    "old_length": 9,
                    "new_start": 2779,
                    "new_length": 6,
                    "hunk": "@@ -2784,9 +2779,6 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::pointToPoint(\n     PreProcess pre,\n     PostProcess post,\n     const char* profilingTitle) {\n-  if (enableNanCheck_) {\n-    checkForNan(tensor);\n-  }\n   // avoidRecordStreams_ note:\n   // send, recv, and irecv should be ok with avoidRecordStreams,\n   // However, for isend, I don't think the API requires the user\n"
                }
            ],
            "whole_deleted": "-  enableNanCheck_ = getCvarBool(TORCH_NCCL_NAN_CHECK, false);\n-            << \", TORCH_NCCL_NAN_CHECK: \" << enableNanCheck_\n-  if (enableNanCheck_) {\n-    checkForNan(input);\n-  }\n-  if (enableNanCheck_) {\n-    checkForNan(tensor);\n-  }\n",
            "whole_added": "",
            "whole_hunk": "@@ -748,7 +748,6 @@ ProcessGroupNCCL::ProcessGroupNCCL(\n   // both timeout and other errors.\n   dumpOnException_ = getCvarBool(TORCH_NCCL_DUMP_ON_TIMEOUT, false) ||\n       (dist_debug_level_ >= DebugLevel::Detail);\n-  enableNanCheck_ = getCvarBool(TORCH_NCCL_NAN_CHECK, false);\n   heartbeat_ = 1ULL;\n   monitorThreadEnabled_.store(getCvarBool(TORCH_NCCL_ENABLE_MONITORING, true));\n   heartbeatTimeoutInSec_ =\n@@ -837,7 +836,6 @@ ProcessGroupNCCL::ProcessGroupNCCL(\n             << \", TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: \" << heartbeatTimeoutInSec_\n             << \", TORCH_NCCL_TRACE_BUFFER_SIZE: \" << ncclTraceBufferSize_\n             << \", TORCH_NCCL_COORD_CHECK_MILSEC: \" << coordCheckIntervalMilSec_\n-            << \", TORCH_NCCL_NAN_CHECK: \" << enableNanCheck_\n             << \", PG Name: \" << options_->group_name;\n \n   if (options_->global_ranks_in_group.empty()) {\n@@ -2426,9 +2424,6 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::collective(\n     OpType opType,\n     const char* profilingTitle,\n     bool avoidRecordStreams) {\n-  if (enableNanCheck_) {\n-    checkForNan(input);\n-  }\n   // Environment setting by the user may add onto collective call's option\n   avoidRecordStreams |= avoidRecordStreams_;\n   c10::cuda::CaptureStatus capture_status =\n@@ -2784,9 +2779,6 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::pointToPoint(\n     PreProcess pre,\n     PostProcess post,\n     const char* profilingTitle) {\n-  if (enableNanCheck_) {\n-    checkForNan(tensor);\n-  }\n   // avoidRecordStreams_ note:\n   // send, recv, and irecv should be ok with avoidRecordStreams,\n   // However, for isend, I don't think the API requires the user\n"
        },
        {
            "name": "ProcessGroupNCCL.hpp",
            "path": "torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp",
            "patches": [
                {
                    "old_start": 100,
                    "old_length": 8,
                    "new_start": 100,
                    "new_length": 6,
                    "hunk": "@@ -100,8 +100,6 @@ static std::vector<std::string> TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC = {\n static std::vector<std::string> TORCH_NCCL_COORD_CHECK_MILSEC = {\n     \"TORCH_NCCL_COORD_CHECK_MILSEC\"};\n \n-static std::vector<std::string> TORCH_NCCL_NAN_CHECK = {\"TORCH_NCCL_NAN_CHECK\"};\n-\n constexpr const char* NCCL_BACKEND_NAME = \"nccl\";\n \n constexpr const char* EXCEPTION_DUMP = \"exception_dump\";\n"
                },
                {
                    "old_start": 1026,
                    "old_length": 9,
                    "new_start": 1024,
                    "new_length": 6,
                    "hunk": "@@ -1026,9 +1024,6 @@ class TORCH_API ProcessGroupNCCL : public Backend {\n   // timeout and nccl errors.\n   bool dumpOnException_;\n \n-  // Whether or not to enable nan check for input tensors to collectives.\n-  bool enableNanCheck_;\n-\n   // Whether or not to create start CUDAEvent and enable timing for start\n   // and end events. Note that enableTiming_ is always true if desyncDebug_\n   // is set to true.\n"
                }
            ],
            "whole_deleted": "-static std::vector<std::string> TORCH_NCCL_NAN_CHECK = {\"TORCH_NCCL_NAN_CHECK\"};\n-\n-  // Whether or not to enable nan check for input tensors to collectives.\n-  bool enableNanCheck_;\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -100,8 +100,6 @@ static std::vector<std::string> TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC = {\n static std::vector<std::string> TORCH_NCCL_COORD_CHECK_MILSEC = {\n     \"TORCH_NCCL_COORD_CHECK_MILSEC\"};\n \n-static std::vector<std::string> TORCH_NCCL_NAN_CHECK = {\"TORCH_NCCL_NAN_CHECK\"};\n-\n constexpr const char* NCCL_BACKEND_NAME = \"nccl\";\n \n constexpr const char* EXCEPTION_DUMP = \"exception_dump\";\n@@ -1026,9 +1024,6 @@ class TORCH_API ProcessGroupNCCL : public Backend {\n   // timeout and nccl errors.\n   bool dumpOnException_;\n \n-  // Whether or not to enable nan check for input tensors to collectives.\n-  bool enableNanCheck_;\n-\n   // Whether or not to create start CUDAEvent and enable timing for start\n   // and end events. Note that enableTiming_ is always true if desyncDebug_\n   // is set to true.\n"
        },
        {
            "name": "Utils.cu",
            "path": "torch/csrc/distributed/c10d/Utils.cu",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 45,
                    "new_start": 0,
                    "new_length": 0,
                    "hunk": "@@ -1,45 +0,0 @@\n-#include <ATen/Dispatch.h>\n-#include <ATen/cuda/CUDAContext.h>\n-#include <c10/cuda/CUDAGuard.h>\n-#include <torch/csrc/distributed/c10d/Utils.hpp>\n-#include <torch/torch.h>\n-#include <algorithm>\n-\n-namespace c10d {\n-\n-// CUDA kernel to check if data has NAN, device side assert\n-// is raised if NAN is found\n-template <typename T>\n-__global__ void checkForNaN(T* data, size_t size) {\n-  size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n-  size_t stride = blockDim.x * gridDim.x;\n-\n-  for (size_t i = tid; i < size; i += stride) {\n-    CUDA_KERNEL_ASSERT(!isnan(data[i]));\n-  }\n-}\n-\n-// CHECK if a Tensor contains NAN in any of its element\n-void checkForNan(const at::Tensor& tensor) {\n-  // skip check for non float types\n-  if (!torch::is_floating_point(tensor)) {\n-    return;\n-  }\n-  const size_t maxNumThreadsPerBlock = 512;\n-  const size_t maxNumBlocks = 24;\n-  const size_t numThreadsPerBlock =\n-      std::min<size_t>(maxNumThreadsPerBlock, tensor.numel());\n-\n-  const size_t numBlocks = std::min<size_t>(\n-      maxNumBlocks,\n-      (tensor.numel() + numThreadsPerBlock - 1) / numThreadsPerBlock);\n-\n-  AT_DISPATCH_FLOATING_TYPES_AND_HALF(tensor.scalar_type(), \"checkForNaN\", [&] {\n-    checkForNaN<scalar_t><<<numBlocks, numThreadsPerBlock>>>(\n-        tensor.data_ptr<scalar_t>(), tensor.numel());\n-    C10_CUDA_KERNEL_LAUNCH_CHECK();\n-  });\n-\n-}\n-\n-} // namespace c10d\n"
                }
            ],
            "whole_deleted": "-#include <ATen/Dispatch.h>\n-#include <ATen/cuda/CUDAContext.h>\n-#include <c10/cuda/CUDAGuard.h>\n-#include <torch/csrc/distributed/c10d/Utils.hpp>\n-#include <torch/torch.h>\n-#include <algorithm>\n-\n-namespace c10d {\n-\n-// CUDA kernel to check if data has NAN, device side assert\n-// is raised if NAN is found\n-template <typename T>\n-__global__ void checkForNaN(T* data, size_t size) {\n-  size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n-  size_t stride = blockDim.x * gridDim.x;\n-\n-  for (size_t i = tid; i < size; i += stride) {\n-    CUDA_KERNEL_ASSERT(!isnan(data[i]));\n-  }\n-}\n-\n-// CHECK if a Tensor contains NAN in any of its element\n-void checkForNan(const at::Tensor& tensor) {\n-  // skip check for non float types\n-  if (!torch::is_floating_point(tensor)) {\n-    return;\n-  }\n-  const size_t maxNumThreadsPerBlock = 512;\n-  const size_t maxNumBlocks = 24;\n-  const size_t numThreadsPerBlock =\n-      std::min<size_t>(maxNumThreadsPerBlock, tensor.numel());\n-\n-  const size_t numBlocks = std::min<size_t>(\n-      maxNumBlocks,\n-      (tensor.numel() + numThreadsPerBlock - 1) / numThreadsPerBlock);\n-\n-  AT_DISPATCH_FLOATING_TYPES_AND_HALF(tensor.scalar_type(), \"checkForNaN\", [&] {\n-    checkForNaN<scalar_t><<<numBlocks, numThreadsPerBlock>>>(\n-        tensor.data_ptr<scalar_t>(), tensor.numel());\n-    C10_CUDA_KERNEL_LAUNCH_CHECK();\n-  });\n-\n-}\n-\n-} // namespace c10d\n",
            "whole_added": "",
            "whole_hunk": "@@ -1,45 +0,0 @@\n-#include <ATen/Dispatch.h>\n-#include <ATen/cuda/CUDAContext.h>\n-#include <c10/cuda/CUDAGuard.h>\n-#include <torch/csrc/distributed/c10d/Utils.hpp>\n-#include <torch/torch.h>\n-#include <algorithm>\n-\n-namespace c10d {\n-\n-// CUDA kernel to check if data has NAN, device side assert\n-// is raised if NAN is found\n-template <typename T>\n-__global__ void checkForNaN(T* data, size_t size) {\n-  size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n-  size_t stride = blockDim.x * gridDim.x;\n-\n-  for (size_t i = tid; i < size; i += stride) {\n-    CUDA_KERNEL_ASSERT(!isnan(data[i]));\n-  }\n-}\n-\n-// CHECK if a Tensor contains NAN in any of its element\n-void checkForNan(const at::Tensor& tensor) {\n-  // skip check for non float types\n-  if (!torch::is_floating_point(tensor)) {\n-    return;\n-  }\n-  const size_t maxNumThreadsPerBlock = 512;\n-  const size_t maxNumBlocks = 24;\n-  const size_t numThreadsPerBlock =\n-      std::min<size_t>(maxNumThreadsPerBlock, tensor.numel());\n-\n-  const size_t numBlocks = std::min<size_t>(\n-      maxNumBlocks,\n-      (tensor.numel() + numThreadsPerBlock - 1) / numThreadsPerBlock);\n-\n-  AT_DISPATCH_FLOATING_TYPES_AND_HALF(tensor.scalar_type(), \"checkForNaN\", [&] {\n-    checkForNaN<scalar_t><<<numBlocks, numThreadsPerBlock>>>(\n-        tensor.data_ptr<scalar_t>(), tensor.numel());\n-    C10_CUDA_KERNEL_LAUNCH_CHECK();\n-  });\n-\n-}\n-\n-} // namespace c10d\n"
        },
        {
            "name": "Utils.hpp",
            "path": "torch/csrc/distributed/c10d/Utils.hpp",
            "patches": [
                {
                    "old_start": 612,
                    "old_length": 8,
                    "new_start": 612,
                    "new_length": 6,
                    "hunk": "@@ -612,8 +612,6 @@ using SizeType = uint64_t;\n // Since SOCKET_ERROR = -1 in MSVC, so also leverage SYSCHECK_ERR_RETURN_NEG1\n #define SYSCHECK_ERR_RETURN_NEG1(expr) SYSCHECK(expr, __output != -1)\n \n-void checkForNan(const at::Tensor& tensor);\n-\n namespace tcputil {\n \n // Send and receive"
                }
            ],
            "whole_deleted": "-void checkForNan(const at::Tensor& tensor);\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -612,8 +612,6 @@ using SizeType = uint64_t;\n // Since SOCKET_ERROR = -1 in MSVC, so also leverage SYSCHECK_ERR_RETURN_NEG1\n #define SYSCHECK_ERR_RETURN_NEG1(expr) SYSCHECK(expr, __output != -1)\n \n-void checkForNan(const at::Tensor& tensor);\n-\n namespace tcputil {\n \n // Send and receive"
        }
    ]
},
{
    "Id": 537,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/68b518c13e128997a0c7c9ab8ce9508cc4062e3a",
    "date": "2023-08-29T18:25:11+00:00",
    "message": "Add check for out of range pointer. (#107510)\n\n### Summary\n\nHi! We've been fuzzing pytorch with [sydr-fuzz](https://github.com/ispras/oss-sydr-fuzz) and found an error of accessing arbitary address while parsing flatbuffer format using `torch::load` function.\n\npytorch version: 18bcf62bbcf7ffd47e3bcf2596f72aa07a07d65f (the last commit at the moment of reporting the issue)\n\n### Details\nThe vulnerability appears while loading arbitrary user input using `torch::load` function. To detect the error the input must correspond to FlatbufferFileFormat, so the part of parsing flatbuffer in `import_ir_module` function must be executed.\n\nFirstly error can occur in `GetMutableRoot` in `module.h`, where we add pointer to input data buffer with the value, got from dereference of this pointer (which data fully depends on the user input and can be arbitrary). so the resulting `flatbuffer_module` address can be corrupted.\n\nMoreover, we can get the arbitrary address later at `flatbuffer_loader.cpp:305`, when we get `ival` pointer with `Get` method.\nThere in `IndirectHelper::Read` function we add pointer with the offset got from the dereference of this pointer, so the address can be corrupted again.\n\nThe corrupted `ival` pointer is dereferenced at `table.h` in flatbuffers project, where is used to get another address, which is later dereferenced again at `table.h` in flatbuffers project. The resulting corrupted address is written to `func` pointer at `flatbuffer_loader.cpp:274`, which is then used in `parseFunction`, where write access to the address occurs.\n\nTo fix the problem we can compute the end of memory area in `parse_and_initialize_mobile_module` function like this:\n```\nauto* end = static_cast<char*>(data) + size;\n```\nAnd then pass it to all the callees and insert corresponding checks.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/107510\nApproved by: https://github.com/albanD",
    "label": "YES",
    "changes": [
        {
            "name": "flatbuffer_loader.cpp",
            "path": "torch/csrc/jit/mobile/flatbuffer_loader.cpp",
            "patches": [
                {
                    "old_start": 85,
                    "old_length": 7,
                    "new_start": 85,
                    "new_length": 7,
                    "hunk": "@@ -85,7 +85,7 @@ class FlatbufferLoader final {\n   void registerIValueParser(\n       mobile::serialization::IValueUnion ivalue_type,\n       IValueParser parser);\n-  mobile::Module parseModule(mobile::serialization::Module* module);\n+  mobile::Module parseModule(mobile::serialization::Module* module, char* end);\n \n   void extractJitSourceAndConstants(\n       ExtraFilesMap* jit_sources,\n"
                },
                {
                    "old_start": 281,
                    "old_length": 7,
                    "new_start": 281,
                    "new_length": 8,
                    "hunk": "@@ -281,7 +281,8 @@ void FlatbufferLoader::parseAndPopulate(\n }\n \n mobile::Module FlatbufferLoader::parseModule(\n-    mobile::serialization::Module* module) {\n+    mobile::serialization::Module* module,\n+    char* end) {\n   module_ = module;\n   all_ivalues_.clear();\n   all_types_.clear();\n"
                },
                {
                    "old_start": 291,
                    "old_length": 6,
                    "new_start": 292,
                    "new_length": 8,
                    "hunk": "@@ -291,6 +292,8 @@ mobile::Module FlatbufferLoader::parseModule(\n \n   const auto* ivalues = module->ivalues();\n   TORCH_CHECK(ivalues != nullptr, \"Corrupted ivalues field\")\n+  TORCH_CHECK(\n+      reinterpret_cast<const char*>(ivalues) < end, \"Corrupted ivalues field\")\n   all_ivalues_.resize(ivalues->size());\n   all_types_.resize(module->object_types()->size());\n   storages_.resize(module->storage_data_size());\n"
                },
                {
                    "old_start": 303,
                    "old_length": 6,
                    "new_start": 306,
                    "new_length": 8,
                    "hunk": "@@ -303,6 +306,8 @@ mobile::Module FlatbufferLoader::parseModule(\n \n   for (uint32_t i = 0; i < mobile_ivalue_size_; i++) {\n     const auto* ival = ivalues->Get(i);\n+    TORCH_CHECK(\n+        reinterpret_cast<const char*>(ival) < end, \"Corrupted ivalue item\")\n     parseAndPopulate(i, ival);\n   }\n   IValue& module_ivalue = getIValue(module->state_obj());\n"
                },
                {
                    "old_start": 765,
                    "old_length": 7,
                    "new_start": 770,
                    "new_length": 8,
                    "hunk": "@@ -765,7 +770,8 @@ mobile::Module parse_and_initialize_mobile_module(\n   // Flatbuffer doesn't seem to have a way to provide the buffer size when\n   // interacting with the buffer.\n   auto* flatbuffer_module = mobile::serialization::GetMutableModule(data);\n-  mobile::Module m = loader.parseModule(flatbuffer_module);\n+  auto* end = static_cast<char*>(data) + size;\n+  mobile::Module m = loader.parseModule(flatbuffer_module, end);\n   if (extra_files != nullptr) {\n     parseExtraFiles(flatbuffer_module, *extra_files);\n   }\n"
                },
                {
                    "old_start": 807,
                    "old_length": 7,
                    "new_start": 813,
                    "new_length": 8,
                    "hunk": "@@ -807,7 +813,8 @@ mobile::Module parse_and_initialize_mobile_module_for_jit(\n \n   FlatbufferLoader loader;\n   auto* flatbuffer_module = mobile::serialization::GetMutableModule(data);\n-  mobile::Module m = loader.parseModule(flatbuffer_module);\n+  auto* end = static_cast<char*>(data) + size;\n+  mobile::Module m = loader.parseModule(flatbuffer_module, end);\n   if (extra_files != nullptr) {\n     parseExtraFiles(flatbuffer_module, *extra_files);\n   }\n"
                },
                {
                    "old_start": 925,
                    "old_length": 7,
                    "new_start": 932,
                    "new_length": 8,
                    "hunk": "@@ -925,7 +932,8 @@ mobile::Module parse_flatbuffer_no_object(\n         return static_cast<c10::IValue>(obj);\n       });\n \n-  mobile::Module m = loader.parseModule(flatbuffer_module);\n+  auto* end = data.get() + size;\n+  mobile::Module m = loader.parseModule(flatbuffer_module, end);\n   m.set_delete_memory(std::move(data));\n   return m;\n }"
                }
            ],
            "whole_deleted": "-  mobile::Module parseModule(mobile::serialization::Module* module);\n-    mobile::serialization::Module* module) {\n-  mobile::Module m = loader.parseModule(flatbuffer_module);\n-  mobile::Module m = loader.parseModule(flatbuffer_module);\n-  mobile::Module m = loader.parseModule(flatbuffer_module);\n",
            "whole_added": "+  mobile::Module parseModule(mobile::serialization::Module* module, char* end);\n+    mobile::serialization::Module* module,\n+    char* end) {\n+  TORCH_CHECK(\n+      reinterpret_cast<const char*>(ivalues) < end, \"Corrupted ivalues field\")\n+    TORCH_CHECK(\n+        reinterpret_cast<const char*>(ival) < end, \"Corrupted ivalue item\")\n+  auto* end = static_cast<char*>(data) + size;\n+  mobile::Module m = loader.parseModule(flatbuffer_module, end);\n+  auto* end = static_cast<char*>(data) + size;\n+  mobile::Module m = loader.parseModule(flatbuffer_module, end);\n+  auto* end = data.get() + size;\n+  mobile::Module m = loader.parseModule(flatbuffer_module, end);\n",
            "whole_hunk": "@@ -85,7 +85,7 @@ class FlatbufferLoader final {\n   void registerIValueParser(\n       mobile::serialization::IValueUnion ivalue_type,\n       IValueParser parser);\n-  mobile::Module parseModule(mobile::serialization::Module* module);\n+  mobile::Module parseModule(mobile::serialization::Module* module, char* end);\n \n   void extractJitSourceAndConstants(\n       ExtraFilesMap* jit_sources,\n@@ -281,7 +281,8 @@ void FlatbufferLoader::parseAndPopulate(\n }\n \n mobile::Module FlatbufferLoader::parseModule(\n-    mobile::serialization::Module* module) {\n+    mobile::serialization::Module* module,\n+    char* end) {\n   module_ = module;\n   all_ivalues_.clear();\n   all_types_.clear();\n@@ -291,6 +292,8 @@ mobile::Module FlatbufferLoader::parseModule(\n \n   const auto* ivalues = module->ivalues();\n   TORCH_CHECK(ivalues != nullptr, \"Corrupted ivalues field\")\n+  TORCH_CHECK(\n+      reinterpret_cast<const char*>(ivalues) < end, \"Corrupted ivalues field\")\n   all_ivalues_.resize(ivalues->size());\n   all_types_.resize(module->object_types()->size());\n   storages_.resize(module->storage_data_size());\n@@ -303,6 +306,8 @@ mobile::Module FlatbufferLoader::parseModule(\n \n   for (uint32_t i = 0; i < mobile_ivalue_size_; i++) {\n     const auto* ival = ivalues->Get(i);\n+    TORCH_CHECK(\n+        reinterpret_cast<const char*>(ival) < end, \"Corrupted ivalue item\")\n     parseAndPopulate(i, ival);\n   }\n   IValue& module_ivalue = getIValue(module->state_obj());\n@@ -765,7 +770,8 @@ mobile::Module parse_and_initialize_mobile_module(\n   // Flatbuffer doesn't seem to have a way to provide the buffer size when\n   // interacting with the buffer.\n   auto* flatbuffer_module = mobile::serialization::GetMutableModule(data);\n-  mobile::Module m = loader.parseModule(flatbuffer_module);\n+  auto* end = static_cast<char*>(data) + size;\n+  mobile::Module m = loader.parseModule(flatbuffer_module, end);\n   if (extra_files != nullptr) {\n     parseExtraFiles(flatbuffer_module, *extra_files);\n   }\n@@ -807,7 +813,8 @@ mobile::Module parse_and_initialize_mobile_module_for_jit(\n \n   FlatbufferLoader loader;\n   auto* flatbuffer_module = mobile::serialization::GetMutableModule(data);\n-  mobile::Module m = loader.parseModule(flatbuffer_module);\n+  auto* end = static_cast<char*>(data) + size;\n+  mobile::Module m = loader.parseModule(flatbuffer_module, end);\n   if (extra_files != nullptr) {\n     parseExtraFiles(flatbuffer_module, *extra_files);\n   }\n@@ -925,7 +932,8 @@ mobile::Module parse_flatbuffer_no_object(\n         return static_cast<c10::IValue>(obj);\n       });\n \n-  mobile::Module m = loader.parseModule(flatbuffer_module);\n+  auto* end = data.get() + size;\n+  mobile::Module m = loader.parseModule(flatbuffer_module, end);\n   m.set_delete_memory(std::move(data));\n   return m;\n }"
        }
    ]
},
{
    "Id": 163,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/469383755fe416eb1c41fa724762ad3eaecdff07",
    "date": "2024-05-08T05:27:15+00:00",
    "message": "[inductor] add cpp builder code. (#124045)\n\nPrevious full PR https://github.com/pytorch/pytorch/pull/115248 is failed to merge due to fb_code is hard to debug.\nI also tried to submit them as two pieces, https://github.com/pytorch/pytorch/pull/118514 https://github.com/pytorch/pytorch/pull/118515. And they have passed PreCI at that time.\n\nNow I tried to split https://github.com/pytorch/pytorch/pull/115248 into smaller piece, and it is the first step of RFC https://github.com/pytorch/pytorch/issues/124245.\nChanges:\n1. Add cpp builder code, the new cpp_builder support Windows OS.\n2. Add CPU ISA checker which is cross OS and exported from backend cpuinfo.\n3. Switch compiler ISA checker to new cpp builder.\n4. CppCodeCache use the new ISA checker.\n5. Add temprary `test_new_cpp_build_logical` UT to help on transfer to new code.\n<img width=\"1853\" alt=\"Image\" src=\"https://github.com/pytorch/pytorch/assets/8433590/ce6519ab-ba92-4204-b1d6-7d15d2ba2cbe\">\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/124045\nApproved by: https://github.com/jgong5, https://github.com/jansel",
    "label": "NO",
    "changes": [
        {
            "name": "Utils.cpp",
            "path": "aten/src/ATen/cpu/Utils.cpp",
            "patches": [
                {
                    "old_start": 5,
                    "old_length": 6,
                    "new_start": 5,
                    "new_length": 22,
                    "hunk": "@@ -5,6 +5,22 @@\n \n namespace at::cpu {\n \n+bool is_cpu_support_avx2() {\n+#if !defined(__s390x__) && !defined(__powerpc__)\n+  return cpuinfo_initialize() && cpuinfo_has_x86_avx2();\n+#else\n+  return false;\n+#endif\n+}\n+\n+bool is_cpu_support_avx512() {\n+#if !defined(__s390x__) && !defined(__powerpc__)\n+  return cpuinfo_initialize() && cpuinfo_has_x86_avx512f() && cpuinfo_has_x86_avx512vl() && cpuinfo_has_x86_avx512bw() && cpuinfo_has_x86_avx512dq();\n+#else\n+  return false;\n+#endif\n+}\n+\n bool is_cpu_support_vnni() {\n #if !defined(__s390x__) && !defined(__powerpc__)\n   return cpuinfo_initialize() && cpuinfo_has_x86_avx512vnni();\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+bool is_cpu_support_avx2() {\n+#if !defined(__s390x__) && !defined(__powerpc__)\n+  return cpuinfo_initialize() && cpuinfo_has_x86_avx2();\n+#else\n+  return false;\n+#endif\n+}\n+\n+bool is_cpu_support_avx512() {\n+#if !defined(__s390x__) && !defined(__powerpc__)\n+  return cpuinfo_initialize() && cpuinfo_has_x86_avx512f() && cpuinfo_has_x86_avx512vl() && cpuinfo_has_x86_avx512bw() && cpuinfo_has_x86_avx512dq();\n+#else\n+  return false;\n+#endif\n+}\n+\n",
            "whole_hunk": "@@ -5,6 +5,22 @@\n \n namespace at::cpu {\n \n+bool is_cpu_support_avx2() {\n+#if !defined(__s390x__) && !defined(__powerpc__)\n+  return cpuinfo_initialize() && cpuinfo_has_x86_avx2();\n+#else\n+  return false;\n+#endif\n+}\n+\n+bool is_cpu_support_avx512() {\n+#if !defined(__s390x__) && !defined(__powerpc__)\n+  return cpuinfo_initialize() && cpuinfo_has_x86_avx512f() && cpuinfo_has_x86_avx512vl() && cpuinfo_has_x86_avx512bw() && cpuinfo_has_x86_avx512dq();\n+#else\n+  return false;\n+#endif\n+}\n+\n bool is_cpu_support_vnni() {\n #if !defined(__s390x__) && !defined(__powerpc__)\n   return cpuinfo_initialize() && cpuinfo_has_x86_avx512vnni();\n"
        },
        {
            "name": "Utils.h",
            "path": "aten/src/ATen/cpu/Utils.h",
            "patches": [
                {
                    "old_start": 4,
                    "old_length": 6,
                    "new_start": 4,
                    "new_length": 9,
                    "hunk": "@@ -4,6 +4,9 @@\n \n namespace at::cpu {\n \n+TORCH_API bool is_cpu_support_avx2();\n+TORCH_API bool is_cpu_support_avx512();\n+\n // Detect if CPU support Vector Neural Network Instruction.\n TORCH_API bool is_cpu_support_vnni();\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+TORCH_API bool is_cpu_support_avx2();\n+TORCH_API bool is_cpu_support_avx512();\n+\n",
            "whole_hunk": "@@ -4,6 +4,9 @@\n \n namespace at::cpu {\n \n+TORCH_API bool is_cpu_support_avx2();\n+TORCH_API bool is_cpu_support_avx512();\n+\n // Detect if CPU support Vector Neural Network Instruction.\n TORCH_API bool is_cpu_support_vnni();\n \n"
        },
        {
            "name": "test_torchinductor.py",
            "path": "test/inductor/test_torchinductor.py",
            "patches": [
                {
                    "old_start": 6245,
                    "old_length": 6,
                    "new_start": 6245,
                    "new_length": 11,
                    "hunk": "@@ -6245,6 +6245,11 @@ class CommonTemplate:\n \n         self.common(fn, [torch.randn(64, 64)])\n \n+    def test_new_cpp_build_logical(self):\n+        from torch._inductor.codecache import validate_new_cpp_commands\n+\n+        validate_new_cpp_commands()\n+\n     def test_as_strided(self):\n         def fn(x):\n             return (\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    def test_new_cpp_build_logical(self):\n+        from torch._inductor.codecache import validate_new_cpp_commands\n+\n+        validate_new_cpp_commands()\n+\n",
            "whole_hunk": "@@ -6245,6 +6245,11 @@ class CommonTemplate:\n \n         self.common(fn, [torch.randn(64, 64)])\n \n+    def test_new_cpp_build_logical(self):\n+        from torch._inductor.codecache import validate_new_cpp_commands\n+\n+        validate_new_cpp_commands()\n+\n     def test_as_strided(self):\n         def fn(x):\n             return (\n"
        },
        {
            "name": "_cpu.pyi",
            "path": "torch/_C/_cpu.pyi",
            "patches": [
                {
                    "old_start": 2,
                    "old_length": 4,
                    "new_start": 2,
                    "new_length": 6,
                    "hunk": "@@ -2,4 +2,6 @@ from torch.types import _bool\n \n # Defined in torch/csrc/cpu/Module.cpp\n \n+def _is_cpu_support_avx2() -> _bool: ...\n+def _is_cpu_support_avx512() -> _bool: ...\n def _is_cpu_support_vnni() -> _bool: ...\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+def _is_cpu_support_avx2() -> _bool: ...\n+def _is_cpu_support_avx512() -> _bool: ...\n",
            "whole_hunk": "@@ -2,4 +2,6 @@ from torch.types import _bool\n \n # Defined in torch/csrc/cpu/Module.cpp\n \n+def _is_cpu_support_avx2() -> _bool: ...\n+def _is_cpu_support_avx512() -> _bool: ...\n def _is_cpu_support_vnni() -> _bool: ...\n"
        },
        {
            "name": "trace_rules.py",
            "path": "torch/_dynamo/trace_rules.py",
            "patches": [
                {
                    "old_start": 406,
                    "old_length": 6,
                    "new_start": 406,
                    "new_length": 8,
                    "hunk": "@@ -406,6 +406,8 @@ torch_c_binding_in_graph_functions = dict.fromkeys(\n         \"torch._C._construct_CUDA_Tensor_From_Storage_And_Metadata\",\n         \"torch._C._construct_storage_from_data_pointer\",\n         \"torch._C._conv_determine_backend_memory_format\",\n+        \"torch._C._cpu._is_cpu_support_avx2\",\n+        \"torch._C._cpu._is_cpu_support_avx512\",\n         \"torch._C._cpu._is_cpu_support_vnni\",\n         \"torch._C._crash_if_aten_asan\",\n         \"torch._C._crash_if_csrc_asan\",\n"
                },
                {
                    "old_start": 2420,
                    "old_length": 6,
                    "new_start": 2422,
                    "new_length": 8,
                    "hunk": "@@ -2420,6 +2422,8 @@ torch_non_c_binding_in_graph_functions = dict.fromkeys(\n         \"torch.chain_matmul\",\n         \"torch.compile\",\n         \"torch.compiled_with_cxx11_abi\",\n+        \"torch.cpu._is_cpu_support_avx2\",\n+        \"torch.cpu._is_cpu_support_avx512\",\n         \"torch.cpu._is_cpu_support_vnni\",\n         \"torch.cpu.current_device\",\n         \"torch.cpu.current_stream\",\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"torch._C._cpu._is_cpu_support_avx2\",\n+        \"torch._C._cpu._is_cpu_support_avx512\",\n+        \"torch.cpu._is_cpu_support_avx2\",\n+        \"torch.cpu._is_cpu_support_avx512\",\n",
            "whole_hunk": "@@ -406,6 +406,8 @@ torch_c_binding_in_graph_functions = dict.fromkeys(\n         \"torch._C._construct_CUDA_Tensor_From_Storage_And_Metadata\",\n         \"torch._C._construct_storage_from_data_pointer\",\n         \"torch._C._conv_determine_backend_memory_format\",\n+        \"torch._C._cpu._is_cpu_support_avx2\",\n+        \"torch._C._cpu._is_cpu_support_avx512\",\n         \"torch._C._cpu._is_cpu_support_vnni\",\n         \"torch._C._crash_if_aten_asan\",\n         \"torch._C._crash_if_csrc_asan\",\n@@ -2420,6 +2422,8 @@ torch_non_c_binding_in_graph_functions = dict.fromkeys(\n         \"torch.chain_matmul\",\n         \"torch.compile\",\n         \"torch.compiled_with_cxx11_abi\",\n+        \"torch.cpu._is_cpu_support_avx2\",\n+        \"torch.cpu._is_cpu_support_avx512\",\n         \"torch.cpu._is_cpu_support_vnni\",\n         \"torch.cpu.current_device\",\n         \"torch.cpu.current_stream\",\n"
        },
        {
            "name": "codecache.py",
            "path": "torch/_inductor/codecache.py",
            "patches": [
                {
                    "old_start": 2,
                    "old_length": 6,
                    "new_start": 2,
                    "new_length": 7,
                    "hunk": "@@ -2,6 +2,7 @@ from __future__ import annotations\n \n import base64\n import copyreg\n+import ctypes\n import dataclasses\n import functools\n import hashlib\n"
                },
                {
                    "old_start": 82,
                    "old_length": 6,
                    "new_start": 83,
                    "new_length": 8,
                    "hunk": "@@ -82,6 +83,8 @@ _HERE = os.path.abspath(__file__)\n _TORCH_PATH = os.path.dirname(os.path.dirname(_HERE))\n _LINKER_SCRIPT = os.path.join(_TORCH_PATH, \"_inductor/script.ld\")\n \n+_IS_WINDOWS = sys.platform == \"win32\"\n+\n if config.is_fbcode():\n     from triton.fb import build_paths\n     from triton.fb.build import _run_build_command\n"
                },
                {
                    "old_start": 1191,
                    "old_length": 7,
                    "new_start": 1194,
                    "new_length": 7,
                    "hunk": "@@ -1191,7 +1194,7 @@ def _get_isa_dry_compile_fingerprint(isa_flags: str) -> str:\n \n class VecISA:\n     _bit_width: int\n-    _macro: str\n+    _macro: List[str]\n     _arch_flags: str\n     _dtype_nelements: Dict[torch.dtype, int]\n \n"
                },
                {
                    "old_start": 1237,
                    "old_length": 7,
                    "new_start": 1240,
                    "new_length": 7,
                    "hunk": "@@ -1237,7 +1240,7 @@ cdll.LoadLibrary(\"__lib_path__\")\n     def nelements(self, dtype: torch.dtype = torch.float) -> int:\n         return self._dtype_nelements[dtype]\n \n-    def build_macro(self) -> str:\n+    def build_macro(self) -> List[str]:\n         return self._macro\n \n     def build_arch_flags(self) -> str:\n"
                },
                {
                    "old_start": 1248,
                    "old_length": 6,
                    "new_start": 1251,
                    "new_length": 8,
                    "hunk": "@@ -1248,6 +1251,8 @@ cdll.LoadLibrary(\"__lib_path__\")\n \n     @functools.lru_cache(None)\n     def __bool__(self) -> bool:\n+        from torch._inductor.cpp_builder import CppBuilder, CppTorchOptions\n+\n         if config.cpp.vec_isa_ok is not None:\n             return config.cpp.vec_isa_ok\n \n"
                },
                {
                    "old_start": 1264,
                    "old_length": 16,
                    "new_start": 1269,
                    "new_length": 21,
                    "hunk": "@@ -1264,16 +1269,21 @@ cdll.LoadLibrary(\"__lib_path__\")\n         lock_dir = get_lock_dir()\n         lock = FileLock(os.path.join(lock_dir, key + \".lock\"), timeout=LOCK_TIMEOUT)\n         with lock:\n-            output_path = input_path[:-3] + \"so\"\n-            build_cmd = shlex.split(\n-                cpp_compile_command(\n-                    input_path, output_path, warning_all=False, vec_isa=self\n-                )\n+            output_dir = os.path.dirname(input_path)\n+            buid_options = CppTorchOptions(chosen_isa=self, warning_all=False)\n+            x86_isa_help_builder = CppBuilder(\n+                key,\n+                [input_path],\n+                buid_options,\n+                output_dir,\n             )\n             try:\n                 # Check if the output file exist, and compile when not.\n+                output_path = x86_isa_help_builder.get_target_file_path()\n                 if not os.path.isfile(output_path):\n-                    compile_file(input_path, output_path, build_cmd)\n+                    status, target_file = x86_isa_help_builder.build()\n+                    if status:\n+                        return False\n \n                 # Check build result\n                 subprocess.check_call(\n"
                },
                {
                    "old_start": 1294,
                    "old_length": 7,
                    "new_start": 1304,
                    "new_length": 7,
                    "hunk": "@@ -1294,7 +1304,7 @@ cdll.LoadLibrary(\"__lib_path__\")\n @dataclasses.dataclass\n class VecNEON(VecISA):\n     _bit_width = 256  # This is required to leverage the compute implemented in aten/src/ATen/cpu/vec/vec256/vec256_float_neon.h\n-    _macro = \"-DCPU_CAPABILITY_NEON\"\n+    _macro = [\"CPU_CAPABILITY_NEON\"]\n     _arch_flags = \"\"  # Unused\n     _dtype_nelements = {torch.float: 8, torch.bfloat16: 16, torch.float16: 16}\n \n"
                },
                {
                    "old_start": 1307,
                    "old_length": 8,
                    "new_start": 1317,
                    "new_length": 12,
                    "hunk": "@@ -1307,8 +1317,12 @@ class VecNEON(VecISA):\n @dataclasses.dataclass\n class VecAVX512(VecISA):\n     _bit_width = 512\n-    _macro = \"-DCPU_CAPABILITY_AVX512\"\n-    _arch_flags = \"-mavx512f -mavx512dq -mavx512vl -mavx512bw -mfma\"\n+    _macro = [\"CPU_CAPABILITY_AVX512\"]\n+    _arch_flags = (\n+        \"-mavx512f -mavx512dq -mavx512vl -mavx512bw -mfma\"\n+        if not _IS_WINDOWS\n+        else \"/arch:AVX512\"\n+    )  # TODO: use cflags\n     _dtype_nelements = {torch.float: 16, torch.bfloat16: 32, torch.float16: 32}\n \n     def __str__(self) -> str:\n"
                },
                {
                    "old_start": 1320,
                    "old_length": 8,
                    "new_start": 1334,
                    "new_length": 10,
                    "hunk": "@@ -1320,8 +1334,10 @@ class VecAVX512(VecISA):\n @dataclasses.dataclass\n class VecAVX2(VecISA):\n     _bit_width = 256\n-    _macro = \"-DCPU_CAPABILITY_AVX2\"\n-    _arch_flags = \"-mavx2 -mfma\"\n+    _macro = [\"CPU_CAPABILITY_AVX2\"]\n+    _arch_flags = (\n+        \"-mavx2 -mfma\" if not _IS_WINDOWS else \"/arch:AVX2\"\n+    )  # TODO: use cflags\n     _dtype_nelements = {torch.float: 8, torch.bfloat16: 16, torch.float16: 16}\n \n     def __str__(self) -> str:\n"
                },
                {
                    "old_start": 1333,
                    "old_length": 7,
                    "new_start": 1349,
                    "new_length": 11,
                    "hunk": "@@ -1333,7 +1349,11 @@ class VecAVX2(VecISA):\n @dataclasses.dataclass\n class VecZVECTOR(VecISA):\n     _bit_width = 256\n-    _macro = \"-DCPU_CAPABILITY_ZVECTOR -DCPU_CAPABILITY=ZVECTOR -DHAVE_ZVECTOR_CPU_DEFINITION\"\n+    _macro = [\n+        \"CPU_CAPABILITY_ZVECTOR\",\n+        \"CPU_CAPABILITY=ZVECTOR\",\n+        \"HAVE_ZVECTOR_CPU_DEFINITION\",\n+    ]\n     _arch_flags = \"-mvx -mzvector\"\n     _dtype_nelements = {torch.float: 8, torch.bfloat16: 16, torch.float16: 16}\n \n"
                },
                {
                    "old_start": 1345,
                    "old_length": 7,
                    "new_start": 1365,
                    "new_length": 7,
                    "hunk": "@@ -1345,7 +1365,7 @@ class VecZVECTOR(VecISA):\n \n class InvalidVecISA(VecISA):\n     _bit_width = 0\n-    _macro = \"\"\n+    _macro = [\"\"]\n     _arch_flags = \"\"\n     _dtype_nelements = {}\n \n"
                },
                {
                    "old_start": 1358,
                    "old_length": 6,
                    "new_start": 1378,
                    "new_length": 31,
                    "hunk": "@@ -1358,6 +1378,31 @@ class InvalidVecISA(VecISA):\n     __hash__: Callable[[VecISA], Any] = VecISA.__hash__\n \n \n+def x86_isa_checker() -> List[str]:\n+    supported_isa: List[str] = []\n+\n+    def _check_and_append_supported_isa(\n+        dest: List[str], isa_supported: bool, isa_name: str\n+    ):\n+        if isa_supported is True:\n+            dest.append(isa_name)\n+\n+    Arch = platform.machine()\n+    \"\"\"\n+    Arch value is x86_64 on Linux, and the value is AMD64 on Windows.\n+    \"\"\"\n+    if Arch != \"x86_64\" and Arch != \"AMD64\":\n+        return supported_isa\n+\n+    avx2 = torch.cpu._is_cpu_support_avx2()\n+    avx512 = torch.cpu._is_cpu_support_avx512()\n+\n+    _check_and_append_supported_isa(supported_isa, avx2, \"avx2\")\n+    _check_and_append_supported_isa(supported_isa, avx512, \"avx512\")\n+\n+    return supported_isa\n+\n+\n invalid_vec_isa = InvalidVecISA()\n supported_vec_isa_list = [VecAVX512(), VecAVX2(), VecNEON()]\n \n"
                },
                {
                    "old_start": 1370,
                    "old_length": 7,
                    "new_start": 1415,
                    "new_length": 8,
                    "hunk": "@@ -1370,7 +1415,8 @@ def valid_vec_isa_list() -> List[VecISA]:\n     if sys.platform == \"darwin\" and platform.processor() == \"arm\":\n         return [VecNEON()]\n \n-    if sys.platform != \"linux\":\n+    cur_os = sys.platform\n+    if cur_os != \"linux\" and cur_os != \"win32\":\n         return []\n \n     if platform.machine() == \"s390x\":\n"
                },
                {
                    "old_start": 1388,
                    "old_length": 12,
                    "new_start": 1434,
                    "new_length": 11,
                    "hunk": "@@ -1388,12 +1434,11 @@ def valid_vec_isa_list() -> List[VecISA]:\n         return []\n \n     isa_list = []\n-    with open(\"/proc/cpuinfo\") as _cpu_info:\n-        _cpu_info_content = _cpu_info.read()\n-        for isa in supported_vec_isa_list:\n-            if str(isa) in _cpu_info_content and isa:\n-                isa_list.append(isa)\n-        return isa_list\n+    _cpu_supported_isa = x86_isa_checker()\n+    for isa in supported_vec_isa_list:\n+        if str(isa) in _cpu_supported_isa:\n+            isa_list.append(isa)\n+    return isa_list\n \n \n def pick_vec_isa() -> VecISA:\n"
                },
                {
                    "old_start": 1569,
                    "old_length": 7,
                    "new_start": 1615,
                    "new_length": 14,
                    "hunk": "@@ -1569,7 +1615,14 @@ def get_include_and_linking_paths(\n     _set_gpu_runtime_env()\n     from torch.utils import cpp_extension\n \n-    macros = vec_isa.build_macro() if vec_isa != invalid_vec_isa else \"\"\n+    # Remove below in the further\n+    # macros = \"-D {}\".format(vec_isa.build_macro()) if vec_isa != invalid_vec_isa else \"\"\n+    macros = \"\"\n+    if vec_isa != invalid_vec_isa:\n+        for x in vec_isa.build_macro():\n+            macros_def = f\"-D{x} \"\n+            macros += macros_def\n+\n     build_arch_flags = \"\"\n     if sys.platform == \"linux\" and (\n         include_pytorch\n"
                },
                {
                    "old_start": 1789,
                    "old_length": 7,
                    "new_start": 1842,
                    "new_length": 7,
                    "hunk": "@@ -1789,7 +1842,7 @@ def cpp_compile_command(\n             {get_warning_all_flag(warning_all)} {cpp_flags()}\n             {get_glibcxx_abi_build_flags()}\n             {ipaths_str} {lpaths} {libs} {build_arch_flags}\n-            {macros} {linker_paths} {clang_flags}\n+            {macros} {linker_paths} {clang_flags} {cpp_wrapper_flags()}\n             {optimization_flags()}\n             {use_custom_generated_macros()}\n             {use_fb_internal_macros()}\n"
                },
                {
                    "old_start": 2041,
                    "old_length": 7,
                    "new_start": 2094,
                    "new_length": 6,
                    "hunk": "@@ -2041,7 +2094,6 @@ class AotCodeCompiler:\n             def _to_bytes(t: torch.Tensor) -> bytes:\n                 # This serializes the tensor's untyped_storage to bytes by accessing\n                 # the raw data of the underlying structure.\n-                import ctypes\n \n                 if t.numel() == 0:\n                     return b\"\"\n"
                },
                {
                    "old_start": 2265,
                    "old_length": 8,
                    "new_start": 2317,
                    "new_length": 18,
                    "hunk": "@@ -2265,8 +2317,18 @@ class CppCodeCache:\n             \"cuda\": cuda,\n             \"vec_isa\": pick_vec_isa(),\n         }\n-        cpp_command = repr(cpp_compile_command(\"i\", \"o\", **compile_command))\n-        key, input_path = write(source_code, \"cpp\", extra=cpp_command)\n+\n+        from torch._inductor.cpp_builder import CppBuilder, CppTorchOptions\n+\n+        picked_vec_isa = pick_vec_isa()\n+        dummy_builder = CppBuilder(\"i\", [\"o\"], CppTorchOptions(picked_vec_isa))\n+        # write function will calc source_code hash, the same source code with different\n+        # ISA level should be generate different hash.\n+        # So we need get a command_line which contains isa related parameter as a part of hash key.\n+        # And then pass the command_line to below write function as extra parameter to\n+        # guarantee the source code hash contains ISA difference.\n+        dummy_cmd = dummy_builder.get_command_line()\n+        key, input_path = write(source_code, \"cpp\", extra=dummy_cmd)\n \n         if key not in cls.cache:\n             from filelock import FileLock\n"
                },
                {
                    "old_start": 2535,
                    "old_length": 7,
                    "new_start": 2597,
                    "new_length": 85,
                    "hunk": "@@ -2535,7 +2597,85 @@ class CppWrapperCodeCache(CppPythonBindingsCodeCache):\n     )\n \n \n-@clear_on_fresh_inductor_cache\n+# TODO: Will remove the temp code after switch to new cpp_builder\n+def _temp_validate_new_and_old_command(new_cmd: List[str], old_cmd: List[str]):\n+    new_diff: List[str] = [x for x in new_cmd if x not in old_cmd]\n+    old_diff: List[str] = [y for y in old_cmd if y not in new_cmd]\n+\n+    if new_diff or old_diff:\n+        print(\"!!! new_cmd: \", new_cmd)\n+        print(\"!!! old_cmd: \", old_cmd)\n+        print(\"!!! new_diff: \", new_diff)\n+        print(\"!!! old_diff: \", old_diff)\n+        raise RuntimeError(\"Error in new and old command different.\")\n+\n+\n+def _do_validate_cpp_commands(\n+    include_pytorch: bool, cuda: bool, compile_only: bool, mmap_weights: bool\n+):\n+    # PreCI will failed if test machine can't run cuda.\n+    test_cuda = torch.cuda.is_available() and cuda\n+    input_path = \"/temp/dummy_input.cpp\"\n+    output_path = \"/temp/dummy_output.so\"\n+    if compile_only:\n+        output_path = \"/temp/dummy_output.o\"\n+    picked_isa = pick_vec_isa()\n+\n+    old_cmd = cpp_compile_command(\n+        input=input_path,\n+        output=output_path,\n+        include_pytorch=include_pytorch,\n+        vec_isa=picked_isa,\n+        cuda=test_cuda,\n+        aot_mode=False,\n+        compile_only=compile_only,\n+        use_absolute_path=False,\n+        use_mmap_weights=mmap_weights,\n+    ).split(\" \")\n+\n+    from torch._inductor.cpp_builder import CppBuilder, CppTorchCudaOptions\n+\n+    dummy_build_option = CppTorchCudaOptions(\n+        chosen_isa=picked_isa,\n+        include_pytorch=include_pytorch,\n+        use_cuda=test_cuda,\n+        compile_only=compile_only,\n+        use_mmap_weights=mmap_weights,\n+    )\n+\n+    dummy_builder = CppBuilder(\n+        name=\"dummy_output\",\n+        sources=input_path,\n+        BuildOption=dummy_build_option,\n+        output_dir=\"/temp/\",\n+        compile_only=compile_only,\n+        use_absolute_path=False,\n+    )\n+    new_cmd = dummy_builder.get_command_line().split(\" \")\n+\n+    _temp_validate_new_and_old_command(new_cmd, old_cmd)\n+\n+\n+# TODO: Will remove the temp code after switch to new cpp_builder\n+# It could help on sync new cpp_builder generate same command line as the old one.\n+def validate_new_cpp_commands():\n+    cuda = [True, False]\n+    use_mmap_weights = [True, False]\n+    compile_only = [True, False]\n+    include_pytorch = [True, False]\n+\n+    for x in cuda:\n+        for y in use_mmap_weights:\n+            for z in compile_only:\n+                for m in include_pytorch:\n+                    print(\n+                        f\"!!! cuda:{x}, use_mmap_weights:{y}, compile_only:{z}, include_pytorch:{m}\"\n+                    )\n+                    _do_validate_cpp_commands(\n+                        include_pytorch=m, cuda=x, mmap_weights=y, compile_only=z\n+                    )\n+\n+\n class PyCodeCache:\n     cache: Dict[str, ModuleType] = dict()\n     linemaps: Dict[str, List[Tuple[Any, ...]]] = dict()\n"
                }
            ],
            "whole_deleted": "-    _macro: str\n-    def build_macro(self) -> str:\n-            output_path = input_path[:-3] + \"so\"\n-            build_cmd = shlex.split(\n-                cpp_compile_command(\n-                    input_path, output_path, warning_all=False, vec_isa=self\n-                )\n-                    compile_file(input_path, output_path, build_cmd)\n-    _macro = \"-DCPU_CAPABILITY_NEON\"\n-    _macro = \"-DCPU_CAPABILITY_AVX512\"\n-    _arch_flags = \"-mavx512f -mavx512dq -mavx512vl -mavx512bw -mfma\"\n-    _macro = \"-DCPU_CAPABILITY_AVX2\"\n-    _arch_flags = \"-mavx2 -mfma\"\n-    _macro = \"-DCPU_CAPABILITY_ZVECTOR -DCPU_CAPABILITY=ZVECTOR -DHAVE_ZVECTOR_CPU_DEFINITION\"\n-    _macro = \"\"\n-    if sys.platform != \"linux\":\n-    with open(\"/proc/cpuinfo\") as _cpu_info:\n-        _cpu_info_content = _cpu_info.read()\n-        for isa in supported_vec_isa_list:\n-            if str(isa) in _cpu_info_content and isa:\n-                isa_list.append(isa)\n-        return isa_list\n-    macros = vec_isa.build_macro() if vec_isa != invalid_vec_isa else \"\"\n-            {macros} {linker_paths} {clang_flags}\n-                import ctypes\n-        cpp_command = repr(cpp_compile_command(\"i\", \"o\", **compile_command))\n-        key, input_path = write(source_code, \"cpp\", extra=cpp_command)\n-@clear_on_fresh_inductor_cache\n",
            "whole_added": "+import ctypes\n+_IS_WINDOWS = sys.platform == \"win32\"\n+\n+    _macro: List[str]\n+    def build_macro(self) -> List[str]:\n+        from torch._inductor.cpp_builder import CppBuilder, CppTorchOptions\n+\n+            output_dir = os.path.dirname(input_path)\n+            buid_options = CppTorchOptions(chosen_isa=self, warning_all=False)\n+            x86_isa_help_builder = CppBuilder(\n+                key,\n+                [input_path],\n+                buid_options,\n+                output_dir,\n+                output_path = x86_isa_help_builder.get_target_file_path()\n+                    status, target_file = x86_isa_help_builder.build()\n+                    if status:\n+                        return False\n+    _macro = [\"CPU_CAPABILITY_NEON\"]\n+    _macro = [\"CPU_CAPABILITY_AVX512\"]\n+    _arch_flags = (\n+        \"-mavx512f -mavx512dq -mavx512vl -mavx512bw -mfma\"\n+        if not _IS_WINDOWS\n+        else \"/arch:AVX512\"\n+    )  # TODO: use cflags\n+    _macro = [\"CPU_CAPABILITY_AVX2\"]\n+    _arch_flags = (\n+        \"-mavx2 -mfma\" if not _IS_WINDOWS else \"/arch:AVX2\"\n+    )  # TODO: use cflags\n+    _macro = [\n+        \"CPU_CAPABILITY_ZVECTOR\",\n+        \"CPU_CAPABILITY=ZVECTOR\",\n+        \"HAVE_ZVECTOR_CPU_DEFINITION\",\n+    ]\n+    _macro = [\"\"]\n+def x86_isa_checker() -> List[str]:\n+    supported_isa: List[str] = []\n+\n+    def _check_and_append_supported_isa(\n+        dest: List[str], isa_supported: bool, isa_name: str\n+    ):\n+        if isa_supported is True:\n+            dest.append(isa_name)\n+\n+    Arch = platform.machine()\n+    \"\"\"\n+    Arch value is x86_64 on Linux, and the value is AMD64 on Windows.\n+    \"\"\"\n+    if Arch != \"x86_64\" and Arch != \"AMD64\":\n+        return supported_isa\n+\n+    avx2 = torch.cpu._is_cpu_support_avx2()\n+    avx512 = torch.cpu._is_cpu_support_avx512()\n+\n+    _check_and_append_supported_isa(supported_isa, avx2, \"avx2\")\n+    _check_and_append_supported_isa(supported_isa, avx512, \"avx512\")\n+\n+    return supported_isa\n+\n+\n+    cur_os = sys.platform\n+    if cur_os != \"linux\" and cur_os != \"win32\":\n+    _cpu_supported_isa = x86_isa_checker()\n+    for isa in supported_vec_isa_list:\n+        if str(isa) in _cpu_supported_isa:\n+            isa_list.append(isa)\n+    return isa_list\n+    # Remove below in the further\n+    # macros = \"-D {}\".format(vec_isa.build_macro()) if vec_isa != invalid_vec_isa else \"\"\n+    macros = \"\"\n+    if vec_isa != invalid_vec_isa:\n+        for x in vec_isa.build_macro():\n+            macros_def = f\"-D{x} \"\n+            macros += macros_def\n+\n+            {macros} {linker_paths} {clang_flags} {cpp_wrapper_flags()}\n+\n+        from torch._inductor.cpp_builder import CppBuilder, CppTorchOptions\n+\n+        picked_vec_isa = pick_vec_isa()\n+        dummy_builder = CppBuilder(\"i\", [\"o\"], CppTorchOptions(picked_vec_isa))\n+        # write function will calc source_code hash, the same source code with different\n+        # ISA level should be generate different hash.\n+        # So we need get a command_line which contains isa related parameter as a part of hash key.\n+        # And then pass the command_line to below write function as extra parameter to\n+        # guarantee the source code hash contains ISA difference.\n+        dummy_cmd = dummy_builder.get_command_line()\n+        key, input_path = write(source_code, \"cpp\", extra=dummy_cmd)\n+# TODO: Will remove the temp code after switch to new cpp_builder\n+def _temp_validate_new_and_old_command(new_cmd: List[str], old_cmd: List[str]):\n+    new_diff: List[str] = [x for x in new_cmd if x not in old_cmd]\n+    old_diff: List[str] = [y for y in old_cmd if y not in new_cmd]\n+\n+    if new_diff or old_diff:\n+        print(\"!!! new_cmd: \", new_cmd)\n+        print(\"!!! old_cmd: \", old_cmd)\n+        print(\"!!! new_diff: \", new_diff)\n+        print(\"!!! old_diff: \", old_diff)\n+        raise RuntimeError(\"Error in new and old command different.\")\n+\n+\n+def _do_validate_cpp_commands(\n+    include_pytorch: bool, cuda: bool, compile_only: bool, mmap_weights: bool\n+):\n+    # PreCI will failed if test machine can't run cuda.\n+    test_cuda = torch.cuda.is_available() and cuda\n+    input_path = \"/temp/dummy_input.cpp\"\n+    output_path = \"/temp/dummy_output.so\"\n+    if compile_only:\n+        output_path = \"/temp/dummy_output.o\"\n+    picked_isa = pick_vec_isa()\n+\n+    old_cmd = cpp_compile_command(\n+        input=input_path,\n+        output=output_path,\n+        include_pytorch=include_pytorch,\n+        vec_isa=picked_isa,\n+        cuda=test_cuda,\n+        aot_mode=False,\n+        compile_only=compile_only,\n+        use_absolute_path=False,\n+        use_mmap_weights=mmap_weights,\n+    ).split(\" \")\n+\n+    from torch._inductor.cpp_builder import CppBuilder, CppTorchCudaOptions\n+\n+    dummy_build_option = CppTorchCudaOptions(\n+        chosen_isa=picked_isa,\n+        include_pytorch=include_pytorch,\n+        use_cuda=test_cuda,\n+        compile_only=compile_only,\n+        use_mmap_weights=mmap_weights,\n+    )\n+\n+    dummy_builder = CppBuilder(\n+        name=\"dummy_output\",\n+        sources=input_path,\n+        BuildOption=dummy_build_option,\n+        output_dir=\"/temp/\",\n+        compile_only=compile_only,\n+        use_absolute_path=False,\n+    )\n+    new_cmd = dummy_builder.get_command_line().split(\" \")\n+\n+    _temp_validate_new_and_old_command(new_cmd, old_cmd)\n+\n+\n+# TODO: Will remove the temp code after switch to new cpp_builder\n+# It could help on sync new cpp_builder generate same command line as the old one.\n+def validate_new_cpp_commands():\n+    cuda = [True, False]\n+    use_mmap_weights = [True, False]\n+    compile_only = [True, False]\n+    include_pytorch = [True, False]\n+\n+    for x in cuda:\n+        for y in use_mmap_weights:\n+            for z in compile_only:\n+                for m in include_pytorch:\n+                    print(\n+                        f\"!!! cuda:{x}, use_mmap_weights:{y}, compile_only:{z}, include_pytorch:{m}\"\n+                    )\n+                    _do_validate_cpp_commands(\n+                        include_pytorch=m, cuda=x, mmap_weights=y, compile_only=z\n+                    )\n+\n+\n",
            "whole_hunk": "@@ -2,6 +2,7 @@ from __future__ import annotations\n \n import base64\n import copyreg\n+import ctypes\n import dataclasses\n import functools\n import hashlib\n@@ -82,6 +83,8 @@ _HERE = os.path.abspath(__file__)\n _TORCH_PATH = os.path.dirname(os.path.dirname(_HERE))\n _LINKER_SCRIPT = os.path.join(_TORCH_PATH, \"_inductor/script.ld\")\n \n+_IS_WINDOWS = sys.platform == \"win32\"\n+\n if config.is_fbcode():\n     from triton.fb import build_paths\n     from triton.fb.build import _run_build_command\n@@ -1191,7 +1194,7 @@ def _get_isa_dry_compile_fingerprint(isa_flags: str) -> str:\n \n class VecISA:\n     _bit_width: int\n-    _macro: str\n+    _macro: List[str]\n     _arch_flags: str\n     _dtype_nelements: Dict[torch.dtype, int]\n \n@@ -1237,7 +1240,7 @@ cdll.LoadLibrary(\"__lib_path__\")\n     def nelements(self, dtype: torch.dtype = torch.float) -> int:\n         return self._dtype_nelements[dtype]\n \n-    def build_macro(self) -> str:\n+    def build_macro(self) -> List[str]:\n         return self._macro\n \n     def build_arch_flags(self) -> str:\n@@ -1248,6 +1251,8 @@ cdll.LoadLibrary(\"__lib_path__\")\n \n     @functools.lru_cache(None)\n     def __bool__(self) -> bool:\n+        from torch._inductor.cpp_builder import CppBuilder, CppTorchOptions\n+\n         if config.cpp.vec_isa_ok is not None:\n             return config.cpp.vec_isa_ok\n \n@@ -1264,16 +1269,21 @@ cdll.LoadLibrary(\"__lib_path__\")\n         lock_dir = get_lock_dir()\n         lock = FileLock(os.path.join(lock_dir, key + \".lock\"), timeout=LOCK_TIMEOUT)\n         with lock:\n-            output_path = input_path[:-3] + \"so\"\n-            build_cmd = shlex.split(\n-                cpp_compile_command(\n-                    input_path, output_path, warning_all=False, vec_isa=self\n-                )\n+            output_dir = os.path.dirname(input_path)\n+            buid_options = CppTorchOptions(chosen_isa=self, warning_all=False)\n+            x86_isa_help_builder = CppBuilder(\n+                key,\n+                [input_path],\n+                buid_options,\n+                output_dir,\n             )\n             try:\n                 # Check if the output file exist, and compile when not.\n+                output_path = x86_isa_help_builder.get_target_file_path()\n                 if not os.path.isfile(output_path):\n-                    compile_file(input_path, output_path, build_cmd)\n+                    status, target_file = x86_isa_help_builder.build()\n+                    if status:\n+                        return False\n \n                 # Check build result\n                 subprocess.check_call(\n@@ -1294,7 +1304,7 @@ cdll.LoadLibrary(\"__lib_path__\")\n @dataclasses.dataclass\n class VecNEON(VecISA):\n     _bit_width = 256  # This is required to leverage the compute implemented in aten/src/ATen/cpu/vec/vec256/vec256_float_neon.h\n-    _macro = \"-DCPU_CAPABILITY_NEON\"\n+    _macro = [\"CPU_CAPABILITY_NEON\"]\n     _arch_flags = \"\"  # Unused\n     _dtype_nelements = {torch.float: 8, torch.bfloat16: 16, torch.float16: 16}\n \n@@ -1307,8 +1317,12 @@ class VecNEON(VecISA):\n @dataclasses.dataclass\n class VecAVX512(VecISA):\n     _bit_width = 512\n-    _macro = \"-DCPU_CAPABILITY_AVX512\"\n-    _arch_flags = \"-mavx512f -mavx512dq -mavx512vl -mavx512bw -mfma\"\n+    _macro = [\"CPU_CAPABILITY_AVX512\"]\n+    _arch_flags = (\n+        \"-mavx512f -mavx512dq -mavx512vl -mavx512bw -mfma\"\n+        if not _IS_WINDOWS\n+        else \"/arch:AVX512\"\n+    )  # TODO: use cflags\n     _dtype_nelements = {torch.float: 16, torch.bfloat16: 32, torch.float16: 32}\n \n     def __str__(self) -> str:\n@@ -1320,8 +1334,10 @@ class VecAVX512(VecISA):\n @dataclasses.dataclass\n class VecAVX2(VecISA):\n     _bit_width = 256\n-    _macro = \"-DCPU_CAPABILITY_AVX2\"\n-    _arch_flags = \"-mavx2 -mfma\"\n+    _macro = [\"CPU_CAPABILITY_AVX2\"]\n+    _arch_flags = (\n+        \"-mavx2 -mfma\" if not _IS_WINDOWS else \"/arch:AVX2\"\n+    )  # TODO: use cflags\n     _dtype_nelements = {torch.float: 8, torch.bfloat16: 16, torch.float16: 16}\n \n     def __str__(self) -> str:\n@@ -1333,7 +1349,11 @@ class VecAVX2(VecISA):\n @dataclasses.dataclass\n class VecZVECTOR(VecISA):\n     _bit_width = 256\n-    _macro = \"-DCPU_CAPABILITY_ZVECTOR -DCPU_CAPABILITY=ZVECTOR -DHAVE_ZVECTOR_CPU_DEFINITION\"\n+    _macro = [\n+        \"CPU_CAPABILITY_ZVECTOR\",\n+        \"CPU_CAPABILITY=ZVECTOR\",\n+        \"HAVE_ZVECTOR_CPU_DEFINITION\",\n+    ]\n     _arch_flags = \"-mvx -mzvector\"\n     _dtype_nelements = {torch.float: 8, torch.bfloat16: 16, torch.float16: 16}\n \n@@ -1345,7 +1365,7 @@ class VecZVECTOR(VecISA):\n \n class InvalidVecISA(VecISA):\n     _bit_width = 0\n-    _macro = \"\"\n+    _macro = [\"\"]\n     _arch_flags = \"\"\n     _dtype_nelements = {}\n \n@@ -1358,6 +1378,31 @@ class InvalidVecISA(VecISA):\n     __hash__: Callable[[VecISA], Any] = VecISA.__hash__\n \n \n+def x86_isa_checker() -> List[str]:\n+    supported_isa: List[str] = []\n+\n+    def _check_and_append_supported_isa(\n+        dest: List[str], isa_supported: bool, isa_name: str\n+    ):\n+        if isa_supported is True:\n+            dest.append(isa_name)\n+\n+    Arch = platform.machine()\n+    \"\"\"\n+    Arch value is x86_64 on Linux, and the value is AMD64 on Windows.\n+    \"\"\"\n+    if Arch != \"x86_64\" and Arch != \"AMD64\":\n+        return supported_isa\n+\n+    avx2 = torch.cpu._is_cpu_support_avx2()\n+    avx512 = torch.cpu._is_cpu_support_avx512()\n+\n+    _check_and_append_supported_isa(supported_isa, avx2, \"avx2\")\n+    _check_and_append_supported_isa(supported_isa, avx512, \"avx512\")\n+\n+    return supported_isa\n+\n+\n invalid_vec_isa = InvalidVecISA()\n supported_vec_isa_list = [VecAVX512(), VecAVX2(), VecNEON()]\n \n@@ -1370,7 +1415,8 @@ def valid_vec_isa_list() -> List[VecISA]:\n     if sys.platform == \"darwin\" and platform.processor() == \"arm\":\n         return [VecNEON()]\n \n-    if sys.platform != \"linux\":\n+    cur_os = sys.platform\n+    if cur_os != \"linux\" and cur_os != \"win32\":\n         return []\n \n     if platform.machine() == \"s390x\":\n@@ -1388,12 +1434,11 @@ def valid_vec_isa_list() -> List[VecISA]:\n         return []\n \n     isa_list = []\n-    with open(\"/proc/cpuinfo\") as _cpu_info:\n-        _cpu_info_content = _cpu_info.read()\n-        for isa in supported_vec_isa_list:\n-            if str(isa) in _cpu_info_content and isa:\n-                isa_list.append(isa)\n-        return isa_list\n+    _cpu_supported_isa = x86_isa_checker()\n+    for isa in supported_vec_isa_list:\n+        if str(isa) in _cpu_supported_isa:\n+            isa_list.append(isa)\n+    return isa_list\n \n \n def pick_vec_isa() -> VecISA:\n@@ -1569,7 +1615,14 @@ def get_include_and_linking_paths(\n     _set_gpu_runtime_env()\n     from torch.utils import cpp_extension\n \n-    macros = vec_isa.build_macro() if vec_isa != invalid_vec_isa else \"\"\n+    # Remove below in the further\n+    # macros = \"-D {}\".format(vec_isa.build_macro()) if vec_isa != invalid_vec_isa else \"\"\n+    macros = \"\"\n+    if vec_isa != invalid_vec_isa:\n+        for x in vec_isa.build_macro():\n+            macros_def = f\"-D{x} \"\n+            macros += macros_def\n+\n     build_arch_flags = \"\"\n     if sys.platform == \"linux\" and (\n         include_pytorch\n@@ -1789,7 +1842,7 @@ def cpp_compile_command(\n             {get_warning_all_flag(warning_all)} {cpp_flags()}\n             {get_glibcxx_abi_build_flags()}\n             {ipaths_str} {lpaths} {libs} {build_arch_flags}\n-            {macros} {linker_paths} {clang_flags}\n+            {macros} {linker_paths} {clang_flags} {cpp_wrapper_flags()}\n             {optimization_flags()}\n             {use_custom_generated_macros()}\n             {use_fb_internal_macros()}\n@@ -2041,7 +2094,6 @@ class AotCodeCompiler:\n             def _to_bytes(t: torch.Tensor) -> bytes:\n                 # This serializes the tensor's untyped_storage to bytes by accessing\n                 # the raw data of the underlying structure.\n-                import ctypes\n \n                 if t.numel() == 0:\n                     return b\"\"\n@@ -2265,8 +2317,18 @@ class CppCodeCache:\n             \"cuda\": cuda,\n             \"vec_isa\": pick_vec_isa(),\n         }\n-        cpp_command = repr(cpp_compile_command(\"i\", \"o\", **compile_command))\n-        key, input_path = write(source_code, \"cpp\", extra=cpp_command)\n+\n+        from torch._inductor.cpp_builder import CppBuilder, CppTorchOptions\n+\n+        picked_vec_isa = pick_vec_isa()\n+        dummy_builder = CppBuilder(\"i\", [\"o\"], CppTorchOptions(picked_vec_isa))\n+        # write function will calc source_code hash, the same source code with different\n+        # ISA level should be generate different hash.\n+        # So we need get a command_line which contains isa related parameter as a part of hash key.\n+        # And then pass the command_line to below write function as extra parameter to\n+        # guarantee the source code hash contains ISA difference.\n+        dummy_cmd = dummy_builder.get_command_line()\n+        key, input_path = write(source_code, \"cpp\", extra=dummy_cmd)\n \n         if key not in cls.cache:\n             from filelock import FileLock\n@@ -2535,7 +2597,85 @@ class CppWrapperCodeCache(CppPythonBindingsCodeCache):\n     )\n \n \n-@clear_on_fresh_inductor_cache\n+# TODO: Will remove the temp code after switch to new cpp_builder\n+def _temp_validate_new_and_old_command(new_cmd: List[str], old_cmd: List[str]):\n+    new_diff: List[str] = [x for x in new_cmd if x not in old_cmd]\n+    old_diff: List[str] = [y for y in old_cmd if y not in new_cmd]\n+\n+    if new_diff or old_diff:\n+        print(\"!!! new_cmd: \", new_cmd)\n+        print(\"!!! old_cmd: \", old_cmd)\n+        print(\"!!! new_diff: \", new_diff)\n+        print(\"!!! old_diff: \", old_diff)\n+        raise RuntimeError(\"Error in new and old command different.\")\n+\n+\n+def _do_validate_cpp_commands(\n+    include_pytorch: bool, cuda: bool, compile_only: bool, mmap_weights: bool\n+):\n+    # PreCI will failed if test machine can't run cuda.\n+    test_cuda = torch.cuda.is_available() and cuda\n+    input_path = \"/temp/dummy_input.cpp\"\n+    output_path = \"/temp/dummy_output.so\"\n+    if compile_only:\n+        output_path = \"/temp/dummy_output.o\"\n+    picked_isa = pick_vec_isa()\n+\n+    old_cmd = cpp_compile_command(\n+        input=input_path,\n+        output=output_path,\n+        include_pytorch=include_pytorch,\n+        vec_isa=picked_isa,\n+        cuda=test_cuda,\n+        aot_mode=False,\n+        compile_only=compile_only,\n+        use_absolute_path=False,\n+        use_mmap_weights=mmap_weights,\n+    ).split(\" \")\n+\n+    from torch._inductor.cpp_builder import CppBuilder, CppTorchCudaOptions\n+\n+    dummy_build_option = CppTorchCudaOptions(\n+        chosen_isa=picked_isa,\n+        include_pytorch=include_pytorch,\n+        use_cuda=test_cuda,\n+        compile_only=compile_only,\n+        use_mmap_weights=mmap_weights,\n+    )\n+\n+    dummy_builder = CppBuilder(\n+        name=\"dummy_output\",\n+        sources=input_path,\n+        BuildOption=dummy_build_option,\n+        output_dir=\"/temp/\",\n+        compile_only=compile_only,\n+        use_absolute_path=False,\n+    )\n+    new_cmd = dummy_builder.get_command_line().split(\" \")\n+\n+    _temp_validate_new_and_old_command(new_cmd, old_cmd)\n+\n+\n+# TODO: Will remove the temp code after switch to new cpp_builder\n+# It could help on sync new cpp_builder generate same command line as the old one.\n+def validate_new_cpp_commands():\n+    cuda = [True, False]\n+    use_mmap_weights = [True, False]\n+    compile_only = [True, False]\n+    include_pytorch = [True, False]\n+\n+    for x in cuda:\n+        for y in use_mmap_weights:\n+            for z in compile_only:\n+                for m in include_pytorch:\n+                    print(\n+                        f\"!!! cuda:{x}, use_mmap_weights:{y}, compile_only:{z}, include_pytorch:{m}\"\n+                    )\n+                    _do_validate_cpp_commands(\n+                        include_pytorch=m, cuda=x, mmap_weights=y, compile_only=z\n+                    )\n+\n+\n class PyCodeCache:\n     cache: Dict[str, ModuleType] = dict()\n     linemaps: Dict[str, List[Tuple[Any, ...]]] = dict()\n"
        },
        {
            "name": "cpp_builder.py",
            "path": "torch/_inductor/cpp_builder.py",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 1139,
                    "hunk": "@@ -0,0 +1,1139 @@\n+# This CPP JIT builder is designed to support both Windows and Linux OS.\n+# The design document please check this RFC: https://github.com/pytorch/pytorch/issues/124245\n+\n+import copy\n+import errno\n+import functools\n+import logging\n+import os\n+import platform\n+import re\n+import shlex\n+import shutil\n+import subprocess\n+import sys\n+import sysconfig\n+import warnings\n+from pathlib import Path\n+from typing import List, Tuple, Union\n+\n+import torch\n+from torch._inductor import config, exc\n+from torch._inductor.codecache import (\n+    get_lock_dir,\n+    invalid_vec_isa,\n+    LOCK_TIMEOUT,\n+    VecISA,\n+)\n+from torch._inductor.runtime.runtime_utils import cache_dir\n+\n+if config.is_fbcode():\n+    from triton.fb import build_paths  # noqa: F401\n+\n+    from torch._inductor.fb.utils import (\n+        log_global_cache_errors,\n+        log_global_cache_stats,\n+        log_global_cache_vals,\n+        use_global_cache,\n+    )\n+else:\n+\n+    def log_global_cache_errors(*args, **kwargs):\n+        pass\n+\n+    def log_global_cache_stats(*args, **kwargs):\n+        pass\n+\n+    def log_global_cache_vals(*args, **kwargs):\n+        pass\n+\n+    def use_global_cache() -> bool:\n+        return False\n+\n+\n+# Windows need setup a temp dir to store .obj files.\n+_BUILD_TEMP_DIR = \"CxxBuild\"\n+\n+# initialize variables for compilation\n+_IS_LINUX = sys.platform.startswith(\"linux\")\n+_IS_MACOS = sys.platform.startswith(\"darwin\")\n+_IS_WINDOWS = sys.platform == \"win32\"\n+\n+\n+log = logging.getLogger(__name__)\n+\n+\n+@functools.lru_cache(1)\n+def cpp_compiler_search(search: str) -> str:\n+    for cxx in search:\n+        try:\n+            if cxx is None:\n+                # gxx package is only available for Linux\n+                # according to https://anaconda.org/conda-forge/gxx/\n+                if sys.platform != \"linux\":\n+                    continue\n+                # Do not install GXX by default\n+                if not os.getenv(\"TORCH_INDUCTOR_INSTALL_GXX\"):\n+                    continue\n+                from filelock import FileLock\n+\n+                lock_dir = get_lock_dir()\n+                lock = FileLock(\n+                    os.path.join(lock_dir, \"g++.lock\"), timeout=LOCK_TIMEOUT\n+                )\n+                with lock:\n+                    cxx = install_gcc_via_conda()\n+            subprocess.check_output([cxx, \"--version\"])\n+            return cxx\n+        except (subprocess.SubprocessError, FileNotFoundError, ImportError):\n+            continue\n+    raise exc.InvalidCxxCompiler()  # noqa: RSE102\n+\n+\n+def install_gcc_via_conda() -> str:\n+    \"\"\"On older systems, this is a quick way to get a modern compiler\"\"\"\n+    prefix = os.path.join(cache_dir(), \"gcc\")\n+    cxx_path = os.path.join(prefix, \"bin\", \"g++\")\n+    if not os.path.exists(cxx_path):\n+        log.info(\"Downloading GCC via conda\")\n+        conda = os.environ.get(\"CONDA_EXE\", \"conda\")\n+        if conda is None:\n+            conda = shutil.which(\"conda\")\n+        if conda is not None:\n+            subprocess.check_call(\n+                [\n+                    conda,\n+                    \"create\",\n+                    f\"--prefix={prefix}\",\n+                    \"--channel=conda-forge\",\n+                    \"--quiet\",\n+                    \"-y\",\n+                    \"python=3.8\",\n+                    \"gxx\",\n+                ],\n+                stdout=subprocess.PIPE,\n+            )\n+    return cxx_path\n+\n+\n+def _get_cpp_compiler() -> str:\n+    if _IS_WINDOWS:\n+        compiler = os.environ.get(\"CXX\", \"cl\")\n+    else:\n+        if config.is_fbcode():\n+            return build_paths.cc()\n+        if isinstance(config.cpp.cxx, (list, tuple)):\n+            search = tuple(config.cpp.cxx)\n+        else:\n+            search = (config.cpp.cxx,)\n+        compiler = cpp_compiler_search(search)\n+    return compiler\n+\n+\n+def _is_gcc(cpp_compiler) -> bool:\n+    return bool(re.search(r\"(gcc|g\\+\\+)\", cpp_compiler))\n+\n+\n+def is_gcc() -> bool:\n+    return _is_gcc(_get_cpp_compiler())\n+\n+\n+def _is_clang(cpp_compiler) -> bool:\n+    # Mac OS apple clang maybe named as gcc, need check compiler info.\n+    if sys.platform == \"darwin\":\n+        return is_apple_clang(cpp_compiler)\n+    return bool(re.search(r\"(clang|clang\\+\\+)\", cpp_compiler))\n+\n+\n+def is_clang() -> bool:\n+    compiler = _get_cpp_compiler()\n+    return _is_clang(compiler)\n+\n+\n+@functools.lru_cache(None)\n+def is_apple_clang(cpp_compiler) -> bool:\n+    version_string = subprocess.check_output([cpp_compiler, \"--version\"]).decode(\"utf8\")\n+    return \"Apple\" in version_string.splitlines()[0]\n+\n+\n+def _append_list(dest_list: List[str], src_list: List[str]):\n+    for item in src_list:\n+        dest_list.append(copy.deepcopy(item))\n+\n+\n+def _remove_duplication_in_list(orig_list: List[str]) -> List[str]:\n+    new_list: List[str] = []\n+    for item in orig_list:\n+        if item not in new_list:\n+            new_list.append(item)\n+    return new_list\n+\n+\n+def _create_if_dir_not_exist(path_dir):\n+    if not os.path.exists(path_dir):\n+        try:\n+            Path(path_dir).mkdir(parents=True, exist_ok=True)\n+        except OSError as exc:  # Guard against race condition\n+            if exc.errno != errno.EEXIST:\n+                raise RuntimeError(  # noqa: TRY200 (Use `raise from`)\n+                    f\"Fail to create path {path_dir}\"\n+                )\n+\n+\n+def _remove_dir(path_dir):\n+    if os.path.exists(path_dir):\n+        for root, dirs, files in os.walk(path_dir, topdown=False):\n+            for name in files:\n+                file_path = os.path.join(root, name)\n+                os.remove(file_path)\n+            for name in dirs:\n+                dir_path = os.path.join(root, name)\n+                os.rmdir(dir_path)\n+        os.rmdir(path_dir)\n+\n+\n+def run_command_line(cmd_line, cwd=None):\n+    cmd = shlex.split(cmd_line)\n+    try:\n+        status = subprocess.check_output(args=cmd, cwd=cwd, stderr=subprocess.STDOUT)\n+    except subprocess.CalledProcessError as e:\n+        output = e.output.decode(\"utf-8\")\n+        openmp_problem = \"'omp.h' file not found\" in output or \"libomp\" in output\n+        if openmp_problem and sys.platform == \"darwin\":\n+            instruction = (\n+                \"\\n\\nOpenMP support not found. Please try one of the following solutions:\\n\"\n+                \"(1) Set the `CXX` environment variable to a compiler other than Apple clang++/g++ \"\n+                \"that has builtin OpenMP support;\\n\"\n+                \"(2) install OpenMP via conda: `conda install llvm-openmp`;\\n\"\n+                \"(3) install libomp via brew: `brew install libomp`;\\n\"\n+                \"(4) manually setup OpenMP and set the `OMP_PREFIX` environment variable to point to a path\"\n+                \" with `include/omp.h` under it.\"\n+            )\n+            output += instruction\n+        raise exc.CppCompileError(cmd, output) from e\n+    return status\n+\n+\n+class BuildOptionsBase:\n+    \"\"\"\n+    This is the Base class for store cxx build options, as a template.\n+    Acturally, to build a cxx shared library. We just need to select a compiler\n+    and maintains the suitable args.\n+    \"\"\"\n+\n+    def __init__(self) -> None:\n+        self._compiler = \"\"\n+        self._definations: List[str] = []\n+        self._include_dirs: List[str] = []\n+        self._cflags: List[str] = []\n+        self._ldflags: List[str] = []\n+        self._libraries_dirs: List[str] = []\n+        self._libraries: List[str] = []\n+        # Some args is hard to abstract to OS compatable, passthough it directly.\n+        self._passthough_args: List[str] = []\n+\n+        self._aot_mode = False\n+\n+    def _remove_duplicate_options(self):\n+        self._definations = _remove_duplication_in_list(self._definations)\n+        self._include_dirs = _remove_duplication_in_list(self._include_dirs)\n+        self._cflags = _remove_duplication_in_list(self._cflags)\n+        self._ldflags = _remove_duplication_in_list(self._ldflags)\n+        self._libraries_dirs = _remove_duplication_in_list(self._libraries_dirs)\n+        self._libraries = _remove_duplication_in_list(self._libraries)\n+        self._passthough_args = _remove_duplication_in_list(self._passthough_args)\n+\n+    def get_compiler(self) -> str:\n+        return self._compiler\n+\n+    def get_definations(self) -> List[str]:\n+        return self._definations\n+\n+    def get_include_dirs(self) -> List[str]:\n+        return self._include_dirs\n+\n+    def get_cflags(self) -> List[str]:\n+        return self._cflags\n+\n+    def get_ldflags(self) -> List[str]:\n+        return self._ldflags\n+\n+    def get_libraries_dirs(self) -> List[str]:\n+        return self._libraries_dirs\n+\n+    def get_libraries(self) -> List[str]:\n+        return self._libraries\n+\n+    def get_passthough_args(self) -> List[str]:\n+        return self._passthough_args\n+\n+    def get_aot_mode(self) -> bool:\n+        return self._aot_mode\n+\n+\n+def _get_warning_all_cflag(warning_all: bool = True) -> List[str]:\n+    if not _IS_WINDOWS:\n+        return [\"Wall\"] if warning_all else []\n+    else:\n+        return []\n+\n+\n+def _get_cpp_std_cflag(std_num: str = \"c++17\") -> List[str]:\n+    if _IS_WINDOWS:\n+        return [f\"std:{std_num}\"]\n+    else:\n+        return [f\"std={std_num}\"]\n+\n+\n+def _get_linux_cpp_cflags(cpp_compiler) -> List[str]:\n+    if not _IS_WINDOWS:\n+        cflags = [\"Wno-unused-variable\", \"Wno-unknown-pragmas\"]\n+        if _is_clang(cpp_compiler):\n+            cflags.append(\"Werror=ignored-optimization-argument\")\n+        return cflags\n+    else:\n+        return []\n+\n+\n+def _get_optimization_cflags() -> List[str]:\n+    if _IS_WINDOWS:\n+        return [\"O2\"]\n+    else:\n+        cflags = [\"O0\", \"g\"] if config.aot_inductor.debug_compile else [\"O3\", \"DNDEBUG\"]\n+        cflags.append(\"ffast-math\")\n+        cflags.append(\"fno-finite-math-only\")\n+\n+        if not config.cpp.enable_unsafe_math_opt_flag:\n+            cflags.append(\"fno-unsafe-math-optimizations\")\n+        if not config.cpp.enable_floating_point_contract_flag:\n+            cflags.append(\"ffp-contract=off\")\n+\n+        if config.is_fbcode():\n+            # FIXME: passing `-fopenmp` adds libgomp.so to the generated shared library's dependencies.\n+            # This causes `ldopen` to fail in fbcode, because libgomp does not exist in the default paths.\n+            # We will fix it later by exposing the lib path.\n+            return cflags\n+\n+        if sys.platform == \"darwin\":\n+            # Per https://mac.r-project.org/openmp/ right way to pass `openmp` flags to MacOS is via `-Xclang`\n+            # Also, `-march=native` is unrecognized option on M1\n+            cflags.append(\"Xclang\")\n+        else:\n+            if platform.machine() == \"ppc64le\":\n+                cflags.append(\"mcpu=native\")\n+            else:\n+                cflags.append(\"march=native\")\n+\n+        # Internal cannot find libgomp.so\n+        if not config.is_fbcode():\n+            cflags.append(\"fopenmp\")\n+\n+        return cflags\n+\n+\n+def _get_shared_cflag(compile_only: bool) -> List[str]:\n+    if _IS_WINDOWS:\n+        SHARED_FLAG = [\"DLL\"]\n+    else:\n+        if compile_only:\n+            return [\"fPIC\"]\n+        if platform.system() == \"Darwin\" and \"clang\" in _get_cpp_compiler():\n+            # This causes undefined symbols to behave the same as linux\n+            return [\"shared\", \"fPIC\", \"undefined dynamic_lookup\"]\n+        else:\n+            return [\"shared\", \"fPIC\"]\n+\n+    return SHARED_FLAG\n+\n+\n+def get_cpp_options(cpp_compiler, compile_only: bool, warning_all: bool = True):\n+    definations: List[str] = []\n+    include_dirs: List[str] = []\n+    cflags: List[str] = []\n+    ldflags: List[str] = []\n+    libraries_dirs: List[str] = []\n+    libraries: List[str] = []\n+    passthough_args: List[str] = []\n+\n+    cflags = (\n+        _get_shared_cflag(compile_only)\n+        + _get_optimization_cflags()\n+        + _get_warning_all_cflag(warning_all)\n+        + _get_cpp_std_cflag()\n+        + _get_linux_cpp_cflags(cpp_compiler)\n+    )\n+\n+    return (\n+        definations,\n+        include_dirs,\n+        cflags,\n+        ldflags,\n+        libraries_dirs,\n+        libraries,\n+        passthough_args,\n+    )\n+\n+\n+class CppOptions(BuildOptionsBase):\n+    \"\"\"\n+    This class is inherited from BuildOptionsBase, and as cxx build options.\n+    This option need contains basic cxx build option, which contains:\n+    1. OS related args.\n+    2. Toolchains related args.\n+    3. Cxx standard related args.\n+    Note:\n+    1. This Options is good for assist modules build, such as x86_isa_help.\n+    \"\"\"\n+\n+    def __init__(self, compile_only: bool, warning_all: bool = True) -> None:\n+        super().__init__()\n+        self._compiler = _get_cpp_compiler()\n+\n+        (\n+            definations,\n+            include_dirs,\n+            cflags,\n+            ldflags,\n+            libraries_dirs,\n+            libraries,\n+            passthough_args,\n+        ) = get_cpp_options(cpp_compiler=self._compiler, compile_only=compile_only)\n+\n+        _append_list(self._definations, definations)\n+        _append_list(self._include_dirs, include_dirs)\n+        _append_list(self._cflags, cflags)\n+        _append_list(self._ldflags, ldflags)\n+        _append_list(self._libraries_dirs, libraries_dirs)\n+        _append_list(self._libraries, libraries)\n+        _append_list(self._passthough_args, passthough_args)\n+        self._remove_duplicate_options()\n+\n+\n+def _get_glibcxx_abi_build_flags() -> List[str]:\n+    if not _IS_WINDOWS:\n+        return [\"-D_GLIBCXX_USE_CXX11_ABI=\" + str(int(torch._C._GLIBCXX_USE_CXX11_ABI))]\n+    else:\n+        return []\n+\n+\n+def _get_torch_cpp_wrapper_defination() -> List[str]:\n+    return [\"TORCH_INDUCTOR_CPP_WRAPPER\"]\n+\n+\n+def _use_custom_generated_macros() -> List[str]:\n+    return [\" C10_USING_CUSTOM_GENERATED_MACROS\"]\n+\n+\n+def _use_fb_internal_macros() -> List[str]:\n+    if not _IS_WINDOWS:\n+        if config.is_fbcode():\n+            # openmp_lib = build_paths.openmp_lib()\n+            preprocessor_flags = \" \".join(\n+                (\n+                    \"-D C10_USE_GLOG\",\n+                    \"-D C10_USE_MINIMAL_GLOG\",\n+                    \"-D C10_DISABLE_TENSORIMPL_EXTENSIBILITY\",\n+                )\n+            )\n+            # return [f\"-Wp,-fopenmp {openmp_lib} {preprocessor_flags}\"]\n+            return [f\"{preprocessor_flags}\"]\n+        else:\n+            return []\n+    else:\n+        return []\n+\n+\n+def _use_standard_sys_dir_headers():\n+    cflags: List[str] = []\n+    include_dirs: List[str] = []\n+    if _IS_WINDOWS:\n+        return cflags, include_dirs\n+\n+    if config.is_fbcode():\n+        cflags.append(\"nostdinc\")\n+        include_dirs.append(build_paths.sleef())\n+        include_dirs.append(build_paths.cc_include())\n+        include_dirs.append(build_paths.libgcc())\n+        include_dirs.append(build_paths.libgcc_arch())\n+        include_dirs.append(build_paths.libgcc_backward())\n+        include_dirs.append(build_paths.glibc())\n+        include_dirs.append(build_paths.linux_kernel())\n+        include_dirs.append(\"include\")\n+\n+    return cflags, include_dirs\n+\n+\n+@functools.lru_cache\n+def _cpp_prefix_path() -> str:\n+    from torch._inductor.codecache import write  # TODO\n+\n+    path = Path(Path(__file__).parent).parent / \"codegen/cpp_prefix.h\"\n+    with path.open() as f:\n+        content = f.read()\n+        _, filename = write(\n+            content,\n+            \"h\",\n+        )\n+    return filename\n+\n+\n+def _get_build_args_of_chosen_isa(chosen_isa: VecISA):\n+    macros = []\n+    build_flags = []\n+    if chosen_isa != invalid_vec_isa:\n+        # Add Windows support later.\n+        for x in chosen_isa.build_macro():\n+            macros.append(copy.deepcopy(x))\n+\n+        build_flags = [chosen_isa.build_arch_flags()]\n+\n+    if config.is_fbcode() and chosen_isa != invalid_vec_isa:\n+        cap = str(chosen_isa).upper()\n+        macros = [\n+            f\"CPU_CAPABILITY={cap}\",\n+            f\"CPU_CAPABILITY_{cap}\",\n+            f\"HAVE_{cap}_CPU_DEFINITION\",\n+        ]\n+\n+    return macros, build_flags\n+\n+\n+def _get_torch_related_args(include_pytorch: bool, aot_mode: bool):\n+    from torch.utils.cpp_extension import _TORCH_PATH, TORCH_LIB_PATH\n+\n+    include_dirs = [\n+        os.path.join(_TORCH_PATH, \"include\"),\n+        os.path.join(_TORCH_PATH, \"include\", \"torch\", \"csrc\", \"api\", \"include\"),\n+        # Some internal (old) Torch headers don't properly prefix their includes,\n+        # so we need to pass -Itorch/lib/include/TH as well.\n+        os.path.join(_TORCH_PATH, \"include\", \"TH\"),\n+        os.path.join(_TORCH_PATH, \"include\", \"THC\"),\n+    ]\n+    libraries_dirs = [TORCH_LIB_PATH]\n+    libraries = []\n+    if sys.platform != \"darwin\":\n+        libraries = [\"torch\", \"torch_cpu\"]\n+        if not aot_mode:\n+            libraries.append(\"torch_python\")\n+\n+    # Unconditionally import c10 for non-abi-compatible mode to use TORCH_CHECK - See PyTorch #108690\n+    if not config.abi_compatible:\n+        libraries.append(\"c10\")\n+        libraries_dirs.append(TORCH_LIB_PATH)\n+\n+    return include_dirs, libraries_dirs, libraries\n+\n+\n+def _get_python_related_args():\n+    python_include_dirs = []\n+    python_include_path = sysconfig.get_path(\n+        \"include\", scheme=\"nt\" if _IS_WINDOWS else \"posix_prefix\"\n+    )\n+    if python_include_path is not None:\n+        python_include_dirs.append(python_include_path)\n+\n+    if _IS_WINDOWS:\n+        python_path = os.path.dirname(sys.executable)\n+        python_lib_path = [os.path.join(python_path, \"libs\")]\n+    else:\n+        python_lib_path = [sysconfig.get_config_var(\"LIBDIR\")]\n+\n+    return python_include_dirs, python_lib_path\n+\n+\n+def _get_openmp_args(cpp_compiler):\n+    cflags: List[str] = []\n+    ldflags: List[str] = []\n+    include_dir_paths: List[str] = []\n+    lib_dir_paths: List[str] = []\n+    libs: List[str] = []\n+    if _IS_MACOS:\n+        from torch._inductor.codecache import (\n+            homebrew_libomp,\n+            is_conda_llvm_openmp_installed,\n+        )\n+\n+        # only Apple builtin compilers (Apple Clang++) require openmp\n+        omp_available = not is_apple_clang(cpp_compiler)\n+\n+        # check the `OMP_PREFIX` environment first\n+        omp_prefix = os.getenv(\"OMP_PREFIX\")\n+        if omp_prefix is not None:\n+            header_path = os.path.join(omp_prefix, \"include\", \"omp.h\")\n+            valid_env = os.path.exists(header_path)\n+            if valid_env:\n+                include_dir_paths.append(os.path.join(omp_prefix, \"include\"))\n+                lib_dir_paths.append(os.path.join(omp_prefix, \"lib\"))\n+            else:\n+                warnings.warn(\"environment variable `OMP_PREFIX` is invalid.\")\n+            omp_available = omp_available or valid_env\n+\n+        if not omp_available:\n+            libs.append(\"omp\")\n+\n+        # prefer to use openmp from `conda install llvm-openmp`\n+        conda_prefix = os.getenv(\"CONDA_PREFIX\")\n+        if not omp_available and conda_prefix is not None:\n+            omp_available = is_conda_llvm_openmp_installed()\n+            if omp_available:\n+                conda_lib_path = os.path.join(conda_prefix, \"lib\")\n+                include_dir_paths.append(os.path.join(conda_prefix, \"include\"))\n+                lib_dir_paths.append(conda_lib_path)\n+                # Prefer Intel OpenMP on x86 machine\n+                if os.uname().machine == \"x86_64\" and os.path.exists(\n+                    os.path.join(conda_lib_path, \"libiomp5.dylib\")\n+                ):\n+                    libs.append(\"iomp5\")\n+\n+        # next, try to use openmp from `brew install libomp`\n+        if not omp_available:\n+            omp_available, libomp_path = homebrew_libomp()\n+            if omp_available:\n+                include_dir_paths.append(os.path.join(libomp_path, \"include\"))\n+                lib_dir_paths.append(os.path.join(libomp_path, \"lib\"))\n+\n+        # if openmp is still not available, we let the compiler to have a try,\n+        # and raise error together with instructions at compilation error later\n+    elif _IS_WINDOWS:\n+        # /openmp, /openmp:llvm\n+        # llvm on Windows, new openmp: https://devblogs.microsoft.com/cppblog/msvc-openmp-update/\n+        # msvc openmp: https://learn.microsoft.com/zh-cn/cpp/build/reference/openmp-enable-openmp-2-0-support?view=msvc-170\n+\n+        cflags.append(\"openmp\")\n+        libs = []\n+    else:\n+        if config.is_fbcode():\n+            libs.append(\"omp\")\n+            include_dir_paths.append(build_paths.openmp())\n+        else:\n+            if _is_clang(cpp_compiler):\n+                # TODO: fix issue, can't find omp.h\n+                cflags.append(\"fopenmp\")\n+                libs.append(\"gomp\")\n+            else:\n+                cflags.append(\"fopenmp\")\n+                libs.append(\"gomp\")\n+\n+    return cflags, ldflags, include_dir_paths, lib_dir_paths, libs\n+\n+\n+def get_mmap_self_macro(use_mmap_weights: bool) -> List[str]:\n+    macros = []\n+    if use_mmap_weights:\n+        macros.append(\" USE_MMAP_SELF\")\n+    return macros\n+\n+\n+def get_cpp_torch_options(\n+    cpp_compiler,\n+    chosen_isa: VecISA,\n+    include_pytorch: bool,\n+    aot_mode: bool,\n+    compile_only: bool,\n+    use_mmap_weights: bool,\n+):\n+    definations: List[str] = []\n+    include_dirs: List[str] = []\n+    cflags: List[str] = []\n+    ldflags: List[str] = []\n+    libraries_dirs: List[str] = []\n+    libraries: List[str] = []\n+    passthough_args: List[str] = []\n+\n+    torch_cpp_wrapper_definations = _get_torch_cpp_wrapper_defination()\n+    use_custom_generated_macros_definations = _use_custom_generated_macros()\n+\n+    sys_dir_header_cflags, sys_dir_header_include_dirs = _use_standard_sys_dir_headers()\n+\n+    isa_macros, isa_ps_args_build_flags = _get_build_args_of_chosen_isa(chosen_isa)\n+\n+    (\n+        torch_include_dirs,\n+        torch_libraries_dirs,\n+        torch_libraries,\n+    ) = _get_torch_related_args(include_pytorch=include_pytorch, aot_mode=aot_mode)\n+\n+    python_include_dirs, python_libraries_dirs = _get_python_related_args()\n+\n+    (\n+        omp_cflags,\n+        omp_ldflags,\n+        omp_include_dir_paths,\n+        omp_lib_dir_paths,\n+        omp_lib,\n+    ) = _get_openmp_args(cpp_compiler)\n+\n+    cxx_abi_passthough_args = _get_glibcxx_abi_build_flags()\n+    fb_macro_passthough_args = _use_fb_internal_macros()\n+\n+    mmap_self_macros = get_mmap_self_macro(use_mmap_weights)\n+\n+    definations = (\n+        torch_cpp_wrapper_definations\n+        + use_custom_generated_macros_definations\n+        + isa_macros\n+        + fb_macro_passthough_args\n+        + mmap_self_macros\n+    )\n+    include_dirs = (\n+        sys_dir_header_include_dirs\n+        + python_include_dirs\n+        + torch_include_dirs\n+        + omp_include_dir_paths\n+    )\n+    cflags = sys_dir_header_cflags + omp_cflags\n+    ldflags = omp_ldflags\n+    libraries_dirs = python_libraries_dirs + torch_libraries_dirs + omp_lib_dir_paths\n+    libraries = torch_libraries + omp_lib\n+    passthough_args = isa_ps_args_build_flags + cxx_abi_passthough_args\n+\n+    return (\n+        definations,\n+        include_dirs,\n+        cflags,\n+        ldflags,\n+        libraries_dirs,\n+        libraries,\n+        passthough_args,\n+    )\n+\n+\n+class CppTorchOptions(CppOptions):\n+    \"\"\"\n+    This class is inherited from CppTorchOptions, which automatic contains\n+    base cxx build options. And then it will maintains torch related build\n+    args.\n+    1. Torch include_directories, libraries, libraries_directories.\n+    2. Python include_directories, libraries, libraries_directories.\n+    3. OpenMP related.\n+    4. Torch MACROs.\n+    5. MISC\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        chosen_isa: VecISA,\n+        include_pytorch: bool = False,\n+        warning_all: bool = True,\n+        aot_mode: bool = False,\n+        compile_only: bool = False,\n+        use_mmap_weights: bool = False,\n+    ) -> None:\n+        super().__init__(compile_only=compile_only, warning_all=warning_all)\n+\n+        self._aot_mode = aot_mode\n+\n+        (\n+            torch_definations,\n+            torch_include_dirs,\n+            torch_cflags,\n+            torch_ldflags,\n+            torch_libraries_dirs,\n+            torch_libraries,\n+            torch_passthough_args,\n+        ) = get_cpp_torch_options(\n+            cpp_compiler=self._compiler,\n+            chosen_isa=chosen_isa,\n+            include_pytorch=include_pytorch,\n+            aot_mode=aot_mode,\n+            compile_only=compile_only,\n+            use_mmap_weights=use_mmap_weights,\n+        )\n+\n+        if compile_only:\n+            torch_libraries_dirs = []\n+            torch_libraries = []\n+\n+        _append_list(self._definations, torch_definations)\n+        _append_list(self._include_dirs, torch_include_dirs)\n+        _append_list(self._cflags, torch_cflags)\n+        _append_list(self._ldflags, torch_ldflags)\n+        _append_list(self._libraries_dirs, torch_libraries_dirs)\n+        _append_list(self._libraries, torch_libraries)\n+        _append_list(self._passthough_args, torch_passthough_args)\n+        self._remove_duplicate_options()\n+\n+\n+def _get_cuda_related_args(aot_mode: bool):\n+    definations: List[str] = []\n+    include_dirs: List[str] = []\n+    cflags: List[str] = []\n+    ldflags: List[str] = []\n+    libraries_dirs: List[str] = []\n+    libraries: List[str] = []\n+    passthough_args: List[str] = []\n+\n+    # if not use cuda, don't call this function.\n+    use_cuda = True\n+\n+    if (\n+        config.is_fbcode()\n+        and \"CUDA_HOME\" not in os.environ\n+        and \"CUDA_PATH\" not in os.environ\n+    ):\n+        os.environ[\"CUDA_HOME\"] = os.path.dirname(build_paths.cuda())\n+\n+    from torch.utils import cpp_extension\n+\n+    include_dirs = cpp_extension.include_paths(use_cuda)\n+    libraries_dirs = cpp_extension.library_paths(use_cuda)\n+\n+    definations.append(\" USE_ROCM\" if torch.version.hip else \" USE_CUDA\")\n+\n+    if torch.version.hip is not None:\n+        if config.is_fbcode():\n+            libraries += [\"amdhip64\"]\n+        else:\n+            libraries += [\"c10_hip\", \"torch_hip\"]\n+            definations.append(\" __HIP_PLATFORM_AMD__\")\n+    else:\n+        if config.is_fbcode():\n+            libraries += [\"cuda\"]\n+        else:\n+            if config.is_fbcode():\n+                libraries += [\"cuda\"]\n+            else:\n+                libraries += [\"c10_cuda\", \"cuda\", \"torch_cuda\"]\n+\n+    if aot_mode:\n+        cpp_prefix_include_dir = [f\"{os.path.dirname(_cpp_prefix_path())}\"]\n+        include_dirs += cpp_prefix_include_dir\n+\n+        if config.is_fbcode():\n+            include_dirs.append(build_paths.cuda())\n+            # This is a special treatment for Meta internal cuda-12 where all libs\n+            # are in lib/cuda-12 and lib/cuda-12/stubs\n+\n+            # TODO: Verify process libcudart_static.a\n+            for i, path in enumerate(libraries_dirs):\n+                if path.startswith(os.environ[\"CUDA_HOME\"]) and not os.path.exists(\n+                    f\"{path}/libcudart_static.a\"\n+                ):\n+                    for root, dirs, files in os.walk(path):\n+                        if \"libcudart_static.a\" in files:\n+                            libraries_dirs[i] = os.path.join(path, root)\n+                            libraries_dirs.append(\n+                                os.path.join(libraries_dirs[i], \"stubs\")\n+                            )\n+                            break\n+        else:\n+            if not _IS_WINDOWS:\n+                # TODO: make static link better on Linux.\n+                passthough_args = [\"-Wl,-Bstatic -lcudart_static -Wl,-Bdynamic\"]\n+            else:\n+                libraries.append(\"cudart_static\")\n+\n+    return (\n+        definations,\n+        include_dirs,\n+        cflags,\n+        ldflags,\n+        libraries_dirs,\n+        libraries,\n+        passthough_args,\n+    )\n+\n+\n+def get_cpp_torch_cuda_options(aot_mode: bool = False):\n+    definations: List[str] = []\n+    include_dirs: List[str] = []\n+    cflags: List[str] = []\n+    ldflags: List[str] = []\n+    libraries_dirs: List[str] = []\n+    libraries: List[str] = []\n+    passthough_args: List[str] = []\n+\n+    (\n+        definations,\n+        include_dirs,\n+        cflags,\n+        ldflags,\n+        libraries_dirs,\n+        libraries,\n+        passthough_args,\n+    ) = _get_cuda_related_args(aot_mode)\n+\n+    return (\n+        definations,\n+        include_dirs,\n+        cflags,\n+        ldflags,\n+        libraries_dirs,\n+        libraries,\n+        passthough_args,\n+    )\n+\n+\n+class CppTorchCudaOptions(CppTorchOptions):\n+    \"\"\"\n+    This class is inherited from CppTorchOptions, which automatic contains\n+    base cxx build options and torch common build options. And then it will\n+    maintains cuda device related build args.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        chosen_isa: VecISA,\n+        include_pytorch: bool = False,\n+        use_cuda: bool = True,\n+        aot_mode: bool = False,\n+        compile_only: bool = False,\n+        use_mmap_weights: bool = False,\n+    ) -> None:\n+        # from torch._inductor.codecache import pick_vec_isa\n+\n+        super().__init__(\n+            chosen_isa=chosen_isa,\n+            include_pytorch=include_pytorch,\n+            aot_mode=aot_mode,\n+            compile_only=compile_only,\n+            use_mmap_weights=use_mmap_weights,\n+        )\n+\n+        cuda_definations: List[str] = []\n+        cuda_include_dirs: List[str] = []\n+        cuda_cflags: List[str] = []\n+        cuda_ldflags: List[str] = []\n+        cuda_libraries_dirs: List[str] = []\n+        cuda_libraries: List[str] = []\n+        cuda_passthough_args: List[str] = []\n+\n+        if use_cuda:\n+            (\n+                cuda_definations,\n+                cuda_include_dirs,\n+                cuda_cflags,\n+                cuda_ldflags,\n+                cuda_libraries_dirs,\n+                cuda_libraries,\n+                cuda_passthough_args,\n+            ) = get_cpp_torch_cuda_options(aot_mode=aot_mode)\n+\n+        if compile_only:\n+            cuda_libraries_dirs = []\n+            cuda_libraries = []\n+\n+        _append_list(self._definations, cuda_definations)\n+        _append_list(self._include_dirs, cuda_include_dirs)\n+        _append_list(self._cflags, cuda_cflags)\n+        _append_list(self._ldflags, cuda_ldflags)\n+        _append_list(self._libraries_dirs, cuda_libraries_dirs)\n+        _append_list(self._libraries, cuda_libraries)\n+        _append_list(self._passthough_args, cuda_passthough_args)\n+        self._remove_duplicate_options()\n+\n+\n+def get_name_and_dir_from_output_file_path(\n+    aot_mode: bool, use_absolute_path: bool, file_path: str\n+):\n+    name_and_ext = os.path.basename(file_path)\n+    name, ext = os.path.splitext(name_and_ext)\n+    dir = os.path.dirname(file_path)\n+\n+    if config.is_fbcode():\n+        if not (aot_mode and not use_absolute_path):\n+            dir = \".\"\n+    return name, dir\n+\n+\n+class CppBuilder:\n+    \"\"\"\n+    CppBuilder is a cpp jit builder, and it supports both Windows, Linux and MacOS.\n+    Args:\n+        name:\n+            1. Build target name, the final target file will append extension type automatically.\n+            2. Due to the CppBuilder is supports mutliple OS, it will maintains ext for OS difference.\n+\n+        sources:\n+            Source code file list to be built.\n+\n+        BuildOption:\n+            Build options to the builder.\n+\n+        output_dir:\n+            1. The output_dir the taget file will output to.\n+            2. The default value is empty string, and then the use current dir as output dir.\n+            3. Final target file: output_dir/name.ext\n+    \"\"\"\n+\n+    def get_shared_lib_ext(self) -> str:\n+        SHARED_LIB_EXT = \".dll\" if _IS_WINDOWS else \".so\"\n+        return SHARED_LIB_EXT\n+\n+    def get_object_ext(self) -> str:\n+        EXT = \".obj\" if _IS_WINDOWS else \".o\"\n+        return EXT\n+\n+    def __init__(\n+        self,\n+        name: str,\n+        sources: Union[str, List[str]],\n+        BuildOption: BuildOptionsBase,\n+        output_dir: str = \"\",\n+        compile_only: bool = False,\n+        use_absolute_path: bool = False,\n+    ) -> None:\n+        self._compiler = \"\"\n+        self._cflags_args = \"\"\n+        self._definations_args = \"\"\n+        self._include_dirs_args = \"\"\n+        self._ldflags_args = \"\"\n+        self._libraries_dirs_args = \"\"\n+        self._libraries_args = \"\"\n+        self._passthough_parameters_args = \"\"\n+\n+        self._output_dir = \"\"\n+        self._target_file = \"\"\n+\n+        self._name = name\n+\n+        if isinstance(sources, str):\n+            sources = [sources]\n+\n+        if config.is_fbcode():\n+            if BuildOption.get_aot_mode() and not use_absolute_path:\n+                inp_name = sources\n+                # output process @ get_name_and_dir_from_output_file_path\n+            else:\n+                # We need to copy any absolute-path torch includes\n+                inp_name = [os.path.basename(i) for i in sources]\n+\n+            self._sources_args = \" \".join(inp_name)\n+\n+            if _is_clang(self._compiler):\n+                self._passthough_parameters_args += \" --rtlib=compiler-rt\"\n+                self._passthough_parameters_args += \" -fuse-ld=lld\"\n+                self._passthough_parameters_args += \" -B\" + build_paths.glibc_lib()\n+                self._passthough_parameters_args += \" -L\" + build_paths.glibc_lib()\n+        else:\n+            self._sources_args = \" \".join(sources)\n+\n+        self._compile_only = compile_only\n+\n+        if len(output_dir) == 0:\n+            self._output_dir = os.path.dirname(os.path.abspath(__file__))\n+        else:\n+            self._output_dir = output_dir\n+\n+        file_ext = self.get_object_ext() if compile_only else self.get_shared_lib_ext()\n+        self._target_file = os.path.join(self._output_dir, f\"{self._name}{file_ext}\")\n+\n+        self._compiler = BuildOption.get_compiler()\n+\n+        for cflag in BuildOption.get_cflags():\n+            if _IS_WINDOWS:\n+                self._cflags_args += f\"/{cflag} \"\n+            else:\n+                self._cflags_args += f\"-{cflag} \"\n+\n+        for defination in BuildOption.get_definations():\n+            if _IS_WINDOWS:\n+                self._definations_args += f\"/D {defination} \"\n+            else:\n+                self._definations_args += f\"-D{defination} \"\n+\n+        for inc_dir in BuildOption.get_include_dirs():\n+            if _IS_WINDOWS:\n+                self._include_dirs_args += f\"/I {inc_dir} \"\n+            else:\n+                self._include_dirs_args += f\"-I{inc_dir} \"\n+\n+        for ldflag in BuildOption.get_ldflags():\n+            if _IS_WINDOWS:\n+                self._ldflags_args += f\"/{ldflag} \"\n+            else:\n+                self._ldflags_args += f\"-{ldflag} \"\n+\n+        for lib_dir in BuildOption.get_libraries_dirs():\n+            if _IS_WINDOWS:\n+                self._libraries_dirs_args += f'/LIBPATH:\"{lib_dir}\" '\n+            else:\n+                self._libraries_dirs_args += f\"-L{lib_dir} \"\n+\n+        for lib in BuildOption.get_libraries():\n+            if _IS_WINDOWS:\n+                self._libraries_args += f'\"{lib}.lib\" '\n+            else:\n+                self._libraries_args += f\"-l{lib} \"\n+\n+        for passthough_arg in BuildOption.get_passthough_args():\n+            self._passthough_parameters_args += f\"{passthough_arg} \"\n+\n+    def get_command_line(self) -> str:\n+        def format_build_command(\n+            compiler,\n+            sources,\n+            include_dirs_args,\n+            definations_args,\n+            cflags_args,\n+            ldflags_args,\n+            libraries_args,\n+            libraries_dirs_args,\n+            passthougn_args,\n+            target_file,\n+        ):\n+            if _IS_WINDOWS:\n+                # https://learn.microsoft.com/en-us/cpp/build/walkthrough-compile-a-c-program-on-the-command-line?view=msvc-1704\n+                # https://stackoverflow.com/a/31566153\n+                cmd = (\n+                    f\"{compiler} {include_dirs_args} {definations_args} {cflags_args} {sources} \"\n+                    f\"{passthougn_args} /LD /Fe{target_file} /link {libraries_dirs_args} {libraries_args} {ldflags_args} \"\n+                )\n+                cmd = cmd.replace(\"\\\\\", \"/\")\n+            else:\n+                compile_only_arg = \"-c\" if self._compile_only else \"\"\n+                cmd = re.sub(\n+                    r\"[ \\n]+\",\n+                    \" \",\n+                    f\"\"\"\n+                    {compiler} {sources} {definations_args} {cflags_args} {include_dirs_args}\n+                    {passthougn_args} {ldflags_args} {libraries_args} {libraries_dirs_args} {compile_only_arg} -o {target_file}\n+                    \"\"\",\n+                ).strip()\n+            return cmd\n+\n+        command_line = format_build_command(\n+            compiler=self._compiler,\n+            sources=self._sources_args,\n+            include_dirs_args=self._include_dirs_args,\n+            definations_args=self._definations_args,\n+            cflags_args=self._cflags_args,\n+            ldflags_args=self._ldflags_args,\n+            libraries_args=self._libraries_args,\n+            libraries_dirs_args=self._libraries_dirs_args,\n+            passthougn_args=self._passthough_parameters_args,\n+            target_file=self._target_file,\n+        )\n+        return command_line\n+\n+    def get_target_file_path(self):\n+        return self._target_file\n+\n+    def convert_to_cpp_extension_args(self):\n+        include_dirs = self._include_dirs_args\n+        cflags = (\n+            self._cflags_args\n+            + self._definations_args\n+            + self._passthough_parameters_args\n+        )\n+        ldflags = self._ldflags_args + self._libraries_args + self._libraries_dirs_args\n+\n+        return include_dirs, cflags, ldflags\n+\n+    def build(self) -> Tuple[int, str]:\n+        \"\"\"\n+        It is must need a temperary directory to store object files in Windows.\n+        After build completed, delete the temperary directory to save disk space.\n+        \"\"\"\n+        _create_if_dir_not_exist(self._output_dir)\n+        _build_tmp_dir = os.path.join(\n+            self._output_dir, f\"{self._name}_{_BUILD_TEMP_DIR}\"\n+        )\n+        _create_if_dir_not_exist(_build_tmp_dir)\n+\n+        build_cmd = self.get_command_line()\n+\n+        status = run_command_line(build_cmd, cwd=_build_tmp_dir)\n+\n+        _remove_dir(_build_tmp_dir)\n+        return status, self._target_file\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+# This CPP JIT builder is designed to support both Windows and Linux OS.\n+# The design document please check this RFC: https://github.com/pytorch/pytorch/issues/124245\n+\n+import copy\n+import errno\n+import functools\n+import logging\n+import os\n+import platform\n+import re\n+import shlex\n+import shutil\n+import subprocess\n+import sys\n+import sysconfig\n+import warnings\n+from pathlib import Path\n+from typing import List, Tuple, Union\n+\n+import torch\n+from torch._inductor import config, exc\n+from torch._inductor.codecache import (\n+    get_lock_dir,\n+    invalid_vec_isa,\n+    LOCK_TIMEOUT,\n+    VecISA,\n+)\n+from torch._inductor.runtime.runtime_utils import cache_dir\n+\n+if config.is_fbcode():\n+    from triton.fb import build_paths  # noqa: F401\n+\n+    from torch._inductor.fb.utils import (\n+        log_global_cache_errors,\n+        log_global_cache_stats,\n+        log_global_cache_vals,\n+        use_global_cache,\n+    )\n+else:\n+\n+    def log_global_cache_errors(*args, **kwargs):\n+        pass\n+\n+    def log_global_cache_stats(*args, **kwargs):\n+        pass\n+\n+    def log_global_cache_vals(*args, **kwargs):\n+        pass\n+\n+    def use_global_cache() -> bool:\n+        return False\n+\n+\n+# Windows need setup a temp dir to store .obj files.\n+_BUILD_TEMP_DIR = \"CxxBuild\"\n+\n+# initialize variables for compilation\n+_IS_LINUX = sys.platform.startswith(\"linux\")\n+_IS_MACOS = sys.platform.startswith(\"darwin\")\n+_IS_WINDOWS = sys.platform == \"win32\"\n+\n+\n+log = logging.getLogger(__name__)\n+\n+\n+@functools.lru_cache(1)\n+def cpp_compiler_search(search: str) -> str:\n+    for cxx in search:\n+        try:\n+            if cxx is None:\n+                # gxx package is only available for Linux\n+                # according to https://anaconda.org/conda-forge/gxx/\n+                if sys.platform != \"linux\":\n+                    continue\n+                # Do not install GXX by default\n+                if not os.getenv(\"TORCH_INDUCTOR_INSTALL_GXX\"):\n+                    continue\n+                from filelock import FileLock\n+\n+                lock_dir = get_lock_dir()\n+                lock = FileLock(\n+                    os.path.join(lock_dir, \"g++.lock\"), timeout=LOCK_TIMEOUT\n+                )\n+                with lock:\n+                    cxx = install_gcc_via_conda()\n+            subprocess.check_output([cxx, \"--version\"])\n+            return cxx\n+        except (subprocess.SubprocessError, FileNotFoundError, ImportError):\n+            continue\n+    raise exc.InvalidCxxCompiler()  # noqa: RSE102\n+\n+\n+def install_gcc_via_conda() -> str:\n+    \"\"\"On older systems, this is a quick way to get a modern compiler\"\"\"\n+    prefix = os.path.join(cache_dir(), \"gcc\")\n+    cxx_path = os.path.join(prefix, \"bin\", \"g++\")\n+    if not os.path.exists(cxx_path):\n+        log.info(\"Downloading GCC via conda\")\n+        conda = os.environ.get(\"CONDA_EXE\", \"conda\")\n+        if conda is None:\n+            conda = shutil.which(\"conda\")\n+        if conda is not None:\n+            subprocess.check_call(\n+                [\n+                    conda,\n+                    \"create\",\n+                    f\"--prefix={prefix}\",\n+                    \"--channel=conda-forge\",\n+                    \"--quiet\",\n+                    \"-y\",\n+                    \"python=3.8\",\n+                    \"gxx\",\n+                ],\n+                stdout=subprocess.PIPE,\n+            )\n+    return cxx_path\n+\n+\n+def _get_cpp_compiler() -> str:\n+    if _IS_WINDOWS:\n+        compiler = os.environ.get(\"CXX\", \"cl\")\n+    else:\n+        if config.is_fbcode():\n+            return build_paths.cc()\n+        if isinstance(config.cpp.cxx, (list, tuple)):\n+            search = tuple(config.cpp.cxx)\n+        else:\n+            search = (config.cpp.cxx,)\n+        compiler = cpp_compiler_search(search)\n+    return compiler\n+\n+\n+def _is_gcc(cpp_compiler) -> bool:\n+    return bool(re.search(r\"(gcc|g\\+\\+)\", cpp_compiler))\n+\n+\n+def is_gcc() -> bool:\n+    return _is_gcc(_get_cpp_compiler())\n+\n+\n+def _is_clang(cpp_compiler) -> bool:\n+    # Mac OS apple clang maybe named as gcc, need check compiler info.\n+    if sys.platform == \"darwin\":\n+        return is_apple_clang(cpp_compiler)\n+    return bool(re.search(r\"(clang|clang\\+\\+)\", cpp_compiler))\n+\n+\n+def is_clang() -> bool:\n+    compiler = _get_cpp_compiler()\n+    return _is_clang(compiler)\n+\n+\n+@functools.lru_cache(None)\n+def is_apple_clang(cpp_compiler) -> bool:\n+    version_string = subprocess.check_output([cpp_compiler, \"--version\"]).decode(\"utf8\")\n+    return \"Apple\" in version_string.splitlines()[0]\n+\n+\n+def _append_list(dest_list: List[str], src_list: List[str]):\n+    for item in src_list:\n+        dest_list.append(copy.deepcopy(item))\n+\n+\n+def _remove_duplication_in_list(orig_list: List[str]) -> List[str]:\n+    new_list: List[str] = []\n+    for item in orig_list:\n+        if item not in new_list:\n+            new_list.append(item)\n+    return new_list\n+\n+\n+def _create_if_dir_not_exist(path_dir):\n+    if not os.path.exists(path_dir):\n+        try:\n+            Path(path_dir).mkdir(parents=True, exist_ok=True)\n+        except OSError as exc:  # Guard against race condition\n+            if exc.errno != errno.EEXIST:\n+                raise RuntimeError(  # noqa: TRY200 (Use `raise from`)\n+                    f\"Fail to create path {path_dir}\"\n+                )\n+\n+\n+def _remove_dir(path_dir):\n+    if os.path.exists(path_dir):\n+        for root, dirs, files in os.walk(path_dir, topdown=False):\n+            for name in files:\n+                file_path = os.path.join(root, name)\n+                os.remove(file_path)\n+            for name in dirs:\n+                dir_path = os.path.join(root, name)\n+                os.rmdir(dir_path)\n+        os.rmdir(path_dir)\n+\n+\n+def run_command_line(cmd_line, cwd=None):\n+    cmd = shlex.split(cmd_line)\n+    try:\n+        status = subprocess.check_output(args=cmd, cwd=cwd, stderr=subprocess.STDOUT)\n+    except subprocess.CalledProcessError as e:\n+        output = e.output.decode(\"utf-8\")\n+        openmp_problem = \"'omp.h' file not found\" in output or \"libomp\" in output\n+        if openmp_problem and sys.platform == \"darwin\":\n+            instruction = (\n+                \"\\n\\nOpenMP support not found. Please try one of the following solutions:\\n\"\n+                \"(1) Set the `CXX` environment variable to a compiler other than Apple clang++/g++ \"\n+                \"that has builtin OpenMP support;\\n\"\n+                \"(2) install OpenMP via conda: `conda install llvm-openmp`;\\n\"\n+                \"(3) install libomp via brew: `brew install libomp`;\\n\"\n+                \"(4) manually setup OpenMP and set the `OMP_PREFIX` environment variable to point to a path\"\n+                \" with `include/omp.h` under it.\"\n+            )\n+            output += instruction\n+        raise exc.CppCompileError(cmd, output) from e\n+    return status\n+\n+\n+class BuildOptionsBase:\n+    \"\"\"\n+    This is the Base class for store cxx build options, as a template.\n+    Acturally, to build a cxx shared library. We just need to select a compiler\n+    and maintains the suitable args.\n+    \"\"\"\n+\n+    def __init__(self) -> None:\n+        self._compiler = \"\"\n+        self._definations: List[str] = []\n+        self._include_dirs: List[str] = []\n+        self._cflags: List[str] = []\n+        self._ldflags: List[str] = []\n+        self._libraries_dirs: List[str] = []\n+        self._libraries: List[str] = []\n+        # Some args is hard to abstract to OS compatable, passthough it directly.\n+        self._passthough_args: List[str] = []\n+\n+        self._aot_mode = False\n+\n+    def _remove_duplicate_options(self):\n+        self._definations = _remove_duplication_in_list(self._definations)\n+        self._include_dirs = _remove_duplication_in_list(self._include_dirs)\n+        self._cflags = _remove_duplication_in_list(self._cflags)\n+        self._ldflags = _remove_duplication_in_list(self._ldflags)\n+        self._libraries_dirs = _remove_duplication_in_list(self._libraries_dirs)\n+        self._libraries = _remove_duplication_in_list(self._libraries)\n+        self._passthough_args = _remove_duplication_in_list(self._passthough_args)\n+\n+    def get_compiler(self) -> str:\n+        return self._compiler\n+\n+    def get_definations(self) -> List[str]:\n+        return self._definations\n+\n+    def get_include_dirs(self) -> List[str]:\n+        return self._include_dirs\n+\n+    def get_cflags(self) -> List[str]:\n+        return self._cflags\n+\n+    def get_ldflags(self) -> List[str]:\n+        return self._ldflags\n+\n+    def get_libraries_dirs(self) -> List[str]:\n+        return self._libraries_dirs\n+\n+    def get_libraries(self) -> List[str]:\n+        return self._libraries\n+\n+    def get_passthough_args(self) -> List[str]:\n+        return self._passthough_args\n+\n+    def get_aot_mode(self) -> bool:\n+        return self._aot_mode\n+\n+\n+def _get_warning_all_cflag(warning_all: bool = True) -> List[str]:\n+    if not _IS_WINDOWS:\n+        return [\"Wall\"] if warning_all else []\n+    else:\n+        return []\n+\n+\n+def _get_cpp_std_cflag(std_num: str = \"c++17\") -> List[str]:\n+    if _IS_WINDOWS:\n+        return [f\"std:{std_num}\"]\n+    else:\n+        return [f\"std={std_num}\"]\n+\n+\n+def _get_linux_cpp_cflags(cpp_compiler) -> List[str]:\n+    if not _IS_WINDOWS:\n+        cflags = [\"Wno-unused-variable\", \"Wno-unknown-pragmas\"]\n+        if _is_clang(cpp_compiler):\n+            cflags.append(\"Werror=ignored-optimization-argument\")\n+        return cflags\n+    else:\n+        return []\n+\n+\n+def _get_optimization_cflags() -> List[str]:\n+    if _IS_WINDOWS:\n+        return [\"O2\"]\n+    else:\n+        cflags = [\"O0\", \"g\"] if config.aot_inductor.debug_compile else [\"O3\", \"DNDEBUG\"]\n+        cflags.append(\"ffast-math\")\n+        cflags.append(\"fno-finite-math-only\")\n+\n+        if not config.cpp.enable_unsafe_math_opt_flag:\n+            cflags.append(\"fno-unsafe-math-optimizations\")\n+        if not config.cpp.enable_floating_point_contract_flag:\n+            cflags.append(\"ffp-contract=off\")\n+\n+        if config.is_fbcode():\n+            # FIXME: passing `-fopenmp` adds libgomp.so to the generated shared library's dependencies.\n+            # This causes `ldopen` to fail in fbcode, because libgomp does not exist in the default paths.\n+            # We will fix it later by exposing the lib path.\n+            return cflags\n+\n+        if sys.platform == \"darwin\":\n+            # Per https://mac.r-project.org/openmp/ right way to pass `openmp` flags to MacOS is via `-Xclang`\n+            # Also, `-march=native` is unrecognized option on M1\n+            cflags.append(\"Xclang\")\n+        else:\n+            if platform.machine() == \"ppc64le\":\n+                cflags.append(\"mcpu=native\")\n+            else:\n+                cflags.append(\"march=native\")\n+\n+        # Internal cannot find libgomp.so\n+        if not config.is_fbcode():\n+            cflags.append(\"fopenmp\")\n+\n+        return cflags\n+\n+\n+def _get_shared_cflag(compile_only: bool) -> List[str]:\n+    if _IS_WINDOWS:\n+        SHARED_FLAG = [\"DLL\"]\n+    else:\n+        if compile_only:\n+            return [\"fPIC\"]\n+        if platform.system() == \"Darwin\" and \"clang\" in _get_cpp_compiler():\n+            # This causes undefined symbols to behave the same as linux\n+            return [\"shared\", \"fPIC\", \"undefined dynamic_lookup\"]\n+        else:\n+            return [\"shared\", \"fPIC\"]\n+\n+    return SHARED_FLAG\n+\n+\n+def get_cpp_options(cpp_compiler, compile_only: bool, warning_all: bool = True):\n+    definations: List[str] = []\n+    include_dirs: List[str] = []\n+    cflags: List[str] = []\n+    ldflags: List[str] = []\n+    libraries_dirs: List[str] = []\n+    libraries: List[str] = []\n+    passthough_args: List[str] = []\n+\n+    cflags = (\n+        _get_shared_cflag(compile_only)\n+        + _get_optimization_cflags()\n+        + _get_warning_all_cflag(warning_all)\n+        + _get_cpp_std_cflag()\n+        + _get_linux_cpp_cflags(cpp_compiler)\n+    )\n+\n+    return (\n+        definations,\n+        include_dirs,\n+        cflags,\n+        ldflags,\n+        libraries_dirs,\n+        libraries,\n+        passthough_args,\n+    )\n+\n+\n+class CppOptions(BuildOptionsBase):\n+    \"\"\"\n+    This class is inherited from BuildOptionsBase, and as cxx build options.\n+    This option need contains basic cxx build option, which contains:\n+    1. OS related args.\n+    2. Toolchains related args.\n+    3. Cxx standard related args.\n+    Note:\n+    1. This Options is good for assist modules build, such as x86_isa_help.\n+    \"\"\"\n+\n+    def __init__(self, compile_only: bool, warning_all: bool = True) -> None:\n+        super().__init__()\n+        self._compiler = _get_cpp_compiler()\n+\n+        (\n+            definations,\n+            include_dirs,\n+            cflags,\n+            ldflags,\n+            libraries_dirs,\n+            libraries,\n+            passthough_args,\n+        ) = get_cpp_options(cpp_compiler=self._compiler, compile_only=compile_only)\n+\n+        _append_list(self._definations, definations)\n+        _append_list(self._include_dirs, include_dirs)\n+        _append_list(self._cflags, cflags)\n+        _append_list(self._ldflags, ldflags)\n+        _append_list(self._libraries_dirs, libraries_dirs)\n+        _append_list(self._libraries, libraries)\n+        _append_list(self._passthough_args, passthough_args)\n+        self._remove_duplicate_options()\n+\n+\n+def _get_glibcxx_abi_build_flags() -> List[str]:\n+    if not _IS_WINDOWS:\n+        return [\"-D_GLIBCXX_USE_CXX11_ABI=\" + str(int(torch._C._GLIBCXX_USE_CXX11_ABI))]\n+    else:\n+        return []\n+\n+\n+def _get_torch_cpp_wrapper_defination() -> List[str]:\n+    return [\"TORCH_INDUCTOR_CPP_WRAPPER\"]\n+\n+\n+def _use_custom_generated_macros() -> List[str]:\n+    return [\" C10_USING_CUSTOM_GENERATED_MACROS\"]\n+\n+\n+def _use_fb_internal_macros() -> List[str]:\n+    if not _IS_WINDOWS:\n+        if config.is_fbcode():\n+            # openmp_lib = build_paths.openmp_lib()\n+            preprocessor_flags = \" \".join(\n+                (\n+                    \"-D C10_USE_GLOG\",\n+                    \"-D C10_USE_MINIMAL_GLOG\",\n+                    \"-D C10_DISABLE_TENSORIMPL_EXTENSIBILITY\",\n+                )\n+            )\n+            # return [f\"-Wp,-fopenmp {openmp_lib} {preprocessor_flags}\"]\n+            return [f\"{preprocessor_flags}\"]\n+        else:\n+            return []\n+    else:\n+        return []\n+\n+\n+def _use_standard_sys_dir_headers():\n+    cflags: List[str] = []\n+    include_dirs: List[str] = []\n+    if _IS_WINDOWS:\n+        return cflags, include_dirs\n+\n+    if config.is_fbcode():\n+        cflags.append(\"nostdinc\")\n+        include_dirs.append(build_paths.sleef())\n+        include_dirs.append(build_paths.cc_include())\n+        include_dirs.append(build_paths.libgcc())\n+        include_dirs.append(build_paths.libgcc_arch())\n+        include_dirs.append(build_paths.libgcc_backward())\n+        include_dirs.append(build_paths.glibc())\n+        include_dirs.append(build_paths.linux_kernel())\n+        include_dirs.append(\"include\")\n+\n+    return cflags, include_dirs\n+\n+\n+@functools.lru_cache\n+def _cpp_prefix_path() -> str:\n+    from torch._inductor.codecache import write  # TODO\n+\n+    path = Path(Path(__file__).parent).parent / \"codegen/cpp_prefix.h\"\n+    with path.open() as f:\n+        content = f.read()\n+        _, filename = write(\n+            content,\n+            \"h\",\n+        )\n+    return filename\n+\n+\n+def _get_build_args_of_chosen_isa(chosen_isa: VecISA):\n+    macros = []\n+    build_flags = []\n+    if chosen_isa != invalid_vec_isa:\n+        # Add Windows support later.\n+        for x in chosen_isa.build_macro():\n+            macros.append(copy.deepcopy(x))\n+\n+        build_flags = [chosen_isa.build_arch_flags()]\n+\n+    if config.is_fbcode() and chosen_isa != invalid_vec_isa:\n+        cap = str(chosen_isa).upper()\n+        macros = [\n+            f\"CPU_CAPABILITY={cap}\",\n+            f\"CPU_CAPABILITY_{cap}\",\n+            f\"HAVE_{cap}_CPU_DEFINITION\",\n+        ]\n+\n+    return macros, build_flags\n+\n+\n+def _get_torch_related_args(include_pytorch: bool, aot_mode: bool):\n+    from torch.utils.cpp_extension import _TORCH_PATH, TORCH_LIB_PATH\n+\n+    include_dirs = [\n+        os.path.join(_TORCH_PATH, \"include\"),\n+        os.path.join(_TORCH_PATH, \"include\", \"torch\", \"csrc\", \"api\", \"include\"),\n+        # Some internal (old) Torch headers don't properly prefix their includes,\n+        # so we need to pass -Itorch/lib/include/TH as well.\n+        os.path.join(_TORCH_PATH, \"include\", \"TH\"),\n+        os.path.join(_TORCH_PATH, \"include\", \"THC\"),\n+    ]\n+    libraries_dirs = [TORCH_LIB_PATH]\n+    libraries = []\n+    if sys.platform != \"darwin\":\n+        libraries = [\"torch\", \"torch_cpu\"]\n+        if not aot_mode:\n+            libraries.append(\"torch_python\")\n+\n+    # Unconditionally import c10 for non-abi-compatible mode to use TORCH_CHECK - See PyTorch #108690\n+    if not config.abi_compatible:\n+        libraries.append(\"c10\")\n+        libraries_dirs.append(TORCH_LIB_PATH)\n+\n+    return include_dirs, libraries_dirs, libraries\n+\n+\n+def _get_python_related_args():\n+    python_include_dirs = []\n+    python_include_path = sysconfig.get_path(\n+        \"include\", scheme=\"nt\" if _IS_WINDOWS else \"posix_prefix\"\n+    )\n+    if python_include_path is not None:\n+        python_include_dirs.append(python_include_path)\n+\n+    if _IS_WINDOWS:\n+        python_path = os.path.dirname(sys.executable)\n+        python_lib_path = [os.path.join(python_path, \"libs\")]\n+    else:\n+        python_lib_path = [sysconfig.get_config_var(\"LIBDIR\")]\n+\n+    return python_include_dirs, python_lib_path\n+\n+\n+def _get_openmp_args(cpp_compiler):\n+    cflags: List[str] = []\n+    ldflags: List[str] = []\n+    include_dir_paths: List[str] = []\n+    lib_dir_paths: List[str] = []\n+    libs: List[str] = []\n+    if _IS_MACOS:\n+        from torch._inductor.codecache import (\n+            homebrew_libomp,\n+            is_conda_llvm_openmp_installed,\n+        )\n+\n+        # only Apple builtin compilers (Apple Clang++) require openmp\n+        omp_available = not is_apple_clang(cpp_compiler)\n+\n+        # check the `OMP_PREFIX` environment first\n+        omp_prefix = os.getenv(\"OMP_PREFIX\")\n+        if omp_prefix is not None:\n+            header_path = os.path.join(omp_prefix, \"include\", \"omp.h\")\n+            valid_env = os.path.exists(header_path)\n+            if valid_env:\n+                include_dir_paths.append(os.path.join(omp_prefix, \"include\"))\n+                lib_dir_paths.append(os.path.join(omp_prefix, \"lib\"))\n+            else:\n+                warnings.warn(\"environment variable `OMP_PREFIX` is invalid.\")\n+            omp_available = omp_available or valid_env\n+\n+        if not omp_available:\n+            libs.append(\"omp\")\n+\n+        # prefer to use openmp from `conda install llvm-openmp`\n+        conda_prefix = os.getenv(\"CONDA_PREFIX\")\n+        if not omp_available and conda_prefix is not None:\n+            omp_available = is_conda_llvm_openmp_installed()\n+            if omp_available:\n+                conda_lib_path = os.path.join(conda_prefix, \"lib\")\n+                include_dir_paths.append(os.path.join(conda_prefix, \"include\"))\n+                lib_dir_paths.append(conda_lib_path)\n+                # Prefer Intel OpenMP on x86 machine\n+                if os.uname().machine == \"x86_64\" and os.path.exists(\n+                    os.path.join(conda_lib_path, \"libiomp5.dylib\")\n+                ):\n+                    libs.append(\"iomp5\")\n+\n+        # next, try to use openmp from `brew install libomp`\n+        if not omp_available:\n+            omp_available, libomp_path = homebrew_libomp()\n+            if omp_available:\n+                include_dir_paths.append(os.path.join(libomp_path, \"include\"))\n+                lib_dir_paths.append(os.path.join(libomp_path, \"lib\"))\n+\n+        # if openmp is still not available, we let the compiler to have a try,\n+        # and raise error together with instructions at compilation error later\n+    elif _IS_WINDOWS:\n+        # /openmp, /openmp:llvm\n+        # llvm on Windows, new openmp: https://devblogs.microsoft.com/cppblog/msvc-openmp-update/\n+        # msvc openmp: https://learn.microsoft.com/zh-cn/cpp/build/reference/openmp-enable-openmp-2-0-support?view=msvc-170\n+\n+        cflags.append(\"openmp\")\n+        libs = []\n+    else:\n+        if config.is_fbcode():\n+            libs.append(\"omp\")\n+            include_dir_paths.append(build_paths.openmp())\n+        else:\n+            if _is_clang(cpp_compiler):\n+                # TODO: fix issue, can't find omp.h\n+                cflags.append(\"fopenmp\")\n+                libs.append(\"gomp\")\n+            else:\n+                cflags.append(\"fopenmp\")\n+                libs.append(\"gomp\")\n+\n+    return cflags, ldflags, include_dir_paths, lib_dir_paths, libs\n+\n+\n+def get_mmap_self_macro(use_mmap_weights: bool) -> List[str]:\n+    macros = []\n+    if use_mmap_weights:\n+        macros.append(\" USE_MMAP_SELF\")\n+    return macros\n+\n+\n+def get_cpp_torch_options(\n+    cpp_compiler,\n+    chosen_isa: VecISA,\n+    include_pytorch: bool,\n+    aot_mode: bool,\n+    compile_only: bool,\n+    use_mmap_weights: bool,\n+):\n+    definations: List[str] = []\n+    include_dirs: List[str] = []\n+    cflags: List[str] = []\n+    ldflags: List[str] = []\n+    libraries_dirs: List[str] = []\n+    libraries: List[str] = []\n+    passthough_args: List[str] = []\n+\n+    torch_cpp_wrapper_definations = _get_torch_cpp_wrapper_defination()\n+    use_custom_generated_macros_definations = _use_custom_generated_macros()\n+\n+    sys_dir_header_cflags, sys_dir_header_include_dirs = _use_standard_sys_dir_headers()\n+\n+    isa_macros, isa_ps_args_build_flags = _get_build_args_of_chosen_isa(chosen_isa)\n+\n+    (\n+        torch_include_dirs,\n+        torch_libraries_dirs,\n+        torch_libraries,\n+    ) = _get_torch_related_args(include_pytorch=include_pytorch, aot_mode=aot_mode)\n+\n+    python_include_dirs, python_libraries_dirs = _get_python_related_args()\n+\n+    (\n+        omp_cflags,\n+        omp_ldflags,\n+        omp_include_dir_paths,\n+        omp_lib_dir_paths,\n+        omp_lib,\n+    ) = _get_openmp_args(cpp_compiler)\n+\n+    cxx_abi_passthough_args = _get_glibcxx_abi_build_flags()\n+    fb_macro_passthough_args = _use_fb_internal_macros()\n+\n+    mmap_self_macros = get_mmap_self_macro(use_mmap_weights)\n+\n+    definations = (\n+        torch_cpp_wrapper_definations\n+        + use_custom_generated_macros_definations\n+        + isa_macros\n+        + fb_macro_passthough_args\n+        + mmap_self_macros\n+    )\n+    include_dirs = (\n+        sys_dir_header_include_dirs\n+        + python_include_dirs\n+        + torch_include_dirs\n+        + omp_include_dir_paths\n+    )\n+    cflags = sys_dir_header_cflags + omp_cflags\n+    ldflags = omp_ldflags\n+    libraries_dirs = python_libraries_dirs + torch_libraries_dirs + omp_lib_dir_paths\n+    libraries = torch_libraries + omp_lib\n+    passthough_args = isa_ps_args_build_flags + cxx_abi_passthough_args\n+\n+    return (\n+        definations,\n+        include_dirs,\n+        cflags,\n+        ldflags,\n+        libraries_dirs,\n+        libraries,\n+        passthough_args,\n+    )\n+\n+\n+class CppTorchOptions(CppOptions):\n+    \"\"\"\n+    This class is inherited from CppTorchOptions, which automatic contains\n+    base cxx build options. And then it will maintains torch related build\n+    args.\n+    1. Torch include_directories, libraries, libraries_directories.\n+    2. Python include_directories, libraries, libraries_directories.\n+    3. OpenMP related.\n+    4. Torch MACROs.\n+    5. MISC\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        chosen_isa: VecISA,\n+        include_pytorch: bool = False,\n+        warning_all: bool = True,\n+        aot_mode: bool = False,\n+        compile_only: bool = False,\n+        use_mmap_weights: bool = False,\n+    ) -> None:\n+        super().__init__(compile_only=compile_only, warning_all=warning_all)\n+\n+        self._aot_mode = aot_mode\n+\n+        (\n+            torch_definations,\n+            torch_include_dirs,\n+            torch_cflags,\n+            torch_ldflags,\n+            torch_libraries_dirs,\n+            torch_libraries,\n+            torch_passthough_args,\n+        ) = get_cpp_torch_options(\n+            cpp_compiler=self._compiler,\n+            chosen_isa=chosen_isa,\n+            include_pytorch=include_pytorch,\n+            aot_mode=aot_mode,\n+            compile_only=compile_only,\n+            use_mmap_weights=use_mmap_weights,\n+        )\n+\n+        if compile_only:\n+            torch_libraries_dirs = []\n+            torch_libraries = []\n+\n+        _append_list(self._definations, torch_definations)\n+        _append_list(self._include_dirs, torch_include_dirs)\n+        _append_list(self._cflags, torch_cflags)\n+        _append_list(self._ldflags, torch_ldflags)\n+        _append_list(self._libraries_dirs, torch_libraries_dirs)\n+        _append_list(self._libraries, torch_libraries)\n+        _append_list(self._passthough_args, torch_passthough_args)\n+        self._remove_duplicate_options()\n+\n+\n+def _get_cuda_related_args(aot_mode: bool):\n+    definations: List[str] = []\n+    include_dirs: List[str] = []\n+    cflags: List[str] = []\n+    ldflags: List[str] = []\n+    libraries_dirs: List[str] = []\n+    libraries: List[str] = []\n+    passthough_args: List[str] = []\n+\n+    # if not use cuda, don't call this function.\n+    use_cuda = True\n+\n+    if (\n+        config.is_fbcode()\n+        and \"CUDA_HOME\" not in os.environ\n+        and \"CUDA_PATH\" not in os.environ\n+    ):\n+        os.environ[\"CUDA_HOME\"] = os.path.dirname(build_paths.cuda())\n+\n+    from torch.utils import cpp_extension\n+\n+    include_dirs = cpp_extension.include_paths(use_cuda)\n+    libraries_dirs = cpp_extension.library_paths(use_cuda)\n+\n+    definations.append(\" USE_ROCM\" if torch.version.hip else \" USE_CUDA\")\n+\n+    if torch.version.hip is not None:\n+        if config.is_fbcode():\n+            libraries += [\"amdhip64\"]\n+        else:\n+            libraries += [\"c10_hip\", \"torch_hip\"]\n+            definations.append(\" __HIP_PLATFORM_AMD__\")\n+    else:\n+        if config.is_fbcode():\n+            libraries += [\"cuda\"]\n+        else:\n+            if config.is_fbcode():\n+                libraries += [\"cuda\"]\n+            else:\n+                libraries += [\"c10_cuda\", \"cuda\", \"torch_cuda\"]\n+\n+    if aot_mode:\n+        cpp_prefix_include_dir = [f\"{os.path.dirname(_cpp_prefix_path())}\"]\n+        include_dirs += cpp_prefix_include_dir\n+\n+        if config.is_fbcode():\n+            include_dirs.append(build_paths.cuda())\n+            # This is a special treatment for Meta internal cuda-12 where all libs\n+            # are in lib/cuda-12 and lib/cuda-12/stubs\n+\n+            # TODO: Verify process libcudart_static.a\n+            for i, path in enumerate(libraries_dirs):\n+                if path.startswith(os.environ[\"CUDA_HOME\"]) and not os.path.exists(\n+                    f\"{path}/libcudart_static.a\"\n+                ):\n+                    for root, dirs, files in os.walk(path):\n+                        if \"libcudart_static.a\" in files:\n+                            libraries_dirs[i] = os.path.join(path, root)\n+                            libraries_dirs.append(\n+                                os.path.join(libraries_dirs[i], \"stubs\")\n+                            )\n+                            break\n+        else:\n+            if not _IS_WINDOWS:\n+                # TODO: make static link better on Linux.\n+                passthough_args = [\"-Wl,-Bstatic -lcudart_static -Wl,-Bdynamic\"]\n+            else:\n+                libraries.append(\"cudart_static\")\n+\n+    return (\n+        definations,\n+        include_dirs,\n+        cflags,\n+        ldflags,\n+        libraries_dirs,\n+        libraries,\n+        passthough_args,\n+    )\n+\n+\n+def get_cpp_torch_cuda_options(aot_mode: bool = False):\n+    definations: List[str] = []\n+    include_dirs: List[str] = []\n+    cflags: List[str] = []\n+    ldflags: List[str] = []\n+    libraries_dirs: List[str] = []\n+    libraries: List[str] = []\n+    passthough_args: List[str] = []\n+\n+    (\n+        definations,\n+        include_dirs,\n+        cflags,\n+        ldflags,\n+        libraries_dirs,\n+        libraries,\n+        passthough_args,\n+    ) = _get_cuda_related_args(aot_mode)\n+\n+    return (\n+        definations,\n+        include_dirs,\n+        cflags,\n+        ldflags,\n+        libraries_dirs,\n+        libraries,\n+        passthough_args,\n+    )\n+\n+\n+class CppTorchCudaOptions(CppTorchOptions):\n+    \"\"\"\n+    This class is inherited from CppTorchOptions, which automatic contains\n+    base cxx build options and torch common build options. And then it will\n+    maintains cuda device related build args.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        chosen_isa: VecISA,\n+        include_pytorch: bool = False,\n+        use_cuda: bool = True,\n+        aot_mode: bool = False,\n+        compile_only: bool = False,\n+        use_mmap_weights: bool = False,\n+    ) -> None:\n+        # from torch._inductor.codecache import pick_vec_isa\n+\n+        super().__init__(\n+            chosen_isa=chosen_isa,\n+            include_pytorch=include_pytorch,\n+            aot_mode=aot_mode,\n+            compile_only=compile_only,\n+            use_mmap_weights=use_mmap_weights,\n+        )\n+\n+        cuda_definations: List[str] = []\n+        cuda_include_dirs: List[str] = []\n+        cuda_cflags: List[str] = []\n+        cuda_ldflags: List[str] = []\n+        cuda_libraries_dirs: List[str] = []\n+        cuda_libraries: List[str] = []\n+        cuda_passthough_args: List[str] = []\n+\n+        if use_cuda:\n+            (\n+                cuda_definations,\n+                cuda_include_dirs,\n+                cuda_cflags,\n+                cuda_ldflags,\n+                cuda_libraries_dirs,\n+                cuda_libraries,\n+                cuda_passthough_args,\n+            ) = get_cpp_torch_cuda_options(aot_mode=aot_mode)\n+\n+        if compile_only:\n+            cuda_libraries_dirs = []\n+            cuda_libraries = []\n+\n+        _append_list(self._definations, cuda_definations)\n+        _append_list(self._include_dirs, cuda_include_dirs)\n+        _append_list(self._cflags, cuda_cflags)\n+        _append_list(self._ldflags, cuda_ldflags)\n+        _append_list(self._libraries_dirs, cuda_libraries_dirs)\n+        _append_list(self._libraries, cuda_libraries)\n+        _append_list(self._passthough_args, cuda_passthough_args)\n+        self._remove_duplicate_options()\n+\n+\n+def get_name_and_dir_from_output_file_path(\n+    aot_mode: bool, use_absolute_path: bool, file_path: str\n+):\n+    name_and_ext = os.path.basename(file_path)\n+    name, ext = os.path.splitext(name_and_ext)\n+    dir = os.path.dirname(file_path)\n+\n+    if config.is_fbcode():\n+        if not (aot_mode and not use_absolute_path):\n+            dir = \".\"\n+    return name, dir\n+\n+\n+class CppBuilder:\n+    \"\"\"\n+    CppBuilder is a cpp jit builder, and it supports both Windows, Linux and MacOS.\n+    Args:\n+        name:\n+            1. Build target name, the final target file will append extension type automatically.\n+            2. Due to the CppBuilder is supports mutliple OS, it will maintains ext for OS difference.\n+\n+        sources:\n+            Source code file list to be built.\n+\n+        BuildOption:\n+            Build options to the builder.\n+\n+        output_dir:\n+            1. The output_dir the taget file will output to.\n+            2. The default value is empty string, and then the use current dir as output dir.\n+            3. Final target file: output_dir/name.ext\n+    \"\"\"\n+\n+    def get_shared_lib_ext(self) -> str:\n+        SHARED_LIB_EXT = \".dll\" if _IS_WINDOWS else \".so\"\n+        return SHARED_LIB_EXT\n+\n+    def get_object_ext(self) -> str:\n+        EXT = \".obj\" if _IS_WINDOWS else \".o\"\n+        return EXT\n+\n+    def __init__(\n+        self,\n+        name: str,\n+        sources: Union[str, List[str]],\n+        BuildOption: BuildOptionsBase,\n+        output_dir: str = \"\",\n+        compile_only: bool = False,\n+        use_absolute_path: bool = False,\n+    ) -> None:\n+        self._compiler = \"\"\n+        self._cflags_args = \"\"\n+        self._definations_args = \"\"\n+        self._include_dirs_args = \"\"\n+        self._ldflags_args = \"\"\n+        self._libraries_dirs_args = \"\"\n+        self._libraries_args = \"\"\n+        self._passthough_parameters_args = \"\"\n+\n+        self._output_dir = \"\"\n+        self._target_file = \"\"\n+\n+        self._name = name\n+\n+        if isinstance(sources, str):\n+            sources = [sources]\n+\n+        if config.is_fbcode():\n+            if BuildOption.get_aot_mode() and not use_absolute_path:\n+                inp_name = sources\n+                # output process @ get_name_and_dir_from_output_file_path\n+            else:\n+                # We need to copy any absolute-path torch includes\n+                inp_name = [os.path.basename(i) for i in sources]\n+\n+            self._sources_args = \" \".join(inp_name)\n+\n+            if _is_clang(self._compiler):\n+                self._passthough_parameters_args += \" --rtlib=compiler-rt\"\n+                self._passthough_parameters_args += \" -fuse-ld=lld\"\n+                self._passthough_parameters_args += \" -B\" + build_paths.glibc_lib()\n+                self._passthough_parameters_args += \" -L\" + build_paths.glibc_lib()\n+        else:\n+            self._sources_args = \" \".join(sources)\n+\n+        self._compile_only = compile_only\n+\n+        if len(output_dir) == 0:\n+            self._output_dir = os.path.dirname(os.path.abspath(__file__))\n+        else:\n+            self._output_dir = output_dir\n+\n+        file_ext = self.get_object_ext() if compile_only else self.get_shared_lib_ext()\n+        self._target_file = os.path.join(self._output_dir, f\"{self._name}{file_ext}\")\n+\n+        self._compiler = BuildOption.get_compiler()\n+\n+        for cflag in BuildOption.get_cflags():\n+            if _IS_WINDOWS:\n+                self._cflags_args += f\"/{cflag} \"\n+            else:\n+                self._cflags_args += f\"-{cflag} \"\n+\n+        for defination in BuildOption.get_definations():\n+            if _IS_WINDOWS:\n+                self._definations_args += f\"/D {defination} \"\n+            else:\n+                self._definations_args += f\"-D{defination} \"\n+\n+        for inc_dir in BuildOption.get_include_dirs():\n+            if _IS_WINDOWS:\n+                self._include_dirs_args += f\"/I {inc_dir} \"\n+            else:\n+                self._include_dirs_args += f\"-I{inc_dir} \"\n+\n+        for ldflag in BuildOption.get_ldflags():\n+            if _IS_WINDOWS:\n+                self._ldflags_args += f\"/{ldflag} \"\n+            else:\n+                self._ldflags_args += f\"-{ldflag} \"\n+\n+        for lib_dir in BuildOption.get_libraries_dirs():\n+            if _IS_WINDOWS:\n+                self._libraries_dirs_args += f'/LIBPATH:\"{lib_dir}\" '\n+            else:\n+                self._libraries_dirs_args += f\"-L{lib_dir} \"\n+\n+        for lib in BuildOption.get_libraries():\n+            if _IS_WINDOWS:\n+                self._libraries_args += f'\"{lib}.lib\" '\n+            else:\n+                self._libraries_args += f\"-l{lib} \"\n+\n+        for passthough_arg in BuildOption.get_passthough_args():\n+            self._passthough_parameters_args += f\"{passthough_arg} \"\n+\n+    def get_command_line(self) -> str:\n+        def format_build_command(\n+            compiler,\n+            sources,\n+            include_dirs_args,\n+            definations_args,\n+            cflags_args,\n+            ldflags_args,\n+            libraries_args,\n+            libraries_dirs_args,\n+            passthougn_args,\n+            target_file,\n+        ):\n+            if _IS_WINDOWS:\n+                # https://learn.microsoft.com/en-us/cpp/build/walkthrough-compile-a-c-program-on-the-command-line?view=msvc-1704\n+                # https://stackoverflow.com/a/31566153\n+                cmd = (\n+                    f\"{compiler} {include_dirs_args} {definations_args} {cflags_args} {sources} \"\n+                    f\"{passthougn_args} /LD /Fe{target_file} /link {libraries_dirs_args} {libraries_args} {ldflags_args} \"\n+                )\n+                cmd = cmd.replace(\"\\\\\", \"/\")\n+            else:\n+                compile_only_arg = \"-c\" if self._compile_only else \"\"\n+                cmd = re.sub(\n+                    r\"[ \\n]+\",\n+                    \" \",\n+                    f\"\"\"\n+                    {compiler} {sources} {definations_args} {cflags_args} {include_dirs_args}\n+                    {passthougn_args} {ldflags_args} {libraries_args} {libraries_dirs_args} {compile_only_arg} -o {target_file}\n+                    \"\"\",\n+                ).strip()\n+            return cmd\n+\n+        command_line = format_build_command(\n+            compiler=self._compiler,\n+            sources=self._sources_args,\n+            include_dirs_args=self._include_dirs_args,\n+            definations_args=self._definations_args,\n+            cflags_args=self._cflags_args,\n+            ldflags_args=self._ldflags_args,\n+            libraries_args=self._libraries_args,\n+            libraries_dirs_args=self._libraries_dirs_args,\n+            passthougn_args=self._passthough_parameters_args,\n+            target_file=self._target_file,\n+        )\n+        return command_line\n+\n+    def get_target_file_path(self):\n+        return self._target_file\n+\n+    def convert_to_cpp_extension_args(self):\n+        include_dirs = self._include_dirs_args\n+        cflags = (\n+            self._cflags_args\n+            + self._definations_args\n+            + self._passthough_parameters_args\n+        )\n+        ldflags = self._ldflags_args + self._libraries_args + self._libraries_dirs_args\n+\n+        return include_dirs, cflags, ldflags\n+\n+    def build(self) -> Tuple[int, str]:\n+        \"\"\"\n+        It is must need a temperary directory to store object files in Windows.\n+        After build completed, delete the temperary directory to save disk space.\n+        \"\"\"\n+        _create_if_dir_not_exist(self._output_dir)\n+        _build_tmp_dir = os.path.join(\n+            self._output_dir, f\"{self._name}_{_BUILD_TEMP_DIR}\"\n+        )\n+        _create_if_dir_not_exist(_build_tmp_dir)\n+\n+        build_cmd = self.get_command_line()\n+\n+        status = run_command_line(build_cmd, cwd=_build_tmp_dir)\n+\n+        _remove_dir(_build_tmp_dir)\n+        return status, self._target_file\n",
            "whole_hunk": "@@ -0,0 +1,1139 @@\n+# This CPP JIT builder is designed to support both Windows and Linux OS.\n+# The design document please check this RFC: https://github.com/pytorch/pytorch/issues/124245\n+\n+import copy\n+import errno\n+import functools\n+import logging\n+import os\n+import platform\n+import re\n+import shlex\n+import shutil\n+import subprocess\n+import sys\n+import sysconfig\n+import warnings\n+from pathlib import Path\n+from typing import List, Tuple, Union\n+\n+import torch\n+from torch._inductor import config, exc\n+from torch._inductor.codecache import (\n+    get_lock_dir,\n+    invalid_vec_isa,\n+    LOCK_TIMEOUT,\n+    VecISA,\n+)\n+from torch._inductor.runtime.runtime_utils import cache_dir\n+\n+if config.is_fbcode():\n+    from triton.fb import build_paths  # noqa: F401\n+\n+    from torch._inductor.fb.utils import (\n+        log_global_cache_errors,\n+        log_global_cache_stats,\n+        log_global_cache_vals,\n+        use_global_cache,\n+    )\n+else:\n+\n+    def log_global_cache_errors(*args, **kwargs):\n+        pass\n+\n+    def log_global_cache_stats(*args, **kwargs):\n+        pass\n+\n+    def log_global_cache_vals(*args, **kwargs):\n+        pass\n+\n+    def use_global_cache() -> bool:\n+        return False\n+\n+\n+# Windows need setup a temp dir to store .obj files.\n+_BUILD_TEMP_DIR = \"CxxBuild\"\n+\n+# initialize variables for compilation\n+_IS_LINUX = sys.platform.startswith(\"linux\")\n+_IS_MACOS = sys.platform.startswith(\"darwin\")\n+_IS_WINDOWS = sys.platform == \"win32\"\n+\n+\n+log = logging.getLogger(__name__)\n+\n+\n+@functools.lru_cache(1)\n+def cpp_compiler_search(search: str) -> str:\n+    for cxx in search:\n+        try:\n+            if cxx is None:\n+                # gxx package is only available for Linux\n+                # according to https://anaconda.org/conda-forge/gxx/\n+                if sys.platform != \"linux\":\n+                    continue\n+                # Do not install GXX by default\n+                if not os.getenv(\"TORCH_INDUCTOR_INSTALL_GXX\"):\n+                    continue\n+                from filelock import FileLock\n+\n+                lock_dir = get_lock_dir()\n+                lock = FileLock(\n+                    os.path.join(lock_dir, \"g++.lock\"), timeout=LOCK_TIMEOUT\n+                )\n+                with lock:\n+                    cxx = install_gcc_via_conda()\n+            subprocess.check_output([cxx, \"--version\"])\n+            return cxx\n+        except (subprocess.SubprocessError, FileNotFoundError, ImportError):\n+            continue\n+    raise exc.InvalidCxxCompiler()  # noqa: RSE102\n+\n+\n+def install_gcc_via_conda() -> str:\n+    \"\"\"On older systems, this is a quick way to get a modern compiler\"\"\"\n+    prefix = os.path.join(cache_dir(), \"gcc\")\n+    cxx_path = os.path.join(prefix, \"bin\", \"g++\")\n+    if not os.path.exists(cxx_path):\n+        log.info(\"Downloading GCC via conda\")\n+        conda = os.environ.get(\"CONDA_EXE\", \"conda\")\n+        if conda is None:\n+            conda = shutil.which(\"conda\")\n+        if conda is not None:\n+            subprocess.check_call(\n+                [\n+                    conda,\n+                    \"create\",\n+                    f\"--prefix={prefix}\",\n+                    \"--channel=conda-forge\",\n+                    \"--quiet\",\n+                    \"-y\",\n+                    \"python=3.8\",\n+                    \"gxx\",\n+                ],\n+                stdout=subprocess.PIPE,\n+            )\n+    return cxx_path\n+\n+\n+def _get_cpp_compiler() -> str:\n+    if _IS_WINDOWS:\n+        compiler = os.environ.get(\"CXX\", \"cl\")\n+    else:\n+        if config.is_fbcode():\n+            return build_paths.cc()\n+        if isinstance(config.cpp.cxx, (list, tuple)):\n+            search = tuple(config.cpp.cxx)\n+        else:\n+            search = (config.cpp.cxx,)\n+        compiler = cpp_compiler_search(search)\n+    return compiler\n+\n+\n+def _is_gcc(cpp_compiler) -> bool:\n+    return bool(re.search(r\"(gcc|g\\+\\+)\", cpp_compiler))\n+\n+\n+def is_gcc() -> bool:\n+    return _is_gcc(_get_cpp_compiler())\n+\n+\n+def _is_clang(cpp_compiler) -> bool:\n+    # Mac OS apple clang maybe named as gcc, need check compiler info.\n+    if sys.platform == \"darwin\":\n+        return is_apple_clang(cpp_compiler)\n+    return bool(re.search(r\"(clang|clang\\+\\+)\", cpp_compiler))\n+\n+\n+def is_clang() -> bool:\n+    compiler = _get_cpp_compiler()\n+    return _is_clang(compiler)\n+\n+\n+@functools.lru_cache(None)\n+def is_apple_clang(cpp_compiler) -> bool:\n+    version_string = subprocess.check_output([cpp_compiler, \"--version\"]).decode(\"utf8\")\n+    return \"Apple\" in version_string.splitlines()[0]\n+\n+\n+def _append_list(dest_list: List[str], src_list: List[str]):\n+    for item in src_list:\n+        dest_list.append(copy.deepcopy(item))\n+\n+\n+def _remove_duplication_in_list(orig_list: List[str]) -> List[str]:\n+    new_list: List[str] = []\n+    for item in orig_list:\n+        if item not in new_list:\n+            new_list.append(item)\n+    return new_list\n+\n+\n+def _create_if_dir_not_exist(path_dir):\n+    if not os.path.exists(path_dir):\n+        try:\n+            Path(path_dir).mkdir(parents=True, exist_ok=True)\n+        except OSError as exc:  # Guard against race condition\n+            if exc.errno != errno.EEXIST:\n+                raise RuntimeError(  # noqa: TRY200 (Use `raise from`)\n+                    f\"Fail to create path {path_dir}\"\n+                )\n+\n+\n+def _remove_dir(path_dir):\n+    if os.path.exists(path_dir):\n+        for root, dirs, files in os.walk(path_dir, topdown=False):\n+            for name in files:\n+                file_path = os.path.join(root, name)\n+                os.remove(file_path)\n+            for name in dirs:\n+                dir_path = os.path.join(root, name)\n+                os.rmdir(dir_path)\n+        os.rmdir(path_dir)\n+\n+\n+def run_command_line(cmd_line, cwd=None):\n+    cmd = shlex.split(cmd_line)\n+    try:\n+        status = subprocess.check_output(args=cmd, cwd=cwd, stderr=subprocess.STDOUT)\n+    except subprocess.CalledProcessError as e:\n+        output = e.output.decode(\"utf-8\")\n+        openmp_problem = \"'omp.h' file not found\" in output or \"libomp\" in output\n+        if openmp_problem and sys.platform == \"darwin\":\n+            instruction = (\n+                \"\\n\\nOpenMP support not found. Please try one of the following solutions:\\n\"\n+                \"(1) Set the `CXX` environment variable to a compiler other than Apple clang++/g++ \"\n+                \"that has builtin OpenMP support;\\n\"\n+                \"(2) install OpenMP via conda: `conda install llvm-openmp`;\\n\"\n+                \"(3) install libomp via brew: `brew install libomp`;\\n\"\n+                \"(4) manually setup OpenMP and set the `OMP_PREFIX` environment variable to point to a path\"\n+                \" with `include/omp.h` under it.\"\n+            )\n+            output += instruction\n+        raise exc.CppCompileError(cmd, output) from e\n+    return status\n+\n+\n+class BuildOptionsBase:\n+    \"\"\"\n+    This is the Base class for store cxx build options, as a template.\n+    Acturally, to build a cxx shared library. We just need to select a compiler\n+    and maintains the suitable args.\n+    \"\"\"\n+\n+    def __init__(self) -> None:\n+        self._compiler = \"\"\n+        self._definations: List[str] = []\n+        self._include_dirs: List[str] = []\n+        self._cflags: List[str] = []\n+        self._ldflags: List[str] = []\n+        self._libraries_dirs: List[str] = []\n+        self._libraries: List[str] = []\n+        # Some args is hard to abstract to OS compatable, passthough it directly.\n+        self._passthough_args: List[str] = []\n+\n+        self._aot_mode = False\n+\n+    def _remove_duplicate_options(self):\n+        self._definations = _remove_duplication_in_list(self._definations)\n+        self._include_dirs = _remove_duplication_in_list(self._include_dirs)\n+        self._cflags = _remove_duplication_in_list(self._cflags)\n+        self._ldflags = _remove_duplication_in_list(self._ldflags)\n+        self._libraries_dirs = _remove_duplication_in_list(self._libraries_dirs)\n+        self._libraries = _remove_duplication_in_list(self._libraries)\n+        self._passthough_args = _remove_duplication_in_list(self._passthough_args)\n+\n+    def get_compiler(self) -> str:\n+        return self._compiler\n+\n+    def get_definations(self) -> List[str]:\n+        return self._definations\n+\n+    def get_include_dirs(self) -> List[str]:\n+        return self._include_dirs\n+\n+    def get_cflags(self) -> List[str]:\n+        return self._cflags\n+\n+    def get_ldflags(self) -> List[str]:\n+        return self._ldflags\n+\n+    def get_libraries_dirs(self) -> List[str]:\n+        return self._libraries_dirs\n+\n+    def get_libraries(self) -> List[str]:\n+        return self._libraries\n+\n+    def get_passthough_args(self) -> List[str]:\n+        return self._passthough_args\n+\n+    def get_aot_mode(self) -> bool:\n+        return self._aot_mode\n+\n+\n+def _get_warning_all_cflag(warning_all: bool = True) -> List[str]:\n+    if not _IS_WINDOWS:\n+        return [\"Wall\"] if warning_all else []\n+    else:\n+        return []\n+\n+\n+def _get_cpp_std_cflag(std_num: str = \"c++17\") -> List[str]:\n+    if _IS_WINDOWS:\n+        return [f\"std:{std_num}\"]\n+    else:\n+        return [f\"std={std_num}\"]\n+\n+\n+def _get_linux_cpp_cflags(cpp_compiler) -> List[str]:\n+    if not _IS_WINDOWS:\n+        cflags = [\"Wno-unused-variable\", \"Wno-unknown-pragmas\"]\n+        if _is_clang(cpp_compiler):\n+            cflags.append(\"Werror=ignored-optimization-argument\")\n+        return cflags\n+    else:\n+        return []\n+\n+\n+def _get_optimization_cflags() -> List[str]:\n+    if _IS_WINDOWS:\n+        return [\"O2\"]\n+    else:\n+        cflags = [\"O0\", \"g\"] if config.aot_inductor.debug_compile else [\"O3\", \"DNDEBUG\"]\n+        cflags.append(\"ffast-math\")\n+        cflags.append(\"fno-finite-math-only\")\n+\n+        if not config.cpp.enable_unsafe_math_opt_flag:\n+            cflags.append(\"fno-unsafe-math-optimizations\")\n+        if not config.cpp.enable_floating_point_contract_flag:\n+            cflags.append(\"ffp-contract=off\")\n+\n+        if config.is_fbcode():\n+            # FIXME: passing `-fopenmp` adds libgomp.so to the generated shared library's dependencies.\n+            # This causes `ldopen` to fail in fbcode, because libgomp does not exist in the default paths.\n+            # We will fix it later by exposing the lib path.\n+            return cflags\n+\n+        if sys.platform == \"darwin\":\n+            # Per https://mac.r-project.org/openmp/ right way to pass `openmp` flags to MacOS is via `-Xclang`\n+            # Also, `-march=native` is unrecognized option on M1\n+            cflags.append(\"Xclang\")\n+        else:\n+            if platform.machine() == \"ppc64le\":\n+                cflags.append(\"mcpu=native\")\n+            else:\n+                cflags.append(\"march=native\")\n+\n+        # Internal cannot find libgomp.so\n+        if not config.is_fbcode():\n+            cflags.append(\"fopenmp\")\n+\n+        return cflags\n+\n+\n+def _get_shared_cflag(compile_only: bool) -> List[str]:\n+    if _IS_WINDOWS:\n+        SHARED_FLAG = [\"DLL\"]\n+    else:\n+        if compile_only:\n+            return [\"fPIC\"]\n+        if platform.system() == \"Darwin\" and \"clang\" in _get_cpp_compiler():\n+            # This causes undefined symbols to behave the same as linux\n+            return [\"shared\", \"fPIC\", \"undefined dynamic_lookup\"]\n+        else:\n+            return [\"shared\", \"fPIC\"]\n+\n+    return SHARED_FLAG\n+\n+\n+def get_cpp_options(cpp_compiler, compile_only: bool, warning_all: bool = True):\n+    definations: List[str] = []\n+    include_dirs: List[str] = []\n+    cflags: List[str] = []\n+    ldflags: List[str] = []\n+    libraries_dirs: List[str] = []\n+    libraries: List[str] = []\n+    passthough_args: List[str] = []\n+\n+    cflags = (\n+        _get_shared_cflag(compile_only)\n+        + _get_optimization_cflags()\n+        + _get_warning_all_cflag(warning_all)\n+        + _get_cpp_std_cflag()\n+        + _get_linux_cpp_cflags(cpp_compiler)\n+    )\n+\n+    return (\n+        definations,\n+        include_dirs,\n+        cflags,\n+        ldflags,\n+        libraries_dirs,\n+        libraries,\n+        passthough_args,\n+    )\n+\n+\n+class CppOptions(BuildOptionsBase):\n+    \"\"\"\n+    This class is inherited from BuildOptionsBase, and as cxx build options.\n+    This option need contains basic cxx build option, which contains:\n+    1. OS related args.\n+    2. Toolchains related args.\n+    3. Cxx standard related args.\n+    Note:\n+    1. This Options is good for assist modules build, such as x86_isa_help.\n+    \"\"\"\n+\n+    def __init__(self, compile_only: bool, warning_all: bool = True) -> None:\n+        super().__init__()\n+        self._compiler = _get_cpp_compiler()\n+\n+        (\n+            definations,\n+            include_dirs,\n+            cflags,\n+            ldflags,\n+            libraries_dirs,\n+            libraries,\n+            passthough_args,\n+        ) = get_cpp_options(cpp_compiler=self._compiler, compile_only=compile_only)\n+\n+        _append_list(self._definations, definations)\n+        _append_list(self._include_dirs, include_dirs)\n+        _append_list(self._cflags, cflags)\n+        _append_list(self._ldflags, ldflags)\n+        _append_list(self._libraries_dirs, libraries_dirs)\n+        _append_list(self._libraries, libraries)\n+        _append_list(self._passthough_args, passthough_args)\n+        self._remove_duplicate_options()\n+\n+\n+def _get_glibcxx_abi_build_flags() -> List[str]:\n+    if not _IS_WINDOWS:\n+        return [\"-D_GLIBCXX_USE_CXX11_ABI=\" + str(int(torch._C._GLIBCXX_USE_CXX11_ABI))]\n+    else:\n+        return []\n+\n+\n+def _get_torch_cpp_wrapper_defination() -> List[str]:\n+    return [\"TORCH_INDUCTOR_CPP_WRAPPER\"]\n+\n+\n+def _use_custom_generated_macros() -> List[str]:\n+    return [\" C10_USING_CUSTOM_GENERATED_MACROS\"]\n+\n+\n+def _use_fb_internal_macros() -> List[str]:\n+    if not _IS_WINDOWS:\n+        if config.is_fbcode():\n+            # openmp_lib = build_paths.openmp_lib()\n+            preprocessor_flags = \" \".join(\n+                (\n+                    \"-D C10_USE_GLOG\",\n+                    \"-D C10_USE_MINIMAL_GLOG\",\n+                    \"-D C10_DISABLE_TENSORIMPL_EXTENSIBILITY\",\n+                )\n+            )\n+            # return [f\"-Wp,-fopenmp {openmp_lib} {preprocessor_flags}\"]\n+            return [f\"{preprocessor_flags}\"]\n+        else:\n+            return []\n+    else:\n+        return []\n+\n+\n+def _use_standard_sys_dir_headers():\n+    cflags: List[str] = []\n+    include_dirs: List[str] = []\n+    if _IS_WINDOWS:\n+        return cflags, include_dirs\n+\n+    if config.is_fbcode():\n+        cflags.append(\"nostdinc\")\n+        include_dirs.append(build_paths.sleef())\n+        include_dirs.append(build_paths.cc_include())\n+        include_dirs.append(build_paths.libgcc())\n+        include_dirs.append(build_paths.libgcc_arch())\n+        include_dirs.append(build_paths.libgcc_backward())\n+        include_dirs.append(build_paths.glibc())\n+        include_dirs.append(build_paths.linux_kernel())\n+        include_dirs.append(\"include\")\n+\n+    return cflags, include_dirs\n+\n+\n+@functools.lru_cache\n+def _cpp_prefix_path() -> str:\n+    from torch._inductor.codecache import write  # TODO\n+\n+    path = Path(Path(__file__).parent).parent / \"codegen/cpp_prefix.h\"\n+    with path.open() as f:\n+        content = f.read()\n+        _, filename = write(\n+            content,\n+            \"h\",\n+        )\n+    return filename\n+\n+\n+def _get_build_args_of_chosen_isa(chosen_isa: VecISA):\n+    macros = []\n+    build_flags = []\n+    if chosen_isa != invalid_vec_isa:\n+        # Add Windows support later.\n+        for x in chosen_isa.build_macro():\n+            macros.append(copy.deepcopy(x))\n+\n+        build_flags = [chosen_isa.build_arch_flags()]\n+\n+    if config.is_fbcode() and chosen_isa != invalid_vec_isa:\n+        cap = str(chosen_isa).upper()\n+        macros = [\n+            f\"CPU_CAPABILITY={cap}\",\n+            f\"CPU_CAPABILITY_{cap}\",\n+            f\"HAVE_{cap}_CPU_DEFINITION\",\n+        ]\n+\n+    return macros, build_flags\n+\n+\n+def _get_torch_related_args(include_pytorch: bool, aot_mode: bool):\n+    from torch.utils.cpp_extension import _TORCH_PATH, TORCH_LIB_PATH\n+\n+    include_dirs = [\n+        os.path.join(_TORCH_PATH, \"include\"),\n+        os.path.join(_TORCH_PATH, \"include\", \"torch\", \"csrc\", \"api\", \"include\"),\n+        # Some internal (old) Torch headers don't properly prefix their includes,\n+        # so we need to pass -Itorch/lib/include/TH as well.\n+        os.path.join(_TORCH_PATH, \"include\", \"TH\"),\n+        os.path.join(_TORCH_PATH, \"include\", \"THC\"),\n+    ]\n+    libraries_dirs = [TORCH_LIB_PATH]\n+    libraries = []\n+    if sys.platform != \"darwin\":\n+        libraries = [\"torch\", \"torch_cpu\"]\n+        if not aot_mode:\n+            libraries.append(\"torch_python\")\n+\n+    # Unconditionally import c10 for non-abi-compatible mode to use TORCH_CHECK - See PyTorch #108690\n+    if not config.abi_compatible:\n+        libraries.append(\"c10\")\n+        libraries_dirs.append(TORCH_LIB_PATH)\n+\n+    return include_dirs, libraries_dirs, libraries\n+\n+\n+def _get_python_related_args():\n+    python_include_dirs = []\n+    python_include_path = sysconfig.get_path(\n+        \"include\", scheme=\"nt\" if _IS_WINDOWS else \"posix_prefix\"\n+    )\n+    if python_include_path is not None:\n+        python_include_dirs.append(python_include_path)\n+\n+    if _IS_WINDOWS:\n+        python_path = os.path.dirname(sys.executable)\n+        python_lib_path = [os.path.join(python_path, \"libs\")]\n+    else:\n+        python_lib_path = [sysconfig.get_config_var(\"LIBDIR\")]\n+\n+    return python_include_dirs, python_lib_path\n+\n+\n+def _get_openmp_args(cpp_compiler):\n+    cflags: List[str] = []\n+    ldflags: List[str] = []\n+    include_dir_paths: List[str] = []\n+    lib_dir_paths: List[str] = []\n+    libs: List[str] = []\n+    if _IS_MACOS:\n+        from torch._inductor.codecache import (\n+            homebrew_libomp,\n+            is_conda_llvm_openmp_installed,\n+        )\n+\n+        # only Apple builtin compilers (Apple Clang++) require openmp\n+        omp_available = not is_apple_clang(cpp_compiler)\n+\n+        # check the `OMP_PREFIX` environment first\n+        omp_prefix = os.getenv(\"OMP_PREFIX\")\n+        if omp_prefix is not None:\n+            header_path = os.path.join(omp_prefix, \"include\", \"omp.h\")\n+            valid_env = os.path.exists(header_path)\n+            if valid_env:\n+                include_dir_paths.append(os.path.join(omp_prefix, \"include\"))\n+                lib_dir_paths.append(os.path.join(omp_prefix, \"lib\"))\n+            else:\n+                warnings.warn(\"environment variable `OMP_PREFIX` is invalid.\")\n+            omp_available = omp_available or valid_env\n+\n+        if not omp_available:\n+            libs.append(\"omp\")\n+\n+        # prefer to use openmp from `conda install llvm-openmp`\n+        conda_prefix = os.getenv(\"CONDA_PREFIX\")\n+        if not omp_available and conda_prefix is not None:\n+            omp_available = is_conda_llvm_openmp_installed()\n+            if omp_available:\n+                conda_lib_path = os.path.join(conda_prefix, \"lib\")\n+                include_dir_paths.append(os.path.join(conda_prefix, \"include\"))\n+                lib_dir_paths.append(conda_lib_path)\n+                # Prefer Intel OpenMP on x86 machine\n+                if os.uname().machine == \"x86_64\" and os.path.exists(\n+                    os.path.join(conda_lib_path, \"libiomp5.dylib\")\n+                ):\n+                    libs.append(\"iomp5\")\n+\n+        # next, try to use openmp from `brew install libomp`\n+        if not omp_available:\n+            omp_available, libomp_path = homebrew_libomp()\n+            if omp_available:\n+                include_dir_paths.append(os.path.join(libomp_path, \"include\"))\n+                lib_dir_paths.append(os.path.join(libomp_path, \"lib\"))\n+\n+        # if openmp is still not available, we let the compiler to have a try,\n+        # and raise error together with instructions at compilation error later\n+    elif _IS_WINDOWS:\n+        # /openmp, /openmp:llvm\n+        # llvm on Windows, new openmp: https://devblogs.microsoft.com/cppblog/msvc-openmp-update/\n+        # msvc openmp: https://learn.microsoft.com/zh-cn/cpp/build/reference/openmp-enable-openmp-2-0-support?view=msvc-170\n+\n+        cflags.append(\"openmp\")\n+        libs = []\n+    else:\n+        if config.is_fbcode():\n+            libs.append(\"omp\")\n+            include_dir_paths.append(build_paths.openmp())\n+        else:\n+            if _is_clang(cpp_compiler):\n+                # TODO: fix issue, can't find omp.h\n+                cflags.append(\"fopenmp\")\n+                libs.append(\"gomp\")\n+            else:\n+                cflags.append(\"fopenmp\")\n+                libs.append(\"gomp\")\n+\n+    return cflags, ldflags, include_dir_paths, lib_dir_paths, libs\n+\n+\n+def get_mmap_self_macro(use_mmap_weights: bool) -> List[str]:\n+    macros = []\n+    if use_mmap_weights:\n+        macros.append(\" USE_MMAP_SELF\")\n+    return macros\n+\n+\n+def get_cpp_torch_options(\n+    cpp_compiler,\n+    chosen_isa: VecISA,\n+    include_pytorch: bool,\n+    aot_mode: bool,\n+    compile_only: bool,\n+    use_mmap_weights: bool,\n+):\n+    definations: List[str] = []\n+    include_dirs: List[str] = []\n+    cflags: List[str] = []\n+    ldflags: List[str] = []\n+    libraries_dirs: List[str] = []\n+    libraries: List[str] = []\n+    passthough_args: List[str] = []\n+\n+    torch_cpp_wrapper_definations = _get_torch_cpp_wrapper_defination()\n+    use_custom_generated_macros_definations = _use_custom_generated_macros()\n+\n+    sys_dir_header_cflags, sys_dir_header_include_dirs = _use_standard_sys_dir_headers()\n+\n+    isa_macros, isa_ps_args_build_flags = _get_build_args_of_chosen_isa(chosen_isa)\n+\n+    (\n+        torch_include_dirs,\n+        torch_libraries_dirs,\n+        torch_libraries,\n+    ) = _get_torch_related_args(include_pytorch=include_pytorch, aot_mode=aot_mode)\n+\n+    python_include_dirs, python_libraries_dirs = _get_python_related_args()\n+\n+    (\n+        omp_cflags,\n+        omp_ldflags,\n+        omp_include_dir_paths,\n+        omp_lib_dir_paths,\n+        omp_lib,\n+    ) = _get_openmp_args(cpp_compiler)\n+\n+    cxx_abi_passthough_args = _get_glibcxx_abi_build_flags()\n+    fb_macro_passthough_args = _use_fb_internal_macros()\n+\n+    mmap_self_macros = get_mmap_self_macro(use_mmap_weights)\n+\n+    definations = (\n+        torch_cpp_wrapper_definations\n+        + use_custom_generated_macros_definations\n+        + isa_macros\n+        + fb_macro_passthough_args\n+        + mmap_self_macros\n+    )\n+    include_dirs = (\n+        sys_dir_header_include_dirs\n+        + python_include_dirs\n+        + torch_include_dirs\n+        + omp_include_dir_paths\n+    )\n+    cflags = sys_dir_header_cflags + omp_cflags\n+    ldflags = omp_ldflags\n+    libraries_dirs = python_libraries_dirs + torch_libraries_dirs + omp_lib_dir_paths\n+    libraries = torch_libraries + omp_lib\n+    passthough_args = isa_ps_args_build_flags + cxx_abi_passthough_args\n+\n+    return (\n+        definations,\n+        include_dirs,\n+        cflags,\n+        ldflags,\n+        libraries_dirs,\n+        libraries,\n+        passthough_args,\n+    )\n+\n+\n+class CppTorchOptions(CppOptions):\n+    \"\"\"\n+    This class is inherited from CppTorchOptions, which automatic contains\n+    base cxx build options. And then it will maintains torch related build\n+    args.\n+    1. Torch include_directories, libraries, libraries_directories.\n+    2. Python include_directories, libraries, libraries_directories.\n+    3. OpenMP related.\n+    4. Torch MACROs.\n+    5. MISC\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        chosen_isa: VecISA,\n+        include_pytorch: bool = False,\n+        warning_all: bool = True,\n+        aot_mode: bool = False,\n+        compile_only: bool = False,\n+        use_mmap_weights: bool = False,\n+    ) -> None:\n+        super().__init__(compile_only=compile_only, warning_all=warning_all)\n+\n+        self._aot_mode = aot_mode\n+\n+        (\n+            torch_definations,\n+            torch_include_dirs,\n+            torch_cflags,\n+            torch_ldflags,\n+            torch_libraries_dirs,\n+            torch_libraries,\n+            torch_passthough_args,\n+        ) = get_cpp_torch_options(\n+            cpp_compiler=self._compiler,\n+            chosen_isa=chosen_isa,\n+            include_pytorch=include_pytorch,\n+            aot_mode=aot_mode,\n+            compile_only=compile_only,\n+            use_mmap_weights=use_mmap_weights,\n+        )\n+\n+        if compile_only:\n+            torch_libraries_dirs = []\n+            torch_libraries = []\n+\n+        _append_list(self._definations, torch_definations)\n+        _append_list(self._include_dirs, torch_include_dirs)\n+        _append_list(self._cflags, torch_cflags)\n+        _append_list(self._ldflags, torch_ldflags)\n+        _append_list(self._libraries_dirs, torch_libraries_dirs)\n+        _append_list(self._libraries, torch_libraries)\n+        _append_list(self._passthough_args, torch_passthough_args)\n+        self._remove_duplicate_options()\n+\n+\n+def _get_cuda_related_args(aot_mode: bool):\n+    definations: List[str] = []\n+    include_dirs: List[str] = []\n+    cflags: List[str] = []\n+    ldflags: List[str] = []\n+    libraries_dirs: List[str] = []\n+    libraries: List[str] = []\n+    passthough_args: List[str] = []\n+\n+    # if not use cuda, don't call this function.\n+    use_cuda = True\n+\n+    if (\n+        config.is_fbcode()\n+        and \"CUDA_HOME\" not in os.environ\n+        and \"CUDA_PATH\" not in os.environ\n+    ):\n+        os.environ[\"CUDA_HOME\"] = os.path.dirname(build_paths.cuda())\n+\n+    from torch.utils import cpp_extension\n+\n+    include_dirs = cpp_extension.include_paths(use_cuda)\n+    libraries_dirs = cpp_extension.library_paths(use_cuda)\n+\n+    definations.append(\" USE_ROCM\" if torch.version.hip else \" USE_CUDA\")\n+\n+    if torch.version.hip is not None:\n+        if config.is_fbcode():\n+            libraries += [\"amdhip64\"]\n+        else:\n+            libraries += [\"c10_hip\", \"torch_hip\"]\n+            definations.append(\" __HIP_PLATFORM_AMD__\")\n+    else:\n+        if config.is_fbcode():\n+            libraries += [\"cuda\"]\n+        else:\n+            if config.is_fbcode():\n+                libraries += [\"cuda\"]\n+            else:\n+                libraries += [\"c10_cuda\", \"cuda\", \"torch_cuda\"]\n+\n+    if aot_mode:\n+        cpp_prefix_include_dir = [f\"{os.path.dirname(_cpp_prefix_path())}\"]\n+        include_dirs += cpp_prefix_include_dir\n+\n+        if config.is_fbcode():\n+            include_dirs.append(build_paths.cuda())\n+            # This is a special treatment for Meta internal cuda-12 where all libs\n+            # are in lib/cuda-12 and lib/cuda-12/stubs\n+\n+            # TODO: Verify process libcudart_static.a\n+            for i, path in enumerate(libraries_dirs):\n+                if path.startswith(os.environ[\"CUDA_HOME\"]) and not os.path.exists(\n+                    f\"{path}/libcudart_static.a\"\n+                ):\n+                    for root, dirs, files in os.walk(path):\n+                        if \"libcudart_static.a\" in files:\n+                            libraries_dirs[i] = os.path.join(path, root)\n+                            libraries_dirs.append(\n+                                os.path.join(libraries_dirs[i], \"stubs\")\n+                            )\n+                            break\n+        else:\n+            if not _IS_WINDOWS:\n+                # TODO: make static link better on Linux.\n+                passthough_args = [\"-Wl,-Bstatic -lcudart_static -Wl,-Bdynamic\"]\n+            else:\n+                libraries.append(\"cudart_static\")\n+\n+    return (\n+        definations,\n+        include_dirs,\n+        cflags,\n+        ldflags,\n+        libraries_dirs,\n+        libraries,\n+        passthough_args,\n+    )\n+\n+\n+def get_cpp_torch_cuda_options(aot_mode: bool = False):\n+    definations: List[str] = []\n+    include_dirs: List[str] = []\n+    cflags: List[str] = []\n+    ldflags: List[str] = []\n+    libraries_dirs: List[str] = []\n+    libraries: List[str] = []\n+    passthough_args: List[str] = []\n+\n+    (\n+        definations,\n+        include_dirs,\n+        cflags,\n+        ldflags,\n+        libraries_dirs,\n+        libraries,\n+        passthough_args,\n+    ) = _get_cuda_related_args(aot_mode)\n+\n+    return (\n+        definations,\n+        include_dirs,\n+        cflags,\n+        ldflags,\n+        libraries_dirs,\n+        libraries,\n+        passthough_args,\n+    )\n+\n+\n+class CppTorchCudaOptions(CppTorchOptions):\n+    \"\"\"\n+    This class is inherited from CppTorchOptions, which automatic contains\n+    base cxx build options and torch common build options. And then it will\n+    maintains cuda device related build args.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        chosen_isa: VecISA,\n+        include_pytorch: bool = False,\n+        use_cuda: bool = True,\n+        aot_mode: bool = False,\n+        compile_only: bool = False,\n+        use_mmap_weights: bool = False,\n+    ) -> None:\n+        # from torch._inductor.codecache import pick_vec_isa\n+\n+        super().__init__(\n+            chosen_isa=chosen_isa,\n+            include_pytorch=include_pytorch,\n+            aot_mode=aot_mode,\n+            compile_only=compile_only,\n+            use_mmap_weights=use_mmap_weights,\n+        )\n+\n+        cuda_definations: List[str] = []\n+        cuda_include_dirs: List[str] = []\n+        cuda_cflags: List[str] = []\n+        cuda_ldflags: List[str] = []\n+        cuda_libraries_dirs: List[str] = []\n+        cuda_libraries: List[str] = []\n+        cuda_passthough_args: List[str] = []\n+\n+        if use_cuda:\n+            (\n+                cuda_definations,\n+                cuda_include_dirs,\n+                cuda_cflags,\n+                cuda_ldflags,\n+                cuda_libraries_dirs,\n+                cuda_libraries,\n+                cuda_passthough_args,\n+            ) = get_cpp_torch_cuda_options(aot_mode=aot_mode)\n+\n+        if compile_only:\n+            cuda_libraries_dirs = []\n+            cuda_libraries = []\n+\n+        _append_list(self._definations, cuda_definations)\n+        _append_list(self._include_dirs, cuda_include_dirs)\n+        _append_list(self._cflags, cuda_cflags)\n+        _append_list(self._ldflags, cuda_ldflags)\n+        _append_list(self._libraries_dirs, cuda_libraries_dirs)\n+        _append_list(self._libraries, cuda_libraries)\n+        _append_list(self._passthough_args, cuda_passthough_args)\n+        self._remove_duplicate_options()\n+\n+\n+def get_name_and_dir_from_output_file_path(\n+    aot_mode: bool, use_absolute_path: bool, file_path: str\n+):\n+    name_and_ext = os.path.basename(file_path)\n+    name, ext = os.path.splitext(name_and_ext)\n+    dir = os.path.dirname(file_path)\n+\n+    if config.is_fbcode():\n+        if not (aot_mode and not use_absolute_path):\n+            dir = \".\"\n+    return name, dir\n+\n+\n+class CppBuilder:\n+    \"\"\"\n+    CppBuilder is a cpp jit builder, and it supports both Windows, Linux and MacOS.\n+    Args:\n+        name:\n+            1. Build target name, the final target file will append extension type automatically.\n+            2. Due to the CppBuilder is supports mutliple OS, it will maintains ext for OS difference.\n+\n+        sources:\n+            Source code file list to be built.\n+\n+        BuildOption:\n+            Build options to the builder.\n+\n+        output_dir:\n+            1. The output_dir the taget file will output to.\n+            2. The default value is empty string, and then the use current dir as output dir.\n+            3. Final target file: output_dir/name.ext\n+    \"\"\"\n+\n+    def get_shared_lib_ext(self) -> str:\n+        SHARED_LIB_EXT = \".dll\" if _IS_WINDOWS else \".so\"\n+        return SHARED_LIB_EXT\n+\n+    def get_object_ext(self) -> str:\n+        EXT = \".obj\" if _IS_WINDOWS else \".o\"\n+        return EXT\n+\n+    def __init__(\n+        self,\n+        name: str,\n+        sources: Union[str, List[str]],\n+        BuildOption: BuildOptionsBase,\n+        output_dir: str = \"\",\n+        compile_only: bool = False,\n+        use_absolute_path: bool = False,\n+    ) -> None:\n+        self._compiler = \"\"\n+        self._cflags_args = \"\"\n+        self._definations_args = \"\"\n+        self._include_dirs_args = \"\"\n+        self._ldflags_args = \"\"\n+        self._libraries_dirs_args = \"\"\n+        self._libraries_args = \"\"\n+        self._passthough_parameters_args = \"\"\n+\n+        self._output_dir = \"\"\n+        self._target_file = \"\"\n+\n+        self._name = name\n+\n+        if isinstance(sources, str):\n+            sources = [sources]\n+\n+        if config.is_fbcode():\n+            if BuildOption.get_aot_mode() and not use_absolute_path:\n+                inp_name = sources\n+                # output process @ get_name_and_dir_from_output_file_path\n+            else:\n+                # We need to copy any absolute-path torch includes\n+                inp_name = [os.path.basename(i) for i in sources]\n+\n+            self._sources_args = \" \".join(inp_name)\n+\n+            if _is_clang(self._compiler):\n+                self._passthough_parameters_args += \" --rtlib=compiler-rt\"\n+                self._passthough_parameters_args += \" -fuse-ld=lld\"\n+                self._passthough_parameters_args += \" -B\" + build_paths.glibc_lib()\n+                self._passthough_parameters_args += \" -L\" + build_paths.glibc_lib()\n+        else:\n+            self._sources_args = \" \".join(sources)\n+\n+        self._compile_only = compile_only\n+\n+        if len(output_dir) == 0:\n+            self._output_dir = os.path.dirname(os.path.abspath(__file__))\n+        else:\n+            self._output_dir = output_dir\n+\n+        file_ext = self.get_object_ext() if compile_only else self.get_shared_lib_ext()\n+        self._target_file = os.path.join(self._output_dir, f\"{self._name}{file_ext}\")\n+\n+        self._compiler = BuildOption.get_compiler()\n+\n+        for cflag in BuildOption.get_cflags():\n+            if _IS_WINDOWS:\n+                self._cflags_args += f\"/{cflag} \"\n+            else:\n+                self._cflags_args += f\"-{cflag} \"\n+\n+        for defination in BuildOption.get_definations():\n+            if _IS_WINDOWS:\n+                self._definations_args += f\"/D {defination} \"\n+            else:\n+                self._definations_args += f\"-D{defination} \"\n+\n+        for inc_dir in BuildOption.get_include_dirs():\n+            if _IS_WINDOWS:\n+                self._include_dirs_args += f\"/I {inc_dir} \"\n+            else:\n+                self._include_dirs_args += f\"-I{inc_dir} \"\n+\n+        for ldflag in BuildOption.get_ldflags():\n+            if _IS_WINDOWS:\n+                self._ldflags_args += f\"/{ldflag} \"\n+            else:\n+                self._ldflags_args += f\"-{ldflag} \"\n+\n+        for lib_dir in BuildOption.get_libraries_dirs():\n+            if _IS_WINDOWS:\n+                self._libraries_dirs_args += f'/LIBPATH:\"{lib_dir}\" '\n+            else:\n+                self._libraries_dirs_args += f\"-L{lib_dir} \"\n+\n+        for lib in BuildOption.get_libraries():\n+            if _IS_WINDOWS:\n+                self._libraries_args += f'\"{lib}.lib\" '\n+            else:\n+                self._libraries_args += f\"-l{lib} \"\n+\n+        for passthough_arg in BuildOption.get_passthough_args():\n+            self._passthough_parameters_args += f\"{passthough_arg} \"\n+\n+    def get_command_line(self) -> str:\n+        def format_build_command(\n+            compiler,\n+            sources,\n+            include_dirs_args,\n+            definations_args,\n+            cflags_args,\n+            ldflags_args,\n+            libraries_args,\n+            libraries_dirs_args,\n+            passthougn_args,\n+            target_file,\n+        ):\n+            if _IS_WINDOWS:\n+                # https://learn.microsoft.com/en-us/cpp/build/walkthrough-compile-a-c-program-on-the-command-line?view=msvc-1704\n+                # https://stackoverflow.com/a/31566153\n+                cmd = (\n+                    f\"{compiler} {include_dirs_args} {definations_args} {cflags_args} {sources} \"\n+                    f\"{passthougn_args} /LD /Fe{target_file} /link {libraries_dirs_args} {libraries_args} {ldflags_args} \"\n+                )\n+                cmd = cmd.replace(\"\\\\\", \"/\")\n+            else:\n+                compile_only_arg = \"-c\" if self._compile_only else \"\"\n+                cmd = re.sub(\n+                    r\"[ \\n]+\",\n+                    \" \",\n+                    f\"\"\"\n+                    {compiler} {sources} {definations_args} {cflags_args} {include_dirs_args}\n+                    {passthougn_args} {ldflags_args} {libraries_args} {libraries_dirs_args} {compile_only_arg} -o {target_file}\n+                    \"\"\",\n+                ).strip()\n+            return cmd\n+\n+        command_line = format_build_command(\n+            compiler=self._compiler,\n+            sources=self._sources_args,\n+            include_dirs_args=self._include_dirs_args,\n+            definations_args=self._definations_args,\n+            cflags_args=self._cflags_args,\n+            ldflags_args=self._ldflags_args,\n+            libraries_args=self._libraries_args,\n+            libraries_dirs_args=self._libraries_dirs_args,\n+            passthougn_args=self._passthough_parameters_args,\n+            target_file=self._target_file,\n+        )\n+        return command_line\n+\n+    def get_target_file_path(self):\n+        return self._target_file\n+\n+    def convert_to_cpp_extension_args(self):\n+        include_dirs = self._include_dirs_args\n+        cflags = (\n+            self._cflags_args\n+            + self._definations_args\n+            + self._passthough_parameters_args\n+        )\n+        ldflags = self._ldflags_args + self._libraries_args + self._libraries_dirs_args\n+\n+        return include_dirs, cflags, ldflags\n+\n+    def build(self) -> Tuple[int, str]:\n+        \"\"\"\n+        It is must need a temperary directory to store object files in Windows.\n+        After build completed, delete the temperary directory to save disk space.\n+        \"\"\"\n+        _create_if_dir_not_exist(self._output_dir)\n+        _build_tmp_dir = os.path.join(\n+            self._output_dir, f\"{self._name}_{_BUILD_TEMP_DIR}\"\n+        )\n+        _create_if_dir_not_exist(_build_tmp_dir)\n+\n+        build_cmd = self.get_command_line()\n+\n+        status = run_command_line(build_cmd, cwd=_build_tmp_dir)\n+\n+        _remove_dir(_build_tmp_dir)\n+        return status, self._target_file\n"
        },
        {
            "name": "__init__.py",
            "path": "torch/cpu/__init__.py",
            "patches": [
                {
                    "old_start": 27,
                    "old_length": 6,
                    "new_start": 27,
                    "new_length": 16,
                    "hunk": "@@ -27,6 +27,16 @@ __all__ = [\n _device_t = Union[_device, str, int, None]\n \n \n+def _is_cpu_support_avx2() -> bool:\n+    r\"\"\"Returns a bool indicating if CPU supports AVX2.\"\"\"\n+    return torch._C._cpu._is_cpu_support_avx2()\n+\n+\n+def _is_cpu_support_avx512() -> bool:\n+    r\"\"\"Returns a bool indicating if CPU supports AVX512.\"\"\"\n+    return torch._C._cpu._is_cpu_support_avx512()\n+\n+\n def _is_cpu_support_vnni() -> bool:\n     r\"\"\"Returns a bool indicating if CPU supports VNNI.\"\"\"\n     return torch._C._cpu._is_cpu_support_vnni()\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+def _is_cpu_support_avx2() -> bool:\n+    r\"\"\"Returns a bool indicating if CPU supports AVX2.\"\"\"\n+    return torch._C._cpu._is_cpu_support_avx2()\n+\n+\n+def _is_cpu_support_avx512() -> bool:\n+    r\"\"\"Returns a bool indicating if CPU supports AVX512.\"\"\"\n+    return torch._C._cpu._is_cpu_support_avx512()\n+\n+\n",
            "whole_hunk": "@@ -27,6 +27,16 @@ __all__ = [\n _device_t = Union[_device, str, int, None]\n \n \n+def _is_cpu_support_avx2() -> bool:\n+    r\"\"\"Returns a bool indicating if CPU supports AVX2.\"\"\"\n+    return torch._C._cpu._is_cpu_support_avx2()\n+\n+\n+def _is_cpu_support_avx512() -> bool:\n+    r\"\"\"Returns a bool indicating if CPU supports AVX512.\"\"\"\n+    return torch._C._cpu._is_cpu_support_avx512()\n+\n+\n def _is_cpu_support_vnni() -> bool:\n     r\"\"\"Returns a bool indicating if CPU supports VNNI.\"\"\"\n     return torch._C._cpu._is_cpu_support_vnni()\n"
        },
        {
            "name": "Module.cpp",
            "path": "torch/csrc/cpu/Module.cpp",
            "patches": [
                {
                    "old_start": 2,
                    "old_length": 15,
                    "new_start": 2,
                    "new_length": 15,
                    "hunk": "@@ -2,15 +2,15 @@\n #include <torch/csrc/cpu/Module.h>\n #include <torch/csrc/utils/pybind.h>\n \n-namespace torch {\n-namespace cpu {\n+namespace torch::cpu {\n \n void initModule(PyObject* module) {\n   auto m = py::handle(module).cast<py::module>();\n \n   auto cpu = m.def_submodule(\"_cpu\", \"cpu related pybind.\");\n+  cpu.def(\"_is_cpu_support_avx2\", at::cpu::is_cpu_support_avx2);\n+  cpu.def(\"_is_cpu_support_avx512\", at::cpu::is_cpu_support_avx512);\n   cpu.def(\"_is_cpu_support_vnni\", at::cpu::is_cpu_support_vnni);\n }\n \n-} // namespace cpu\n-} // namespace torch\n+} // namespace torch::cpu"
                }
            ],
            "whole_deleted": "-namespace torch {\n-namespace cpu {\n-} // namespace cpu\n-} // namespace torch\n",
            "whole_added": "+namespace torch::cpu {\n+  cpu.def(\"_is_cpu_support_avx2\", at::cpu::is_cpu_support_avx2);\n+  cpu.def(\"_is_cpu_support_avx512\", at::cpu::is_cpu_support_avx512);\n+} // namespace torch::cpu\n",
            "whole_hunk": "@@ -2,15 +2,15 @@\n #include <torch/csrc/cpu/Module.h>\n #include <torch/csrc/utils/pybind.h>\n \n-namespace torch {\n-namespace cpu {\n+namespace torch::cpu {\n \n void initModule(PyObject* module) {\n   auto m = py::handle(module).cast<py::module>();\n \n   auto cpu = m.def_submodule(\"_cpu\", \"cpu related pybind.\");\n+  cpu.def(\"_is_cpu_support_avx2\", at::cpu::is_cpu_support_avx2);\n+  cpu.def(\"_is_cpu_support_avx512\", at::cpu::is_cpu_support_avx512);\n   cpu.def(\"_is_cpu_support_vnni\", at::cpu::is_cpu_support_vnni);\n }\n \n-} // namespace cpu\n-} // namespace torch\n+} // namespace torch::cpu"
        }
    ]
},
{
    "Id": 75,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/7d33ff59ba4ed920f590cb3e8f3e1bd571c78f62",
    "date": "2024-06-19T15:57:21+00:00",
    "message": "[Split Build]Use same package (#127934)\n\nThis PR removes the second separate package we were using for the libtorch wheel.\nIn terms of testing that this works we will look use the PRs above this in the stack.\n\nAs for sanity checking these are the wheels that are produced by running\n```\npython setup.py clean && BUILD_LIBTORCH_WHL=1 with-proxy python setup.py bdist_whee\nl && BUILD_PYTHON_ONLY=1 with-proxy python setup.py bdist_wheel --cmake\n```\n\n```\nsahanp@devgpu086 ~/pytorch ((5f15e171\u2026))> ls -al dist/                                                        (pytorch-3.10)\ntotal 677236\ndrwxr-xr-x 1 sahanp users       188 Jun  4 12:19 ./\ndrwxr-xr-x 1 sahanp users      1696 Jun  4 12:59 ../\n-rw-r--r-- 1 sahanp users  81405742 Jun  4 12:19 torch-2.4.0a0+gitca0a73c-cp310-cp310-linux_x86_64.whl\n-rw-r--r-- 1 sahanp users 612076919 Jun  4 12:19 libtorch-2.4.0a0+gitca0a73c-py3-none-any.whl\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/127934\nApproved by: https://github.com/atalman",
    "label": "NO",
    "changes": [
        {
            "name": "build.sh",
            "path": ".ci/pytorch/build.sh",
            "patches": [
                {
                    "old_start": 284,
                    "old_length": 12,
                    "new_start": 284,
                    "new_length": 26,
                    "hunk": "@@ -284,12 +284,26 @@ else\n         # Which should be backward compatible with Numpy-1.X\n         python -mpip install --pre numpy==2.0.0rc1\n       fi\n-      WERROR=1 python setup.py bdist_wheel\n+\n+      WERROR=1 python setup.py clean\n+\n+      if [[ \"$USE_SPLIT_BUILD\" == \"true\" ]]; then\n+        BUILD_LIBTORCH_WHL=1 BUILD_PYTHON_ONLY=0 python setup.py bdist_wheel\n+        BUILD_LIBTORCH_WHL=0 BUILD_PYTHON_ONLY=1 python setup.py bdist_wheel --cmake\n+      else\n+        WERROR=1 python setup.py bdist_wheel\n+      fi\n     else\n+      python setup.py clean\n       if [[ \"$BUILD_ENVIRONMENT\" == *xla* ]]; then\n         source .ci/pytorch/install_cache_xla.sh\n       fi\n-      python setup.py bdist_wheel\n+      if [[ \"$USE_SPLIT_BUILD\" == \"true\" ]]; then\n+        echo \"USE_SPLIT_BUILD cannot be used with xla or rocm\"\n+        exit 1\n+      else\n+        python setup.py bdist_wheel\n+      fi\n     fi\n     pip_install_whl \"$(echo dist/*.whl)\"\n \n"
                },
                {
                    "old_start": 328,
                    "old_length": 9,
                    "new_start": 342,
                    "new_length": 10,
                    "hunk": "@@ -328,9 +342,10 @@ else\n     CUSTOM_OP_TEST=\"$PWD/test/custom_operator\"\n     python --version\n     SITE_PACKAGES=\"$(python -c 'from distutils.sysconfig import get_python_lib; print(get_python_lib())')\"\n+\n     mkdir -p \"$CUSTOM_OP_BUILD\"\n     pushd \"$CUSTOM_OP_BUILD\"\n-    cmake \"$CUSTOM_OP_TEST\" -DCMAKE_PREFIX_PATH=\"$SITE_PACKAGES/torch\" -DPython_EXECUTABLE=\"$(which python)\" \\\n+    cmake \"$CUSTOM_OP_TEST\" -DCMAKE_PREFIX_PATH=\"$SITE_PACKAGES/torch;$SITE_PACKAGES\" -DPython_EXECUTABLE=\"$(which python)\" \\\n           -DCMAKE_MODULE_PATH=\"$CUSTOM_TEST_MODULE_PATH\" -DUSE_ROCM=\"$CUSTOM_TEST_USE_ROCM\"\n     make VERBOSE=1\n     popd\n"
                },
                {
                    "old_start": 343,
                    "old_length": 7,
                    "new_start": 358,
                    "new_length": 7,
                    "hunk": "@@ -343,7 +358,7 @@ else\n     SITE_PACKAGES=\"$(python -c 'from distutils.sysconfig import get_python_lib; print(get_python_lib())')\"\n     mkdir -p \"$JIT_HOOK_BUILD\"\n     pushd \"$JIT_HOOK_BUILD\"\n-    cmake \"$JIT_HOOK_TEST\" -DCMAKE_PREFIX_PATH=\"$SITE_PACKAGES/torch\" -DPython_EXECUTABLE=\"$(which python)\" \\\n+    cmake \"$JIT_HOOK_TEST\" -DCMAKE_PREFIX_PATH=\"$SITE_PACKAGES/torch;$SITE_PACKAGES\" -DPython_EXECUTABLE=\"$(which python)\" \\\n           -DCMAKE_MODULE_PATH=\"$CUSTOM_TEST_MODULE_PATH\" -DUSE_ROCM=\"$CUSTOM_TEST_USE_ROCM\"\n     make VERBOSE=1\n     popd\n"
                },
                {
                    "old_start": 355,
                    "old_length": 7,
                    "new_start": 370,
                    "new_length": 7,
                    "hunk": "@@ -355,7 +370,7 @@ else\n     python --version\n     mkdir -p \"$CUSTOM_BACKEND_BUILD\"\n     pushd \"$CUSTOM_BACKEND_BUILD\"\n-    cmake \"$CUSTOM_BACKEND_TEST\" -DCMAKE_PREFIX_PATH=\"$SITE_PACKAGES/torch\" -DPython_EXECUTABLE=\"$(which python)\" \\\n+    cmake \"$CUSTOM_BACKEND_TEST\" -DCMAKE_PREFIX_PATH=\"$SITE_PACKAGES/torch;$SITE_PACKAGES\" -DPython_EXECUTABLE=\"$(which python)\" \\\n           -DCMAKE_MODULE_PATH=\"$CUSTOM_TEST_MODULE_PATH\" -DUSE_ROCM=\"$CUSTOM_TEST_USE_ROCM\"\n     make VERBOSE=1\n     popd\n"
                }
            ],
            "whole_deleted": "-      WERROR=1 python setup.py bdist_wheel\n-      python setup.py bdist_wheel\n-    cmake \"$CUSTOM_OP_TEST\" -DCMAKE_PREFIX_PATH=\"$SITE_PACKAGES/torch\" -DPython_EXECUTABLE=\"$(which python)\" \\\n-    cmake \"$JIT_HOOK_TEST\" -DCMAKE_PREFIX_PATH=\"$SITE_PACKAGES/torch\" -DPython_EXECUTABLE=\"$(which python)\" \\\n-    cmake \"$CUSTOM_BACKEND_TEST\" -DCMAKE_PREFIX_PATH=\"$SITE_PACKAGES/torch\" -DPython_EXECUTABLE=\"$(which python)\" \\\n",
            "whole_added": "+\n+      WERROR=1 python setup.py clean\n+\n+      if [[ \"$USE_SPLIT_BUILD\" == \"true\" ]]; then\n+        BUILD_LIBTORCH_WHL=1 BUILD_PYTHON_ONLY=0 python setup.py bdist_wheel\n+        BUILD_LIBTORCH_WHL=0 BUILD_PYTHON_ONLY=1 python setup.py bdist_wheel --cmake\n+      else\n+        WERROR=1 python setup.py bdist_wheel\n+      fi\n+      python setup.py clean\n+      if [[ \"$USE_SPLIT_BUILD\" == \"true\" ]]; then\n+        echo \"USE_SPLIT_BUILD cannot be used with xla or rocm\"\n+        exit 1\n+      else\n+        python setup.py bdist_wheel\n+      fi\n+\n+    cmake \"$CUSTOM_OP_TEST\" -DCMAKE_PREFIX_PATH=\"$SITE_PACKAGES/torch;$SITE_PACKAGES\" -DPython_EXECUTABLE=\"$(which python)\" \\\n+    cmake \"$JIT_HOOK_TEST\" -DCMAKE_PREFIX_PATH=\"$SITE_PACKAGES/torch;$SITE_PACKAGES\" -DPython_EXECUTABLE=\"$(which python)\" \\\n+    cmake \"$CUSTOM_BACKEND_TEST\" -DCMAKE_PREFIX_PATH=\"$SITE_PACKAGES/torch;$SITE_PACKAGES\" -DPython_EXECUTABLE=\"$(which python)\" \\\n",
            "whole_hunk": "@@ -284,12 +284,26 @@ else\n         # Which should be backward compatible with Numpy-1.X\n         python -mpip install --pre numpy==2.0.0rc1\n       fi\n-      WERROR=1 python setup.py bdist_wheel\n+\n+      WERROR=1 python setup.py clean\n+\n+      if [[ \"$USE_SPLIT_BUILD\" == \"true\" ]]; then\n+        BUILD_LIBTORCH_WHL=1 BUILD_PYTHON_ONLY=0 python setup.py bdist_wheel\n+        BUILD_LIBTORCH_WHL=0 BUILD_PYTHON_ONLY=1 python setup.py bdist_wheel --cmake\n+      else\n+        WERROR=1 python setup.py bdist_wheel\n+      fi\n     else\n+      python setup.py clean\n       if [[ \"$BUILD_ENVIRONMENT\" == *xla* ]]; then\n         source .ci/pytorch/install_cache_xla.sh\n       fi\n-      python setup.py bdist_wheel\n+      if [[ \"$USE_SPLIT_BUILD\" == \"true\" ]]; then\n+        echo \"USE_SPLIT_BUILD cannot be used with xla or rocm\"\n+        exit 1\n+      else\n+        python setup.py bdist_wheel\n+      fi\n     fi\n     pip_install_whl \"$(echo dist/*.whl)\"\n \n@@ -328,9 +342,10 @@ else\n     CUSTOM_OP_TEST=\"$PWD/test/custom_operator\"\n     python --version\n     SITE_PACKAGES=\"$(python -c 'from distutils.sysconfig import get_python_lib; print(get_python_lib())')\"\n+\n     mkdir -p \"$CUSTOM_OP_BUILD\"\n     pushd \"$CUSTOM_OP_BUILD\"\n-    cmake \"$CUSTOM_OP_TEST\" -DCMAKE_PREFIX_PATH=\"$SITE_PACKAGES/torch\" -DPython_EXECUTABLE=\"$(which python)\" \\\n+    cmake \"$CUSTOM_OP_TEST\" -DCMAKE_PREFIX_PATH=\"$SITE_PACKAGES/torch;$SITE_PACKAGES\" -DPython_EXECUTABLE=\"$(which python)\" \\\n           -DCMAKE_MODULE_PATH=\"$CUSTOM_TEST_MODULE_PATH\" -DUSE_ROCM=\"$CUSTOM_TEST_USE_ROCM\"\n     make VERBOSE=1\n     popd\n@@ -343,7 +358,7 @@ else\n     SITE_PACKAGES=\"$(python -c 'from distutils.sysconfig import get_python_lib; print(get_python_lib())')\"\n     mkdir -p \"$JIT_HOOK_BUILD\"\n     pushd \"$JIT_HOOK_BUILD\"\n-    cmake \"$JIT_HOOK_TEST\" -DCMAKE_PREFIX_PATH=\"$SITE_PACKAGES/torch\" -DPython_EXECUTABLE=\"$(which python)\" \\\n+    cmake \"$JIT_HOOK_TEST\" -DCMAKE_PREFIX_PATH=\"$SITE_PACKAGES/torch;$SITE_PACKAGES\" -DPython_EXECUTABLE=\"$(which python)\" \\\n           -DCMAKE_MODULE_PATH=\"$CUSTOM_TEST_MODULE_PATH\" -DUSE_ROCM=\"$CUSTOM_TEST_USE_ROCM\"\n     make VERBOSE=1\n     popd\n@@ -355,7 +370,7 @@ else\n     python --version\n     mkdir -p \"$CUSTOM_BACKEND_BUILD\"\n     pushd \"$CUSTOM_BACKEND_BUILD\"\n-    cmake \"$CUSTOM_BACKEND_TEST\" -DCMAKE_PREFIX_PATH=\"$SITE_PACKAGES/torch\" -DPython_EXECUTABLE=\"$(which python)\" \\\n+    cmake \"$CUSTOM_BACKEND_TEST\" -DCMAKE_PREFIX_PATH=\"$SITE_PACKAGES/torch;$SITE_PACKAGES\" -DPython_EXECUTABLE=\"$(which python)\" \\\n           -DCMAKE_MODULE_PATH=\"$CUSTOM_TEST_MODULE_PATH\" -DUSE_ROCM=\"$CUSTOM_TEST_USE_ROCM\"\n     make VERBOSE=1\n     popd\n"
        },
        {
            "name": "setup.py",
            "path": "setup.py",
            "patches": [
                {
                    "old_start": 199,
                    "old_length": 7,
                    "new_start": 199,
                    "new_length": 6,
                    "hunk": "@@ -199,7 +199,6 @@\n #      Builds pytorch as a wheel using libtorch.so from a seperate wheel\n \n import os\n-import pkgutil\n import sys\n \n if sys.platform == \"win32\" and sys.maxsize.bit_length() == 31:\n"
                },
                {
                    "old_start": 210,
                    "old_length": 19,
                    "new_start": 209,
                    "new_length": 6,
                    "hunk": "@@ -210,19 +209,6 @@ if sys.platform == \"win32\" and sys.maxsize.bit_length() == 31:\n \n import platform\n \n-\n-def _get_package_path(package_name):\n-    loader = pkgutil.find_loader(package_name)\n-    if loader:\n-        # The package might be a namespace package, so get_data may fail\n-        try:\n-            file_path = loader.get_filename()\n-            return os.path.dirname(file_path)\n-        except AttributeError:\n-            pass\n-    return None\n-\n-\n BUILD_LIBTORCH_WHL = os.getenv(\"BUILD_LIBTORCH_WHL\", \"0\") == \"1\"\n BUILD_PYTHON_ONLY = os.getenv(\"BUILD_PYTHON_ONLY\", \"0\") == \"1\"\n \n"
                },
                {
                    "old_start": 237,
                    "old_length": 6,
                    "new_start": 223,
                    "new_length": 7,
                    "hunk": "@@ -237,6 +223,7 @@ if sys.version_info < python_min_version:\n import filecmp\n import glob\n import importlib\n+import importlib.util\n import json\n import shutil\n import subprocess\n"
                },
                {
                    "old_start": 253,
                    "old_length": 15,
                    "new_start": 240,
                    "new_length": 24,
                    "hunk": "@@ -253,15 +240,24 @@ from setuptools.dist import Distribution\n from tools.build_pytorch_libs import build_caffe2\n from tools.generate_torch_version import get_torch_version\n from tools.setup_helpers.cmake import CMake\n-from tools.setup_helpers.env import (\n-    build_type,\n-    IS_DARWIN,\n-    IS_LINUX,\n-    IS_WINDOWS,\n-    LIBTORCH_PKG_NAME,\n-)\n+from tools.setup_helpers.env import build_type, IS_DARWIN, IS_LINUX, IS_WINDOWS\n from tools.setup_helpers.generate_linker_script import gen_linker_script\n \n+\n+def _get_package_path(package_name):\n+    spec = importlib.util.find_spec(package_name)\n+    if spec:\n+        # The package might be a namespace package, so get_data may fail\n+        try:\n+            loader = spec.loader\n+            if loader is not None:\n+                file_path = loader.get_filename()  # type: ignore[attr-defined]\n+                return os.path.dirname(file_path)\n+        except AttributeError:\n+            pass\n+    return None\n+\n+\n # set up appropriate env variables\n if BUILD_LIBTORCH_WHL:\n     # Set up environment variables for ONLY building libtorch.so and not libtorch_python.so\n"
                },
                {
                    "old_start": 271,
                    "old_length": 7,
                    "new_start": 267,
                    "new_length": 7,
                    "hunk": "@@ -271,7 +267,7 @@ if BUILD_LIBTORCH_WHL:\n \n if BUILD_PYTHON_ONLY:\n     os.environ[\"BUILD_LIBTORCHLESS\"] = \"ON\"\n-    os.environ[\"LIBTORCH_LIB_PATH\"] = f\"{_get_package_path(LIBTORCH_PKG_NAME)}/lib\"\n+    os.environ[\"LIBTORCH_LIB_PATH\"] = f\"{_get_package_path('torch')}/lib\"\n \n ################################################################################\n # Parameters parsed from environment\n"
                },
                {
                    "old_start": 347,
                    "old_length": 9,
                    "new_start": 343,
                    "new_length": 12,
                    "hunk": "@@ -347,9 +343,12 @@ cmake_python_include_dir = sysconfig.get_path(\"include\")\n # Version, create_version_file, and package_name\n ################################################################################\n \n-DEFAULT_PACKAGE_NAME = LIBTORCH_PKG_NAME if BUILD_LIBTORCH_WHL else \"torch\"\n+package_name = os.getenv(\"TORCH_PACKAGE_NAME\", \"torch\")\n+LIBTORCH_PKG_NAME = os.getenv(\"LIBTORCH_PACKAGE_NAME\", \"libtorch\")\n+if BUILD_LIBTORCH_WHL:\n+    package_name = LIBTORCH_PKG_NAME\n+\n \n-package_name = os.getenv(\"TORCH_PACKAGE_NAME\", DEFAULT_PACKAGE_NAME)\n package_type = os.getenv(\"PACKAGE_TYPE\", \"wheel\")\n version = get_torch_version()\n report(f\"Building wheel {package_name}-{version}\")\n"
                },
                {
                    "old_start": 1125,
                    "old_length": 8,
                    "new_start": 1123,
                    "new_length": 6,
                    "hunk": "@@ -1125,8 +1123,6 @@ def main():\n         raise RuntimeError(\n             \"Conflict: 'BUILD_LIBTORCH_WHL' and 'BUILD_PYTHON_ONLY' can't both be 1. Set one to 0 and rerun.\"\n         )\n-\n-    # the list of runtime dependencies required by this built package\n     install_requires = [\n         \"filelock\",\n         \"typing-extensions>=4.8.0\",\n"
                },
                {
                    "old_start": 1141,
                    "old_length": 7,
                    "new_start": 1137,
                    "new_length": 7,
                    "hunk": "@@ -1141,7 +1137,7 @@ def main():\n         install_requires.append(\"setuptools\")\n \n     if BUILD_PYTHON_ONLY:\n-        install_requires.append(LIBTORCH_PKG_NAME)\n+        install_requires.append(f\"{LIBTORCH_PKG_NAME}=={get_torch_version()}\")\n \n     use_prioritized_text = str(os.getenv(\"USE_PRIORITIZED_TEXT_FOR_LD\", \"\"))\n     if (\n"
                },
                {
                    "old_start": 1219,
                    "old_length": 6,
                    "new_start": 1214,
                    "new_length": 7,
                    "hunk": "@@ -1219,6 +1214,7 @@ def main():\n         \"utils/data/*.pyi\",\n         \"utils/data/datapipes/*.pyi\",\n         \"lib/*.pdb\",\n+        \"lib/*shm*\",\n         \"lib/torch_shm_manager\",\n         \"lib/*.h\",\n         \"include/*.h\",\n"
                },
                {
                    "old_start": 1383,
                    "old_length": 15,
                    "new_start": 1379,
                    "new_length": 15,
                    "hunk": "@@ -1383,15 +1379,15 @@ def main():\n         \"utils/model_dump/*.mjs\",\n     ]\n \n-    if BUILD_PYTHON_ONLY:\n+    if not BUILD_LIBTORCH_WHL:\n         torch_package_data.extend(\n             [\n-                \"lib/libtorch_python*\",\n-                \"lib/*shm*\",\n-                \"lib/libtorch_global_deps*\",\n+                \"lib/libtorch_python.so\",\n+                \"lib/libtorch_python.dylib\",\n+                \"lib/libtorch_python.dll\",\n             ]\n         )\n-    else:\n+    if not BUILD_PYTHON_ONLY:\n         torch_package_data.extend(\n             [\n                 \"lib/*.so*\",\n"
                },
                {
                    "old_start": 1442,
                    "old_length": 28,
                    "new_start": 1438,
                    "new_length": 18,
                    "hunk": "@@ -1442,28 +1438,18 @@ def main():\n         \"packaged/autograd/*\",\n         \"packaged/autograd/templates/*\",\n     ]\n+    package_data = {\n+        \"torch\": torch_package_data,\n+    }\n \n-    if BUILD_LIBTORCH_WHL:\n-        modified_packages = []\n-        for package in packages:\n-            parts = package.split(\".\")\n-            if parts[0] == \"torch\":\n-                modified_packages.append(DEFAULT_PACKAGE_NAME + package[len(\"torch\") :])\n-        packages = modified_packages\n-        package_dir = {LIBTORCH_PKG_NAME: \"torch\"}\n-        torch_package_dir_name = LIBTORCH_PKG_NAME\n-        package_data = {LIBTORCH_PKG_NAME: torch_package_data}\n-        extensions = []\n+    if not BUILD_LIBTORCH_WHL:\n+        package_data[\"torchgen\"] = torchgen_package_data\n+        package_data[\"caffe2\"] = [\n+            \"python/serialized_test/data/operator_test/*.zip\",\n+        ]\n     else:\n-        torch_package_dir_name = \"torch\"\n-        package_dir = {}\n-        package_data = {\n-            \"torch\": torch_package_data,\n-            \"torchgen\": torchgen_package_data,\n-            \"caffe2\": [\n-                \"python/serialized_test/data/operator_test/*.zip\",\n-            ],\n-        }\n+        # no extensions in BUILD_LIBTORCH_WHL mode\n+        extensions = []\n \n     setup(\n         name=package_name,\n"
                },
                {
                    "old_start": 1481,
                    "old_length": 7,
                    "new_start": 1467,
                    "new_length": 6,
                    "hunk": "@@ -1481,7 +1467,6 @@ def main():\n         install_requires=install_requires,\n         extras_require=extras_require,\n         package_data=package_data,\n-        package_dir=package_dir,\n         url=\"https://pytorch.org/\",\n         download_url=\"https://github.com/pytorch/pytorch/tags\",\n         author=\"PyTorch Team\",\n"
                }
            ],
            "whole_deleted": "-import pkgutil\n-\n-def _get_package_path(package_name):\n-    loader = pkgutil.find_loader(package_name)\n-    if loader:\n-        # The package might be a namespace package, so get_data may fail\n-        try:\n-            file_path = loader.get_filename()\n-            return os.path.dirname(file_path)\n-        except AttributeError:\n-            pass\n-    return None\n-\n-\n-from tools.setup_helpers.env import (\n-    build_type,\n-    IS_DARWIN,\n-    IS_LINUX,\n-    IS_WINDOWS,\n-    LIBTORCH_PKG_NAME,\n-)\n-    os.environ[\"LIBTORCH_LIB_PATH\"] = f\"{_get_package_path(LIBTORCH_PKG_NAME)}/lib\"\n-DEFAULT_PACKAGE_NAME = LIBTORCH_PKG_NAME if BUILD_LIBTORCH_WHL else \"torch\"\n-package_name = os.getenv(\"TORCH_PACKAGE_NAME\", DEFAULT_PACKAGE_NAME)\n-\n-    # the list of runtime dependencies required by this built package\n-        install_requires.append(LIBTORCH_PKG_NAME)\n-    if BUILD_PYTHON_ONLY:\n-                \"lib/libtorch_python*\",\n-                \"lib/*shm*\",\n-                \"lib/libtorch_global_deps*\",\n-    else:\n-    if BUILD_LIBTORCH_WHL:\n-        modified_packages = []\n-        for package in packages:\n-            parts = package.split(\".\")\n-            if parts[0] == \"torch\":\n-                modified_packages.append(DEFAULT_PACKAGE_NAME + package[len(\"torch\") :])\n-        packages = modified_packages\n-        package_dir = {LIBTORCH_PKG_NAME: \"torch\"}\n-        torch_package_dir_name = LIBTORCH_PKG_NAME\n-        package_data = {LIBTORCH_PKG_NAME: torch_package_data}\n-        extensions = []\n-        torch_package_dir_name = \"torch\"\n-        package_dir = {}\n-        package_data = {\n-            \"torch\": torch_package_data,\n-            \"torchgen\": torchgen_package_data,\n-            \"caffe2\": [\n-                \"python/serialized_test/data/operator_test/*.zip\",\n-            ],\n-        }\n-        package_dir=package_dir,\n",
            "whole_added": "+import importlib.util\n+from tools.setup_helpers.env import build_type, IS_DARWIN, IS_LINUX, IS_WINDOWS\n+\n+def _get_package_path(package_name):\n+    spec = importlib.util.find_spec(package_name)\n+    if spec:\n+        # The package might be a namespace package, so get_data may fail\n+        try:\n+            loader = spec.loader\n+            if loader is not None:\n+                file_path = loader.get_filename()  # type: ignore[attr-defined]\n+                return os.path.dirname(file_path)\n+        except AttributeError:\n+            pass\n+    return None\n+\n+\n+    os.environ[\"LIBTORCH_LIB_PATH\"] = f\"{_get_package_path('torch')}/lib\"\n+package_name = os.getenv(\"TORCH_PACKAGE_NAME\", \"torch\")\n+LIBTORCH_PKG_NAME = os.getenv(\"LIBTORCH_PACKAGE_NAME\", \"libtorch\")\n+if BUILD_LIBTORCH_WHL:\n+    package_name = LIBTORCH_PKG_NAME\n+\n+        install_requires.append(f\"{LIBTORCH_PKG_NAME}=={get_torch_version()}\")\n+        \"lib/*shm*\",\n+    if not BUILD_LIBTORCH_WHL:\n+                \"lib/libtorch_python.so\",\n+                \"lib/libtorch_python.dylib\",\n+                \"lib/libtorch_python.dll\",\n+    if not BUILD_PYTHON_ONLY:\n+    package_data = {\n+        \"torch\": torch_package_data,\n+    }\n+    if not BUILD_LIBTORCH_WHL:\n+        package_data[\"torchgen\"] = torchgen_package_data\n+        package_data[\"caffe2\"] = [\n+            \"python/serialized_test/data/operator_test/*.zip\",\n+        ]\n+        # no extensions in BUILD_LIBTORCH_WHL mode\n+        extensions = []\n",
            "whole_hunk": "@@ -199,7 +199,6 @@\n #      Builds pytorch as a wheel using libtorch.so from a seperate wheel\n \n import os\n-import pkgutil\n import sys\n \n if sys.platform == \"win32\" and sys.maxsize.bit_length() == 31:\n@@ -210,19 +209,6 @@ if sys.platform == \"win32\" and sys.maxsize.bit_length() == 31:\n \n import platform\n \n-\n-def _get_package_path(package_name):\n-    loader = pkgutil.find_loader(package_name)\n-    if loader:\n-        # The package might be a namespace package, so get_data may fail\n-        try:\n-            file_path = loader.get_filename()\n-            return os.path.dirname(file_path)\n-        except AttributeError:\n-            pass\n-    return None\n-\n-\n BUILD_LIBTORCH_WHL = os.getenv(\"BUILD_LIBTORCH_WHL\", \"0\") == \"1\"\n BUILD_PYTHON_ONLY = os.getenv(\"BUILD_PYTHON_ONLY\", \"0\") == \"1\"\n \n@@ -237,6 +223,7 @@ if sys.version_info < python_min_version:\n import filecmp\n import glob\n import importlib\n+import importlib.util\n import json\n import shutil\n import subprocess\n@@ -253,15 +240,24 @@ from setuptools.dist import Distribution\n from tools.build_pytorch_libs import build_caffe2\n from tools.generate_torch_version import get_torch_version\n from tools.setup_helpers.cmake import CMake\n-from tools.setup_helpers.env import (\n-    build_type,\n-    IS_DARWIN,\n-    IS_LINUX,\n-    IS_WINDOWS,\n-    LIBTORCH_PKG_NAME,\n-)\n+from tools.setup_helpers.env import build_type, IS_DARWIN, IS_LINUX, IS_WINDOWS\n from tools.setup_helpers.generate_linker_script import gen_linker_script\n \n+\n+def _get_package_path(package_name):\n+    spec = importlib.util.find_spec(package_name)\n+    if spec:\n+        # The package might be a namespace package, so get_data may fail\n+        try:\n+            loader = spec.loader\n+            if loader is not None:\n+                file_path = loader.get_filename()  # type: ignore[attr-defined]\n+                return os.path.dirname(file_path)\n+        except AttributeError:\n+            pass\n+    return None\n+\n+\n # set up appropriate env variables\n if BUILD_LIBTORCH_WHL:\n     # Set up environment variables for ONLY building libtorch.so and not libtorch_python.so\n@@ -271,7 +267,7 @@ if BUILD_LIBTORCH_WHL:\n \n if BUILD_PYTHON_ONLY:\n     os.environ[\"BUILD_LIBTORCHLESS\"] = \"ON\"\n-    os.environ[\"LIBTORCH_LIB_PATH\"] = f\"{_get_package_path(LIBTORCH_PKG_NAME)}/lib\"\n+    os.environ[\"LIBTORCH_LIB_PATH\"] = f\"{_get_package_path('torch')}/lib\"\n \n ################################################################################\n # Parameters parsed from environment\n@@ -347,9 +343,12 @@ cmake_python_include_dir = sysconfig.get_path(\"include\")\n # Version, create_version_file, and package_name\n ################################################################################\n \n-DEFAULT_PACKAGE_NAME = LIBTORCH_PKG_NAME if BUILD_LIBTORCH_WHL else \"torch\"\n+package_name = os.getenv(\"TORCH_PACKAGE_NAME\", \"torch\")\n+LIBTORCH_PKG_NAME = os.getenv(\"LIBTORCH_PACKAGE_NAME\", \"libtorch\")\n+if BUILD_LIBTORCH_WHL:\n+    package_name = LIBTORCH_PKG_NAME\n+\n \n-package_name = os.getenv(\"TORCH_PACKAGE_NAME\", DEFAULT_PACKAGE_NAME)\n package_type = os.getenv(\"PACKAGE_TYPE\", \"wheel\")\n version = get_torch_version()\n report(f\"Building wheel {package_name}-{version}\")\n@@ -1125,8 +1123,6 @@ def main():\n         raise RuntimeError(\n             \"Conflict: 'BUILD_LIBTORCH_WHL' and 'BUILD_PYTHON_ONLY' can't both be 1. Set one to 0 and rerun.\"\n         )\n-\n-    # the list of runtime dependencies required by this built package\n     install_requires = [\n         \"filelock\",\n         \"typing-extensions>=4.8.0\",\n@@ -1141,7 +1137,7 @@ def main():\n         install_requires.append(\"setuptools\")\n \n     if BUILD_PYTHON_ONLY:\n-        install_requires.append(LIBTORCH_PKG_NAME)\n+        install_requires.append(f\"{LIBTORCH_PKG_NAME}=={get_torch_version()}\")\n \n     use_prioritized_text = str(os.getenv(\"USE_PRIORITIZED_TEXT_FOR_LD\", \"\"))\n     if (\n@@ -1219,6 +1214,7 @@ def main():\n         \"utils/data/*.pyi\",\n         \"utils/data/datapipes/*.pyi\",\n         \"lib/*.pdb\",\n+        \"lib/*shm*\",\n         \"lib/torch_shm_manager\",\n         \"lib/*.h\",\n         \"include/*.h\",\n@@ -1383,15 +1379,15 @@ def main():\n         \"utils/model_dump/*.mjs\",\n     ]\n \n-    if BUILD_PYTHON_ONLY:\n+    if not BUILD_LIBTORCH_WHL:\n         torch_package_data.extend(\n             [\n-                \"lib/libtorch_python*\",\n-                \"lib/*shm*\",\n-                \"lib/libtorch_global_deps*\",\n+                \"lib/libtorch_python.so\",\n+                \"lib/libtorch_python.dylib\",\n+                \"lib/libtorch_python.dll\",\n             ]\n         )\n-    else:\n+    if not BUILD_PYTHON_ONLY:\n         torch_package_data.extend(\n             [\n                 \"lib/*.so*\",\n@@ -1442,28 +1438,18 @@ def main():\n         \"packaged/autograd/*\",\n         \"packaged/autograd/templates/*\",\n     ]\n+    package_data = {\n+        \"torch\": torch_package_data,\n+    }\n \n-    if BUILD_LIBTORCH_WHL:\n-        modified_packages = []\n-        for package in packages:\n-            parts = package.split(\".\")\n-            if parts[0] == \"torch\":\n-                modified_packages.append(DEFAULT_PACKAGE_NAME + package[len(\"torch\") :])\n-        packages = modified_packages\n-        package_dir = {LIBTORCH_PKG_NAME: \"torch\"}\n-        torch_package_dir_name = LIBTORCH_PKG_NAME\n-        package_data = {LIBTORCH_PKG_NAME: torch_package_data}\n-        extensions = []\n+    if not BUILD_LIBTORCH_WHL:\n+        package_data[\"torchgen\"] = torchgen_package_data\n+        package_data[\"caffe2\"] = [\n+            \"python/serialized_test/data/operator_test/*.zip\",\n+        ]\n     else:\n-        torch_package_dir_name = \"torch\"\n-        package_dir = {}\n-        package_data = {\n-            \"torch\": torch_package_data,\n-            \"torchgen\": torchgen_package_data,\n-            \"caffe2\": [\n-                \"python/serialized_test/data/operator_test/*.zip\",\n-            ],\n-        }\n+        # no extensions in BUILD_LIBTORCH_WHL mode\n+        extensions = []\n \n     setup(\n         name=package_name,\n@@ -1481,7 +1467,6 @@ def main():\n         install_requires=install_requires,\n         extras_require=extras_require,\n         package_data=package_data,\n-        package_dir=package_dir,\n         url=\"https://pytorch.org/\",\n         download_url=\"https://github.com/pytorch/pytorch/tags\",\n         author=\"PyTorch Team\",\n"
        },
        {
            "name": "env.py",
            "path": "tools/setup_helpers/env.py",
            "patches": [
                {
                    "old_start": 21,
                    "old_length": 8,
                    "new_start": 21,
                    "new_length": 6,
                    "hunk": "@@ -21,8 +21,6 @@ IS_64BIT = struct.calcsize(\"P\") == 8\n \n BUILD_DIR = \"build\"\n \n-LIBTORCH_PKG_NAME = \"libtorchsplit\"\n-\n \n def check_env_flag(name: str, default: str = \"\") -> bool:\n     return os.getenv(name, default).upper() in [\"ON\", \"1\", \"YES\", \"TRUE\", \"Y\"]\n"
                }
            ],
            "whole_deleted": "-LIBTORCH_PKG_NAME = \"libtorchsplit\"\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -21,8 +21,6 @@ IS_64BIT = struct.calcsize(\"P\") == 8\n \n BUILD_DIR = \"build\"\n \n-LIBTORCH_PKG_NAME = \"libtorchsplit\"\n-\n \n def check_env_flag(name: str, default: str = \"\") -> bool:\n     return os.getenv(name, default).upper() in [\"ON\", \"1\", \"YES\", \"TRUE\", \"Y\"]\n"
        },
        {
            "name": "CMakeLists.txt",
            "path": "torch/CMakeLists.txt",
            "patches": [
                {
                    "old_start": 309,
                    "old_length": 6,
                    "new_start": 309,
                    "new_length": 51,
                    "hunk": "@@ -309,6 +309,51 @@ if(HAVE_SOVERSION)\n   set_target_properties(torch_python PROPERTIES\n       VERSION ${TORCH_VERSION} SOVERSION ${TORCH_SOVERSION})\n endif()\n+\n+# in case of the split build we need to add compile definitions\n+if(BUILD_LIBTORCHLESS)\n+  if(USE_UCC AND USE_C10D_UCC)\n+    target_compile_definitions(torch_python PRIVATE USE_C10D_UCC)\n+  endif()\n+\n+  if(USE_UCC AND USE_C10D_NCCL)\n+    target_compile_definitions(torch_python PRIVATE USE_C10D_NCCL)\n+  endif()\n+\n+  if(USE_DISTRIBUTED)\n+    target_compile_definitions(torch_python PRIVATE USE_DISTRIBUTED)\n+  endif()\n+\n+  if(USE_MPI AND USE_C10D_MPI)\n+    target_compile_definitions(torch_python PRIVATE USE_C10D_MPI)\n+  endif()\n+\n+  if(USE_GLOO AND USE_C10D_GLOO)\n+    target_compile_definitions(torch_python PRIVATE USE_C10D_GLOO)\n+  endif()\n+\n+  if(NOT WIN32)\n+    target_compile_definitions(torch_python PRIVATE USE_RPC)\n+  endif()\n+\n+  if(USE_TENSORPIPE)\n+    target_compile_definitions(torch_python PRIVATE USE_TENSORPIPE)\n+  endif()\n+\n+  set(EXPERIMENTAL_SINGLE_THREAD_POOL \"0\" CACHE STRING\n+  \"Experimental option to use a single thread pool for inter- and intra-op parallelism\")\n+  if(\"${EXPERIMENTAL_SINGLE_THREAD_POOL}\")\n+    target_compile_definitions(torch_python PRIVATE \"-DAT_EXPERIMENTAL_SINGLE_THREAD_POOL=1\")\n+  endif()\n+\n+  if(MSVC AND NOT BUILD_SHARED_LIBS)\n+    target_compile_definitions(torch_python PRIVATE \"AT_CORE_STATIC_WINDOWS=1\")\n+  endif()\n+\n+\n+\n+endif()\n+\n add_dependencies(torch_python torch_python_stubs)\n add_dependencies(torch_python flatbuffers)\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+\n+# in case of the split build we need to add compile definitions\n+if(BUILD_LIBTORCHLESS)\n+  if(USE_UCC AND USE_C10D_UCC)\n+    target_compile_definitions(torch_python PRIVATE USE_C10D_UCC)\n+  endif()\n+\n+  if(USE_UCC AND USE_C10D_NCCL)\n+    target_compile_definitions(torch_python PRIVATE USE_C10D_NCCL)\n+  endif()\n+\n+  if(USE_DISTRIBUTED)\n+    target_compile_definitions(torch_python PRIVATE USE_DISTRIBUTED)\n+  endif()\n+\n+  if(USE_MPI AND USE_C10D_MPI)\n+    target_compile_definitions(torch_python PRIVATE USE_C10D_MPI)\n+  endif()\n+\n+  if(USE_GLOO AND USE_C10D_GLOO)\n+    target_compile_definitions(torch_python PRIVATE USE_C10D_GLOO)\n+  endif()\n+\n+  if(NOT WIN32)\n+    target_compile_definitions(torch_python PRIVATE USE_RPC)\n+  endif()\n+\n+  if(USE_TENSORPIPE)\n+    target_compile_definitions(torch_python PRIVATE USE_TENSORPIPE)\n+  endif()\n+\n+  set(EXPERIMENTAL_SINGLE_THREAD_POOL \"0\" CACHE STRING\n+  \"Experimental option to use a single thread pool for inter- and intra-op parallelism\")\n+  if(\"${EXPERIMENTAL_SINGLE_THREAD_POOL}\")\n+    target_compile_definitions(torch_python PRIVATE \"-DAT_EXPERIMENTAL_SINGLE_THREAD_POOL=1\")\n+  endif()\n+\n+  if(MSVC AND NOT BUILD_SHARED_LIBS)\n+    target_compile_definitions(torch_python PRIVATE \"AT_CORE_STATIC_WINDOWS=1\")\n+  endif()\n+\n+\n+\n+endif()\n+\n",
            "whole_hunk": "@@ -309,6 +309,51 @@ if(HAVE_SOVERSION)\n   set_target_properties(torch_python PROPERTIES\n       VERSION ${TORCH_VERSION} SOVERSION ${TORCH_SOVERSION})\n endif()\n+\n+# in case of the split build we need to add compile definitions\n+if(BUILD_LIBTORCHLESS)\n+  if(USE_UCC AND USE_C10D_UCC)\n+    target_compile_definitions(torch_python PRIVATE USE_C10D_UCC)\n+  endif()\n+\n+  if(USE_UCC AND USE_C10D_NCCL)\n+    target_compile_definitions(torch_python PRIVATE USE_C10D_NCCL)\n+  endif()\n+\n+  if(USE_DISTRIBUTED)\n+    target_compile_definitions(torch_python PRIVATE USE_DISTRIBUTED)\n+  endif()\n+\n+  if(USE_MPI AND USE_C10D_MPI)\n+    target_compile_definitions(torch_python PRIVATE USE_C10D_MPI)\n+  endif()\n+\n+  if(USE_GLOO AND USE_C10D_GLOO)\n+    target_compile_definitions(torch_python PRIVATE USE_C10D_GLOO)\n+  endif()\n+\n+  if(NOT WIN32)\n+    target_compile_definitions(torch_python PRIVATE USE_RPC)\n+  endif()\n+\n+  if(USE_TENSORPIPE)\n+    target_compile_definitions(torch_python PRIVATE USE_TENSORPIPE)\n+  endif()\n+\n+  set(EXPERIMENTAL_SINGLE_THREAD_POOL \"0\" CACHE STRING\n+  \"Experimental option to use a single thread pool for inter- and intra-op parallelism\")\n+  if(\"${EXPERIMENTAL_SINGLE_THREAD_POOL}\")\n+    target_compile_definitions(torch_python PRIVATE \"-DAT_EXPERIMENTAL_SINGLE_THREAD_POOL=1\")\n+  endif()\n+\n+  if(MSVC AND NOT BUILD_SHARED_LIBS)\n+    target_compile_definitions(torch_python PRIVATE \"AT_CORE_STATIC_WINDOWS=1\")\n+  endif()\n+\n+\n+\n+endif()\n+\n add_dependencies(torch_python torch_python_stubs)\n add_dependencies(torch_python flatbuffers)\n \n"
        },
        {
            "name": "__init__.py",
            "path": "torch/__init__.py",
            "patches": [
                {
                    "old_start": 271,
                    "old_length": 38,
                    "new_start": 271,
                    "new_length": 6,
                    "hunk": "@@ -271,38 +271,6 @@ def _preload_cuda_deps(lib_folder, lib_name):\n \n # See Note [Global dependencies]\n def _load_global_deps() -> None:\n-    LIBTORCH_PKG_NAME = \"libtorchsplit\"\n-\n-    def find_package_path(package_name):\n-        spec = importlib.util.find_spec(package_name)\n-        if spec:\n-            # The package might be a namespace package, so get_data may fail\n-            try:\n-                loader = spec.loader\n-                if loader is not None:\n-                    file_path = loader.get_filename()  # type: ignore[attr-defined]\n-                    return os.path.dirname(file_path)\n-            except AttributeError:\n-                pass\n-        return None\n-\n-    def load_shared_libraries(library_path):\n-        lib_dir = os.path.join(library_path, \"lib\")\n-        if not os.path.exists(lib_dir):\n-            return\n-\n-        # Find all shared library files with the appropriate extension\n-        library_files = [f for f in os.listdir(lib_dir) if f.endswith(lib_ext)]\n-        if not library_files:\n-            return\n-\n-        for lib_file in library_files:\n-            lib_path = os.path.join(lib_dir, lib_file)\n-            try:\n-                ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)\n-            except OSError as err:\n-                print(f\"Failed to load {lib_path}: {err}\")\n-\n     if _running_with_deploy() or platform.system() == \"Windows\":\n         return\n \n"
                },
                {
                    "old_start": 312,
                    "old_length": 11,
                    "new_start": 280,
                    "new_length": 6,
                    "hunk": "@@ -312,11 +280,6 @@ def _load_global_deps() -> None:\n     here = os.path.abspath(__file__)\n     global_deps_lib_path = os.path.join(os.path.dirname(here), \"lib\", lib_name)\n \n-    split_build_lib_name = LIBTORCH_PKG_NAME\n-    library_path = find_package_path(split_build_lib_name)\n-\n-    if library_path:\n-        global_deps_lib_path = os.path.join(library_path, \"lib\", lib_name)\n     try:\n         ctypes.CDLL(global_deps_lib_path, mode=ctypes.RTLD_GLOBAL)\n     except OSError as err:\n"
                },
                {
                    "old_start": 344,
                    "old_length": 10,
                    "new_start": 307,
                    "new_length": 6,
                    "hunk": "@@ -344,10 +307,6 @@ def _load_global_deps() -> None:\n             _preload_cuda_deps(lib_folder, lib_name)\n         ctypes.CDLL(global_deps_lib_path, mode=ctypes.RTLD_GLOBAL)\n \n-    if library_path:\n-        # loading libtorch_global_deps first due its special logic\n-        load_shared_libraries(library_path)\n-\n \n if (USE_RTLD_GLOBAL_WITH_LIBTORCH or os.getenv(\"TORCH_USE_RTLD_GLOBAL\")) and (\n     _running_with_deploy() or platform.system() != \"Windows\""
                }
            ],
            "whole_deleted": "-    LIBTORCH_PKG_NAME = \"libtorchsplit\"\n-\n-    def find_package_path(package_name):\n-        spec = importlib.util.find_spec(package_name)\n-        if spec:\n-            # The package might be a namespace package, so get_data may fail\n-            try:\n-                loader = spec.loader\n-                if loader is not None:\n-                    file_path = loader.get_filename()  # type: ignore[attr-defined]\n-                    return os.path.dirname(file_path)\n-            except AttributeError:\n-                pass\n-        return None\n-\n-    def load_shared_libraries(library_path):\n-        lib_dir = os.path.join(library_path, \"lib\")\n-        if not os.path.exists(lib_dir):\n-            return\n-\n-        # Find all shared library files with the appropriate extension\n-        library_files = [f for f in os.listdir(lib_dir) if f.endswith(lib_ext)]\n-        if not library_files:\n-            return\n-\n-        for lib_file in library_files:\n-            lib_path = os.path.join(lib_dir, lib_file)\n-            try:\n-                ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)\n-            except OSError as err:\n-                print(f\"Failed to load {lib_path}: {err}\")\n-\n-    split_build_lib_name = LIBTORCH_PKG_NAME\n-    library_path = find_package_path(split_build_lib_name)\n-\n-    if library_path:\n-        global_deps_lib_path = os.path.join(library_path, \"lib\", lib_name)\n-    if library_path:\n-        # loading libtorch_global_deps first due its special logic\n-        load_shared_libraries(library_path)\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -271,38 +271,6 @@ def _preload_cuda_deps(lib_folder, lib_name):\n \n # See Note [Global dependencies]\n def _load_global_deps() -> None:\n-    LIBTORCH_PKG_NAME = \"libtorchsplit\"\n-\n-    def find_package_path(package_name):\n-        spec = importlib.util.find_spec(package_name)\n-        if spec:\n-            # The package might be a namespace package, so get_data may fail\n-            try:\n-                loader = spec.loader\n-                if loader is not None:\n-                    file_path = loader.get_filename()  # type: ignore[attr-defined]\n-                    return os.path.dirname(file_path)\n-            except AttributeError:\n-                pass\n-        return None\n-\n-    def load_shared_libraries(library_path):\n-        lib_dir = os.path.join(library_path, \"lib\")\n-        if not os.path.exists(lib_dir):\n-            return\n-\n-        # Find all shared library files with the appropriate extension\n-        library_files = [f for f in os.listdir(lib_dir) if f.endswith(lib_ext)]\n-        if not library_files:\n-            return\n-\n-        for lib_file in library_files:\n-            lib_path = os.path.join(lib_dir, lib_file)\n-            try:\n-                ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)\n-            except OSError as err:\n-                print(f\"Failed to load {lib_path}: {err}\")\n-\n     if _running_with_deploy() or platform.system() == \"Windows\":\n         return\n \n@@ -312,11 +280,6 @@ def _load_global_deps() -> None:\n     here = os.path.abspath(__file__)\n     global_deps_lib_path = os.path.join(os.path.dirname(here), \"lib\", lib_name)\n \n-    split_build_lib_name = LIBTORCH_PKG_NAME\n-    library_path = find_package_path(split_build_lib_name)\n-\n-    if library_path:\n-        global_deps_lib_path = os.path.join(library_path, \"lib\", lib_name)\n     try:\n         ctypes.CDLL(global_deps_lib_path, mode=ctypes.RTLD_GLOBAL)\n     except OSError as err:\n@@ -344,10 +307,6 @@ def _load_global_deps() -> None:\n             _preload_cuda_deps(lib_folder, lib_name)\n         ctypes.CDLL(global_deps_lib_path, mode=ctypes.RTLD_GLOBAL)\n \n-    if library_path:\n-        # loading libtorch_global_deps first due its special logic\n-        load_shared_libraries(library_path)\n-\n \n if (USE_RTLD_GLOBAL_WITH_LIBTORCH or os.getenv(\"TORCH_USE_RTLD_GLOBAL\")) and (\n     _running_with_deploy() or platform.system() != \"Windows\""
        }
    ]
},
{
    "Id": 320,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/978faf1fa29444f78a7ca805f8abc032cb29e0d8",
    "date": "2024-01-27T05:28:46+00:00",
    "message": "Use an op counter to decide when to realize a kernel (#117030)\n\nInstead of checking the number of bytes in the string representation\nof the kernel\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/117030\nApproved by: https://github.com/lezcano, https://github.com/peterbell10",
    "label": "NO",
    "changes": [
        {
            "name": "test_perf.py",
            "path": "test/inductor/test_perf.py",
            "patches": [
                {
                    "old_start": 447,
                    "old_length": 7,
                    "new_start": 447,
                    "new_length": 7,
                    "hunk": "@@ -447,7 +447,7 @@ class SchedulerFusionTests(TestCase):\n     def setUpClass(cls):\n         super().setUpClass()\n         cls._stack = contextlib.ExitStack()\n-        cls._stack.enter_context(patch.object(config, \"realize_bytes_threshold\", 0))\n+        cls._stack.enter_context(patch.object(config, \"realize_opcount_threshold\", 0))\n \n     @classmethod\n     def tearDownClass(cls):\n"
                },
                {
                    "old_start": 808,
                    "old_length": 7,
                    "new_start": 808,
                    "new_length": 7,
                    "hunk": "@@ -808,7 +808,7 @@ class WouldBeNiceIfItWorked:\n         self.assertExpectedInline(count_numel(f, *inp), \"\"\"200\"\"\")\n \n     # TODO: The greedy fusion strategy results in suboptimal grouping\n-    @patch.object(config, \"realize_bytes_threshold\", 0)\n+    @patch.object(config, \"realize_opcount_threshold\", 0)\n     def test_fusion_choice4(self):\n         def f(a, b, b2):\n             c = a + b\n"
                }
            ],
            "whole_deleted": "-        cls._stack.enter_context(patch.object(config, \"realize_bytes_threshold\", 0))\n-    @patch.object(config, \"realize_bytes_threshold\", 0)\n",
            "whole_added": "+        cls._stack.enter_context(patch.object(config, \"realize_opcount_threshold\", 0))\n+    @patch.object(config, \"realize_opcount_threshold\", 0)\n",
            "whole_hunk": "@@ -447,7 +447,7 @@ class SchedulerFusionTests(TestCase):\n     def setUpClass(cls):\n         super().setUpClass()\n         cls._stack = contextlib.ExitStack()\n-        cls._stack.enter_context(patch.object(config, \"realize_bytes_threshold\", 0))\n+        cls._stack.enter_context(patch.object(config, \"realize_opcount_threshold\", 0))\n \n     @classmethod\n     def tearDownClass(cls):\n@@ -808,7 +808,7 @@ class WouldBeNiceIfItWorked:\n         self.assertExpectedInline(count_numel(f, *inp), \"\"\"200\"\"\")\n \n     # TODO: The greedy fusion strategy results in suboptimal grouping\n-    @patch.object(config, \"realize_bytes_threshold\", 0)\n+    @patch.object(config, \"realize_opcount_threshold\", 0)\n     def test_fusion_choice4(self):\n         def f(a, b, b2):\n             c = a + b\n"
        },
        {
            "name": "config.py",
            "path": "torch/_inductor/config.py",
            "patches": [
                {
                    "old_start": 247,
                    "old_length": 7,
                    "new_start": 247,
                    "new_length": 7,
                    "hunk": "@@ -247,7 +247,7 @@ warn_mix_layout = os.environ.get(\"TORCHINDUCTOR_WARN_MIX_LAYOUT\") == \"1\"\n # For fanouts, rematerialization can lead to exponential blowup. So, have\n # smaller threshold\n realize_reads_threshold = 4\n-realize_bytes_threshold = 2000\n+realize_opcount_threshold = 30\n \n # Threshold to prevent excessive accumulation of ops in one buffer during lowering\n realize_acc_reads_threshold = 8\n"
                }
            ],
            "whole_deleted": "-realize_bytes_threshold = 2000\n",
            "whole_added": "+realize_opcount_threshold = 30\n",
            "whole_hunk": "@@ -247,7 +247,7 @@ warn_mix_layout = os.environ.get(\"TORCHINDUCTOR_WARN_MIX_LAYOUT\") == \"1\"\n # For fanouts, rematerialization can lead to exponential blowup. So, have\n # smaller threshold\n realize_reads_threshold = 4\n-realize_bytes_threshold = 2000\n+realize_opcount_threshold = 30\n \n # Threshold to prevent excessive accumulation of ops in one buffer during lowering\n realize_acc_reads_threshold = 8\n"
        },
        {
            "name": "graph.py",
            "path": "torch/_inductor/graph.py",
            "patches": [
                {
                    "old_start": 968,
                    "old_length": 7,
                    "new_start": 968,
                    "new_length": 7,
                    "hunk": "@@ -968,7 +968,7 @@ class GraphLowering(torch.fx.Interpreter):\n                 curr = result.data.data\n                 if isinstance(curr, Pointwise):\n                     # Use inner fn as a rough proxy. Good enough.\n-                    if curr.inner_fn_str_len() > config.realize_bytes_threshold:\n+                    if curr.has_large_inner_fn():\n                         result.realize()\n \n         # This is not complete, but it doesn't have to be: origin_node\n"
                }
            ],
            "whole_deleted": "-                    if curr.inner_fn_str_len() > config.realize_bytes_threshold:\n",
            "whole_added": "+                    if curr.has_large_inner_fn():\n",
            "whole_hunk": "@@ -968,7 +968,7 @@ class GraphLowering(torch.fx.Interpreter):\n                 curr = result.data.data\n                 if isinstance(curr, Pointwise):\n                     # Use inner fn as a rough proxy. Good enough.\n-                    if curr.inner_fn_str_len() > config.realize_bytes_threshold:\n+                    if curr.has_large_inner_fn():\n                         result.realize()\n \n         # This is not complete, but it doesn't have to be: origin_node\n"
        },
        {
            "name": "ir.py",
            "path": "torch/_inductor/ir.py",
            "patches": [
                {
                    "old_start": 333,
                    "old_length": 6,
                    "new_start": 333,
                    "new_length": 31,
                    "hunk": "@@ -333,6 +333,31 @@ class IRNode:\n     realize_hint: Callable[[], None]\n \n \n+class _OpCounterCSE:\n+    \"\"\"Shim to count how many ops are used\"\"\"\n+\n+    def __init__(self, inner):\n+        super().__init__()\n+        self.parent_handler = inner\n+        self.op_count = 0\n+        self.var_names = {}\n+\n+    def __getattr__(self, name):\n+        def inner(*args, **kwargs):\n+            val = getattr(self.parent_handler, name)(*args, **kwargs)\n+            if name == \"indirect_indexing\":\n+                return val\n+            if val not in self.var_names:\n+                varname = f\"tmp{self.op_count}\"\n+                self.op_count += 1\n+                self.var_names[val] = varname\n+                return varname\n+            else:\n+                return self.var_names[val]\n+\n+        return inner\n+\n+\n @dataclasses.dataclass\n class Loops(IRNode):\n     device: torch.device\n"
                },
                {
                    "old_start": 400,
                    "old_length": 12,
                    "new_start": 425,
                    "new_length": 27,
                    "hunk": "@@ -400,12 +425,27 @@ class Loops(IRNode):\n         ]\n \n     @cache_on_self\n-    def inner_fn_str_len(self):\n-        return len(self.inner_fn_str())\n+    def inner_fn_opcount(self):\n+        from .ir import FlexibleLayout\n+\n+        opcounter = _OpCounterCSE(V.MockHandler())\n+\n+        with V.set_ops_handler(opcounter), patch.object(\n+            FlexibleLayout, \"allow_indexing\", True\n+        ):\n+            result = self.inner_fn(*self.inner_fn_args())\n+            return opcounter.op_count\n+\n+    def inner_fn_args(self):\n+        return (self._index(self.ranges),)\n \n     def inner_fn_str(self):\n-        index = self._index(self.ranges)\n-        return V.KernelFormatterHandler.ir_to_string(self.inner_fn, index)\n+        return V.KernelFormatterHandler.ir_to_string(\n+            self.inner_fn, *self.inner_fn_args()\n+        )\n+\n+    def has_large_inner_fn(self):\n+        return self.inner_fn_opcount() > config.realize_opcount_threshold\n \n     def inner_fn_free_unbacked_symbols(self):\n         index = self._index(self.ranges)\n"
                },
                {
                    "old_start": 612,
                    "old_length": 14,
                    "new_start": 652,
                    "new_length": 10,
                    "hunk": "@@ -612,14 +652,10 @@ class Reduction(Loops):\n     def index_length(self):\n         return len(self.ranges) + len(self.reduction_ranges)\n \n-    def inner_fn_str(self):\n+    def inner_fn_args(self):\n         index = self._index(self.ranges)\n         rindex = self._index(self.reduction_ranges, \"r\")\n-        return V.KernelFormatterHandler.ir_to_string(\n-            self.inner_fn,\n-            index,\n-            rindex,\n-        )\n+        return (index, rindex)\n \n     def inner_fn_free_unbacked_symbols(self):\n         index = self._index(self.ranges)\n"
                },
                {
                    "old_start": 1605,
                    "old_length": 14,
                    "new_start": 1641,
                    "new_length": 11,
                    "hunk": "@@ -1605,14 +1641,11 @@ class Scan(Loops):\n     def index_length(self):\n         return len(self.ranges) + len(self.scan_ranges)\n \n-    def inner_fn_str(self):\n+    def inner_fn_args(self):\n         index = self._index(self.ranges)\n         rindex = self._index(self.scan_ranges, \"r\")\n         idx = self.reindex(index, rindex)\n-        return V.KernelFormatterHandler.ir_to_string(\n-            self.inner_fn,\n-            idx,\n-        )\n+        return (idx,)\n \n     def inner_fn_free_unbacked_symbols(self):\n         index = self._index(self.ranges)\n"
                },
                {
                    "old_start": 6473,
                    "old_length": 7,
                    "new_start": 6506,
                    "new_length": 7,
                    "hunk": "@@ -6473,7 +6506,7 @@ class StorageBox(MutableBox):\n     def has_exceeded_max_reads(self):\n         return isinstance(self.data, Pointwise) and (\n             self.num_reads() > config.realize_acc_reads_threshold\n-            or self.inner_fn_str_len() > config.realize_bytes_threshold\n+            or self.has_large_inner_fn()\n         )\n \n     def mark_reuse(self, users):\n"
                },
                {
                    "old_start": 6495,
                    "old_length": 7,
                    "new_start": 6528,
                    "new_length": 7,
                    "hunk": "@@ -6495,7 +6528,7 @@ class StorageBox(MutableBox):\n             and isinstance(self.data, (Pointwise, Reduction))\n             and (\n                 self.num_reads() > config.realize_reads_threshold\n-                or len(self.inner_fn_str()) > config.realize_bytes_threshold\n+                or self.has_large_inner_fn()\n                 or (is_cpu(self.data) and should_realize_on_cpu(self.data))\n             )\n         ):"
                }
            ],
            "whole_deleted": "-    def inner_fn_str_len(self):\n-        return len(self.inner_fn_str())\n-        index = self._index(self.ranges)\n-        return V.KernelFormatterHandler.ir_to_string(self.inner_fn, index)\n-    def inner_fn_str(self):\n-        return V.KernelFormatterHandler.ir_to_string(\n-            self.inner_fn,\n-            index,\n-            rindex,\n-        )\n-    def inner_fn_str(self):\n-        return V.KernelFormatterHandler.ir_to_string(\n-            self.inner_fn,\n-            idx,\n-        )\n-            or self.inner_fn_str_len() > config.realize_bytes_threshold\n-                or len(self.inner_fn_str()) > config.realize_bytes_threshold\n",
            "whole_added": "+class _OpCounterCSE:\n+    \"\"\"Shim to count how many ops are used\"\"\"\n+\n+    def __init__(self, inner):\n+        super().__init__()\n+        self.parent_handler = inner\n+        self.op_count = 0\n+        self.var_names = {}\n+\n+    def __getattr__(self, name):\n+        def inner(*args, **kwargs):\n+            val = getattr(self.parent_handler, name)(*args, **kwargs)\n+            if name == \"indirect_indexing\":\n+                return val\n+            if val not in self.var_names:\n+                varname = f\"tmp{self.op_count}\"\n+                self.op_count += 1\n+                self.var_names[val] = varname\n+                return varname\n+            else:\n+                return self.var_names[val]\n+\n+        return inner\n+\n+\n+    def inner_fn_opcount(self):\n+        from .ir import FlexibleLayout\n+\n+        opcounter = _OpCounterCSE(V.MockHandler())\n+\n+        with V.set_ops_handler(opcounter), patch.object(\n+            FlexibleLayout, \"allow_indexing\", True\n+        ):\n+            result = self.inner_fn(*self.inner_fn_args())\n+            return opcounter.op_count\n+\n+    def inner_fn_args(self):\n+        return (self._index(self.ranges),)\n+        return V.KernelFormatterHandler.ir_to_string(\n+            self.inner_fn, *self.inner_fn_args()\n+        )\n+\n+    def has_large_inner_fn(self):\n+        return self.inner_fn_opcount() > config.realize_opcount_threshold\n+    def inner_fn_args(self):\n+        return (index, rindex)\n+    def inner_fn_args(self):\n+        return (idx,)\n+            or self.has_large_inner_fn()\n+                or self.has_large_inner_fn()\n",
            "whole_hunk": "@@ -333,6 +333,31 @@ class IRNode:\n     realize_hint: Callable[[], None]\n \n \n+class _OpCounterCSE:\n+    \"\"\"Shim to count how many ops are used\"\"\"\n+\n+    def __init__(self, inner):\n+        super().__init__()\n+        self.parent_handler = inner\n+        self.op_count = 0\n+        self.var_names = {}\n+\n+    def __getattr__(self, name):\n+        def inner(*args, **kwargs):\n+            val = getattr(self.parent_handler, name)(*args, **kwargs)\n+            if name == \"indirect_indexing\":\n+                return val\n+            if val not in self.var_names:\n+                varname = f\"tmp{self.op_count}\"\n+                self.op_count += 1\n+                self.var_names[val] = varname\n+                return varname\n+            else:\n+                return self.var_names[val]\n+\n+        return inner\n+\n+\n @dataclasses.dataclass\n class Loops(IRNode):\n     device: torch.device\n@@ -400,12 +425,27 @@ class Loops(IRNode):\n         ]\n \n     @cache_on_self\n-    def inner_fn_str_len(self):\n-        return len(self.inner_fn_str())\n+    def inner_fn_opcount(self):\n+        from .ir import FlexibleLayout\n+\n+        opcounter = _OpCounterCSE(V.MockHandler())\n+\n+        with V.set_ops_handler(opcounter), patch.object(\n+            FlexibleLayout, \"allow_indexing\", True\n+        ):\n+            result = self.inner_fn(*self.inner_fn_args())\n+            return opcounter.op_count\n+\n+    def inner_fn_args(self):\n+        return (self._index(self.ranges),)\n \n     def inner_fn_str(self):\n-        index = self._index(self.ranges)\n-        return V.KernelFormatterHandler.ir_to_string(self.inner_fn, index)\n+        return V.KernelFormatterHandler.ir_to_string(\n+            self.inner_fn, *self.inner_fn_args()\n+        )\n+\n+    def has_large_inner_fn(self):\n+        return self.inner_fn_opcount() > config.realize_opcount_threshold\n \n     def inner_fn_free_unbacked_symbols(self):\n         index = self._index(self.ranges)\n@@ -612,14 +652,10 @@ class Reduction(Loops):\n     def index_length(self):\n         return len(self.ranges) + len(self.reduction_ranges)\n \n-    def inner_fn_str(self):\n+    def inner_fn_args(self):\n         index = self._index(self.ranges)\n         rindex = self._index(self.reduction_ranges, \"r\")\n-        return V.KernelFormatterHandler.ir_to_string(\n-            self.inner_fn,\n-            index,\n-            rindex,\n-        )\n+        return (index, rindex)\n \n     def inner_fn_free_unbacked_symbols(self):\n         index = self._index(self.ranges)\n@@ -1605,14 +1641,11 @@ class Scan(Loops):\n     def index_length(self):\n         return len(self.ranges) + len(self.scan_ranges)\n \n-    def inner_fn_str(self):\n+    def inner_fn_args(self):\n         index = self._index(self.ranges)\n         rindex = self._index(self.scan_ranges, \"r\")\n         idx = self.reindex(index, rindex)\n-        return V.KernelFormatterHandler.ir_to_string(\n-            self.inner_fn,\n-            idx,\n-        )\n+        return (idx,)\n \n     def inner_fn_free_unbacked_symbols(self):\n         index = self._index(self.ranges)\n@@ -6473,7 +6506,7 @@ class StorageBox(MutableBox):\n     def has_exceeded_max_reads(self):\n         return isinstance(self.data, Pointwise) and (\n             self.num_reads() > config.realize_acc_reads_threshold\n-            or self.inner_fn_str_len() > config.realize_bytes_threshold\n+            or self.has_large_inner_fn()\n         )\n \n     def mark_reuse(self, users):\n@@ -6495,7 +6528,7 @@ class StorageBox(MutableBox):\n             and isinstance(self.data, (Pointwise, Reduction))\n             and (\n                 self.num_reads() > config.realize_reads_threshold\n-                or len(self.inner_fn_str()) > config.realize_bytes_threshold\n+                or self.has_large_inner_fn()\n                 or (is_cpu(self.data) and should_realize_on_cpu(self.data))\n             )\n         ):"
        }
    ]
},
{
    "Id": 229,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/2367d0dacdded111d7859fdec23a6a3d87c19ca5",
    "date": "2024-03-26T23:25:15+00:00",
    "message": "[AOTInductor] Add tensor_constantX to pass constant buffer update's check (#122562) (#122690)\n\nSummary:\n\nDuring tracing, some constants (tensor_constant{idx}) are being generated internally.\nThose constants are neither parameters or buffers, and users have zero control on them.\n\nTo accomodate this, we should allow users not passing in those constants generated internally but still be able the constants in the model.\n\nTest Plan:\nIncluded in commit.\n```\nbuild/bin/test_aot_inductor\n```\n\nReviewed By: zoranzhao\n\nDifferential Revision: D55354548\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/122690\nApproved by: https://github.com/khabinov",
    "label": "NO",
    "changes": [
        {
            "name": "test.cpp",
            "path": "test/cpp/aot_inductor/test.cpp",
            "patches": [
                {
                    "old_start": 241,
                    "old_length": 6,
                    "new_start": 241,
                    "new_length": 45,
                    "hunk": "@@ -241,6 +241,45 @@ void test_aoti_double_buffering(\n   ASSERT_TRUE(torch::allclose(ref_output_tensors[0], actual_output_tensors[0]));\n }\n \n+void test_aoti_double_buffering_with_tensor_constants() {\n+  torch::NoGradGuard no_grad;\n+\n+  std::string data_path = (std::filesystem::path(\n+                               STRINGIZE(CMAKE_CURRENT_BINARY_DIR)) /\n+                               \"data_with_tensor_constants.pt\")\n+                               .string();\n+\n+  torch::jit::script::Module data_loader = torch::jit::load(data_path);\n+  std::string path_attr = \"model_so_path\";\n+  std::string inputs_attr = \"inputs\";\n+  std::string w_attr = \"w\";\n+  std::string outputs_attr = \"outputs\";\n+  const auto& model_so_path = data_loader.attr(path_attr.c_str()).toStringRef();\n+  auto input_tensors =\n+      data_loader.attr(inputs_attr.c_str()).toTensorList().vec();\n+  const auto& w_tensors = data_loader.attr(w_attr.c_str()).toTensor();\n+  const auto& ref_output_tensors =\n+      data_loader.attr(outputs_attr.c_str()).toTensorList().vec();\n+\n+  torch::inductor::TensorConstantMap real_map;\n+  real_map.emplace(\"L__self___w\", new at::Tensor(w_tensors));\n+\n+  std::unique_ptr<torch::inductor::AOTIModelContainerRunner> runner;\n+  runner = std::make_unique<torch::inductor::AOTIModelContainerRunnerCuda>(\n+      model_so_path.c_str());\n+\n+  // By default, buffer #1 get loaded with burned in weights. Correct results.\n+  auto actual_output_tensors = runner->run(input_tensors);\n+  ASSERT_TRUE(torch::allclose(ref_output_tensors[0], actual_output_tensors[0]));\n+\n+  // We update the weights to buffer #2 and activate it. This should still\n+  // produce correct result, since we would have copied the tensor_constants.\n+  runner->update_inactive_constant_buffer(real_map);\n+  runner->swap_constant_buffer();\n+  actual_output_tensors = runner->run(input_tensors);\n+  ASSERT_TRUE(torch::allclose(ref_output_tensors[0], actual_output_tensors[0]));\n+}\n+\n } // namespace\n \n namespace torch {\n"
                },
                {
                    "old_start": 279,
                    "old_length": 6,
                    "new_start": 318,
                    "new_length": 10,
                    "hunk": "@@ -279,6 +318,10 @@ TEST(AotInductorTest, RuntimeUpdateInactiveConstantsCuda) {\n TEST(AotInductorTest, UpdateInactiveConstantsCuda) {\n   test_aoti_double_buffering(\"cuda\", false);\n }\n+\n+TEST(AotInductorTest, UpdateInactiveConstantsWithTensorConstantsCuda) {\n+  test_aoti_double_buffering_with_tensor_constants();\n+}\n #endif\n \n } // namespace inductor\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+void test_aoti_double_buffering_with_tensor_constants() {\n+  torch::NoGradGuard no_grad;\n+\n+  std::string data_path = (std::filesystem::path(\n+                               STRINGIZE(CMAKE_CURRENT_BINARY_DIR)) /\n+                               \"data_with_tensor_constants.pt\")\n+                               .string();\n+\n+  torch::jit::script::Module data_loader = torch::jit::load(data_path);\n+  std::string path_attr = \"model_so_path\";\n+  std::string inputs_attr = \"inputs\";\n+  std::string w_attr = \"w\";\n+  std::string outputs_attr = \"outputs\";\n+  const auto& model_so_path = data_loader.attr(path_attr.c_str()).toStringRef();\n+  auto input_tensors =\n+      data_loader.attr(inputs_attr.c_str()).toTensorList().vec();\n+  const auto& w_tensors = data_loader.attr(w_attr.c_str()).toTensor();\n+  const auto& ref_output_tensors =\n+      data_loader.attr(outputs_attr.c_str()).toTensorList().vec();\n+\n+  torch::inductor::TensorConstantMap real_map;\n+  real_map.emplace(\"L__self___w\", new at::Tensor(w_tensors));\n+\n+  std::unique_ptr<torch::inductor::AOTIModelContainerRunner> runner;\n+  runner = std::make_unique<torch::inductor::AOTIModelContainerRunnerCuda>(\n+      model_so_path.c_str());\n+\n+  // By default, buffer #1 get loaded with burned in weights. Correct results.\n+  auto actual_output_tensors = runner->run(input_tensors);\n+  ASSERT_TRUE(torch::allclose(ref_output_tensors[0], actual_output_tensors[0]));\n+\n+  // We update the weights to buffer #2 and activate it. This should still\n+  // produce correct result, since we would have copied the tensor_constants.\n+  runner->update_inactive_constant_buffer(real_map);\n+  runner->swap_constant_buffer();\n+  actual_output_tensors = runner->run(input_tensors);\n+  ASSERT_TRUE(torch::allclose(ref_output_tensors[0], actual_output_tensors[0]));\n+}\n+\n+\n+TEST(AotInductorTest, UpdateInactiveConstantsWithTensorConstantsCuda) {\n+  test_aoti_double_buffering_with_tensor_constants();\n+}\n",
            "whole_hunk": "@@ -241,6 +241,45 @@ void test_aoti_double_buffering(\n   ASSERT_TRUE(torch::allclose(ref_output_tensors[0], actual_output_tensors[0]));\n }\n \n+void test_aoti_double_buffering_with_tensor_constants() {\n+  torch::NoGradGuard no_grad;\n+\n+  std::string data_path = (std::filesystem::path(\n+                               STRINGIZE(CMAKE_CURRENT_BINARY_DIR)) /\n+                               \"data_with_tensor_constants.pt\")\n+                               .string();\n+\n+  torch::jit::script::Module data_loader = torch::jit::load(data_path);\n+  std::string path_attr = \"model_so_path\";\n+  std::string inputs_attr = \"inputs\";\n+  std::string w_attr = \"w\";\n+  std::string outputs_attr = \"outputs\";\n+  const auto& model_so_path = data_loader.attr(path_attr.c_str()).toStringRef();\n+  auto input_tensors =\n+      data_loader.attr(inputs_attr.c_str()).toTensorList().vec();\n+  const auto& w_tensors = data_loader.attr(w_attr.c_str()).toTensor();\n+  const auto& ref_output_tensors =\n+      data_loader.attr(outputs_attr.c_str()).toTensorList().vec();\n+\n+  torch::inductor::TensorConstantMap real_map;\n+  real_map.emplace(\"L__self___w\", new at::Tensor(w_tensors));\n+\n+  std::unique_ptr<torch::inductor::AOTIModelContainerRunner> runner;\n+  runner = std::make_unique<torch::inductor::AOTIModelContainerRunnerCuda>(\n+      model_so_path.c_str());\n+\n+  // By default, buffer #1 get loaded with burned in weights. Correct results.\n+  auto actual_output_tensors = runner->run(input_tensors);\n+  ASSERT_TRUE(torch::allclose(ref_output_tensors[0], actual_output_tensors[0]));\n+\n+  // We update the weights to buffer #2 and activate it. This should still\n+  // produce correct result, since we would have copied the tensor_constants.\n+  runner->update_inactive_constant_buffer(real_map);\n+  runner->swap_constant_buffer();\n+  actual_output_tensors = runner->run(input_tensors);\n+  ASSERT_TRUE(torch::allclose(ref_output_tensors[0], actual_output_tensors[0]));\n+}\n+\n } // namespace\n \n namespace torch {\n@@ -279,6 +318,10 @@ TEST(AotInductorTest, RuntimeUpdateInactiveConstantsCuda) {\n TEST(AotInductorTest, UpdateInactiveConstantsCuda) {\n   test_aoti_double_buffering(\"cuda\", false);\n }\n+\n+TEST(AotInductorTest, UpdateInactiveConstantsWithTensorConstantsCuda) {\n+  test_aoti_double_buffering_with_tensor_constants();\n+}\n #endif\n \n } // namespace inductor\n"
        },
        {
            "name": "test.py",
            "path": "test/cpp/aot_inductor/test.py",
            "patches": [
                {
                    "old_start": 17,
                    "old_length": 38,
                    "new_start": 17,
                    "new_length": 74,
                    "hunk": "@@ -17,38 +17,74 @@ class Net(torch.nn.Module):\n         w = w_relu + self.w_add\n         return torch.matmul(x, w)\n \n+class NetWithTensorConstants(torch.nn.Module):\n+    def __init__(self):\n+        super().__init__()\n+        self.w = torch.randn(30, 1, device=\"cuda\")\n+\n+    def forward(self, x, y):\n+        z = self.w * x * y\n+        return z[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17]]\n+\n data = {}\n+data_with_tensor_constants = {}\n+\n+# Basice AOTI model test generation.\n+def generate_basic_tests():\n+    for device in [\"cpu\", \"cuda\"]:\n+        for use_runtime_constant_folding in [True, False]:\n+            if device == \"cpu\" and use_runtime_constant_folding:\n+                # We do not test runtime const folding for cpu mode.\n+                continue\n+            model = Net(device).to(device=device)\n+            x = torch.randn((4, 4), device=device)\n+            with torch.no_grad():\n+                ref_output = model(x)\n+\n+            torch._dynamo.reset()\n+            with torch.no_grad():\n+                dim0_x = Dim(\"dim0_x\", min=1, max=1024)\n+                dynamic_shapes = {\"x\": {0: dim0_x}}\n+                model_so_path = aot_compile(\n+                    model,\n+                    (x,),\n+                    dynamic_shapes=dynamic_shapes,\n+                    options={\"aot_inductor.use_runtime_constant_folding\": use_runtime_constant_folding})\n+\n+            suffix = f\"{device}\"\n+            if use_runtime_constant_folding:\n+                suffix += \"_use_runtime_constant_folding\"\n+            data.update({\n+                f\"model_so_path_{suffix}\": model_so_path,\n+                f\"inputs_{suffix}\": [x],\n+                f\"outputs_{suffix}\": [ref_output],\n+                f\"w_pre_{suffix}\": model.w_pre,\n+                f\"w_add_{suffix}\": model.w_add,\n+            })\n+\n+# AOTI model which will create additional tensors during autograd.\n+def generate_test_with_additional_tensors():\n+    model = NetWithTensorConstants()\n+    x = torch.randn((30, 1), device=\"cuda\")\n+    y = torch.randn((30, 1), device=\"cuda\")\n+    with torch.no_grad():\n+        ref_output = model(x, y)\n+\n+    torch._dynamo.reset()\n+    with torch.no_grad():\n+        model_so_path = aot_compile(\n+            model,\n+            (x, y))\n+\n+    data_with_tensor_constants.update({\n+        \"model_so_path\": model_so_path,\n+        \"inputs\": [x, y],\n+        \"outputs\": [ref_output],\n+        \"w\": model.w,\n+    })\n \n-for device in [\"cpu\", \"cuda\"]:\n-    for use_runtime_constant_folding in [True, False]:\n-        if device == \"cpu\" and use_runtime_constant_folding:\n-            # We do not test runtime const folding for cpu mode.\n-            continue\n-        model = Net(device).to(device=device)\n-        x = torch.randn((4, 4), device=device)\n-        with torch.no_grad():\n-            ref_output = model(x)\n-\n-        torch._dynamo.reset()\n-        with torch.no_grad():\n-            dim0_x = Dim(\"dim0_x\", min=1, max=1024)\n-            dynamic_shapes = {\"x\": {0: dim0_x}}\n-            model_so_path = aot_compile(\n-                model,\n-                (x,),\n-                dynamic_shapes=dynamic_shapes,\n-                options={\"aot_inductor.use_runtime_constant_folding\": use_runtime_constant_folding})\n-\n-        suffix = f\"{device}\"\n-        if use_runtime_constant_folding:\n-            suffix += \"_use_runtime_constant_folding\"\n-        data.update({\n-            f\"model_so_path_{suffix}\": model_so_path,\n-            f\"inputs_{suffix}\": [x],\n-            f\"outputs_{suffix}\": [ref_output],\n-            f\"w_pre_{suffix}\": model.w_pre,\n-            f\"w_add_{suffix}\": model.w_add,\n-        })\n+generate_basic_tests()\n+generate_test_with_additional_tensors()\n \n # Use this to communicate tensors to the cpp code\n class Serializer(torch.nn.Module):\n"
                },
                {
                    "old_start": 58,
                    "old_length": 3,
                    "new_start": 94,
                    "new_length": 4,
                    "hunk": "@@ -58,3 +94,4 @@ class Serializer(torch.nn.Module):\n             setattr(self, key, data[key])\n \n torch.jit.script(Serializer(data)).save(\"data.pt\")\n+torch.jit.script(Serializer(data_with_tensor_constants)).save(\"data_with_tensor_constants.pt\")\n"
                }
            ],
            "whole_deleted": "-for device in [\"cpu\", \"cuda\"]:\n-    for use_runtime_constant_folding in [True, False]:\n-        if device == \"cpu\" and use_runtime_constant_folding:\n-            # We do not test runtime const folding for cpu mode.\n-            continue\n-        model = Net(device).to(device=device)\n-        x = torch.randn((4, 4), device=device)\n-        with torch.no_grad():\n-            ref_output = model(x)\n-\n-        torch._dynamo.reset()\n-        with torch.no_grad():\n-            dim0_x = Dim(\"dim0_x\", min=1, max=1024)\n-            dynamic_shapes = {\"x\": {0: dim0_x}}\n-            model_so_path = aot_compile(\n-                model,\n-                (x,),\n-                dynamic_shapes=dynamic_shapes,\n-                options={\"aot_inductor.use_runtime_constant_folding\": use_runtime_constant_folding})\n-\n-        suffix = f\"{device}\"\n-        if use_runtime_constant_folding:\n-            suffix += \"_use_runtime_constant_folding\"\n-        data.update({\n-            f\"model_so_path_{suffix}\": model_so_path,\n-            f\"inputs_{suffix}\": [x],\n-            f\"outputs_{suffix}\": [ref_output],\n-            f\"w_pre_{suffix}\": model.w_pre,\n-            f\"w_add_{suffix}\": model.w_add,\n-        })\n",
            "whole_added": "+class NetWithTensorConstants(torch.nn.Module):\n+    def __init__(self):\n+        super().__init__()\n+        self.w = torch.randn(30, 1, device=\"cuda\")\n+\n+    def forward(self, x, y):\n+        z = self.w * x * y\n+        return z[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17]]\n+\n+data_with_tensor_constants = {}\n+\n+# Basice AOTI model test generation.\n+def generate_basic_tests():\n+    for device in [\"cpu\", \"cuda\"]:\n+        for use_runtime_constant_folding in [True, False]:\n+            if device == \"cpu\" and use_runtime_constant_folding:\n+                # We do not test runtime const folding for cpu mode.\n+                continue\n+            model = Net(device).to(device=device)\n+            x = torch.randn((4, 4), device=device)\n+            with torch.no_grad():\n+                ref_output = model(x)\n+\n+            torch._dynamo.reset()\n+            with torch.no_grad():\n+                dim0_x = Dim(\"dim0_x\", min=1, max=1024)\n+                dynamic_shapes = {\"x\": {0: dim0_x}}\n+                model_so_path = aot_compile(\n+                    model,\n+                    (x,),\n+                    dynamic_shapes=dynamic_shapes,\n+                    options={\"aot_inductor.use_runtime_constant_folding\": use_runtime_constant_folding})\n+\n+            suffix = f\"{device}\"\n+            if use_runtime_constant_folding:\n+                suffix += \"_use_runtime_constant_folding\"\n+            data.update({\n+                f\"model_so_path_{suffix}\": model_so_path,\n+                f\"inputs_{suffix}\": [x],\n+                f\"outputs_{suffix}\": [ref_output],\n+                f\"w_pre_{suffix}\": model.w_pre,\n+                f\"w_add_{suffix}\": model.w_add,\n+            })\n+\n+# AOTI model which will create additional tensors during autograd.\n+def generate_test_with_additional_tensors():\n+    model = NetWithTensorConstants()\n+    x = torch.randn((30, 1), device=\"cuda\")\n+    y = torch.randn((30, 1), device=\"cuda\")\n+    with torch.no_grad():\n+        ref_output = model(x, y)\n+\n+    torch._dynamo.reset()\n+    with torch.no_grad():\n+        model_so_path = aot_compile(\n+            model,\n+            (x, y))\n+\n+    data_with_tensor_constants.update({\n+        \"model_so_path\": model_so_path,\n+        \"inputs\": [x, y],\n+        \"outputs\": [ref_output],\n+        \"w\": model.w,\n+    })\n+generate_basic_tests()\n+generate_test_with_additional_tensors()\n+torch.jit.script(Serializer(data_with_tensor_constants)).save(\"data_with_tensor_constants.pt\")\n",
            "whole_hunk": "@@ -17,38 +17,74 @@ class Net(torch.nn.Module):\n         w = w_relu + self.w_add\n         return torch.matmul(x, w)\n \n+class NetWithTensorConstants(torch.nn.Module):\n+    def __init__(self):\n+        super().__init__()\n+        self.w = torch.randn(30, 1, device=\"cuda\")\n+\n+    def forward(self, x, y):\n+        z = self.w * x * y\n+        return z[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17]]\n+\n data = {}\n+data_with_tensor_constants = {}\n+\n+# Basice AOTI model test generation.\n+def generate_basic_tests():\n+    for device in [\"cpu\", \"cuda\"]:\n+        for use_runtime_constant_folding in [True, False]:\n+            if device == \"cpu\" and use_runtime_constant_folding:\n+                # We do not test runtime const folding for cpu mode.\n+                continue\n+            model = Net(device).to(device=device)\n+            x = torch.randn((4, 4), device=device)\n+            with torch.no_grad():\n+                ref_output = model(x)\n+\n+            torch._dynamo.reset()\n+            with torch.no_grad():\n+                dim0_x = Dim(\"dim0_x\", min=1, max=1024)\n+                dynamic_shapes = {\"x\": {0: dim0_x}}\n+                model_so_path = aot_compile(\n+                    model,\n+                    (x,),\n+                    dynamic_shapes=dynamic_shapes,\n+                    options={\"aot_inductor.use_runtime_constant_folding\": use_runtime_constant_folding})\n+\n+            suffix = f\"{device}\"\n+            if use_runtime_constant_folding:\n+                suffix += \"_use_runtime_constant_folding\"\n+            data.update({\n+                f\"model_so_path_{suffix}\": model_so_path,\n+                f\"inputs_{suffix}\": [x],\n+                f\"outputs_{suffix}\": [ref_output],\n+                f\"w_pre_{suffix}\": model.w_pre,\n+                f\"w_add_{suffix}\": model.w_add,\n+            })\n+\n+# AOTI model which will create additional tensors during autograd.\n+def generate_test_with_additional_tensors():\n+    model = NetWithTensorConstants()\n+    x = torch.randn((30, 1), device=\"cuda\")\n+    y = torch.randn((30, 1), device=\"cuda\")\n+    with torch.no_grad():\n+        ref_output = model(x, y)\n+\n+    torch._dynamo.reset()\n+    with torch.no_grad():\n+        model_so_path = aot_compile(\n+            model,\n+            (x, y))\n+\n+    data_with_tensor_constants.update({\n+        \"model_so_path\": model_so_path,\n+        \"inputs\": [x, y],\n+        \"outputs\": [ref_output],\n+        \"w\": model.w,\n+    })\n \n-for device in [\"cpu\", \"cuda\"]:\n-    for use_runtime_constant_folding in [True, False]:\n-        if device == \"cpu\" and use_runtime_constant_folding:\n-            # We do not test runtime const folding for cpu mode.\n-            continue\n-        model = Net(device).to(device=device)\n-        x = torch.randn((4, 4), device=device)\n-        with torch.no_grad():\n-            ref_output = model(x)\n-\n-        torch._dynamo.reset()\n-        with torch.no_grad():\n-            dim0_x = Dim(\"dim0_x\", min=1, max=1024)\n-            dynamic_shapes = {\"x\": {0: dim0_x}}\n-            model_so_path = aot_compile(\n-                model,\n-                (x,),\n-                dynamic_shapes=dynamic_shapes,\n-                options={\"aot_inductor.use_runtime_constant_folding\": use_runtime_constant_folding})\n-\n-        suffix = f\"{device}\"\n-        if use_runtime_constant_folding:\n-            suffix += \"_use_runtime_constant_folding\"\n-        data.update({\n-            f\"model_so_path_{suffix}\": model_so_path,\n-            f\"inputs_{suffix}\": [x],\n-            f\"outputs_{suffix}\": [ref_output],\n-            f\"w_pre_{suffix}\": model.w_pre,\n-            f\"w_add_{suffix}\": model.w_add,\n-        })\n+generate_basic_tests()\n+generate_test_with_additional_tensors()\n \n # Use this to communicate tensors to the cpp code\n class Serializer(torch.nn.Module):\n@@ -58,3 +94,4 @@ class Serializer(torch.nn.Module):\n             setattr(self, key, data[key])\n \n torch.jit.script(Serializer(data)).save(\"data.pt\")\n+torch.jit.script(Serializer(data_with_tensor_constants)).save(\"data_with_tensor_constants.pt\")\n"
        },
        {
            "name": "model_container.h",
            "path": "torch/csrc/inductor/aoti_runtime/model_container.h",
            "patches": [
                {
                    "old_start": 218,
                    "old_length": 6,
                    "new_start": 218,
                    "new_length": 9,
                    "hunk": "@@ -218,6 +218,9 @@ class AOTInductorModelContainer {\n     pending_models_available_.notify_one();\n   }\n \n+  bool _is_tensor_constant(const std::string& constant_name) const {\n+    return constant_name.rfind(\"_tensor_constant\", 0) == 0;\n+  }\n   // This function updates the buffer for storing constants.\n   // It will update the buffer, the mapping and the array mapping.\n   void update_constant_buffer(\n"
                },
                {
                    "old_start": 232,
                    "old_length": 6,
                    "new_start": 235,
                    "new_length": 7,
                    "hunk": "@@ -232,6 +235,7 @@ class AOTInductorModelContainer {\n \n     auto* constants_blob_ptr =\n         static_cast<uint8_t*>(get_constant_blob_ptr(use_inactive));\n+    auto original_constants_map = get_constants_map(!use_inactive);\n     auto constants_map_to_update = get_constants_map(use_inactive);\n \n     if (validate_full_update) {\n"
                },
                {
                    "old_start": 243,
                    "old_length": 6,
                    "new_start": 247,
                    "new_length": 13,
                    "hunk": "@@ -243,6 +247,13 @@ class AOTInductorModelContainer {\n         auto constant_name = std::string(models_[0]->constant_name(idx));\n         auto it = constants_map.find(constant_name);\n         if (it == constants_map.end()) {\n+          if (_is_tensor_constant(constant_name)) {\n+            // tracing sometimes creates tensors that are non-existent in\n+            // original graph. We could skip those and do a direct copy.\n+            std::cerr << \"[WARNING] Found constant \" << constant_name\n+                      << \" in model, but not provided by user!\\n\";\n+            continue;\n+          }\n           throw std::runtime_error(\n               std::string(\"Cannot find constants \") + constant_name +\n               std::string(\" in constants_map!\"));\n"
                },
                {
                    "old_start": 253,
                    "old_length": 17,
                    "new_start": 264,
                    "new_length": 25,
                    "hunk": "@@ -253,17 +264,25 @@ class AOTInductorModelContainer {\n     for (size_t idx = 0; idx < num_constants; idx++) {\n       auto constant_name = std::string(models_[0]->constant_name(idx));\n       auto it = constants_map.find(constant_name);\n-      if (it == constants_map.end()) {\n+      if (it == constants_map.end() &&\n+          !(_is_tensor_constant(constant_name) && use_inactive)) {\n         continue;\n       }\n \n+      AtenTensorHandle tensor;\n+      if (_is_tensor_constant(constant_name) && use_inactive) {\n+        tensor = original_constants_map->find(constant_name)->second.get();\n+      } else {\n+        tensor = it->second;\n+      }\n+\n       // Move the data to container handled blob.\n       uint8_t* internal_constants_ptr =\n           constants_blob_ptr + constants_internal_offset_[idx];\n       void* user_constant_ptr;\n       int64_t constant_size;\n-      aoti_torch_get_data_ptr(it->second, &user_constant_ptr);\n-      aoti_torch_get_storage_size(it->second, &constant_size);\n+      aoti_torch_get_data_ptr(tensor, &user_constant_ptr);\n+      aoti_torch_get_storage_size(tensor, &constant_size);\n \n       AOTI_RUNTIME_DEVICE_CHECK(cudaMemcpy(\n           internal_constants_ptr,\n"
                },
                {
                    "old_start": 278,
                    "old_length": 9,
                    "new_start": 297,
                    "new_length": 9,
                    "hunk": "@@ -278,9 +297,9 @@ class AOTInductorModelContainer {\n       int64_t* stride;\n       int64_t offset;\n       int device_idx = models_[0]->get_device_idx();\n-      AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_strides(it->second, &stride));\n+      AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_strides(tensor, &stride));\n       AOTI_TORCH_ERROR_CODE_CHECK(\n-          aoti_torch_get_storage_offset(it->second, &offset));\n+          aoti_torch_get_storage_offset(tensor, &offset));\n       AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_create_tensor_from_blob(\n           internal_constants_ptr,\n           models_[0]->constant_ndim(idx),"
                }
            ],
            "whole_deleted": "-      if (it == constants_map.end()) {\n-      aoti_torch_get_data_ptr(it->second, &user_constant_ptr);\n-      aoti_torch_get_storage_size(it->second, &constant_size);\n-      AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_strides(it->second, &stride));\n-          aoti_torch_get_storage_offset(it->second, &offset));\n",
            "whole_added": "+  bool _is_tensor_constant(const std::string& constant_name) const {\n+    return constant_name.rfind(\"_tensor_constant\", 0) == 0;\n+  }\n+    auto original_constants_map = get_constants_map(!use_inactive);\n+          if (_is_tensor_constant(constant_name)) {\n+            // tracing sometimes creates tensors that are non-existent in\n+            // original graph. We could skip those and do a direct copy.\n+            std::cerr << \"[WARNING] Found constant \" << constant_name\n+                      << \" in model, but not provided by user!\\n\";\n+            continue;\n+          }\n+      if (it == constants_map.end() &&\n+          !(_is_tensor_constant(constant_name) && use_inactive)) {\n+      AtenTensorHandle tensor;\n+      if (_is_tensor_constant(constant_name) && use_inactive) {\n+        tensor = original_constants_map->find(constant_name)->second.get();\n+      } else {\n+        tensor = it->second;\n+      }\n+\n+      aoti_torch_get_data_ptr(tensor, &user_constant_ptr);\n+      aoti_torch_get_storage_size(tensor, &constant_size);\n+      AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_strides(tensor, &stride));\n+          aoti_torch_get_storage_offset(tensor, &offset));\n",
            "whole_hunk": "@@ -218,6 +218,9 @@ class AOTInductorModelContainer {\n     pending_models_available_.notify_one();\n   }\n \n+  bool _is_tensor_constant(const std::string& constant_name) const {\n+    return constant_name.rfind(\"_tensor_constant\", 0) == 0;\n+  }\n   // This function updates the buffer for storing constants.\n   // It will update the buffer, the mapping and the array mapping.\n   void update_constant_buffer(\n@@ -232,6 +235,7 @@ class AOTInductorModelContainer {\n \n     auto* constants_blob_ptr =\n         static_cast<uint8_t*>(get_constant_blob_ptr(use_inactive));\n+    auto original_constants_map = get_constants_map(!use_inactive);\n     auto constants_map_to_update = get_constants_map(use_inactive);\n \n     if (validate_full_update) {\n@@ -243,6 +247,13 @@ class AOTInductorModelContainer {\n         auto constant_name = std::string(models_[0]->constant_name(idx));\n         auto it = constants_map.find(constant_name);\n         if (it == constants_map.end()) {\n+          if (_is_tensor_constant(constant_name)) {\n+            // tracing sometimes creates tensors that are non-existent in\n+            // original graph. We could skip those and do a direct copy.\n+            std::cerr << \"[WARNING] Found constant \" << constant_name\n+                      << \" in model, but not provided by user!\\n\";\n+            continue;\n+          }\n           throw std::runtime_error(\n               std::string(\"Cannot find constants \") + constant_name +\n               std::string(\" in constants_map!\"));\n@@ -253,17 +264,25 @@ class AOTInductorModelContainer {\n     for (size_t idx = 0; idx < num_constants; idx++) {\n       auto constant_name = std::string(models_[0]->constant_name(idx));\n       auto it = constants_map.find(constant_name);\n-      if (it == constants_map.end()) {\n+      if (it == constants_map.end() &&\n+          !(_is_tensor_constant(constant_name) && use_inactive)) {\n         continue;\n       }\n \n+      AtenTensorHandle tensor;\n+      if (_is_tensor_constant(constant_name) && use_inactive) {\n+        tensor = original_constants_map->find(constant_name)->second.get();\n+      } else {\n+        tensor = it->second;\n+      }\n+\n       // Move the data to container handled blob.\n       uint8_t* internal_constants_ptr =\n           constants_blob_ptr + constants_internal_offset_[idx];\n       void* user_constant_ptr;\n       int64_t constant_size;\n-      aoti_torch_get_data_ptr(it->second, &user_constant_ptr);\n-      aoti_torch_get_storage_size(it->second, &constant_size);\n+      aoti_torch_get_data_ptr(tensor, &user_constant_ptr);\n+      aoti_torch_get_storage_size(tensor, &constant_size);\n \n       AOTI_RUNTIME_DEVICE_CHECK(cudaMemcpy(\n           internal_constants_ptr,\n@@ -278,9 +297,9 @@ class AOTInductorModelContainer {\n       int64_t* stride;\n       int64_t offset;\n       int device_idx = models_[0]->get_device_idx();\n-      AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_strides(it->second, &stride));\n+      AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_strides(tensor, &stride));\n       AOTI_TORCH_ERROR_CODE_CHECK(\n-          aoti_torch_get_storage_offset(it->second, &offset));\n+          aoti_torch_get_storage_offset(tensor, &offset));\n       AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_create_tensor_from_blob(\n           internal_constants_ptr,\n           models_[0]->constant_ndim(idx),"
        }
    ]
},
{
    "Id": 68,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/8bfd9e98154b83fc6bb0d4407e103d0eb65331ae",
    "date": "2024-06-25T06:01:50+00:00",
    "message": "[cuDNN] Graph-capturable cuDNN CTCLoss (#128271)\n\ncuDNN v8.x added a graph-capturable CTCLoss, which slots \"neatly\" into the `Tensor` variant\n\n~~WIP as cuDNN has a restriction on the max target length (255), but this is not checkable in the graph-capture case, so the UX around warnings/error-messages here might need to be tuned...~~\nCurrently checks restriction on max target length during warmup run(s), and bails out during capture if this constraint was violated during warmup.\n\nCC @ptrblck\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128271\nApproved by: https://github.com/ezyang, https://github.com/malfet",
    "label": "YES",
    "changes": [
        {
            "name": "Descriptors.h",
            "path": "aten/src/ATen/cudnn/Descriptors.h",
            "patches": [
                {
                    "old_start": 357,
                    "old_length": 6,
                    "new_start": 357,
                    "new_length": 24,
                    "hunk": "@@ -357,6 +357,24 @@ struct TORCH_CUDA_CPP_API CTCLossDescriptor\n     AT_CUDNN_CHECK(\n         cudnnSetCTCLossDescriptorEx(mut_desc(), datatype, normMode, gradMode));\n   }\n+  void set_v8_v9(\n+      cudnnDataType_t datatype,\n+      cudnnLossNormalizationMode_t normMode,\n+      cudnnNanPropagation_t gradMode,\n+      int maxLabelLength) {\n+#if defined(CUDNN_VERSION) && CUDNN_VERSION >= 90000\n+    auto gradModev9 = CUDNN_CTC_ZERO_OOB_GRADIENTS;\n+    if (gradMode == cudnnNanPropagation_t::CUDNN_PROPAGATE_NAN) {\n+      gradModev9 = CUDNN_CTC_SKIP_OOB_GRADIENTS;\n+    }\n+    AT_CUDNN_CHECK(\n+        cudnnSetCTCLossDescriptor_v9(mut_desc(), datatype, normMode, gradModev9, maxLabelLength));\n+#else\n+    AT_CUDNN_CHECK(\n+        cudnnSetCTCLossDescriptor_v8(mut_desc(), datatype, normMode, gradMode, maxLabelLength));\n+#endif\n+  }\n+\n };\n \n struct TORCH_CUDA_CPP_API ActivationDescriptor\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  void set_v8_v9(\n+      cudnnDataType_t datatype,\n+      cudnnLossNormalizationMode_t normMode,\n+      cudnnNanPropagation_t gradMode,\n+      int maxLabelLength) {\n+#if defined(CUDNN_VERSION) && CUDNN_VERSION >= 90000\n+    auto gradModev9 = CUDNN_CTC_ZERO_OOB_GRADIENTS;\n+    if (gradMode == cudnnNanPropagation_t::CUDNN_PROPAGATE_NAN) {\n+      gradModev9 = CUDNN_CTC_SKIP_OOB_GRADIENTS;\n+    }\n+    AT_CUDNN_CHECK(\n+        cudnnSetCTCLossDescriptor_v9(mut_desc(), datatype, normMode, gradModev9, maxLabelLength));\n+#else\n+    AT_CUDNN_CHECK(\n+        cudnnSetCTCLossDescriptor_v8(mut_desc(), datatype, normMode, gradMode, maxLabelLength));\n+#endif\n+  }\n+\n",
            "whole_hunk": "@@ -357,6 +357,24 @@ struct TORCH_CUDA_CPP_API CTCLossDescriptor\n     AT_CUDNN_CHECK(\n         cudnnSetCTCLossDescriptorEx(mut_desc(), datatype, normMode, gradMode));\n   }\n+  void set_v8_v9(\n+      cudnnDataType_t datatype,\n+      cudnnLossNormalizationMode_t normMode,\n+      cudnnNanPropagation_t gradMode,\n+      int maxLabelLength) {\n+#if defined(CUDNN_VERSION) && CUDNN_VERSION >= 90000\n+    auto gradModev9 = CUDNN_CTC_ZERO_OOB_GRADIENTS;\n+    if (gradMode == cudnnNanPropagation_t::CUDNN_PROPAGATE_NAN) {\n+      gradModev9 = CUDNN_CTC_SKIP_OOB_GRADIENTS;\n+    }\n+    AT_CUDNN_CHECK(\n+        cudnnSetCTCLossDescriptor_v9(mut_desc(), datatype, normMode, gradModev9, maxLabelLength));\n+#else\n+    AT_CUDNN_CHECK(\n+        cudnnSetCTCLossDescriptor_v8(mut_desc(), datatype, normMode, gradMode, maxLabelLength));\n+#endif\n+  }\n+\n };\n \n struct TORCH_CUDA_CPP_API ActivationDescriptor\n"
        },
        {
            "name": "LossCTC.cpp",
            "path": "aten/src/ATen/native/LossCTC.cpp",
            "patches": [
                {
                    "old_start": 539,
                    "old_length": 8,
                    "new_start": 539,
                    "new_length": 13,
                    "hunk": "@@ -539,8 +539,13 @@ Tensor ctc_loss(const Tensor& log_probs_, const Tensor& targets, IntArrayRef inp\n \n // Convenience function accepting Tensors\n Tensor ctc_loss(const Tensor& log_probs, const Tensor& targets, const Tensor& input_lengths, const Tensor& target_lengths, int64_t BLANK, int64_t reduction, bool zero_infinity) {\n+  // we don't want to convert to IntArrayRef if we can dispatch to cuDNN (this allows graph-capturable ctc_loss)\n+  bool use_cudnn =\n+      (log_probs.device().type() == at::kCUDA) &&\n+      at::_use_cudnn_ctc_loss(\n+          log_probs, targets, input_lengths, target_lengths, BLANK);\n   if (at::areAnyTensorSubclassLike(\n-          {log_probs, targets, input_lengths, target_lengths})) {\n+          {log_probs, targets, input_lengths, target_lengths}) || use_cudnn) {\n     // Composite Compliant path for TensorSubclasses\n     return ctc_loss_impl(log_probs, targets, input_lengths, target_lengths, BLANK, reduction, zero_infinity);\n   }\n"
                }
            ],
            "whole_deleted": "-          {log_probs, targets, input_lengths, target_lengths})) {\n",
            "whole_added": "+  // we don't want to convert to IntArrayRef if we can dispatch to cuDNN (this allows graph-capturable ctc_loss)\n+  bool use_cudnn =\n+      (log_probs.device().type() == at::kCUDA) &&\n+      at::_use_cudnn_ctc_loss(\n+          log_probs, targets, input_lengths, target_lengths, BLANK);\n+          {log_probs, targets, input_lengths, target_lengths}) || use_cudnn) {\n",
            "whole_hunk": "@@ -539,8 +539,13 @@ Tensor ctc_loss(const Tensor& log_probs_, const Tensor& targets, IntArrayRef inp\n \n // Convenience function accepting Tensors\n Tensor ctc_loss(const Tensor& log_probs, const Tensor& targets, const Tensor& input_lengths, const Tensor& target_lengths, int64_t BLANK, int64_t reduction, bool zero_infinity) {\n+  // we don't want to convert to IntArrayRef if we can dispatch to cuDNN (this allows graph-capturable ctc_loss)\n+  bool use_cudnn =\n+      (log_probs.device().type() == at::kCUDA) &&\n+      at::_use_cudnn_ctc_loss(\n+          log_probs, targets, input_lengths, target_lengths, BLANK);\n   if (at::areAnyTensorSubclassLike(\n-          {log_probs, targets, input_lengths, target_lengths})) {\n+          {log_probs, targets, input_lengths, target_lengths}) || use_cudnn) {\n     // Composite Compliant path for TensorSubclasses\n     return ctc_loss_impl(log_probs, targets, input_lengths, target_lengths, BLANK, reduction, zero_infinity);\n   }\n"
        },
        {
            "name": "LossCTC.cpp",
            "path": "aten/src/ATen/native/cudnn/LossCTC.cpp",
            "patches": [
                {
                    "old_start": 2,
                    "old_length": 6,
                    "new_start": 2,
                    "new_length": 7,
                    "hunk": "@@ -2,6 +2,7 @@\n #include <ATen/Config.h>\n #include <ATen/core/Tensor.h>\n #include <ATen/cuda/CUDAConfig.h>\n+#include <ATen/cuda/CUDAGraphsUtils.cuh>\n #if AT_CUDNN_ENABLED()\n #include <ATen/cudnn/Descriptors.h>\n #endif\n"
                },
                {
                    "old_start": 62,
                    "old_length": 7,
                    "new_start": 63,
                    "new_length": 7,
                    "hunk": "@@ -62,7 +63,7 @@ std::tuple<Tensor, Tensor> _cudnn_ctc_loss_tensor(\n     int64_t BLANK,\n     bool deterministic,\n     bool zero_infinity) {\n-  AT_ERROR(\"cudnn_ctc_loss: ATen not compiled with cuDNN >= 7 support\");\n+  AT_ERROR(\"cudnn_ctc_loss: ATen not compiled with cuDNN >= 8 support\");\n }\n \n } // namespace native\n"
                },
                {
                    "old_start": 80,
                    "old_length": 6,
                    "new_start": 81,
                    "new_length": 11,
                    "hunk": "@@ -80,6 +81,11 @@ std::tuple<Tensor, Tensor> _cudnn_ctc_loss_tensor(\n namespace at {\n namespace native {\n \n+namespace {\n+// \"cache\" whether we've previously failed the target lengths check\n+static bool tensor_failed_target_lengths_check = false;\n+} // namespace\n+\n bool _use_cudnn_ctc_loss(\n     const Tensor& log_probs,\n     const Tensor& targets,\n"
                },
                {
                    "old_start": 91,
                    "old_length": 9,
                    "new_start": 97,
                    "new_length": 8,
                    "hunk": "@@ -91,9 +97,8 @@ bool _use_cudnn_ctc_loss(\n   bool use_cudnn = ctx.userEnabledCuDNN() && (BLANK == 0) &&\n       (targets.dim() == 1) && (log_probs.scalar_type() == at::kFloat) &&\n       (targets.scalar_type() == at::kInt) &&\n-      (log_probs.device().type() == at::kCUDA) &&\n       (targets.device().type() == at::kCPU) && (targets.is_contiguous()) &&\n-      (log_probs.dim() == 3);\n+      (log_probs.device().type() == at::kCUDA) && (log_probs.dim() == 3);\n \n   if (use_cudnn) {\n     // we don't know that input_lengths and target_lengths have the same size\n"
                },
                {
                    "old_start": 118,
                    "old_length": 11,
                    "new_start": 123,
                    "new_length": 42,
                    "hunk": "@@ -118,11 +123,42 @@ bool _use_cudnn_ctc_loss_tensor(\n     const Tensor& input_lengths,\n     const Tensor& target_lengths,\n     int64_t BLANK) {\n-  Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());\n-  IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n-  return at::_use_cudnn_ctc_loss(log_probs, targets, il, tl, BLANK);\n+  auto& ctx = at::globalContext();\n+\n+  bool use_cudnn = ctx.userEnabledCuDNN() && (BLANK == 0) &&\n+      (targets.dim() == 1) && (log_probs.scalar_type() == at::kFloat) &&\n+      (targets.scalar_type() == at::kInt) &&\n+      (log_probs.device().type() == at::kCUDA) && (targets.is_contiguous()) &&\n+      (log_probs.dim() == 3) && (input_lengths.scalar_type() == at::kInt) &&\n+      (target_lengths.scalar_type() == at::kInt);\n+\n+  if (at::cuda::currentStreamCaptureStatus() == at::cuda::CaptureStatus::None) {\n+    Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n+    IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n+    for (const auto b : c10::irange(tl.size())) {\n+      // target length < 256 is documented, but we see illegal memory accesses\n+      // when target lengths > input lengths for CuDNN\n+      Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n+      Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n+      IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());\n+      IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n+      use_cudnn = use_cudnn && (tl[b] < 256) && (tl[b] <= il[b]);\n+      if (!use_cudnn) {\n+        tensor_failed_target_lengths_check = true;\n+        break;\n+      }\n+    }\n+  } else {\n+    use_cudnn = use_cudnn && !tensor_failed_target_lengths_check;\n+    if (tensor_failed_target_lengths_check) {\n+      TORCH_WARN(\n+          \"cuDNN max target length restriction < 256 cannot be checked during graph capture,\"\n+          \" but target length >= 256 was observed previously e.g., during warmup, so we\"\n+          \" presume it is unsafe to dispatch to cuDNN ctc_loss.\");\n+    }\n+  }\n+\n+  return use_cudnn;\n }\n \n std::tuple<Tensor, Tensor> _cudnn_ctc_loss(\n"
                },
                {
                    "old_start": 211,
                    "old_length": 19,
                    "new_start": 247,
                    "new_length": 90,
                    "hunk": "@@ -211,19 +247,90 @@ std::tuple<Tensor, Tensor> _cudnn_ctc_loss(\n }\n \n std::tuple<Tensor, Tensor> _cudnn_ctc_loss_tensor(\n-    const Tensor& log_probs,\n-    const Tensor& targets,\n+    const Tensor& log_probs_t,\n+    const Tensor& targets_t,\n     const Tensor& input_lengths,\n     const Tensor& target_lengths,\n     int64_t BLANK,\n     bool deterministic,\n     bool zero_infinity) {\n-  Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());\n-  IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n-  return at::_cudnn_ctc_loss(\n-      log_probs, targets, il, tl, BLANK, deterministic, zero_infinity);\n+  Tensor targets_t_ = targets_t;\n+  if (targets_t.device().type() == at::kCPU) {\n+    targets_t_ = targets_t.to(Device(at::kCUDA));\n+  }\n+  const CheckedFrom c = \"cudnn_ctc_loss\";\n+  const TensorArg log_probs{log_probs_t, \"log_probs\", 1};\n+  const TensorArg targets{targets_t_, \"targets\", 2};\n+  checkDim(c, log_probs, 3);\n+  checkScalarType(c, log_probs, kFloat);\n+  checkDim(c, targets, 1);\n+  checkScalarType(c, targets, kInt);\n+  checkContiguous(c, targets); // ?\n+  checkBackend(c, {*log_probs}, Backend::CUDA);\n+  checkBackend(c, {*targets}, Backend::CUDA);\n+  const auto batch_size = log_probs->size(1);\n+  int64_t input_lengths_size =\n+      input_lengths.sizes().size() ? input_lengths.size(0) : 1;\n+  int64_t target_lengths_size =\n+      target_lengths.sizes().size() ? target_lengths.size(0) : 1;\n+  TORCH_CHECK(\n+      input_lengths_size == batch_size,\n+      \"input_lengths needs to have size to match batch_size\");\n+  TORCH_CHECK(\n+      target_lengths_size == batch_size,\n+      \"target_lengths needs to have size to match batch_size\");\n+\n+  TORCH_CHECK(BLANK == 0, \"blank must be label 0 for cudnn_ctc_loss\");\n+  // checked in dispatch:\n+  // assert other conditions for cudnnCTCLoss: all label lengths <= 256\n+  // all input lengths = logprob.size(0)\n+\n+  const auto handle = getCudnnHandle();\n+\n+  const cudnnCTCLossAlgo_t algo =\n+      (deterministic ? CUDNN_CTC_LOSS_ALGO_DETERMINISTIC\n+                     : CUDNN_CTC_LOSS_ALGO_NON_DETERMINISTIC);\n+\n+  CTCLossDescriptor ctc_loss_desc;\n+\n+  ctc_loss_desc.set_v8_v9(\n+      CUDNN_DATA_FLOAT,\n+      CUDNN_LOSS_NORMALIZATION_SOFTMAX,\n+      CUDNN_PROPAGATE_NAN,\n+      255);\n+  TensorDescriptor log_probs_desc{log_probs_t};\n+  Tensor grad = at::empty_like(log_probs_t, LEGACY_CONTIGUOUS_MEMORY_FORMAT);\n+  TensorDescriptor grad_desc{grad};\n+\n+  size_t workspace_size;\n+  AT_CUDNN_CHECK(cudnnGetCTCLossWorkspaceSize_v8(\n+      handle,\n+      algo,\n+      ctc_loss_desc.desc(),\n+      log_probs_desc.desc(),\n+      grad_desc.desc(),\n+      &workspace_size));\n+  Tensor workspace =\n+      at::empty(workspace_size, log_probs->options().dtype(kByte));\n+  Tensor costs = at::empty({log_probs->size(1)}, log_probs->options());\n+\n+  AT_CUDNN_CHECK(cudnnCTCLoss_v8(\n+      handle,\n+      algo,\n+      ctc_loss_desc.desc(),\n+      log_probs_desc.desc(),\n+      log_probs_t.data_ptr(),\n+      targets_t_.data_ptr<int>(),\n+      target_lengths.data_ptr<int>(),\n+      input_lengths.data_ptr<int>(),\n+      costs.data_ptr(),\n+      grad_desc.desc(),\n+      grad.data_ptr(),\n+      workspace_size,\n+      workspace.data_ptr()\n+\n+          ));\n+  return std::make_tuple(costs, grad);\n }\n \n } // namespace native\n"
                }
            ],
            "whole_deleted": "-  AT_ERROR(\"cudnn_ctc_loss: ATen not compiled with cuDNN >= 7 support\");\n-      (log_probs.device().type() == at::kCUDA) &&\n-      (log_probs.dim() == 3);\n-  Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());\n-  IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n-  return at::_use_cudnn_ctc_loss(log_probs, targets, il, tl, BLANK);\n-    const Tensor& log_probs,\n-    const Tensor& targets,\n-  Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());\n-  IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n-  return at::_cudnn_ctc_loss(\n-      log_probs, targets, il, tl, BLANK, deterministic, zero_infinity);\n",
            "whole_added": "+#include <ATen/cuda/CUDAGraphsUtils.cuh>\n+  AT_ERROR(\"cudnn_ctc_loss: ATen not compiled with cuDNN >= 8 support\");\n+namespace {\n+// \"cache\" whether we've previously failed the target lengths check\n+static bool tensor_failed_target_lengths_check = false;\n+} // namespace\n+\n+      (log_probs.device().type() == at::kCUDA) && (log_probs.dim() == 3);\n+  auto& ctx = at::globalContext();\n+\n+  bool use_cudnn = ctx.userEnabledCuDNN() && (BLANK == 0) &&\n+      (targets.dim() == 1) && (log_probs.scalar_type() == at::kFloat) &&\n+      (targets.scalar_type() == at::kInt) &&\n+      (log_probs.device().type() == at::kCUDA) && (targets.is_contiguous()) &&\n+      (log_probs.dim() == 3) && (input_lengths.scalar_type() == at::kInt) &&\n+      (target_lengths.scalar_type() == at::kInt);\n+\n+  if (at::cuda::currentStreamCaptureStatus() == at::cuda::CaptureStatus::None) {\n+    Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n+    IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n+    for (const auto b : c10::irange(tl.size())) {\n+      // target length < 256 is documented, but we see illegal memory accesses\n+      // when target lengths > input lengths for CuDNN\n+      Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n+      Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n+      IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());\n+      IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n+      use_cudnn = use_cudnn && (tl[b] < 256) && (tl[b] <= il[b]);\n+      if (!use_cudnn) {\n+        tensor_failed_target_lengths_check = true;\n+        break;\n+      }\n+    }\n+  } else {\n+    use_cudnn = use_cudnn && !tensor_failed_target_lengths_check;\n+    if (tensor_failed_target_lengths_check) {\n+      TORCH_WARN(\n+          \"cuDNN max target length restriction < 256 cannot be checked during graph capture,\"\n+          \" but target length >= 256 was observed previously e.g., during warmup, so we\"\n+          \" presume it is unsafe to dispatch to cuDNN ctc_loss.\");\n+    }\n+  }\n+\n+  return use_cudnn;\n+    const Tensor& log_probs_t,\n+    const Tensor& targets_t,\n+  Tensor targets_t_ = targets_t;\n+  if (targets_t.device().type() == at::kCPU) {\n+    targets_t_ = targets_t.to(Device(at::kCUDA));\n+  }\n+  const CheckedFrom c = \"cudnn_ctc_loss\";\n+  const TensorArg log_probs{log_probs_t, \"log_probs\", 1};\n+  const TensorArg targets{targets_t_, \"targets\", 2};\n+  checkDim(c, log_probs, 3);\n+  checkScalarType(c, log_probs, kFloat);\n+  checkDim(c, targets, 1);\n+  checkScalarType(c, targets, kInt);\n+  checkContiguous(c, targets); // ?\n+  checkBackend(c, {*log_probs}, Backend::CUDA);\n+  checkBackend(c, {*targets}, Backend::CUDA);\n+  const auto batch_size = log_probs->size(1);\n+  int64_t input_lengths_size =\n+      input_lengths.sizes().size() ? input_lengths.size(0) : 1;\n+  int64_t target_lengths_size =\n+      target_lengths.sizes().size() ? target_lengths.size(0) : 1;\n+  TORCH_CHECK(\n+      input_lengths_size == batch_size,\n+      \"input_lengths needs to have size to match batch_size\");\n+  TORCH_CHECK(\n+      target_lengths_size == batch_size,\n+      \"target_lengths needs to have size to match batch_size\");\n+\n+  TORCH_CHECK(BLANK == 0, \"blank must be label 0 for cudnn_ctc_loss\");\n+  // checked in dispatch:\n+  // assert other conditions for cudnnCTCLoss: all label lengths <= 256\n+  // all input lengths = logprob.size(0)\n+\n+  const auto handle = getCudnnHandle();\n+\n+  const cudnnCTCLossAlgo_t algo =\n+      (deterministic ? CUDNN_CTC_LOSS_ALGO_DETERMINISTIC\n+                     : CUDNN_CTC_LOSS_ALGO_NON_DETERMINISTIC);\n+\n+  CTCLossDescriptor ctc_loss_desc;\n+\n+  ctc_loss_desc.set_v8_v9(\n+      CUDNN_DATA_FLOAT,\n+      CUDNN_LOSS_NORMALIZATION_SOFTMAX,\n+      CUDNN_PROPAGATE_NAN,\n+      255);\n+  TensorDescriptor log_probs_desc{log_probs_t};\n+  Tensor grad = at::empty_like(log_probs_t, LEGACY_CONTIGUOUS_MEMORY_FORMAT);\n+  TensorDescriptor grad_desc{grad};\n+\n+  size_t workspace_size;\n+  AT_CUDNN_CHECK(cudnnGetCTCLossWorkspaceSize_v8(\n+      handle,\n+      algo,\n+      ctc_loss_desc.desc(),\n+      log_probs_desc.desc(),\n+      grad_desc.desc(),\n+      &workspace_size));\n+  Tensor workspace =\n+      at::empty(workspace_size, log_probs->options().dtype(kByte));\n+  Tensor costs = at::empty({log_probs->size(1)}, log_probs->options());\n+\n+  AT_CUDNN_CHECK(cudnnCTCLoss_v8(\n+      handle,\n+      algo,\n+      ctc_loss_desc.desc(),\n+      log_probs_desc.desc(),\n+      log_probs_t.data_ptr(),\n+      targets_t_.data_ptr<int>(),\n+      target_lengths.data_ptr<int>(),\n+      input_lengths.data_ptr<int>(),\n+      costs.data_ptr(),\n+      grad_desc.desc(),\n+      grad.data_ptr(),\n+      workspace_size,\n+      workspace.data_ptr()\n+\n+          ));\n+  return std::make_tuple(costs, grad);\n",
            "whole_hunk": "@@ -2,6 +2,7 @@\n #include <ATen/Config.h>\n #include <ATen/core/Tensor.h>\n #include <ATen/cuda/CUDAConfig.h>\n+#include <ATen/cuda/CUDAGraphsUtils.cuh>\n #if AT_CUDNN_ENABLED()\n #include <ATen/cudnn/Descriptors.h>\n #endif\n@@ -62,7 +63,7 @@ std::tuple<Tensor, Tensor> _cudnn_ctc_loss_tensor(\n     int64_t BLANK,\n     bool deterministic,\n     bool zero_infinity) {\n-  AT_ERROR(\"cudnn_ctc_loss: ATen not compiled with cuDNN >= 7 support\");\n+  AT_ERROR(\"cudnn_ctc_loss: ATen not compiled with cuDNN >= 8 support\");\n }\n \n } // namespace native\n@@ -80,6 +81,11 @@ std::tuple<Tensor, Tensor> _cudnn_ctc_loss_tensor(\n namespace at {\n namespace native {\n \n+namespace {\n+// \"cache\" whether we've previously failed the target lengths check\n+static bool tensor_failed_target_lengths_check = false;\n+} // namespace\n+\n bool _use_cudnn_ctc_loss(\n     const Tensor& log_probs,\n     const Tensor& targets,\n@@ -91,9 +97,8 @@ bool _use_cudnn_ctc_loss(\n   bool use_cudnn = ctx.userEnabledCuDNN() && (BLANK == 0) &&\n       (targets.dim() == 1) && (log_probs.scalar_type() == at::kFloat) &&\n       (targets.scalar_type() == at::kInt) &&\n-      (log_probs.device().type() == at::kCUDA) &&\n       (targets.device().type() == at::kCPU) && (targets.is_contiguous()) &&\n-      (log_probs.dim() == 3);\n+      (log_probs.device().type() == at::kCUDA) && (log_probs.dim() == 3);\n \n   if (use_cudnn) {\n     // we don't know that input_lengths and target_lengths have the same size\n@@ -118,11 +123,42 @@ bool _use_cudnn_ctc_loss_tensor(\n     const Tensor& input_lengths,\n     const Tensor& target_lengths,\n     int64_t BLANK) {\n-  Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());\n-  IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n-  return at::_use_cudnn_ctc_loss(log_probs, targets, il, tl, BLANK);\n+  auto& ctx = at::globalContext();\n+\n+  bool use_cudnn = ctx.userEnabledCuDNN() && (BLANK == 0) &&\n+      (targets.dim() == 1) && (log_probs.scalar_type() == at::kFloat) &&\n+      (targets.scalar_type() == at::kInt) &&\n+      (log_probs.device().type() == at::kCUDA) && (targets.is_contiguous()) &&\n+      (log_probs.dim() == 3) && (input_lengths.scalar_type() == at::kInt) &&\n+      (target_lengths.scalar_type() == at::kInt);\n+\n+  if (at::cuda::currentStreamCaptureStatus() == at::cuda::CaptureStatus::None) {\n+    Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n+    IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n+    for (const auto b : c10::irange(tl.size())) {\n+      // target length < 256 is documented, but we see illegal memory accesses\n+      // when target lengths > input lengths for CuDNN\n+      Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n+      Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n+      IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());\n+      IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n+      use_cudnn = use_cudnn && (tl[b] < 256) && (tl[b] <= il[b]);\n+      if (!use_cudnn) {\n+        tensor_failed_target_lengths_check = true;\n+        break;\n+      }\n+    }\n+  } else {\n+    use_cudnn = use_cudnn && !tensor_failed_target_lengths_check;\n+    if (tensor_failed_target_lengths_check) {\n+      TORCH_WARN(\n+          \"cuDNN max target length restriction < 256 cannot be checked during graph capture,\"\n+          \" but target length >= 256 was observed previously e.g., during warmup, so we\"\n+          \" presume it is unsafe to dispatch to cuDNN ctc_loss.\");\n+    }\n+  }\n+\n+  return use_cudnn;\n }\n \n std::tuple<Tensor, Tensor> _cudnn_ctc_loss(\n@@ -211,19 +247,90 @@ std::tuple<Tensor, Tensor> _cudnn_ctc_loss(\n }\n \n std::tuple<Tensor, Tensor> _cudnn_ctc_loss_tensor(\n-    const Tensor& log_probs,\n-    const Tensor& targets,\n+    const Tensor& log_probs_t,\n+    const Tensor& targets_t,\n     const Tensor& input_lengths,\n     const Tensor& target_lengths,\n     int64_t BLANK,\n     bool deterministic,\n     bool zero_infinity) {\n-  Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n-  IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());\n-  IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n-  return at::_cudnn_ctc_loss(\n-      log_probs, targets, il, tl, BLANK, deterministic, zero_infinity);\n+  Tensor targets_t_ = targets_t;\n+  if (targets_t.device().type() == at::kCPU) {\n+    targets_t_ = targets_t.to(Device(at::kCUDA));\n+  }\n+  const CheckedFrom c = \"cudnn_ctc_loss\";\n+  const TensorArg log_probs{log_probs_t, \"log_probs\", 1};\n+  const TensorArg targets{targets_t_, \"targets\", 2};\n+  checkDim(c, log_probs, 3);\n+  checkScalarType(c, log_probs, kFloat);\n+  checkDim(c, targets, 1);\n+  checkScalarType(c, targets, kInt);\n+  checkContiguous(c, targets); // ?\n+  checkBackend(c, {*log_probs}, Backend::CUDA);\n+  checkBackend(c, {*targets}, Backend::CUDA);\n+  const auto batch_size = log_probs->size(1);\n+  int64_t input_lengths_size =\n+      input_lengths.sizes().size() ? input_lengths.size(0) : 1;\n+  int64_t target_lengths_size =\n+      target_lengths.sizes().size() ? target_lengths.size(0) : 1;\n+  TORCH_CHECK(\n+      input_lengths_size == batch_size,\n+      \"input_lengths needs to have size to match batch_size\");\n+  TORCH_CHECK(\n+      target_lengths_size == batch_size,\n+      \"target_lengths needs to have size to match batch_size\");\n+\n+  TORCH_CHECK(BLANK == 0, \"blank must be label 0 for cudnn_ctc_loss\");\n+  // checked in dispatch:\n+  // assert other conditions for cudnnCTCLoss: all label lengths <= 256\n+  // all input lengths = logprob.size(0)\n+\n+  const auto handle = getCudnnHandle();\n+\n+  const cudnnCTCLossAlgo_t algo =\n+      (deterministic ? CUDNN_CTC_LOSS_ALGO_DETERMINISTIC\n+                     : CUDNN_CTC_LOSS_ALGO_NON_DETERMINISTIC);\n+\n+  CTCLossDescriptor ctc_loss_desc;\n+\n+  ctc_loss_desc.set_v8_v9(\n+      CUDNN_DATA_FLOAT,\n+      CUDNN_LOSS_NORMALIZATION_SOFTMAX,\n+      CUDNN_PROPAGATE_NAN,\n+      255);\n+  TensorDescriptor log_probs_desc{log_probs_t};\n+  Tensor grad = at::empty_like(log_probs_t, LEGACY_CONTIGUOUS_MEMORY_FORMAT);\n+  TensorDescriptor grad_desc{grad};\n+\n+  size_t workspace_size;\n+  AT_CUDNN_CHECK(cudnnGetCTCLossWorkspaceSize_v8(\n+      handle,\n+      algo,\n+      ctc_loss_desc.desc(),\n+      log_probs_desc.desc(),\n+      grad_desc.desc(),\n+      &workspace_size));\n+  Tensor workspace =\n+      at::empty(workspace_size, log_probs->options().dtype(kByte));\n+  Tensor costs = at::empty({log_probs->size(1)}, log_probs->options());\n+\n+  AT_CUDNN_CHECK(cudnnCTCLoss_v8(\n+      handle,\n+      algo,\n+      ctc_loss_desc.desc(),\n+      log_probs_desc.desc(),\n+      log_probs_t.data_ptr(),\n+      targets_t_.data_ptr<int>(),\n+      target_lengths.data_ptr<int>(),\n+      input_lengths.data_ptr<int>(),\n+      costs.data_ptr(),\n+      grad_desc.desc(),\n+      grad.data_ptr(),\n+      workspace_size,\n+      workspace.data_ptr()\n+\n+          ));\n+  return std::make_tuple(costs, grad);\n }\n \n } // namespace native\n"
        },
        {
            "name": "test_nn.py",
            "path": "test/test_nn.py",
            "patches": [
                {
                    "old_start": 11144,
                    "old_length": 6,
                    "new_start": 11144,
                    "new_length": 35,
                    "hunk": "@@ -11144,6 +11144,35 @@ class TestNNDeviceType(NNTestCase):\n         grad_cudnn, = torch.autograd.grad(loss_cudnn, log_probs, grad_out)\n         self.assertEqual(grad_cudnn, grad_native, atol=1e-4, rtol=0)\n \n+    @onlyCUDA\n+    @skipCUDAIfRocm(msg=\"skipped Cudnn test on ROCm\")\n+    @skipCUDAIfCudnnVersionLessThan(8000)\n+    def test_ctc_loss_cudnn_tensor(self, device):\n+        batch_size = 16\n+        input_length = 30\n+        num_labels = 101\n+        target_length = 15\n+        targets = torch.randint(1, num_labels, (batch_size * target_length,),\n+                                device='cuda', dtype=torch.long)\n+        log_probs = torch.log_softmax(torch.randn(input_length, batch_size, num_labels, device='cuda', dtype=torch.float), 2)\n+        log_probs.requires_grad_()\n+\n+        input_lengths = batch_size * [input_length]\n+        input_lengths = torch.linspace(start=15, end=input_length, steps=batch_size, dtype=torch.long, device='cuda')\n+        target_lengths = torch.tensor(batch_size * [target_length], dtype=torch.long, device='cuda')\n+        grad_out = torch.randn(batch_size, device='cuda', dtype=torch.float)\n+        with torch.backends.cudnn.flags(enabled=False):\n+            loss_native = torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, reduction='none')\n+            grad_native, = torch.autograd.grad(loss_native, log_probs, grad_out)\n+        loss_cudnn = torch.nn.functional.ctc_loss(log_probs,\n+                                                  targets.to('cuda', torch.int32),\n+                                                  input_lengths.to('cuda', torch.int32),\n+                                                  target_lengths.to('cuda', torch.int32),\n+                                                  reduction='none')\n+        self.assertTrue(\"Cudnn\" in str(loss_cudnn.grad_fn))\n+        grad_cudnn, = torch.autograd.grad(loss_cudnn, log_probs, grad_out)\n+        self.assertEqual(grad_cudnn, grad_native, atol=1e-4, rtol=0)\n+\n     @dtypesIfCUDA(torch.half, torch.float, torch.double)\n     @dtypes(torch.float)\n     @tf32_on_and_off(0.005)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    @onlyCUDA\n+    @skipCUDAIfRocm(msg=\"skipped Cudnn test on ROCm\")\n+    @skipCUDAIfCudnnVersionLessThan(8000)\n+    def test_ctc_loss_cudnn_tensor(self, device):\n+        batch_size = 16\n+        input_length = 30\n+        num_labels = 101\n+        target_length = 15\n+        targets = torch.randint(1, num_labels, (batch_size * target_length,),\n+                                device='cuda', dtype=torch.long)\n+        log_probs = torch.log_softmax(torch.randn(input_length, batch_size, num_labels, device='cuda', dtype=torch.float), 2)\n+        log_probs.requires_grad_()\n+\n+        input_lengths = batch_size * [input_length]\n+        input_lengths = torch.linspace(start=15, end=input_length, steps=batch_size, dtype=torch.long, device='cuda')\n+        target_lengths = torch.tensor(batch_size * [target_length], dtype=torch.long, device='cuda')\n+        grad_out = torch.randn(batch_size, device='cuda', dtype=torch.float)\n+        with torch.backends.cudnn.flags(enabled=False):\n+            loss_native = torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, reduction='none')\n+            grad_native, = torch.autograd.grad(loss_native, log_probs, grad_out)\n+        loss_cudnn = torch.nn.functional.ctc_loss(log_probs,\n+                                                  targets.to('cuda', torch.int32),\n+                                                  input_lengths.to('cuda', torch.int32),\n+                                                  target_lengths.to('cuda', torch.int32),\n+                                                  reduction='none')\n+        self.assertTrue(\"Cudnn\" in str(loss_cudnn.grad_fn))\n+        grad_cudnn, = torch.autograd.grad(loss_cudnn, log_probs, grad_out)\n+        self.assertEqual(grad_cudnn, grad_native, atol=1e-4, rtol=0)\n+\n",
            "whole_hunk": "@@ -11144,6 +11144,35 @@ class TestNNDeviceType(NNTestCase):\n         grad_cudnn, = torch.autograd.grad(loss_cudnn, log_probs, grad_out)\n         self.assertEqual(grad_cudnn, grad_native, atol=1e-4, rtol=0)\n \n+    @onlyCUDA\n+    @skipCUDAIfRocm(msg=\"skipped Cudnn test on ROCm\")\n+    @skipCUDAIfCudnnVersionLessThan(8000)\n+    def test_ctc_loss_cudnn_tensor(self, device):\n+        batch_size = 16\n+        input_length = 30\n+        num_labels = 101\n+        target_length = 15\n+        targets = torch.randint(1, num_labels, (batch_size * target_length,),\n+                                device='cuda', dtype=torch.long)\n+        log_probs = torch.log_softmax(torch.randn(input_length, batch_size, num_labels, device='cuda', dtype=torch.float), 2)\n+        log_probs.requires_grad_()\n+\n+        input_lengths = batch_size * [input_length]\n+        input_lengths = torch.linspace(start=15, end=input_length, steps=batch_size, dtype=torch.long, device='cuda')\n+        target_lengths = torch.tensor(batch_size * [target_length], dtype=torch.long, device='cuda')\n+        grad_out = torch.randn(batch_size, device='cuda', dtype=torch.float)\n+        with torch.backends.cudnn.flags(enabled=False):\n+            loss_native = torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, reduction='none')\n+            grad_native, = torch.autograd.grad(loss_native, log_probs, grad_out)\n+        loss_cudnn = torch.nn.functional.ctc_loss(log_probs,\n+                                                  targets.to('cuda', torch.int32),\n+                                                  input_lengths.to('cuda', torch.int32),\n+                                                  target_lengths.to('cuda', torch.int32),\n+                                                  reduction='none')\n+        self.assertTrue(\"Cudnn\" in str(loss_cudnn.grad_fn))\n+        grad_cudnn, = torch.autograd.grad(loss_cudnn, log_probs, grad_out)\n+        self.assertEqual(grad_cudnn, grad_native, atol=1e-4, rtol=0)\n+\n     @dtypesIfCUDA(torch.half, torch.float, torch.double)\n     @dtypes(torch.float)\n     @tf32_on_and_off(0.005)"
        }
    ]
},
{
    "Id": 545,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/31b044570221e577c0aa27fbc7153d98c8fba30b",
    "date": "2023-08-23T05:27:57+00:00",
    "message": "Fix torch.compile with FakeTensor that has SymInt sizes (#107662)\n\n**Motivation:**\nWhen input FakeTensor to torch.compile has SymInt sizes (e.g. make_fx(opt_f, tracing_mode=\"symbolic\"):\n1. We cannot create a FakeTensor from that input in dynamo due to the SymInts.\n2. We cannot check input tensors in guard check function and will abort due to tensor check calls sizes/strides.\n\nFor 1, we specialize the FakeTensor's SymInts using their hints. This is mostly safe since inputs mostly have concrete shapes and not computed from some DynamicOutputShape ops. We'll throw a data dependent error if the symint is unbacked.\n\nFor 2, we replace size/stride calls with the sym_* variants in TENSOR_CHECK guards' check function.\n\n**Test Plan:**\nSee added tests.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/107662\nApproved by: https://github.com/ezyang",
    "label": "YES",
    "changes": [
        {
            "name": "test_subclasses.py",
            "path": "test/dynamo/test_subclasses.py",
            "patches": [
                {
                    "old_start": 13,
                    "old_length": 6,
                    "new_start": 13,
                    "new_length": 8,
                    "hunk": "@@ -13,6 +13,8 @@ from torch._dynamo.testing import normalize_gm\n from torch._functorch.aot_autograd import to_fun\n from torch._higher_order_ops.wrap import wrap\n \n+from torch.fx.experimental.symbolic_shapes import DimDynamic, ShapeEnv\n+\n \n class MockSubclass(torch.Tensor):\n     @classmethod\n"
                },
                {
                    "old_start": 120,
                    "old_length": 29,
                    "new_start": 122,
                    "new_length": 74,
                    "hunk": "@@ -120,29 +122,74 @@ class SubclassTests(torch._dynamo.test_case.TestCase):\n         res = fn(input)\n         self.assertIsInstance(res, LocalSubclass)\n \n-    def test_compile_with_fake_tensor(self):\n+    def test_compile_with_fake_tensor_dynamic_dim(self):\n         x = torch.randn([3, 4])\n-        x2 = torch.randn([4, 3])\n-        cnt = torch._dynamo.testing.CompileCounter()\n \n-        @torch.compile(backend=cnt, fullgraph=True)\n         def f(x):\n             return torch.sin(x)\n \n-        f(x)\n-        self.assertEqual(cnt.frame_count, 1)\n-        self.assertEqual(cnt.op_count, 1)\n+        def test_dynamic_dim(f, x, dim_dynamic, exp_frame_count, exp_op_count):\n+            torch._dynamo.reset()\n+            cnt = torch._dynamo.testing.CompileCounter()\n+\n+            opt_f = torch.compile(f, backend=cnt, fullgraph=True)\n+\n+            x1 = torch.rand_like(x)\n+            f(x)\n+            f(torch.randn([4, 3]))\n+            shape_env = ShapeEnv()\n+            with torch._subclasses.fake_tensor.FakeTensorMode(\n+                shape_env=shape_env\n+            ) as fake_mode:\n+                x_fake = fake_mode.from_tensor(\n+                    x, dynamic_dims=[dim_dynamic for i in range(x.dim())]\n+                )\n+                x1_fake = fake_mode.from_tensor(\n+                    x1, dynamic_dims=[dim_dynamic for i in range(x.dim())]\n+                )\n+                opt_f(x_fake)\n+                opt_f(x1_fake)\n \n-        f(x2)\n-        self.assertEqual(cnt.frame_count, 2)\n-        self.assertEqual(cnt.op_count, 2)\n+            self.assertEqual(cnt.frame_count, exp_frame_count)\n+            self.assertEqual(cnt.op_count, exp_op_count)\n \n-        with torch._subclasses.fake_tensor.FakeTensorMode() as fake_mode:\n-            fake_tensor = fake_mode.from_tensor(x)\n-            f(fake_tensor)\n+        test_dynamic_dim(f, x, DimDynamic.DYNAMIC, 1, 1)\n+        test_dynamic_dim(f, x, DimDynamic.DUCK, 1, 1)\n+        test_dynamic_dim(f, x, DimDynamic.STATIC, 1, 1)\n \n-        self.assertEqual(cnt.frame_count, 3)\n-        self.assertEqual(cnt.op_count, 3)\n+    def test_compile_with_fake_tensor_automatic_dynamic(self):\n+        def f(x):\n+            return torch.sin(x)\n+\n+        def test_automatic_dynamic(f, inps, dim_dynamic, exp_frame_count, exp_op_count):\n+            torch._dynamo.reset()\n+            cnt = torch._dynamo.testing.CompileCounter()\n+            opt_f = torch.compile(f, backend=cnt, fullgraph=True)\n+\n+            shape_env = ShapeEnv()\n+            with torch._subclasses.fake_tensor.FakeTensorMode(\n+                shape_env=shape_env\n+            ) as fake_mode:\n+                for inp in inps:\n+                    fake_inp = fake_mode.from_tensor(\n+                        inp, dynamic_dims=[dim_dynamic for i in range(x.dim())]\n+                    )\n+                    opt_f(fake_inp)\n+            self.assertEqual(cnt.frame_count, exp_frame_count)\n+            self.assertEqual(cnt.op_count, exp_op_count)\n+\n+        x = torch.randn([3, 4])\n+        y = torch.randn([4, 5])\n+        z = torch.randn([5, 6])\n+        a = torch.randn([3, 5])\n+        b = torch.randn([4, 4])\n+        for dim_dynamic in [DimDynamic.DYNAMIC, DimDynamic.DUCK, DimDynamic.STATIC]:\n+            # Recompile once, first with dim 0 and 1 become Dynamic\n+            test_automatic_dynamic(f, [x, y, z], dim_dynamic, 2, 2)\n+            # Recompile 2 times, first with dim 1 become Dynamic, second with dim 0 becomes Dynamic.\n+            test_automatic_dynamic(f, [x, a, z], dim_dynamic, 3, 3)\n+            # Recompile 2 times, first with dim 0 become Dynamic, second with dim 1 becomes Dynamic.\n+            test_automatic_dynamic(f, [x, b, z], dim_dynamic, 3, 3)\n \n     def test_compile_with_functionalization(self):\n         x = torch.randn([3, 4])\n"
                }
            ],
            "whole_deleted": "-    def test_compile_with_fake_tensor(self):\n-        x2 = torch.randn([4, 3])\n-        cnt = torch._dynamo.testing.CompileCounter()\n-        @torch.compile(backend=cnt, fullgraph=True)\n-        f(x)\n-        self.assertEqual(cnt.frame_count, 1)\n-        self.assertEqual(cnt.op_count, 1)\n-        f(x2)\n-        self.assertEqual(cnt.frame_count, 2)\n-        self.assertEqual(cnt.op_count, 2)\n-        with torch._subclasses.fake_tensor.FakeTensorMode() as fake_mode:\n-            fake_tensor = fake_mode.from_tensor(x)\n-            f(fake_tensor)\n-        self.assertEqual(cnt.frame_count, 3)\n-        self.assertEqual(cnt.op_count, 3)\n",
            "whole_added": "+from torch.fx.experimental.symbolic_shapes import DimDynamic, ShapeEnv\n+\n+    def test_compile_with_fake_tensor_dynamic_dim(self):\n+        def test_dynamic_dim(f, x, dim_dynamic, exp_frame_count, exp_op_count):\n+            torch._dynamo.reset()\n+            cnt = torch._dynamo.testing.CompileCounter()\n+\n+            opt_f = torch.compile(f, backend=cnt, fullgraph=True)\n+\n+            x1 = torch.rand_like(x)\n+            f(x)\n+            f(torch.randn([4, 3]))\n+            shape_env = ShapeEnv()\n+            with torch._subclasses.fake_tensor.FakeTensorMode(\n+                shape_env=shape_env\n+            ) as fake_mode:\n+                x_fake = fake_mode.from_tensor(\n+                    x, dynamic_dims=[dim_dynamic for i in range(x.dim())]\n+                )\n+                x1_fake = fake_mode.from_tensor(\n+                    x1, dynamic_dims=[dim_dynamic for i in range(x.dim())]\n+                )\n+                opt_f(x_fake)\n+                opt_f(x1_fake)\n+            self.assertEqual(cnt.frame_count, exp_frame_count)\n+            self.assertEqual(cnt.op_count, exp_op_count)\n+        test_dynamic_dim(f, x, DimDynamic.DYNAMIC, 1, 1)\n+        test_dynamic_dim(f, x, DimDynamic.DUCK, 1, 1)\n+        test_dynamic_dim(f, x, DimDynamic.STATIC, 1, 1)\n+    def test_compile_with_fake_tensor_automatic_dynamic(self):\n+        def f(x):\n+            return torch.sin(x)\n+\n+        def test_automatic_dynamic(f, inps, dim_dynamic, exp_frame_count, exp_op_count):\n+            torch._dynamo.reset()\n+            cnt = torch._dynamo.testing.CompileCounter()\n+            opt_f = torch.compile(f, backend=cnt, fullgraph=True)\n+\n+            shape_env = ShapeEnv()\n+            with torch._subclasses.fake_tensor.FakeTensorMode(\n+                shape_env=shape_env\n+            ) as fake_mode:\n+                for inp in inps:\n+                    fake_inp = fake_mode.from_tensor(\n+                        inp, dynamic_dims=[dim_dynamic for i in range(x.dim())]\n+                    )\n+                    opt_f(fake_inp)\n+            self.assertEqual(cnt.frame_count, exp_frame_count)\n+            self.assertEqual(cnt.op_count, exp_op_count)\n+\n+        x = torch.randn([3, 4])\n+        y = torch.randn([4, 5])\n+        z = torch.randn([5, 6])\n+        a = torch.randn([3, 5])\n+        b = torch.randn([4, 4])\n+        for dim_dynamic in [DimDynamic.DYNAMIC, DimDynamic.DUCK, DimDynamic.STATIC]:\n+            # Recompile once, first with dim 0 and 1 become Dynamic\n+            test_automatic_dynamic(f, [x, y, z], dim_dynamic, 2, 2)\n+            # Recompile 2 times, first with dim 1 become Dynamic, second with dim 0 becomes Dynamic.\n+            test_automatic_dynamic(f, [x, a, z], dim_dynamic, 3, 3)\n+            # Recompile 2 times, first with dim 0 become Dynamic, second with dim 1 becomes Dynamic.\n+            test_automatic_dynamic(f, [x, b, z], dim_dynamic, 3, 3)\n",
            "whole_hunk": "@@ -13,6 +13,8 @@ from torch._dynamo.testing import normalize_gm\n from torch._functorch.aot_autograd import to_fun\n from torch._higher_order_ops.wrap import wrap\n \n+from torch.fx.experimental.symbolic_shapes import DimDynamic, ShapeEnv\n+\n \n class MockSubclass(torch.Tensor):\n     @classmethod\n@@ -120,29 +122,74 @@ class SubclassTests(torch._dynamo.test_case.TestCase):\n         res = fn(input)\n         self.assertIsInstance(res, LocalSubclass)\n \n-    def test_compile_with_fake_tensor(self):\n+    def test_compile_with_fake_tensor_dynamic_dim(self):\n         x = torch.randn([3, 4])\n-        x2 = torch.randn([4, 3])\n-        cnt = torch._dynamo.testing.CompileCounter()\n \n-        @torch.compile(backend=cnt, fullgraph=True)\n         def f(x):\n             return torch.sin(x)\n \n-        f(x)\n-        self.assertEqual(cnt.frame_count, 1)\n-        self.assertEqual(cnt.op_count, 1)\n+        def test_dynamic_dim(f, x, dim_dynamic, exp_frame_count, exp_op_count):\n+            torch._dynamo.reset()\n+            cnt = torch._dynamo.testing.CompileCounter()\n+\n+            opt_f = torch.compile(f, backend=cnt, fullgraph=True)\n+\n+            x1 = torch.rand_like(x)\n+            f(x)\n+            f(torch.randn([4, 3]))\n+            shape_env = ShapeEnv()\n+            with torch._subclasses.fake_tensor.FakeTensorMode(\n+                shape_env=shape_env\n+            ) as fake_mode:\n+                x_fake = fake_mode.from_tensor(\n+                    x, dynamic_dims=[dim_dynamic for i in range(x.dim())]\n+                )\n+                x1_fake = fake_mode.from_tensor(\n+                    x1, dynamic_dims=[dim_dynamic for i in range(x.dim())]\n+                )\n+                opt_f(x_fake)\n+                opt_f(x1_fake)\n \n-        f(x2)\n-        self.assertEqual(cnt.frame_count, 2)\n-        self.assertEqual(cnt.op_count, 2)\n+            self.assertEqual(cnt.frame_count, exp_frame_count)\n+            self.assertEqual(cnt.op_count, exp_op_count)\n \n-        with torch._subclasses.fake_tensor.FakeTensorMode() as fake_mode:\n-            fake_tensor = fake_mode.from_tensor(x)\n-            f(fake_tensor)\n+        test_dynamic_dim(f, x, DimDynamic.DYNAMIC, 1, 1)\n+        test_dynamic_dim(f, x, DimDynamic.DUCK, 1, 1)\n+        test_dynamic_dim(f, x, DimDynamic.STATIC, 1, 1)\n \n-        self.assertEqual(cnt.frame_count, 3)\n-        self.assertEqual(cnt.op_count, 3)\n+    def test_compile_with_fake_tensor_automatic_dynamic(self):\n+        def f(x):\n+            return torch.sin(x)\n+\n+        def test_automatic_dynamic(f, inps, dim_dynamic, exp_frame_count, exp_op_count):\n+            torch._dynamo.reset()\n+            cnt = torch._dynamo.testing.CompileCounter()\n+            opt_f = torch.compile(f, backend=cnt, fullgraph=True)\n+\n+            shape_env = ShapeEnv()\n+            with torch._subclasses.fake_tensor.FakeTensorMode(\n+                shape_env=shape_env\n+            ) as fake_mode:\n+                for inp in inps:\n+                    fake_inp = fake_mode.from_tensor(\n+                        inp, dynamic_dims=[dim_dynamic for i in range(x.dim())]\n+                    )\n+                    opt_f(fake_inp)\n+            self.assertEqual(cnt.frame_count, exp_frame_count)\n+            self.assertEqual(cnt.op_count, exp_op_count)\n+\n+        x = torch.randn([3, 4])\n+        y = torch.randn([4, 5])\n+        z = torch.randn([5, 6])\n+        a = torch.randn([3, 5])\n+        b = torch.randn([4, 4])\n+        for dim_dynamic in [DimDynamic.DYNAMIC, DimDynamic.DUCK, DimDynamic.STATIC]:\n+            # Recompile once, first with dim 0 and 1 become Dynamic\n+            test_automatic_dynamic(f, [x, y, z], dim_dynamic, 2, 2)\n+            # Recompile 2 times, first with dim 1 become Dynamic, second with dim 0 becomes Dynamic.\n+            test_automatic_dynamic(f, [x, a, z], dim_dynamic, 3, 3)\n+            # Recompile 2 times, first with dim 0 become Dynamic, second with dim 1 becomes Dynamic.\n+            test_automatic_dynamic(f, [x, b, z], dim_dynamic, 3, 3)\n \n     def test_compile_with_functionalization(self):\n         x = torch.randn([3, 4])\n"
        },
        {
            "name": "guards.cpp",
            "path": "torch/csrc/dynamo/guards.cpp",
            "patches": [
                {
                    "old_start": 54,
                    "old_length": 8,
                    "new_start": 54,
                    "new_length": 8,
                    "hunk": "@@ -54,8 +54,8 @@ class TensorCheck {\n     if (ndim != dim_) {\n       return false;\n     }\n-    const auto& sizes = v.sizes();\n-    const auto& strides = v.strides();\n+    const auto& sizes = v.sym_sizes();\n+    const auto& strides = v.sym_strides();\n     for (auto i : c10::irange(ndim)) {\n       auto known_size = sizes_[i];\n       auto known_stride = strides_[i];\n"
                },
                {
                    "old_start": 113,
                    "old_length": 8,
                    "new_start": 113,
                    "new_length": 8,
                    "hunk": "@@ -113,8 +113,8 @@ class TensorCheck {\n                   << ndim;\n       return fail_reason.str();\n     }\n-    const auto& sizes = v.sizes();\n-    const auto& strides = v.strides();\n+    const auto& sizes = v.sym_sizes();\n+    const auto& strides = v.sym_strides();\n     for (auto i : c10::irange(ndim)) {\n       auto known_size = sizes_[i];\n       auto known_stride = strides_[i];\n"
                }
            ],
            "whole_deleted": "-    const auto& sizes = v.sizes();\n-    const auto& strides = v.strides();\n-    const auto& sizes = v.sizes();\n-    const auto& strides = v.strides();\n",
            "whole_added": "+    const auto& sizes = v.sym_sizes();\n+    const auto& strides = v.sym_strides();\n+    const auto& sizes = v.sym_sizes();\n+    const auto& strides = v.sym_strides();\n",
            "whole_hunk": "@@ -54,8 +54,8 @@ class TensorCheck {\n     if (ndim != dim_) {\n       return false;\n     }\n-    const auto& sizes = v.sizes();\n-    const auto& strides = v.strides();\n+    const auto& sizes = v.sym_sizes();\n+    const auto& strides = v.sym_strides();\n     for (auto i : c10::irange(ndim)) {\n       auto known_size = sizes_[i];\n       auto known_stride = strides_[i];\n@@ -113,8 +113,8 @@ class TensorCheck {\n                   << ndim;\n       return fail_reason.str();\n     }\n-    const auto& sizes = v.sizes();\n-    const auto& strides = v.strides();\n+    const auto& sizes = v.sym_sizes();\n+    const auto& strides = v.sym_strides();\n     for (auto i : c10::irange(ndim)) {\n       auto known_size = sizes_[i];\n       auto known_stride = strides_[i];\n"
        },
        {
            "name": "symbolic_shapes.py",
            "path": "torch/fx/experimental/symbolic_shapes.py",
            "patches": [
                {
                    "old_start": 2209,
                    "old_length": 11,
                    "new_start": 2209,
                    "new_length": 19,
                    "hunk": "@@ -2209,11 +2209,19 @@ class ShapeEnv:\n                            ex: torch.Tensor,\n                            source: Source,\n                            dynamic_dims: DimList[DimDynamic],\n-                           constraint_dims: DimList[DimConstraint],\n-                           ) -> List[sympy.Expr]:\n+                           constraint_dims: DimList[DimConstraint]) -> List[sympy.Expr]:\n+        return self._produce_dyn_sizes_from_int_tuple(tuple(ex.size()), source, dynamic_dims, constraint_dims)\n+\n+    def _produce_dyn_sizes_from_int_tuple(self,\n+                                          tensor_size: Tuple[int],\n+                                          source: Source,\n+                                          dynamic_dims: DimList[DimDynamic],\n+                                          constraint_dims: List[DimConstraint]\n+                                          ) -> List[sympy.Expr]:\n+        assert all(isinstance(val, int) for val in tensor_size), f\"Expect size to be a plain tuple of ints but got {tensor_size}\"\n         from torch._dynamo.source import TensorPropertySource, TensorProperty\n         size = []\n-        for i, val in enumerate(ex.size()):\n+        for i, val in enumerate(tensor_size):\n             size.append(self.create_symbol(\n                 val, TensorPropertySource(source, TensorProperty.SIZE, i), dynamic_dims[i], constraint_dims[i]\n             ))\n"
                },
                {
                    "old_start": 2232,
                    "old_length": 6,
                    "new_start": 2240,
                    "new_length": 53,
                    "hunk": "@@ -2232,6 +2240,53 @@ class ShapeEnv:\n         We try our best to express stride in terms of the sizes, so as to not\n         introduce new symbolic variables.\n         \"\"\"\n+\n+\n+        # Dynamo may want to wrap FakeTensors with SymInt sizes up e.g. make_fx(opt_f(), tracing_mode=\"symbolic\").\n+        # We create symbols in shape_env using the backed hints behind SymInt.\n+\n+        # Case 1: when SymInt is backed, dynamo can proceed with FakeTensors that have concrete shape.\n+        # produce_guards will trigger specializations on the outer stuff\n+\n+        # Case 2: when the SymInt is unbacked, we will throw an data dependent error in require_hint().\n+        #\n+        # It's probably good for now but it's important to note that this approach has implications for\n+        # the original shape_env when checking guards in different order.\n+\n+        # Example:\n+        # ---------\n+        # Consider a function \"opt_f\" as shown below:\n+\n+        # @torch.compile()\n+        # def opt_f(x: bool, y: Tensor):\n+        #   if x == True:\n+        #     return y + torch.randn([4])\n+        #   else:\n+        #     return y\n+        # Depending on the sequence of calls, we might install two different sets of guards:\n+\n+        # 1. opt_f(False, y):\n+        #    - \"x == False\" (always works for any size y)\n+\n+        # 2. opt_f(True, y):\n+        #    - Triggers recompilation and results in guards like:\n+        #      - \"x == True and y.size(0) == 4\"\n+        #      - (or \"y.size(0) == 4 and x == True\")\n+\n+        # The order of checking the guards matters. In this specific example:\n+        # If True branch guard check precedes False branch and for True branch, y.size(0) check precedes x == True,\n+        # we may have an unnessary shape speciliazation for y.\n+        def maybe_specialize_sym_int_with_hint(maybe_sym) -> int:\n+            assert isinstance(maybe_sym, (int, torch.SymInt))\n+            if isinstance(maybe_sym, SymInt):\n+                assert maybe_sym.node.shape_env is not self, \\\n+                    \"expect the symbol is created from an shape env other than current one.\"\n+                return maybe_sym.node.require_hint()\n+            return maybe_sym\n+\n+        ex_size = tuple(maybe_specialize_sym_int_with_hint(sz) for sz in ex.size())\n+        ex_stride = tuple(maybe_specialize_sym_int_with_hint(sd) for sd in ex.stride())\n+        ex_storage_offset = maybe_specialize_sym_int_with_hint(ex.storage_offset())\n         dim = ex.dim()\n \n         # Reimplement the legacy behavior\n"
                },
                {
                    "old_start": 2263,
                    "old_length": 31,
                    "new_start": 2318,
                    "new_length": 31,
                    "hunk": "@@ -2263,31 +2318,31 @@ class ShapeEnv:\n         assert len(constraint_dims) == dim\n \n         from torch._dynamo.source import TensorPropertySource, TensorProperty\n-        size: List[sympy.Expr] = self._produce_dyn_sizes(ex, source, dynamic_dims, constraint_dims)\n+        size: List[sympy.Expr] = self._produce_dyn_sizes_from_int_tuple(ex_size, source, dynamic_dims, constraint_dims)\n         stride: List[Optional[sympy.Expr]] = [None] * len(size)\n-        for i, val in enumerate(ex.stride()):\n+        for i, val in enumerate(ex_stride):\n             if val in (0, 1):\n                 stride[i] = sympy.Integer(val)\n         while any(x is None for x in stride):\n             candidates = {\n-                ex.size(i) * ex.stride()[i]: size[i] * stride[i]\n+                ex_size[i] * ex_stride[i]: size[i] * stride[i]\n                 for i in range(len(size))\n-                if stride[i] is not None and ex.stride()[i] >= 0\n+                if stride[i] is not None and ex_stride[i] >= 0\n             }\n             # iterate over unbound strides in sorted order\n             val_list = sorted(\n-                [(ex.stride()[i], i) for i in range(len(stride)) if stride[i] is None]\n+                [(ex_stride[i], i) for i in range(len(stride)) if stride[i] is None]\n             )\n             for _, i in val_list:\n-                if stride[i] is None and ex.stride()[i] in candidates:\n-                    stride[i] = candidates[ex.stride()[i]]\n-                    candidates[ex.size(i) * ex.stride()[i]] = size[i] * stride[i]\n+                if stride[i] is None and ex_stride[i] in candidates:\n+                    stride[i] = candidates[ex_stride[i]]\n+                    candidates[ex_size[i] * ex_stride[i]] = size[i] * stride[i]\n \n             if any(x is None for x in stride):\n                 # bind the smallest unbound stride to a new variable\n                 val, i = min(\n                     [\n-                        (ex.stride()[i], i)\n+                        (ex_stride[i], i)\n                         for i in range(len(stride))\n                         if stride[i] is None\n                     ]\n"
                },
                {
                    "old_start": 2302,
                    "old_length": 7,
                    "new_start": 2357,
                    "new_length": 7,
                    "hunk": "@@ -2302,7 +2357,7 @@ class ShapeEnv:\n \n         sym_sizes = [\n             self.create_symintnode(sym, hint=hint, source=TensorPropertySource(source, TensorProperty.SIZE, i))\n-            for i, (sym, hint) in enumerate(zip(size, ex.size()))\n+            for i, (sym, hint) in enumerate(zip(size, ex_size))\n         ]\n         sym_stride = []\n         for i, stride_expr in enumerate(stride):\n"
                },
                {
                    "old_start": 2310,
                    "old_length": 14,
                    "new_start": 2365,
                    "new_length": 14,
                    "hunk": "@@ -2310,14 +2365,14 @@ class ShapeEnv:\n             # we computed\n             assert stride_expr is not None\n             sym_stride.append(self.create_symintnode(\n-                stride_expr, hint=ex.stride(i), source=TensorPropertySource(source, TensorProperty.STRIDE, i)\n+                stride_expr, hint=ex_stride[i], source=TensorPropertySource(source, TensorProperty.STRIDE, i)\n             ))\n         sym_storage_offset = self.create_symintnode(self.create_symbol(\n-            ex.storage_offset(),\n+            ex_storage_offset,\n             TensorPropertySource(source, TensorProperty.STORAGE_OFFSET),\n             dynamic_dim=dynamic_strides_offset,\n             constraint_dim=None,\n-        ), hint=ex.storage_offset(), source=TensorPropertySource(source, TensorProperty.STORAGE_OFFSET))\n+        ), hint=ex_storage_offset, source=TensorPropertySource(source, TensorProperty.STORAGE_OFFSET))\n         return sym_sizes, sym_stride, sym_storage_offset\n \n     # If you know what the current hint value of the SymInt to be created"
                }
            ],
            "whole_deleted": "-                           constraint_dims: DimList[DimConstraint],\n-                           ) -> List[sympy.Expr]:\n-        for i, val in enumerate(ex.size()):\n-        size: List[sympy.Expr] = self._produce_dyn_sizes(ex, source, dynamic_dims, constraint_dims)\n-        for i, val in enumerate(ex.stride()):\n-                ex.size(i) * ex.stride()[i]: size[i] * stride[i]\n-                if stride[i] is not None and ex.stride()[i] >= 0\n-                [(ex.stride()[i], i) for i in range(len(stride)) if stride[i] is None]\n-                if stride[i] is None and ex.stride()[i] in candidates:\n-                    stride[i] = candidates[ex.stride()[i]]\n-                    candidates[ex.size(i) * ex.stride()[i]] = size[i] * stride[i]\n-                        (ex.stride()[i], i)\n-            for i, (sym, hint) in enumerate(zip(size, ex.size()))\n-                stride_expr, hint=ex.stride(i), source=TensorPropertySource(source, TensorProperty.STRIDE, i)\n-            ex.storage_offset(),\n-        ), hint=ex.storage_offset(), source=TensorPropertySource(source, TensorProperty.STORAGE_OFFSET))\n",
            "whole_added": "+                           constraint_dims: DimList[DimConstraint]) -> List[sympy.Expr]:\n+        return self._produce_dyn_sizes_from_int_tuple(tuple(ex.size()), source, dynamic_dims, constraint_dims)\n+\n+    def _produce_dyn_sizes_from_int_tuple(self,\n+                                          tensor_size: Tuple[int],\n+                                          source: Source,\n+                                          dynamic_dims: DimList[DimDynamic],\n+                                          constraint_dims: List[DimConstraint]\n+                                          ) -> List[sympy.Expr]:\n+        assert all(isinstance(val, int) for val in tensor_size), f\"Expect size to be a plain tuple of ints but got {tensor_size}\"\n+        for i, val in enumerate(tensor_size):\n+\n+\n+        # Dynamo may want to wrap FakeTensors with SymInt sizes up e.g. make_fx(opt_f(), tracing_mode=\"symbolic\").\n+        # We create symbols in shape_env using the backed hints behind SymInt.\n+\n+        # Case 1: when SymInt is backed, dynamo can proceed with FakeTensors that have concrete shape.\n+        # produce_guards will trigger specializations on the outer stuff\n+\n+        # Case 2: when the SymInt is unbacked, we will throw an data dependent error in require_hint().\n+        #\n+        # It's probably good for now but it's important to note that this approach has implications for\n+        # the original shape_env when checking guards in different order.\n+\n+        # Example:\n+        # ---------\n+        # Consider a function \"opt_f\" as shown below:\n+\n+        # @torch.compile()\n+        # def opt_f(x: bool, y: Tensor):\n+        #   if x == True:\n+        #     return y + torch.randn([4])\n+        #   else:\n+        #     return y\n+        # Depending on the sequence of calls, we might install two different sets of guards:\n+\n+        # 1. opt_f(False, y):\n+        #    - \"x == False\" (always works for any size y)\n+\n+        # 2. opt_f(True, y):\n+        #    - Triggers recompilation and results in guards like:\n+        #      - \"x == True and y.size(0) == 4\"\n+        #      - (or \"y.size(0) == 4 and x == True\")\n+\n+        # The order of checking the guards matters. In this specific example:\n+        # If True branch guard check precedes False branch and for True branch, y.size(0) check precedes x == True,\n+        # we may have an unnessary shape speciliazation for y.\n+        def maybe_specialize_sym_int_with_hint(maybe_sym) -> int:\n+            assert isinstance(maybe_sym, (int, torch.SymInt))\n+            if isinstance(maybe_sym, SymInt):\n+                assert maybe_sym.node.shape_env is not self, \\\n+                    \"expect the symbol is created from an shape env other than current one.\"\n+                return maybe_sym.node.require_hint()\n+            return maybe_sym\n+\n+        ex_size = tuple(maybe_specialize_sym_int_with_hint(sz) for sz in ex.size())\n+        ex_stride = tuple(maybe_specialize_sym_int_with_hint(sd) for sd in ex.stride())\n+        ex_storage_offset = maybe_specialize_sym_int_with_hint(ex.storage_offset())\n+        size: List[sympy.Expr] = self._produce_dyn_sizes_from_int_tuple(ex_size, source, dynamic_dims, constraint_dims)\n+        for i, val in enumerate(ex_stride):\n+                ex_size[i] * ex_stride[i]: size[i] * stride[i]\n+                if stride[i] is not None and ex_stride[i] >= 0\n+                [(ex_stride[i], i) for i in range(len(stride)) if stride[i] is None]\n+                if stride[i] is None and ex_stride[i] in candidates:\n+                    stride[i] = candidates[ex_stride[i]]\n+                    candidates[ex_size[i] * ex_stride[i]] = size[i] * stride[i]\n+                        (ex_stride[i], i)\n+            for i, (sym, hint) in enumerate(zip(size, ex_size))\n+                stride_expr, hint=ex_stride[i], source=TensorPropertySource(source, TensorProperty.STRIDE, i)\n+            ex_storage_offset,\n+        ), hint=ex_storage_offset, source=TensorPropertySource(source, TensorProperty.STORAGE_OFFSET))\n",
            "whole_hunk": "@@ -2209,11 +2209,19 @@ class ShapeEnv:\n                            ex: torch.Tensor,\n                            source: Source,\n                            dynamic_dims: DimList[DimDynamic],\n-                           constraint_dims: DimList[DimConstraint],\n-                           ) -> List[sympy.Expr]:\n+                           constraint_dims: DimList[DimConstraint]) -> List[sympy.Expr]:\n+        return self._produce_dyn_sizes_from_int_tuple(tuple(ex.size()), source, dynamic_dims, constraint_dims)\n+\n+    def _produce_dyn_sizes_from_int_tuple(self,\n+                                          tensor_size: Tuple[int],\n+                                          source: Source,\n+                                          dynamic_dims: DimList[DimDynamic],\n+                                          constraint_dims: List[DimConstraint]\n+                                          ) -> List[sympy.Expr]:\n+        assert all(isinstance(val, int) for val in tensor_size), f\"Expect size to be a plain tuple of ints but got {tensor_size}\"\n         from torch._dynamo.source import TensorPropertySource, TensorProperty\n         size = []\n-        for i, val in enumerate(ex.size()):\n+        for i, val in enumerate(tensor_size):\n             size.append(self.create_symbol(\n                 val, TensorPropertySource(source, TensorProperty.SIZE, i), dynamic_dims[i], constraint_dims[i]\n             ))\n@@ -2232,6 +2240,53 @@ class ShapeEnv:\n         We try our best to express stride in terms of the sizes, so as to not\n         introduce new symbolic variables.\n         \"\"\"\n+\n+\n+        # Dynamo may want to wrap FakeTensors with SymInt sizes up e.g. make_fx(opt_f(), tracing_mode=\"symbolic\").\n+        # We create symbols in shape_env using the backed hints behind SymInt.\n+\n+        # Case 1: when SymInt is backed, dynamo can proceed with FakeTensors that have concrete shape.\n+        # produce_guards will trigger specializations on the outer stuff\n+\n+        # Case 2: when the SymInt is unbacked, we will throw an data dependent error in require_hint().\n+        #\n+        # It's probably good for now but it's important to note that this approach has implications for\n+        # the original shape_env when checking guards in different order.\n+\n+        # Example:\n+        # ---------\n+        # Consider a function \"opt_f\" as shown below:\n+\n+        # @torch.compile()\n+        # def opt_f(x: bool, y: Tensor):\n+        #   if x == True:\n+        #     return y + torch.randn([4])\n+        #   else:\n+        #     return y\n+        # Depending on the sequence of calls, we might install two different sets of guards:\n+\n+        # 1. opt_f(False, y):\n+        #    - \"x == False\" (always works for any size y)\n+\n+        # 2. opt_f(True, y):\n+        #    - Triggers recompilation and results in guards like:\n+        #      - \"x == True and y.size(0) == 4\"\n+        #      - (or \"y.size(0) == 4 and x == True\")\n+\n+        # The order of checking the guards matters. In this specific example:\n+        # If True branch guard check precedes False branch and for True branch, y.size(0) check precedes x == True,\n+        # we may have an unnessary shape speciliazation for y.\n+        def maybe_specialize_sym_int_with_hint(maybe_sym) -> int:\n+            assert isinstance(maybe_sym, (int, torch.SymInt))\n+            if isinstance(maybe_sym, SymInt):\n+                assert maybe_sym.node.shape_env is not self, \\\n+                    \"expect the symbol is created from an shape env other than current one.\"\n+                return maybe_sym.node.require_hint()\n+            return maybe_sym\n+\n+        ex_size = tuple(maybe_specialize_sym_int_with_hint(sz) for sz in ex.size())\n+        ex_stride = tuple(maybe_specialize_sym_int_with_hint(sd) for sd in ex.stride())\n+        ex_storage_offset = maybe_specialize_sym_int_with_hint(ex.storage_offset())\n         dim = ex.dim()\n \n         # Reimplement the legacy behavior\n@@ -2263,31 +2318,31 @@ class ShapeEnv:\n         assert len(constraint_dims) == dim\n \n         from torch._dynamo.source import TensorPropertySource, TensorProperty\n-        size: List[sympy.Expr] = self._produce_dyn_sizes(ex, source, dynamic_dims, constraint_dims)\n+        size: List[sympy.Expr] = self._produce_dyn_sizes_from_int_tuple(ex_size, source, dynamic_dims, constraint_dims)\n         stride: List[Optional[sympy.Expr]] = [None] * len(size)\n-        for i, val in enumerate(ex.stride()):\n+        for i, val in enumerate(ex_stride):\n             if val in (0, 1):\n                 stride[i] = sympy.Integer(val)\n         while any(x is None for x in stride):\n             candidates = {\n-                ex.size(i) * ex.stride()[i]: size[i] * stride[i]\n+                ex_size[i] * ex_stride[i]: size[i] * stride[i]\n                 for i in range(len(size))\n-                if stride[i] is not None and ex.stride()[i] >= 0\n+                if stride[i] is not None and ex_stride[i] >= 0\n             }\n             # iterate over unbound strides in sorted order\n             val_list = sorted(\n-                [(ex.stride()[i], i) for i in range(len(stride)) if stride[i] is None]\n+                [(ex_stride[i], i) for i in range(len(stride)) if stride[i] is None]\n             )\n             for _, i in val_list:\n-                if stride[i] is None and ex.stride()[i] in candidates:\n-                    stride[i] = candidates[ex.stride()[i]]\n-                    candidates[ex.size(i) * ex.stride()[i]] = size[i] * stride[i]\n+                if stride[i] is None and ex_stride[i] in candidates:\n+                    stride[i] = candidates[ex_stride[i]]\n+                    candidates[ex_size[i] * ex_stride[i]] = size[i] * stride[i]\n \n             if any(x is None for x in stride):\n                 # bind the smallest unbound stride to a new variable\n                 val, i = min(\n                     [\n-                        (ex.stride()[i], i)\n+                        (ex_stride[i], i)\n                         for i in range(len(stride))\n                         if stride[i] is None\n                     ]\n@@ -2302,7 +2357,7 @@ class ShapeEnv:\n \n         sym_sizes = [\n             self.create_symintnode(sym, hint=hint, source=TensorPropertySource(source, TensorProperty.SIZE, i))\n-            for i, (sym, hint) in enumerate(zip(size, ex.size()))\n+            for i, (sym, hint) in enumerate(zip(size, ex_size))\n         ]\n         sym_stride = []\n         for i, stride_expr in enumerate(stride):\n@@ -2310,14 +2365,14 @@ class ShapeEnv:\n             # we computed\n             assert stride_expr is not None\n             sym_stride.append(self.create_symintnode(\n-                stride_expr, hint=ex.stride(i), source=TensorPropertySource(source, TensorProperty.STRIDE, i)\n+                stride_expr, hint=ex_stride[i], source=TensorPropertySource(source, TensorProperty.STRIDE, i)\n             ))\n         sym_storage_offset = self.create_symintnode(self.create_symbol(\n-            ex.storage_offset(),\n+            ex_storage_offset,\n             TensorPropertySource(source, TensorProperty.STORAGE_OFFSET),\n             dynamic_dim=dynamic_strides_offset,\n             constraint_dim=None,\n-        ), hint=ex.storage_offset(), source=TensorPropertySource(source, TensorProperty.STORAGE_OFFSET))\n+        ), hint=ex_storage_offset, source=TensorPropertySource(source, TensorProperty.STORAGE_OFFSET))\n         return sym_sizes, sym_stride, sym_storage_offset\n \n     # If you know what the current hint value of the SymInt to be created"
        }
    ]
},
{
    "Id": 63,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/cf392d8a89a86f563b5223a889ef7f7027bd182e",
    "date": "2024-06-27T06:01:47+00:00",
    "message": "[pytorch][cuda] Generate kernels for 5x5 filters on depth wise convolution backward (#129609)\n\nIn #125362 we improved the default implementation of depth wise convolution 2D forward pass by precomputing boundaries of accessed slices instead of doing expensive edge checks in the inner loops. We also generated kernels for 5x5 filters as this is a common problem size.\n\nIn this PR we tried to applied the same strategy for the backward kernel but we only saw good gains just by generating code for 5x5 filters. We could also write a fallback implementation that precomputes access boundaries when filter size and stride are not known at compile time may bring some speedup but that kernel would very rarely be called.\n\nThis PR also hints the thread count at compile time and leaves only the unroll directive that seems to help performance.\n\nBefore:\n\n```\n         B      C      iH      iW    kH    kW  conv2d-backward (cuda)  conv2d-fp16-backward (cuda)\n0      8.0   64.0  1024.0  1008.0   5.0   5.0               89.002686                    26.400480\n1      8.0   64.0  1008.0  1008.0   5.0   5.0               88.885025                    25.995296\n2      4.0   48.0   720.0   539.0   6.0   1.0                9.488832                     9.091136\n3      4.0  120.0   379.0   283.0   6.0   1.0                4.194640                     3.844432\n4      4.0   32.0   713.0   532.0   6.0   1.0                8.027296                     7.700064\n5      4.0    3.0   712.0   542.0  31.0  31.0               15.618095                    15.097760\n6      4.0  120.0   379.0   288.0   1.0   6.0                3.788224                     3.499648\n7   1024.0  384.0     1.0   928.0   1.0   3.0               18.988289                    14.152768\n8      4.0   24.0   687.0   512.0   6.0   1.0                6.902704                     6.685056\n9     96.0   96.0   112.0   112.0   5.0   5.0               15.672400                     4.953984\n10    96.0   80.0    56.0    56.0   5.0   5.0                3.261152                     1.250320\n11    64.0  128.0    64.0    84.0   3.0   3.0                3.172192                     1.515648\n12    16.0  960.0     7.0     7.0   5.0   5.0                0.197024                     0.072736\n13    16.0   64.0   112.0   112.0   3.0   3.0                1.126240                     0.650304\n```\n\nAfter\n```\nconv2d-performance:\n         B      C      iH      iW    kH    kW  conv2d-backward (cuda)  conv2d-fp16-backward (cuda)\n0      8.0   64.0  1024.0  1008.0   5.0   5.0               76.278656                    26.418720\n1      8.0   64.0  1008.0  1008.0   5.0   5.0               73.211617                    26.018433\n2      4.0   48.0   720.0   539.0   6.0   1.0                8.901312                     9.322912\n3      4.0  120.0   379.0   283.0   6.0   1.0                3.815616                     3.992208\n4      4.0   32.0   713.0   532.0   6.0   1.0                7.753024                     8.032433\n5      4.0    3.0   712.0   542.0  31.0  31.0               15.244144                    15.277296\n6      4.0  120.0   379.0   288.0   1.0   6.0                3.503264                     3.552976\n7   1024.0  384.0     1.0   928.0   1.0   3.0               16.682976                    14.167969\n8      4.0   24.0   687.0   512.0   6.0   1.0                6.802576                     7.019040\n9     96.0   96.0   112.0   112.0   5.0   5.0               12.713024                     4.958656\n10    96.0   80.0    56.0    56.0   5.0   5.0                2.648352                     1.254752\n11    64.0  128.0    64.0    84.0   3.0   3.0                3.213568                     1.517952\n12    16.0  960.0     7.0     7.0   5.0   5.0                0.182208                     0.076256\n13    16.0   64.0   112.0   112.0   3.0   3.0                1.139952                     0.652432\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/129609\nApproved by: https://github.com/ezyang, https://github.com/eqy",
    "label": "NO",
    "changes": [
        {
            "name": "DepthwiseConv2d.cu",
            "path": "aten/src/ATen/native/cuda/DepthwiseConv2d.cu",
            "patches": [
                {
                    "old_start": 213,
                    "old_length": 6,
                    "new_start": 213,
                    "new_length": 9,
                    "hunk": "@@ -213,6 +213,9 @@ conv_depthwise2d_forward_kernel(\n }\n \n template <int kSize, int stride, typename scalar_t, typename index_t>\n+#if !defined(USE_ROCM)\n+C10_LAUNCH_BOUNDS_1(at::cuda::detail::CUDA_NUM_THREADS)\n+#endif\n __global__ void conv_depthwise2d_backward_kernel(\n     const PackedTensorAccessor32<const scalar_t, 4, DefaultPtrTraits> grad_output,\n     PackedTensorAccessor32<scalar_t, 4, DefaultPtrTraits> grad_input,\n"
                },
                {
                    "old_start": 245,
                    "old_length": 17,
                    "new_start": 248,
                    "new_length": 11,
                    "hunk": "@@ -245,17 +248,11 @@ __global__ void conv_depthwise2d_backward_kernel(\n \n     acc_t value(0);\n \n-#if !defined(USE_ROCM)\n-#pragma unroll\n-#endif\n     for (int multiplier = 0; multiplier < depthwiseMultiplier; ++multiplier) {\n       int och = (c * depthwiseMultiplier) + multiplier;\n       int weightOffset = och * kernelHeight * kernelWidth;\n-#if !defined(USE_ROCM)\n-#pragma unroll\n-#endif\n       for (int kh = 0; kh < KH_LIMIT; ++kh) {\n-#if defined(USE_ROCM)\n+#if !defined(USE_ROCM)\n #pragma unroll\n #endif\n         for (int kw = 0; kw < KW_LIMIT; ++kw) {\n"
                },
                {
                    "old_start": 508,
                    "old_length": 7,
                    "new_start": 504,
                    "new_length": 24,
                    "hunk": "@@ -508,7 +504,24 @@ void conv_depthwise2d_backward_out(\n     auto grad_input_a = grad_input.packed_accessor32<scalar_t, 4>();\n     auto weight_a = weight.packed_accessor32<const scalar_t, 4>();\n \n-    if (kW == 3 && kH == 3) {\n+    if (kW == 5 && kH == 5) {\n+      if (dW == 1 && dH == 1){\n+        conv_depthwise2d_backward_kernel<5, 1><<<grid, block, 0, stream>>>(\n+            grad_output_a, grad_input_a, weight_a, n, inputChannels, depthwiseMultiplier, outputChannels, width,\n+            height, outputWidth, outputHeight, kW, kH, dW, dH, padW, padH, dilationW, dilationH);\n+        C10_CUDA_KERNEL_LAUNCH_CHECK();\n+      } else if (dW == 2 && dH == 2) {\n+        conv_depthwise2d_backward_kernel<5, 2><<<grid, block, 0, stream>>>(\n+            grad_output_a, grad_input_a, weight_a, n, inputChannels, depthwiseMultiplier, outputChannels, width,\n+            height, outputWidth, outputHeight, kW, kH, dW, dH, padW, padH, dilationW, dilationH);\n+        C10_CUDA_KERNEL_LAUNCH_CHECK();\n+      } else {\n+        conv_depthwise2d_backward_kernel<5, 0><<<grid, block, 0, stream>>>(\n+            grad_output_a, grad_input_a, weight_a, n, inputChannels, depthwiseMultiplier, outputChannels, width,\n+            height, outputWidth, outputHeight, kW, kH, dW, dH, padW, padH, dilationW, dilationH);\n+        C10_CUDA_KERNEL_LAUNCH_CHECK();\n+      }\n+    } else if (kW == 3 && kH == 3) {\n       if (dW == 1 && dH == 1){\n         conv_depthwise2d_backward_kernel<3, 1><<<grid, block, 0, stream>>>(\n             grad_output_a, grad_input_a, weight_a, n, inputChannels, depthwiseMultiplier, outputChannels, width,"
                }
            ],
            "whole_deleted": "-#if !defined(USE_ROCM)\n-#pragma unroll\n-#endif\n-#if !defined(USE_ROCM)\n-#pragma unroll\n-#endif\n-#if defined(USE_ROCM)\n-    if (kW == 3 && kH == 3) {\n",
            "whole_added": "+#if !defined(USE_ROCM)\n+C10_LAUNCH_BOUNDS_1(at::cuda::detail::CUDA_NUM_THREADS)\n+#endif\n+#if !defined(USE_ROCM)\n+    if (kW == 5 && kH == 5) {\n+      if (dW == 1 && dH == 1){\n+        conv_depthwise2d_backward_kernel<5, 1><<<grid, block, 0, stream>>>(\n+            grad_output_a, grad_input_a, weight_a, n, inputChannels, depthwiseMultiplier, outputChannels, width,\n+            height, outputWidth, outputHeight, kW, kH, dW, dH, padW, padH, dilationW, dilationH);\n+        C10_CUDA_KERNEL_LAUNCH_CHECK();\n+      } else if (dW == 2 && dH == 2) {\n+        conv_depthwise2d_backward_kernel<5, 2><<<grid, block, 0, stream>>>(\n+            grad_output_a, grad_input_a, weight_a, n, inputChannels, depthwiseMultiplier, outputChannels, width,\n+            height, outputWidth, outputHeight, kW, kH, dW, dH, padW, padH, dilationW, dilationH);\n+        C10_CUDA_KERNEL_LAUNCH_CHECK();\n+      } else {\n+        conv_depthwise2d_backward_kernel<5, 0><<<grid, block, 0, stream>>>(\n+            grad_output_a, grad_input_a, weight_a, n, inputChannels, depthwiseMultiplier, outputChannels, width,\n+            height, outputWidth, outputHeight, kW, kH, dW, dH, padW, padH, dilationW, dilationH);\n+        C10_CUDA_KERNEL_LAUNCH_CHECK();\n+      }\n+    } else if (kW == 3 && kH == 3) {\n",
            "whole_hunk": "@@ -213,6 +213,9 @@ conv_depthwise2d_forward_kernel(\n }\n \n template <int kSize, int stride, typename scalar_t, typename index_t>\n+#if !defined(USE_ROCM)\n+C10_LAUNCH_BOUNDS_1(at::cuda::detail::CUDA_NUM_THREADS)\n+#endif\n __global__ void conv_depthwise2d_backward_kernel(\n     const PackedTensorAccessor32<const scalar_t, 4, DefaultPtrTraits> grad_output,\n     PackedTensorAccessor32<scalar_t, 4, DefaultPtrTraits> grad_input,\n@@ -245,17 +248,11 @@ __global__ void conv_depthwise2d_backward_kernel(\n \n     acc_t value(0);\n \n-#if !defined(USE_ROCM)\n-#pragma unroll\n-#endif\n     for (int multiplier = 0; multiplier < depthwiseMultiplier; ++multiplier) {\n       int och = (c * depthwiseMultiplier) + multiplier;\n       int weightOffset = och * kernelHeight * kernelWidth;\n-#if !defined(USE_ROCM)\n-#pragma unroll\n-#endif\n       for (int kh = 0; kh < KH_LIMIT; ++kh) {\n-#if defined(USE_ROCM)\n+#if !defined(USE_ROCM)\n #pragma unroll\n #endif\n         for (int kw = 0; kw < KW_LIMIT; ++kw) {\n@@ -508,7 +504,24 @@ void conv_depthwise2d_backward_out(\n     auto grad_input_a = grad_input.packed_accessor32<scalar_t, 4>();\n     auto weight_a = weight.packed_accessor32<const scalar_t, 4>();\n \n-    if (kW == 3 && kH == 3) {\n+    if (kW == 5 && kH == 5) {\n+      if (dW == 1 && dH == 1){\n+        conv_depthwise2d_backward_kernel<5, 1><<<grid, block, 0, stream>>>(\n+            grad_output_a, grad_input_a, weight_a, n, inputChannels, depthwiseMultiplier, outputChannels, width,\n+            height, outputWidth, outputHeight, kW, kH, dW, dH, padW, padH, dilationW, dilationH);\n+        C10_CUDA_KERNEL_LAUNCH_CHECK();\n+      } else if (dW == 2 && dH == 2) {\n+        conv_depthwise2d_backward_kernel<5, 2><<<grid, block, 0, stream>>>(\n+            grad_output_a, grad_input_a, weight_a, n, inputChannels, depthwiseMultiplier, outputChannels, width,\n+            height, outputWidth, outputHeight, kW, kH, dW, dH, padW, padH, dilationW, dilationH);\n+        C10_CUDA_KERNEL_LAUNCH_CHECK();\n+      } else {\n+        conv_depthwise2d_backward_kernel<5, 0><<<grid, block, 0, stream>>>(\n+            grad_output_a, grad_input_a, weight_a, n, inputChannels, depthwiseMultiplier, outputChannels, width,\n+            height, outputWidth, outputHeight, kW, kH, dW, dH, padW, padH, dilationW, dilationH);\n+        C10_CUDA_KERNEL_LAUNCH_CHECK();\n+      }\n+    } else if (kW == 3 && kH == 3) {\n       if (dW == 1 && dH == 1){\n         conv_depthwise2d_backward_kernel<3, 1><<<grid, block, 0, stream>>>(\n             grad_output_a, grad_input_a, weight_a, n, inputChannels, depthwiseMultiplier, outputChannels, width,"
        }
    ]
},
{
    "Id": 231,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/57a3d00b0659e4ac37c4a35a36c71f710e89197a",
    "date": "2024-03-25T22:05:20+00:00",
    "message": "[AOTInductor] Add tensor_constantX to pass constant buffer update's check (#122562)\n\nSummary:\nDuring tracing, some constants (tensor_constant{idx}) are being generated internally.\nThose constants are neither parameters or buffers, and users have zero control on them.\n\nTo accomodate this, we should allow users not passing in those constants generated internally but still be able the constants in the model.\n\nTest Plan:\nIncluded in commit.\n```\nbuild/bin/test_aot_inductor\n```\n\nDifferential Revision: D55286634\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/122562\nApproved by: https://github.com/chenyang78, https://github.com/khabinov",
    "label": "NO",
    "changes": [
        {
            "name": "test.cpp",
            "path": "test/cpp/aot_inductor/test.cpp",
            "patches": [
                {
                    "old_start": 241,
                    "old_length": 6,
                    "new_start": 241,
                    "new_length": 45,
                    "hunk": "@@ -241,6 +241,45 @@ void test_aoti_double_buffering(\n   ASSERT_TRUE(torch::allclose(ref_output_tensors[0], actual_output_tensors[0]));\n }\n \n+void test_aoti_double_buffering_with_tensor_constants() {\n+  torch::NoGradGuard no_grad;\n+\n+  std::string data_path = (std::filesystem::path(\n+                               STRINGIZE(CMAKE_CURRENT_BINARY_DIR)) /\n+                               \"data_with_tensor_constants.pt\")\n+                               .string();\n+\n+  torch::jit::script::Module data_loader = torch::jit::load(data_path);\n+  std::string path_attr = \"model_so_path\";\n+  std::string inputs_attr = \"inputs\";\n+  std::string w_attr = \"w\";\n+  std::string outputs_attr = \"outputs\";\n+  const auto& model_so_path = data_loader.attr(path_attr.c_str()).toStringRef();\n+  auto input_tensors =\n+      data_loader.attr(inputs_attr.c_str()).toTensorList().vec();\n+  const auto& w_tensors = data_loader.attr(w_attr.c_str()).toTensor();\n+  const auto& ref_output_tensors =\n+      data_loader.attr(outputs_attr.c_str()).toTensorList().vec();\n+\n+  torch::inductor::TensorConstantMap real_map;\n+  real_map.emplace(\"L__self___w\", new at::Tensor(w_tensors));\n+\n+  std::unique_ptr<torch::inductor::AOTIModelContainerRunner> runner;\n+  runner = std::make_unique<torch::inductor::AOTIModelContainerRunnerCuda>(\n+      model_so_path.c_str());\n+\n+  // By default, buffer #1 get loaded with burned in weights. Correct results.\n+  auto actual_output_tensors = runner->run(input_tensors);\n+  ASSERT_TRUE(torch::allclose(ref_output_tensors[0], actual_output_tensors[0]));\n+\n+  // We update the weights to buffer #2 and activate it. This should still\n+  // produce correct result, since we would have copied the tensor_constants.\n+  runner->update_inactive_constant_buffer(real_map);\n+  runner->swap_constant_buffer();\n+  actual_output_tensors = runner->run(input_tensors);\n+  ASSERT_TRUE(torch::allclose(ref_output_tensors[0], actual_output_tensors[0]));\n+}\n+\n } // namespace\n \n namespace torch {\n"
                },
                {
                    "old_start": 279,
                    "old_length": 6,
                    "new_start": 318,
                    "new_length": 10,
                    "hunk": "@@ -279,6 +318,10 @@ TEST(AotInductorTest, RuntimeUpdateInactiveConstantsCuda) {\n TEST(AotInductorTest, UpdateInactiveConstantsCuda) {\n   test_aoti_double_buffering(\"cuda\", false);\n }\n+\n+TEST(AotInductorTest, UpdateInactiveConstantsWithTensorConstantsCuda) {\n+  test_aoti_double_buffering_with_tensor_constants();\n+}\n #endif\n \n } // namespace inductor\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+void test_aoti_double_buffering_with_tensor_constants() {\n+  torch::NoGradGuard no_grad;\n+\n+  std::string data_path = (std::filesystem::path(\n+                               STRINGIZE(CMAKE_CURRENT_BINARY_DIR)) /\n+                               \"data_with_tensor_constants.pt\")\n+                               .string();\n+\n+  torch::jit::script::Module data_loader = torch::jit::load(data_path);\n+  std::string path_attr = \"model_so_path\";\n+  std::string inputs_attr = \"inputs\";\n+  std::string w_attr = \"w\";\n+  std::string outputs_attr = \"outputs\";\n+  const auto& model_so_path = data_loader.attr(path_attr.c_str()).toStringRef();\n+  auto input_tensors =\n+      data_loader.attr(inputs_attr.c_str()).toTensorList().vec();\n+  const auto& w_tensors = data_loader.attr(w_attr.c_str()).toTensor();\n+  const auto& ref_output_tensors =\n+      data_loader.attr(outputs_attr.c_str()).toTensorList().vec();\n+\n+  torch::inductor::TensorConstantMap real_map;\n+  real_map.emplace(\"L__self___w\", new at::Tensor(w_tensors));\n+\n+  std::unique_ptr<torch::inductor::AOTIModelContainerRunner> runner;\n+  runner = std::make_unique<torch::inductor::AOTIModelContainerRunnerCuda>(\n+      model_so_path.c_str());\n+\n+  // By default, buffer #1 get loaded with burned in weights. Correct results.\n+  auto actual_output_tensors = runner->run(input_tensors);\n+  ASSERT_TRUE(torch::allclose(ref_output_tensors[0], actual_output_tensors[0]));\n+\n+  // We update the weights to buffer #2 and activate it. This should still\n+  // produce correct result, since we would have copied the tensor_constants.\n+  runner->update_inactive_constant_buffer(real_map);\n+  runner->swap_constant_buffer();\n+  actual_output_tensors = runner->run(input_tensors);\n+  ASSERT_TRUE(torch::allclose(ref_output_tensors[0], actual_output_tensors[0]));\n+}\n+\n+\n+TEST(AotInductorTest, UpdateInactiveConstantsWithTensorConstantsCuda) {\n+  test_aoti_double_buffering_with_tensor_constants();\n+}\n",
            "whole_hunk": "@@ -241,6 +241,45 @@ void test_aoti_double_buffering(\n   ASSERT_TRUE(torch::allclose(ref_output_tensors[0], actual_output_tensors[0]));\n }\n \n+void test_aoti_double_buffering_with_tensor_constants() {\n+  torch::NoGradGuard no_grad;\n+\n+  std::string data_path = (std::filesystem::path(\n+                               STRINGIZE(CMAKE_CURRENT_BINARY_DIR)) /\n+                               \"data_with_tensor_constants.pt\")\n+                               .string();\n+\n+  torch::jit::script::Module data_loader = torch::jit::load(data_path);\n+  std::string path_attr = \"model_so_path\";\n+  std::string inputs_attr = \"inputs\";\n+  std::string w_attr = \"w\";\n+  std::string outputs_attr = \"outputs\";\n+  const auto& model_so_path = data_loader.attr(path_attr.c_str()).toStringRef();\n+  auto input_tensors =\n+      data_loader.attr(inputs_attr.c_str()).toTensorList().vec();\n+  const auto& w_tensors = data_loader.attr(w_attr.c_str()).toTensor();\n+  const auto& ref_output_tensors =\n+      data_loader.attr(outputs_attr.c_str()).toTensorList().vec();\n+\n+  torch::inductor::TensorConstantMap real_map;\n+  real_map.emplace(\"L__self___w\", new at::Tensor(w_tensors));\n+\n+  std::unique_ptr<torch::inductor::AOTIModelContainerRunner> runner;\n+  runner = std::make_unique<torch::inductor::AOTIModelContainerRunnerCuda>(\n+      model_so_path.c_str());\n+\n+  // By default, buffer #1 get loaded with burned in weights. Correct results.\n+  auto actual_output_tensors = runner->run(input_tensors);\n+  ASSERT_TRUE(torch::allclose(ref_output_tensors[0], actual_output_tensors[0]));\n+\n+  // We update the weights to buffer #2 and activate it. This should still\n+  // produce correct result, since we would have copied the tensor_constants.\n+  runner->update_inactive_constant_buffer(real_map);\n+  runner->swap_constant_buffer();\n+  actual_output_tensors = runner->run(input_tensors);\n+  ASSERT_TRUE(torch::allclose(ref_output_tensors[0], actual_output_tensors[0]));\n+}\n+\n } // namespace\n \n namespace torch {\n@@ -279,6 +318,10 @@ TEST(AotInductorTest, RuntimeUpdateInactiveConstantsCuda) {\n TEST(AotInductorTest, UpdateInactiveConstantsCuda) {\n   test_aoti_double_buffering(\"cuda\", false);\n }\n+\n+TEST(AotInductorTest, UpdateInactiveConstantsWithTensorConstantsCuda) {\n+  test_aoti_double_buffering_with_tensor_constants();\n+}\n #endif\n \n } // namespace inductor\n"
        },
        {
            "name": "test.py",
            "path": "test/cpp/aot_inductor/test.py",
            "patches": [
                {
                    "old_start": 17,
                    "old_length": 38,
                    "new_start": 17,
                    "new_length": 74,
                    "hunk": "@@ -17,38 +17,74 @@ class Net(torch.nn.Module):\n         w = w_relu + self.w_add\n         return torch.matmul(x, w)\n \n+class NetWithTensorConstants(torch.nn.Module):\n+    def __init__(self):\n+        super().__init__()\n+        self.w = torch.randn(30, 1, device=\"cuda\")\n+\n+    def forward(self, x, y):\n+        z = self.w * x * y\n+        return z[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17]]\n+\n data = {}\n+data_with_tensor_constants = {}\n+\n+# Basice AOTI model test generation.\n+def generate_basic_tests():\n+    for device in [\"cpu\", \"cuda\"]:\n+        for use_runtime_constant_folding in [True, False]:\n+            if device == \"cpu\" and use_runtime_constant_folding:\n+                # We do not test runtime const folding for cpu mode.\n+                continue\n+            model = Net(device).to(device=device)\n+            x = torch.randn((4, 4), device=device)\n+            with torch.no_grad():\n+                ref_output = model(x)\n+\n+            torch._dynamo.reset()\n+            with torch.no_grad():\n+                dim0_x = Dim(\"dim0_x\", min=1, max=1024)\n+                dynamic_shapes = {\"x\": {0: dim0_x}}\n+                model_so_path = aot_compile(\n+                    model,\n+                    (x,),\n+                    dynamic_shapes=dynamic_shapes,\n+                    options={\"aot_inductor.use_runtime_constant_folding\": use_runtime_constant_folding})\n+\n+            suffix = f\"{device}\"\n+            if use_runtime_constant_folding:\n+                suffix += \"_use_runtime_constant_folding\"\n+            data.update({\n+                f\"model_so_path_{suffix}\": model_so_path,\n+                f\"inputs_{suffix}\": [x],\n+                f\"outputs_{suffix}\": [ref_output],\n+                f\"w_pre_{suffix}\": model.w_pre,\n+                f\"w_add_{suffix}\": model.w_add,\n+            })\n+\n+# AOTI model which will create additional tensors during autograd.\n+def generate_test_with_additional_tensors():\n+    model = NetWithTensorConstants()\n+    x = torch.randn((30, 1), device=\"cuda\")\n+    y = torch.randn((30, 1), device=\"cuda\")\n+    with torch.no_grad():\n+        ref_output = model(x, y)\n+\n+    torch._dynamo.reset()\n+    with torch.no_grad():\n+        model_so_path = aot_compile(\n+            model,\n+            (x, y))\n+\n+    data_with_tensor_constants.update({\n+        \"model_so_path\": model_so_path,\n+        \"inputs\": [x, y],\n+        \"outputs\": [ref_output],\n+        \"w\": model.w,\n+    })\n \n-for device in [\"cpu\", \"cuda\"]:\n-    for use_runtime_constant_folding in [True, False]:\n-        if device == \"cpu\" and use_runtime_constant_folding:\n-            # We do not test runtime const folding for cpu mode.\n-            continue\n-        model = Net(device).to(device=device)\n-        x = torch.randn((4, 4), device=device)\n-        with torch.no_grad():\n-            ref_output = model(x)\n-\n-        torch._dynamo.reset()\n-        with torch.no_grad():\n-            dim0_x = Dim(\"dim0_x\", min=1, max=1024)\n-            dynamic_shapes = {\"x\": {0: dim0_x}}\n-            model_so_path = aot_compile(\n-                model,\n-                (x,),\n-                dynamic_shapes=dynamic_shapes,\n-                options={\"aot_inductor.use_runtime_constant_folding\": use_runtime_constant_folding})\n-\n-        suffix = f\"{device}\"\n-        if use_runtime_constant_folding:\n-            suffix += \"_use_runtime_constant_folding\"\n-        data.update({\n-            f\"model_so_path_{suffix}\": model_so_path,\n-            f\"inputs_{suffix}\": [x],\n-            f\"outputs_{suffix}\": [ref_output],\n-            f\"w_pre_{suffix}\": model.w_pre,\n-            f\"w_add_{suffix}\": model.w_add,\n-        })\n+generate_basic_tests()\n+generate_test_with_additional_tensors()\n \n # Use this to communicate tensors to the cpp code\n class Serializer(torch.nn.Module):\n"
                },
                {
                    "old_start": 58,
                    "old_length": 3,
                    "new_start": 94,
                    "new_length": 4,
                    "hunk": "@@ -58,3 +94,4 @@ class Serializer(torch.nn.Module):\n             setattr(self, key, data[key])\n \n torch.jit.script(Serializer(data)).save(\"data.pt\")\n+torch.jit.script(Serializer(data_with_tensor_constants)).save(\"data_with_tensor_constants.pt\")\n"
                }
            ],
            "whole_deleted": "-for device in [\"cpu\", \"cuda\"]:\n-    for use_runtime_constant_folding in [True, False]:\n-        if device == \"cpu\" and use_runtime_constant_folding:\n-            # We do not test runtime const folding for cpu mode.\n-            continue\n-        model = Net(device).to(device=device)\n-        x = torch.randn((4, 4), device=device)\n-        with torch.no_grad():\n-            ref_output = model(x)\n-\n-        torch._dynamo.reset()\n-        with torch.no_grad():\n-            dim0_x = Dim(\"dim0_x\", min=1, max=1024)\n-            dynamic_shapes = {\"x\": {0: dim0_x}}\n-            model_so_path = aot_compile(\n-                model,\n-                (x,),\n-                dynamic_shapes=dynamic_shapes,\n-                options={\"aot_inductor.use_runtime_constant_folding\": use_runtime_constant_folding})\n-\n-        suffix = f\"{device}\"\n-        if use_runtime_constant_folding:\n-            suffix += \"_use_runtime_constant_folding\"\n-        data.update({\n-            f\"model_so_path_{suffix}\": model_so_path,\n-            f\"inputs_{suffix}\": [x],\n-            f\"outputs_{suffix}\": [ref_output],\n-            f\"w_pre_{suffix}\": model.w_pre,\n-            f\"w_add_{suffix}\": model.w_add,\n-        })\n",
            "whole_added": "+class NetWithTensorConstants(torch.nn.Module):\n+    def __init__(self):\n+        super().__init__()\n+        self.w = torch.randn(30, 1, device=\"cuda\")\n+\n+    def forward(self, x, y):\n+        z = self.w * x * y\n+        return z[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17]]\n+\n+data_with_tensor_constants = {}\n+\n+# Basice AOTI model test generation.\n+def generate_basic_tests():\n+    for device in [\"cpu\", \"cuda\"]:\n+        for use_runtime_constant_folding in [True, False]:\n+            if device == \"cpu\" and use_runtime_constant_folding:\n+                # We do not test runtime const folding for cpu mode.\n+                continue\n+            model = Net(device).to(device=device)\n+            x = torch.randn((4, 4), device=device)\n+            with torch.no_grad():\n+                ref_output = model(x)\n+\n+            torch._dynamo.reset()\n+            with torch.no_grad():\n+                dim0_x = Dim(\"dim0_x\", min=1, max=1024)\n+                dynamic_shapes = {\"x\": {0: dim0_x}}\n+                model_so_path = aot_compile(\n+                    model,\n+                    (x,),\n+                    dynamic_shapes=dynamic_shapes,\n+                    options={\"aot_inductor.use_runtime_constant_folding\": use_runtime_constant_folding})\n+\n+            suffix = f\"{device}\"\n+            if use_runtime_constant_folding:\n+                suffix += \"_use_runtime_constant_folding\"\n+            data.update({\n+                f\"model_so_path_{suffix}\": model_so_path,\n+                f\"inputs_{suffix}\": [x],\n+                f\"outputs_{suffix}\": [ref_output],\n+                f\"w_pre_{suffix}\": model.w_pre,\n+                f\"w_add_{suffix}\": model.w_add,\n+            })\n+\n+# AOTI model which will create additional tensors during autograd.\n+def generate_test_with_additional_tensors():\n+    model = NetWithTensorConstants()\n+    x = torch.randn((30, 1), device=\"cuda\")\n+    y = torch.randn((30, 1), device=\"cuda\")\n+    with torch.no_grad():\n+        ref_output = model(x, y)\n+\n+    torch._dynamo.reset()\n+    with torch.no_grad():\n+        model_so_path = aot_compile(\n+            model,\n+            (x, y))\n+\n+    data_with_tensor_constants.update({\n+        \"model_so_path\": model_so_path,\n+        \"inputs\": [x, y],\n+        \"outputs\": [ref_output],\n+        \"w\": model.w,\n+    })\n+generate_basic_tests()\n+generate_test_with_additional_tensors()\n+torch.jit.script(Serializer(data_with_tensor_constants)).save(\"data_with_tensor_constants.pt\")\n",
            "whole_hunk": "@@ -17,38 +17,74 @@ class Net(torch.nn.Module):\n         w = w_relu + self.w_add\n         return torch.matmul(x, w)\n \n+class NetWithTensorConstants(torch.nn.Module):\n+    def __init__(self):\n+        super().__init__()\n+        self.w = torch.randn(30, 1, device=\"cuda\")\n+\n+    def forward(self, x, y):\n+        z = self.w * x * y\n+        return z[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17]]\n+\n data = {}\n+data_with_tensor_constants = {}\n+\n+# Basice AOTI model test generation.\n+def generate_basic_tests():\n+    for device in [\"cpu\", \"cuda\"]:\n+        for use_runtime_constant_folding in [True, False]:\n+            if device == \"cpu\" and use_runtime_constant_folding:\n+                # We do not test runtime const folding for cpu mode.\n+                continue\n+            model = Net(device).to(device=device)\n+            x = torch.randn((4, 4), device=device)\n+            with torch.no_grad():\n+                ref_output = model(x)\n+\n+            torch._dynamo.reset()\n+            with torch.no_grad():\n+                dim0_x = Dim(\"dim0_x\", min=1, max=1024)\n+                dynamic_shapes = {\"x\": {0: dim0_x}}\n+                model_so_path = aot_compile(\n+                    model,\n+                    (x,),\n+                    dynamic_shapes=dynamic_shapes,\n+                    options={\"aot_inductor.use_runtime_constant_folding\": use_runtime_constant_folding})\n+\n+            suffix = f\"{device}\"\n+            if use_runtime_constant_folding:\n+                suffix += \"_use_runtime_constant_folding\"\n+            data.update({\n+                f\"model_so_path_{suffix}\": model_so_path,\n+                f\"inputs_{suffix}\": [x],\n+                f\"outputs_{suffix}\": [ref_output],\n+                f\"w_pre_{suffix}\": model.w_pre,\n+                f\"w_add_{suffix}\": model.w_add,\n+            })\n+\n+# AOTI model which will create additional tensors during autograd.\n+def generate_test_with_additional_tensors():\n+    model = NetWithTensorConstants()\n+    x = torch.randn((30, 1), device=\"cuda\")\n+    y = torch.randn((30, 1), device=\"cuda\")\n+    with torch.no_grad():\n+        ref_output = model(x, y)\n+\n+    torch._dynamo.reset()\n+    with torch.no_grad():\n+        model_so_path = aot_compile(\n+            model,\n+            (x, y))\n+\n+    data_with_tensor_constants.update({\n+        \"model_so_path\": model_so_path,\n+        \"inputs\": [x, y],\n+        \"outputs\": [ref_output],\n+        \"w\": model.w,\n+    })\n \n-for device in [\"cpu\", \"cuda\"]:\n-    for use_runtime_constant_folding in [True, False]:\n-        if device == \"cpu\" and use_runtime_constant_folding:\n-            # We do not test runtime const folding for cpu mode.\n-            continue\n-        model = Net(device).to(device=device)\n-        x = torch.randn((4, 4), device=device)\n-        with torch.no_grad():\n-            ref_output = model(x)\n-\n-        torch._dynamo.reset()\n-        with torch.no_grad():\n-            dim0_x = Dim(\"dim0_x\", min=1, max=1024)\n-            dynamic_shapes = {\"x\": {0: dim0_x}}\n-            model_so_path = aot_compile(\n-                model,\n-                (x,),\n-                dynamic_shapes=dynamic_shapes,\n-                options={\"aot_inductor.use_runtime_constant_folding\": use_runtime_constant_folding})\n-\n-        suffix = f\"{device}\"\n-        if use_runtime_constant_folding:\n-            suffix += \"_use_runtime_constant_folding\"\n-        data.update({\n-            f\"model_so_path_{suffix}\": model_so_path,\n-            f\"inputs_{suffix}\": [x],\n-            f\"outputs_{suffix}\": [ref_output],\n-            f\"w_pre_{suffix}\": model.w_pre,\n-            f\"w_add_{suffix}\": model.w_add,\n-        })\n+generate_basic_tests()\n+generate_test_with_additional_tensors()\n \n # Use this to communicate tensors to the cpp code\n class Serializer(torch.nn.Module):\n@@ -58,3 +94,4 @@ class Serializer(torch.nn.Module):\n             setattr(self, key, data[key])\n \n torch.jit.script(Serializer(data)).save(\"data.pt\")\n+torch.jit.script(Serializer(data_with_tensor_constants)).save(\"data_with_tensor_constants.pt\")\n"
        },
        {
            "name": "model_container.h",
            "path": "torch/csrc/inductor/aoti_runtime/model_container.h",
            "patches": [
                {
                    "old_start": 218,
                    "old_length": 6,
                    "new_start": 218,
                    "new_length": 9,
                    "hunk": "@@ -218,6 +218,9 @@ class AOTInductorModelContainer {\n     pending_models_available_.notify_one();\n   }\n \n+  bool _is_tensor_constant(const std::string& constant_name) {\n+    return constant_name.rfind(\"_tensor_constant\", 0) == 0;\n+  }\n   // This function updates the buffer for storing constants.\n   // It will update the buffer, the mapping and the array mapping.\n   void update_constant_buffer(\n"
                },
                {
                    "old_start": 232,
                    "old_length": 6,
                    "new_start": 235,
                    "new_length": 7,
                    "hunk": "@@ -232,6 +235,7 @@ class AOTInductorModelContainer {\n \n     auto* constants_blob_ptr =\n         static_cast<uint8_t*>(get_constant_blob_ptr(use_inactive));\n+    auto original_constants_map = get_constants_map(!use_inactive);\n     auto constants_map_to_update = get_constants_map(use_inactive);\n \n     if (validate_full_update) {\n"
                },
                {
                    "old_start": 243,
                    "old_length": 6,
                    "new_start": 247,
                    "new_length": 13,
                    "hunk": "@@ -243,6 +247,13 @@ class AOTInductorModelContainer {\n         auto constant_name = std::string(models_[0]->constant_name(idx));\n         auto it = constants_map.find(constant_name);\n         if (it == constants_map.end()) {\n+          if (_is_tensor_constant(constant_name)) {\n+            // tracing sometimes create tensors that are non-existent in\n+            // original graph. We could skip those and do a direct copy.\n+            std::cerr << \"[WARNING] Found constant \" << constant_name\n+                      << \" in model, but not provided by user!\\n\";\n+            continue;\n+          }\n           throw std::runtime_error(\n               std::string(\"Cannot find constants \") + constant_name +\n               std::string(\" in constants_map!\"));\n"
                },
                {
                    "old_start": 253,
                    "old_length": 17,
                    "new_start": 264,
                    "new_length": 25,
                    "hunk": "@@ -253,17 +264,25 @@ class AOTInductorModelContainer {\n     for (size_t idx = 0; idx < num_constants; idx++) {\n       auto constant_name = std::string(models_[0]->constant_name(idx));\n       auto it = constants_map.find(constant_name);\n-      if (it == constants_map.end()) {\n+      if (it == constants_map.end() &&\n+          !(_is_tensor_constant(constant_name) && use_inactive)) {\n         continue;\n       }\n \n+      AtenTensorHandle tensor;\n+      if (_is_tensor_constant(constant_name) && use_inactive) {\n+        tensor = original_constants_map->find(constant_name)->second.get();\n+      } else {\n+        tensor = it->second;\n+      }\n+\n       // Move the data to container handled blob.\n       uint8_t* internal_constants_ptr =\n           constants_blob_ptr + constants_internal_offset_[idx];\n       void* user_constant_ptr;\n       int64_t constant_size;\n-      aoti_torch_get_data_ptr(it->second, &user_constant_ptr);\n-      aoti_torch_get_storage_size(it->second, &constant_size);\n+      aoti_torch_get_data_ptr(tensor, &user_constant_ptr);\n+      aoti_torch_get_storage_size(tensor, &constant_size);\n \n       AOTI_RUNTIME_DEVICE_CHECK(cudaMemcpy(\n           internal_constants_ptr,\n"
                },
                {
                    "old_start": 278,
                    "old_length": 9,
                    "new_start": 297,
                    "new_length": 9,
                    "hunk": "@@ -278,9 +297,9 @@ class AOTInductorModelContainer {\n       int64_t* stride;\n       int64_t offset;\n       int device_idx = models_[0]->get_device_idx();\n-      AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_strides(it->second, &stride));\n+      AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_strides(tensor, &stride));\n       AOTI_TORCH_ERROR_CODE_CHECK(\n-          aoti_torch_get_storage_offset(it->second, &offset));\n+          aoti_torch_get_storage_offset(tensor, &offset));\n       AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_create_tensor_from_blob(\n           internal_constants_ptr,\n           models_[0]->constant_ndim(idx),"
                }
            ],
            "whole_deleted": "-      if (it == constants_map.end()) {\n-      aoti_torch_get_data_ptr(it->second, &user_constant_ptr);\n-      aoti_torch_get_storage_size(it->second, &constant_size);\n-      AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_strides(it->second, &stride));\n-          aoti_torch_get_storage_offset(it->second, &offset));\n",
            "whole_added": "+  bool _is_tensor_constant(const std::string& constant_name) {\n+    return constant_name.rfind(\"_tensor_constant\", 0) == 0;\n+  }\n+    auto original_constants_map = get_constants_map(!use_inactive);\n+          if (_is_tensor_constant(constant_name)) {\n+            // tracing sometimes create tensors that are non-existent in\n+            // original graph. We could skip those and do a direct copy.\n+            std::cerr << \"[WARNING] Found constant \" << constant_name\n+                      << \" in model, but not provided by user!\\n\";\n+            continue;\n+          }\n+      if (it == constants_map.end() &&\n+          !(_is_tensor_constant(constant_name) && use_inactive)) {\n+      AtenTensorHandle tensor;\n+      if (_is_tensor_constant(constant_name) && use_inactive) {\n+        tensor = original_constants_map->find(constant_name)->second.get();\n+      } else {\n+        tensor = it->second;\n+      }\n+\n+      aoti_torch_get_data_ptr(tensor, &user_constant_ptr);\n+      aoti_torch_get_storage_size(tensor, &constant_size);\n+      AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_strides(tensor, &stride));\n+          aoti_torch_get_storage_offset(tensor, &offset));\n",
            "whole_hunk": "@@ -218,6 +218,9 @@ class AOTInductorModelContainer {\n     pending_models_available_.notify_one();\n   }\n \n+  bool _is_tensor_constant(const std::string& constant_name) {\n+    return constant_name.rfind(\"_tensor_constant\", 0) == 0;\n+  }\n   // This function updates the buffer for storing constants.\n   // It will update the buffer, the mapping and the array mapping.\n   void update_constant_buffer(\n@@ -232,6 +235,7 @@ class AOTInductorModelContainer {\n \n     auto* constants_blob_ptr =\n         static_cast<uint8_t*>(get_constant_blob_ptr(use_inactive));\n+    auto original_constants_map = get_constants_map(!use_inactive);\n     auto constants_map_to_update = get_constants_map(use_inactive);\n \n     if (validate_full_update) {\n@@ -243,6 +247,13 @@ class AOTInductorModelContainer {\n         auto constant_name = std::string(models_[0]->constant_name(idx));\n         auto it = constants_map.find(constant_name);\n         if (it == constants_map.end()) {\n+          if (_is_tensor_constant(constant_name)) {\n+            // tracing sometimes create tensors that are non-existent in\n+            // original graph. We could skip those and do a direct copy.\n+            std::cerr << \"[WARNING] Found constant \" << constant_name\n+                      << \" in model, but not provided by user!\\n\";\n+            continue;\n+          }\n           throw std::runtime_error(\n               std::string(\"Cannot find constants \") + constant_name +\n               std::string(\" in constants_map!\"));\n@@ -253,17 +264,25 @@ class AOTInductorModelContainer {\n     for (size_t idx = 0; idx < num_constants; idx++) {\n       auto constant_name = std::string(models_[0]->constant_name(idx));\n       auto it = constants_map.find(constant_name);\n-      if (it == constants_map.end()) {\n+      if (it == constants_map.end() &&\n+          !(_is_tensor_constant(constant_name) && use_inactive)) {\n         continue;\n       }\n \n+      AtenTensorHandle tensor;\n+      if (_is_tensor_constant(constant_name) && use_inactive) {\n+        tensor = original_constants_map->find(constant_name)->second.get();\n+      } else {\n+        tensor = it->second;\n+      }\n+\n       // Move the data to container handled blob.\n       uint8_t* internal_constants_ptr =\n           constants_blob_ptr + constants_internal_offset_[idx];\n       void* user_constant_ptr;\n       int64_t constant_size;\n-      aoti_torch_get_data_ptr(it->second, &user_constant_ptr);\n-      aoti_torch_get_storage_size(it->second, &constant_size);\n+      aoti_torch_get_data_ptr(tensor, &user_constant_ptr);\n+      aoti_torch_get_storage_size(tensor, &constant_size);\n \n       AOTI_RUNTIME_DEVICE_CHECK(cudaMemcpy(\n           internal_constants_ptr,\n@@ -278,9 +297,9 @@ class AOTInductorModelContainer {\n       int64_t* stride;\n       int64_t offset;\n       int device_idx = models_[0]->get_device_idx();\n-      AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_strides(it->second, &stride));\n+      AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_strides(tensor, &stride));\n       AOTI_TORCH_ERROR_CODE_CHECK(\n-          aoti_torch_get_storage_offset(it->second, &offset));\n+          aoti_torch_get_storage_offset(tensor, &offset));\n       AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_create_tensor_from_blob(\n           internal_constants_ptr,\n           models_[0]->constant_ndim(idx),"
        }
    ]
},
{
    "Id": 493,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/fd6c993eeaacda7ef6b83f59ad3474aed0d52eaf",
    "date": "2023-10-02T17:34:31+00:00",
    "message": "Add missing CUDA error check (#110368)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110368\nApproved by: https://github.com/Skylion007",
    "label": "YES",
    "changes": [
        {
            "name": "flash_bwd_launch_template.h",
            "path": "aten/src/ATen/native/transformers/cuda/flash_attn/flash_bwd_launch_template.h",
            "patches": [
                {
                    "old_start": 182,
                    "old_length": 6,
                    "new_start": 182,
                    "new_length": 9,
                    "hunk": "@@ -182,6 +182,9 @@ void run_mha_bwd_hdim32(Flash_bwd_params &params, cudaStream_t stream, const boo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         if (max_smem_per_block >= 2 * ((3 * 128 + 2 * 128) * Headdim + 2 * 128 * 128)) { // 104 KB\n             if constexpr(!Is_dropout) {  // We can afford more registers to keep V in registers\n"
                },
                {
                    "old_start": 203,
                    "old_length": 6,
                    "new_start": 206,
                    "new_length": 9,
                    "hunk": "@@ -203,6 +206,9 @@ void run_mha_bwd_hdim64(Flash_bwd_params &params, cudaStream_t stream, const boo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     // printf(\"max_smem_per_block = %d\\n\", max_smem_per_block);\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         // Changing AtomLayoutMdQ from 2 to 4 takes the same time\n"
                },
                {
                    "old_start": 246,
                    "old_length": 6,
                    "new_start": 252,
                    "new_length": 9,
                    "hunk": "@@ -246,6 +252,9 @@ void run_mha_bwd_hdim96(Flash_bwd_params &params, cudaStream_t stream, const boo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     // printf(\"max_smem_per_block = %d\\n\", max_smem_per_block);\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         // if (params.h == params.h_k) {\n"
                },
                {
                    "old_start": 273,
                    "old_length": 6,
                    "new_start": 282,
                    "new_length": 9,
                    "hunk": "@@ -273,6 +282,9 @@ void run_mha_bwd_hdim128(Flash_bwd_params &params, cudaStream_t stream, const bo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     // printf(\"max_smem_per_block = %d\\n\", max_smem_per_block);\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         // if (params.h == params.h_k) {\n"
                },
                {
                    "old_start": 309,
                    "old_length": 6,
                    "new_start": 321,
                    "new_length": 9,
                    "hunk": "@@ -309,6 +321,9 @@ void run_mha_bwd_hdim160(Flash_bwd_params &params, cudaStream_t stream, const bo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         if (max_smem_per_block >= 116 * 1024) {\n             run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 64, 8, 4, 4, 4, false, false, T>, Is_dropout>(params, stream, configure);\n"
                },
                {
                    "old_start": 326,
                    "old_length": 6,
                    "new_start": 341,
                    "new_length": 9,
                    "hunk": "@@ -326,6 +341,9 @@ void run_mha_bwd_hdim192(Flash_bwd_params &params, cudaStream_t stream, const bo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         if (max_smem_per_block >= 136 * 1024) {\n             run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 64, 8, 4, 2, 2, false, false, T>, Is_dropout>(params, stream, configure);\n"
                },
                {
                    "old_start": 351,
                    "old_length": 6,
                    "new_start": 369,
                    "new_length": 9,
                    "hunk": "@@ -351,6 +369,9 @@ void run_mha_bwd_hdim256(Flash_bwd_params &params, cudaStream_t stream, const bo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         if (max_smem_per_block >= 176 * 1024) {  // H100\n             run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 64, 8, 4, 2, 2, false, false, T>, Is_dropout>(params, stream, configure);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n",
            "whole_hunk": "@@ -182,6 +182,9 @@ void run_mha_bwd_hdim32(Flash_bwd_params &params, cudaStream_t stream, const boo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         if (max_smem_per_block >= 2 * ((3 * 128 + 2 * 128) * Headdim + 2 * 128 * 128)) { // 104 KB\n             if constexpr(!Is_dropout) {  // We can afford more registers to keep V in registers\n@@ -203,6 +206,9 @@ void run_mha_bwd_hdim64(Flash_bwd_params &params, cudaStream_t stream, const boo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     // printf(\"max_smem_per_block = %d\\n\", max_smem_per_block);\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         // Changing AtomLayoutMdQ from 2 to 4 takes the same time\n@@ -246,6 +252,9 @@ void run_mha_bwd_hdim96(Flash_bwd_params &params, cudaStream_t stream, const boo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     // printf(\"max_smem_per_block = %d\\n\", max_smem_per_block);\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         // if (params.h == params.h_k) {\n@@ -273,6 +282,9 @@ void run_mha_bwd_hdim128(Flash_bwd_params &params, cudaStream_t stream, const bo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     // printf(\"max_smem_per_block = %d\\n\", max_smem_per_block);\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         // if (params.h == params.h_k) {\n@@ -309,6 +321,9 @@ void run_mha_bwd_hdim160(Flash_bwd_params &params, cudaStream_t stream, const bo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         if (max_smem_per_block >= 116 * 1024) {\n             run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 64, 8, 4, 4, 4, false, false, T>, Is_dropout>(params, stream, configure);\n@@ -326,6 +341,9 @@ void run_mha_bwd_hdim192(Flash_bwd_params &params, cudaStream_t stream, const bo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         if (max_smem_per_block >= 136 * 1024) {\n             run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 64, 8, 4, 2, 2, false, false, T>, Is_dropout>(params, stream, configure);\n@@ -351,6 +369,9 @@ void run_mha_bwd_hdim256(Flash_bwd_params &params, cudaStream_t stream, const bo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         if (max_smem_per_block >= 176 * 1024) {  // H100\n             run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 64, 8, 4, 2, 2, false, false, T>, Is_dropout>(params, stream, configure);"
        }
    ]
},
{
    "Id": 464,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/d97332f8391e9d1f0c3fa019376d04b523bfb06c",
    "date": "2023-10-27T16:54:23+00:00",
    "message": "Add cuda status checks to FA templates (#112229)\n\n# Summary\ncuda status checks were accidentely removed on latest update\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/112229\nApproved by: https://github.com/Skylion007",
    "label": "YES",
    "changes": [
        {
            "name": "flash_bwd_launch_template.h",
            "path": "aten/src/ATen/native/transformers/cuda/flash_attn/flash_bwd_launch_template.h",
            "patches": [
                {
                    "old_start": 149,
                    "old_length": 6,
                    "new_start": 149,
                    "new_length": 9,
                    "hunk": "@@ -149,6 +149,9 @@ void run_mha_bwd_hdim32(Flash_bwd_params &params, cudaStream_t stream, const boo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         if (max_smem_per_block >= 2 * ((3 * 128 + 2 * 128) * Headdim + 2 * 128 * 128)) { // 104 KB\n             if constexpr(!Is_dropout) {  // We can afford more registers to keep V in registers\n"
                },
                {
                    "old_start": 170,
                    "old_length": 6,
                    "new_start": 173,
                    "new_length": 9,
                    "hunk": "@@ -170,6 +173,9 @@ void run_mha_bwd_hdim64(Flash_bwd_params &params, cudaStream_t stream, const boo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     // printf(\"max_smem_per_block = %d\\n\", max_smem_per_block);\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         // Changing AtomLayoutMdQ from 2 to 4 takes the same time\n"
                },
                {
                    "old_start": 213,
                    "old_length": 6,
                    "new_start": 219,
                    "new_length": 9,
                    "hunk": "@@ -213,6 +219,9 @@ void run_mha_bwd_hdim96(Flash_bwd_params &params, cudaStream_t stream, const boo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     // printf(\"max_smem_per_block = %d\\n\", max_smem_per_block);\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         // if (params.h == params.h_k) {\n"
                },
                {
                    "old_start": 240,
                    "old_length": 6,
                    "new_start": 249,
                    "new_length": 9,
                    "hunk": "@@ -240,6 +249,9 @@ void run_mha_bwd_hdim128(Flash_bwd_params &params, cudaStream_t stream, const bo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     // printf(\"max_smem_per_block = %d\\n\", max_smem_per_block);\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         // if (params.h == params.h_k) {\n"
                },
                {
                    "old_start": 276,
                    "old_length": 6,
                    "new_start": 288,
                    "new_length": 9,
                    "hunk": "@@ -276,6 +288,9 @@ void run_mha_bwd_hdim160(Flash_bwd_params &params, cudaStream_t stream, const bo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         if (max_smem_per_block >= 116 * 1024) {\n             run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 64, 8, 4, 4, 4, false, false, T>, Is_dropout>(params, stream, configure);\n"
                },
                {
                    "old_start": 293,
                    "old_length": 6,
                    "new_start": 308,
                    "new_length": 9,
                    "hunk": "@@ -293,6 +308,9 @@ void run_mha_bwd_hdim192(Flash_bwd_params &params, cudaStream_t stream, const bo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         if (max_smem_per_block >= 136 * 1024) {\n             run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 64, 8, 4, 2, 2, false, false, T>, Is_dropout>(params, stream, configure);\n"
                },
                {
                    "old_start": 318,
                    "old_length": 6,
                    "new_start": 336,
                    "new_length": 9,
                    "hunk": "@@ -318,6 +336,9 @@ void run_mha_bwd_hdim256(Flash_bwd_params &params, cudaStream_t stream, const bo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         if (max_smem_per_block >= 176 * 1024) {  // H100\n             run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 64, 8, 4, 2, 2, false, false, T>, Is_dropout>(params, stream, configure);\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n",
            "whole_hunk": "@@ -149,6 +149,9 @@ void run_mha_bwd_hdim32(Flash_bwd_params &params, cudaStream_t stream, const boo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         if (max_smem_per_block >= 2 * ((3 * 128 + 2 * 128) * Headdim + 2 * 128 * 128)) { // 104 KB\n             if constexpr(!Is_dropout) {  // We can afford more registers to keep V in registers\n@@ -170,6 +173,9 @@ void run_mha_bwd_hdim64(Flash_bwd_params &params, cudaStream_t stream, const boo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     // printf(\"max_smem_per_block = %d\\n\", max_smem_per_block);\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         // Changing AtomLayoutMdQ from 2 to 4 takes the same time\n@@ -213,6 +219,9 @@ void run_mha_bwd_hdim96(Flash_bwd_params &params, cudaStream_t stream, const boo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     // printf(\"max_smem_per_block = %d\\n\", max_smem_per_block);\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         // if (params.h == params.h_k) {\n@@ -240,6 +249,9 @@ void run_mha_bwd_hdim128(Flash_bwd_params &params, cudaStream_t stream, const bo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     // printf(\"max_smem_per_block = %d\\n\", max_smem_per_block);\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         // if (params.h == params.h_k) {\n@@ -276,6 +288,9 @@ void run_mha_bwd_hdim160(Flash_bwd_params &params, cudaStream_t stream, const bo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         if (max_smem_per_block >= 116 * 1024) {\n             run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 64, 8, 4, 4, 4, false, false, T>, Is_dropout>(params, stream, configure);\n@@ -293,6 +308,9 @@ void run_mha_bwd_hdim192(Flash_bwd_params &params, cudaStream_t stream, const bo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         if (max_smem_per_block >= 136 * 1024) {\n             run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 64, 8, 4, 2, 2, false, false, T>, Is_dropout>(params, stream, configure);\n@@ -318,6 +336,9 @@ void run_mha_bwd_hdim256(Flash_bwd_params &params, cudaStream_t stream, const bo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         if (max_smem_per_block >= 176 * 1024) {  // H100\n             run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 64, 8, 4, 2, 2, false, false, T>, Is_dropout>(params, stream, configure);\n"
        },
        {
            "name": "flash_fwd_launch_template.h",
            "path": "aten/src/ATen/native/transformers/cuda/flash_attn/flash_fwd_launch_template.h",
            "patches": [
                {
                    "old_start": 299,
                    "old_length": 6,
                    "new_start": 299,
                    "new_length": 9,
                    "hunk": "@@ -299,6 +299,9 @@ void run_mha_fwd_hdim224(Flash_fwd_params &params, cudaStream_t stream) {\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     // printf(\"max_smem_per_block = %d\\n\", max_smem_per_block);\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         BOOL_SWITCH(params.is_causal, Is_causal, [&] {\n"
                },
                {
                    "old_start": 327,
                    "old_length": 6,
                    "new_start": 330,
                    "new_length": 9,
                    "hunk": "@@ -327,6 +330,9 @@ void run_mha_fwd_hdim256(Flash_fwd_params &params, cudaStream_t stream) {\n         &max_smem_per_sm, cudaDevAttrMaxSharedMemoryPerMultiprocessor, device);\n     status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     // printf(\"max_smem_per_sm = %d, max_smem_per_block = %d\\n\", max_smem_per_sm, max_smem_per_block);\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         BOOL_SWITCH(params.is_causal, Is_causal, [&] {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n",
            "whole_hunk": "@@ -299,6 +299,9 @@ void run_mha_fwd_hdim224(Flash_fwd_params &params, cudaStream_t stream) {\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     // printf(\"max_smem_per_block = %d\\n\", max_smem_per_block);\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         BOOL_SWITCH(params.is_causal, Is_causal, [&] {\n@@ -327,6 +330,9 @@ void run_mha_fwd_hdim256(Flash_fwd_params &params, cudaStream_t stream) {\n         &max_smem_per_sm, cudaDevAttrMaxSharedMemoryPerMultiprocessor, device);\n     status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     // printf(\"max_smem_per_sm = %d, max_smem_per_block = %d\\n\", max_smem_per_sm, max_smem_per_block);\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         BOOL_SWITCH(params.is_causal, Is_causal, [&] {"
        }
    ]
},
{
    "Id": 507,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/d65e067baa2ef224444e180594c4882b5a55efde",
    "date": "2023-09-21T22:40:58+00:00",
    "message": "Updates to attn_mask handiling in mem_eff (#109620)\n\n# Summar\nAlign internal changes to what is xformers: https://github.com/facebookresearch/xformers/commit/a67cd575315a6b59c16d735fe6dac66419d18e7b\n\nWe have actually already removed the bias 4d view so this is in theory is a no op and really just increased safety checks\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109620\nApproved by: https://github.com/cpuhrsch",
    "label": "NO",
    "changes": [
        {
            "name": "attention.cpp",
            "path": "aten/src/ATen/native/transformers/attention.cpp",
            "patches": [
                {
                    "old_start": 523,
                    "old_length": 7,
                    "new_start": 523,
                    "new_length": 7,
                    "hunk": "@@ -523,7 +523,7 @@ at::Tensor preprocess_mask(\n   }\n   // Check and make the tensor contiguous if needed\n   if (attn_mask.sym_stride(0) % 16 != 0 || attn_mask.sym_stride(1) % 16 != 0 ||\n-      attn_mask.sym_stride(2) % 16 != 0) {\n+      attn_mask.sym_stride(2) % 16 != 0 || attn_mask.sym_stride(3) != 1) {\n     return attn_mask.contiguous();\n   }\n \n"
                }
            ],
            "whole_deleted": "-      attn_mask.sym_stride(2) % 16 != 0) {\n",
            "whole_added": "+      attn_mask.sym_stride(2) % 16 != 0 || attn_mask.sym_stride(3) != 1) {\n",
            "whole_hunk": "@@ -523,7 +523,7 @@ at::Tensor preprocess_mask(\n   }\n   // Check and make the tensor contiguous if needed\n   if (attn_mask.sym_stride(0) % 16 != 0 || attn_mask.sym_stride(1) % 16 != 0 ||\n-      attn_mask.sym_stride(2) % 16 != 0) {\n+      attn_mask.sym_stride(2) % 16 != 0 || attn_mask.sym_stride(3) != 1) {\n     return attn_mask.contiguous();\n   }\n \n"
        },
        {
            "name": "attention.cu",
            "path": "aten/src/ATen/native/transformers/cuda/attention.cu",
            "patches": [
                {
                    "old_start": 1080,
                    "old_length": 19,
                    "new_start": 1080,
                    "new_length": 25,
                    "hunk": "@@ -1080,19 +1080,25 @@ std::tuple<at::Tensor, at::Tensor, Tensor, Tensor> _efficient_attention_forward(\n           \"invalid dtype for bias - should match query's dtype\");\n       p.attn_bias_ptr = (scalar_t*)bias->data_ptr();\n \n-      // assign strides for bias, viewed as\n-      // (batch_sz, n_heads, n_queries, n_keys)\n-      // We make sure to expand prior to calling the kernel\n-      const at::Tensor& bias_4d_view = *bias;\n-      TORCH_CHECK(bias_4d_view.dim()==4);\n-      TORCH_CHECK(bias_4d_view.size(0)==B);\n-      TORCH_CHECK(bias_4d_view.size(1)==num_heads);\n-      TORCH_CHECK(bias_4d_view.size(2)==M);\n-      TORCH_CHECK(bias_4d_view.size(3)==N);\n-\n-      ASSIGN_CHECK_OVERFLOW(p.bias_strideB, bias_4d_view.stride(0));\n-      ASSIGN_CHECK_OVERFLOW(p.bias_strideH, bias_4d_view.stride(1));\n-      ASSIGN_CHECK_OVERFLOW(p.bias_strideM, bias_4d_view.stride(2));\n+      TORCH_CHECK(bias->dim() == 4, \"Bias expected in BMHK format\");\n+      TORCH_CHECK(\n+          bias->size(0) == query.size(0),\n+          \"attn_bias: wrong shape (batch dimension)\");\n+      TORCH_CHECK(\n+          bias->size(1) == query.size(2),\n+          \"attn_bias: wrong shape (head dimension)\");\n+      TORCH_CHECK(\n+          bias->size(2) == query.size(1),\n+          \"attn_bias: wrong shape (seqlenQ dimension)\");\n+      TORCH_CHECK(\n+          bias->size(3) == key.size(1),\n+          \"attn_bias: wrong shape (seqlenKV dimension)\");\n+      ASSIGN_CHECK_OVERFLOW(p.bias_strideB, bias->stride(0));\n+      ASSIGN_CHECK_OVERFLOW(p.bias_strideH, bias->stride(1));\n+      ASSIGN_CHECK_OVERFLOW(p.bias_strideM, bias->stride(2));\n+      TORCH_CHECK(\n+          bias->stride(3) == 1,\n+          \"attn_bias: wrong alignment (last dimension must be contiguous)\");\n     }\n \n     p.use_dropout = use_dropout;\n"
                }
            ],
            "whole_deleted": "-      // assign strides for bias, viewed as\n-      // (batch_sz, n_heads, n_queries, n_keys)\n-      // We make sure to expand prior to calling the kernel\n-      const at::Tensor& bias_4d_view = *bias;\n-      TORCH_CHECK(bias_4d_view.dim()==4);\n-      TORCH_CHECK(bias_4d_view.size(0)==B);\n-      TORCH_CHECK(bias_4d_view.size(1)==num_heads);\n-      TORCH_CHECK(bias_4d_view.size(2)==M);\n-      TORCH_CHECK(bias_4d_view.size(3)==N);\n-\n-      ASSIGN_CHECK_OVERFLOW(p.bias_strideB, bias_4d_view.stride(0));\n-      ASSIGN_CHECK_OVERFLOW(p.bias_strideH, bias_4d_view.stride(1));\n-      ASSIGN_CHECK_OVERFLOW(p.bias_strideM, bias_4d_view.stride(2));\n",
            "whole_added": "+      TORCH_CHECK(bias->dim() == 4, \"Bias expected in BMHK format\");\n+      TORCH_CHECK(\n+          bias->size(0) == query.size(0),\n+          \"attn_bias: wrong shape (batch dimension)\");\n+      TORCH_CHECK(\n+          bias->size(1) == query.size(2),\n+          \"attn_bias: wrong shape (head dimension)\");\n+      TORCH_CHECK(\n+          bias->size(2) == query.size(1),\n+          \"attn_bias: wrong shape (seqlenQ dimension)\");\n+      TORCH_CHECK(\n+          bias->size(3) == key.size(1),\n+          \"attn_bias: wrong shape (seqlenKV dimension)\");\n+      ASSIGN_CHECK_OVERFLOW(p.bias_strideB, bias->stride(0));\n+      ASSIGN_CHECK_OVERFLOW(p.bias_strideH, bias->stride(1));\n+      ASSIGN_CHECK_OVERFLOW(p.bias_strideM, bias->stride(2));\n+      TORCH_CHECK(\n+          bias->stride(3) == 1,\n+          \"attn_bias: wrong alignment (last dimension must be contiguous)\");\n",
            "whole_hunk": "@@ -1080,19 +1080,25 @@ std::tuple<at::Tensor, at::Tensor, Tensor, Tensor> _efficient_attention_forward(\n           \"invalid dtype for bias - should match query's dtype\");\n       p.attn_bias_ptr = (scalar_t*)bias->data_ptr();\n \n-      // assign strides for bias, viewed as\n-      // (batch_sz, n_heads, n_queries, n_keys)\n-      // We make sure to expand prior to calling the kernel\n-      const at::Tensor& bias_4d_view = *bias;\n-      TORCH_CHECK(bias_4d_view.dim()==4);\n-      TORCH_CHECK(bias_4d_view.size(0)==B);\n-      TORCH_CHECK(bias_4d_view.size(1)==num_heads);\n-      TORCH_CHECK(bias_4d_view.size(2)==M);\n-      TORCH_CHECK(bias_4d_view.size(3)==N);\n-\n-      ASSIGN_CHECK_OVERFLOW(p.bias_strideB, bias_4d_view.stride(0));\n-      ASSIGN_CHECK_OVERFLOW(p.bias_strideH, bias_4d_view.stride(1));\n-      ASSIGN_CHECK_OVERFLOW(p.bias_strideM, bias_4d_view.stride(2));\n+      TORCH_CHECK(bias->dim() == 4, \"Bias expected in BMHK format\");\n+      TORCH_CHECK(\n+          bias->size(0) == query.size(0),\n+          \"attn_bias: wrong shape (batch dimension)\");\n+      TORCH_CHECK(\n+          bias->size(1) == query.size(2),\n+          \"attn_bias: wrong shape (head dimension)\");\n+      TORCH_CHECK(\n+          bias->size(2) == query.size(1),\n+          \"attn_bias: wrong shape (seqlenQ dimension)\");\n+      TORCH_CHECK(\n+          bias->size(3) == key.size(1),\n+          \"attn_bias: wrong shape (seqlenKV dimension)\");\n+      ASSIGN_CHECK_OVERFLOW(p.bias_strideB, bias->stride(0));\n+      ASSIGN_CHECK_OVERFLOW(p.bias_strideH, bias->stride(1));\n+      ASSIGN_CHECK_OVERFLOW(p.bias_strideM, bias->stride(2));\n+      TORCH_CHECK(\n+          bias->stride(3) == 1,\n+          \"attn_bias: wrong alignment (last dimension must be contiguous)\");\n     }\n \n     p.use_dropout = use_dropout;\n"
        },
        {
            "name": "attention_backward.cu",
            "path": "aten/src/ATen/native/transformers/cuda/attention_backward.cu",
            "patches": [
                {
                    "old_start": 360,
                    "old_length": 40,
                    "new_start": 360,
                    "new_length": 32,
                    "hunk": "@@ -360,40 +360,32 @@ _efficient_attention_backward(\n \n       p.bias_ptr = (scalar_t*)bias->data_ptr();\n \n-      // assign strides for bias, viewed as:\n-      // (batch_sz, n_heads, n_queries, n_keys)\n-      // We make sure to expand prior to calling the kernel\n-      const at::Tensor& bias_4d_view = *bias;\n-      TORCH_CHECK(bias_4d_view.dim()==4);\n-      TORCH_CHECK(bias_4d_view.size(0)==B);\n-      TORCH_CHECK(bias_4d_view.size(1)==nH);\n-      TORCH_CHECK(bias_4d_view.size(2)==M);\n-      TORCH_CHECK(bias_4d_view.size(3)==N);\n-      ASSIGN_CHECK_OVERFLOW(p.bias_strideB, bias_4d_view.stride(0));\n-      ASSIGN_CHECK_OVERFLOW(p.bias_strideH, bias_4d_view.stride(1));\n-      ASSIGN_CHECK_OVERFLOW(p.bias_strideM, bias_4d_view.stride(2));\n+      TORCH_CHECK(bias->dim() == 4, \"Bias expected in BMHK format\");\n+      TORCH_CHECK(\n+          bias->size(0) == query.size(0),\n+          \"attn_bias: wrong shape (batch dimension)\");\n+      TORCH_CHECK(\n+          bias->size(1) == query.size(2),\n+          \"attn_bias: wrong shape (head dimension)\");\n+      TORCH_CHECK(\n+          bias->size(2) == query.size(1),\n+          \"attn_bias: wrong shape (seqlenQ dimension)\");\n+      TORCH_CHECK(\n+          bias->size(3) == key.size(1),\n+          \"attn_bias: wrong shape (seqlenKV dimension)\");\n+      TORCH_CHECK(\n+          bias->stride(3) == 1,\n+          \"attn_bias: wrong alignment (last dimension must be contiguous)\");\n+      ASSIGN_CHECK_OVERFLOW(p.bias_strideB, bias->stride(0));\n+      ASSIGN_CHECK_OVERFLOW(p.bias_strideH, bias->stride(1));\n+      ASSIGN_CHECK_OVERFLOW(p.bias_strideM, bias->stride(2));\n \n       if (bias_requires_grad) {\n         p.grad_bias_ptr = (scalar_t*)grad_bias.data_ptr();\n \n-        // assign strides for gB, viewed as\n-        // (batch_sz, n_heads, n_queries, n_keys). might have different strides\n-        // than B, for example if bias tensor was created with\n-        // torch.tensor((B * nH, 1, nK)).expand((B * nH, nQ, nK)),\n-        // different values of Q will point to the same memory\n-        // locations, meaning bias.stride(1) == 0, while we'd want\n-        // grad_bias.stride(1) == nK\n-        // We have expanded the input prior to calling the forward kernel\n-        const at::Tensor& grad_bias_4d_view = grad_bias;\n-        TORCH_CHECK(grad_bias_4d_view.dim()==4);\n-        TORCH_CHECK(grad_bias_4d_view.size(0)==B);\n-        TORCH_CHECK(grad_bias_4d_view.size(1)==nH);\n-        TORCH_CHECK(grad_bias_4d_view.size(2)==M);\n-        TORCH_CHECK(grad_bias_4d_view.size(3)==N);\n-\n-        ASSIGN_CHECK_OVERFLOW(p.gB_strideB, grad_bias_4d_view.stride(0));\n-        ASSIGN_CHECK_OVERFLOW(p.gB_strideH, grad_bias_4d_view.stride(1));\n-        ASSIGN_CHECK_OVERFLOW(p.gB_strideM, grad_bias_4d_view.stride(2));\n+        ASSIGN_CHECK_OVERFLOW(p.gB_strideB, grad_bias.stride(0));\n+        ASSIGN_CHECK_OVERFLOW(p.gB_strideH, grad_bias.stride(1));\n+        ASSIGN_CHECK_OVERFLOW(p.gB_strideM, grad_bias.stride(2));\n       }\n     }\n \n"
                }
            ],
            "whole_deleted": "-      // assign strides for bias, viewed as:\n-      // (batch_sz, n_heads, n_queries, n_keys)\n-      // We make sure to expand prior to calling the kernel\n-      const at::Tensor& bias_4d_view = *bias;\n-      TORCH_CHECK(bias_4d_view.dim()==4);\n-      TORCH_CHECK(bias_4d_view.size(0)==B);\n-      TORCH_CHECK(bias_4d_view.size(1)==nH);\n-      TORCH_CHECK(bias_4d_view.size(2)==M);\n-      TORCH_CHECK(bias_4d_view.size(3)==N);\n-      ASSIGN_CHECK_OVERFLOW(p.bias_strideB, bias_4d_view.stride(0));\n-      ASSIGN_CHECK_OVERFLOW(p.bias_strideH, bias_4d_view.stride(1));\n-      ASSIGN_CHECK_OVERFLOW(p.bias_strideM, bias_4d_view.stride(2));\n-        // assign strides for gB, viewed as\n-        // (batch_sz, n_heads, n_queries, n_keys). might have different strides\n-        // than B, for example if bias tensor was created with\n-        // torch.tensor((B * nH, 1, nK)).expand((B * nH, nQ, nK)),\n-        // different values of Q will point to the same memory\n-        // locations, meaning bias.stride(1) == 0, while we'd want\n-        // grad_bias.stride(1) == nK\n-        // We have expanded the input prior to calling the forward kernel\n-        const at::Tensor& grad_bias_4d_view = grad_bias;\n-        TORCH_CHECK(grad_bias_4d_view.dim()==4);\n-        TORCH_CHECK(grad_bias_4d_view.size(0)==B);\n-        TORCH_CHECK(grad_bias_4d_view.size(1)==nH);\n-        TORCH_CHECK(grad_bias_4d_view.size(2)==M);\n-        TORCH_CHECK(grad_bias_4d_view.size(3)==N);\n-\n-        ASSIGN_CHECK_OVERFLOW(p.gB_strideB, grad_bias_4d_view.stride(0));\n-        ASSIGN_CHECK_OVERFLOW(p.gB_strideH, grad_bias_4d_view.stride(1));\n-        ASSIGN_CHECK_OVERFLOW(p.gB_strideM, grad_bias_4d_view.stride(2));\n",
            "whole_added": "+      TORCH_CHECK(bias->dim() == 4, \"Bias expected in BMHK format\");\n+      TORCH_CHECK(\n+          bias->size(0) == query.size(0),\n+          \"attn_bias: wrong shape (batch dimension)\");\n+      TORCH_CHECK(\n+          bias->size(1) == query.size(2),\n+          \"attn_bias: wrong shape (head dimension)\");\n+      TORCH_CHECK(\n+          bias->size(2) == query.size(1),\n+          \"attn_bias: wrong shape (seqlenQ dimension)\");\n+      TORCH_CHECK(\n+          bias->size(3) == key.size(1),\n+          \"attn_bias: wrong shape (seqlenKV dimension)\");\n+      TORCH_CHECK(\n+          bias->stride(3) == 1,\n+          \"attn_bias: wrong alignment (last dimension must be contiguous)\");\n+      ASSIGN_CHECK_OVERFLOW(p.bias_strideB, bias->stride(0));\n+      ASSIGN_CHECK_OVERFLOW(p.bias_strideH, bias->stride(1));\n+      ASSIGN_CHECK_OVERFLOW(p.bias_strideM, bias->stride(2));\n+        ASSIGN_CHECK_OVERFLOW(p.gB_strideB, grad_bias.stride(0));\n+        ASSIGN_CHECK_OVERFLOW(p.gB_strideH, grad_bias.stride(1));\n+        ASSIGN_CHECK_OVERFLOW(p.gB_strideM, grad_bias.stride(2));\n",
            "whole_hunk": "@@ -360,40 +360,32 @@ _efficient_attention_backward(\n \n       p.bias_ptr = (scalar_t*)bias->data_ptr();\n \n-      // assign strides for bias, viewed as:\n-      // (batch_sz, n_heads, n_queries, n_keys)\n-      // We make sure to expand prior to calling the kernel\n-      const at::Tensor& bias_4d_view = *bias;\n-      TORCH_CHECK(bias_4d_view.dim()==4);\n-      TORCH_CHECK(bias_4d_view.size(0)==B);\n-      TORCH_CHECK(bias_4d_view.size(1)==nH);\n-      TORCH_CHECK(bias_4d_view.size(2)==M);\n-      TORCH_CHECK(bias_4d_view.size(3)==N);\n-      ASSIGN_CHECK_OVERFLOW(p.bias_strideB, bias_4d_view.stride(0));\n-      ASSIGN_CHECK_OVERFLOW(p.bias_strideH, bias_4d_view.stride(1));\n-      ASSIGN_CHECK_OVERFLOW(p.bias_strideM, bias_4d_view.stride(2));\n+      TORCH_CHECK(bias->dim() == 4, \"Bias expected in BMHK format\");\n+      TORCH_CHECK(\n+          bias->size(0) == query.size(0),\n+          \"attn_bias: wrong shape (batch dimension)\");\n+      TORCH_CHECK(\n+          bias->size(1) == query.size(2),\n+          \"attn_bias: wrong shape (head dimension)\");\n+      TORCH_CHECK(\n+          bias->size(2) == query.size(1),\n+          \"attn_bias: wrong shape (seqlenQ dimension)\");\n+      TORCH_CHECK(\n+          bias->size(3) == key.size(1),\n+          \"attn_bias: wrong shape (seqlenKV dimension)\");\n+      TORCH_CHECK(\n+          bias->stride(3) == 1,\n+          \"attn_bias: wrong alignment (last dimension must be contiguous)\");\n+      ASSIGN_CHECK_OVERFLOW(p.bias_strideB, bias->stride(0));\n+      ASSIGN_CHECK_OVERFLOW(p.bias_strideH, bias->stride(1));\n+      ASSIGN_CHECK_OVERFLOW(p.bias_strideM, bias->stride(2));\n \n       if (bias_requires_grad) {\n         p.grad_bias_ptr = (scalar_t*)grad_bias.data_ptr();\n \n-        // assign strides for gB, viewed as\n-        // (batch_sz, n_heads, n_queries, n_keys). might have different strides\n-        // than B, for example if bias tensor was created with\n-        // torch.tensor((B * nH, 1, nK)).expand((B * nH, nQ, nK)),\n-        // different values of Q will point to the same memory\n-        // locations, meaning bias.stride(1) == 0, while we'd want\n-        // grad_bias.stride(1) == nK\n-        // We have expanded the input prior to calling the forward kernel\n-        const at::Tensor& grad_bias_4d_view = grad_bias;\n-        TORCH_CHECK(grad_bias_4d_view.dim()==4);\n-        TORCH_CHECK(grad_bias_4d_view.size(0)==B);\n-        TORCH_CHECK(grad_bias_4d_view.size(1)==nH);\n-        TORCH_CHECK(grad_bias_4d_view.size(2)==M);\n-        TORCH_CHECK(grad_bias_4d_view.size(3)==N);\n-\n-        ASSIGN_CHECK_OVERFLOW(p.gB_strideB, grad_bias_4d_view.stride(0));\n-        ASSIGN_CHECK_OVERFLOW(p.gB_strideH, grad_bias_4d_view.stride(1));\n-        ASSIGN_CHECK_OVERFLOW(p.gB_strideM, grad_bias_4d_view.stride(2));\n+        ASSIGN_CHECK_OVERFLOW(p.gB_strideB, grad_bias.stride(0));\n+        ASSIGN_CHECK_OVERFLOW(p.gB_strideH, grad_bias.stride(1));\n+        ASSIGN_CHECK_OVERFLOW(p.gB_strideM, grad_bias.stride(2));\n       }\n     }\n \n"
        },
        {
            "name": "kernel_backward.h",
            "path": "aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernel_backward.h",
            "patches": [
                {
                    "old_start": 1212,
                    "old_length": 7,
                    "new_start": 1212,
                    "new_length": 7,
                    "hunk": "@@ -1212,7 +1212,7 @@ struct AttentionBackwardKernel {\n           p.num_heads <= 1 || p.bias_strideH % kMinimumAlignment == 0,\n           \"attn_bias is not correctly aligned (strideH)\");\n       TORCH_CHECK(\n-          p.bias_strideM % kMinimumAlignment == 0,\n+          p.num_queries <= 1 || p.bias_strideM % kMinimumAlignment == 0,\n           \"attn_bias is not correctly aligned (strideM)\");\n     }\n     if (p.grad_bias_ptr) {\n"
                }
            ],
            "whole_deleted": "-          p.bias_strideM % kMinimumAlignment == 0,\n",
            "whole_added": "+          p.num_queries <= 1 || p.bias_strideM % kMinimumAlignment == 0,\n",
            "whole_hunk": "@@ -1212,7 +1212,7 @@ struct AttentionBackwardKernel {\n           p.num_heads <= 1 || p.bias_strideH % kMinimumAlignment == 0,\n           \"attn_bias is not correctly aligned (strideH)\");\n       TORCH_CHECK(\n-          p.bias_strideM % kMinimumAlignment == 0,\n+          p.num_queries <= 1 || p.bias_strideM % kMinimumAlignment == 0,\n           \"attn_bias is not correctly aligned (strideM)\");\n     }\n     if (p.grad_bias_ptr) {\n"
        },
        {
            "name": "kernel_forward.h",
            "path": "aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernel_forward.h",
            "patches": [
                {
                    "old_start": 581,
                    "old_length": 8,
                    "new_start": 581,
                    "new_length": 8,
                    "hunk": "@@ -581,8 +581,8 @@ struct AttentionKernel {\n           p.num_heads <= 1 || p.bias_strideH % kAlignmentQ == 0,\n           \"attn_bias is not correctly aligned (strideH)\");\n       TORCH_CHECK(\n-          p.bias_strideM % kAlignmentQ == 0,\n-          \"attn_bias is not correctly aligned\");\n+          p.num_queries <= 1 || p.bias_strideM % kAlignmentQ == 0,\n+          \"attn_bias is not correctly aligned (strideM)\");\n     }\n     TORCH_CHECK(\n         p.q_strideM % kAlignmentQ == 0,"
                }
            ],
            "whole_deleted": "-          p.bias_strideM % kAlignmentQ == 0,\n-          \"attn_bias is not correctly aligned\");\n",
            "whole_added": "+          p.num_queries <= 1 || p.bias_strideM % kAlignmentQ == 0,\n+          \"attn_bias is not correctly aligned (strideM)\");\n",
            "whole_hunk": "@@ -581,8 +581,8 @@ struct AttentionKernel {\n           p.num_heads <= 1 || p.bias_strideH % kAlignmentQ == 0,\n           \"attn_bias is not correctly aligned (strideH)\");\n       TORCH_CHECK(\n-          p.bias_strideM % kAlignmentQ == 0,\n-          \"attn_bias is not correctly aligned\");\n+          p.num_queries <= 1 || p.bias_strideM % kAlignmentQ == 0,\n+          \"attn_bias is not correctly aligned (strideM)\");\n     }\n     TORCH_CHECK(\n         p.q_strideM % kAlignmentQ == 0,"
        }
    ]
},
{
    "Id": 460,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/92cc52ab0e48a27d77becd37f1683fd442992120",
    "date": "2023-10-30T16:43:20+00:00",
    "message": "[CPU SDP] Remove mem efficient attn checks in CPU (#112375)\n\nIt doesn't seem like memory efficient attention can be used on CPU, as we don't check for it when iterating backends in `select_sdp_backend_cpp`. So removing some of the logic around mem efficient attention selection.\n\nCreated from CodeHub with https://fburl.com/edit-in-codehub\n\nDifferential Revision: [D50775562](https://our.internmc.facebook.com/intern/diff/D50775562/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D50775562/)!\nPull Request resolved: https://github.com/pytorch/pytorch/pull/112375\nApproved by: https://github.com/drisspg",
    "label": "NO",
    "changes": [
        {
            "name": "sdp_utils_cpp.cpp",
            "path": "aten/src/ATen/native/transformers/sdp_utils_cpp.cpp",
            "patches": [
                {
                    "old_start": 61,
                    "old_length": 11,
                    "new_start": 61,
                    "new_length": 9,
                    "hunk": "@@ -61,11 +61,9 @@ bool use_flash_attention_cpp(sdp_params const& params, bool debug) {\n SDPBackend select_sdp_backend_cpp(sdp_params const& kernel_params) {\n   // This function defines the priority order of the different sdp backends\n   // 1. Flash Attention\n-  // 2. Mem Efficient Attention\n-  // 3. Math fallback\n+  // 2. Math fallback\n   auto& ctx = at::globalContext();\n-  if (!ctx.userEnabledMathSDP() && !ctx.userEnabledFlashSDP() &&\n-      !ctx.userEnabledMemEfficientSDP()) {\n+  if (!ctx.userEnabledMathSDP() && !ctx.userEnabledFlashSDP()) {\n     return SDPBackend::error;\n   }\n   // Get ideal kernel ordering\n"
                },
                {
                    "old_start": 91,
                    "old_length": 7,
                    "new_start": 89,
                    "new_length": 7,
                    "hunk": "@@ -91,7 +89,7 @@ SDPBackend select_sdp_backend_cpp(sdp_params const& kernel_params) {\n     }\n   }\n   // If we have gotten to this point then two things have happened:\n-  // 1. use_flash_attention or use_mem_efficient did not satisfy the\n+  // 1. use_flash_attention did not satisfy the\n   // constraints to be ran\n   // 2. The user has explicitly disabled the math kernel\n   // We then re-run the kernel checks with debug enabled to print out the"
                }
            ],
            "whole_deleted": "-  // 2. Mem Efficient Attention\n-  // 3. Math fallback\n-  if (!ctx.userEnabledMathSDP() && !ctx.userEnabledFlashSDP() &&\n-      !ctx.userEnabledMemEfficientSDP()) {\n-  // 1. use_flash_attention or use_mem_efficient did not satisfy the\n",
            "whole_added": "+  // 2. Math fallback\n+  if (!ctx.userEnabledMathSDP() && !ctx.userEnabledFlashSDP()) {\n+  // 1. use_flash_attention did not satisfy the\n",
            "whole_hunk": "@@ -61,11 +61,9 @@ bool use_flash_attention_cpp(sdp_params const& params, bool debug) {\n SDPBackend select_sdp_backend_cpp(sdp_params const& kernel_params) {\n   // This function defines the priority order of the different sdp backends\n   // 1. Flash Attention\n-  // 2. Mem Efficient Attention\n-  // 3. Math fallback\n+  // 2. Math fallback\n   auto& ctx = at::globalContext();\n-  if (!ctx.userEnabledMathSDP() && !ctx.userEnabledFlashSDP() &&\n-      !ctx.userEnabledMemEfficientSDP()) {\n+  if (!ctx.userEnabledMathSDP() && !ctx.userEnabledFlashSDP()) {\n     return SDPBackend::error;\n   }\n   // Get ideal kernel ordering\n@@ -91,7 +89,7 @@ SDPBackend select_sdp_backend_cpp(sdp_params const& kernel_params) {\n     }\n   }\n   // If we have gotten to this point then two things have happened:\n-  // 1. use_flash_attention or use_mem_efficient did not satisfy the\n+  // 1. use_flash_attention did not satisfy the\n   // constraints to be ran\n   // 2. The user has explicitly disabled the math kernel\n   // We then re-run the kernel checks with debug enabled to print out the"
        }
    ]
},
{
    "Id": 33,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/9912209743b7adbcbb82050829a47c00db2881c1",
    "date": "2024-07-10T01:18:55+00:00",
    "message": "check if the input fx graph of aot_compile return tuple (#129824)\n\nFixes https://github.com/pytorch/pytorch/issues/129719\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/129824\nApproved by: https://github.com/angelayi, https://github.com/yushangdi",
    "label": "YES",
    "changes": [
        {
            "name": "test_aot_inductor.py",
            "path": "test/inductor/test_aot_inductor.py",
            "patches": [
                {
                    "old_start": 1659,
                    "old_length": 6,
                    "new_start": 1659,
                    "new_length": 26,
                    "hunk": "@@ -1659,6 +1659,26 @@ class AOTInductorTestsTemplate:\n                 gm, tuple(i.to(self.device) for i in example_inputs)\n             )\n \n+    def test_fx_gm_return_tuple_validation(self):\n+        from torch.fx.experimental.proxy_tensor import make_fx\n+\n+        class Model(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+\n+            def forward(self, x, y):\n+                return x + y\n+\n+        example_inputs = (torch.randn(10, 10), torch.randn(10, 10))\n+\n+        gm = make_fx(Model(), tracing_mode=\"symbolic\")(*example_inputs)\n+        with self.assertRaisesRegex(\n+            AssertionError,\n+            r\"Graph output must be a tuple\\(\\). This is so that we can avoid \"\n+            \"pytree processing of the outputs.\",\n+        ):\n+            torch._inductor.aot_compile(gm, example_inputs)\n+\n     @unittest.mock.patch(\"torch._inductor.graph.supported_dtype_of_cpp_wrapper\")\n     def test_unsupported_input_dtype(self, supported_dtype_of_cpp_wrapper_mock):\n         supported_dtype_of_cpp_wrapper_mock.return_value = False\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    def test_fx_gm_return_tuple_validation(self):\n+        from torch.fx.experimental.proxy_tensor import make_fx\n+\n+        class Model(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+\n+            def forward(self, x, y):\n+                return x + y\n+\n+        example_inputs = (torch.randn(10, 10), torch.randn(10, 10))\n+\n+        gm = make_fx(Model(), tracing_mode=\"symbolic\")(*example_inputs)\n+        with self.assertRaisesRegex(\n+            AssertionError,\n+            r\"Graph output must be a tuple\\(\\). This is so that we can avoid \"\n+            \"pytree processing of the outputs.\",\n+        ):\n+            torch._inductor.aot_compile(gm, example_inputs)\n+\n",
            "whole_hunk": "@@ -1659,6 +1659,26 @@ class AOTInductorTestsTemplate:\n                 gm, tuple(i.to(self.device) for i in example_inputs)\n             )\n \n+    def test_fx_gm_return_tuple_validation(self):\n+        from torch.fx.experimental.proxy_tensor import make_fx\n+\n+        class Model(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+\n+            def forward(self, x, y):\n+                return x + y\n+\n+        example_inputs = (torch.randn(10, 10), torch.randn(10, 10))\n+\n+        gm = make_fx(Model(), tracing_mode=\"symbolic\")(*example_inputs)\n+        with self.assertRaisesRegex(\n+            AssertionError,\n+            r\"Graph output must be a tuple\\(\\). This is so that we can avoid \"\n+            \"pytree processing of the outputs.\",\n+        ):\n+            torch._inductor.aot_compile(gm, example_inputs)\n+\n     @unittest.mock.patch(\"torch._inductor.graph.supported_dtype_of_cpp_wrapper\")\n     def test_unsupported_input_dtype(self, supported_dtype_of_cpp_wrapper_mock):\n         supported_dtype_of_cpp_wrapper_mock.return_value = False\n"
        },
        {
            "name": "__init__.py",
            "path": "torch/_inductor/__init__.py",
            "patches": [
                {
                    "old_start": 48,
                    "old_length": 7,
                    "new_start": 48,
                    "new_length": 13,
                    "hunk": "@@ -48,7 +48,13 @@ def aot_compile(\n     Returns:\n         Path to the generated shared library\n     \"\"\"\n-    from .compile_fx import compile_fx_aot\n+    from .compile_fx import compile_fx_aot, graph_returns_tuple\n+\n+    assert graph_returns_tuple(gm), (\n+        \"Graph output must be a tuple(). This is so that we can avoid \"\n+        \"pytree processing of the outputs. Please change the module to \"\n+        \"have tuple outputs.\"\n+    )\n \n     # We will serialize the pytree info into the .so as constant strings\n     in_spec = None"
                }
            ],
            "whole_deleted": "-    from .compile_fx import compile_fx_aot\n",
            "whole_added": "+    from .compile_fx import compile_fx_aot, graph_returns_tuple\n+\n+    assert graph_returns_tuple(gm), (\n+        \"Graph output must be a tuple(). This is so that we can avoid \"\n+        \"pytree processing of the outputs. Please change the module to \"\n+        \"have tuple outputs.\"\n+    )\n",
            "whole_hunk": "@@ -48,7 +48,13 @@ def aot_compile(\n     Returns:\n         Path to the generated shared library\n     \"\"\"\n-    from .compile_fx import compile_fx_aot\n+    from .compile_fx import compile_fx_aot, graph_returns_tuple\n+\n+    assert graph_returns_tuple(gm), (\n+        \"Graph output must be a tuple(). This is so that we can avoid \"\n+        \"pytree processing of the outputs. Please change the module to \"\n+        \"have tuple outputs.\"\n+    )\n \n     # We will serialize the pytree info into the .so as constant strings\n     in_spec = None"
        }
    ]
},
{
    "Id": 432,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/7ccca60927cdccde63d6a1d40480950f24e9877a",
    "date": "2023-11-10T18:54:19+00:00",
    "message": "[BE] Remove stale CUDA version check from cpp_extension.py (#113447)\n\nAs at least CUDA-11.x is needed to build PyTorch on latest trunk\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113447\nApproved by: https://github.com/Skylion007, https://github.com/atalman, https://github.com/PaliC, https://github.com/huydhn",
    "label": "NO",
    "changes": [
        {
            "name": "cpp_extension.py",
            "path": "torch/utils/cpp_extension.py",
            "patches": [
                {
                    "old_start": 2344,
                    "old_length": 17,
                    "new_start": 2344,
                    "new_length": 12,
                    "hunk": "@@ -2344,17 +2344,12 @@ def _write_ninja_file(path,\n     if with_cuda:\n         cuda_compile_rule = ['rule cuda_compile']\n         nvcc_gendeps = ''\n-        # --generate-dependencies-with-compile was added in CUDA 10.2.\n-        # Compilation will work on earlier CUDA versions but header file\n-        # dependencies are not correctly computed.\n-        required_cuda_version = '11.0'\n-        if torch.version.cuda is not None and TorchVersion(torch.version.cuda) >= required_cuda_version:\n-            cuda_compile_rule.append('  depfile = $out.d')\n-            cuda_compile_rule.append('  deps = gcc')\n-            # Note: non-system deps with nvcc are only supported\n-            # on Linux so use --generate-dependencies-with-compile\n-            # to make this work on Windows too.\n-            nvcc_gendeps = '--generate-dependencies-with-compile --dependency-output $out.d'\n+        cuda_compile_rule.append('  depfile = $out.d')\n+        cuda_compile_rule.append('  deps = gcc')\n+        # Note: non-system deps with nvcc are only supported\n+        # on Linux so use --generate-dependencies-with-compile\n+        # to make this work on Windows too.\n+        nvcc_gendeps = '--generate-dependencies-with-compile --dependency-output $out.d'\n         cuda_compile_rule.append(\n             f'  command = $nvcc {nvcc_gendeps} $cuda_cflags -c $in -o $out $cuda_post_cflags')\n "
                }
            ],
            "whole_deleted": "-        # --generate-dependencies-with-compile was added in CUDA 10.2.\n-        # Compilation will work on earlier CUDA versions but header file\n-        # dependencies are not correctly computed.\n-        required_cuda_version = '11.0'\n-        if torch.version.cuda is not None and TorchVersion(torch.version.cuda) >= required_cuda_version:\n-            cuda_compile_rule.append('  depfile = $out.d')\n-            cuda_compile_rule.append('  deps = gcc')\n-            # Note: non-system deps with nvcc are only supported\n-            # on Linux so use --generate-dependencies-with-compile\n-            # to make this work on Windows too.\n-            nvcc_gendeps = '--generate-dependencies-with-compile --dependency-output $out.d'\n",
            "whole_added": "+        cuda_compile_rule.append('  depfile = $out.d')\n+        cuda_compile_rule.append('  deps = gcc')\n+        # Note: non-system deps with nvcc are only supported\n+        # on Linux so use --generate-dependencies-with-compile\n+        # to make this work on Windows too.\n+        nvcc_gendeps = '--generate-dependencies-with-compile --dependency-output $out.d'\n",
            "whole_hunk": "@@ -2344,17 +2344,12 @@ def _write_ninja_file(path,\n     if with_cuda:\n         cuda_compile_rule = ['rule cuda_compile']\n         nvcc_gendeps = ''\n-        # --generate-dependencies-with-compile was added in CUDA 10.2.\n-        # Compilation will work on earlier CUDA versions but header file\n-        # dependencies are not correctly computed.\n-        required_cuda_version = '11.0'\n-        if torch.version.cuda is not None and TorchVersion(torch.version.cuda) >= required_cuda_version:\n-            cuda_compile_rule.append('  depfile = $out.d')\n-            cuda_compile_rule.append('  deps = gcc')\n-            # Note: non-system deps with nvcc are only supported\n-            # on Linux so use --generate-dependencies-with-compile\n-            # to make this work on Windows too.\n-            nvcc_gendeps = '--generate-dependencies-with-compile --dependency-output $out.d'\n+        cuda_compile_rule.append('  depfile = $out.d')\n+        cuda_compile_rule.append('  deps = gcc')\n+        # Note: non-system deps with nvcc are only supported\n+        # on Linux so use --generate-dependencies-with-compile\n+        # to make this work on Windows too.\n+        nvcc_gendeps = '--generate-dependencies-with-compile --dependency-output $out.d'\n         cuda_compile_rule.append(\n             f'  command = $nvcc {nvcc_gendeps} $cuda_cflags -c $in -o $out $cuda_post_cflags')\n "
        }
    ]
},
{
    "Id": 284,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/516f38a144bf60f44c81342fe2327acce3829ece",
    "date": "2024-02-15T02:08:57+00:00",
    "message": "[RelEng] Define `BUILD_BUNDLE_PTXAS` (#119750)\n\nThat would bundle PTXAS into a `bin` folder\n\nWhen compiling for Triton, define `TRITION_PTXAS_PATH` if `ptxas` is bundled with PyTorch Needed to make PyTorch compiled against CUDA-11.8 usable with 11.8 driver, as Triton is bundled with latest (CUDA-12.3 at time of PyTorch-2.2 release) ptxas\n\nNeeds https://github.com/pytorch/builder/commit/5c814e2527b3f5797488bf57d9d5425e63dcc1ac to produce valid binary builds\n\nTest plan:\n- Create dummy ptxas in `torch/bin` folder and observe `torch.compile` fail with backtrace in Triton module.\n- Run following script (to be added to binary tests ) against CUDA-11.8 wheel:\n```python\nimport torch\nimport triton\n\n@torch.compile\ndef foo(x: torch.Tensor) -> torch.Tensor:\n  return torch.sin(x) + torch.cos(x)\n\nx=torch.rand(3, 3, device=\"cuda\")\nprint(foo(x))\n# And check that CUDA versions match\ncuda_version = torch.version.cuda\nptxas_version = triton.backends.nvidia.compiler.get_ptxas_version().decode(\"ascii\")\nassert cuda_version in ptxas_version, f\"CUDA version mismatch: torch build with {cuda_version}, but Triton uses ptxs {ptxas_version}\"\n```\n\nFixes https://github.com/pytorch/pytorch/issues/119054\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/119750\nApproved by: https://github.com/jansel, https://github.com/atalman",
    "label": "YES",
    "changes": [
        {
            "name": "CMakeLists.txt",
            "path": "CMakeLists.txt",
            "patches": [
                {
                    "old_start": 351,
                    "old_length": 6,
                    "new_start": 351,
                    "new_length": 8,
                    "hunk": "@@ -351,6 +351,8 @@ cmake_dependent_option(\n     \"NOT INTERN_BUILD_MOBILE\" OFF)\n cmake_dependent_option(\n     BUILD_FUNCTORCH \"Build Functorch\" ON \"BUILD_PYTHON\" OFF)\n+cmake_dependent_option(\n+    BUILD_BUNDLE_PTXAS \"Bundle PTX into torch/bin fodler\" OFF \"USE_CUDA\" OFF)\n \n option(USE_MIMALLOC \"Use mimalloc\" OFF)\n # Enable third party mimalloc library to improve memory allocation performance on Windows.\n"
                },
                {
                    "old_start": 1241,
                    "old_length": 3,
                    "new_start": 1243,
                    "new_length": 12,
                    "hunk": "@@ -1241,3 +1243,12 @@ if(DEFINED USE_CUSTOM_DEBINFO)\n     set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -g\")\n     set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -g\")\n endif()\n+\n+# Bundle PTXAS if needed\n+if(BUILD_BUNDLE_PTXAS AND USE_CUDA)\n+   if(NOT EXISTS \"${PROJECT_SOURCE_DIR}/build/bin/ptxas\")\n+     message(STATUS \"Copying PTXAS into the bin folder\")\n+     file(COPY \"${CUDAToolkit_BIN_DIR}/ptxas\" DESTINATION \"${PROJECT_BINARY_DIR}\")\n+   endif()\n+   install(PROGRAMS \"${PROJECT_BINARY_DIR}/ptxas\" DESTINATION \"${CMAKE_INSTALL_BINDIR}\")\n+endif()\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+cmake_dependent_option(\n+    BUILD_BUNDLE_PTXAS \"Bundle PTX into torch/bin fodler\" OFF \"USE_CUDA\" OFF)\n+\n+# Bundle PTXAS if needed\n+if(BUILD_BUNDLE_PTXAS AND USE_CUDA)\n+   if(NOT EXISTS \"${PROJECT_SOURCE_DIR}/build/bin/ptxas\")\n+     message(STATUS \"Copying PTXAS into the bin folder\")\n+     file(COPY \"${CUDAToolkit_BIN_DIR}/ptxas\" DESTINATION \"${PROJECT_BINARY_DIR}\")\n+   endif()\n+   install(PROGRAMS \"${PROJECT_BINARY_DIR}/ptxas\" DESTINATION \"${CMAKE_INSTALL_BINDIR}\")\n+endif()\n",
            "whole_hunk": "@@ -351,6 +351,8 @@ cmake_dependent_option(\n     \"NOT INTERN_BUILD_MOBILE\" OFF)\n cmake_dependent_option(\n     BUILD_FUNCTORCH \"Build Functorch\" ON \"BUILD_PYTHON\" OFF)\n+cmake_dependent_option(\n+    BUILD_BUNDLE_PTXAS \"Bundle PTX into torch/bin fodler\" OFF \"USE_CUDA\" OFF)\n \n option(USE_MIMALLOC \"Use mimalloc\" OFF)\n # Enable third party mimalloc library to improve memory allocation performance on Windows.\n@@ -1241,3 +1243,12 @@ if(DEFINED USE_CUSTOM_DEBINFO)\n     set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -g\")\n     set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -g\")\n endif()\n+\n+# Bundle PTXAS if needed\n+if(BUILD_BUNDLE_PTXAS AND USE_CUDA)\n+   if(NOT EXISTS \"${PROJECT_SOURCE_DIR}/build/bin/ptxas\")\n+     message(STATUS \"Copying PTXAS into the bin folder\")\n+     file(COPY \"${CUDAToolkit_BIN_DIR}/ptxas\" DESTINATION \"${PROJECT_BINARY_DIR}\")\n+   endif()\n+   install(PROGRAMS \"${PROJECT_BINARY_DIR}/ptxas\" DESTINATION \"${CMAKE_INSTALL_BINDIR}\")\n+endif()\n"
        },
        {
            "name": "codecache.py",
            "path": "torch/_inductor/codecache.py",
            "patches": [
                {
                    "old_start": 2391,
                    "old_length": 6,
                    "new_start": 2391,
                    "new_length": 20,
                    "hunk": "@@ -2391,6 +2391,20 @@ def caching_device_properties():\n             device_interface.Worker.get_device_properties()\n \n \n+def _set_triton_ptxas_path() -> None:\n+    if os.environ.get(\"TRITON_PTXAS_PATH\") is not None:\n+        return\n+    ptxas_path = os.path.abspath(\n+        os.path.join(os.path.dirname(__file__), \"..\", \"bin\", \"ptxas\")\n+    )\n+    if not os.path.exists(ptxas_path):\n+        return\n+    if os.path.isfile(ptxas_path) and os.access(ptxas_path, os.X_OK):\n+        os.environ[\"TRITON_PTXAS_PATH\"] = ptxas_path\n+    else:\n+        warnings.warn(f\"{ptxas_path} exists but is not an executable\")\n+\n+\n def _worker_compile(\n     kernel_name: str, source_code: str, cc: int, device: torch.device\n ) -> None:\n"
                },
                {
                    "old_start": 2401,
                    "old_length": 6,
                    "new_start": 2415,
                    "new_length": 7,
                    "hunk": "@@ -2401,6 +2415,7 @@ def _worker_compile(\n \n \n def _load_kernel(kernel_name: str, source_code: str) -> ModuleType:\n+    _set_triton_ptxas_path()\n     kernel = TritonCodeCache.load(kernel_name, source_code)\n     kernel.precompile()\n     return kernel"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+def _set_triton_ptxas_path() -> None:\n+    if os.environ.get(\"TRITON_PTXAS_PATH\") is not None:\n+        return\n+    ptxas_path = os.path.abspath(\n+        os.path.join(os.path.dirname(__file__), \"..\", \"bin\", \"ptxas\")\n+    )\n+    if not os.path.exists(ptxas_path):\n+        return\n+    if os.path.isfile(ptxas_path) and os.access(ptxas_path, os.X_OK):\n+        os.environ[\"TRITON_PTXAS_PATH\"] = ptxas_path\n+    else:\n+        warnings.warn(f\"{ptxas_path} exists but is not an executable\")\n+\n+\n+    _set_triton_ptxas_path()\n",
            "whole_hunk": "@@ -2391,6 +2391,20 @@ def caching_device_properties():\n             device_interface.Worker.get_device_properties()\n \n \n+def _set_triton_ptxas_path() -> None:\n+    if os.environ.get(\"TRITON_PTXAS_PATH\") is not None:\n+        return\n+    ptxas_path = os.path.abspath(\n+        os.path.join(os.path.dirname(__file__), \"..\", \"bin\", \"ptxas\")\n+    )\n+    if not os.path.exists(ptxas_path):\n+        return\n+    if os.path.isfile(ptxas_path) and os.access(ptxas_path, os.X_OK):\n+        os.environ[\"TRITON_PTXAS_PATH\"] = ptxas_path\n+    else:\n+        warnings.warn(f\"{ptxas_path} exists but is not an executable\")\n+\n+\n def _worker_compile(\n     kernel_name: str, source_code: str, cc: int, device: torch.device\n ) -> None:\n@@ -2401,6 +2415,7 @@ def _worker_compile(\n \n \n def _load_kernel(kernel_name: str, source_code: str) -> ModuleType:\n+    _set_triton_ptxas_path()\n     kernel = TritonCodeCache.load(kernel_name, source_code)\n     kernel.precompile()\n     return kernel"
        }
    ]
},
{
    "Id": 0,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/745324e487c451998e4e91d074677ebe417ca1d1",
    "date": "2024-07-18T17:40:58+00:00",
    "message": "[export] turn on hybrid symints by default (#130775)\n\nSets `prefer_deferred_runtime_asserts_over_guards=True` for export, so any guards emitted from `SymNode.expect_true` (for example, guards that are implicitly required to be true for an op to succeed) won't lead to constraint violations. Instead these should appear in the graph as runtime asserts, or potentially as replacement expressions for placeholder shapes.\n\nFor example, this reshape op should emit s0 * s1 = s2, deferred as a runtime assert.\n```\nx = torch.randn(4, 8)  # [s0, s1]\ny = torch.randn(32)  # [s2]\nout = x.reshape(-1) + y\n# this emits Eq(s0 * s1, s2), and we represent y's shape as [s0*s1] in the graph.\n```\n\nHowever, other complex guards can still cause export to fail, for instance guards emitted from `SymNode.guard_bool/guard_size_oblivious` (e.g. explicit if-else conditions in user code or lower-level op implementations hit during tracing) can still raise constraint violations. These can be deferred with `allow_complex_guards_as_runtime_asserts=True`. We don't yet make this default, because while this makes export more likely to succeed, it results in non-trivial asserts being emitted that often represent specialization to a variant of the op, or checks related to 0/1 specialization.\n\nWe also remove forced specializations for export and kill the `_disable_forced_specializations` flag - now any guard we can't express with Dims/DerivedDims either are handled with Hybrid SymInts, or should be resolved with rewriting or deferring.\n\nFollow up:\nCurrently, `ShapeEnv._set_replacement()` is called for complex equality expressions (e.g. s2 -> s0*s1 in the example above), and the ExportedProgram stores `s0*s1` in the input placeholder. This isn't checked for validity when the program is run, so an option is to avoid replacement and/or runtime assert on equality.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130775\nApproved by: https://github.com/avikchaudhuri",
    "label": "NO",
    "changes": [
        {
            "name": "test_export.py",
            "path": "test/dynamo/test_export.py",
            "patches": [
                {
                    "old_start": 2928,
                    "old_length": 7,
                    "new_start": 2928,
                    "new_length": 7,
                    "hunk": "@@ -2928,7 +2928,7 @@ def forward(self, x):\n         dynamic_shapes = {\"x\": (dim0,)}\n         with self.assertRaisesRegex(\n             torch._dynamo.exc.UserError,\n-            \"must be specialized.*guards generated.*too complex\",\n+            r\"Constraints violated \\(dim0\\)\",\n         ):\n             torch.export.export(foo, (x,), dynamic_shapes=dynamic_shapes)\n \n"
                },
                {
                    "old_start": 2936,
                    "old_length": 7,
                    "new_start": 2936,
                    "new_length": 7,
                    "hunk": "@@ -2936,7 +2936,7 @@ def forward(self, x):\n \n         with self.assertRaisesRegex(\n             torch._dynamo.exc.UserError,\n-            \"Not all values.*satisfy the generated guard\",\n+            r\"Constraints violated \\(dim0\\)\",\n         ):\n             torch.export.export(qux, (x,), dynamic_shapes=dynamic_shapes)\n \n"
                }
            ],
            "whole_deleted": "-            \"must be specialized.*guards generated.*too complex\",\n-            \"Not all values.*satisfy the generated guard\",\n",
            "whole_added": "+            r\"Constraints violated \\(dim0\\)\",\n+            r\"Constraints violated \\(dim0\\)\",\n",
            "whole_hunk": "@@ -2928,7 +2928,7 @@ def forward(self, x):\n         dynamic_shapes = {\"x\": (dim0,)}\n         with self.assertRaisesRegex(\n             torch._dynamo.exc.UserError,\n-            \"must be specialized.*guards generated.*too complex\",\n+            r\"Constraints violated \\(dim0\\)\",\n         ):\n             torch.export.export(foo, (x,), dynamic_shapes=dynamic_shapes)\n \n@@ -2936,7 +2936,7 @@ def forward(self, x):\n \n         with self.assertRaisesRegex(\n             torch._dynamo.exc.UserError,\n-            \"Not all values.*satisfy the generated guard\",\n+            r\"Constraints violated \\(dim0\\)\",\n         ):\n             torch.export.export(qux, (x,), dynamic_shapes=dynamic_shapes)\n \n"
        },
        {
            "name": "test_export.py",
            "path": "test/export/test_export.py",
            "patches": [
                {
                    "old_start": 1905,
                    "old_length": 17,
                    "new_start": 1905,
                    "new_length": 19,
                    "hunk": "@@ -1905,17 +1905,19 @@ def forward(self, p_linear_weight, p_linear_bias, b_buffer, x):\n         dim0_x = torch.export.Dim(\"dim0_x\", min=3)\n         dim1_x = torch.export.Dim(\"dim1_x\", max=8000)\n         dynamic_shapes = {\"x\": (dim0_x, dim1_x)}\n+        em = torch.export._trace._export(\n+            m,\n+            (a,),\n+            dynamic_shapes=dynamic_shapes,\n+            allow_complex_guards_as_runtime_asserts=True,\n+        )\n+        em.module()(torch.randn(4, 3))\n         with self.assertRaisesRegex(\n-            torch._dynamo.exc.UserError,\n-            (\n-                \"Specializations unexpectedly required\"\n-                \".*\\n.*\\\\[0\\\\] must be specialized to 3.*guards.*too complex(.*\\n)*.*\"\n-                \"Suggested fixes:(.*\\n)*.*\"\n-                \"dim0_x = 3(.*\\n)*.*\"\n-                \"dim1_x = 2\\\\*_dim1_x\"\n-            ),\n+            RuntimeError,\n+            r\"Runtime assertion failed for expression Eq\\(Mod\\(s0\\*s1, s0 \\- 1\\), 0\\)\",\n         ):\n-            torch.export.export(m, (a,), dynamic_shapes=dynamic_shapes)\n+            em.module()(torch.randn(4, 5))\n+\n         dim0_x = None\n         dim1_x = 2 * torch.export.Dim(\"_dim1_x\", max=4000)\n         dynamic_shapes = {\"x\": (dim0_x, dim1_x)}\n"
                },
                {
                    "old_start": 5773,
                    "old_length": 8,
                    "new_start": 5775,
                    "new_length": 8,
                    "hunk": "@@ -5773,8 +5775,8 @@ def forward(self, x, y):\n         export(f, (inputs,), dynamic_shapes=dynamic_shapes)\n \n     def test_disable_forced_specializations_ok(self):\n-        # check that _disable_forced_specializations and allow_complex_guards_as_runtime_asserts flags\n-        # both behave correctly, avoiding forced specializations and deferring to runtime.\n+        # check that we don't force specialization, and defer to runtime asserts\n+        # with allow_complex_guards_as_runtime_asserts=True to successfully export\n         # case 1: modulo guards\n         from torch.export import dims\n \n"
                },
                {
                    "old_start": 5784,
                    "old_length": 25,
                    "new_start": 5786,
                    "new_length": 6,
                    "hunk": "@@ -5784,25 +5786,6 @@ def forward(self, x, y):\n \n         inputs = (torch.randn(10, 72),)\n         dx, dy = dims(\"dx\", \"dy\")\n-        with self.assertRaisesRegex(  # this will force specialize\n-            torch._dynamo.exc.UserError,\n-            r\".*Specializations unexpectedly required(.*\\n)*\"\n-            r\".*dx = .* must be specialized to 10 because the guards generated for it are too complex(.*\\n)*\"\n-            r\".*dy = .* must be specialized to 72 because the guards generated for it are too complex(.*\\n)*\",\n-        ):\n-            export(\n-                Mod4Reshape(),\n-                inputs,\n-                dynamic_shapes={\"x\": (dx, dy)},\n-            )\n-\n-        torch.export._trace._export(  # just check this successfully compiles\n-            Mod4Reshape(),\n-            inputs,\n-            dynamic_shapes={\"x\": (dx, dy)},\n-            strict=False,\n-            _disable_forced_specializations=True,\n-        )\n         ep = torch.export._trace._export(\n             Mod4Reshape(),\n             inputs,\n"
                },
                {
                    "old_start": 5834,
                    "old_length": 30,
                    "new_start": 5817,
                    "new_length": 13,
                    "hunk": "@@ -5834,30 +5817,13 @@ def forward(self, x, y):\n             \"y\": [Dim(f\"dy{i}\", min=2) for i in range(2)],\n             \"z\": [Dim(f\"dz{i}\", min=4) for i in range(1)],\n         }\n-        with self.assertRaisesRegex(  # this will force specialize\n-            torch._dynamo.exc.UserError,\n-            r\".*Specializations unexpectedly required(.*\\n)*\"\n-            r\".*dx0 = .* must be specialized to 6 because the guards generated for it are too complex(.*\\n)*\"\n-            r\".*dx1 = .* must be specialized to 8 because the guards generated for it are too complex(.*\\n)*\",\n-        ):\n-            export(\n-                FreeReshape(),\n-                inputs,\n-                dynamic_shapes=dynamic_shapes,\n-            )\n-        torch.export._trace._export(\n-            FreeReshape(),\n-            inputs,\n-            dynamic_shapes=dynamic_shapes,\n-            strict=False,\n-            _disable_forced_specializations=True,\n-        )\n         ep = torch.export._trace._export(\n             FreeReshape(),\n             inputs,\n             dynamic_shapes=dynamic_shapes,\n             allow_complex_guards_as_runtime_asserts=True,\n         )\n+        ep = export(FreeReshape(), inputs, dynamic_shapes=dynamic_shapes)\n         out1 = ep.module()(torch.randn(48, 1), torch.randn(4, 12), torch.randn(48))\n         self.assertEqual(out1.shape, torch.ones(48).shape)\n         out2 = ep.module()(torch.randn(5, 8), torch.randn(4, 10), torch.randn(40))\n"
                },
                {
                    "old_start": 5881,
                    "old_length": 28,
                    "new_start": 5847,
                    "new_length": 6,
                    "hunk": "@@ -5881,28 +5847,6 @@ def forward(self, x, y):\n             \"x\": (Dim(\"dx0\", min=2), Dim(\"dx1\", min=2), Dim(\"dx2\", min=2)),\n             \"y\": (Dim(\"dy\", min=8),),\n         }\n-        with self.assertRaisesRegex(  # this will force specialize\n-            torch._dynamo.exc.UserError,\n-            r\".*Specializations unexpectedly required(.*\\n)*\"\n-            r\"Suggested fixes:(.*\\n)*\"\n-            r\".*dx0 = 4(.*\\n)*\"\n-            r\".*dx1 = 3(.*\\n)*\"\n-            r\".*dx2 = 2(.*\\n)*\"\n-            r\".*dy = 24(.*\\n)*\",\n-        ):\n-            export(\n-                Reshape3d(),\n-                inputs,\n-                dynamic_shapes=dynamic_shapes,\n-            )\n-\n-        torch.export._trace._export(\n-            Reshape3d(),\n-            inputs,\n-            dynamic_shapes=dynamic_shapes,\n-            strict=False,\n-            _disable_forced_specializations=True,\n-        )\n         ep = torch.export._trace._export(\n             Reshape3d(),\n             inputs,\n"
                },
                {
                    "old_start": 5918,
                    "old_length": 7,
                    "new_start": 5862,
                    "new_length": 7,
                    "hunk": "@@ -5918,7 +5862,7 @@ def forward(self, x, y):\n             ep.module()(torch.randn(4, 3, 2), torch.randn(10))  # fail\n \n     def test_disable_forced_specializations_errors(self):\n-        # check error messages with disable_forced_specializations = False/True\n+        # check error messages with hybrid symints\n         class Foo(torch.nn.Module):\n             def forward(self, w, x, y, z):\n                 return w.reshape([-1]) + x, y + z  # simple: s0*s1 = s2, s3 = s4\n"
                },
                {
                    "old_start": 5935,
                    "old_length": 34,
                    "new_start": 5879,
                    "new_length": 17,
                    "hunk": "@@ -5935,34 +5879,17 @@ def forward(self, x, y):\n             \"y\": [Dim(\"dy\")],  # y & z incorrect, export is supposed to fail.\n             \"z\": [Dim(\"dz\")],  # suggested fix should be to match these up.\n         }\n-        with self.assertRaisesRegex(  # if allow = False, suggested fixes should specialize 3, 4, 12.\n-            torch._dynamo.exc.UserError,\n-            r\".*Specializations unexpectedly required(.*\\n)*\"\n-            r\"Suggested fixes:(.*\\n)*\"\n-            r\".*dw0 = 3(.*\\n)*\"\n-            r\".*dw1 = 4(.*\\n)*\"\n-            r\".*dx0 = 12(.*\\n)*\"\n-            r\".*dz = dy(.*\\n)*\",\n-        ):\n-            torch.export._trace._export(\n-                Foo(),\n-                inputs,\n-                dynamic_shapes=dynamic_shapes,\n-                strict=False,\n-                _disable_forced_specializations=False,\n-            )\n         with self.assertRaisesRegex(  # if disable=True, suggested fixes should not specialize.\n             torch._dynamo.exc.UserError,\n             r\".*Constraints violated(.*\\n)*\"\n             r\"Suggested fixes:(.*\\n)*\"\n             r\".*dz = dy(.*\\n)*\",\n         ) as msg:\n-            torch.export._trace._export(\n+            export(\n                 Foo(),\n                 inputs,\n                 dynamic_shapes=dynamic_shapes,\n                 strict=False,\n-                _disable_forced_specializations=True,\n             )\n \n     # TODO requires_grad doesn't seem to work with serialization.\n"
                },
                {
                    "old_start": 6276,
                    "old_length": 6,
                    "new_start": 6203,
                    "new_length": 39,
                    "hunk": "@@ -6276,6 +6203,39 @@ def forward(self, x, y):\n             ep.graph_module.code\n         )\n \n+    def test_slice_with_floordiv(self):\n+        # slice operation emits runtime assert s0//2 <= s1\n+        class M1(torch.nn.Module):\n+            def forward(self, x, y):\n+                d = x.size(0) // 2\n+                return y[d:]\n+\n+        class M(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.m1 = M1()\n+\n+            def forward(self, x, y):\n+                d = x.size(0) // 2\n+                m1_res = self.m1(x, y)\n+                return y[d:] + m1_res\n+\n+        inputs = (torch.ones(10), torch.ones(10))\n+        d0 = torch.export.Dim(\"d0\", max=2048)\n+        d1 = torch.export.Dim(\"d1\", max=2048)\n+        ep = export(\n+            M(),\n+            inputs,\n+            dynamic_shapes=((d0,), (d1,)),\n+        )\n+        ep.module()(torch.ones(8), torch.ones(4))\n+        ep.module()(torch.ones(8), torch.ones(5))\n+        with self.assertRaisesRegex(\n+            RuntimeError,\n+            r\"Runtime assertion failed for expression \\(s0//2\\) \\<\\= s1\",\n+        ):\n+            ep.module()(torch.ones(10), torch.ones(4))\n+\n \n @unittest.skipIf(not torchdynamo.is_dynamo_supported(), \"dynamo isn't support\")\n class TestOneOffModelExportResult(TestCase):\n"
                }
            ],
            "whole_deleted": "-            torch._dynamo.exc.UserError,\n-            (\n-                \"Specializations unexpectedly required\"\n-                \".*\\n.*\\\\[0\\\\] must be specialized to 3.*guards.*too complex(.*\\n)*.*\"\n-                \"Suggested fixes:(.*\\n)*.*\"\n-                \"dim0_x = 3(.*\\n)*.*\"\n-                \"dim1_x = 2\\\\*_dim1_x\"\n-            ),\n-            torch.export.export(m, (a,), dynamic_shapes=dynamic_shapes)\n-        # check that _disable_forced_specializations and allow_complex_guards_as_runtime_asserts flags\n-        # both behave correctly, avoiding forced specializations and deferring to runtime.\n-        with self.assertRaisesRegex(  # this will force specialize\n-            torch._dynamo.exc.UserError,\n-            r\".*Specializations unexpectedly required(.*\\n)*\"\n-            r\".*dx = .* must be specialized to 10 because the guards generated for it are too complex(.*\\n)*\"\n-            r\".*dy = .* must be specialized to 72 because the guards generated for it are too complex(.*\\n)*\",\n-        ):\n-            export(\n-                Mod4Reshape(),\n-                inputs,\n-                dynamic_shapes={\"x\": (dx, dy)},\n-            )\n-\n-        torch.export._trace._export(  # just check this successfully compiles\n-            Mod4Reshape(),\n-            inputs,\n-            dynamic_shapes={\"x\": (dx, dy)},\n-            strict=False,\n-            _disable_forced_specializations=True,\n-        )\n-        with self.assertRaisesRegex(  # this will force specialize\n-            torch._dynamo.exc.UserError,\n-            r\".*Specializations unexpectedly required(.*\\n)*\"\n-            r\".*dx0 = .* must be specialized to 6 because the guards generated for it are too complex(.*\\n)*\"\n-            r\".*dx1 = .* must be specialized to 8 because the guards generated for it are too complex(.*\\n)*\",\n-        ):\n-            export(\n-                FreeReshape(),\n-                inputs,\n-                dynamic_shapes=dynamic_shapes,\n-            )\n-        torch.export._trace._export(\n-            FreeReshape(),\n-            inputs,\n-            dynamic_shapes=dynamic_shapes,\n-            strict=False,\n-            _disable_forced_specializations=True,\n-        )\n-        with self.assertRaisesRegex(  # this will force specialize\n-            torch._dynamo.exc.UserError,\n-            r\".*Specializations unexpectedly required(.*\\n)*\"\n-            r\"Suggested fixes:(.*\\n)*\"\n-            r\".*dx0 = 4(.*\\n)*\"\n-            r\".*dx1 = 3(.*\\n)*\"\n-            r\".*dx2 = 2(.*\\n)*\"\n-            r\".*dy = 24(.*\\n)*\",\n-        ):\n-            export(\n-                Reshape3d(),\n-                inputs,\n-                dynamic_shapes=dynamic_shapes,\n-            )\n-\n-        torch.export._trace._export(\n-            Reshape3d(),\n-            inputs,\n-            dynamic_shapes=dynamic_shapes,\n-            strict=False,\n-            _disable_forced_specializations=True,\n-        )\n-        # check error messages with disable_forced_specializations = False/True\n-        with self.assertRaisesRegex(  # if allow = False, suggested fixes should specialize 3, 4, 12.\n-            torch._dynamo.exc.UserError,\n-            r\".*Specializations unexpectedly required(.*\\n)*\"\n-            r\"Suggested fixes:(.*\\n)*\"\n-            r\".*dw0 = 3(.*\\n)*\"\n-            r\".*dw1 = 4(.*\\n)*\"\n-            r\".*dx0 = 12(.*\\n)*\"\n-            r\".*dz = dy(.*\\n)*\",\n-        ):\n-            torch.export._trace._export(\n-                Foo(),\n-                inputs,\n-                dynamic_shapes=dynamic_shapes,\n-                strict=False,\n-                _disable_forced_specializations=False,\n-            )\n-            torch.export._trace._export(\n-                _disable_forced_specializations=True,\n",
            "whole_added": "+        em = torch.export._trace._export(\n+            m,\n+            (a,),\n+            dynamic_shapes=dynamic_shapes,\n+            allow_complex_guards_as_runtime_asserts=True,\n+        )\n+        em.module()(torch.randn(4, 3))\n+            RuntimeError,\n+            r\"Runtime assertion failed for expression Eq\\(Mod\\(s0\\*s1, s0 \\- 1\\), 0\\)\",\n+            em.module()(torch.randn(4, 5))\n+\n+        # check that we don't force specialization, and defer to runtime asserts\n+        # with allow_complex_guards_as_runtime_asserts=True to successfully export\n+        ep = export(FreeReshape(), inputs, dynamic_shapes=dynamic_shapes)\n+        # check error messages with hybrid symints\n+            export(\n+    def test_slice_with_floordiv(self):\n+        # slice operation emits runtime assert s0//2 <= s1\n+        class M1(torch.nn.Module):\n+            def forward(self, x, y):\n+                d = x.size(0) // 2\n+                return y[d:]\n+\n+        class M(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.m1 = M1()\n+\n+            def forward(self, x, y):\n+                d = x.size(0) // 2\n+                m1_res = self.m1(x, y)\n+                return y[d:] + m1_res\n+\n+        inputs = (torch.ones(10), torch.ones(10))\n+        d0 = torch.export.Dim(\"d0\", max=2048)\n+        d1 = torch.export.Dim(\"d1\", max=2048)\n+        ep = export(\n+            M(),\n+            inputs,\n+            dynamic_shapes=((d0,), (d1,)),\n+        )\n+        ep.module()(torch.ones(8), torch.ones(4))\n+        ep.module()(torch.ones(8), torch.ones(5))\n+        with self.assertRaisesRegex(\n+            RuntimeError,\n+            r\"Runtime assertion failed for expression \\(s0//2\\) \\<\\= s1\",\n+        ):\n+            ep.module()(torch.ones(10), torch.ones(4))\n+\n",
            "whole_hunk": "@@ -1905,17 +1905,19 @@ def forward(self, p_linear_weight, p_linear_bias, b_buffer, x):\n         dim0_x = torch.export.Dim(\"dim0_x\", min=3)\n         dim1_x = torch.export.Dim(\"dim1_x\", max=8000)\n         dynamic_shapes = {\"x\": (dim0_x, dim1_x)}\n+        em = torch.export._trace._export(\n+            m,\n+            (a,),\n+            dynamic_shapes=dynamic_shapes,\n+            allow_complex_guards_as_runtime_asserts=True,\n+        )\n+        em.module()(torch.randn(4, 3))\n         with self.assertRaisesRegex(\n-            torch._dynamo.exc.UserError,\n-            (\n-                \"Specializations unexpectedly required\"\n-                \".*\\n.*\\\\[0\\\\] must be specialized to 3.*guards.*too complex(.*\\n)*.*\"\n-                \"Suggested fixes:(.*\\n)*.*\"\n-                \"dim0_x = 3(.*\\n)*.*\"\n-                \"dim1_x = 2\\\\*_dim1_x\"\n-            ),\n+            RuntimeError,\n+            r\"Runtime assertion failed for expression Eq\\(Mod\\(s0\\*s1, s0 \\- 1\\), 0\\)\",\n         ):\n-            torch.export.export(m, (a,), dynamic_shapes=dynamic_shapes)\n+            em.module()(torch.randn(4, 5))\n+\n         dim0_x = None\n         dim1_x = 2 * torch.export.Dim(\"_dim1_x\", max=4000)\n         dynamic_shapes = {\"x\": (dim0_x, dim1_x)}\n@@ -5773,8 +5775,8 @@ def forward(self, x, y):\n         export(f, (inputs,), dynamic_shapes=dynamic_shapes)\n \n     def test_disable_forced_specializations_ok(self):\n-        # check that _disable_forced_specializations and allow_complex_guards_as_runtime_asserts flags\n-        # both behave correctly, avoiding forced specializations and deferring to runtime.\n+        # check that we don't force specialization, and defer to runtime asserts\n+        # with allow_complex_guards_as_runtime_asserts=True to successfully export\n         # case 1: modulo guards\n         from torch.export import dims\n \n@@ -5784,25 +5786,6 @@ def forward(self, x, y):\n \n         inputs = (torch.randn(10, 72),)\n         dx, dy = dims(\"dx\", \"dy\")\n-        with self.assertRaisesRegex(  # this will force specialize\n-            torch._dynamo.exc.UserError,\n-            r\".*Specializations unexpectedly required(.*\\n)*\"\n-            r\".*dx = .* must be specialized to 10 because the guards generated for it are too complex(.*\\n)*\"\n-            r\".*dy = .* must be specialized to 72 because the guards generated for it are too complex(.*\\n)*\",\n-        ):\n-            export(\n-                Mod4Reshape(),\n-                inputs,\n-                dynamic_shapes={\"x\": (dx, dy)},\n-            )\n-\n-        torch.export._trace._export(  # just check this successfully compiles\n-            Mod4Reshape(),\n-            inputs,\n-            dynamic_shapes={\"x\": (dx, dy)},\n-            strict=False,\n-            _disable_forced_specializations=True,\n-        )\n         ep = torch.export._trace._export(\n             Mod4Reshape(),\n             inputs,\n@@ -5834,30 +5817,13 @@ def forward(self, x, y):\n             \"y\": [Dim(f\"dy{i}\", min=2) for i in range(2)],\n             \"z\": [Dim(f\"dz{i}\", min=4) for i in range(1)],\n         }\n-        with self.assertRaisesRegex(  # this will force specialize\n-            torch._dynamo.exc.UserError,\n-            r\".*Specializations unexpectedly required(.*\\n)*\"\n-            r\".*dx0 = .* must be specialized to 6 because the guards generated for it are too complex(.*\\n)*\"\n-            r\".*dx1 = .* must be specialized to 8 because the guards generated for it are too complex(.*\\n)*\",\n-        ):\n-            export(\n-                FreeReshape(),\n-                inputs,\n-                dynamic_shapes=dynamic_shapes,\n-            )\n-        torch.export._trace._export(\n-            FreeReshape(),\n-            inputs,\n-            dynamic_shapes=dynamic_shapes,\n-            strict=False,\n-            _disable_forced_specializations=True,\n-        )\n         ep = torch.export._trace._export(\n             FreeReshape(),\n             inputs,\n             dynamic_shapes=dynamic_shapes,\n             allow_complex_guards_as_runtime_asserts=True,\n         )\n+        ep = export(FreeReshape(), inputs, dynamic_shapes=dynamic_shapes)\n         out1 = ep.module()(torch.randn(48, 1), torch.randn(4, 12), torch.randn(48))\n         self.assertEqual(out1.shape, torch.ones(48).shape)\n         out2 = ep.module()(torch.randn(5, 8), torch.randn(4, 10), torch.randn(40))\n@@ -5881,28 +5847,6 @@ def forward(self, x, y):\n             \"x\": (Dim(\"dx0\", min=2), Dim(\"dx1\", min=2), Dim(\"dx2\", min=2)),\n             \"y\": (Dim(\"dy\", min=8),),\n         }\n-        with self.assertRaisesRegex(  # this will force specialize\n-            torch._dynamo.exc.UserError,\n-            r\".*Specializations unexpectedly required(.*\\n)*\"\n-            r\"Suggested fixes:(.*\\n)*\"\n-            r\".*dx0 = 4(.*\\n)*\"\n-            r\".*dx1 = 3(.*\\n)*\"\n-            r\".*dx2 = 2(.*\\n)*\"\n-            r\".*dy = 24(.*\\n)*\",\n-        ):\n-            export(\n-                Reshape3d(),\n-                inputs,\n-                dynamic_shapes=dynamic_shapes,\n-            )\n-\n-        torch.export._trace._export(\n-            Reshape3d(),\n-            inputs,\n-            dynamic_shapes=dynamic_shapes,\n-            strict=False,\n-            _disable_forced_specializations=True,\n-        )\n         ep = torch.export._trace._export(\n             Reshape3d(),\n             inputs,\n@@ -5918,7 +5862,7 @@ def forward(self, x, y):\n             ep.module()(torch.randn(4, 3, 2), torch.randn(10))  # fail\n \n     def test_disable_forced_specializations_errors(self):\n-        # check error messages with disable_forced_specializations = False/True\n+        # check error messages with hybrid symints\n         class Foo(torch.nn.Module):\n             def forward(self, w, x, y, z):\n                 return w.reshape([-1]) + x, y + z  # simple: s0*s1 = s2, s3 = s4\n@@ -5935,34 +5879,17 @@ def forward(self, x, y):\n             \"y\": [Dim(\"dy\")],  # y & z incorrect, export is supposed to fail.\n             \"z\": [Dim(\"dz\")],  # suggested fix should be to match these up.\n         }\n-        with self.assertRaisesRegex(  # if allow = False, suggested fixes should specialize 3, 4, 12.\n-            torch._dynamo.exc.UserError,\n-            r\".*Specializations unexpectedly required(.*\\n)*\"\n-            r\"Suggested fixes:(.*\\n)*\"\n-            r\".*dw0 = 3(.*\\n)*\"\n-            r\".*dw1 = 4(.*\\n)*\"\n-            r\".*dx0 = 12(.*\\n)*\"\n-            r\".*dz = dy(.*\\n)*\",\n-        ):\n-            torch.export._trace._export(\n-                Foo(),\n-                inputs,\n-                dynamic_shapes=dynamic_shapes,\n-                strict=False,\n-                _disable_forced_specializations=False,\n-            )\n         with self.assertRaisesRegex(  # if disable=True, suggested fixes should not specialize.\n             torch._dynamo.exc.UserError,\n             r\".*Constraints violated(.*\\n)*\"\n             r\"Suggested fixes:(.*\\n)*\"\n             r\".*dz = dy(.*\\n)*\",\n         ) as msg:\n-            torch.export._trace._export(\n+            export(\n                 Foo(),\n                 inputs,\n                 dynamic_shapes=dynamic_shapes,\n                 strict=False,\n-                _disable_forced_specializations=True,\n             )\n \n     # TODO requires_grad doesn't seem to work with serialization.\n@@ -6276,6 +6203,39 @@ def forward(self, x, y):\n             ep.graph_module.code\n         )\n \n+    def test_slice_with_floordiv(self):\n+        # slice operation emits runtime assert s0//2 <= s1\n+        class M1(torch.nn.Module):\n+            def forward(self, x, y):\n+                d = x.size(0) // 2\n+                return y[d:]\n+\n+        class M(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.m1 = M1()\n+\n+            def forward(self, x, y):\n+                d = x.size(0) // 2\n+                m1_res = self.m1(x, y)\n+                return y[d:] + m1_res\n+\n+        inputs = (torch.ones(10), torch.ones(10))\n+        d0 = torch.export.Dim(\"d0\", max=2048)\n+        d1 = torch.export.Dim(\"d1\", max=2048)\n+        ep = export(\n+            M(),\n+            inputs,\n+            dynamic_shapes=((d0,), (d1,)),\n+        )\n+        ep.module()(torch.ones(8), torch.ones(4))\n+        ep.module()(torch.ones(8), torch.ones(5))\n+        with self.assertRaisesRegex(\n+            RuntimeError,\n+            r\"Runtime assertion failed for expression \\(s0//2\\) \\<\\= s1\",\n+        ):\n+            ep.module()(torch.ones(10), torch.ones(4))\n+\n \n @unittest.skipIf(not torchdynamo.is_dynamo_supported(), \"dynamo isn't support\")\n class TestOneOffModelExportResult(TestCase):\n"
        },
        {
            "name": "non_strict_utils.py",
            "path": "torch/_export/non_strict_utils.py",
            "patches": [
                {
                    "old_start": 2,
                    "old_length": 7,
                    "new_start": 2,
                    "new_length": 7,
                    "hunk": "@@ -2,7 +2,7 @@\n import contextlib\n import inspect\n from collections import defaultdict\n-from typing import Any, Callable, Dict, List, Optional, Tuple, TYPE_CHECKING, Union\n+from typing import Any, Callable, Dict, List, Tuple, TYPE_CHECKING, Union\n \n import torch\n import torch.utils._pytree as pytree\n"
                },
                {
                    "old_start": 166,
                    "old_length": 7,
                    "new_start": 166,
                    "new_length": 7,
                    "hunk": "@@ -166,7 +166,7 @@ def make_fake_inputs(\n             shape_env=ShapeEnv(\n                 tracked_fakes=[],\n                 co_fields=co_fields,\n-                prefer_deferred_runtime_asserts_over_guards=allow_complex_guards_as_runtime_asserts,\n+                prefer_deferred_runtime_asserts_over_guards=True,\n                 allow_complex_guards_as_runtime_asserts=allow_complex_guards_as_runtime_asserts,\n             ),\n             allow_non_fake_inputs=True,\n"
                },
                {
                    "old_start": 176,
                    "old_length": 7,
                    "new_start": 176,
                    "new_length": 7,
                    "hunk": "@@ -176,7 +176,7 @@ def make_fake_inputs(\n         fake_mode = FakeTensorMode(\n             shape_env=ShapeEnv(\n                 tracked_fakes=[],\n-                prefer_deferred_runtime_asserts_over_guards=allow_complex_guards_as_runtime_asserts,\n+                prefer_deferred_runtime_asserts_over_guards=True,\n                 allow_complex_guards_as_runtime_asserts=allow_complex_guards_as_runtime_asserts,\n             ),\n             allow_non_fake_inputs=True,\n"
                },
                {
                    "old_start": 242,
                    "old_length": 7,
                    "new_start": 242,
                    "new_length": 6,
                    "hunk": "@@ -242,7 +242,6 @@ def produce_guards_and_solve_constraints(\n     dynamic_shapes: Union[Dict[str, Any], Tuple[Any], List[Any], None],\n     equalities_inputs: EqualityConstraint,\n     original_signature: inspect.Signature,\n-    _disable_forced_specializations: Optional[bool] = False,\n     _is_torch_jit_trace=False,\n ):\n     \"\"\"\n"
                },
                {
                    "old_start": 254,
                    "old_length": 7,
                    "new_start": 253,
                    "new_length": 6,
                    "hunk": "@@ -254,7 +253,6 @@ def produce_guards_and_solve_constraints(\n     Additional inputs:\n         equalities_inputs: the equality constraints to use for guards\n         original_signature: the signature of the forward method\n-        _disable_forced_specializations: if True, avoids forced specializations\n     \"\"\"\n     shape_env = fake_mode.shape_env\n     assert shape_env is not None\n"
                },
                {
                    "old_start": 271,
                    "old_length": 7,
                    "new_start": 269,
                    "new_length": 6,
                    "hunk": "@@ -271,7 +269,6 @@ def produce_guards_and_solve_constraints(\n             input_contexts=input_contexts,\n             equalities_inputs=equalities_inputs,\n             ignore_static=False,\n-            _disable_forced_specializations=_disable_forced_specializations,\n         )\n     except ConstraintViolationError as e:\n         constraint_violation_error = e\n"
                },
                {
                    "old_start": 284,
                    "old_length": 9,
                    "new_start": 281,
                    "new_length": 7,
                    "hunk": "@@ -284,9 +281,7 @@ def produce_guards_and_solve_constraints(\n         # TODO(avik): Maybe record the constraint violation error instead and replay later?\n         assert constraint_violation_error\n         raise constraint_violation_error\n-    dim_constraints.solve(\n-        _disable_forced_specializations=_disable_forced_specializations\n-    )\n+    dim_constraints.solve()\n     dim_constraints.remove_redundant_dynamic_results()\n     forced_specializations = dim_constraints.forced_specializations()\n     if not _is_torch_jit_trace:\n"
                }
            ],
            "whole_deleted": "-from typing import Any, Callable, Dict, List, Optional, Tuple, TYPE_CHECKING, Union\n-                prefer_deferred_runtime_asserts_over_guards=allow_complex_guards_as_runtime_asserts,\n-                prefer_deferred_runtime_asserts_over_guards=allow_complex_guards_as_runtime_asserts,\n-    _disable_forced_specializations: Optional[bool] = False,\n-        _disable_forced_specializations: if True, avoids forced specializations\n-            _disable_forced_specializations=_disable_forced_specializations,\n-    dim_constraints.solve(\n-        _disable_forced_specializations=_disable_forced_specializations\n-    )\n",
            "whole_added": "+from typing import Any, Callable, Dict, List, Tuple, TYPE_CHECKING, Union\n+                prefer_deferred_runtime_asserts_over_guards=True,\n+                prefer_deferred_runtime_asserts_over_guards=True,\n+    dim_constraints.solve()\n",
            "whole_hunk": "@@ -2,7 +2,7 @@\n import contextlib\n import inspect\n from collections import defaultdict\n-from typing import Any, Callable, Dict, List, Optional, Tuple, TYPE_CHECKING, Union\n+from typing import Any, Callable, Dict, List, Tuple, TYPE_CHECKING, Union\n \n import torch\n import torch.utils._pytree as pytree\n@@ -166,7 +166,7 @@ def make_fake_inputs(\n             shape_env=ShapeEnv(\n                 tracked_fakes=[],\n                 co_fields=co_fields,\n-                prefer_deferred_runtime_asserts_over_guards=allow_complex_guards_as_runtime_asserts,\n+                prefer_deferred_runtime_asserts_over_guards=True,\n                 allow_complex_guards_as_runtime_asserts=allow_complex_guards_as_runtime_asserts,\n             ),\n             allow_non_fake_inputs=True,\n@@ -176,7 +176,7 @@ def make_fake_inputs(\n         fake_mode = FakeTensorMode(\n             shape_env=ShapeEnv(\n                 tracked_fakes=[],\n-                prefer_deferred_runtime_asserts_over_guards=allow_complex_guards_as_runtime_asserts,\n+                prefer_deferred_runtime_asserts_over_guards=True,\n                 allow_complex_guards_as_runtime_asserts=allow_complex_guards_as_runtime_asserts,\n             ),\n             allow_non_fake_inputs=True,\n@@ -242,7 +242,6 @@ def produce_guards_and_solve_constraints(\n     dynamic_shapes: Union[Dict[str, Any], Tuple[Any], List[Any], None],\n     equalities_inputs: EqualityConstraint,\n     original_signature: inspect.Signature,\n-    _disable_forced_specializations: Optional[bool] = False,\n     _is_torch_jit_trace=False,\n ):\n     \"\"\"\n@@ -254,7 +253,6 @@ def produce_guards_and_solve_constraints(\n     Additional inputs:\n         equalities_inputs: the equality constraints to use for guards\n         original_signature: the signature of the forward method\n-        _disable_forced_specializations: if True, avoids forced specializations\n     \"\"\"\n     shape_env = fake_mode.shape_env\n     assert shape_env is not None\n@@ -271,7 +269,6 @@ def produce_guards_and_solve_constraints(\n             input_contexts=input_contexts,\n             equalities_inputs=equalities_inputs,\n             ignore_static=False,\n-            _disable_forced_specializations=_disable_forced_specializations,\n         )\n     except ConstraintViolationError as e:\n         constraint_violation_error = e\n@@ -284,9 +281,7 @@ def produce_guards_and_solve_constraints(\n         # TODO(avik): Maybe record the constraint violation error instead and replay later?\n         assert constraint_violation_error\n         raise constraint_violation_error\n-    dim_constraints.solve(\n-        _disable_forced_specializations=_disable_forced_specializations\n-    )\n+    dim_constraints.solve()\n     dim_constraints.remove_redundant_dynamic_results()\n     forced_specializations = dim_constraints.forced_specializations()\n     if not _is_torch_jit_trace:\n"
        },
        {
            "name": "serialize.py",
            "path": "torch/_export/serde/serialize.py",
            "patches": [
                {
                    "old_start": 172,
                    "old_length": 12,
                    "new_start": 172,
                    "new_length": 15,
                    "hunk": "@@ -172,12 +172,15 @@ _SYM_INT_OPS = {\n     operator.sub,\n     operator.floordiv,\n     operator.mod,\n+    operator.pow,\n     torch.sym_int,\n     torch.sym_float,\n     torch.sym_ite,\n     torch.sym_max,\n     torch.sym_min,\n     torch.sym_sqrt,\n+    torch.ops.aten.sym_size.int,\n+    torch.ops.aten.sym_stride.int,\n }\n \n \n"
                },
                {
                    "old_start": 215,
                    "old_length": 11,
                    "new_start": 218,
                    "new_length": 11,
                    "hunk": "@@ -215,11 +218,11 @@ def deserialize_device(d: Device) -> torch.device:\n \n \n def serialize_sym_int(s: Union[int, torch.SymInt]) -> SymInt:\n-    if isinstance(s, (torch.SymInt, int)):\n+    if isinstance(s, (torch.SymInt, sympy.Symbol, int)):\n         if symbolic_shapes.is_concrete_int(s):\n             return SymInt.create(as_int=int(s))\n         else:\n-            assert isinstance(s, torch.SymInt)\n+            assert isinstance(s, (torch.SymInt, sympy.Symbol))\n             if s.node.hint is None:\n                 return SymInt.create(as_expr=SymExpr(str(s)))\n             else:\n"
                },
                {
                    "old_start": 487,
                    "old_length": 9,
                    "new_start": 490,
                    "new_length": 13,
                    "hunk": "@@ -487,9 +490,13 @@ class GraphModuleSerializer(metaclass=Final):\n         if node.target is operator.getitem:\n             return\n \n-        if node.target in _SYM_INT_OPS:\n+        meta_val = node.meta.get(\"val\")\n+        if (\n+            node.target in _SYM_INT_OPS\n+            or node.target in _SYM_BOOL_OPS\n+            or (meta_val is not None and isinstance(meta_val, (torch.SymInt, torch.SymBool)))\n+        ):\n             assert len(node.kwargs) == 0\n-            meta_val = node.meta[\"val\"]\n             ex_node = Node(\n                 target=self.serialize_operator(node.target),\n                 inputs=self.serialize_sym_op_inputs(node.target, node.args),\n"
                },
                {
                    "old_start": 497,
                    "old_length": 17,
                    "new_start": 504,
                    "new_length": 8,
                    "hunk": "@@ -497,17 +504,8 @@ class GraphModuleSerializer(metaclass=Final):\n                     Argument.create(\n                         as_sym_int=self.serialize_sym_int_output(node.name, meta_val)\n                     )\n-                ],\n-                metadata=self.serialize_metadata(node),\n-            )\n-        elif node.target in _SYM_BOOL_OPS:\n-            assert len(node.kwargs) == 0\n-            meta_val = node.meta[\"val\"]\n-            ex_node = Node(\n-                target=self.serialize_operator(node.target),\n-                inputs=self.serialize_sym_op_inputs(node.target, node.args),\n-                outputs=[\n-                    Argument.create(\n+                    if (node.target in _SYM_INT_OPS or isinstance(meta_val, torch.SymInt))\n+                    else Argument.create(\n                         as_sym_bool=self.serialize_sym_bool_output(node.name, meta_val)\n                     )\n                 ],\n"
                },
                {
                    "old_start": 1538,
                    "old_length": 6,
                    "new_start": 1536,
                    "new_length": 15,
                    "hunk": "@@ -1538,6 +1536,15 @@ class GraphModuleDeserializer(metaclass=Final):\n     def deserialize_sym_bool(self, s: SymBool) -> Union[bool, torch.SymBool]:\n         val = s.value\n         if s.type == \"as_expr\":\n+            # first we sympify this just to access any untracked symbols\n+            expr = sympy.sympify(val.expr_str)\n+            for sym in expr.free_symbols:\n+                if (\n+                    not isinstance(sym, sympy.Number)\n+                    and str(sym) not in self.symbol_name_to_symbol\n+                ):\n+                    self.deserialize_sym_int(SymInt.create(as_expr=SymExpr(str(sym))))\n+            # then we sympify again using locals to correctly reify with the constructed symbols\n             expr = sympy.sympify(val.expr_str, locals=self.symbol_name_to_symbol)\n             return self.shape_env.create_symboolnode(expr)\n         elif s.type == \"as_bool\":\n"
                },
                {
                    "old_start": 1661,
                    "old_length": 7,
                    "new_start": 1668,
                    "new_length": 11,
                    "hunk": "@@ -1661,7 +1668,11 @@ class GraphModuleDeserializer(metaclass=Final):\n         return self.graph\n \n     def deserialize_node(self, serialized_node: Node, target: Callable) -> None:\n-        if target in _SYM_BOOL_OPS or target in _SYM_INT_OPS:\n+        if (\n+            target in _SYM_BOOL_OPS\n+            or target in _SYM_INT_OPS\n+            or target == torch.ops.aten.item.default  # this can produce either SymInt or SymBool\n+        ):\n             name = serialized_node.outputs[0].value.as_name\n             args = self.deserialize_sym_op_inputs(serialized_node.inputs)\n \n"
                }
            ],
            "whole_deleted": "-    if isinstance(s, (torch.SymInt, int)):\n-            assert isinstance(s, torch.SymInt)\n-        if node.target in _SYM_INT_OPS:\n-            meta_val = node.meta[\"val\"]\n-                ],\n-                metadata=self.serialize_metadata(node),\n-            )\n-        elif node.target in _SYM_BOOL_OPS:\n-            assert len(node.kwargs) == 0\n-            meta_val = node.meta[\"val\"]\n-            ex_node = Node(\n-                target=self.serialize_operator(node.target),\n-                inputs=self.serialize_sym_op_inputs(node.target, node.args),\n-                outputs=[\n-                    Argument.create(\n-        if target in _SYM_BOOL_OPS or target in _SYM_INT_OPS:\n",
            "whole_added": "+    operator.pow,\n+    torch.ops.aten.sym_size.int,\n+    torch.ops.aten.sym_stride.int,\n+    if isinstance(s, (torch.SymInt, sympy.Symbol, int)):\n+            assert isinstance(s, (torch.SymInt, sympy.Symbol))\n+        meta_val = node.meta.get(\"val\")\n+        if (\n+            node.target in _SYM_INT_OPS\n+            or node.target in _SYM_BOOL_OPS\n+            or (meta_val is not None and isinstance(meta_val, (torch.SymInt, torch.SymBool)))\n+        ):\n+                    if (node.target in _SYM_INT_OPS or isinstance(meta_val, torch.SymInt))\n+                    else Argument.create(\n+            # first we sympify this just to access any untracked symbols\n+            expr = sympy.sympify(val.expr_str)\n+            for sym in expr.free_symbols:\n+                if (\n+                    not isinstance(sym, sympy.Number)\n+                    and str(sym) not in self.symbol_name_to_symbol\n+                ):\n+                    self.deserialize_sym_int(SymInt.create(as_expr=SymExpr(str(sym))))\n+            # then we sympify again using locals to correctly reify with the constructed symbols\n+        if (\n+            target in _SYM_BOOL_OPS\n+            or target in _SYM_INT_OPS\n+            or target == torch.ops.aten.item.default  # this can produce either SymInt or SymBool\n+        ):\n",
            "whole_hunk": "@@ -172,12 +172,15 @@ _SYM_INT_OPS = {\n     operator.sub,\n     operator.floordiv,\n     operator.mod,\n+    operator.pow,\n     torch.sym_int,\n     torch.sym_float,\n     torch.sym_ite,\n     torch.sym_max,\n     torch.sym_min,\n     torch.sym_sqrt,\n+    torch.ops.aten.sym_size.int,\n+    torch.ops.aten.sym_stride.int,\n }\n \n \n@@ -215,11 +218,11 @@ def deserialize_device(d: Device) -> torch.device:\n \n \n def serialize_sym_int(s: Union[int, torch.SymInt]) -> SymInt:\n-    if isinstance(s, (torch.SymInt, int)):\n+    if isinstance(s, (torch.SymInt, sympy.Symbol, int)):\n         if symbolic_shapes.is_concrete_int(s):\n             return SymInt.create(as_int=int(s))\n         else:\n-            assert isinstance(s, torch.SymInt)\n+            assert isinstance(s, (torch.SymInt, sympy.Symbol))\n             if s.node.hint is None:\n                 return SymInt.create(as_expr=SymExpr(str(s)))\n             else:\n@@ -487,9 +490,13 @@ class GraphModuleSerializer(metaclass=Final):\n         if node.target is operator.getitem:\n             return\n \n-        if node.target in _SYM_INT_OPS:\n+        meta_val = node.meta.get(\"val\")\n+        if (\n+            node.target in _SYM_INT_OPS\n+            or node.target in _SYM_BOOL_OPS\n+            or (meta_val is not None and isinstance(meta_val, (torch.SymInt, torch.SymBool)))\n+        ):\n             assert len(node.kwargs) == 0\n-            meta_val = node.meta[\"val\"]\n             ex_node = Node(\n                 target=self.serialize_operator(node.target),\n                 inputs=self.serialize_sym_op_inputs(node.target, node.args),\n@@ -497,17 +504,8 @@ class GraphModuleSerializer(metaclass=Final):\n                     Argument.create(\n                         as_sym_int=self.serialize_sym_int_output(node.name, meta_val)\n                     )\n-                ],\n-                metadata=self.serialize_metadata(node),\n-            )\n-        elif node.target in _SYM_BOOL_OPS:\n-            assert len(node.kwargs) == 0\n-            meta_val = node.meta[\"val\"]\n-            ex_node = Node(\n-                target=self.serialize_operator(node.target),\n-                inputs=self.serialize_sym_op_inputs(node.target, node.args),\n-                outputs=[\n-                    Argument.create(\n+                    if (node.target in _SYM_INT_OPS or isinstance(meta_val, torch.SymInt))\n+                    else Argument.create(\n                         as_sym_bool=self.serialize_sym_bool_output(node.name, meta_val)\n                     )\n                 ],\n@@ -1538,6 +1536,15 @@ class GraphModuleDeserializer(metaclass=Final):\n     def deserialize_sym_bool(self, s: SymBool) -> Union[bool, torch.SymBool]:\n         val = s.value\n         if s.type == \"as_expr\":\n+            # first we sympify this just to access any untracked symbols\n+            expr = sympy.sympify(val.expr_str)\n+            for sym in expr.free_symbols:\n+                if (\n+                    not isinstance(sym, sympy.Number)\n+                    and str(sym) not in self.symbol_name_to_symbol\n+                ):\n+                    self.deserialize_sym_int(SymInt.create(as_expr=SymExpr(str(sym))))\n+            # then we sympify again using locals to correctly reify with the constructed symbols\n             expr = sympy.sympify(val.expr_str, locals=self.symbol_name_to_symbol)\n             return self.shape_env.create_symboolnode(expr)\n         elif s.type == \"as_bool\":\n@@ -1661,7 +1668,11 @@ class GraphModuleDeserializer(metaclass=Final):\n         return self.graph\n \n     def deserialize_node(self, serialized_node: Node, target: Callable) -> None:\n-        if target in _SYM_BOOL_OPS or target in _SYM_INT_OPS:\n+        if (\n+            target in _SYM_BOOL_OPS\n+            or target in _SYM_INT_OPS\n+            or target == torch.ops.aten.item.default  # this can produce either SymInt or SymBool\n+        ):\n             name = serialized_node.outputs[0].value.as_name\n             args = self.deserialize_sym_op_inputs(serialized_node.inputs)\n \n"
        },
        {
            "name": "utils.py",
            "path": "torch/_export/utils.py",
            "patches": [
                {
                    "old_start": 148,
                    "old_length": 9,
                    "new_start": 148,
                    "new_length": 9,
                    "hunk": "@@ -148,9 +148,9 @@ def _check_input_constraints_for_graph(\n                                 )\n                 else:\n                     if arg_dim != node_dim:\n-                        if isinstance(\n-                            node_dim, torch.SymInt\n-                        ):  # this means we deferred a guard from export analysis to runtime, let this pass\n+                        if isinstance(node_dim, torch.SymInt):\n+                            # this means we deferred a guard from export analysis to runtime, let this pass\n+                            # we'll add a runtime assert checking equality to this replacement expression\n                             continue\n                         raise RuntimeError(\n                             f\"Expected input at {get_keystr(key_path)}.shape[{j}] to be equal to \"\n"
                }
            ],
            "whole_deleted": "-                        if isinstance(\n-                            node_dim, torch.SymInt\n-                        ):  # this means we deferred a guard from export analysis to runtime, let this pass\n",
            "whole_added": "+                        if isinstance(node_dim, torch.SymInt):\n+                            # this means we deferred a guard from export analysis to runtime, let this pass\n+                            # we'll add a runtime assert checking equality to this replacement expression\n",
            "whole_hunk": "@@ -148,9 +148,9 @@ def _check_input_constraints_for_graph(\n                                 )\n                 else:\n                     if arg_dim != node_dim:\n-                        if isinstance(\n-                            node_dim, torch.SymInt\n-                        ):  # this means we deferred a guard from export analysis to runtime, let this pass\n+                        if isinstance(node_dim, torch.SymInt):\n+                            # this means we deferred a guard from export analysis to runtime, let this pass\n+                            # we'll add a runtime assert checking equality to this replacement expression\n                             continue\n                         raise RuntimeError(\n                             f\"Expected input at {get_keystr(key_path)}.shape[{j}] to be equal to \"\n"
        },
        {
            "name": "_trace.py",
            "path": "torch/export/_trace.py",
            "patches": [
                {
                    "old_start": 96,
                    "old_length": 6,
                    "new_start": 96,
                    "new_length": 11,
                    "hunk": "@@ -96,6 +96,11 @@ class ExportDynamoConfig:\n     reorderable_logging_functions: Set[Callable] = dataclasses.field(\n         default_factory=set\n     )\n+    # Emit runtime asserts after AOTAutograd instead.\n+    # This isn't really necessary, and isn't much more efficient since the runtime asserts pass does CSE,\n+    # but if we want to reason more about what guards/runtime asserts to emit,\n+    # this makes it a bit cleaner to do from the export side. Also no real point in running this twice.\n+    do_not_emit_runtime_asserts = True\n \n \n @dataclasses.dataclass\n"
                },
                {
                    "old_start": 549,
                    "old_length": 7,
                    "new_start": 554,
                    "new_length": 7,
                    "hunk": "@@ -549,7 +554,7 @@ def _export_to_torch_ir(\n                     disable_constraint_solver=disable_constraint_solver,\n                     # currently the following 2 flags are tied together for export purposes,\n                     # but untangle for sake of dynamo export api\n-                    prefer_deferred_runtime_asserts_over_guards=allow_complex_guards_as_runtime_asserts,\n+                    prefer_deferred_runtime_asserts_over_guards=True,\n                     allow_complex_guards_as_runtime_asserts=allow_complex_guards_as_runtime_asserts,\n                     _log_export_usage=_log_export_usage,\n                     same_signature=same_signature,\n"
                },
                {
                    "old_start": 580,
                    "old_length": 6,
                    "new_start": 585,
                    "new_length": 7,
                    "hunk": "@@ -580,6 +585,7 @@ def _export_to_aten_ir(\n     fake_kwargs,\n     fake_params_buffers,\n     constant_attrs: ConstantAttrMap,\n+    produce_guards_callback=None,\n     *,\n     transform=lambda x: x,  # TODO(zhxchen17) Revisit if this is needed later.\n     pre_dispatch=False,\n"
                },
                {
                    "old_start": 625,
                    "old_length": 16,
                    "new_start": 631,
                    "new_length": 27,
                    "hunk": "@@ -625,16 +631,27 @@ def _export_to_aten_ir(\n     if isinstance(mod, torch.fx.GraphModule) and hasattr(mod, \"meta\"):\n         gm.meta.update(mod.meta)\n \n-    # Run this pass before creating input/output specs, since size-related CSE/DCE might affect output signature.\n-    # Overwrite output specs afterwards.\n-    from torch._dynamo import config as _dynamo_config\n     from torch._functorch._aot_autograd.input_output_analysis import _graph_output_names\n     from torch._guards import detect_fake_mode\n \n+    # Run produce guards before we handle runtime asserts.\n+    # This means we run the export solver before the runtime asserts pass.\n+    # Right now this doesn't mean much - the export solver is only there for suggested fixes,\n+    # and we won't even get to constraint solving if that's needed.\n+    # But if in future we want to control what runtime asserts are emitted for export,\n+    # or rely on produce_guards + solver for some simplification on runtime asserts, this probably makes sense.\n+    if produce_guards_callback:\n+        try:\n+            produce_guards_callback(gm)\n+        except (ConstraintViolationError, ValueRangeError) as e:\n+            raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: B904\n+\n+    # Run runtime asserts pass before creating input/output specs, since size-related CSE/DCE might affect output signature.\n+    # Overwrite output specs afterwards.\n     flat_fake_args = pytree.tree_leaves((fake_args, fake_kwargs))\n     fake_mode = detect_fake_mode(flat_fake_args)\n \n-    if not _dynamo_config.do_not_emit_runtime_asserts:\n+    if not torch._dynamo.config.do_not_emit_runtime_asserts:\n         stack_trace = (\n             'File \"torch/fx/passes/runtime_assert.py\", line 24, '\n             \"in insert_deferred_runtime_asserts\"\n"
                },
                {
                    "old_start": 1100,
                    "old_length": 7,
                    "new_start": 1117,
                    "new_length": 6,
                    "hunk": "@@ -1100,7 +1117,6 @@ def _strict_export(\n     original_state_dict: Dict[str, Any],\n     orig_in_spec: TreeSpec,\n     allow_complex_guards_as_runtime_asserts: bool,\n-    _disable_forced_specializations: Optional[bool],\n     _is_torch_jit_trace: bool,\n ) -> ExportArtifact:\n     lower_to_aten = functools.partial(_export_to_aten_ir, pre_dispatch=pre_dispatch)\n"
                },
                {
                    "old_start": 1114,
                    "old_length": 7,
                    "new_start": 1130,
                    "new_length": 6,
                    "hunk": "@@ -1114,7 +1130,6 @@ def _strict_export(\n         original_state_dict=original_state_dict,\n         orig_in_spec=orig_in_spec,\n         allow_complex_guards_as_runtime_asserts=allow_complex_guards_as_runtime_asserts,\n-        _disable_forced_specializations=_disable_forced_specializations,\n         _is_torch_jit_trace=_is_torch_jit_trace,\n         lower_to_aten_callback=lower_to_aten,\n     )\n"
                },
                {
                    "old_start": 1130,
                    "old_length": 7,
                    "new_start": 1145,
                    "new_length": 6,
                    "hunk": "@@ -1130,7 +1145,6 @@ def _strict_export_lower_to_aten_ir(\n     original_state_dict: Dict[str, Any],\n     orig_in_spec: TreeSpec,\n     allow_complex_guards_as_runtime_asserts: bool,\n-    _disable_forced_specializations: Optional[bool],\n     _is_torch_jit_trace: bool,\n     lower_to_aten_callback: Callable,\n ) -> ExportArtifact:\n"
                },
                {
                    "old_start": 1303,
                    "old_length": 6,
                    "new_start": 1317,
                    "new_length": 7,
                    "hunk": "@@ -1303,6 +1317,7 @@ def _export_to_aten_ir_make_fx(\n     fake_kwargs,\n     fake_params_buffers,\n     constant_attrs: ConstantAttrMap,\n+    produce_guards_callback=None,\n     transform=lambda x: x,\n ) -> ATenExportArtifact:\n     @contextmanager\n"
                },
                {
                    "old_start": 1469,
                    "old_length": 13,
                    "new_start": 1484,
                    "new_length": 18,
                    "hunk": "@@ -1469,13 +1484,18 @@ def _export_to_aten_ir_make_fx(\n         input_specs=input_specs, output_specs=output_specs\n     )\n \n+    # See comment in _export_to_aten_ir()\n+    if produce_guards_callback:\n+        try:\n+            produce_guards_callback(gm)\n+        except (ConstraintViolationError, ValueRangeError) as e:\n+            raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: B904\n+\n     from torch._guards import detect_fake_mode\n \n     fake_mode = detect_fake_mode(flat_args)\n \n-    from torch._dynamo import config as _dynamo_config\n-\n-    if not _dynamo_config.do_not_emit_runtime_asserts:\n+    if not torch._dynamo.config.do_not_emit_runtime_asserts:\n         stack_trace = (\n             'File \"torch/fx/passes/runtime_assert.py\", line 24, '\n             \"in insert_deferred_runtime_asserts\"\n"
                },
                {
                    "old_start": 1534,
                    "old_length": 7,
                    "new_start": 1554,
                    "new_length": 6,
                    "hunk": "@@ -1534,7 +1554,6 @@ def _non_strict_export(\n     original_state_dict: Dict[str, Any],\n     orig_in_spec: TreeSpec,\n     allow_complex_guards_as_runtime_asserts: bool,\n-    _disable_forced_specializations: Optional[bool],\n     _is_torch_jit_trace: bool,\n     _is_training: bool = False,\n ) -> ExportArtifact:\n"
                },
                {
                    "old_start": 1625,
                    "old_length": 6,
                    "new_start": 1644,
                    "new_length": 16,
                    "hunk": "@@ -1625,6 +1644,16 @@ def _non_strict_export(\n \n     fake_params_buffers = make_fake_params_buffers(fake_mode, _get_params_buffers(mod))\n \n+    def _produce_guards_callback(gm):\n+        return produce_guards_and_solve_constraints(\n+            fake_mode=fake_mode,\n+            gm=gm,\n+            dynamic_shapes=dynamic_shapes,\n+            equalities_inputs=equalities_inputs,\n+            original_signature=original_signature,\n+            _is_torch_jit_trace=_is_torch_jit_trace,\n+        )\n+\n     with fake_mode:\n         with _fakify_script_objects(mod, fake_args, fake_kwargs, fake_mode) as (\n             patched_mod,\n"
                },
                {
                    "old_start": 1648,
                    "old_length": 6,
                    "new_start": 1677,
                    "new_length": 7,
                    "hunk": "@@ -1648,6 +1677,7 @@ def _non_strict_export(\n                 new_fake_kwargs,\n                 fake_params_buffers,\n                 new_fake_constant_attrs,\n+                produce_guards_callback=_produce_guards_callback,\n                 transform=_tuplify_outputs,\n             )\n             # aten_export_artifact.constants contains only fake script objects, we need to map them back\n"
                },
                {
                    "old_start": 1656,
                    "old_length": 19,
                    "new_start": 1686,
                    "new_length": 6,
                    "hunk": "@@ -1656,19 +1686,6 @@ def _non_strict_export(\n                 for fqn, obj in aten_export_artifact.constants.items()\n             }\n \n-    try:\n-        produce_guards_and_solve_constraints(\n-            fake_mode,\n-            aten_export_artifact.gm,\n-            dynamic_shapes,\n-            equalities_inputs,\n-            original_signature,\n-            _disable_forced_specializations=_disable_forced_specializations,\n-            _is_torch_jit_trace=_is_torch_jit_trace,\n-        )\n-    except (ConstraintViolationError, ValueRangeError) as e:\n-        raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: B904\n-\n     _rewrite_non_persistent_buffers(\n         mod, aten_export_artifact.sig, aten_export_artifact.constants\n     )\n"
                },
                {
                    "old_start": 1733,
                    "old_length": 7,
                    "new_start": 1750,
                    "new_length": 6,
                    "hunk": "@@ -1733,7 +1750,6 @@ def _export_for_training(\n         original_state_dict=original_state_dict,\n         orig_in_spec=orig_in_spec,\n         allow_complex_guards_as_runtime_asserts=False,\n-        _disable_forced_specializations=False,\n         _is_torch_jit_trace=False,\n     )\n \n"
                },
                {
                    "old_start": 1821,
                    "old_length": 7,
                    "new_start": 1837,
                    "new_length": 6,
                    "hunk": "@@ -1821,7 +1837,6 @@ def _export(\n     preserve_module_call_signature: Tuple[str, ...] = (),\n     pre_dispatch: bool = False,\n     allow_complex_guards_as_runtime_asserts: bool = False,\n-    _disable_forced_specializations: Optional[bool] = False,\n     _is_torch_jit_trace: bool = False,\n ) -> ExportedProgram:\n     \"\"\"\n"
                },
                {
                    "old_start": 1864,
                    "old_length": 13,
                    "new_start": 1879,
                    "new_length": 6,
                    "hunk": "@@ -1864,13 +1879,6 @@ def _export(\n          Additionally, if TORCH_DYNAMO_DO_NOT_EMIT_RUNTIME_ASSERTS=1 is specified, we will allow complex constraints\n          while not emitting runtime asserts, returning a cleaner graph with lesser guarantees around dynamic shapes.\n \n-        _disable_forced_specializations:\n-         Similar to allow_complex_guards_as_runtime_asserts, but only avoids specializing to static values if set to True.\n-         For complex guards that don't specialize, this flag doesn't have any effect. Ideally this would be subsumed by\n-         allow_complex_guards_as_runtime_asserts, but this handles one additional case: single-variable equalities where\n-         the symbol is solvable for a concrete value (e.g. Eq(s0 // 4, 400) -> s0 = 1600). If set to True, this flag will\n-         avoid specializations. Direct equalities (e.g. s0 = 4), will still specialize.\n-\n     Returns:\n         An ExportedProgram containing the traced method.\n     \"\"\"\n"
                },
                {
                    "old_start": 1880,
                    "old_length": 12,
                    "new_start": 1888,
                    "new_length": 6,
                    "hunk": "@@ -1880,12 +1888,6 @@ def _export(\n             f\"Expecting `args` to be a tuple of example positional inputs, got {type(args)}\",\n         )\n \n-    if _disable_forced_specializations and strict:\n-        raise UserError(\n-            UserErrorType.INVALID_INPUT,\n-            \"_disable_forced_specializations can be only be specified in non-strict mode.\",\n-        )\n-\n     global _EXPORT_FLAGS, _EXPORT_MODULE_HIERARCHY\n     _EXPORT_MODULE_HIERARCHY = _get_module_hierarchy(mod)\n \n"
                },
                {
                    "old_start": 1919,
                    "old_length": 7,
                    "new_start": 1921,
                    "new_length": 6,
                    "hunk": "@@ -1919,7 +1921,6 @@ def _export(\n         original_state_dict,\n         orig_in_spec,\n         allow_complex_guards_as_runtime_asserts,\n-        _disable_forced_specializations,\n         _is_torch_jit_trace,\n     )\n     # Decompose here for readability.\n"
                }
            ],
            "whole_deleted": "-                    prefer_deferred_runtime_asserts_over_guards=allow_complex_guards_as_runtime_asserts,\n-    # Run this pass before creating input/output specs, since size-related CSE/DCE might affect output signature.\n-    # Overwrite output specs afterwards.\n-    from torch._dynamo import config as _dynamo_config\n-    if not _dynamo_config.do_not_emit_runtime_asserts:\n-    _disable_forced_specializations: Optional[bool],\n-        _disable_forced_specializations=_disable_forced_specializations,\n-    _disable_forced_specializations: Optional[bool],\n-    from torch._dynamo import config as _dynamo_config\n-\n-    if not _dynamo_config.do_not_emit_runtime_asserts:\n-    _disable_forced_specializations: Optional[bool],\n-    try:\n-        produce_guards_and_solve_constraints(\n-            fake_mode,\n-            aten_export_artifact.gm,\n-            dynamic_shapes,\n-            equalities_inputs,\n-            original_signature,\n-            _disable_forced_specializations=_disable_forced_specializations,\n-            _is_torch_jit_trace=_is_torch_jit_trace,\n-        )\n-    except (ConstraintViolationError, ValueRangeError) as e:\n-        raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: B904\n-\n-        _disable_forced_specializations=False,\n-    _disable_forced_specializations: Optional[bool] = False,\n-        _disable_forced_specializations:\n-         Similar to allow_complex_guards_as_runtime_asserts, but only avoids specializing to static values if set to True.\n-         For complex guards that don't specialize, this flag doesn't have any effect. Ideally this would be subsumed by\n-         allow_complex_guards_as_runtime_asserts, but this handles one additional case: single-variable equalities where\n-         the symbol is solvable for a concrete value (e.g. Eq(s0 // 4, 400) -> s0 = 1600). If set to True, this flag will\n-         avoid specializations. Direct equalities (e.g. s0 = 4), will still specialize.\n-\n-    if _disable_forced_specializations and strict:\n-        raise UserError(\n-            UserErrorType.INVALID_INPUT,\n-            \"_disable_forced_specializations can be only be specified in non-strict mode.\",\n-        )\n-\n-        _disable_forced_specializations,\n",
            "whole_added": "+    # Emit runtime asserts after AOTAutograd instead.\n+    # This isn't really necessary, and isn't much more efficient since the runtime asserts pass does CSE,\n+    # but if we want to reason more about what guards/runtime asserts to emit,\n+    # this makes it a bit cleaner to do from the export side. Also no real point in running this twice.\n+    do_not_emit_runtime_asserts = True\n+                    prefer_deferred_runtime_asserts_over_guards=True,\n+    produce_guards_callback=None,\n+    # Run produce guards before we handle runtime asserts.\n+    # This means we run the export solver before the runtime asserts pass.\n+    # Right now this doesn't mean much - the export solver is only there for suggested fixes,\n+    # and we won't even get to constraint solving if that's needed.\n+    # But if in future we want to control what runtime asserts are emitted for export,\n+    # or rely on produce_guards + solver for some simplification on runtime asserts, this probably makes sense.\n+    if produce_guards_callback:\n+        try:\n+            produce_guards_callback(gm)\n+        except (ConstraintViolationError, ValueRangeError) as e:\n+            raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: B904\n+\n+    # Run runtime asserts pass before creating input/output specs, since size-related CSE/DCE might affect output signature.\n+    # Overwrite output specs afterwards.\n+    if not torch._dynamo.config.do_not_emit_runtime_asserts:\n+    produce_guards_callback=None,\n+    # See comment in _export_to_aten_ir()\n+    if produce_guards_callback:\n+        try:\n+            produce_guards_callback(gm)\n+        except (ConstraintViolationError, ValueRangeError) as e:\n+            raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: B904\n+\n+    if not torch._dynamo.config.do_not_emit_runtime_asserts:\n+    def _produce_guards_callback(gm):\n+        return produce_guards_and_solve_constraints(\n+            fake_mode=fake_mode,\n+            gm=gm,\n+            dynamic_shapes=dynamic_shapes,\n+            equalities_inputs=equalities_inputs,\n+            original_signature=original_signature,\n+            _is_torch_jit_trace=_is_torch_jit_trace,\n+        )\n+\n+                produce_guards_callback=_produce_guards_callback,\n",
            "whole_hunk": "@@ -96,6 +96,11 @@ class ExportDynamoConfig:\n     reorderable_logging_functions: Set[Callable] = dataclasses.field(\n         default_factory=set\n     )\n+    # Emit runtime asserts after AOTAutograd instead.\n+    # This isn't really necessary, and isn't much more efficient since the runtime asserts pass does CSE,\n+    # but if we want to reason more about what guards/runtime asserts to emit,\n+    # this makes it a bit cleaner to do from the export side. Also no real point in running this twice.\n+    do_not_emit_runtime_asserts = True\n \n \n @dataclasses.dataclass\n@@ -549,7 +554,7 @@ def _export_to_torch_ir(\n                     disable_constraint_solver=disable_constraint_solver,\n                     # currently the following 2 flags are tied together for export purposes,\n                     # but untangle for sake of dynamo export api\n-                    prefer_deferred_runtime_asserts_over_guards=allow_complex_guards_as_runtime_asserts,\n+                    prefer_deferred_runtime_asserts_over_guards=True,\n                     allow_complex_guards_as_runtime_asserts=allow_complex_guards_as_runtime_asserts,\n                     _log_export_usage=_log_export_usage,\n                     same_signature=same_signature,\n@@ -580,6 +585,7 @@ def _export_to_aten_ir(\n     fake_kwargs,\n     fake_params_buffers,\n     constant_attrs: ConstantAttrMap,\n+    produce_guards_callback=None,\n     *,\n     transform=lambda x: x,  # TODO(zhxchen17) Revisit if this is needed later.\n     pre_dispatch=False,\n@@ -625,16 +631,27 @@ def _export_to_aten_ir(\n     if isinstance(mod, torch.fx.GraphModule) and hasattr(mod, \"meta\"):\n         gm.meta.update(mod.meta)\n \n-    # Run this pass before creating input/output specs, since size-related CSE/DCE might affect output signature.\n-    # Overwrite output specs afterwards.\n-    from torch._dynamo import config as _dynamo_config\n     from torch._functorch._aot_autograd.input_output_analysis import _graph_output_names\n     from torch._guards import detect_fake_mode\n \n+    # Run produce guards before we handle runtime asserts.\n+    # This means we run the export solver before the runtime asserts pass.\n+    # Right now this doesn't mean much - the export solver is only there for suggested fixes,\n+    # and we won't even get to constraint solving if that's needed.\n+    # But if in future we want to control what runtime asserts are emitted for export,\n+    # or rely on produce_guards + solver for some simplification on runtime asserts, this probably makes sense.\n+    if produce_guards_callback:\n+        try:\n+            produce_guards_callback(gm)\n+        except (ConstraintViolationError, ValueRangeError) as e:\n+            raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: B904\n+\n+    # Run runtime asserts pass before creating input/output specs, since size-related CSE/DCE might affect output signature.\n+    # Overwrite output specs afterwards.\n     flat_fake_args = pytree.tree_leaves((fake_args, fake_kwargs))\n     fake_mode = detect_fake_mode(flat_fake_args)\n \n-    if not _dynamo_config.do_not_emit_runtime_asserts:\n+    if not torch._dynamo.config.do_not_emit_runtime_asserts:\n         stack_trace = (\n             'File \"torch/fx/passes/runtime_assert.py\", line 24, '\n             \"in insert_deferred_runtime_asserts\"\n@@ -1100,7 +1117,6 @@ def _strict_export(\n     original_state_dict: Dict[str, Any],\n     orig_in_spec: TreeSpec,\n     allow_complex_guards_as_runtime_asserts: bool,\n-    _disable_forced_specializations: Optional[bool],\n     _is_torch_jit_trace: bool,\n ) -> ExportArtifact:\n     lower_to_aten = functools.partial(_export_to_aten_ir, pre_dispatch=pre_dispatch)\n@@ -1114,7 +1130,6 @@ def _strict_export(\n         original_state_dict=original_state_dict,\n         orig_in_spec=orig_in_spec,\n         allow_complex_guards_as_runtime_asserts=allow_complex_guards_as_runtime_asserts,\n-        _disable_forced_specializations=_disable_forced_specializations,\n         _is_torch_jit_trace=_is_torch_jit_trace,\n         lower_to_aten_callback=lower_to_aten,\n     )\n@@ -1130,7 +1145,6 @@ def _strict_export_lower_to_aten_ir(\n     original_state_dict: Dict[str, Any],\n     orig_in_spec: TreeSpec,\n     allow_complex_guards_as_runtime_asserts: bool,\n-    _disable_forced_specializations: Optional[bool],\n     _is_torch_jit_trace: bool,\n     lower_to_aten_callback: Callable,\n ) -> ExportArtifact:\n@@ -1303,6 +1317,7 @@ def _export_to_aten_ir_make_fx(\n     fake_kwargs,\n     fake_params_buffers,\n     constant_attrs: ConstantAttrMap,\n+    produce_guards_callback=None,\n     transform=lambda x: x,\n ) -> ATenExportArtifact:\n     @contextmanager\n@@ -1469,13 +1484,18 @@ def _export_to_aten_ir_make_fx(\n         input_specs=input_specs, output_specs=output_specs\n     )\n \n+    # See comment in _export_to_aten_ir()\n+    if produce_guards_callback:\n+        try:\n+            produce_guards_callback(gm)\n+        except (ConstraintViolationError, ValueRangeError) as e:\n+            raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: B904\n+\n     from torch._guards import detect_fake_mode\n \n     fake_mode = detect_fake_mode(flat_args)\n \n-    from torch._dynamo import config as _dynamo_config\n-\n-    if not _dynamo_config.do_not_emit_runtime_asserts:\n+    if not torch._dynamo.config.do_not_emit_runtime_asserts:\n         stack_trace = (\n             'File \"torch/fx/passes/runtime_assert.py\", line 24, '\n             \"in insert_deferred_runtime_asserts\"\n@@ -1534,7 +1554,6 @@ def _non_strict_export(\n     original_state_dict: Dict[str, Any],\n     orig_in_spec: TreeSpec,\n     allow_complex_guards_as_runtime_asserts: bool,\n-    _disable_forced_specializations: Optional[bool],\n     _is_torch_jit_trace: bool,\n     _is_training: bool = False,\n ) -> ExportArtifact:\n@@ -1625,6 +1644,16 @@ def _non_strict_export(\n \n     fake_params_buffers = make_fake_params_buffers(fake_mode, _get_params_buffers(mod))\n \n+    def _produce_guards_callback(gm):\n+        return produce_guards_and_solve_constraints(\n+            fake_mode=fake_mode,\n+            gm=gm,\n+            dynamic_shapes=dynamic_shapes,\n+            equalities_inputs=equalities_inputs,\n+            original_signature=original_signature,\n+            _is_torch_jit_trace=_is_torch_jit_trace,\n+        )\n+\n     with fake_mode:\n         with _fakify_script_objects(mod, fake_args, fake_kwargs, fake_mode) as (\n             patched_mod,\n@@ -1648,6 +1677,7 @@ def _non_strict_export(\n                 new_fake_kwargs,\n                 fake_params_buffers,\n                 new_fake_constant_attrs,\n+                produce_guards_callback=_produce_guards_callback,\n                 transform=_tuplify_outputs,\n             )\n             # aten_export_artifact.constants contains only fake script objects, we need to map them back\n@@ -1656,19 +1686,6 @@ def _non_strict_export(\n                 for fqn, obj in aten_export_artifact.constants.items()\n             }\n \n-    try:\n-        produce_guards_and_solve_constraints(\n-            fake_mode,\n-            aten_export_artifact.gm,\n-            dynamic_shapes,\n-            equalities_inputs,\n-            original_signature,\n-            _disable_forced_specializations=_disable_forced_specializations,\n-            _is_torch_jit_trace=_is_torch_jit_trace,\n-        )\n-    except (ConstraintViolationError, ValueRangeError) as e:\n-        raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: B904\n-\n     _rewrite_non_persistent_buffers(\n         mod, aten_export_artifact.sig, aten_export_artifact.constants\n     )\n@@ -1733,7 +1750,6 @@ def _export_for_training(\n         original_state_dict=original_state_dict,\n         orig_in_spec=orig_in_spec,\n         allow_complex_guards_as_runtime_asserts=False,\n-        _disable_forced_specializations=False,\n         _is_torch_jit_trace=False,\n     )\n \n@@ -1821,7 +1837,6 @@ def _export(\n     preserve_module_call_signature: Tuple[str, ...] = (),\n     pre_dispatch: bool = False,\n     allow_complex_guards_as_runtime_asserts: bool = False,\n-    _disable_forced_specializations: Optional[bool] = False,\n     _is_torch_jit_trace: bool = False,\n ) -> ExportedProgram:\n     \"\"\"\n@@ -1864,13 +1879,6 @@ def _export(\n          Additionally, if TORCH_DYNAMO_DO_NOT_EMIT_RUNTIME_ASSERTS=1 is specified, we will allow complex constraints\n          while not emitting runtime asserts, returning a cleaner graph with lesser guarantees around dynamic shapes.\n \n-        _disable_forced_specializations:\n-         Similar to allow_complex_guards_as_runtime_asserts, but only avoids specializing to static values if set to True.\n-         For complex guards that don't specialize, this flag doesn't have any effect. Ideally this would be subsumed by\n-         allow_complex_guards_as_runtime_asserts, but this handles one additional case: single-variable equalities where\n-         the symbol is solvable for a concrete value (e.g. Eq(s0 // 4, 400) -> s0 = 1600). If set to True, this flag will\n-         avoid specializations. Direct equalities (e.g. s0 = 4), will still specialize.\n-\n     Returns:\n         An ExportedProgram containing the traced method.\n     \"\"\"\n@@ -1880,12 +1888,6 @@ def _export(\n             f\"Expecting `args` to be a tuple of example positional inputs, got {type(args)}\",\n         )\n \n-    if _disable_forced_specializations and strict:\n-        raise UserError(\n-            UserErrorType.INVALID_INPUT,\n-            \"_disable_forced_specializations can be only be specified in non-strict mode.\",\n-        )\n-\n     global _EXPORT_FLAGS, _EXPORT_MODULE_HIERARCHY\n     _EXPORT_MODULE_HIERARCHY = _get_module_hierarchy(mod)\n \n@@ -1919,7 +1921,6 @@ def _export(\n         original_state_dict,\n         orig_in_spec,\n         allow_complex_guards_as_runtime_asserts,\n-        _disable_forced_specializations,\n         _is_torch_jit_trace,\n     )\n     # Decompose here for readability.\n"
        },
        {
            "name": "exported_program.py",
            "path": "torch/export/exported_program.py",
            "patches": [
                {
                    "old_start": 502,
                    "old_length": 14,
                    "new_start": 502,
                    "new_length": 13,
                    "hunk": "@@ -502,14 +502,13 @@ def _decompose_and_get_gm_with_new_signature_constants(\n \n     # Run this pass before creating input/output specs, since size-related CSE/DCE might affect output signature.\n     # Overwrite output specs afterwards.\n-    from torch._dynamo import config as _dynamo_config\n     from torch._export.passes._node_metadata_hook import (\n         _node_metadata_hook,\n         _set_node_metadata_hook,\n     )\n     from torch._functorch._aot_autograd.input_output_analysis import _graph_output_names\n \n-    if not _dynamo_config.do_not_emit_runtime_asserts:\n+    if not torch._dynamo.config.do_not_emit_runtime_asserts:\n         stack_trace = (\n             'File \"torch/fx/passes/runtime_assert.py\", line 24, '\n             \"in insert_deferred_runtime_asserts\"\n"
                }
            ],
            "whole_deleted": "-    from torch._dynamo import config as _dynamo_config\n-    if not _dynamo_config.do_not_emit_runtime_asserts:\n",
            "whole_added": "+    if not torch._dynamo.config.do_not_emit_runtime_asserts:\n",
            "whole_hunk": "@@ -502,14 +502,13 @@ def _decompose_and_get_gm_with_new_signature_constants(\n \n     # Run this pass before creating input/output specs, since size-related CSE/DCE might affect output signature.\n     # Overwrite output specs afterwards.\n-    from torch._dynamo import config as _dynamo_config\n     from torch._export.passes._node_metadata_hook import (\n         _node_metadata_hook,\n         _set_node_metadata_hook,\n     )\n     from torch._functorch._aot_autograd.input_output_analysis import _graph_output_names\n \n-    if not _dynamo_config.do_not_emit_runtime_asserts:\n+    if not torch._dynamo.config.do_not_emit_runtime_asserts:\n         stack_trace = (\n             'File \"torch/fx/passes/runtime_assert.py\", line 24, '\n             \"in insert_deferred_runtime_asserts\"\n"
        },
        {
            "name": "symbolic_shapes.py",
            "path": "torch/fx/experimental/symbolic_shapes.py",
            "patches": [
                {
                    "old_start": 1772,
                    "old_length": 38,
                    "new_start": 1772,
                    "new_length": 7,
                    "hunk": "@@ -1772,38 +1772,7 @@ class DimConstraints:\n             self._inconsistencies.clear()\n             raise ValueError(f\"The following inconsistencies were found:\\n{msg}\")\n \n-    def _force_specialization(self, s):\n-        val = self._var_to_val[s]\n-        self._static_results.add(f\"{self._dcp.symbol_to_source[s][0].name()} == {val}\")\n-        self._substitutions[s] = val\n-\n-    def _specialize_divisor_symbols(self):\n-        for expr in self._multivariate_inequalities:\n-            for atom in expr.atoms(FloorDiv, Mod):\n-                _, divisor = atom.args\n-                for s in divisor.free_symbols:\n-                    self._force_specialization(s)\n-\n-        multivariate_inequalities = self._multivariate_inequalities\n-        self._multivariate_inequalities = set()\n-        for expr in multivariate_inequalities:\n-            self.add(expr.xreplace(self._substitutions))\n-        self._raise_inconsistencies()\n-        self._univariate_inequalities = {\n-            s: exprs\n-            for s, exprs in self._univariate_inequalities.items()\n-            if s not in self._substitutions\n-        }\n-        self._congruences = {\n-            s: congruences\n-            for s, congruences in self._congruences.items()\n-            if s not in self._substitutions\n-        }\n-\n-    def solve(\n-        self,\n-        _disable_forced_specializations=False,\n-    ):\n+    def solve(self):\n         \"\"\"Solve the system of constraint equations to find simplified constraints\n         \"\"\"\n         self._raise_inconsistencies()\n"
                },
                {
                    "old_start": 1818,
                    "old_length": 12,
                    "new_start": 1787,
                    "new_length": 10,
                    "hunk": "@@ -1818,12 +1787,10 @@ class DimConstraints:\n             assert isinstance(solution, sympy.Eq), f\"Expected an equality constraint for {s}, got {solution}\"\n             symbol, val = solution.args\n             assert symbol == s, f\"Expected a constraint on {s} instead of on {symbol}\"\n-            # really don't force specializations here\n-            if not (_disable_forced_specializations and s in self._marked_dynamic):\n-                # because this is univariate, the solution is a specialization\n-                self._static_results.add(f\"{self._dcp.symbol_to_source[s][0].name()} == {val}\")\n-                # add this as a substitution to simplify other constraints\n-                self._substitutions[s] = val\n+            # because this is univariate, the solution is a specialization\n+            self._static_results.add(f\"{self._dcp.symbol_to_source[s][0].name()} == {val}\")\n+            # add this as a substitution to simplify other constraints\n+            self._substitutions[s] = val\n \n             # simplify multivariate inequalities: some of them will now become univariate!\n             multivariate_inequalities = self._multivariate_inequalities\n"
                },
                {
                    "old_start": 1832,
                    "old_length": 9,
                    "new_start": 1799,
                    "new_length": 6,
                    "hunk": "@@ -1832,9 +1799,6 @@ class DimConstraints:\n                 self.add(expr.xreplace({s: self._substitutions[s]}))\n             self._raise_inconsistencies()\n \n-        if not _disable_forced_specializations:\n-            self._specialize_divisor_symbols()\n-\n         # solve linear congruences\n         # NOTE(avik): We do not need to solve them for symbols that have already been specialized.\n         reduced_congruences = self._reduce_congruences()\n"
                },
                {
                    "old_start": 1850,
                    "old_length": 9,
                    "new_start": 1814,
                    "new_length": 6,
                    "hunk": "@@ -1850,9 +1814,6 @@ class DimConstraints:\n                         self._dcp.symbol_to_source[tmp] = [ConstantSource(tmp_name)]\n                         r = try_solve(sympy.Eq(base, divisor * tmp), s)\n                         self._dynamic_results.add(self._dcp.doprint(sympy.Eq(s, r[1])))\n-                    elif not _disable_forced_specializations:\n-                        self._force_specialization(s)\n-                        self._univariate_inequalities.pop(s, None)\n \n         # remaining symbols have only pure inequalities (no equalities)\n         for s, exprs in self._univariate_inequalities.items():\n"
                },
                {
                    "old_start": 1875,
                    "old_length": 11,
                    "new_start": 1836,
                    "new_length": 6,
                    "hunk": "@@ -1875,11 +1836,6 @@ class DimConstraints:\n         symbolic_equivalences = self._symbolic_equivalences\n         self._symbolic_equivalences = []\n         for source, expr in symbolic_equivalences:\n-            if not _disable_forced_specializations and not _is_supported_equivalence(expr):\n-                for s in expr.free_symbols:\n-                    self._force_specialization(s)\n-                    sexpr = self._dcp._print_Symbol(s)\n-                    self._dynamic_results = {r for r in self._dynamic_results if sexpr not in r}\n             self.add_equality(source, expr.xreplace(self._substitutions))\n \n         # remaining symbolic equivalences become dynamic equality constraints\n"
                },
                {
                    "old_start": 2893,
                    "old_length": 6,
                    "new_start": 2849,
                    "new_length": 7,
                    "hunk": "@@ -2893,6 +2849,7 @@ class ShapeEnv:\n         we know statically is already True but we are checking it again in a way\n         that is not clearly dischargeable.\n         \"\"\"\n+        # self.prefer_deferred_runtime_asserts_over_guards = False\n         self.runtime_asserts_frozen = True\n \n     def _create_symbol_for_source(self, source: Source) -> Optional[sympy.Symbol]:\n"
                },
                {
                    "old_start": 3656,
                    "old_length": 7,
                    "new_start": 3613,
                    "new_length": 6,
                    "hunk": "@@ -3656,7 +3613,6 @@ class ShapeEnv:\n         # (See docs on EqualityConstraint for details of the encoding.)\n         equalities_inputs: Optional[EqualityConstraint] = None,\n         _simplified=False,\n-        _disable_forced_specializations=False,\n         # Indicates if we should produce guards for known static values.\n         ignore_static=True,\n     ) -> List[str]:\n"
                },
                {
                    "old_start": 4096,
                    "old_length": 13,
                    "new_start": 4052,
                    "new_length": 12,
                    "hunk": "@@ -4096,13 +4052,12 @@ class ShapeEnv:\n                     constraints = symbol_to_constraints[symbol]\n                     for c in constraints:\n                         if isinstance(c, StrictMinMaxConstraint):\n-                            if not _disable_forced_specializations:\n-                                var_with_range = self._render_range_for_constraint_violation(source, c)\n-                                msg = (\n-                                    f\"Not all values of {var_with_range} \"\n-                                    f\"satisfy the generated guard {guard_expr}.\"\n-                                )\n-                                record_constraint_violation(c.warn_only, self._debug_name(source), msg)\n+                            var_with_range = self._render_range_for_constraint_violation(source, c)\n+                            msg = (\n+                                f\"Not all values of {var_with_range} \"\n+                                f\"satisfy the generated guard {guard_expr}.\"\n+                            )\n+                            record_constraint_violation(c.warn_only, self._debug_name(source), msg)\n                         elif isinstance(c, RelaxedUnspecConstraint):\n                             # This is fine, we allow guards here as long as it\n                             # didn't constrain it to one value  (we don't\n"
                },
                {
                    "old_start": 4123,
                    "old_length": 6,
                    "new_start": 4078,
                    "new_length": 17,
                    "hunk": "@@ -4123,6 +4078,17 @@ class ShapeEnv:\n                 continue\n             issue_guard(guard)\n \n+        # Because there are guards that export's constraint solver can suggest good fixes for, that we may have\n+        # deferred as runtime asserts, and that produce_guards() alone won't do anything with (e.g. divisiblity guards),\n+        # we want to send runtime asserts to export's constraint solver too. These will still stay in the graph as asserts,\n+        # but export's constraint solver can decide whether to do anything with them (i.e. raise an error and provide\n+        # suggested fixes, or decide it's out of scope and leave as a runtime assert in the graph).\n+        for ra in self.deferred_runtime_asserts.get(None, []):\n+            if self._maybe_evaluate_static(ra.expr, axioms=()) is not None:\n+                continue\n+            expr = self.simplify(ra.expr)\n+            self.dim_constraints.add(expr)\n+\n         # 3. Every symbol must be within its value range (this handles 0/1\n         # specialization too).\n         for symbol, sources in symbol_to_source.items():"
                }
            ],
            "whole_deleted": "-    def _force_specialization(self, s):\n-        val = self._var_to_val[s]\n-        self._static_results.add(f\"{self._dcp.symbol_to_source[s][0].name()} == {val}\")\n-        self._substitutions[s] = val\n-\n-    def _specialize_divisor_symbols(self):\n-        for expr in self._multivariate_inequalities:\n-            for atom in expr.atoms(FloorDiv, Mod):\n-                _, divisor = atom.args\n-                for s in divisor.free_symbols:\n-                    self._force_specialization(s)\n-\n-        multivariate_inequalities = self._multivariate_inequalities\n-        self._multivariate_inequalities = set()\n-        for expr in multivariate_inequalities:\n-            self.add(expr.xreplace(self._substitutions))\n-        self._raise_inconsistencies()\n-        self._univariate_inequalities = {\n-            s: exprs\n-            for s, exprs in self._univariate_inequalities.items()\n-            if s not in self._substitutions\n-        }\n-        self._congruences = {\n-            s: congruences\n-            for s, congruences in self._congruences.items()\n-            if s not in self._substitutions\n-        }\n-\n-    def solve(\n-        self,\n-        _disable_forced_specializations=False,\n-    ):\n-            # really don't force specializations here\n-            if not (_disable_forced_specializations and s in self._marked_dynamic):\n-                # because this is univariate, the solution is a specialization\n-                self._static_results.add(f\"{self._dcp.symbol_to_source[s][0].name()} == {val}\")\n-                # add this as a substitution to simplify other constraints\n-                self._substitutions[s] = val\n-        if not _disable_forced_specializations:\n-            self._specialize_divisor_symbols()\n-\n-                    elif not _disable_forced_specializations:\n-                        self._force_specialization(s)\n-                        self._univariate_inequalities.pop(s, None)\n-            if not _disable_forced_specializations and not _is_supported_equivalence(expr):\n-                for s in expr.free_symbols:\n-                    self._force_specialization(s)\n-                    sexpr = self._dcp._print_Symbol(s)\n-                    self._dynamic_results = {r for r in self._dynamic_results if sexpr not in r}\n-        _disable_forced_specializations=False,\n-                            if not _disable_forced_specializations:\n-                                var_with_range = self._render_range_for_constraint_violation(source, c)\n-                                msg = (\n-                                    f\"Not all values of {var_with_range} \"\n-                                    f\"satisfy the generated guard {guard_expr}.\"\n-                                )\n-                                record_constraint_violation(c.warn_only, self._debug_name(source), msg)\n",
            "whole_added": "+    def solve(self):\n+            # because this is univariate, the solution is a specialization\n+            self._static_results.add(f\"{self._dcp.symbol_to_source[s][0].name()} == {val}\")\n+            # add this as a substitution to simplify other constraints\n+            self._substitutions[s] = val\n+        # self.prefer_deferred_runtime_asserts_over_guards = False\n+                            var_with_range = self._render_range_for_constraint_violation(source, c)\n+                            msg = (\n+                                f\"Not all values of {var_with_range} \"\n+                                f\"satisfy the generated guard {guard_expr}.\"\n+                            )\n+                            record_constraint_violation(c.warn_only, self._debug_name(source), msg)\n+        # Because there are guards that export's constraint solver can suggest good fixes for, that we may have\n+        # deferred as runtime asserts, and that produce_guards() alone won't do anything with (e.g. divisiblity guards),\n+        # we want to send runtime asserts to export's constraint solver too. These will still stay in the graph as asserts,\n+        # but export's constraint solver can decide whether to do anything with them (i.e. raise an error and provide\n+        # suggested fixes, or decide it's out of scope and leave as a runtime assert in the graph).\n+        for ra in self.deferred_runtime_asserts.get(None, []):\n+            if self._maybe_evaluate_static(ra.expr, axioms=()) is not None:\n+                continue\n+            expr = self.simplify(ra.expr)\n+            self.dim_constraints.add(expr)\n+\n",
            "whole_hunk": "@@ -1772,38 +1772,7 @@ class DimConstraints:\n             self._inconsistencies.clear()\n             raise ValueError(f\"The following inconsistencies were found:\\n{msg}\")\n \n-    def _force_specialization(self, s):\n-        val = self._var_to_val[s]\n-        self._static_results.add(f\"{self._dcp.symbol_to_source[s][0].name()} == {val}\")\n-        self._substitutions[s] = val\n-\n-    def _specialize_divisor_symbols(self):\n-        for expr in self._multivariate_inequalities:\n-            for atom in expr.atoms(FloorDiv, Mod):\n-                _, divisor = atom.args\n-                for s in divisor.free_symbols:\n-                    self._force_specialization(s)\n-\n-        multivariate_inequalities = self._multivariate_inequalities\n-        self._multivariate_inequalities = set()\n-        for expr in multivariate_inequalities:\n-            self.add(expr.xreplace(self._substitutions))\n-        self._raise_inconsistencies()\n-        self._univariate_inequalities = {\n-            s: exprs\n-            for s, exprs in self._univariate_inequalities.items()\n-            if s not in self._substitutions\n-        }\n-        self._congruences = {\n-            s: congruences\n-            for s, congruences in self._congruences.items()\n-            if s not in self._substitutions\n-        }\n-\n-    def solve(\n-        self,\n-        _disable_forced_specializations=False,\n-    ):\n+    def solve(self):\n         \"\"\"Solve the system of constraint equations to find simplified constraints\n         \"\"\"\n         self._raise_inconsistencies()\n@@ -1818,12 +1787,10 @@ class DimConstraints:\n             assert isinstance(solution, sympy.Eq), f\"Expected an equality constraint for {s}, got {solution}\"\n             symbol, val = solution.args\n             assert symbol == s, f\"Expected a constraint on {s} instead of on {symbol}\"\n-            # really don't force specializations here\n-            if not (_disable_forced_specializations and s in self._marked_dynamic):\n-                # because this is univariate, the solution is a specialization\n-                self._static_results.add(f\"{self._dcp.symbol_to_source[s][0].name()} == {val}\")\n-                # add this as a substitution to simplify other constraints\n-                self._substitutions[s] = val\n+            # because this is univariate, the solution is a specialization\n+            self._static_results.add(f\"{self._dcp.symbol_to_source[s][0].name()} == {val}\")\n+            # add this as a substitution to simplify other constraints\n+            self._substitutions[s] = val\n \n             # simplify multivariate inequalities: some of them will now become univariate!\n             multivariate_inequalities = self._multivariate_inequalities\n@@ -1832,9 +1799,6 @@ class DimConstraints:\n                 self.add(expr.xreplace({s: self._substitutions[s]}))\n             self._raise_inconsistencies()\n \n-        if not _disable_forced_specializations:\n-            self._specialize_divisor_symbols()\n-\n         # solve linear congruences\n         # NOTE(avik): We do not need to solve them for symbols that have already been specialized.\n         reduced_congruences = self._reduce_congruences()\n@@ -1850,9 +1814,6 @@ class DimConstraints:\n                         self._dcp.symbol_to_source[tmp] = [ConstantSource(tmp_name)]\n                         r = try_solve(sympy.Eq(base, divisor * tmp), s)\n                         self._dynamic_results.add(self._dcp.doprint(sympy.Eq(s, r[1])))\n-                    elif not _disable_forced_specializations:\n-                        self._force_specialization(s)\n-                        self._univariate_inequalities.pop(s, None)\n \n         # remaining symbols have only pure inequalities (no equalities)\n         for s, exprs in self._univariate_inequalities.items():\n@@ -1875,11 +1836,6 @@ class DimConstraints:\n         symbolic_equivalences = self._symbolic_equivalences\n         self._symbolic_equivalences = []\n         for source, expr in symbolic_equivalences:\n-            if not _disable_forced_specializations and not _is_supported_equivalence(expr):\n-                for s in expr.free_symbols:\n-                    self._force_specialization(s)\n-                    sexpr = self._dcp._print_Symbol(s)\n-                    self._dynamic_results = {r for r in self._dynamic_results if sexpr not in r}\n             self.add_equality(source, expr.xreplace(self._substitutions))\n \n         # remaining symbolic equivalences become dynamic equality constraints\n@@ -2893,6 +2849,7 @@ class ShapeEnv:\n         we know statically is already True but we are checking it again in a way\n         that is not clearly dischargeable.\n         \"\"\"\n+        # self.prefer_deferred_runtime_asserts_over_guards = False\n         self.runtime_asserts_frozen = True\n \n     def _create_symbol_for_source(self, source: Source) -> Optional[sympy.Symbol]:\n@@ -3656,7 +3613,6 @@ class ShapeEnv:\n         # (See docs on EqualityConstraint for details of the encoding.)\n         equalities_inputs: Optional[EqualityConstraint] = None,\n         _simplified=False,\n-        _disable_forced_specializations=False,\n         # Indicates if we should produce guards for known static values.\n         ignore_static=True,\n     ) -> List[str]:\n@@ -4096,13 +4052,12 @@ class ShapeEnv:\n                     constraints = symbol_to_constraints[symbol]\n                     for c in constraints:\n                         if isinstance(c, StrictMinMaxConstraint):\n-                            if not _disable_forced_specializations:\n-                                var_with_range = self._render_range_for_constraint_violation(source, c)\n-                                msg = (\n-                                    f\"Not all values of {var_with_range} \"\n-                                    f\"satisfy the generated guard {guard_expr}.\"\n-                                )\n-                                record_constraint_violation(c.warn_only, self._debug_name(source), msg)\n+                            var_with_range = self._render_range_for_constraint_violation(source, c)\n+                            msg = (\n+                                f\"Not all values of {var_with_range} \"\n+                                f\"satisfy the generated guard {guard_expr}.\"\n+                            )\n+                            record_constraint_violation(c.warn_only, self._debug_name(source), msg)\n                         elif isinstance(c, RelaxedUnspecConstraint):\n                             # This is fine, we allow guards here as long as it\n                             # didn't constrain it to one value  (we don't\n@@ -4123,6 +4078,17 @@ class ShapeEnv:\n                 continue\n             issue_guard(guard)\n \n+        # Because there are guards that export's constraint solver can suggest good fixes for, that we may have\n+        # deferred as runtime asserts, and that produce_guards() alone won't do anything with (e.g. divisiblity guards),\n+        # we want to send runtime asserts to export's constraint solver too. These will still stay in the graph as asserts,\n+        # but export's constraint solver can decide whether to do anything with them (i.e. raise an error and provide\n+        # suggested fixes, or decide it's out of scope and leave as a runtime assert in the graph).\n+        for ra in self.deferred_runtime_asserts.get(None, []):\n+            if self._maybe_evaluate_static(ra.expr, axioms=()) is not None:\n+                continue\n+            expr = self.simplify(ra.expr)\n+            self.dim_constraints.add(expr)\n+\n         # 3. Every symbol must be within its value range (this handles 0/1\n         # specialization too).\n         for symbol, sources in symbol_to_source.items():"
        }
    ]
},
{
    "Id": 11,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/096dc444cee3ebf8c1868ceb92ca4d6ede9f9cc1",
    "date": "2024-07-16T08:39:00+00:00",
    "message": "Keep zero check be compatible with different sympy versions (#130729)\n\n# Motivation\nI found a difference between sympy 1.12 and 1.13.\n```python\n# for 1.12\n>>> import sympy\n>>> a = sympy.Number(0.0)\n>>> a == 0\nTrue\n```\n```python\n# for 1.13\n>>> import sympy\n>>> a = sympy.Number(0.0)\n>>> a == 0\nFalse\n```\nThe different behavior will impact the result of [safe_mul](https://github.com/pytorch/pytorch/blob/6beec34b1c6803d5f6648c3cd7c262d6432374c8/torch/utils/_sympy/value_ranges.py#L521-L528), resulting in an incorrect results when `a = sympy.Number(0.0)`, `b = inf` and the result is `nan` if sympy version is 1.13. (the expected result is **0**)\n```python\ndef safe_mul(a, b):\n    # Make unknown() * wrap(0.0) == wrap(0.0)\n    if a == 0.0:\n        return a\n    elif b == 0.0:\n        return b\n    else:\n        return a * b\n```\n\nIn different sympy versions, `sympy.Number(0)` always has the same behavior that equals to 0.0.\n```python\n>>> import sympy\n>>> a = sympy.Number(0)\n>>> a == 0.0\nTrue # for different sympy versions\n```\nSo, use 0.0 when checking zero in safe_mul to keep compatible with different sympy versions.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130729\nApproved by: https://github.com/lezcano, https://github.com/EikanWang",
    "label": "YES",
    "changes": [
        {
            "name": "test_sympy_utils.py",
            "path": "test/test_sympy_utils.py",
            "patches": [
                {
                    "old_start": 241,
                    "old_length": 6,
                    "new_start": 241,
                    "new_length": 10,
                    "hunk": "@@ -241,6 +241,10 @@ class TestValueRanges(TestCase):\n             ValueRangeAnalysis.mul(ValueRanges.wrap(0), ValueRanges.unknown()),\n             ValueRanges.wrap(0),\n         )\n+        self.assertEqual(\n+            ValueRangeAnalysis.mul(ValueRanges.wrap(0.0), ValueRanges.unknown()),\n+            ValueRanges.wrap(0.0),\n+        )\n \n     @parametrize(\"fn\", UNARY_BOOL_OPS)\n     def test_unary_bool_ref_range(self, fn):\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        self.assertEqual(\n+            ValueRangeAnalysis.mul(ValueRanges.wrap(0.0), ValueRanges.unknown()),\n+            ValueRanges.wrap(0.0),\n+        )\n",
            "whole_hunk": "@@ -241,6 +241,10 @@ class TestValueRanges(TestCase):\n             ValueRangeAnalysis.mul(ValueRanges.wrap(0), ValueRanges.unknown()),\n             ValueRanges.wrap(0),\n         )\n+        self.assertEqual(\n+            ValueRangeAnalysis.mul(ValueRanges.wrap(0.0), ValueRanges.unknown()),\n+            ValueRanges.wrap(0.0),\n+        )\n \n     @parametrize(\"fn\", UNARY_BOOL_OPS)\n     def test_unary_bool_ref_range(self, fn):\n"
        },
        {
            "name": "value_ranges.py",
            "path": "torch/utils/_sympy/value_ranges.py",
            "patches": [
                {
                    "old_start": 519,
                    "old_length": 10,
                    "new_start": 519,
                    "new_length": 10,
                    "hunk": "@@ -519,10 +519,10 @@ class SymPyValueRangeAnalysis:\n             return cls.and_(a, b)\n \n         def safe_mul(a, b):\n-            # Make unknown() * wrap(0) == wrap(0)\n-            if a == 0:\n+            # Make unknown() * wrap(0.0) == wrap(0.0)\n+            if a == 0.0:\n                 return a\n-            elif b == 0:\n+            elif b == 0.0:\n                 return b\n             else:\n                 return a * b"
                }
            ],
            "whole_deleted": "-            # Make unknown() * wrap(0) == wrap(0)\n-            if a == 0:\n-            elif b == 0:\n",
            "whole_added": "+            # Make unknown() * wrap(0.0) == wrap(0.0)\n+            if a == 0.0:\n+            elif b == 0.0:\n",
            "whole_hunk": "@@ -519,10 +519,10 @@ class SymPyValueRangeAnalysis:\n             return cls.and_(a, b)\n \n         def safe_mul(a, b):\n-            # Make unknown() * wrap(0) == wrap(0)\n-            if a == 0:\n+            # Make unknown() * wrap(0.0) == wrap(0.0)\n+            if a == 0.0:\n                 return a\n-            elif b == 0:\n+            elif b == 0.0:\n                 return b\n             else:\n                 return a * b"
        }
    ]
},
{
    "Id": 275,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/06f8af30fa125ca3bec56982e53d09d77fa1ec69",
    "date": "2024-03-01T02:37:21+00:00",
    "message": "Change FakeTensor serialization to consider only an _active_ FakeTensor mode (#120848)\n\nSummary: https://github.com/pytorch/pytorch/pull/108186 make some changes related to FakeTensor serialization such that saving and loading a tensor will give us a meta tensor, even if FakeTensor mode is not enabled. This means we can't properly save and load Tensors as part of Fx graph caching. This PR changes the logic to check if there's an _active_ FakeTensor mode.\n\nTest Plan:\n* New unit tests\n* Validated unit tests introduced in https://github.com/pytorch/pytorch/pull/108186 still pass\nPull Request resolved: https://github.com/pytorch/pytorch/pull/120848\nApproved by: https://github.com/eellison, https://github.com/thiagocrepaldi",
    "label": "NO",
    "changes": [
        {
            "name": "test_fake_tensor.py",
            "path": "test/test_fake_tensor.py",
            "patches": [
                {
                    "old_start": 10,
                    "old_length": 6,
                    "new_start": 10,
                    "new_length": 7,
                    "hunk": "@@ -10,6 +10,7 @@ import torch._dynamo\n import itertools\n import numpy as np\n from torch.testing._internal.jit_utils import RUN_CUDA\n+from torch._guards import tracing, TracingContext\n from torch._subclasses.fake_tensor import (\n     _ShapeEnvSettings,\n     extract_tensor_metadata,\n"
                },
                {
                    "old_start": 18,
                    "old_length": 6,
                    "new_start": 19,
                    "new_length": 7,
                    "hunk": "@@ -18,6 +19,7 @@ from torch._subclasses.fake_tensor import (\n     FakeTensorConverter,\n     DynamicOutputShapeException,\n     UnsupportedOperatorException,\n+    unset_fake_temporarily,\n )\n from torch.fx.experimental.symbolic_shapes import ShapeEnv, DimDynamic, free_symbols, StatelessSymbolicContext\n from torch.testing._internal.custom_op_db import custom_op_db\n"
                },
                {
                    "old_start": 35,
                    "old_length": 6,
                    "new_start": 37,
                    "new_length": 7,
                    "hunk": "@@ -35,6 +37,7 @@ import torch._prims as prims\n import contextlib\n import weakref\n import copy\n+import pickle\n import torch._functorch.config\n import torch.testing._internal.optests as optests\n from unittest.mock import patch\n"
                },
                {
                    "old_start": 1323,
                    "old_length": 6,
                    "new_start": 1326,
                    "new_length": 25,
                    "hunk": "@@ -1323,6 +1326,25 @@ class FakeTensorPropTest(TestCase):\n                 torch.load(state_dict_file, map_location=\"cpu\")  # scenario 2\n \n \n+class FakeTensorSerialization(TestCase):\n+    def test_serialization(self):\n+        x = torch.tensor([0], device=\"cpu\")\n+        with FakeTensorMode():\n+            y = pickle.loads(pickle.dumps(x))\n+            self.assertEqual(type(y), FakeTensor)\n+            self.assertEqual(y.device.type, \"meta\")\n+\n+            with unset_fake_temporarily():\n+                y = pickle.loads(pickle.dumps(x))\n+                self.assertEqual(x.device, y.device)\n+\n+    def test_serialization_with_tracing(self):\n+        x = torch.tensor([0], device=\"cpu\")\n+        with tracing(TracingContext(FakeTensorMode())):\n+            y = pickle.loads(pickle.dumps(x))\n+            self.assertEqual(x.device, y.device)\n+\n+\n class FakeTensorDispatchCache(TestCase):\n     def test_shape_env_settings(self):\n         \"\"\"\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+from torch._guards import tracing, TracingContext\n+    unset_fake_temporarily,\n+import pickle\n+class FakeTensorSerialization(TestCase):\n+    def test_serialization(self):\n+        x = torch.tensor([0], device=\"cpu\")\n+        with FakeTensorMode():\n+            y = pickle.loads(pickle.dumps(x))\n+            self.assertEqual(type(y), FakeTensor)\n+            self.assertEqual(y.device.type, \"meta\")\n+\n+            with unset_fake_temporarily():\n+                y = pickle.loads(pickle.dumps(x))\n+                self.assertEqual(x.device, y.device)\n+\n+    def test_serialization_with_tracing(self):\n+        x = torch.tensor([0], device=\"cpu\")\n+        with tracing(TracingContext(FakeTensorMode())):\n+            y = pickle.loads(pickle.dumps(x))\n+            self.assertEqual(x.device, y.device)\n+\n+\n",
            "whole_hunk": "@@ -10,6 +10,7 @@ import torch._dynamo\n import itertools\n import numpy as np\n from torch.testing._internal.jit_utils import RUN_CUDA\n+from torch._guards import tracing, TracingContext\n from torch._subclasses.fake_tensor import (\n     _ShapeEnvSettings,\n     extract_tensor_metadata,\n@@ -18,6 +19,7 @@ from torch._subclasses.fake_tensor import (\n     FakeTensorConverter,\n     DynamicOutputShapeException,\n     UnsupportedOperatorException,\n+    unset_fake_temporarily,\n )\n from torch.fx.experimental.symbolic_shapes import ShapeEnv, DimDynamic, free_symbols, StatelessSymbolicContext\n from torch.testing._internal.custom_op_db import custom_op_db\n@@ -35,6 +37,7 @@ import torch._prims as prims\n import contextlib\n import weakref\n import copy\n+import pickle\n import torch._functorch.config\n import torch.testing._internal.optests as optests\n from unittest.mock import patch\n@@ -1323,6 +1326,25 @@ class FakeTensorPropTest(TestCase):\n                 torch.load(state_dict_file, map_location=\"cpu\")  # scenario 2\n \n \n+class FakeTensorSerialization(TestCase):\n+    def test_serialization(self):\n+        x = torch.tensor([0], device=\"cpu\")\n+        with FakeTensorMode():\n+            y = pickle.loads(pickle.dumps(x))\n+            self.assertEqual(type(y), FakeTensor)\n+            self.assertEqual(y.device.type, \"meta\")\n+\n+            with unset_fake_temporarily():\n+                y = pickle.loads(pickle.dumps(x))\n+                self.assertEqual(x.device, y.device)\n+\n+    def test_serialization_with_tracing(self):\n+        x = torch.tensor([0], device=\"cpu\")\n+        with tracing(TracingContext(FakeTensorMode())):\n+            y = pickle.loads(pickle.dumps(x))\n+            self.assertEqual(x.device, y.device)\n+\n+\n class FakeTensorDispatchCache(TestCase):\n     def test_shape_env_settings(self):\n         \"\"\"\n"
        },
        {
            "name": "_guards.py",
            "path": "torch/_guards.py",
            "patches": [
                {
                    "old_start": 846,
                    "old_length": 3,
                    "new_start": 846,
                    "new_length": 18,
                    "hunk": "@@ -846,3 +846,18 @@ def detect_fake_mode(inputs: Any = None):\n         return fake_mode\n     else:\n         return None\n+\n+\n+def active_fake_mode():\n+    \"\"\"\n+    Inspects the dispatch mode stack for an active fake mode and returns it.\n+    Returns None if no fake mode is active.\n+    \"\"\"\n+    from torch._subclasses.fake_tensor import FakeTensorMode\n+    from torch.utils._python_dispatch import _get_current_dispatch_mode_stack\n+\n+    for _, m in enumerate(reversed(_get_current_dispatch_mode_stack())):\n+        if isinstance(m, FakeTensorMode):\n+            return m\n+\n+    return None\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+\n+\n+def active_fake_mode():\n+    \"\"\"\n+    Inspects the dispatch mode stack for an active fake mode and returns it.\n+    Returns None if no fake mode is active.\n+    \"\"\"\n+    from torch._subclasses.fake_tensor import FakeTensorMode\n+    from torch.utils._python_dispatch import _get_current_dispatch_mode_stack\n+\n+    for _, m in enumerate(reversed(_get_current_dispatch_mode_stack())):\n+        if isinstance(m, FakeTensorMode):\n+            return m\n+\n+    return None\n",
            "whole_hunk": "@@ -846,3 +846,18 @@ def detect_fake_mode(inputs: Any = None):\n         return fake_mode\n     else:\n         return None\n+\n+\n+def active_fake_mode():\n+    \"\"\"\n+    Inspects the dispatch mode stack for an active fake mode and returns it.\n+    Returns None if no fake mode is active.\n+    \"\"\"\n+    from torch._subclasses.fake_tensor import FakeTensorMode\n+    from torch.utils._python_dispatch import _get_current_dispatch_mode_stack\n+\n+    for _, m in enumerate(reversed(_get_current_dispatch_mode_stack())):\n+        if isinstance(m, FakeTensorMode):\n+            return m\n+\n+    return None\n"
        },
        {
            "name": "serialization.py",
            "path": "torch/serialization.py",
            "patches": [
                {
                    "old_start": 1196,
                    "old_length": 7,
                    "new_start": 1196,
                    "new_length": 7,
                    "hunk": "@@ -1196,7 +1196,7 @@ def _legacy_load(f, map_location, pickle_module, **pickle_load_args):\n             nbytes = numel * torch._utils._element_size(dtype)\n \n             if root_key not in deserialized_objects:\n-                if torch._guards.detect_fake_mode(None) is not None:\n+                if torch._guards.active_fake_mode() is not None:\n                     obj = cast(Storage, torch.UntypedStorage(nbytes, device='meta'))\n                 else:\n                     obj = cast(Storage, torch.UntypedStorage(nbytes))\n"
                },
                {
                    "old_start": 1272,
                    "old_length": 7,
                    "new_start": 1272,
                    "new_length": 7,
                    "hunk": "@@ -1272,7 +1272,7 @@ def _legacy_load(f, map_location, pickle_module, **pickle_load_args):\n \n     deserialized_storage_keys = pickle_module.load(f, **pickle_load_args)\n \n-    if torch._guards.detect_fake_mode(None) is None:\n+    if torch._guards.active_fake_mode() is None:\n         offset = f.tell() if f_should_read_directly else None\n         for key in deserialized_storage_keys:\n             assert key in deserialized_objects"
                }
            ],
            "whole_deleted": "-                if torch._guards.detect_fake_mode(None) is not None:\n-    if torch._guards.detect_fake_mode(None) is None:\n",
            "whole_added": "+                if torch._guards.active_fake_mode() is not None:\n+    if torch._guards.active_fake_mode() is None:\n",
            "whole_hunk": "@@ -1196,7 +1196,7 @@ def _legacy_load(f, map_location, pickle_module, **pickle_load_args):\n             nbytes = numel * torch._utils._element_size(dtype)\n \n             if root_key not in deserialized_objects:\n-                if torch._guards.detect_fake_mode(None) is not None:\n+                if torch._guards.active_fake_mode() is not None:\n                     obj = cast(Storage, torch.UntypedStorage(nbytes, device='meta'))\n                 else:\n                     obj = cast(Storage, torch.UntypedStorage(nbytes))\n@@ -1272,7 +1272,7 @@ def _legacy_load(f, map_location, pickle_module, **pickle_load_args):\n \n     deserialized_storage_keys = pickle_module.load(f, **pickle_load_args)\n \n-    if torch._guards.detect_fake_mode(None) is None:\n+    if torch._guards.active_fake_mode() is None:\n         offset = f.tell() if f_should_read_directly else None\n         for key in deserialized_storage_keys:\n             assert key in deserialized_objects"
        }
    ]
},
{
    "Id": 22,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/1cae60a87e5bdda8bcf55724a862eeed98a9747e",
    "date": "2024-07-11T18:21:35+00:00",
    "message": "Caching attr_proxy for nn_module attribute to fix guard check failure (#130280)\n\nFixes https://github.com/pytorch/pytorch/issues/129939\n\nDifferential Revision: [D59594605](https://our.internmc.facebook.com/intern/diff/D59594605)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130280\nApproved by: https://github.com/anijain2305",
    "label": "YES",
    "changes": [
        {
            "name": "test_export.py",
            "path": "test/export/test_export.py",
            "patches": [
                {
                    "old_start": 3625,
                    "old_length": 8,
                    "new_start": 3625,
                    "new_length": 6,
                    "hunk": "@@ -3625,8 +3625,6 @@ def forward(self, x):\n         ):\n             torch.export.export(exported_v2.module(), (torch.randn(2, 2),))\n \n-    # https://github.com/pytorch/pytorch/issues/129939\n-    @testing.expectedFailureNonStrict\n     def test_export_cond(self):\n         class A(torch.nn.Module):\n             def __init__(self):\n"
                },
                {
                    "old_start": 4987,
                    "old_length": 8,
                    "new_start": 4985,
                    "new_length": 6,
                    "hunk": "@@ -4987,8 +4985,6 @@ graph():\n         )\n \n     # Guard validation upsets the guard\n-    # https://github.com/pytorch/pytorch/issues/129939\n-    @unittest.expectedFailure\n     def test_cond_with_module_stack_export_with(self):\n         class Bar(torch.nn.Module):\n             def __init__(self):\n"
                }
            ],
            "whole_deleted": "-    # https://github.com/pytorch/pytorch/issues/129939\n-    @testing.expectedFailureNonStrict\n-    # https://github.com/pytorch/pytorch/issues/129939\n-    @unittest.expectedFailure\n",
            "whole_added": "",
            "whole_hunk": "@@ -3625,8 +3625,6 @@ def forward(self, x):\n         ):\n             torch.export.export(exported_v2.module(), (torch.randn(2, 2),))\n \n-    # https://github.com/pytorch/pytorch/issues/129939\n-    @testing.expectedFailureNonStrict\n     def test_export_cond(self):\n         class A(torch.nn.Module):\n             def __init__(self):\n@@ -4987,8 +4985,6 @@ graph():\n         )\n \n     # Guard validation upsets the guard\n-    # https://github.com/pytorch/pytorch/issues/129939\n-    @unittest.expectedFailure\n     def test_cond_with_module_stack_export_with(self):\n         class Bar(torch.nn.Module):\n             def __init__(self):\n"
        },
        {
            "name": "guards.py",
            "path": "torch/_dynamo/guards.py",
            "patches": [
                {
                    "old_start": 102,
                    "old_length": 6,
                    "new_start": 102,
                    "new_length": 7,
                    "hunk": "@@ -102,6 +102,7 @@ from .types import CacheEntry, ExtraState, GuardedCode, GuardFail, GuardFn  # no\n from .utils import (\n     common_constant_types,\n     dict_keys_repr,\n+    get_custom_getattr,\n     guard_failures,\n     istype,\n     key_is_id,\n"
                },
                {
                    "old_start": 110,
                    "old_length": 6,
                    "new_start": 111,
                    "new_length": 7,
                    "hunk": "@@ -110,6 +111,7 @@ from .utils import (\n     tensor_always_has_static_shape,\n     tuple_iterator_getitem,\n     tuple_iterator_len,\n+    unpatched_nn_module_getattr,\n )\n \n if TYPE_CHECKING:\n"
                },
                {
                    "old_start": 877,
                    "old_length": 7,
                    "new_start": 879,
                    "new_length": 11,
                    "hunk": "@@ -877,7 +879,11 @@ class GuardBuilder(GuardBuilderBase):\n         elif istype(source, AttrSource):\n             assert base_guard_manager  # to make mypy happy\n \n-            if isinstance(base_example_value, torch.nn.Module):\n+            if (\n+                isinstance(base_example_value, torch.nn.Module)\n+                and get_custom_getattr(base_example_value)\n+                is unpatched_nn_module_getattr\n+            ):\n                 out = self.getattr_on_nn_module(\n                     source,\n                     base_guard_manager,\n"
                },
                {
                    "old_start": 1162,
                    "old_length": 7,
                    "new_start": 1168,
                    "new_length": 11,
                    "hunk": "@@ -1162,7 +1168,11 @@ class GuardBuilder(GuardBuilderBase):\n \n                 # if the base value is nn.Module, check if we can speedup the\n                 # guard by going through __dict__ attrs.\n-                if isinstance(base_example_value, torch.nn.Module):\n+                if (\n+                    isinstance(base_example_value, torch.nn.Module)\n+                    and get_custom_getattr(base_example_value)\n+                    is unpatched_nn_module_getattr\n+                ):\n                     return self.getattr_on_nn_module(\n                         source,\n                         base_manager,\n"
                }
            ],
            "whole_deleted": "-            if isinstance(base_example_value, torch.nn.Module):\n-                if isinstance(base_example_value, torch.nn.Module):\n",
            "whole_added": "+    get_custom_getattr,\n+    unpatched_nn_module_getattr,\n+            if (\n+                isinstance(base_example_value, torch.nn.Module)\n+                and get_custom_getattr(base_example_value)\n+                is unpatched_nn_module_getattr\n+            ):\n+                if (\n+                    isinstance(base_example_value, torch.nn.Module)\n+                    and get_custom_getattr(base_example_value)\n+                    is unpatched_nn_module_getattr\n+                ):\n",
            "whole_hunk": "@@ -102,6 +102,7 @@ from .types import CacheEntry, ExtraState, GuardedCode, GuardFail, GuardFn  # no\n from .utils import (\n     common_constant_types,\n     dict_keys_repr,\n+    get_custom_getattr,\n     guard_failures,\n     istype,\n     key_is_id,\n@@ -110,6 +111,7 @@ from .utils import (\n     tensor_always_has_static_shape,\n     tuple_iterator_getitem,\n     tuple_iterator_len,\n+    unpatched_nn_module_getattr,\n )\n \n if TYPE_CHECKING:\n@@ -877,7 +879,11 @@ class GuardBuilder(GuardBuilderBase):\n         elif istype(source, AttrSource):\n             assert base_guard_manager  # to make mypy happy\n \n-            if isinstance(base_example_value, torch.nn.Module):\n+            if (\n+                isinstance(base_example_value, torch.nn.Module)\n+                and get_custom_getattr(base_example_value)\n+                is unpatched_nn_module_getattr\n+            ):\n                 out = self.getattr_on_nn_module(\n                     source,\n                     base_guard_manager,\n@@ -1162,7 +1168,11 @@ class GuardBuilder(GuardBuilderBase):\n \n                 # if the base value is nn.Module, check if we can speedup the\n                 # guard by going through __dict__ attrs.\n-                if isinstance(base_example_value, torch.nn.Module):\n+                if (\n+                    isinstance(base_example_value, torch.nn.Module)\n+                    and get_custom_getattr(base_example_value)\n+                    is unpatched_nn_module_getattr\n+                ):\n                     return self.getattr_on_nn_module(\n                         source,\n                         base_manager,\n"
        },
        {
            "name": "proxy_tensor.py",
            "path": "torch/fx/experimental/proxy_tensor.py",
            "patches": [
                {
                    "old_start": 956,
                    "old_length": 6,
                    "new_start": 956,
                    "new_length": 7,
                    "hunk": "@@ -956,6 +956,7 @@ class _ModuleStackTracer(PythonKeyTracer):\n         super().__init__()\n         self.scope_root = scope_root\n         self.proxy_paths = WeakKeyDictionary()\n+        self.attr_proxy_map = WeakKeyDictionary()\n         self.proxy_modules = WeakKeyDictionary()\n         self.counter = 0\n \n"
                },
                {
                    "old_start": 967,
                    "old_length": 6,
                    "new_start": 968,
                    "new_length": 7,
                    "hunk": "@@ -967,6 +968,7 @@ class _ModuleStackTracer(PythonKeyTracer):\n \n         class AttrProxy:\n             def __init__(self, base, path):\n+                # Class is modified to be a subclass of torch.nn.Module\n                 self.__class__ = type(\n                     base.__class__.__name__,\n                     (self.__class__, base.__class__),\n"
                },
                {
                    "old_start": 975,
                    "old_length": 17,
                    "new_start": 977,
                    "new_length": 33,
                    "hunk": "@@ -975,17 +977,33 @@ class _ModuleStackTracer(PythonKeyTracer):\n                 self.__dict__ = base.__dict__\n                 self.__class__.__module__ = base.__class__.__module__\n                 self.__class__.__qualname__ = base.__class__.__qualname__\n+                self.reset_proxy_mapping(base, path)\n+\n+            def reset_proxy_mapping(self, base, path):\n                 self_.proxy_paths[self] = path\n                 self_.proxy_modules[self] = base\n \n             def __getattr__(self, name):\n                 assert isinstance(self, torch.nn.Module)\n+                # Calling into torch.nn.Module.__getattr__ with super(),\n+                # That __getattr__ is patched to be module_getattr_wrapper in _symbolic_trace.py.\n+                # which then calls into _ModuleStackTracer.getattr\n                 attr_val = super().__getattr__(name)\n                 if isinstance(attr_val, AttrProxy):\n                     attr_val = self_.proxy_modules[attr_val]\n                 elif not isinstance(attr_val, torch.nn.Module):\n                     return attr_val\n-                return AttrProxy(attr_val, self_.proxy_paths[self] + \".\" + name)\n+                if attr_val not in self_.attr_proxy_map:\n+                    self_.attr_proxy_map[attr_val] = AttrProxy(attr_val, self_.proxy_paths[self] + \".\" + name)\n+                else:\n+                    # NOTE [caching AttrProxy]. Caching ensures a 1-1 mapping between AttrProxy and the actual attr_val.\n+                    # 1. We reset the proxy_mapping to solve the diamond shape reference problem: we want to record the\n+                    # path as A.B.D instead of A.C.D (the purpose of _ModuleStackTracer).\n+                    # 2. Instead of creating a new AttrProxy, we just reset the proxy_mapping of existing one. This is to avoid\n+                    # dynamo creating multiple guards for the same attr_val but different AttrProxy when exporting\n+                    # a model that calls torch.compile (e.g when a model uses torch.cond.)\n+                    self_.attr_proxy_map[attr_val].reset_proxy_mapping(attr_val, self_.proxy_paths[self] + \".\" + name)\n+                return self_.attr_proxy_map[attr_val]\n \n             @property\n             def _modules(self):\n"
                },
                {
                    "old_start": 1020,
                    "old_length": 7,
                    "new_start": 1038,
                    "new_length": 13,
                    "hunk": "@@ -1020,7 +1038,13 @@ class _ModuleStackTracer(PythonKeyTracer):\n             return super().getattr(attr, attr_val, parameter_proxy_cache)\n         if isinstance(attr_val, self.proxy_type):\n             return attr_val\n-        return self.proxy_type(attr_val, attr)\n+\n+        # See NOTE [caching AttrProxy].\n+        if attr_val not in self.attr_proxy_map:\n+            self.attr_proxy_map[attr_val] = self.proxy_type(attr_val, attr)\n+        else:\n+            self.attr_proxy_map[attr_val].reset_proxy_mapping(attr_val, attr)\n+        return self.attr_proxy_map[attr_val]\n \n     def trace(self, root, concrete_args):\n         res = super().trace(root, concrete_args)"
                }
            ],
            "whole_deleted": "-                return AttrProxy(attr_val, self_.proxy_paths[self] + \".\" + name)\n-        return self.proxy_type(attr_val, attr)\n",
            "whole_added": "+        self.attr_proxy_map = WeakKeyDictionary()\n+                # Class is modified to be a subclass of torch.nn.Module\n+                self.reset_proxy_mapping(base, path)\n+\n+            def reset_proxy_mapping(self, base, path):\n+                # Calling into torch.nn.Module.__getattr__ with super(),\n+                # That __getattr__ is patched to be module_getattr_wrapper in _symbolic_trace.py.\n+                # which then calls into _ModuleStackTracer.getattr\n+                if attr_val not in self_.attr_proxy_map:\n+                    self_.attr_proxy_map[attr_val] = AttrProxy(attr_val, self_.proxy_paths[self] + \".\" + name)\n+                else:\n+                    # NOTE [caching AttrProxy]. Caching ensures a 1-1 mapping between AttrProxy and the actual attr_val.\n+                    # 1. We reset the proxy_mapping to solve the diamond shape reference problem: we want to record the\n+                    # path as A.B.D instead of A.C.D (the purpose of _ModuleStackTracer).\n+                    # 2. Instead of creating a new AttrProxy, we just reset the proxy_mapping of existing one. This is to avoid\n+                    # dynamo creating multiple guards for the same attr_val but different AttrProxy when exporting\n+                    # a model that calls torch.compile (e.g when a model uses torch.cond.)\n+                    self_.attr_proxy_map[attr_val].reset_proxy_mapping(attr_val, self_.proxy_paths[self] + \".\" + name)\n+                return self_.attr_proxy_map[attr_val]\n+\n+        # See NOTE [caching AttrProxy].\n+        if attr_val not in self.attr_proxy_map:\n+            self.attr_proxy_map[attr_val] = self.proxy_type(attr_val, attr)\n+        else:\n+            self.attr_proxy_map[attr_val].reset_proxy_mapping(attr_val, attr)\n+        return self.attr_proxy_map[attr_val]\n",
            "whole_hunk": "@@ -956,6 +956,7 @@ class _ModuleStackTracer(PythonKeyTracer):\n         super().__init__()\n         self.scope_root = scope_root\n         self.proxy_paths = WeakKeyDictionary()\n+        self.attr_proxy_map = WeakKeyDictionary()\n         self.proxy_modules = WeakKeyDictionary()\n         self.counter = 0\n \n@@ -967,6 +968,7 @@ class _ModuleStackTracer(PythonKeyTracer):\n \n         class AttrProxy:\n             def __init__(self, base, path):\n+                # Class is modified to be a subclass of torch.nn.Module\n                 self.__class__ = type(\n                     base.__class__.__name__,\n                     (self.__class__, base.__class__),\n@@ -975,17 +977,33 @@ class _ModuleStackTracer(PythonKeyTracer):\n                 self.__dict__ = base.__dict__\n                 self.__class__.__module__ = base.__class__.__module__\n                 self.__class__.__qualname__ = base.__class__.__qualname__\n+                self.reset_proxy_mapping(base, path)\n+\n+            def reset_proxy_mapping(self, base, path):\n                 self_.proxy_paths[self] = path\n                 self_.proxy_modules[self] = base\n \n             def __getattr__(self, name):\n                 assert isinstance(self, torch.nn.Module)\n+                # Calling into torch.nn.Module.__getattr__ with super(),\n+                # That __getattr__ is patched to be module_getattr_wrapper in _symbolic_trace.py.\n+                # which then calls into _ModuleStackTracer.getattr\n                 attr_val = super().__getattr__(name)\n                 if isinstance(attr_val, AttrProxy):\n                     attr_val = self_.proxy_modules[attr_val]\n                 elif not isinstance(attr_val, torch.nn.Module):\n                     return attr_val\n-                return AttrProxy(attr_val, self_.proxy_paths[self] + \".\" + name)\n+                if attr_val not in self_.attr_proxy_map:\n+                    self_.attr_proxy_map[attr_val] = AttrProxy(attr_val, self_.proxy_paths[self] + \".\" + name)\n+                else:\n+                    # NOTE [caching AttrProxy]. Caching ensures a 1-1 mapping between AttrProxy and the actual attr_val.\n+                    # 1. We reset the proxy_mapping to solve the diamond shape reference problem: we want to record the\n+                    # path as A.B.D instead of A.C.D (the purpose of _ModuleStackTracer).\n+                    # 2. Instead of creating a new AttrProxy, we just reset the proxy_mapping of existing one. This is to avoid\n+                    # dynamo creating multiple guards for the same attr_val but different AttrProxy when exporting\n+                    # a model that calls torch.compile (e.g when a model uses torch.cond.)\n+                    self_.attr_proxy_map[attr_val].reset_proxy_mapping(attr_val, self_.proxy_paths[self] + \".\" + name)\n+                return self_.attr_proxy_map[attr_val]\n \n             @property\n             def _modules(self):\n@@ -1020,7 +1038,13 @@ class _ModuleStackTracer(PythonKeyTracer):\n             return super().getattr(attr, attr_val, parameter_proxy_cache)\n         if isinstance(attr_val, self.proxy_type):\n             return attr_val\n-        return self.proxy_type(attr_val, attr)\n+\n+        # See NOTE [caching AttrProxy].\n+        if attr_val not in self.attr_proxy_map:\n+            self.attr_proxy_map[attr_val] = self.proxy_type(attr_val, attr)\n+        else:\n+            self.attr_proxy_map[attr_val].reset_proxy_mapping(attr_val, attr)\n+        return self.attr_proxy_map[attr_val]\n \n     def trace(self, root, concrete_args):\n         res = super().trace(root, concrete_args)"
        }
    ]
},
{
    "Id": 369,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/be9de33240424c85cbd8dd11bd285830e8ad9b36",
    "date": "2023-12-21T01:35:07+00:00",
    "message": "[Dynamo][9/N] Make SkipFilesVariable wrap functions only (#115963)\n\nMake ```SkipFilesVariable``` only handle function type, and route skipped classes to ```UserDefinedClassVariable```. The reasons behind this are:\n* We'd like to remove ```is_allowed```, so the allowed/disallowed torch classes should have a proper place to handle. We can put them in either ```SkipFilesVariable``` and ```UserDefinedClassVariable``` under the current architecture, but it's  confusing to have two places do one thing.\n   - Going forward, let's make ```SkipFilesVariable``` only handle functions, and probably I'll rename it to ```SkippedFunctionVariable``` in the following PRs.\n   - Let's do dispatch by value's type, all torch classes stuff would go to ```UserDefinedClassVariable``` in the next PR.\n* We'd merge in_graph/skip/inline trace decision into the same API ```trace_rule.lookup```, so probably we have to limit the input to only function for better organizing ```VariableBuilder._wrap``` logics.\n   - Next step, I'll merge ```skipfiles.check``` into ```trace_rules.lookup```, and do the skipfile check before wrapping them into correct variable tracker.\n   - Though the ```TorchCtxManagerClassVariable``` is decided by ```trace_rules.lookup```, I'll refactor it out in the following PRs.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115963\nApproved by: https://github.com/jansel",
    "label": "NO",
    "changes": [
        {
            "name": "utils.py",
            "path": "torch/_dynamo/utils.py",
            "patches": [
                {
                    "old_start": 519,
                    "old_length": 7,
                    "new_start": 519,
                    "new_length": 6,
                    "hunk": "@@ -519,7 +519,6 @@ def is_function(value):\n         value,\n         (\n             types.FunctionType,\n-            types.MethodType,\n             types.BuiltinFunctionType,\n             types.MethodDescriptorType,\n             types.WrapperDescriptorType,\n"
                }
            ],
            "whole_deleted": "-            types.MethodType,\n",
            "whole_added": "",
            "whole_hunk": "@@ -519,7 +519,6 @@ def is_function(value):\n         value,\n         (\n             types.FunctionType,\n-            types.MethodType,\n             types.BuiltinFunctionType,\n             types.MethodDescriptorType,\n             types.WrapperDescriptorType,\n"
        },
        {
            "name": "builder.py",
            "path": "torch/_dynamo/variables/builder.py",
            "patches": [
                {
                    "old_start": 68,
                    "old_length": 6,
                    "new_start": 68,
                    "new_length": 7,
                    "hunk": "@@ -68,6 +68,7 @@ from ..utils import (\n     get_fake_value,\n     get_static_address_type,\n     global_key_name,\n+    is_function,\n     is_namedtuple,\n     is_typing,\n     is_utils_checkpoint,\n"
                },
                {
                    "old_start": 728,
                    "old_length": 7,
                    "new_start": 729,
                    "new_length": 7,
                    "hunk": "@@ -728,7 +729,7 @@ class VariableBuilder:\n                 source=self.source,\n             )\n         elif (\n-            istype(value, (type, types.FunctionType))\n+            is_function(value)\n             and skipfiles.check(value, is_inlined_call=True)\n             and not inspect.getattr_static(value, \"_torchdynamo_inline\", False)\n             and not inspect.getattr_static(value, \"__script_if_tracing_wrapper\", False)\n"
                }
            ],
            "whole_deleted": "-            istype(value, (type, types.FunctionType))\n",
            "whole_added": "+    is_function,\n+            is_function(value)\n",
            "whole_hunk": "@@ -68,6 +68,7 @@ from ..utils import (\n     get_fake_value,\n     get_static_address_type,\n     global_key_name,\n+    is_function,\n     is_namedtuple,\n     is_typing,\n     is_utils_checkpoint,\n@@ -728,7 +729,7 @@ class VariableBuilder:\n                 source=self.source,\n             )\n         elif (\n-            istype(value, (type, types.FunctionType))\n+            is_function(value)\n             and skipfiles.check(value, is_inlined_call=True)\n             and not inspect.getattr_static(value, \"_torchdynamo_inline\", False)\n             and not inspect.getattr_static(value, \"__script_if_tracing_wrapper\", False)\n"
        },
        {
            "name": "misc.py",
            "path": "torch/_dynamo/variables/misc.py",
            "patches": [
                {
                    "old_start": 21,
                    "old_length": 7,
                    "new_start": 21,
                    "new_length": 6,
                    "hunk": "@@ -21,7 +21,6 @@ from ..utils import (\n     proxy_args_kwargs,\n )\n from .base import MutableLocal, VariableTracker\n-from .dicts import DefaultDictVariable\n from .functions import (\n     NestedUserFunctionVariable,\n     UserFunctionVariable,\n"
                },
                {
                    "old_start": 743,
                    "old_length": 26,
                    "new_start": 742,
                    "new_length": 8,
                    "hunk": "@@ -743,26 +742,8 @@ class SkipFilesVariable(VariableTracker):\n     def call_function(\n         self, tx, args: \"List[VariableTracker]\", kwargs: \"Dict[str, VariableTracker]\"\n     ) -> \"VariableTracker\":\n-        from .builtin import BuiltinVariable\n-\n         if inspect.getattr_static(self.value, \"_torchdynamo_disable\", False):\n             unimplemented(f\"call torch._dynamo.disable() wrapped function {self.value}\")\n-        # Allowlist a few popular classes(e.g, collections.OrderedDict) calls in skip files.\n-        elif self.value is collections.OrderedDict:\n-            return BuiltinVariable.call_custom_dict(\n-                tx, collections.OrderedDict, *args, **kwargs\n-            )\n-        elif (\n-            self.value is collections.defaultdict\n-            and len(args) <= 1\n-            and DefaultDictVariable.is_supported_arg(args[0])\n-        ):\n-            return DefaultDictVariable(\n-                {},\n-                collections.defaultdict,\n-                args[0],\n-                mutable_local=MutableLocal(),\n-            )\n         # Fold through the functions(e.g, collections.namedtuple)\n         # that inputs & outputs are all python constants\n         elif (\n"
                },
                {
                    "old_start": 795,
                    "old_length": 25,
                    "new_start": 776,
                    "new_length": 6,
                    "hunk": "@@ -795,25 +776,6 @@ class SkipFilesVariable(VariableTracker):\n                 unimplemented(f\"functools.wraps({fn})\")\n \n             return variables.LambdaVariable(wraps)\n-        elif self.value is collections.deque and not kwargs:\n-            if len(args) == 0:\n-                items = []\n-            elif len(args) == 1 and args[0].has_unpack_var_sequence(tx):\n-                items = args[0].unpack_var_sequence(tx)\n-            else:\n-                unimplemented(\"deque() with more than 1 arg not supported\")\n-            return variables.lists.DequeVariable(items, mutable_local=MutableLocal())\n-        elif self.value is functools.partial:\n-            if not args:\n-                unimplemented(\"functools.partial malformed\")\n-            # The first arg, a callable (the ctor below will assert on types)\n-            fn = args[0]\n-            rest_args = args[1:]\n-            # guards for the produced FunctoolsPartialVariable are installed in FunctoolsPartialVariable ctor from the\n-            # args and keywords\n-            return variables.functions.FunctoolsPartialVariable(\n-                fn, args=rest_args, keywords=kwargs\n-            )\n         else:\n             try:\n                 path = inspect.getfile(self.value)\n"
                },
                {
                    "old_start": 823,
                    "old_length": 24,
                    "new_start": 785,
                    "new_length": 6,
                    "hunk": "@@ -823,24 +785,6 @@ class SkipFilesVariable(VariableTracker):\n             msg += f\"', {self.reason}'\" if self.reason else \"\"\n             unimplemented(msg)\n \n-    def call_method(\n-        self,\n-        tx,\n-        name,\n-        args: \"List[VariableTracker]\",\n-        kwargs: \"Dict[str, VariableTracker]\",\n-    ) -> \"VariableTracker\":\n-        if (\n-            self.value in {collections.OrderedDict, collections.defaultdict}\n-            and name == \"fromkeys\"\n-        ):\n-            from .builtin import BuiltinVariable\n-\n-            return BuiltinVariable.call_custom_dict_fromkeys(\n-                tx, self.value, *args, **kwargs\n-            )\n-        return super().call_method(tx, name, args, kwargs)\n-\n \n class TypingVariable(VariableTracker):\n     def __init__(self, value, **kwargs):\n"
                }
            ],
            "whole_deleted": "-from .dicts import DefaultDictVariable\n-        from .builtin import BuiltinVariable\n-\n-        # Allowlist a few popular classes(e.g, collections.OrderedDict) calls in skip files.\n-        elif self.value is collections.OrderedDict:\n-            return BuiltinVariable.call_custom_dict(\n-                tx, collections.OrderedDict, *args, **kwargs\n-            )\n-        elif (\n-            self.value is collections.defaultdict\n-            and len(args) <= 1\n-            and DefaultDictVariable.is_supported_arg(args[0])\n-        ):\n-            return DefaultDictVariable(\n-                {},\n-                collections.defaultdict,\n-                args[0],\n-                mutable_local=MutableLocal(),\n-            )\n-        elif self.value is collections.deque and not kwargs:\n-            if len(args) == 0:\n-                items = []\n-            elif len(args) == 1 and args[0].has_unpack_var_sequence(tx):\n-                items = args[0].unpack_var_sequence(tx)\n-            else:\n-                unimplemented(\"deque() with more than 1 arg not supported\")\n-            return variables.lists.DequeVariable(items, mutable_local=MutableLocal())\n-        elif self.value is functools.partial:\n-            if not args:\n-                unimplemented(\"functools.partial malformed\")\n-            # The first arg, a callable (the ctor below will assert on types)\n-            fn = args[0]\n-            rest_args = args[1:]\n-            # guards for the produced FunctoolsPartialVariable are installed in FunctoolsPartialVariable ctor from the\n-            # args and keywords\n-            return variables.functions.FunctoolsPartialVariable(\n-                fn, args=rest_args, keywords=kwargs\n-            )\n-    def call_method(\n-        self,\n-        tx,\n-        name,\n-        args: \"List[VariableTracker]\",\n-        kwargs: \"Dict[str, VariableTracker]\",\n-    ) -> \"VariableTracker\":\n-        if (\n-            self.value in {collections.OrderedDict, collections.defaultdict}\n-            and name == \"fromkeys\"\n-        ):\n-            from .builtin import BuiltinVariable\n-\n-            return BuiltinVariable.call_custom_dict_fromkeys(\n-                tx, self.value, *args, **kwargs\n-            )\n-        return super().call_method(tx, name, args, kwargs)\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -21,7 +21,6 @@ from ..utils import (\n     proxy_args_kwargs,\n )\n from .base import MutableLocal, VariableTracker\n-from .dicts import DefaultDictVariable\n from .functions import (\n     NestedUserFunctionVariable,\n     UserFunctionVariable,\n@@ -743,26 +742,8 @@ class SkipFilesVariable(VariableTracker):\n     def call_function(\n         self, tx, args: \"List[VariableTracker]\", kwargs: \"Dict[str, VariableTracker]\"\n     ) -> \"VariableTracker\":\n-        from .builtin import BuiltinVariable\n-\n         if inspect.getattr_static(self.value, \"_torchdynamo_disable\", False):\n             unimplemented(f\"call torch._dynamo.disable() wrapped function {self.value}\")\n-        # Allowlist a few popular classes(e.g, collections.OrderedDict) calls in skip files.\n-        elif self.value is collections.OrderedDict:\n-            return BuiltinVariable.call_custom_dict(\n-                tx, collections.OrderedDict, *args, **kwargs\n-            )\n-        elif (\n-            self.value is collections.defaultdict\n-            and len(args) <= 1\n-            and DefaultDictVariable.is_supported_arg(args[0])\n-        ):\n-            return DefaultDictVariable(\n-                {},\n-                collections.defaultdict,\n-                args[0],\n-                mutable_local=MutableLocal(),\n-            )\n         # Fold through the functions(e.g, collections.namedtuple)\n         # that inputs & outputs are all python constants\n         elif (\n@@ -795,25 +776,6 @@ class SkipFilesVariable(VariableTracker):\n                 unimplemented(f\"functools.wraps({fn})\")\n \n             return variables.LambdaVariable(wraps)\n-        elif self.value is collections.deque and not kwargs:\n-            if len(args) == 0:\n-                items = []\n-            elif len(args) == 1 and args[0].has_unpack_var_sequence(tx):\n-                items = args[0].unpack_var_sequence(tx)\n-            else:\n-                unimplemented(\"deque() with more than 1 arg not supported\")\n-            return variables.lists.DequeVariable(items, mutable_local=MutableLocal())\n-        elif self.value is functools.partial:\n-            if not args:\n-                unimplemented(\"functools.partial malformed\")\n-            # The first arg, a callable (the ctor below will assert on types)\n-            fn = args[0]\n-            rest_args = args[1:]\n-            # guards for the produced FunctoolsPartialVariable are installed in FunctoolsPartialVariable ctor from the\n-            # args and keywords\n-            return variables.functions.FunctoolsPartialVariable(\n-                fn, args=rest_args, keywords=kwargs\n-            )\n         else:\n             try:\n                 path = inspect.getfile(self.value)\n@@ -823,24 +785,6 @@ class SkipFilesVariable(VariableTracker):\n             msg += f\"', {self.reason}'\" if self.reason else \"\"\n             unimplemented(msg)\n \n-    def call_method(\n-        self,\n-        tx,\n-        name,\n-        args: \"List[VariableTracker]\",\n-        kwargs: \"Dict[str, VariableTracker]\",\n-    ) -> \"VariableTracker\":\n-        if (\n-            self.value in {collections.OrderedDict, collections.defaultdict}\n-            and name == \"fromkeys\"\n-        ):\n-            from .builtin import BuiltinVariable\n-\n-            return BuiltinVariable.call_custom_dict_fromkeys(\n-                tx, self.value, *args, **kwargs\n-            )\n-        return super().call_method(tx, name, args, kwargs)\n-\n \n class TypingVariable(VariableTracker):\n     def __init__(self, value, **kwargs):\n"
        },
        {
            "name": "user_defined.py",
            "path": "torch/_dynamo/variables/user_defined.py",
            "patches": [
                {
                    "old_start": 34,
                    "old_length": 7,
                    "new_start": 34,
                    "new_length": 7,
                    "hunk": "@@ -34,7 +34,7 @@ from ..utils import (\n )\n from .base import MutableLocal, VariableTracker\n from .ctx_manager import GenericContextWrappingVariable, NullContextVariable\n-from .dicts import ConstDictVariable\n+from .dicts import ConstDictVariable, DefaultDictVariable\n \n \n class UserDefinedVariable(VariableTracker):\n"
                },
                {
                    "old_start": 70,
                    "old_length": 6,
                    "new_start": 71,
                    "new_length": 14,
                    "hunk": "@@ -70,6 +71,14 @@ class UserDefinedClassVariable(UserDefinedVariable):\n         elif source and inspect.ismemberdescriptor(obj):\n             return VariableBuilder(tx, source)(obj.__get__(self.value))\n \n+        # Special handling of collections.OrderedDict.fromkeys()\n+        # Wrap it as GetAttrVariable(collections.OrderedDict, \"fromkeys\") to make it consistent with\n+        # collections.defaultdict, and both will be handled at UserDefinedClassVariable.call_method().\n+        # Otherwise, it would be wrapped as UserDefinedObjectVariable(collections.OrderedDict.fromkeys),\n+        # and we need duplicate code to handle both cases.\n+        if self.value is collections.OrderedDict and name == \"fromkeys\":\n+            return super().var_getattr(tx, name)\n+\n         if name in getattr(self.value, \"__dict__\", {}) or ConstantVariable.is_literal(\n             obj\n         ):\n"
                },
                {
                    "old_start": 102,
                    "old_length": 6,
                    "new_start": 111,
                    "new_length": 15,
                    "hunk": "@@ -102,6 +111,15 @@ class UserDefinedClassVariable(UserDefinedVariable):\n                 )\n \n             return variables.ListVariable(subs_as_vars, **options)\n+        elif (\n+            self.value in {collections.OrderedDict, collections.defaultdict}\n+            and name == \"fromkeys\"\n+        ):\n+            from .builtin import BuiltinVariable\n+\n+            return BuiltinVariable.call_custom_dict_fromkeys(\n+                tx, self.value, *args, **kwargs\n+            )\n \n         return super().call_method(tx, name, args, kwargs)\n \n"
                },
                {
                    "old_start": 110,
                    "old_length": 9,
                    "new_start": 128,
                    "new_length": 44,
                    "hunk": "@@ -110,9 +128,44 @@ class UserDefinedClassVariable(UserDefinedVariable):\n     ) -> \"VariableTracker\":\n         from ..side_effects import SideEffects\n         from .builder import SourcelessBuilder\n+        from .builtin import BuiltinVariable\n \n         if self.value is contextlib.nullcontext:\n             return NullContextVariable()\n+        elif self.value is collections.OrderedDict:\n+            return BuiltinVariable.call_custom_dict(\n+                tx, collections.OrderedDict, *args, **kwargs\n+            )\n+        elif (\n+            self.value is collections.defaultdict\n+            and len(args) <= 1\n+            and DefaultDictVariable.is_supported_arg(args[0])\n+        ):\n+            return DefaultDictVariable(\n+                {},\n+                collections.defaultdict,\n+                args[0],\n+                mutable_local=MutableLocal(),\n+            )\n+        elif self.value is collections.deque and not kwargs:\n+            if len(args) == 0:\n+                items = []\n+            elif len(args) == 1 and args[0].has_unpack_var_sequence(tx):\n+                items = args[0].unpack_var_sequence(tx)\n+            else:\n+                unimplemented(\"deque() with more than 1 arg not supported\")\n+            return variables.lists.DequeVariable(items, mutable_local=MutableLocal())\n+        elif self.value is functools.partial:\n+            if not args:\n+                unimplemented(\"functools.partial malformed\")\n+            # The first arg, a callable (the ctor below will assert on types)\n+            fn = args[0]\n+            rest_args = args[1:]\n+            # guards for the produced FunctoolsPartialVariable are installed in FunctoolsPartialVariable ctor from the\n+            # args and keywords\n+            return variables.functions.FunctoolsPartialVariable(\n+                fn, args=rest_args, keywords=kwargs\n+            )\n         elif (\n             issubclass(type(self.value), type)\n             and hasattr(self.value, \"__enter__\")"
                }
            ],
            "whole_deleted": "-from .dicts import ConstDictVariable\n",
            "whole_added": "+from .dicts import ConstDictVariable, DefaultDictVariable\n+        # Special handling of collections.OrderedDict.fromkeys()\n+        # Wrap it as GetAttrVariable(collections.OrderedDict, \"fromkeys\") to make it consistent with\n+        # collections.defaultdict, and both will be handled at UserDefinedClassVariable.call_method().\n+        # Otherwise, it would be wrapped as UserDefinedObjectVariable(collections.OrderedDict.fromkeys),\n+        # and we need duplicate code to handle both cases.\n+        if self.value is collections.OrderedDict and name == \"fromkeys\":\n+            return super().var_getattr(tx, name)\n+\n+        elif (\n+            self.value in {collections.OrderedDict, collections.defaultdict}\n+            and name == \"fromkeys\"\n+        ):\n+            from .builtin import BuiltinVariable\n+\n+            return BuiltinVariable.call_custom_dict_fromkeys(\n+                tx, self.value, *args, **kwargs\n+            )\n+        from .builtin import BuiltinVariable\n+        elif self.value is collections.OrderedDict:\n+            return BuiltinVariable.call_custom_dict(\n+                tx, collections.OrderedDict, *args, **kwargs\n+            )\n+        elif (\n+            self.value is collections.defaultdict\n+            and len(args) <= 1\n+            and DefaultDictVariable.is_supported_arg(args[0])\n+        ):\n+            return DefaultDictVariable(\n+                {},\n+                collections.defaultdict,\n+                args[0],\n+                mutable_local=MutableLocal(),\n+            )\n+        elif self.value is collections.deque and not kwargs:\n+            if len(args) == 0:\n+                items = []\n+            elif len(args) == 1 and args[0].has_unpack_var_sequence(tx):\n+                items = args[0].unpack_var_sequence(tx)\n+            else:\n+                unimplemented(\"deque() with more than 1 arg not supported\")\n+            return variables.lists.DequeVariable(items, mutable_local=MutableLocal())\n+        elif self.value is functools.partial:\n+            if not args:\n+                unimplemented(\"functools.partial malformed\")\n+            # The first arg, a callable (the ctor below will assert on types)\n+            fn = args[0]\n+            rest_args = args[1:]\n+            # guards for the produced FunctoolsPartialVariable are installed in FunctoolsPartialVariable ctor from the\n+            # args and keywords\n+            return variables.functions.FunctoolsPartialVariable(\n+                fn, args=rest_args, keywords=kwargs\n+            )\n",
            "whole_hunk": "@@ -34,7 +34,7 @@ from ..utils import (\n )\n from .base import MutableLocal, VariableTracker\n from .ctx_manager import GenericContextWrappingVariable, NullContextVariable\n-from .dicts import ConstDictVariable\n+from .dicts import ConstDictVariable, DefaultDictVariable\n \n \n class UserDefinedVariable(VariableTracker):\n@@ -70,6 +71,14 @@ class UserDefinedClassVariable(UserDefinedVariable):\n         elif source and inspect.ismemberdescriptor(obj):\n             return VariableBuilder(tx, source)(obj.__get__(self.value))\n \n+        # Special handling of collections.OrderedDict.fromkeys()\n+        # Wrap it as GetAttrVariable(collections.OrderedDict, \"fromkeys\") to make it consistent with\n+        # collections.defaultdict, and both will be handled at UserDefinedClassVariable.call_method().\n+        # Otherwise, it would be wrapped as UserDefinedObjectVariable(collections.OrderedDict.fromkeys),\n+        # and we need duplicate code to handle both cases.\n+        if self.value is collections.OrderedDict and name == \"fromkeys\":\n+            return super().var_getattr(tx, name)\n+\n         if name in getattr(self.value, \"__dict__\", {}) or ConstantVariable.is_literal(\n             obj\n         ):\n@@ -102,6 +111,15 @@ class UserDefinedClassVariable(UserDefinedVariable):\n                 )\n \n             return variables.ListVariable(subs_as_vars, **options)\n+        elif (\n+            self.value in {collections.OrderedDict, collections.defaultdict}\n+            and name == \"fromkeys\"\n+        ):\n+            from .builtin import BuiltinVariable\n+\n+            return BuiltinVariable.call_custom_dict_fromkeys(\n+                tx, self.value, *args, **kwargs\n+            )\n \n         return super().call_method(tx, name, args, kwargs)\n \n@@ -110,9 +128,44 @@ class UserDefinedClassVariable(UserDefinedVariable):\n     ) -> \"VariableTracker\":\n         from ..side_effects import SideEffects\n         from .builder import SourcelessBuilder\n+        from .builtin import BuiltinVariable\n \n         if self.value is contextlib.nullcontext:\n             return NullContextVariable()\n+        elif self.value is collections.OrderedDict:\n+            return BuiltinVariable.call_custom_dict(\n+                tx, collections.OrderedDict, *args, **kwargs\n+            )\n+        elif (\n+            self.value is collections.defaultdict\n+            and len(args) <= 1\n+            and DefaultDictVariable.is_supported_arg(args[0])\n+        ):\n+            return DefaultDictVariable(\n+                {},\n+                collections.defaultdict,\n+                args[0],\n+                mutable_local=MutableLocal(),\n+            )\n+        elif self.value is collections.deque and not kwargs:\n+            if len(args) == 0:\n+                items = []\n+            elif len(args) == 1 and args[0].has_unpack_var_sequence(tx):\n+                items = args[0].unpack_var_sequence(tx)\n+            else:\n+                unimplemented(\"deque() with more than 1 arg not supported\")\n+            return variables.lists.DequeVariable(items, mutable_local=MutableLocal())\n+        elif self.value is functools.partial:\n+            if not args:\n+                unimplemented(\"functools.partial malformed\")\n+            # The first arg, a callable (the ctor below will assert on types)\n+            fn = args[0]\n+            rest_args = args[1:]\n+            # guards for the produced FunctoolsPartialVariable are installed in FunctoolsPartialVariable ctor from the\n+            # args and keywords\n+            return variables.functions.FunctoolsPartialVariable(\n+                fn, args=rest_args, keywords=kwargs\n+            )\n         elif (\n             issubclass(type(self.value), type)\n             and hasattr(self.value, \"__enter__\")"
        }
    ]
},
{
    "Id": 101,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/c993f1b37fe167c186911885dd680bba52471aeb",
    "date": "2024-06-10T15:31:03+00:00",
    "message": "Fix edge cases for gather in inductor (#126893)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/126893\nApproved by: https://github.com/peterbell10\nghstack dependencies: #126876",
    "label": "YES",
    "changes": [
        {
            "name": "test_torchinductor_opinfo.py",
            "path": "test/inductor/test_torchinductor_opinfo.py",
            "patches": [
                {
                    "old_start": 441,
                    "old_length": 6,
                    "new_start": 441,
                    "new_length": 7,
                    "hunk": "@@ -441,6 +441,7 @@ inductor_all_samples = {\n     \"cummax\",\n     \"cummin\",\n     \"nextafter\",\n+    \"gather\",\n     \"_chunk_cat\",\n     \"constant_pad_nd\",\n }\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    \"gather\",\n",
            "whole_hunk": "@@ -441,6 +441,7 @@ inductor_all_samples = {\n     \"cummax\",\n     \"cummin\",\n     \"nextafter\",\n+    \"gather\",\n     \"_chunk_cat\",\n     \"constant_pad_nd\",\n }\n"
        },
        {
            "name": "lowering.py",
            "path": "torch/_inductor/lowering.py",
            "patches": [
                {
                    "old_start": 2781,
                    "old_length": 18,
                    "new_start": 2781,
                    "new_length": 29,
                    "hunk": "@@ -2781,18 +2781,29 @@ def gather(x, dim, index, sparse_grad=False):\n     # sparse_grad doesn't affect forward computation,\n     # and backward tracing is taken care of by AOT Autograd\n     assert isinstance(x, TensorBox)\n+    if index.get_numel() == 0:\n+        # Empty index case. Return an empty array with the same shape\n+        return new_empty(x, index.get_size())\n+\n     assert index.get_dtype() == torch.int64\n     size = x.get_size()\n     offset = len(size) == 0\n     dim = _validate_dim(x, dim, offset)\n \n+    if offset:\n+        x = expand(x, [1])\n+        size = [1]\n+\n     x_loader = x.make_loader()\n     index_loader = index.make_loader()\n \n     def fn(idx):\n         idx = list(idx)\n-        if len(idx) != 0:\n-            idx[dim] = ops.indirect_indexing(index_loader(idx), size[dim])\n+        gather_idx = ops.indirect_indexing(index_loader(idx), size[dim])\n+        if len(idx) == 0:\n+            idx = [gather_idx]\n+        else:\n+            idx[dim] = gather_idx\n         return x_loader(idx)\n \n     return Pointwise.create(\n"
                },
                {
                    "old_start": 3272,
                    "old_length": 6,
                    "new_start": 3283,
                    "new_length": 9,
                    "hunk": "@@ -3272,6 +3283,9 @@ def scatter_reduce_(self, dim: int, index, src, reduce, *, include_self: bool =\n     if isinstance(index, TensorBox) and len(index.get_size()) == 0:\n         index = view(index, [1])\n \n+    if index.get_numel() == 0:\n+        return self\n+\n     dim = _validate_dim(self, dim)\n \n     self.realize()\n"
                }
            ],
            "whole_deleted": "-        if len(idx) != 0:\n-            idx[dim] = ops.indirect_indexing(index_loader(idx), size[dim])\n",
            "whole_added": "+    if index.get_numel() == 0:\n+        # Empty index case. Return an empty array with the same shape\n+        return new_empty(x, index.get_size())\n+\n+    if offset:\n+        x = expand(x, [1])\n+        size = [1]\n+\n+        gather_idx = ops.indirect_indexing(index_loader(idx), size[dim])\n+        if len(idx) == 0:\n+            idx = [gather_idx]\n+        else:\n+            idx[dim] = gather_idx\n+    if index.get_numel() == 0:\n+        return self\n+\n",
            "whole_hunk": "@@ -2781,18 +2781,29 @@ def gather(x, dim, index, sparse_grad=False):\n     # sparse_grad doesn't affect forward computation,\n     # and backward tracing is taken care of by AOT Autograd\n     assert isinstance(x, TensorBox)\n+    if index.get_numel() == 0:\n+        # Empty index case. Return an empty array with the same shape\n+        return new_empty(x, index.get_size())\n+\n     assert index.get_dtype() == torch.int64\n     size = x.get_size()\n     offset = len(size) == 0\n     dim = _validate_dim(x, dim, offset)\n \n+    if offset:\n+        x = expand(x, [1])\n+        size = [1]\n+\n     x_loader = x.make_loader()\n     index_loader = index.make_loader()\n \n     def fn(idx):\n         idx = list(idx)\n-        if len(idx) != 0:\n-            idx[dim] = ops.indirect_indexing(index_loader(idx), size[dim])\n+        gather_idx = ops.indirect_indexing(index_loader(idx), size[dim])\n+        if len(idx) == 0:\n+            idx = [gather_idx]\n+        else:\n+            idx[dim] = gather_idx\n         return x_loader(idx)\n \n     return Pointwise.create(\n@@ -3272,6 +3283,9 @@ def scatter_reduce_(self, dim: int, index, src, reduce, *, include_self: bool =\n     if isinstance(index, TensorBox) and len(index.get_size()) == 0:\n         index = view(index, [1])\n \n+    if index.get_numel() == 0:\n+        return self\n+\n     dim = _validate_dim(self, dim)\n \n     self.realize()\n"
        },
        {
            "name": "common_methods_invocations.py",
            "path": "torch/testing/_internal/common_methods_invocations.py",
            "patches": [
                {
                    "old_start": 2623,
                    "old_length": 6,
                    "new_start": 2623,
                    "new_length": 10,
                    "hunk": "@@ -2623,6 +2623,10 @@ def sample_inputs_gather(op_info, device, dtype, requires_grad, **kwargs):\n         make_arg((S,)),\n         0,\n         torch.tensor([], dtype=torch.uint8, device=device))\n+    yield SampleInput(\n+        make_arg((S,)),\n+        0,\n+        torch.tensor([[], []], dtype=torch.uint8, device=device))\n     # 0D tensor case\n     yield SampleInput(\n         make_arg(()),"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    yield SampleInput(\n+        make_arg((S,)),\n+        0,\n+        torch.tensor([[], []], dtype=torch.uint8, device=device))\n",
            "whole_hunk": "@@ -2623,6 +2623,10 @@ def sample_inputs_gather(op_info, device, dtype, requires_grad, **kwargs):\n         make_arg((S,)),\n         0,\n         torch.tensor([], dtype=torch.uint8, device=device))\n+    yield SampleInput(\n+        make_arg((S,)),\n+        0,\n+        torch.tensor([[], []], dtype=torch.uint8, device=device))\n     # 0D tensor case\n     yield SampleInput(\n         make_arg(()),"
        }
    ]
},
{
    "Id": 131,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/6eac3f45c7d6db45fa26507d0d8949ce6fb12dd7",
    "date": "2024-05-23T23:37:43+00:00",
    "message": "Add basic sanity checks for graph ops to cache key (#124745)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/124745\nApproved by: https://github.com/bdhirsh",
    "label": "NO",
    "changes": [
        {
            "name": "test_aot_autograd_cache.py",
            "path": "test/dynamo/test_aot_autograd_cache.py",
            "patches": [
                {
                    "old_start": 5,
                    "old_length": 11,
                    "new_start": 5,
                    "new_length": 18,
                    "hunk": "@@ -5,11 +5,18 @@ import torch._dynamo\n import torch._dynamo.test_case\n \n import torch._functorch._aot_autograd\n-from torch._functorch._aot_autograd.autograd_cache import autograd_cache_hash\n+from torch._functorch._aot_autograd.autograd_cache import (\n+    autograd_cache_hash,\n+    BypassAOTAutogradCache,\n+)\n from torch._functorch._aot_autograd.schemas import AOTConfig\n \n \n class AOTAutogradCachePicklerTests(torch._dynamo.test_case.TestCase):\n+    @property\n+    def device_type(self) -> str:\n+        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n     def default_config(self):\n         return AOTConfig(\n             fw_compiler=None,\n"
                },
                {
                    "old_start": 98,
                    "old_length": 6,
                    "new_start": 105,
                    "new_length": 34,
                    "hunk": "@@ -98,6 +105,34 @@ class AOTAutogradCachePicklerTests(torch._dynamo.test_case.TestCase):\n         c2 = self.gen_cache_key(fn, config2)\n         self.assertNotEqual(c1, c2)\n \n+    def test_incompatible_function(self):\n+        @torch._dynamo.allow_in_graph\n+        class AllowInGraphFunc(torch.autograd.Function):\n+            @staticmethod\n+            def forward(_, x):\n+                torch._dynamo.graph_break()\n+                return x.sin()\n+\n+        def fn(x):\n+            return AllowInGraphFunc.apply(x)\n+\n+        config = self.default_config()\n+        self.assertRaises(\n+            BypassAOTAutogradCache, lambda: self.gen_cache_key(fn, config)\n+        )\n+\n+    def test_normal_torch_function(self):\n+        @torch._dynamo.allow_in_graph\n+        def fn(x):\n+            y = torch.sin(x)\n+            z = torch.cos(x)\n+            w = y + z\n+            w.abs()\n+            return w\n+\n+        config = self.default_config()\n+        self.gen_cache_key(fn, config)\n+\n \n if __name__ == \"__main__\":\n     from torch._dynamo.test_case import run_tests\n"
                }
            ],
            "whole_deleted": "-from torch._functorch._aot_autograd.autograd_cache import autograd_cache_hash\n",
            "whole_added": "+from torch._functorch._aot_autograd.autograd_cache import (\n+    autograd_cache_hash,\n+    BypassAOTAutogradCache,\n+)\n+    @property\n+    def device_type(self) -> str:\n+        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+    def test_incompatible_function(self):\n+        @torch._dynamo.allow_in_graph\n+        class AllowInGraphFunc(torch.autograd.Function):\n+            @staticmethod\n+            def forward(_, x):\n+                torch._dynamo.graph_break()\n+                return x.sin()\n+\n+        def fn(x):\n+            return AllowInGraphFunc.apply(x)\n+\n+        config = self.default_config()\n+        self.assertRaises(\n+            BypassAOTAutogradCache, lambda: self.gen_cache_key(fn, config)\n+        )\n+\n+    def test_normal_torch_function(self):\n+        @torch._dynamo.allow_in_graph\n+        def fn(x):\n+            y = torch.sin(x)\n+            z = torch.cos(x)\n+            w = y + z\n+            w.abs()\n+            return w\n+\n+        config = self.default_config()\n+        self.gen_cache_key(fn, config)\n+\n",
            "whole_hunk": "@@ -5,11 +5,18 @@ import torch._dynamo\n import torch._dynamo.test_case\n \n import torch._functorch._aot_autograd\n-from torch._functorch._aot_autograd.autograd_cache import autograd_cache_hash\n+from torch._functorch._aot_autograd.autograd_cache import (\n+    autograd_cache_hash,\n+    BypassAOTAutogradCache,\n+)\n from torch._functorch._aot_autograd.schemas import AOTConfig\n \n \n class AOTAutogradCachePicklerTests(torch._dynamo.test_case.TestCase):\n+    @property\n+    def device_type(self) -> str:\n+        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n     def default_config(self):\n         return AOTConfig(\n             fw_compiler=None,\n@@ -98,6 +105,34 @@ class AOTAutogradCachePicklerTests(torch._dynamo.test_case.TestCase):\n         c2 = self.gen_cache_key(fn, config2)\n         self.assertNotEqual(c1, c2)\n \n+    def test_incompatible_function(self):\n+        @torch._dynamo.allow_in_graph\n+        class AllowInGraphFunc(torch.autograd.Function):\n+            @staticmethod\n+            def forward(_, x):\n+                torch._dynamo.graph_break()\n+                return x.sin()\n+\n+        def fn(x):\n+            return AllowInGraphFunc.apply(x)\n+\n+        config = self.default_config()\n+        self.assertRaises(\n+            BypassAOTAutogradCache, lambda: self.gen_cache_key(fn, config)\n+        )\n+\n+    def test_normal_torch_function(self):\n+        @torch._dynamo.allow_in_graph\n+        def fn(x):\n+            y = torch.sin(x)\n+            z = torch.cos(x)\n+            w = y + z\n+            w.abs()\n+            return w\n+\n+        config = self.default_config()\n+        self.gen_cache_key(fn, config)\n+\n \n if __name__ == \"__main__\":\n     from torch._dynamo.test_case import run_tests\n"
        },
        {
            "name": "autograd_cache.py",
            "path": "torch/_functorch/_aot_autograd/autograd_cache.py",
            "patches": [
                {
                    "old_start": 3,
                    "old_length": 19,
                    "new_start": 3,
                    "new_length": 95,
                    "hunk": "@@ -3,19 +3,95 @@ Utils for caching the outputs of AOTAutograd\n \"\"\"\n from __future__ import annotations\n \n+import functools\n import logging\n-from typing import TYPE_CHECKING\n+import os\n \n-from torch._inductor.codecache import _ident, FxGraphCachePickler\n+import torch\n+from torch._inductor.codecache import (\n+    _ident,\n+    FxGraphCachePickler,\n+    get_code_hash,\n+    get_inductor_root,\n+)\n+from torch.fx.node import Node\n \n from .schemas import AOTConfig  # noqa: F401\n \n-if TYPE_CHECKING:\n-    import torch\n-\n log = logging.getLogger(__name__)\n \n \n+class BypassAOTAutogradCache(Exception):\n+    pass\n+\n+\n+def check_node_safe(node: Node):\n+    \"\"\"\n+    Checks that the node only uses supported operators. We are starting with very\n+    conservative cacheability constraints, and incrementally adding more support as we expand.\n+    \"\"\"\n+\n+    def is_torch_function(target):\n+        is_builtin_fun_or_type = type(target).__name__ == \"builtin_function_or_method\"\n+        # TODO: handle torch.nn.functional and other non inlined targets, which don't compile down to a builtin\n+        return is_builtin_fun_or_type\n+\n+    def is_tensor(target):\n+        # Tensors always have example values in meta field\n+        return \"example_value\" in target.meta\n+\n+    # I'd love to use a match statement here, but it wasn't introduced until py3.10\n+    if node.op == \"call_function\":\n+        # We support only torch.* functions for now\n+        # We can probably add an allowlist of safe non-torch implementations as well\n+        if not is_torch_function(node.target):\n+            raise BypassAOTAutogradCache(\n+                f\"Unsupported call_function target {node.target}\"\n+            )\n+    elif node.op == \"call_method\":\n+        method_name = node.target\n+        method_target = node.args[0]\n+        # Only support method calls on base tensors\n+        if not is_tensor(method_target):\n+            raise BypassAOTAutogradCache(\n+                f\"Unsupported call_method target {method_target}\"\n+            )\n+        if (\n+            type(method_name) != str\n+            and type(method_name).__name__ != \"method_descriptor\"\n+        ):\n+            raise BypassAOTAutogradCache(\n+                f\"Unsupported call_method method {node.target}: {method_name}\"\n+            )\n+    # Cache safe\n+    elif node.op in (\"placeholder\", \"get_attr\", \"call_module\", \"output\"):\n+        # Assumption today for call_module being a safe op:\n+        # (1) today the only call_module ops that can show up in a graph come from \"built-in-nn-modules\"\n+        # that dynamo assumes are safe to trace. If dynamo assumes they are safely to blindly trace, then\n+        # they should be safe to cache as well.\n+        # (2) in the steady-state (some time in H2?) we shouldn't see these anymore, once inline builtin nn modules by default\n+        # (3) We do not allow user made nn modules in the graph today, only function calls.\n+        pass\n+    else:\n+        raise BypassAOTAutogradCache(f\"Unsupported node op {node.op}\")\n+\n+\n+@functools.lru_cache(None)\n+def get_autograd_code_hash():\n+    autograd_root = os.path.dirname(__file__)\n+    inductor_root = get_inductor_root()\n+    return get_code_hash([autograd_root, inductor_root])\n+\n+\n+def check_cacheable(gm: torch.fx.GraphModule):\n+    \"\"\"\n+    Checks that the graph module only uses supported operators\n+    \"\"\"\n+    nodes = gm.graph.nodes\n+    for node in nodes:\n+        check_node_safe(node)\n+\n+\n class AOTAutogradCacheDetails:\n     \"\"\"\n     Object to capture all the details for a dynamo graph module relevant to computing\n"
                },
                {
                    "old_start": 26,
                    "old_length": 6,
                    "new_start": 102,
                    "new_length": 8,
                    "hunk": "@@ -26,6 +102,8 @@ class AOTAutogradCacheDetails:\n         self.gm = gm  # TODO: we'll handle different parts of the graph module\n         # TODO: We'll want to handle the full_args passed in as well\n         self.config = config  # Gets reduced by the Pickler\n+        check_cacheable(gm)\n+        self.code_hash = get_autograd_code_hash()\n \n     def debug_str(self) -> str:\n         return AOTAutogradCachePickler.debug_str(self)\n"
                }
            ],
            "whole_deleted": "-from typing import TYPE_CHECKING\n-from torch._inductor.codecache import _ident, FxGraphCachePickler\n-if TYPE_CHECKING:\n-    import torch\n-\n",
            "whole_added": "+import functools\n+import os\n+import torch\n+from torch._inductor.codecache import (\n+    _ident,\n+    FxGraphCachePickler,\n+    get_code_hash,\n+    get_inductor_root,\n+)\n+from torch.fx.node import Node\n+class BypassAOTAutogradCache(Exception):\n+    pass\n+\n+\n+def check_node_safe(node: Node):\n+    \"\"\"\n+    Checks that the node only uses supported operators. We are starting with very\n+    conservative cacheability constraints, and incrementally adding more support as we expand.\n+    \"\"\"\n+\n+    def is_torch_function(target):\n+        is_builtin_fun_or_type = type(target).__name__ == \"builtin_function_or_method\"\n+        # TODO: handle torch.nn.functional and other non inlined targets, which don't compile down to a builtin\n+        return is_builtin_fun_or_type\n+\n+    def is_tensor(target):\n+        # Tensors always have example values in meta field\n+        return \"example_value\" in target.meta\n+\n+    # I'd love to use a match statement here, but it wasn't introduced until py3.10\n+    if node.op == \"call_function\":\n+        # We support only torch.* functions for now\n+        # We can probably add an allowlist of safe non-torch implementations as well\n+        if not is_torch_function(node.target):\n+            raise BypassAOTAutogradCache(\n+                f\"Unsupported call_function target {node.target}\"\n+            )\n+    elif node.op == \"call_method\":\n+        method_name = node.target\n+        method_target = node.args[0]\n+        # Only support method calls on base tensors\n+        if not is_tensor(method_target):\n+            raise BypassAOTAutogradCache(\n+                f\"Unsupported call_method target {method_target}\"\n+            )\n+        if (\n+            type(method_name) != str\n+            and type(method_name).__name__ != \"method_descriptor\"\n+        ):\n+            raise BypassAOTAutogradCache(\n+                f\"Unsupported call_method method {node.target}: {method_name}\"\n+            )\n+    # Cache safe\n+    elif node.op in (\"placeholder\", \"get_attr\", \"call_module\", \"output\"):\n+        # Assumption today for call_module being a safe op:\n+        # (1) today the only call_module ops that can show up in a graph come from \"built-in-nn-modules\"\n+        # that dynamo assumes are safe to trace. If dynamo assumes they are safely to blindly trace, then\n+        # they should be safe to cache as well.\n+        # (2) in the steady-state (some time in H2?) we shouldn't see these anymore, once inline builtin nn modules by default\n+        # (3) We do not allow user made nn modules in the graph today, only function calls.\n+        pass\n+    else:\n+        raise BypassAOTAutogradCache(f\"Unsupported node op {node.op}\")\n+\n+\n+@functools.lru_cache(None)\n+def get_autograd_code_hash():\n+    autograd_root = os.path.dirname(__file__)\n+    inductor_root = get_inductor_root()\n+    return get_code_hash([autograd_root, inductor_root])\n+\n+\n+def check_cacheable(gm: torch.fx.GraphModule):\n+    \"\"\"\n+    Checks that the graph module only uses supported operators\n+    \"\"\"\n+    nodes = gm.graph.nodes\n+    for node in nodes:\n+        check_node_safe(node)\n+\n+\n+        check_cacheable(gm)\n+        self.code_hash = get_autograd_code_hash()\n",
            "whole_hunk": "@@ -3,19 +3,95 @@ Utils for caching the outputs of AOTAutograd\n \"\"\"\n from __future__ import annotations\n \n+import functools\n import logging\n-from typing import TYPE_CHECKING\n+import os\n \n-from torch._inductor.codecache import _ident, FxGraphCachePickler\n+import torch\n+from torch._inductor.codecache import (\n+    _ident,\n+    FxGraphCachePickler,\n+    get_code_hash,\n+    get_inductor_root,\n+)\n+from torch.fx.node import Node\n \n from .schemas import AOTConfig  # noqa: F401\n \n-if TYPE_CHECKING:\n-    import torch\n-\n log = logging.getLogger(__name__)\n \n \n+class BypassAOTAutogradCache(Exception):\n+    pass\n+\n+\n+def check_node_safe(node: Node):\n+    \"\"\"\n+    Checks that the node only uses supported operators. We are starting with very\n+    conservative cacheability constraints, and incrementally adding more support as we expand.\n+    \"\"\"\n+\n+    def is_torch_function(target):\n+        is_builtin_fun_or_type = type(target).__name__ == \"builtin_function_or_method\"\n+        # TODO: handle torch.nn.functional and other non inlined targets, which don't compile down to a builtin\n+        return is_builtin_fun_or_type\n+\n+    def is_tensor(target):\n+        # Tensors always have example values in meta field\n+        return \"example_value\" in target.meta\n+\n+    # I'd love to use a match statement here, but it wasn't introduced until py3.10\n+    if node.op == \"call_function\":\n+        # We support only torch.* functions for now\n+        # We can probably add an allowlist of safe non-torch implementations as well\n+        if not is_torch_function(node.target):\n+            raise BypassAOTAutogradCache(\n+                f\"Unsupported call_function target {node.target}\"\n+            )\n+    elif node.op == \"call_method\":\n+        method_name = node.target\n+        method_target = node.args[0]\n+        # Only support method calls on base tensors\n+        if not is_tensor(method_target):\n+            raise BypassAOTAutogradCache(\n+                f\"Unsupported call_method target {method_target}\"\n+            )\n+        if (\n+            type(method_name) != str\n+            and type(method_name).__name__ != \"method_descriptor\"\n+        ):\n+            raise BypassAOTAutogradCache(\n+                f\"Unsupported call_method method {node.target}: {method_name}\"\n+            )\n+    # Cache safe\n+    elif node.op in (\"placeholder\", \"get_attr\", \"call_module\", \"output\"):\n+        # Assumption today for call_module being a safe op:\n+        # (1) today the only call_module ops that can show up in a graph come from \"built-in-nn-modules\"\n+        # that dynamo assumes are safe to trace. If dynamo assumes they are safely to blindly trace, then\n+        # they should be safe to cache as well.\n+        # (2) in the steady-state (some time in H2?) we shouldn't see these anymore, once inline builtin nn modules by default\n+        # (3) We do not allow user made nn modules in the graph today, only function calls.\n+        pass\n+    else:\n+        raise BypassAOTAutogradCache(f\"Unsupported node op {node.op}\")\n+\n+\n+@functools.lru_cache(None)\n+def get_autograd_code_hash():\n+    autograd_root = os.path.dirname(__file__)\n+    inductor_root = get_inductor_root()\n+    return get_code_hash([autograd_root, inductor_root])\n+\n+\n+def check_cacheable(gm: torch.fx.GraphModule):\n+    \"\"\"\n+    Checks that the graph module only uses supported operators\n+    \"\"\"\n+    nodes = gm.graph.nodes\n+    for node in nodes:\n+        check_node_safe(node)\n+\n+\n class AOTAutogradCacheDetails:\n     \"\"\"\n     Object to capture all the details for a dynamo graph module relevant to computing\n@@ -26,6 +102,8 @@ class AOTAutogradCacheDetails:\n         self.gm = gm  # TODO: we'll handle different parts of the graph module\n         # TODO: We'll want to handle the full_args passed in as well\n         self.config = config  # Gets reduced by the Pickler\n+        check_cacheable(gm)\n+        self.code_hash = get_autograd_code_hash()\n \n     def debug_str(self) -> str:\n         return AOTAutogradCachePickler.debug_str(self)\n"
        },
        {
            "name": "codecache.py",
            "path": "torch/_inductor/codecache.py",
            "patches": [
                {
                    "old_start": 566,
                    "old_length": 6,
                    "new_start": 566,
                    "new_length": 19,
                    "hunk": "@@ -566,6 +566,19 @@ class FxGraphCachePickler(pickle.Pickler):\n         return \"\\n\".join(lines)\n \n \n+def get_code_hash(roots):\n+    contents: Dict[str, bytes] = {torch.__version__: b\"\"}\n+    for lib in pkgutil.iter_modules(roots):\n+        spec = lib.module_finder.find_spec(lib.name, None)\n+        assert spec is not None\n+        module = spec.origin\n+        assert module is not None\n+        with open(module, \"rb\") as f:\n+            contents[module] = f.read()\n+\n+    return hashlib.sha256(pickle.dumps(contents)).digest()\n+\n+\n @functools.lru_cache(None)\n def torch_key():\n     \"\"\"\n"
                },
                {
                    "old_start": 573,
                    "old_length": 23,
                    "new_start": 586,
                    "new_length": 17,
                    "hunk": "@@ -573,23 +586,17 @@ def torch_key():\n     \"\"\"\n     if not config.is_fbcode():\n         inductor_root = os.path.dirname(__file__)\n-\n-        contents: Dict[str, bytes] = {torch.__version__: b\"\"}\n-        for lib in pkgutil.iter_modules([inductor_root]):\n-            spec = lib.module_finder.find_spec(lib.name, None)\n-            assert spec is not None\n-            module = spec.origin\n-            assert module is not None\n-            with open(module, \"rb\") as f:\n-                contents[module] = f.read()\n-\n-        return hashlib.sha256(pickle.dumps(contents)).digest()\n+        return get_code_hash([inductor_root])\n \n     from libfb.py import parutil\n \n     return parutil.get_file_contents(\"torch/src_hash.txt\").rstrip()\n \n \n+def get_inductor_root():\n+    return os.path.dirname(__file__)\n+\n+\n @dataclasses.dataclass\n class OrderedSetHolder:\n     \"\"\""
                }
            ],
            "whole_deleted": "-\n-        contents: Dict[str, bytes] = {torch.__version__: b\"\"}\n-        for lib in pkgutil.iter_modules([inductor_root]):\n-            spec = lib.module_finder.find_spec(lib.name, None)\n-            assert spec is not None\n-            module = spec.origin\n-            assert module is not None\n-            with open(module, \"rb\") as f:\n-                contents[module] = f.read()\n-\n-        return hashlib.sha256(pickle.dumps(contents)).digest()\n",
            "whole_added": "+def get_code_hash(roots):\n+    contents: Dict[str, bytes] = {torch.__version__: b\"\"}\n+    for lib in pkgutil.iter_modules(roots):\n+        spec = lib.module_finder.find_spec(lib.name, None)\n+        assert spec is not None\n+        module = spec.origin\n+        assert module is not None\n+        with open(module, \"rb\") as f:\n+            contents[module] = f.read()\n+\n+    return hashlib.sha256(pickle.dumps(contents)).digest()\n+\n+\n+        return get_code_hash([inductor_root])\n+def get_inductor_root():\n+    return os.path.dirname(__file__)\n+\n+\n",
            "whole_hunk": "@@ -566,6 +566,19 @@ class FxGraphCachePickler(pickle.Pickler):\n         return \"\\n\".join(lines)\n \n \n+def get_code_hash(roots):\n+    contents: Dict[str, bytes] = {torch.__version__: b\"\"}\n+    for lib in pkgutil.iter_modules(roots):\n+        spec = lib.module_finder.find_spec(lib.name, None)\n+        assert spec is not None\n+        module = spec.origin\n+        assert module is not None\n+        with open(module, \"rb\") as f:\n+            contents[module] = f.read()\n+\n+    return hashlib.sha256(pickle.dumps(contents)).digest()\n+\n+\n @functools.lru_cache(None)\n def torch_key():\n     \"\"\"\n@@ -573,23 +586,17 @@ def torch_key():\n     \"\"\"\n     if not config.is_fbcode():\n         inductor_root = os.path.dirname(__file__)\n-\n-        contents: Dict[str, bytes] = {torch.__version__: b\"\"}\n-        for lib in pkgutil.iter_modules([inductor_root]):\n-            spec = lib.module_finder.find_spec(lib.name, None)\n-            assert spec is not None\n-            module = spec.origin\n-            assert module is not None\n-            with open(module, \"rb\") as f:\n-                contents[module] = f.read()\n-\n-        return hashlib.sha256(pickle.dumps(contents)).digest()\n+        return get_code_hash([inductor_root])\n \n     from libfb.py import parutil\n \n     return parutil.get_file_contents(\"torch/src_hash.txt\").rstrip()\n \n \n+def get_inductor_root():\n+    return os.path.dirname(__file__)\n+\n+\n @dataclasses.dataclass\n class OrderedSetHolder:\n     \"\"\""
        }
    ]
},
{
    "Id": 89,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/0678742924ebdab90dd6ef1dcce64ffc61799d47",
    "date": "2024-06-13T06:53:17+00:00",
    "message": "[MPS] Add Metal implementation of exp op (#128421)\n\nTo improve accuracy, use `precise::exp()` (and `precise::sin()`/`precise::cos()` for complex flavor)\nReuse `test_exp1` to check that accuracy of `exp` ops is sometimes closer to CPU\n\nFix bug in non-contiguous tensors handling\n\nFixes https://github.com/pytorch/pytorch/issues/84936\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128421\nApproved by: https://github.com/kulinseth\nghstack dependencies: #128373, #128375",
    "label": "NO",
    "changes": [
        {
            "name": "OperationUtils.mm",
            "path": "aten/src/ATen/native/mps/OperationUtils.mm",
            "patches": [
                {
                    "old_start": 659,
                    "old_length": 6,
                    "new_start": 659,
                    "new_length": 7,
                    "hunk": "@@ -659,6 +659,7 @@ id<MTLLibrary> MetalShaderLibrary::compileLibrary(const std::string& src) {\n   MTLCompileOptions* options = [[MTLCompileOptions new] autorelease];\n   [options setLanguageVersion:is_macos_13_or_newer(MacOSVersion::MACOS_VER_14_0_PLUS) ? MTLLanguageVersion3_1\n                                                                                       : MTLLanguageVersion2_3];\n+  // [options setFastMathEnabled: NO];\n   auto str = [NSString stringWithCString:src.c_str() encoding:NSASCIIStringEncoding];\n   auto device = MPSDevice::getInstance()->device();\n   library = [device newLibraryWithSource:str options:options error:&error];\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  // [options setFastMathEnabled: NO];\n",
            "whole_hunk": "@@ -659,6 +659,7 @@ id<MTLLibrary> MetalShaderLibrary::compileLibrary(const std::string& src) {\n   MTLCompileOptions* options = [[MTLCompileOptions new] autorelease];\n   [options setLanguageVersion:is_macos_13_or_newer(MacOSVersion::MACOS_VER_14_0_PLUS) ? MTLLanguageVersion3_1\n                                                                                       : MTLLanguageVersion2_3];\n+  // [options setFastMathEnabled: NO];\n   auto str = [NSString stringWithCString:src.c_str() encoding:NSASCIIStringEncoding];\n   auto device = MPSDevice::getInstance()->device();\n   library = [device newLibraryWithSource:str options:options error:&error];\n"
        },
        {
            "name": "UnaryConstants.h",
            "path": "aten/src/ATen/native/mps/UnaryConstants.h",
            "patches": [
                {
                    "old_start": 9,
                    "old_length": 9,
                    "new_start": 9,
                    "new_length": 9,
                    "hunk": "@@ -9,9 +9,9 @@ constant float b[4] = {{-2.118377725, 1.442710462, -0.329097515, 0.012229801}};\n constant float c[4] = {{-1.970840454, -1.624906493, 3.429567803, 1.641345311}};\n constant float d[2] = {{3.543889200, 1.637067800}};\n \n-kernel void erfinv_mps_kernel( device {0} *output [[buffer(0)]],\n-                            device {1} *input [[buffer(1)]],\n-                            uint index [[thread_position_in_grid]]) {{\n+kernel void erfinv_kernel( device {0} *output [[buffer(0)]],\n+                           device {1} *input [[buffer(1)]],\n+                           uint index [[thread_position_in_grid]]) {{\n \n   float y = input[index];\n   float x, z, num, dem; /*working variables */\n"
                },
                {
                    "old_start": 35,
                    "old_length": 4,
                    "new_start": 35,
                    "new_length": 46,
                    "hunk": "@@ -35,4 +35,46 @@ kernel void erfinv_mps_kernel( device {0} *output [[buffer(0)]],\n   }}\n \n   output[index] = {0}(x);\n-}})METAL\";\n+}}\n+\n+kernel void exp_kernel( device {0} *output [[buffer(0)]],\n+                        device {1} *input [[ buffer(1)]],\n+                        uint index [[thread_position_in_grid]]) {{\n+  output[index] = {0}(precise::exp(input[index]));\n+}}\n+\n+kernel void exp_complex_kernel( device {0}2 *output [[buffer(0)]],\n+                                device {0}2 *input [[ buffer(1)]],\n+                                uint index [[thread_position_in_grid]]) {{\n+  output[index].x = {0}(precise::exp(input[index].x)*precise::cos(input[index].y));\n+  output[index].y = {0}(precise::exp(input[index].x)*precise::sin(input[index].y));\n+}}\n+\n+kernel void tanh_kernel( device {0} *output [[buffer(0)]],\n+                        device {1} *input [[ buffer(1)]],\n+                        uint index [[thread_position_in_grid]]) {{\n+  output[index] = {0}(precise::tanh(input[index]));\n+}}\n+\n+\n+#if __METAL_VERSION__ >= 310\n+bfloat dot(bfloat2 a, bfloat2 b) {{\n+  return a.x * b.x + a.y * b.y;\n+}}\n+#endif\n+\n+template<typename T>\n+T complex_div(T a, T b) {{\n+  auto denom = dot(b, b);\n+  return T(dot(a, b), a.y * b.x - a.x * b.y)/denom;\n+}}\n+\n+kernel void tanh_complex_kernel( device {0}2 *output [[buffer(0)]],\n+                                 device {0}2 *input [[ buffer(1)]],\n+                                 uint index [[thread_position_in_grid]]) {{\n+  //tanh(x+iy)=(tanh(x)+itan(y))/(1+itahnh(x)*tan(y));\n+  auto tanh_x = {0}(precise::tanh(input[index].x));\n+  auto tan_y = {0}(precise::tan(input[index].y));\n+  output[index] = complex_div({0}2(tanh_x, tan_y), {0}2({0}(1), tanh_x * tan_y));\n+}}\n+)METAL\";\n"
                }
            ],
            "whole_deleted": "-kernel void erfinv_mps_kernel( device {0} *output [[buffer(0)]],\n-                            device {1} *input [[buffer(1)]],\n-                            uint index [[thread_position_in_grid]]) {{\n-}})METAL\";\n",
            "whole_added": "+kernel void erfinv_kernel( device {0} *output [[buffer(0)]],\n+                           device {1} *input [[buffer(1)]],\n+                           uint index [[thread_position_in_grid]]) {{\n+}}\n+\n+kernel void exp_kernel( device {0} *output [[buffer(0)]],\n+                        device {1} *input [[ buffer(1)]],\n+                        uint index [[thread_position_in_grid]]) {{\n+  output[index] = {0}(precise::exp(input[index]));\n+}}\n+\n+kernel void exp_complex_kernel( device {0}2 *output [[buffer(0)]],\n+                                device {0}2 *input [[ buffer(1)]],\n+                                uint index [[thread_position_in_grid]]) {{\n+  output[index].x = {0}(precise::exp(input[index].x)*precise::cos(input[index].y));\n+  output[index].y = {0}(precise::exp(input[index].x)*precise::sin(input[index].y));\n+}}\n+\n+kernel void tanh_kernel( device {0} *output [[buffer(0)]],\n+                        device {1} *input [[ buffer(1)]],\n+                        uint index [[thread_position_in_grid]]) {{\n+  output[index] = {0}(precise::tanh(input[index]));\n+}}\n+\n+\n+#if __METAL_VERSION__ >= 310\n+bfloat dot(bfloat2 a, bfloat2 b) {{\n+  return a.x * b.x + a.y * b.y;\n+}}\n+#endif\n+\n+template<typename T>\n+T complex_div(T a, T b) {{\n+  auto denom = dot(b, b);\n+  return T(dot(a, b), a.y * b.x - a.x * b.y)/denom;\n+}}\n+\n+kernel void tanh_complex_kernel( device {0}2 *output [[buffer(0)]],\n+                                 device {0}2 *input [[ buffer(1)]],\n+                                 uint index [[thread_position_in_grid]]) {{\n+  //tanh(x+iy)=(tanh(x)+itan(y))/(1+itahnh(x)*tan(y));\n+  auto tanh_x = {0}(precise::tanh(input[index].x));\n+  auto tan_y = {0}(precise::tan(input[index].y));\n+  output[index] = complex_div({0}2(tanh_x, tan_y), {0}2({0}(1), tanh_x * tan_y));\n+}}\n+)METAL\";\n",
            "whole_hunk": "@@ -9,9 +9,9 @@ constant float b[4] = {{-2.118377725, 1.442710462, -0.329097515, 0.012229801}};\n constant float c[4] = {{-1.970840454, -1.624906493, 3.429567803, 1.641345311}};\n constant float d[2] = {{3.543889200, 1.637067800}};\n \n-kernel void erfinv_mps_kernel( device {0} *output [[buffer(0)]],\n-                            device {1} *input [[buffer(1)]],\n-                            uint index [[thread_position_in_grid]]) {{\n+kernel void erfinv_kernel( device {0} *output [[buffer(0)]],\n+                           device {1} *input [[buffer(1)]],\n+                           uint index [[thread_position_in_grid]]) {{\n \n   float y = input[index];\n   float x, z, num, dem; /*working variables */\n@@ -35,4 +35,46 @@ kernel void erfinv_mps_kernel( device {0} *output [[buffer(0)]],\n   }}\n \n   output[index] = {0}(x);\n-}})METAL\";\n+}}\n+\n+kernel void exp_kernel( device {0} *output [[buffer(0)]],\n+                        device {1} *input [[ buffer(1)]],\n+                        uint index [[thread_position_in_grid]]) {{\n+  output[index] = {0}(precise::exp(input[index]));\n+}}\n+\n+kernel void exp_complex_kernel( device {0}2 *output [[buffer(0)]],\n+                                device {0}2 *input [[ buffer(1)]],\n+                                uint index [[thread_position_in_grid]]) {{\n+  output[index].x = {0}(precise::exp(input[index].x)*precise::cos(input[index].y));\n+  output[index].y = {0}(precise::exp(input[index].x)*precise::sin(input[index].y));\n+}}\n+\n+kernel void tanh_kernel( device {0} *output [[buffer(0)]],\n+                        device {1} *input [[ buffer(1)]],\n+                        uint index [[thread_position_in_grid]]) {{\n+  output[index] = {0}(precise::tanh(input[index]));\n+}}\n+\n+\n+#if __METAL_VERSION__ >= 310\n+bfloat dot(bfloat2 a, bfloat2 b) {{\n+  return a.x * b.x + a.y * b.y;\n+}}\n+#endif\n+\n+template<typename T>\n+T complex_div(T a, T b) {{\n+  auto denom = dot(b, b);\n+  return T(dot(a, b), a.y * b.x - a.x * b.y)/denom;\n+}}\n+\n+kernel void tanh_complex_kernel( device {0}2 *output [[buffer(0)]],\n+                                 device {0}2 *input [[ buffer(1)]],\n+                                 uint index [[thread_position_in_grid]]) {{\n+  //tanh(x+iy)=(tanh(x)+itan(y))/(1+itahnh(x)*tan(y));\n+  auto tanh_x = {0}(precise::tanh(input[index].x));\n+  auto tan_y = {0}(precise::tan(input[index].y));\n+  output[index] = complex_div({0}2(tanh_x, tan_y), {0}2({0}(1), tanh_x * tan_y));\n+}}\n+)METAL\";\n"
        },
        {
            "name": "UnaryKernel.mm",
            "path": "aten/src/ATen/native/mps/operations/UnaryKernel.mm",
            "patches": [
                {
                    "old_start": 8,
                    "old_length": 6,
                    "new_start": 8,
                    "new_length": 8,
                    "hunk": "@@ -8,6 +8,8 @@\n #include <ATen/NativeFunctions.h>\n #else\n #include <ATen/ops/erfinv_native.h>\n+#include <ATen/ops/exp_native.h>\n+#include <ATen/ops/tanh_native.h>\n #endif\n \n #include <fmt/format.h>\n"
                },
                {
                    "old_start": 15,
                    "old_length": 14,
                    "new_start": 17,
                    "new_length": 8,
                    "hunk": "@@ -15,14 +17,8 @@\n namespace at::native {\n static mps::MetalShaderLibrary lib(UNARY_KERNEL_TEMPLATE, 2);\n \n-TORCH_IMPL_FUNC(erfinv_out_mps)(const Tensor& self, const Tensor& output_) {\n-  // handle erfinv ops using metal kernel\n-  // erfinv algorithm ported from aten/src/ATen/native/Math.h\n-  // https://github.com/pytorch/pytorch/blob/4154c8ea159fdaecc71ee9af820ac956193c875b/aten/src/ATen/native/Math.h#L152\n-\n-  TORCH_CHECK(self.scalar_type() != ScalarType::Double, \"MPS does not support erfinv op with scalar type: Double\");\n-\n-  Tensor inputTensor = self;\n+static void exec_unary_kernel(const Tensor& self, const Tensor& output_, const std::string& name) {\n+  Tensor inputTensor = self.contiguous();\n   Tensor outputTensor = output_;\n   bool needs_output_copy = false;\n   uint32_t length = output_.numel();\n"
                },
                {
                    "old_start": 31,
                    "old_length": 11,
                    "new_start": 27,
                    "new_length": 16,
                    "hunk": "@@ -31,11 +27,16 @@ TORCH_IMPL_FUNC(erfinv_out_mps)(const Tensor& self, const Tensor& output_) {\n   }\n   using namespace mps;\n   @autoreleasepool {\n-    auto cplState = lib.getPipelineStateForFunc(\"erfinv_mps_kernel\",\n-                                                {scalarToMetalTypeString(outputTensor), scalarToMetalTypeString(self)});\n+    id<MTLComputePipelineState> cplState = nil;\n+    if (c10::isComplexType(self.scalar_type())) {\n+      auto scalarStr = self.scalar_type() == kComplexFloat ? \"float\" : \"half\";\n+      cplState = lib.getPipelineStateForFunc(name + \"_complex_kernel\", {scalarStr, scalarStr});\n+    } else {\n+      cplState = lib.getPipelineStateForFunc(name + \"_kernel\",\n+                                             {scalarToMetalTypeString(outputTensor), scalarToMetalTypeString(self)});\n+    }\n \n-    if (!self.is_contiguous()) {\n-      inputTensor = inputTensor.contiguous();\n+    if (!outputTensor.is_contiguous()) {\n       outputTensor = outputTensor.contiguous();\n       needs_output_copy = true;\n     }\n"
                },
                {
                    "old_start": 44,
                    "old_length": 7,
                    "new_start": 45,
                    "new_length": 7,
                    "hunk": "@@ -44,7 +45,7 @@ TORCH_IMPL_FUNC(erfinv_out_mps)(const Tensor& self, const Tensor& output_) {\n     dispatch_sync(mpsStream->queue(), ^() {\n       id<MTLComputeCommandEncoder> computeEncoder = mpsStream->commandEncoder();\n \n-      getMPSProfiler().beginProfileKernel(cplState, \"erf_inv\", {inputTensor});\n+      getMPSProfiler().beginProfileKernel(cplState, name, {self});\n \n       [computeEncoder setComputePipelineState:cplState];\n       mtl_setBuffer(computeEncoder, outputTensor, 0);\n"
                },
                {
                    "old_start": 58,
                    "old_length": 4,
                    "new_start": 59,
                    "new_length": 19,
                    "hunk": "@@ -58,4 +59,19 @@ TORCH_IMPL_FUNC(erfinv_out_mps)(const Tensor& self, const Tensor& output_) {\n     output_.copy_(outputTensor);\n   }\n }\n+TORCH_IMPL_FUNC(erfinv_out_mps)(const Tensor& self, const Tensor& output_) {\n+  // handle erfinv ops using metal kernel\n+  // erfinv algorithm ported from aten/src/ATen/native/Math.h\n+  // https://github.com/pytorch/pytorch/blob/4154c8ea159fdaecc71ee9af820ac956193c875b/aten/src/ATen/native/Math.h#L152\n+\n+  TORCH_CHECK(self.scalar_type() != ScalarType::Double, \"MPS does not support erfinv op with scalar type: Double\");\n+  exec_unary_kernel(self, output_, \"erfinv\");\n+}\n+\n+TORCH_IMPL_FUNC(exp_out_mps)(const Tensor& self, const Tensor& output_) {\n+  exec_unary_kernel(self, output_, \"exp\");\n+}\n+TORCH_IMPL_FUNC(tanh_out_mps)(const Tensor& self, const Tensor& output_) {\n+  exec_unary_kernel(self, output_, \"tanh\");\n+}\n } // namespace at::native\n"
                }
            ],
            "whole_deleted": "-TORCH_IMPL_FUNC(erfinv_out_mps)(const Tensor& self, const Tensor& output_) {\n-  // handle erfinv ops using metal kernel\n-  // erfinv algorithm ported from aten/src/ATen/native/Math.h\n-  // https://github.com/pytorch/pytorch/blob/4154c8ea159fdaecc71ee9af820ac956193c875b/aten/src/ATen/native/Math.h#L152\n-\n-  TORCH_CHECK(self.scalar_type() != ScalarType::Double, \"MPS does not support erfinv op with scalar type: Double\");\n-\n-  Tensor inputTensor = self;\n-    auto cplState = lib.getPipelineStateForFunc(\"erfinv_mps_kernel\",\n-                                                {scalarToMetalTypeString(outputTensor), scalarToMetalTypeString(self)});\n-    if (!self.is_contiguous()) {\n-      inputTensor = inputTensor.contiguous();\n-      getMPSProfiler().beginProfileKernel(cplState, \"erf_inv\", {inputTensor});\n",
            "whole_added": "+#include <ATen/ops/exp_native.h>\n+#include <ATen/ops/tanh_native.h>\n+static void exec_unary_kernel(const Tensor& self, const Tensor& output_, const std::string& name) {\n+  Tensor inputTensor = self.contiguous();\n+    id<MTLComputePipelineState> cplState = nil;\n+    if (c10::isComplexType(self.scalar_type())) {\n+      auto scalarStr = self.scalar_type() == kComplexFloat ? \"float\" : \"half\";\n+      cplState = lib.getPipelineStateForFunc(name + \"_complex_kernel\", {scalarStr, scalarStr});\n+    } else {\n+      cplState = lib.getPipelineStateForFunc(name + \"_kernel\",\n+                                             {scalarToMetalTypeString(outputTensor), scalarToMetalTypeString(self)});\n+    }\n+    if (!outputTensor.is_contiguous()) {\n+      getMPSProfiler().beginProfileKernel(cplState, name, {self});\n+TORCH_IMPL_FUNC(erfinv_out_mps)(const Tensor& self, const Tensor& output_) {\n+  // handle erfinv ops using metal kernel\n+  // erfinv algorithm ported from aten/src/ATen/native/Math.h\n+  // https://github.com/pytorch/pytorch/blob/4154c8ea159fdaecc71ee9af820ac956193c875b/aten/src/ATen/native/Math.h#L152\n+\n+  TORCH_CHECK(self.scalar_type() != ScalarType::Double, \"MPS does not support erfinv op with scalar type: Double\");\n+  exec_unary_kernel(self, output_, \"erfinv\");\n+}\n+\n+TORCH_IMPL_FUNC(exp_out_mps)(const Tensor& self, const Tensor& output_) {\n+  exec_unary_kernel(self, output_, \"exp\");\n+}\n+TORCH_IMPL_FUNC(tanh_out_mps)(const Tensor& self, const Tensor& output_) {\n+  exec_unary_kernel(self, output_, \"tanh\");\n+}\n",
            "whole_hunk": "@@ -8,6 +8,8 @@\n #include <ATen/NativeFunctions.h>\n #else\n #include <ATen/ops/erfinv_native.h>\n+#include <ATen/ops/exp_native.h>\n+#include <ATen/ops/tanh_native.h>\n #endif\n \n #include <fmt/format.h>\n@@ -15,14 +17,8 @@\n namespace at::native {\n static mps::MetalShaderLibrary lib(UNARY_KERNEL_TEMPLATE, 2);\n \n-TORCH_IMPL_FUNC(erfinv_out_mps)(const Tensor& self, const Tensor& output_) {\n-  // handle erfinv ops using metal kernel\n-  // erfinv algorithm ported from aten/src/ATen/native/Math.h\n-  // https://github.com/pytorch/pytorch/blob/4154c8ea159fdaecc71ee9af820ac956193c875b/aten/src/ATen/native/Math.h#L152\n-\n-  TORCH_CHECK(self.scalar_type() != ScalarType::Double, \"MPS does not support erfinv op with scalar type: Double\");\n-\n-  Tensor inputTensor = self;\n+static void exec_unary_kernel(const Tensor& self, const Tensor& output_, const std::string& name) {\n+  Tensor inputTensor = self.contiguous();\n   Tensor outputTensor = output_;\n   bool needs_output_copy = false;\n   uint32_t length = output_.numel();\n@@ -31,11 +27,16 @@ TORCH_IMPL_FUNC(erfinv_out_mps)(const Tensor& self, const Tensor& output_) {\n   }\n   using namespace mps;\n   @autoreleasepool {\n-    auto cplState = lib.getPipelineStateForFunc(\"erfinv_mps_kernel\",\n-                                                {scalarToMetalTypeString(outputTensor), scalarToMetalTypeString(self)});\n+    id<MTLComputePipelineState> cplState = nil;\n+    if (c10::isComplexType(self.scalar_type())) {\n+      auto scalarStr = self.scalar_type() == kComplexFloat ? \"float\" : \"half\";\n+      cplState = lib.getPipelineStateForFunc(name + \"_complex_kernel\", {scalarStr, scalarStr});\n+    } else {\n+      cplState = lib.getPipelineStateForFunc(name + \"_kernel\",\n+                                             {scalarToMetalTypeString(outputTensor), scalarToMetalTypeString(self)});\n+    }\n \n-    if (!self.is_contiguous()) {\n-      inputTensor = inputTensor.contiguous();\n+    if (!outputTensor.is_contiguous()) {\n       outputTensor = outputTensor.contiguous();\n       needs_output_copy = true;\n     }\n@@ -44,7 +45,7 @@ TORCH_IMPL_FUNC(erfinv_out_mps)(const Tensor& self, const Tensor& output_) {\n     dispatch_sync(mpsStream->queue(), ^() {\n       id<MTLComputeCommandEncoder> computeEncoder = mpsStream->commandEncoder();\n \n-      getMPSProfiler().beginProfileKernel(cplState, \"erf_inv\", {inputTensor});\n+      getMPSProfiler().beginProfileKernel(cplState, name, {self});\n \n       [computeEncoder setComputePipelineState:cplState];\n       mtl_setBuffer(computeEncoder, outputTensor, 0);\n@@ -58,4 +59,19 @@ TORCH_IMPL_FUNC(erfinv_out_mps)(const Tensor& self, const Tensor& output_) {\n     output_.copy_(outputTensor);\n   }\n }\n+TORCH_IMPL_FUNC(erfinv_out_mps)(const Tensor& self, const Tensor& output_) {\n+  // handle erfinv ops using metal kernel\n+  // erfinv algorithm ported from aten/src/ATen/native/Math.h\n+  // https://github.com/pytorch/pytorch/blob/4154c8ea159fdaecc71ee9af820ac956193c875b/aten/src/ATen/native/Math.h#L152\n+\n+  TORCH_CHECK(self.scalar_type() != ScalarType::Double, \"MPS does not support erfinv op with scalar type: Double\");\n+  exec_unary_kernel(self, output_, \"erfinv\");\n+}\n+\n+TORCH_IMPL_FUNC(exp_out_mps)(const Tensor& self, const Tensor& output_) {\n+  exec_unary_kernel(self, output_, \"exp\");\n+}\n+TORCH_IMPL_FUNC(tanh_out_mps)(const Tensor& self, const Tensor& output_) {\n+  exec_unary_kernel(self, output_, \"tanh\");\n+}\n } // namespace at::native\n"
        },
        {
            "name": "UnaryOps.mm",
            "path": "aten/src/ATen/native/mps/operations/UnaryOps.mm",
            "patches": [
                {
                    "old_start": 26,
                    "old_length": 7,
                    "new_start": 26,
                    "new_length": 6,
                    "hunk": "@@ -26,7 +26,6 @@\n #include <ATen/ops/cumsum_native.h>\n #include <ATen/ops/erf_native.h>\n #include <ATen/ops/exp2_native.h>\n-#include <ATen/ops/exp_native.h>\n #include <ATen/ops/expm1_native.h>\n #include <ATen/ops/floor_native.h>\n #include <ATen/ops/frac_native.h>\n"
                },
                {
                    "old_start": 54,
                    "old_length": 7,
                    "new_start": 53,
                    "new_length": 6,
                    "hunk": "@@ -54,7 +53,6 @@\n #include <ATen/ops/sinh_native.h>\n #include <ATen/ops/sqrt_native.h>\n #include <ATen/ops/tan_native.h>\n-#include <ATen/ops/tanh_native.h>\n #include <ATen/ops/trunc_native.h>\n #include <ATen/ops/view_as_real.h>\n #endif\n"
                },
                {
                    "old_start": 236,
                    "old_length": 7,
                    "new_start": 234,
                    "new_length": 6,
                    "hunk": "@@ -236,7 +234,6 @@ CREATE_MPS_STRUCTURED_UNARY_ROUNDING_TORCH_IMPL_FUNC(round_out_mps, round)\n     });                                                                                                          \\\n   }\n \n-CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(exp_out_mps, exponent)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(exp2_out_mps, exponentBase2)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(reciprocal_out_mps, reciprocal)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(sqrt_out_mps, squareRoot)\n"
                },
                {
                    "old_start": 254,
                    "old_length": 7,
                    "new_start": 251,
                    "new_length": 6,
                    "hunk": "@@ -254,7 +251,6 @@ CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(acos_out_mps, acos)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(atan_out_mps, atan)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(sinh_out_mps, sinh)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(cosh_out_mps, cosh)\n-CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(tanh_out_mps, tanh)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(asinh_out_mps, asinh)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(acosh_out_mps, acosh)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(atanh_out_mps, atanh)\n"
                }
            ],
            "whole_deleted": "-#include <ATen/ops/exp_native.h>\n-#include <ATen/ops/tanh_native.h>\n-CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(exp_out_mps, exponent)\n-CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(tanh_out_mps, tanh)\n",
            "whole_added": "",
            "whole_hunk": "@@ -26,7 +26,6 @@\n #include <ATen/ops/cumsum_native.h>\n #include <ATen/ops/erf_native.h>\n #include <ATen/ops/exp2_native.h>\n-#include <ATen/ops/exp_native.h>\n #include <ATen/ops/expm1_native.h>\n #include <ATen/ops/floor_native.h>\n #include <ATen/ops/frac_native.h>\n@@ -54,7 +53,6 @@\n #include <ATen/ops/sinh_native.h>\n #include <ATen/ops/sqrt_native.h>\n #include <ATen/ops/tan_native.h>\n-#include <ATen/ops/tanh_native.h>\n #include <ATen/ops/trunc_native.h>\n #include <ATen/ops/view_as_real.h>\n #endif\n@@ -236,7 +234,6 @@ CREATE_MPS_STRUCTURED_UNARY_ROUNDING_TORCH_IMPL_FUNC(round_out_mps, round)\n     });                                                                                                          \\\n   }\n \n-CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(exp_out_mps, exponent)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(exp2_out_mps, exponentBase2)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(reciprocal_out_mps, reciprocal)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(sqrt_out_mps, squareRoot)\n@@ -254,7 +251,6 @@ CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(acos_out_mps, acos)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(atan_out_mps, atan)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(sinh_out_mps, sinh)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(cosh_out_mps, cosh)\n-CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(tanh_out_mps, tanh)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(asinh_out_mps, asinh)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(acosh_out_mps, acosh)\n CREATE_MPS_STRUCTURED_UNARY_TORCH_IMPL_FUNC(atanh_out_mps, atanh)\n"
        },
        {
            "name": "test_mps.py",
            "path": "test/test_mps.py",
            "patches": [
                {
                    "old_start": 270,
                    "old_length": 6,
                    "new_start": 270,
                    "new_length": 7,
                    "hunk": "@@ -270,6 +270,7 @@ def mps_ops_modifier(ops):\n         'empty_permuted',\n         'empty_strided',\n         'eye',\n+        'exp',\n         'expand',\n         'expand_as',\n         'flatten',\n"
                },
                {
                    "old_start": 306,
                    "old_length": 6,
                    "new_start": 307,
                    "new_length": 7,
                    "hunk": "@@ -306,6 +307,7 @@ def mps_ops_modifier(ops):\n         'nn.functional.conv_transpose2d',\n         'nn.functional.feature_alpha_dropoutwithout_train',\n         'nn.functional.padcircular',\n+        'nn.functional.tanhshrink',\n         'nn.functional.unfold',\n         'nonzero',\n         'ones',\n"
                },
                {
                    "old_start": 333,
                    "old_length": 6,
                    "new_start": 335,
                    "new_length": 7,
                    "hunk": "@@ -333,6 +335,7 @@ def mps_ops_modifier(ops):\n         'sub',\n         'svd',\n         't',\n+        'tanh',\n         'tensor_split',\n         'transpose',\n         'T',\n"
                },
                {
                    "old_start": 389,
                    "old_length": 7,
                    "new_start": 392,
                    "new_length": 6,
                    "hunk": "@@ -389,7 +392,6 @@ def mps_ops_modifier(ops):\n         'eq',\n         'equal',\n         'exp2',\n-        'exp',\n         'expm1',\n         'fft.fft',\n         'fft.fft2',\n"
                },
                {
                    "old_start": 447,
                    "old_length": 7,
                    "new_start": 449,
                    "new_length": 6,
                    "hunk": "@@ -447,7 +449,6 @@ def mps_ops_modifier(ops):\n         'nn.functional.pixel_unshuffle',\n         'nn.functional.rms_norm',\n         'nn.functional.softsign',\n-        'nn.functional.tanhshrink',\n         'pinverse',\n         'prod',\n         'reciprocal',\n"
                },
                {
                    "old_start": 465,
                    "old_length": 7,
                    "new_start": 466,
                    "new_length": 6,
                    "hunk": "@@ -465,7 +466,6 @@ def mps_ops_modifier(ops):\n         'sum',\n         'sum_to_size',\n         'tan',\n-        'tanh',\n         'tensordot',\n         'trace',\n         'trapz',\n"
                },
                {
                    "old_start": 1612,
                    "old_length": 14,
                    "new_start": 1612,
                    "new_length": 19,
                    "hunk": "@@ -1612,14 +1612,19 @@ class TestAvgPool(TestCaseMPS):\n class TestMPS(TestCaseMPS):\n     def test_exp(self, device=\"mps\", dtype=torch.float):\n         for v in (2, -2) + ((1j, 1 + 1j) if dtype.is_complex else ()):\n-            b = torch.arange(18, device=\"cpu\") / 3 * math.pi\n-            a = torch.tensor(v, dtype=dtype, device=\"cpu\") * b\n-            a = a.to(dtype).to(\"mps\")\n+            b = torch.arange(18, dtype=dtype, device=device) / 3 * math.pi\n+            a = torch.tensor(v, dtype=dtype, device=\"mps\") * b\n             self.compare_with_numpy(torch.exp, np.exp, a)\n \n     def test_exp1(self, device=\"mps\", dtype=torch.float):\n-        input = torch.tensor([-0.1, 3.0, -0.9]).to('mps')\n-        output = torch.exp(input).to('cpu')\n+        input = torch.tensor([-0.1, 1.0, -0.9, 0.1], device=device, dtype=dtype)\n+        output = torch.exp(input)\n+        output_cpu = torch.exp(input.cpu())\n+        # If exponentWithTensor: MPS call is used on M1 running 14.5 test will fail with\n+        # Mismatched elements: 3 / 4 (75.0%)\n+        # Greatest absolute difference: 1.1920928955078125e-07 at index (3,) (up to 1e-08 allowed)\n+        # Greatest relative difference: 1.0786502002702036e-07 at index (3,) (up to 1e-08 allowed)\n+        self.assertEqual(output, output_cpu, atol=1e-8, rtol=1e-8)\n \n     def test_exp_strided_output(self):\n         x = torch.rand((256, 10), device='mps')"
                }
            ],
            "whole_deleted": "-        'exp',\n-        'nn.functional.tanhshrink',\n-        'tanh',\n-            b = torch.arange(18, device=\"cpu\") / 3 * math.pi\n-            a = torch.tensor(v, dtype=dtype, device=\"cpu\") * b\n-            a = a.to(dtype).to(\"mps\")\n-        input = torch.tensor([-0.1, 3.0, -0.9]).to('mps')\n-        output = torch.exp(input).to('cpu')\n",
            "whole_added": "+        'exp',\n+        'nn.functional.tanhshrink',\n+        'tanh',\n+            b = torch.arange(18, dtype=dtype, device=device) / 3 * math.pi\n+            a = torch.tensor(v, dtype=dtype, device=\"mps\") * b\n+        input = torch.tensor([-0.1, 1.0, -0.9, 0.1], device=device, dtype=dtype)\n+        output = torch.exp(input)\n+        output_cpu = torch.exp(input.cpu())\n+        # If exponentWithTensor: MPS call is used on M1 running 14.5 test will fail with\n+        # Mismatched elements: 3 / 4 (75.0%)\n+        # Greatest absolute difference: 1.1920928955078125e-07 at index (3,) (up to 1e-08 allowed)\n+        # Greatest relative difference: 1.0786502002702036e-07 at index (3,) (up to 1e-08 allowed)\n+        self.assertEqual(output, output_cpu, atol=1e-8, rtol=1e-8)\n",
            "whole_hunk": "@@ -270,6 +270,7 @@ def mps_ops_modifier(ops):\n         'empty_permuted',\n         'empty_strided',\n         'eye',\n+        'exp',\n         'expand',\n         'expand_as',\n         'flatten',\n@@ -306,6 +307,7 @@ def mps_ops_modifier(ops):\n         'nn.functional.conv_transpose2d',\n         'nn.functional.feature_alpha_dropoutwithout_train',\n         'nn.functional.padcircular',\n+        'nn.functional.tanhshrink',\n         'nn.functional.unfold',\n         'nonzero',\n         'ones',\n@@ -333,6 +335,7 @@ def mps_ops_modifier(ops):\n         'sub',\n         'svd',\n         't',\n+        'tanh',\n         'tensor_split',\n         'transpose',\n         'T',\n@@ -389,7 +392,6 @@ def mps_ops_modifier(ops):\n         'eq',\n         'equal',\n         'exp2',\n-        'exp',\n         'expm1',\n         'fft.fft',\n         'fft.fft2',\n@@ -447,7 +449,6 @@ def mps_ops_modifier(ops):\n         'nn.functional.pixel_unshuffle',\n         'nn.functional.rms_norm',\n         'nn.functional.softsign',\n-        'nn.functional.tanhshrink',\n         'pinverse',\n         'prod',\n         'reciprocal',\n@@ -465,7 +466,6 @@ def mps_ops_modifier(ops):\n         'sum',\n         'sum_to_size',\n         'tan',\n-        'tanh',\n         'tensordot',\n         'trace',\n         'trapz',\n@@ -1612,14 +1612,19 @@ class TestAvgPool(TestCaseMPS):\n class TestMPS(TestCaseMPS):\n     def test_exp(self, device=\"mps\", dtype=torch.float):\n         for v in (2, -2) + ((1j, 1 + 1j) if dtype.is_complex else ()):\n-            b = torch.arange(18, device=\"cpu\") / 3 * math.pi\n-            a = torch.tensor(v, dtype=dtype, device=\"cpu\") * b\n-            a = a.to(dtype).to(\"mps\")\n+            b = torch.arange(18, dtype=dtype, device=device) / 3 * math.pi\n+            a = torch.tensor(v, dtype=dtype, device=\"mps\") * b\n             self.compare_with_numpy(torch.exp, np.exp, a)\n \n     def test_exp1(self, device=\"mps\", dtype=torch.float):\n-        input = torch.tensor([-0.1, 3.0, -0.9]).to('mps')\n-        output = torch.exp(input).to('cpu')\n+        input = torch.tensor([-0.1, 1.0, -0.9, 0.1], device=device, dtype=dtype)\n+        output = torch.exp(input)\n+        output_cpu = torch.exp(input.cpu())\n+        # If exponentWithTensor: MPS call is used on M1 running 14.5 test will fail with\n+        # Mismatched elements: 3 / 4 (75.0%)\n+        # Greatest absolute difference: 1.1920928955078125e-07 at index (3,) (up to 1e-08 allowed)\n+        # Greatest relative difference: 1.0786502002702036e-07 at index (3,) (up to 1e-08 allowed)\n+        self.assertEqual(output, output_cpu, atol=1e-8, rtol=1e-8)\n \n     def test_exp_strided_output(self):\n         x = torch.rand((256, 10), device='mps')"
        }
    ]
},
{
    "Id": 333,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/560213de2d8f734987e25680e72d565501ab8318",
    "date": "2024-01-18T03:06:42+00:00",
    "message": "[export] Error on not pytree-flattened nodes (#117598)\n\nAttempts to make the input/output mismatch error better by first checking if the inputs/outputs are able to be pytree flattened into supporting types (tensors, symints, ...). So if user passes in some datastructure which does not have a pytree flatten registration, this will error with the message \"It looks like one of the inputs is with type CustomType is not supported or pytree flatten-able.... please register a pytree flatten/unflatten function using the pytree.register_pytree_node API\".\n\nThe check inside of produce_matching should now only error if something unexpected happens (dynamo accidentally adds an input or removes an output), and should be considered an internal error.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/117598\nApproved by: https://github.com/avikchaudhuri, https://github.com/BowenBao",
    "label": "YES",
    "changes": [
        {
            "name": "test_export.py",
            "path": "test/dynamo/test_export.py",
            "patches": [
                {
                    "old_start": 2231,
                    "old_length": 33,
                    "new_start": 2231,
                    "new_length": 55,
                    "hunk": "@@ -2231,33 +2231,55 @@ def forward(self, x):\n             return t.x + t.y\n \n         with self.assertRaisesRegex(\n-            AssertionError,\n-            \"graph-captured input #1, of type .*Tensor.*, \"\n-            \"is not among original inputs of types: .*Tensors\",\n+            UserError,\n+            \"It looks like one of the inputs with type .*Tensors.* \"\n+            \"is not supported or pytree-flattenable\",\n         ):\n-            torch._dynamo.export(\n-                f, Tensors(x=torch.randn(10), y=torch.randn(10)), aten_graph=False\n+            torch._dynamo.export(f, aten_graph=False)(\n+                Tensors(x=torch.randn(10), y=torch.randn(10))\n             )\n \n         def f(x, y):\n             return Tensors(x=x.sin(), y=y.cos())\n \n         with self.assertRaisesRegex(\n-            AssertionError,\n-            \"original output #1 is .*Tensors.*, \"\n-            \"but only the following types are supported\",\n+            UserError,\n+            \"It looks like one of the outputs with type .*Tensors.* \"\n+            \"is not supported or pytree-flattenable\",\n         ):\n-            torch._dynamo.export(f, torch.randn(10), torch.randn(10), aten_graph=False)\n+            torch._dynamo.export(f, aten_graph=False)(torch.randn(10), torch.randn(10))\n+\n+    def test_empty(self):\n+        def f(x):\n+            return x\n+\n+        exported = torch._dynamo.export(f)(torch.randn(3, 3))\n+        out_graph = exported[0]\n+        inp = torch.randn(3, 3)\n+        self.assertTrue(torch._dynamo.utils.same(inp, out_graph(inp)))\n+\n+        class M(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.a = torch.ones(3, 3)\n+\n+            def forward(self):\n+                return self.a\n+\n+        exported = torch._dynamo.export(M())()\n+        out_graph = exported[0]\n+        self.assertTrue(torch._dynamo.utils.same(torch.ones(3, 3), out_graph()))\n \n     def test_none_out(self):\n         def f(x, y):\n             _ = x + y\n \n         with self.assertRaisesRegex(\n-            AssertionError,\n-            \"original output #1 is None, but only the following types are supported\",\n+            UserError,\n+            \"It looks like one of the outputs with type .*None.* \"\n+            \"is not supported or pytree-flattenable\",\n         ):\n-            torch._dynamo.export(f, torch.randn(10), torch.randn(10), aten_graph=False)\n+            torch._dynamo.export(f, aten_graph=False)(torch.randn(10), torch.randn(10))\n \n     def test_primitive_constant_output(self):\n         def foo(x):\n"
                },
                {
                    "old_start": 2266,
                    "old_length": 8,
                    "new_start": 2288,
                    "new_length": 9,
                    "hunk": "@@ -2266,8 +2288,9 @@ def forward(self, x):\n             return y * x, y\n \n         with self.assertRaisesRegex(\n-            AssertionError,\n-            \"original output #2 is 5, but only the following types are supported\",\n+            UserError,\n+            \"It looks like one of the outputs with type .*int.* \"\n+            \"is not supported or pytree-flattenable\",\n         ):\n             torch.export.export(foo, (torch.tensor(3),))\n \n"
                },
                {
                    "old_start": 2276,
                    "old_length": 8,
                    "new_start": 2299,
                    "new_length": 9,
                    "hunk": "@@ -2276,8 +2299,9 @@ def forward(self, x):\n \n         # new behavior\n         with self.assertRaisesRegex(\n-            AssertionError,\n-            \"original output #2 is 5, but only the following types are supported\",\n+            UserError,\n+            \"It looks like one of the outputs with type .*int.* \"\n+            \"is not supported or pytree-flattenable\",\n         ):\n             torch.export.export(bar, (torch.tensor(3), 5))\n \n"
                },
                {
                    "old_start": 2285,
                    "old_length": 8,
                    "new_start": 2309,
                    "new_length": 9,
                    "hunk": "@@ -2285,8 +2309,9 @@ def forward(self, x):\n             return y * x, y - 1\n \n         with self.assertRaisesRegex(\n-            AssertionError,\n-            \"original output #2 is 4, but only the following types are supported\",\n+            UserError,\n+            \"It looks like one of the outputs with type .*int.* \"\n+            \"is not supported or pytree-flattenable\",\n         ):\n             torch.export.export(qux, (torch.tensor(3), 5))\n \n"
                }
            ],
            "whole_deleted": "-            AssertionError,\n-            \"graph-captured input #1, of type .*Tensor.*, \"\n-            \"is not among original inputs of types: .*Tensors\",\n-            torch._dynamo.export(\n-                f, Tensors(x=torch.randn(10), y=torch.randn(10)), aten_graph=False\n-            AssertionError,\n-            \"original output #1 is .*Tensors.*, \"\n-            \"but only the following types are supported\",\n-            torch._dynamo.export(f, torch.randn(10), torch.randn(10), aten_graph=False)\n-            AssertionError,\n-            \"original output #1 is None, but only the following types are supported\",\n-            torch._dynamo.export(f, torch.randn(10), torch.randn(10), aten_graph=False)\n-            AssertionError,\n-            \"original output #2 is 5, but only the following types are supported\",\n-            AssertionError,\n-            \"original output #2 is 5, but only the following types are supported\",\n-            AssertionError,\n-            \"original output #2 is 4, but only the following types are supported\",\n",
            "whole_added": "+            UserError,\n+            \"It looks like one of the inputs with type .*Tensors.* \"\n+            \"is not supported or pytree-flattenable\",\n+            torch._dynamo.export(f, aten_graph=False)(\n+                Tensors(x=torch.randn(10), y=torch.randn(10))\n+            UserError,\n+            \"It looks like one of the outputs with type .*Tensors.* \"\n+            \"is not supported or pytree-flattenable\",\n+            torch._dynamo.export(f, aten_graph=False)(torch.randn(10), torch.randn(10))\n+\n+    def test_empty(self):\n+        def f(x):\n+            return x\n+\n+        exported = torch._dynamo.export(f)(torch.randn(3, 3))\n+        out_graph = exported[0]\n+        inp = torch.randn(3, 3)\n+        self.assertTrue(torch._dynamo.utils.same(inp, out_graph(inp)))\n+\n+        class M(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.a = torch.ones(3, 3)\n+\n+            def forward(self):\n+                return self.a\n+\n+        exported = torch._dynamo.export(M())()\n+        out_graph = exported[0]\n+        self.assertTrue(torch._dynamo.utils.same(torch.ones(3, 3), out_graph()))\n+            UserError,\n+            \"It looks like one of the outputs with type .*None.* \"\n+            \"is not supported or pytree-flattenable\",\n+            torch._dynamo.export(f, aten_graph=False)(torch.randn(10), torch.randn(10))\n+            UserError,\n+            \"It looks like one of the outputs with type .*int.* \"\n+            \"is not supported or pytree-flattenable\",\n+            UserError,\n+            \"It looks like one of the outputs with type .*int.* \"\n+            \"is not supported or pytree-flattenable\",\n+            UserError,\n+            \"It looks like one of the outputs with type .*int.* \"\n+            \"is not supported or pytree-flattenable\",\n",
            "whole_hunk": "@@ -2231,33 +2231,55 @@ def forward(self, x):\n             return t.x + t.y\n \n         with self.assertRaisesRegex(\n-            AssertionError,\n-            \"graph-captured input #1, of type .*Tensor.*, \"\n-            \"is not among original inputs of types: .*Tensors\",\n+            UserError,\n+            \"It looks like one of the inputs with type .*Tensors.* \"\n+            \"is not supported or pytree-flattenable\",\n         ):\n-            torch._dynamo.export(\n-                f, Tensors(x=torch.randn(10), y=torch.randn(10)), aten_graph=False\n+            torch._dynamo.export(f, aten_graph=False)(\n+                Tensors(x=torch.randn(10), y=torch.randn(10))\n             )\n \n         def f(x, y):\n             return Tensors(x=x.sin(), y=y.cos())\n \n         with self.assertRaisesRegex(\n-            AssertionError,\n-            \"original output #1 is .*Tensors.*, \"\n-            \"but only the following types are supported\",\n+            UserError,\n+            \"It looks like one of the outputs with type .*Tensors.* \"\n+            \"is not supported or pytree-flattenable\",\n         ):\n-            torch._dynamo.export(f, torch.randn(10), torch.randn(10), aten_graph=False)\n+            torch._dynamo.export(f, aten_graph=False)(torch.randn(10), torch.randn(10))\n+\n+    def test_empty(self):\n+        def f(x):\n+            return x\n+\n+        exported = torch._dynamo.export(f)(torch.randn(3, 3))\n+        out_graph = exported[0]\n+        inp = torch.randn(3, 3)\n+        self.assertTrue(torch._dynamo.utils.same(inp, out_graph(inp)))\n+\n+        class M(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.a = torch.ones(3, 3)\n+\n+            def forward(self):\n+                return self.a\n+\n+        exported = torch._dynamo.export(M())()\n+        out_graph = exported[0]\n+        self.assertTrue(torch._dynamo.utils.same(torch.ones(3, 3), out_graph()))\n \n     def test_none_out(self):\n         def f(x, y):\n             _ = x + y\n \n         with self.assertRaisesRegex(\n-            AssertionError,\n-            \"original output #1 is None, but only the following types are supported\",\n+            UserError,\n+            \"It looks like one of the outputs with type .*None.* \"\n+            \"is not supported or pytree-flattenable\",\n         ):\n-            torch._dynamo.export(f, torch.randn(10), torch.randn(10), aten_graph=False)\n+            torch._dynamo.export(f, aten_graph=False)(torch.randn(10), torch.randn(10))\n \n     def test_primitive_constant_output(self):\n         def foo(x):\n@@ -2266,8 +2288,9 @@ def forward(self, x):\n             return y * x, y\n \n         with self.assertRaisesRegex(\n-            AssertionError,\n-            \"original output #2 is 5, but only the following types are supported\",\n+            UserError,\n+            \"It looks like one of the outputs with type .*int.* \"\n+            \"is not supported or pytree-flattenable\",\n         ):\n             torch.export.export(foo, (torch.tensor(3),))\n \n@@ -2276,8 +2299,9 @@ def forward(self, x):\n \n         # new behavior\n         with self.assertRaisesRegex(\n-            AssertionError,\n-            \"original output #2 is 5, but only the following types are supported\",\n+            UserError,\n+            \"It looks like one of the outputs with type .*int.* \"\n+            \"is not supported or pytree-flattenable\",\n         ):\n             torch.export.export(bar, (torch.tensor(3), 5))\n \n@@ -2285,8 +2309,9 @@ def forward(self, x):\n             return y * x, y - 1\n \n         with self.assertRaisesRegex(\n-            AssertionError,\n-            \"original output #2 is 4, but only the following types are supported\",\n+            UserError,\n+            \"It looks like one of the outputs with type .*int.* \"\n+            \"is not supported or pytree-flattenable\",\n         ):\n             torch.export.export(qux, (torch.tensor(3), 5))\n \n"
        },
        {
            "name": "test_fx_to_onnx_with_onnxruntime.py",
            "path": "test/onnx/test_fx_to_onnx_with_onnxruntime.py",
            "patches": [
                {
                    "old_start": 95,
                    "old_length": 8,
                    "new_start": 95,
                    "new_length": 7,
                    "hunk": "@@ -95,8 +95,7 @@ class TestFxToOnnxWithOnnxRuntime(onnx_test_common._TestONNXRuntime):\n         self.run_test_with_fx_to_onnx_exporter_and_onnx_runtime(func, (tensor_x,))\n \n     @pytorch_test_common.xfail(\n-        error_message=\"graph-captured input #2, of type <class 'torch.Tensor'>, \"\n-        \"is not among original inputs of types: (<class 'torch.Tensor'>)\",\n+        error_message=\"Unexpectedly found a <class 'torch.Tensor'> in the inputs.\",\n         reason=\"https://github.com/pytorch/pytorch/issues/96379\",\n     )\n     def test_func_with_args_and_tensor_kwargs(self):\n"
                }
            ],
            "whole_deleted": "-        error_message=\"graph-captured input #2, of type <class 'torch.Tensor'>, \"\n-        \"is not among original inputs of types: (<class 'torch.Tensor'>)\",\n",
            "whole_added": "+        error_message=\"Unexpectedly found a <class 'torch.Tensor'> in the inputs.\",\n",
            "whole_hunk": "@@ -95,8 +95,7 @@ class TestFxToOnnxWithOnnxRuntime(onnx_test_common._TestONNXRuntime):\n         self.run_test_with_fx_to_onnx_exporter_and_onnx_runtime(func, (tensor_x,))\n \n     @pytorch_test_common.xfail(\n-        error_message=\"graph-captured input #2, of type <class 'torch.Tensor'>, \"\n-        \"is not among original inputs of types: (<class 'torch.Tensor'>)\",\n+        error_message=\"Unexpectedly found a <class 'torch.Tensor'> in the inputs.\",\n         reason=\"https://github.com/pytorch/pytorch/issues/96379\",\n     )\n     def test_func_with_args_and_tensor_kwargs(self):\n"
        },
        {
            "name": "eval_frame.py",
            "path": "torch/_dynamo/eval_frame.py",
            "patches": [
                {
                    "old_start": 938,
                    "old_length": 60,
                    "new_start": 938,
                    "new_length": 67,
                    "hunk": "@@ -938,60 +938,67 @@ def rewrite_signature(\n ):\n     orig_args, orig_kwargs = pytree.tree_unflatten(flat_args, in_spec)\n \n-    supported_types = (torch.Tensor, torch.SymInt, torch.SymFloat, torch.SymBool)\n-\n-    def is_supported_type(val):\n-        return isinstance(val, supported_types)\n-\n-    def produce_matching(sources, candidates):\n-        source_types = \" or \".join(\n-            [\n-                desc\n-                + \" of types: (\"\n-                + \", \".join([str(type(val)) for val in vals])\n-                + \")\"\n-                for desc, vals in sources.items()\n-            ]\n-        )\n-        source_vals = [val for vals in sources.values() for val in vals]\n+    def check_user_input_output(flat_values, error_type):\n+        supported_types = [\n+            torch.Tensor,\n+            torch.SymInt,\n+            torch.SymFloat,\n+            torch.SymBool,\n+            torch._C.ScriptObject,\n+        ]\n+        if error_type == UserErrorType.INVALID_INPUT:\n+            supported_types.extend([int, str, bool, float])\n+\n+        def is_supported_type(val):\n+            return isinstance(val, tuple(supported_types))\n+\n+        value_type = \"input\" if error_type == UserErrorType.INVALID_INPUT else \"output\"\n+        # We only check that the outputs are not None. Inputs can be None.\n+        for v in flat_values:\n+            if not is_supported_type(v):\n+                if error_type == UserErrorType.INVALID_INPUT and v is None:\n+                    continue\n+\n+                raise UserError(\n+                    error_type,\n+                    f\"It looks like one of the {value_type}s with type `{type(v)}` \"\n+                    \"is not supported or pytree-flattenable. \\n\"\n+                    f\"Exported graphs {value_type}s can only contain the \"\n+                    f\"following supported types: {supported_types}. \\n\"\n+                    \"If you are using a custom class object, \"\n+                    \"please register a pytree_flatten/unflatten function \"\n+                    \"using `torch.utils._pytree.register_pytree_node` or \"\n+                    \"`torch.export.register_dataclass`.\",\n+                )\n+\n+    check_user_input_output(flat_args, UserErrorType.INVALID_INPUT)\n+    flat_results_traced, out_spec_traced = pytree.tree_flatten(dynamo_traced_result)\n+    check_user_input_output(flat_results_traced, UserErrorType.INVALID_OUTPUT)\n+\n+    def produce_matching(debug_type, sources, candidates):\n         matched_elements_positions = []\n         dict_of_source_vals = {}\n-        for i, val in enumerate(source_vals):\n+        for i, val in enumerate(sources):\n             dict_of_source_vals[id(val)] = i\n \n-        for candidate_desc, candidate_vals in candidates.items():\n-            for i, val in enumerate(candidate_vals):\n-                if is_supported_type(val):\n-                    if id(val) in dict_of_source_vals:\n-                        matched_elements_positions.append(dict_of_source_vals[id(val)])\n-                    else:\n-                        raise AssertionError(\n-                            f\"{candidate_desc} #{i+1}, of type {type(val)}, is not among {source_types}\"\n-                            'Set TORCH_LOGS=\"+export\" for more information.'\n-                        )\n-                else:\n-                    raise AssertionError(\n-                        f\"{candidate_desc} #{i+1} is {val}, but only \"\n-                        f\"the following types are supported: {supported_types}\"\n-                        'Set TORCH_LOGS=\"+export\" for more information.'\n-                    )\n+        for i, val in enumerate(candidates):\n+            if id(val) not in dict_of_source_vals:\n+                raise AssertionError(\n+                    f\"Unexpectedly found a {type(val)} in the {debug_type}.\\n\"\n+                    'Please file an issue along with a paste of the logs from TORCH_LOGS=\"+export\"'\n+                )\n+\n+            matched_elements_positions.append(dict_of_source_vals[id(val)])\n \n         return matched_elements_positions\n \n     matched_input_elements_positions = produce_matching(\n-        sources={\"original inputs\": flat_args},\n-        candidates={\"graph-captured input\": graph_captured_input},\n+        \"inputs\", flat_args, graph_captured_input\n     )\n \n-    flat_results_traced, out_spec_traced = pytree.tree_flatten(dynamo_traced_result)\n-\n     assert graph_captured_output is not None\n     matched_output_elements_positions = produce_matching(\n-        sources={\n-            \"graph-captured outputs\": list(graph_captured_output),\n-            \"original inputs\": flat_args,\n-        },\n-        candidates={\"original output\": flat_results_traced},\n+        \"outputs\", list(graph_captured_output) + flat_args, flat_results_traced\n     )\n \n     new_graph = FlattenInputOutputSignature(\n"
                }
            ],
            "whole_deleted": "-    supported_types = (torch.Tensor, torch.SymInt, torch.SymFloat, torch.SymBool)\n-\n-    def is_supported_type(val):\n-        return isinstance(val, supported_types)\n-\n-    def produce_matching(sources, candidates):\n-        source_types = \" or \".join(\n-            [\n-                desc\n-                + \" of types: (\"\n-                + \", \".join([str(type(val)) for val in vals])\n-                + \")\"\n-                for desc, vals in sources.items()\n-            ]\n-        )\n-        source_vals = [val for vals in sources.values() for val in vals]\n-        for i, val in enumerate(source_vals):\n-        for candidate_desc, candidate_vals in candidates.items():\n-            for i, val in enumerate(candidate_vals):\n-                if is_supported_type(val):\n-                    if id(val) in dict_of_source_vals:\n-                        matched_elements_positions.append(dict_of_source_vals[id(val)])\n-                    else:\n-                        raise AssertionError(\n-                            f\"{candidate_desc} #{i+1}, of type {type(val)}, is not among {source_types}\"\n-                            'Set TORCH_LOGS=\"+export\" for more information.'\n-                        )\n-                else:\n-                    raise AssertionError(\n-                        f\"{candidate_desc} #{i+1} is {val}, but only \"\n-                        f\"the following types are supported: {supported_types}\"\n-                        'Set TORCH_LOGS=\"+export\" for more information.'\n-                    )\n-        sources={\"original inputs\": flat_args},\n-        candidates={\"graph-captured input\": graph_captured_input},\n-    flat_results_traced, out_spec_traced = pytree.tree_flatten(dynamo_traced_result)\n-\n-        sources={\n-            \"graph-captured outputs\": list(graph_captured_output),\n-            \"original inputs\": flat_args,\n-        },\n-        candidates={\"original output\": flat_results_traced},\n",
            "whole_added": "+    def check_user_input_output(flat_values, error_type):\n+        supported_types = [\n+            torch.Tensor,\n+            torch.SymInt,\n+            torch.SymFloat,\n+            torch.SymBool,\n+            torch._C.ScriptObject,\n+        ]\n+        if error_type == UserErrorType.INVALID_INPUT:\n+            supported_types.extend([int, str, bool, float])\n+\n+        def is_supported_type(val):\n+            return isinstance(val, tuple(supported_types))\n+\n+        value_type = \"input\" if error_type == UserErrorType.INVALID_INPUT else \"output\"\n+        # We only check that the outputs are not None. Inputs can be None.\n+        for v in flat_values:\n+            if not is_supported_type(v):\n+                if error_type == UserErrorType.INVALID_INPUT and v is None:\n+                    continue\n+\n+                raise UserError(\n+                    error_type,\n+                    f\"It looks like one of the {value_type}s with type `{type(v)}` \"\n+                    \"is not supported or pytree-flattenable. \\n\"\n+                    f\"Exported graphs {value_type}s can only contain the \"\n+                    f\"following supported types: {supported_types}. \\n\"\n+                    \"If you are using a custom class object, \"\n+                    \"please register a pytree_flatten/unflatten function \"\n+                    \"using `torch.utils._pytree.register_pytree_node` or \"\n+                    \"`torch.export.register_dataclass`.\",\n+                )\n+\n+    check_user_input_output(flat_args, UserErrorType.INVALID_INPUT)\n+    flat_results_traced, out_spec_traced = pytree.tree_flatten(dynamo_traced_result)\n+    check_user_input_output(flat_results_traced, UserErrorType.INVALID_OUTPUT)\n+\n+    def produce_matching(debug_type, sources, candidates):\n+        for i, val in enumerate(sources):\n+        for i, val in enumerate(candidates):\n+            if id(val) not in dict_of_source_vals:\n+                raise AssertionError(\n+                    f\"Unexpectedly found a {type(val)} in the {debug_type}.\\n\"\n+                    'Please file an issue along with a paste of the logs from TORCH_LOGS=\"+export\"'\n+                )\n+\n+            matched_elements_positions.append(dict_of_source_vals[id(val)])\n+        \"inputs\", flat_args, graph_captured_input\n+        \"outputs\", list(graph_captured_output) + flat_args, flat_results_traced\n",
            "whole_hunk": "@@ -938,60 +938,67 @@ def rewrite_signature(\n ):\n     orig_args, orig_kwargs = pytree.tree_unflatten(flat_args, in_spec)\n \n-    supported_types = (torch.Tensor, torch.SymInt, torch.SymFloat, torch.SymBool)\n-\n-    def is_supported_type(val):\n-        return isinstance(val, supported_types)\n-\n-    def produce_matching(sources, candidates):\n-        source_types = \" or \".join(\n-            [\n-                desc\n-                + \" of types: (\"\n-                + \", \".join([str(type(val)) for val in vals])\n-                + \")\"\n-                for desc, vals in sources.items()\n-            ]\n-        )\n-        source_vals = [val for vals in sources.values() for val in vals]\n+    def check_user_input_output(flat_values, error_type):\n+        supported_types = [\n+            torch.Tensor,\n+            torch.SymInt,\n+            torch.SymFloat,\n+            torch.SymBool,\n+            torch._C.ScriptObject,\n+        ]\n+        if error_type == UserErrorType.INVALID_INPUT:\n+            supported_types.extend([int, str, bool, float])\n+\n+        def is_supported_type(val):\n+            return isinstance(val, tuple(supported_types))\n+\n+        value_type = \"input\" if error_type == UserErrorType.INVALID_INPUT else \"output\"\n+        # We only check that the outputs are not None. Inputs can be None.\n+        for v in flat_values:\n+            if not is_supported_type(v):\n+                if error_type == UserErrorType.INVALID_INPUT and v is None:\n+                    continue\n+\n+                raise UserError(\n+                    error_type,\n+                    f\"It looks like one of the {value_type}s with type `{type(v)}` \"\n+                    \"is not supported or pytree-flattenable. \\n\"\n+                    f\"Exported graphs {value_type}s can only contain the \"\n+                    f\"following supported types: {supported_types}. \\n\"\n+                    \"If you are using a custom class object, \"\n+                    \"please register a pytree_flatten/unflatten function \"\n+                    \"using `torch.utils._pytree.register_pytree_node` or \"\n+                    \"`torch.export.register_dataclass`.\",\n+                )\n+\n+    check_user_input_output(flat_args, UserErrorType.INVALID_INPUT)\n+    flat_results_traced, out_spec_traced = pytree.tree_flatten(dynamo_traced_result)\n+    check_user_input_output(flat_results_traced, UserErrorType.INVALID_OUTPUT)\n+\n+    def produce_matching(debug_type, sources, candidates):\n         matched_elements_positions = []\n         dict_of_source_vals = {}\n-        for i, val in enumerate(source_vals):\n+        for i, val in enumerate(sources):\n             dict_of_source_vals[id(val)] = i\n \n-        for candidate_desc, candidate_vals in candidates.items():\n-            for i, val in enumerate(candidate_vals):\n-                if is_supported_type(val):\n-                    if id(val) in dict_of_source_vals:\n-                        matched_elements_positions.append(dict_of_source_vals[id(val)])\n-                    else:\n-                        raise AssertionError(\n-                            f\"{candidate_desc} #{i+1}, of type {type(val)}, is not among {source_types}\"\n-                            'Set TORCH_LOGS=\"+export\" for more information.'\n-                        )\n-                else:\n-                    raise AssertionError(\n-                        f\"{candidate_desc} #{i+1} is {val}, but only \"\n-                        f\"the following types are supported: {supported_types}\"\n-                        'Set TORCH_LOGS=\"+export\" for more information.'\n-                    )\n+        for i, val in enumerate(candidates):\n+            if id(val) not in dict_of_source_vals:\n+                raise AssertionError(\n+                    f\"Unexpectedly found a {type(val)} in the {debug_type}.\\n\"\n+                    'Please file an issue along with a paste of the logs from TORCH_LOGS=\"+export\"'\n+                )\n+\n+            matched_elements_positions.append(dict_of_source_vals[id(val)])\n \n         return matched_elements_positions\n \n     matched_input_elements_positions = produce_matching(\n-        sources={\"original inputs\": flat_args},\n-        candidates={\"graph-captured input\": graph_captured_input},\n+        \"inputs\", flat_args, graph_captured_input\n     )\n \n-    flat_results_traced, out_spec_traced = pytree.tree_flatten(dynamo_traced_result)\n-\n     assert graph_captured_output is not None\n     matched_output_elements_positions = produce_matching(\n-        sources={\n-            \"graph-captured outputs\": list(graph_captured_output),\n-            \"original inputs\": flat_args,\n-        },\n-        candidates={\"original output\": flat_results_traced},\n+        \"outputs\", list(graph_captured_output) + flat_args, flat_results_traced\n     )\n \n     new_graph = FlattenInputOutputSignature(\n"
        },
        {
            "name": "exc.py",
            "path": "torch/_dynamo/exc.py",
            "patches": [
                {
                    "old_start": 133,
                    "old_length": 6,
                    "new_start": 133,
                    "new_length": 7,
                    "hunk": "@@ -133,6 +133,7 @@ class UserErrorType(Enum):\n     CONSTRAINT_VIOLATION = auto()\n     DYNAMIC_DIM = auto()\n     INVALID_INPUT = auto()\n+    INVALID_OUTPUT = auto()\n \n \n class UserError(Unsupported):"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    INVALID_OUTPUT = auto()\n",
            "whole_hunk": "@@ -133,6 +133,7 @@ class UserErrorType(Enum):\n     CONSTRAINT_VIOLATION = auto()\n     DYNAMIC_DIM = auto()\n     INVALID_INPUT = auto()\n+    INVALID_OUTPUT = auto()\n \n \n class UserError(Unsupported):"
        }
    ]
},
{
    "Id": 137,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/e6f655697b904401a2685e60dbeeaa94483ce180",
    "date": "2024-05-22T02:15:43+00:00",
    "message": "[AOTI] Fix unsupported type of output=s1 (#126797)\n\nFixes #123036\n\nIn unit test `DynamicShapesCudaWrapperCudaTests.test_scaled_dot_product_attention_cuda_dynamic_shapes_cuda_wrapper`, computed buffer buf3 is compiled to a fallback kernel `aoti_torch_cuda__scaled_dot_product_flash_attention`. It has 9 outputs whose types are `[MultiOutput, MultiOutput, None, None, s1, s1, MultiOutput, MultiOutput,MultiOutput]`. The type `s1` here is passed from [generate_output](https://github.com/pytorch/pytorch/blob/acfe237a71af609e837a34bb38048aa8acb8eb4d/torch/_inductor/ir.py#L5658).\n\nThey type check for Symbol is missing for fallback kernel output generation. This PR fixes this issue by checking `output.is_Symbol`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/126797\nApproved by: https://github.com/desertfire",
    "label": "YES",
    "changes": [
        {
            "name": "test_cuda_cpp_wrapper.py",
            "path": "test/inductor/test_cuda_cpp_wrapper.py",
            "patches": [
                {
                    "old_start": 98,
                    "old_length": 7,
                    "new_start": 98,
                    "new_length": 6,
                    "hunk": "@@ -98,7 +98,6 @@ if TEST_WITH_ROCM:\n if config.abi_compatible:\n     xfail_list = [\n         \"test_profiler_mark_wrapper_call_cuda\",\n-        \"test_scaled_dot_product_attention_cuda_dynamic_shapes\",\n     ]\n     for test_name in xfail_list:\n         test_failures_cuda_wrapper[test_name] = test_torchinductor.TestFailure(\n"
                }
            ],
            "whole_deleted": "-        \"test_scaled_dot_product_attention_cuda_dynamic_shapes\",\n",
            "whole_added": "",
            "whole_hunk": "@@ -98,7 +98,6 @@ if TEST_WITH_ROCM:\n if config.abi_compatible:\n     xfail_list = [\n         \"test_profiler_mark_wrapper_call_cuda\",\n-        \"test_scaled_dot_product_attention_cuda_dynamic_shapes\",\n     ]\n     for test_name in xfail_list:\n         test_failures_cuda_wrapper[test_name] = test_torchinductor.TestFailure(\n"
        },
        {
            "name": "cpp_wrapper_cpu.py",
            "path": "torch/_inductor/codegen/cpp_wrapper_cpu.py",
            "patches": [
                {
                    "old_start": 1223,
                    "old_length": 6,
                    "new_start": 1223,
                    "new_length": 10,
                    "hunk": "@@ -1223,6 +1223,10 @@ class CppWrapperCpu(WrapperCodeGen):\n                 output_name = f\"{output_name_base}_{idx}\"\n                 self.writeline(f\"int64_t {output_name} = {output};\")\n                 output_args.append(f\"&{output_name}\")\n+            elif isinstance(output, sympy.Symbol):\n+                output_name = f\"{output_name_base}_{idx}\"\n+                self.writeline(f\"auto {output_name} = {output};\")\n+                output_args.append(f\"&{output_name}\")\n             elif output is None:\n                 output_args.append(\"nullptr\")\n             else:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+            elif isinstance(output, sympy.Symbol):\n+                output_name = f\"{output_name_base}_{idx}\"\n+                self.writeline(f\"auto {output_name} = {output};\")\n+                output_args.append(f\"&{output_name}\")\n",
            "whole_hunk": "@@ -1223,6 +1223,10 @@ class CppWrapperCpu(WrapperCodeGen):\n                 output_name = f\"{output_name_base}_{idx}\"\n                 self.writeline(f\"int64_t {output_name} = {output};\")\n                 output_args.append(f\"&{output_name}\")\n+            elif isinstance(output, sympy.Symbol):\n+                output_name = f\"{output_name_base}_{idx}\"\n+                self.writeline(f\"auto {output_name} = {output};\")\n+                output_args.append(f\"&{output_name}\")\n             elif output is None:\n                 output_args.append(\"nullptr\")\n             else:"
        }
    ]
},
{
    "Id": 211,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/2fe672b146e776d18fcb794ca3cbb4e52d32ba8e",
    "date": "2024-04-12T01:12:23+00:00",
    "message": "compile: ban mutations on non-compositional uses of as_strided (#122502)\n\nFixes https://github.com/pytorch/pytorch/issues/104505\n\nI was originally going to ban all usages of as_strided + mutation in functionalization. But I'm pretty sure that as_strided + mutation is fine when we are calling as_strided on a base tensor.\n\nSo in this PR I added a slightly more conservative check: if we see an as_strided + mutation, where the input to an as_strided was **another** view op, then I error loudly in functionalization and link to the github issue above (in case anyone runs into this in the real world)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/122502\nApproved by: https://github.com/ezyang, https://github.com/albanD",
    "label": "YES",
    "changes": [
        {
            "name": "FunctionalStorageImpl.cpp",
            "path": "aten/src/ATen/FunctionalStorageImpl.cpp",
            "patches": [
                {
                    "old_start": 10,
                    "old_length": 7,
                    "new_start": 10,
                    "new_length": 7,
                    "hunk": "@@ -10,7 +10,7 @@ namespace at::functionalization {\n \n ViewMeta ViewMeta::to_out_idx(int64_t out_idx) {\n   if (out_idx == this->out_index) return *this;\n-  return ViewMeta(forward_fn, reverse_fn, is_multi_output, out_idx);\n+  return ViewMeta(forward_fn, reverse_fn, is_multi_output, is_as_strided, out_idx);\n }\n \n // Note [Functionalization: Alias Removal Part 2]\n"
                },
                {
                    "old_start": 103,
                    "old_length": 6,
                    "new_start": 103,
                    "new_length": 18,
                    "hunk": "@@ -103,6 +103,18 @@ FunctionalStorageImpl::FunctionalStorageImpl(const Tensor& base)\n \n void FunctionalStorageImpl::add_update(const Tensor& updated_val, const std::vector<ViewMeta>& metas) {\n   TORCH_CHECK(!frozen_, \"cannot mutate tensors with frozen storage\");\n+\n+  if (metas.size() > 1) {\n+    for (size_t i = 1; i < metas.size(); ++i) {\n+      // Skipping this check for XLA. Would be good to add it back, but it is failing XLA CI\n+      TORCH_CHECK(updated_val.device().type() == c10::DeviceType::XLA || !metas[i].is_as_strided,\n+\"During torch.compile, encountered a mutation on a view chain of length \", metas.size(), \", where view \", i,\n+\" was an as_strided() call. as_strided() is non-compositional, and therefore is not possible to functionalize properly today,\"\n+\"so this behavior is banned in compile. As a workaround, you can either remove the mutation from the model code, or you \"\n+\"can insert a graph break right before the mutation with torch._dynamo.graph_break(). If you would like this behavior to \"\n+\"work properly, please comment on https://github.com/pytorch/pytorch/issues/104505.\");\n+    }\n+  }\n   updates_.push_back({updated_val, metas});\n   generation_++;\n }\n"
                }
            ],
            "whole_deleted": "-  return ViewMeta(forward_fn, reverse_fn, is_multi_output, out_idx);\n",
            "whole_added": "+  return ViewMeta(forward_fn, reverse_fn, is_multi_output, is_as_strided, out_idx);\n+\n+  if (metas.size() > 1) {\n+    for (size_t i = 1; i < metas.size(); ++i) {\n+      // Skipping this check for XLA. Would be good to add it back, but it is failing XLA CI\n+      TORCH_CHECK(updated_val.device().type() == c10::DeviceType::XLA || !metas[i].is_as_strided,\n+\"During torch.compile, encountered a mutation on a view chain of length \", metas.size(), \", where view \", i,\n+\" was an as_strided() call. as_strided() is non-compositional, and therefore is not possible to functionalize properly today,\"\n+\"so this behavior is banned in compile. As a workaround, you can either remove the mutation from the model code, or you \"\n+\"can insert a graph break right before the mutation with torch._dynamo.graph_break(). If you would like this behavior to \"\n+\"work properly, please comment on https://github.com/pytorch/pytorch/issues/104505.\");\n+    }\n+  }\n",
            "whole_hunk": "@@ -10,7 +10,7 @@ namespace at::functionalization {\n \n ViewMeta ViewMeta::to_out_idx(int64_t out_idx) {\n   if (out_idx == this->out_index) return *this;\n-  return ViewMeta(forward_fn, reverse_fn, is_multi_output, out_idx);\n+  return ViewMeta(forward_fn, reverse_fn, is_multi_output, is_as_strided, out_idx);\n }\n \n // Note [Functionalization: Alias Removal Part 2]\n@@ -103,6 +103,18 @@ FunctionalStorageImpl::FunctionalStorageImpl(const Tensor& base)\n \n void FunctionalStorageImpl::add_update(const Tensor& updated_val, const std::vector<ViewMeta>& metas) {\n   TORCH_CHECK(!frozen_, \"cannot mutate tensors with frozen storage\");\n+\n+  if (metas.size() > 1) {\n+    for (size_t i = 1; i < metas.size(); ++i) {\n+      // Skipping this check for XLA. Would be good to add it back, but it is failing XLA CI\n+      TORCH_CHECK(updated_val.device().type() == c10::DeviceType::XLA || !metas[i].is_as_strided,\n+\"During torch.compile, encountered a mutation on a view chain of length \", metas.size(), \", where view \", i,\n+\" was an as_strided() call. as_strided() is non-compositional, and therefore is not possible to functionalize properly today,\"\n+\"so this behavior is banned in compile. As a workaround, you can either remove the mutation from the model code, or you \"\n+\"can insert a graph break right before the mutation with torch._dynamo.graph_break(). If you would like this behavior to \"\n+\"work properly, please comment on https://github.com/pytorch/pytorch/issues/104505.\");\n+    }\n+  }\n   updates_.push_back({updated_val, metas});\n   generation_++;\n }\n"
        },
        {
            "name": "FunctionalStorageImpl.h",
            "path": "aten/src/ATen/FunctionalStorageImpl.h",
            "patches": [
                {
                    "old_start": 32,
                    "old_length": 11,
                    "new_start": 32,
                    "new_length": 13,
                    "hunk": "@@ -32,11 +32,13 @@ struct ViewMeta {\n       std::function<Tensor(const Tensor&, int64_t)> forward,\n       std::function<Tensor(const Tensor&, const Tensor&, int64_t)> reverse,\n       bool is_multi_output = false,\n+      bool is_as_strided = false,\n       int64_t out_idx = 0)\n       : forward_fn(std::move(forward)),\n         reverse_fn(std::move(reverse)),\n         out_index(out_idx),\n-        is_multi_output(is_multi_output) {}\n+        is_multi_output(is_multi_output),\n+        is_as_strided(is_as_strided) {}\n \n   std::function<Tensor(const Tensor&, int64_t)> forward_fn;\n   std::function<Tensor(const Tensor&, const Tensor&, int64_t)> reverse_fn;\n"
                },
                {
                    "old_start": 46,
                    "old_length": 6,
                    "new_start": 48,
                    "new_length": 8,
                    "hunk": "@@ -46,6 +48,8 @@ struct ViewMeta {\n   // Tells us if this is a multi-output view\n   bool is_multi_output;\n \n+  bool is_as_strided;\n+\n   // Returns a copy of the current ViewMeta, if out_idx matches the current\n   // out_index. Otherwise, returns a new ViewMeta with the same forward/reverse\n   // functions, but a new out index.\n"
                }
            ],
            "whole_deleted": "-        is_multi_output(is_multi_output) {}\n",
            "whole_added": "+      bool is_as_strided = false,\n+        is_multi_output(is_multi_output),\n+        is_as_strided(is_as_strided) {}\n+  bool is_as_strided;\n+\n",
            "whole_hunk": "@@ -32,11 +32,13 @@ struct ViewMeta {\n       std::function<Tensor(const Tensor&, int64_t)> forward,\n       std::function<Tensor(const Tensor&, const Tensor&, int64_t)> reverse,\n       bool is_multi_output = false,\n+      bool is_as_strided = false,\n       int64_t out_idx = 0)\n       : forward_fn(std::move(forward)),\n         reverse_fn(std::move(reverse)),\n         out_index(out_idx),\n-        is_multi_output(is_multi_output) {}\n+        is_multi_output(is_multi_output),\n+        is_as_strided(is_as_strided) {}\n \n   std::function<Tensor(const Tensor&, int64_t)> forward_fn;\n   std::function<Tensor(const Tensor&, const Tensor&, int64_t)> reverse_fn;\n@@ -46,6 +48,8 @@ struct ViewMeta {\n   // Tells us if this is a multi-output view\n   bool is_multi_output;\n \n+  bool is_as_strided;\n+\n   // Returns a copy of the current ViewMeta, if out_idx matches the current\n   // out_index. Otherwise, returns a new ViewMeta with the same forward/reverse\n   // functions, but a new out index.\n"
        },
        {
            "name": "test_repros.py",
            "path": "test/dynamo/test_repros.py",
            "patches": [
                {
                    "old_start": 4414,
                    "old_length": 6,
                    "new_start": 4414,
                    "new_length": 39,
                    "hunk": "@@ -4414,6 +4414,39 @@ class ReproTests(torch._dynamo.test_case.TestCase):\n         T = IncByTwo\n         self.assertEqual(fn(x), opt_fn(x))\n \n+    # https://github.com/pytorch/pytorch/issues/104505\n+    def test_as_strided_on_base_with_mutation_works(self):\n+        def foo(a):\n+            f = a.as_strided((2,), (1,), 0)\n+            f.add_(1.0)\n+            return a\n+\n+        a = torch.randn(2, 4)\n+        a_ref = a.clone()\n+        out_ref = foo(a_ref)\n+        f_compiled = torch.compile(foo, backend=\"aot_eager\")\n+        out = f_compiled(a)\n+        self.assertEqual(out_ref, out)\n+        self.assertEqual(a_ref, a)\n+\n+    # https://github.com/pytorch/pytorch/issues/104505\n+    def test_as_strided_on_existing_view_banned(self):\n+        def foo(a):\n+            e = a.diagonal()\n+            f = e.as_strided((2,), (1,), 0)\n+            f.add_(1.0)\n+            return a\n+\n+        a = torch.randn(2, 4)\n+        a_ref = a.clone()\n+        out_ref = foo(a_ref)\n+        f_compiled = torch.compile(foo, backend=\"aot_eager\")\n+        with self.assertRaisesRegex(\n+            RuntimeError,\n+            \"encountered a mutation on a view chain of length 2, where view 1 was an as_strided\",\n+        ):\n+            out = f_compiled(a)\n+\n     def test_dont_aggressively_write_assert(self):\n         record_graph = torch._dynamo.testing.EagerAndRecordGraphs()\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    # https://github.com/pytorch/pytorch/issues/104505\n+    def test_as_strided_on_base_with_mutation_works(self):\n+        def foo(a):\n+            f = a.as_strided((2,), (1,), 0)\n+            f.add_(1.0)\n+            return a\n+\n+        a = torch.randn(2, 4)\n+        a_ref = a.clone()\n+        out_ref = foo(a_ref)\n+        f_compiled = torch.compile(foo, backend=\"aot_eager\")\n+        out = f_compiled(a)\n+        self.assertEqual(out_ref, out)\n+        self.assertEqual(a_ref, a)\n+\n+    # https://github.com/pytorch/pytorch/issues/104505\n+    def test_as_strided_on_existing_view_banned(self):\n+        def foo(a):\n+            e = a.diagonal()\n+            f = e.as_strided((2,), (1,), 0)\n+            f.add_(1.0)\n+            return a\n+\n+        a = torch.randn(2, 4)\n+        a_ref = a.clone()\n+        out_ref = foo(a_ref)\n+        f_compiled = torch.compile(foo, backend=\"aot_eager\")\n+        with self.assertRaisesRegex(\n+            RuntimeError,\n+            \"encountered a mutation on a view chain of length 2, where view 1 was an as_strided\",\n+        ):\n+            out = f_compiled(a)\n+\n",
            "whole_hunk": "@@ -4414,6 +4414,39 @@ class ReproTests(torch._dynamo.test_case.TestCase):\n         T = IncByTwo\n         self.assertEqual(fn(x), opt_fn(x))\n \n+    # https://github.com/pytorch/pytorch/issues/104505\n+    def test_as_strided_on_base_with_mutation_works(self):\n+        def foo(a):\n+            f = a.as_strided((2,), (1,), 0)\n+            f.add_(1.0)\n+            return a\n+\n+        a = torch.randn(2, 4)\n+        a_ref = a.clone()\n+        out_ref = foo(a_ref)\n+        f_compiled = torch.compile(foo, backend=\"aot_eager\")\n+        out = f_compiled(a)\n+        self.assertEqual(out_ref, out)\n+        self.assertEqual(a_ref, a)\n+\n+    # https://github.com/pytorch/pytorch/issues/104505\n+    def test_as_strided_on_existing_view_banned(self):\n+        def foo(a):\n+            e = a.diagonal()\n+            f = e.as_strided((2,), (1,), 0)\n+            f.add_(1.0)\n+            return a\n+\n+        a = torch.randn(2, 4)\n+        a_ref = a.clone()\n+        out_ref = foo(a_ref)\n+        f_compiled = torch.compile(foo, backend=\"aot_eager\")\n+        with self.assertRaisesRegex(\n+            RuntimeError,\n+            \"encountered a mutation on a view chain of length 2, where view 1 was an as_strided\",\n+        ):\n+            out = f_compiled(a)\n+\n     def test_dont_aggressively_write_assert(self):\n         record_graph = torch._dynamo.testing.EagerAndRecordGraphs()\n \n"
        },
        {
            "name": "FunctionsManual.cpp",
            "path": "torch/csrc/autograd/FunctionsManual.cpp",
            "patches": [
                {
                    "old_start": 3237,
                    "old_length": 12,
                    "new_start": 3237,
                    "new_length": 11,
                    "hunk": "@@ -3237,12 +3237,11 @@ Tensor as_strided_scatter_backward(\n   // take the perf hit and contiguify grad for now.\n   auto grad_ = grad.contiguous();\n   auto grad_slice = grad_.as_strided_symint(sizes, strides, storage_offset);\n-  auto result =\n-      grad_.new_zeros_symint(input_geometry.sym_sizes())\n-          .as_strided_symint(\n-              input_geometry.sym_sizes(), input_geometry.sym_strides());\n-  auto result_slice =\n-      result.as_strided_symint(sizes, strides, std::move(storage_offset));\n+  auto result_buffer = grad_.new_zeros_symint(input_geometry.sym_sizes());\n+  auto result = result_buffer.as_strided_symint(\n+      input_geometry.sym_sizes(), input_geometry.sym_strides());\n+  auto result_slice = result_buffer.as_strided_symint(\n+      sizes, strides, std::move(storage_offset));\n   result_slice.copy_(grad_slice);\n   return result;\n }\n"
                }
            ],
            "whole_deleted": "-  auto result =\n-      grad_.new_zeros_symint(input_geometry.sym_sizes())\n-          .as_strided_symint(\n-              input_geometry.sym_sizes(), input_geometry.sym_strides());\n-  auto result_slice =\n-      result.as_strided_symint(sizes, strides, std::move(storage_offset));\n",
            "whole_added": "+  auto result_buffer = grad_.new_zeros_symint(input_geometry.sym_sizes());\n+  auto result = result_buffer.as_strided_symint(\n+      input_geometry.sym_sizes(), input_geometry.sym_strides());\n+  auto result_slice = result_buffer.as_strided_symint(\n+      sizes, strides, std::move(storage_offset));\n",
            "whole_hunk": "@@ -3237,12 +3237,11 @@ Tensor as_strided_scatter_backward(\n   // take the perf hit and contiguify grad for now.\n   auto grad_ = grad.contiguous();\n   auto grad_slice = grad_.as_strided_symint(sizes, strides, storage_offset);\n-  auto result =\n-      grad_.new_zeros_symint(input_geometry.sym_sizes())\n-          .as_strided_symint(\n-              input_geometry.sym_sizes(), input_geometry.sym_strides());\n-  auto result_slice =\n-      result.as_strided_symint(sizes, strides, std::move(storage_offset));\n+  auto result_buffer = grad_.new_zeros_symint(input_geometry.sym_sizes());\n+  auto result = result_buffer.as_strided_symint(\n+      input_geometry.sym_sizes(), input_geometry.sym_strides());\n+  auto result_slice = result_buffer.as_strided_symint(\n+      sizes, strides, std::move(storage_offset));\n   result_slice.copy_(grad_slice);\n   return result;\n }\n"
        },
        {
            "name": "gen_functionalization_type.py",
            "path": "torchgen/gen_functionalization_type.py",
            "patches": [
                {
                    "old_start": 431,
                    "old_length": 7,
                    "new_start": 431,
                    "new_length": 8,
                    "hunk": "@@ -431,7 +431,8 @@ def emit_view_functionalization_body(\n         {reverse_lambda.decl()} {{\n           return {reverse_lambda.inner_call()}\n         }},\n-        /*is_multi_output=*/{str(is_multi_output_view).lower()}\n+        /*is_multi_output=*/{str(is_multi_output_view).lower()},\n+        /*is_as_strided=*/{str(str(f.func.name) == 'as_strided').lower()}\n       );\n       auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, {view_tensor_name}, view_meta);\n       // See  Note [Propagating strides in the functionalization pass]"
                }
            ],
            "whole_deleted": "-        /*is_multi_output=*/{str(is_multi_output_view).lower()}\n",
            "whole_added": "+        /*is_multi_output=*/{str(is_multi_output_view).lower()},\n+        /*is_as_strided=*/{str(str(f.func.name) == 'as_strided').lower()}\n",
            "whole_hunk": "@@ -431,7 +431,8 @@ def emit_view_functionalization_body(\n         {reverse_lambda.decl()} {{\n           return {reverse_lambda.inner_call()}\n         }},\n-        /*is_multi_output=*/{str(is_multi_output_view).lower()}\n+        /*is_multi_output=*/{str(is_multi_output_view).lower()},\n+        /*is_as_strided=*/{str(str(f.func.name) == 'as_strided').lower()}\n       );\n       auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, {view_tensor_name}, view_meta);\n       // See  Note [Propagating strides in the functionalization pass]"
        }
    ]
},
{
    "Id": 542,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/0156eeb56488dc203180bd19645f3c26e5a50f42",
    "date": "2023-08-24T16:00:29+00:00",
    "message": "[dynamo] bugfix - make module setattr more restrictive (#107828)\n\nA check got missed in https://github.com/pytorch/pytorch/pull/106092\n\nFixes https://github.com/pytorch/pytorch/issues/107721\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/107828\nApproved by: https://github.com/eellison",
    "label": "YES",
    "changes": [
        {
            "name": "builtin.py",
            "path": "torch/_dynamo/variables/builtin.py",
            "patches": [
                {
                    "old_start": 1176,
                    "old_length": 19,
                    "new_start": 1176,
                    "new_length": 22,
                    "hunk": "@@ -1176,19 +1176,22 @@ class BuiltinVariable(VariableTracker):\n \n                 getattr_var = obj.var_getattr(tx, name_var.as_python_constant())\n \n-                # get_fake_val will return a real tensor here because it's an attribute on the module (get_attr node)\n-                existing_attr = get_fake_value(getattr_var.as_proxy().node, tx)\n-                existing_fake_attr = variables.builder.wrap_to_fake_tensor_and_record(\n-                    existing_attr, tx, source=getattr_var.source, is_tensor=True\n-                )\n+                if isinstance(getattr_var, variables.TensorVariable):\n+                    # get_fake_val will return a real tensor here because it's an attribute on the module (get_attr node)\n+                    existing_attr = get_fake_value(getattr_var.as_proxy().node, tx)\n+                    existing_fake_attr = (\n+                        variables.builder.wrap_to_fake_tensor_and_record(\n+                            existing_attr, tx, source=getattr_var.source, is_tensor=True\n+                        )\n+                    )\n \n-                # same tensor identiy, setattr is a no-op\n-                mod_setattr = inspect.getattr_static(obj.module_type, \"__setattr__\")\n-                if (\n-                    existing_fake_attr is assigning_fake_val\n-                    and mod_setattr is torch.nn.Module.__setattr__\n-                ):\n-                    return getattr_var\n+                    # same tensor identiy, setattr is a no-op\n+                    mod_setattr = inspect.getattr_static(obj.module_type, \"__setattr__\")\n+                    if (\n+                        existing_fake_attr is assigning_fake_val\n+                        and mod_setattr is torch.nn.Module.__setattr__\n+                    ):\n+                        return getattr_var\n \n             obj.convert_to_unspecialized(tx)\n "
                }
            ],
            "whole_deleted": "-                # get_fake_val will return a real tensor here because it's an attribute on the module (get_attr node)\n-                existing_attr = get_fake_value(getattr_var.as_proxy().node, tx)\n-                existing_fake_attr = variables.builder.wrap_to_fake_tensor_and_record(\n-                    existing_attr, tx, source=getattr_var.source, is_tensor=True\n-                )\n-                # same tensor identiy, setattr is a no-op\n-                mod_setattr = inspect.getattr_static(obj.module_type, \"__setattr__\")\n-                if (\n-                    existing_fake_attr is assigning_fake_val\n-                    and mod_setattr is torch.nn.Module.__setattr__\n-                ):\n-                    return getattr_var\n",
            "whole_added": "+                if isinstance(getattr_var, variables.TensorVariable):\n+                    # get_fake_val will return a real tensor here because it's an attribute on the module (get_attr node)\n+                    existing_attr = get_fake_value(getattr_var.as_proxy().node, tx)\n+                    existing_fake_attr = (\n+                        variables.builder.wrap_to_fake_tensor_and_record(\n+                            existing_attr, tx, source=getattr_var.source, is_tensor=True\n+                        )\n+                    )\n+                    # same tensor identiy, setattr is a no-op\n+                    mod_setattr = inspect.getattr_static(obj.module_type, \"__setattr__\")\n+                    if (\n+                        existing_fake_attr is assigning_fake_val\n+                        and mod_setattr is torch.nn.Module.__setattr__\n+                    ):\n+                        return getattr_var\n",
            "whole_hunk": "@@ -1176,19 +1176,22 @@ class BuiltinVariable(VariableTracker):\n \n                 getattr_var = obj.var_getattr(tx, name_var.as_python_constant())\n \n-                # get_fake_val will return a real tensor here because it's an attribute on the module (get_attr node)\n-                existing_attr = get_fake_value(getattr_var.as_proxy().node, tx)\n-                existing_fake_attr = variables.builder.wrap_to_fake_tensor_and_record(\n-                    existing_attr, tx, source=getattr_var.source, is_tensor=True\n-                )\n+                if isinstance(getattr_var, variables.TensorVariable):\n+                    # get_fake_val will return a real tensor here because it's an attribute on the module (get_attr node)\n+                    existing_attr = get_fake_value(getattr_var.as_proxy().node, tx)\n+                    existing_fake_attr = (\n+                        variables.builder.wrap_to_fake_tensor_and_record(\n+                            existing_attr, tx, source=getattr_var.source, is_tensor=True\n+                        )\n+                    )\n \n-                # same tensor identiy, setattr is a no-op\n-                mod_setattr = inspect.getattr_static(obj.module_type, \"__setattr__\")\n-                if (\n-                    existing_fake_attr is assigning_fake_val\n-                    and mod_setattr is torch.nn.Module.__setattr__\n-                ):\n-                    return getattr_var\n+                    # same tensor identiy, setattr is a no-op\n+                    mod_setattr = inspect.getattr_static(obj.module_type, \"__setattr__\")\n+                    if (\n+                        existing_fake_attr is assigning_fake_val\n+                        and mod_setattr is torch.nn.Module.__setattr__\n+                    ):\n+                        return getattr_var\n \n             obj.convert_to_unspecialized(tx)\n "
        }
    ]
},
{
    "Id": 140,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/8c38d0cd648a8cef9518591f3b4dc257104a5fa8",
    "date": "2024-05-20T16:58:08+00:00",
    "message": "[inductor] Fix edge case in JIT vs. AOT fusion after finalizing MultiTemplateBuffer (#126622)\n\n# Context\nHere's a peripheral scenario causing the JIT-pass and AOT-pass to pick different fusions.\n```py\n# JIT -- buf3 is a MultiTemplateBuffer\nV.graph.buffers = [buf0, buf1, buf2, buf3, buf4]\n                                ^          ^\n# JIT pass calls finalize_multi_template_buffers()\nV.graph.buffers = [buf0, buf1, buf2, buf4, *buf3*]\n\n# AOT, note proximity_score(buf2, buf4) is \"better\" for fusion than JIT\nV.graph.buffers = [buf0, buf1, buf2, buf4, *buf3*]\n                                ^    ^\n```\n\nIt happens like this:\n* JIT starts with the original set nodes using V.graph.buffers\n* In JIT, finalize_multi_template_buffers() is called which can change the order of the buffers.\n* This makes the order of buffers/scheduler nodes different.\n* Now, each node's min/max-order is different than before.\n* As a result, the proximity between two nodes is different. https://github.com/pytorch/pytorch/blob/ad67553c5c1672d65b810acd7a6a01e11695098b/torch/_inductor/scheduler.py#L2316-L2335\n\n# Error\n```\n$ TORCH_LOGS=\"+fusion\" python test/inductor/test_max_autotune.py -k test_jit_fusion_matches_aot_fusion\n======================================================================\nFAIL: test_jit_fusion_matches_aot_fusion (__main__.TestMaxAutotune)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  ...\n  File \"/data/users/colinpeppler/pytorch/torch/_inductor/graph.py\", line 1718, in compile_to_fn\n    code, linemap = self.codegen_with_cpp_wrapper()\n  File \"/data/users/colinpeppler/pytorch/torch/_inductor/graph.py\", line 1618, in codegen_with_cpp_wrapper\n    return self.codegen()\n  File \"/data/users/colinpeppler/pytorch/torch/_inductor/graph.py\", line 1636, in codegen\n    self.scheduler.codegen()\n  File \"/data/users/colinpeppler/pytorch/torch/_dynamo/utils.py\", line 210, in time_wrapper\n    r = func(*args, **kwargs)\n  File \"/data/users/colinpeppler/pytorch/torch/_inductor/scheduler.py\", line 2602, in codegen\n    self.get_backend(device).codegen_node(node)  # type: ignore[possibly-undefined]\n  File \"/data/users/colinpeppler/pytorch/torch/_inductor/codegen/cuda_combined_scheduling.py\", line 66, in codegen_node\n    return self._triton_scheduling.codegen_node(node)\n  File \"/data/users/colinpeppler/pytorch/torch/_inductor/codegen/triton.py\", line 3377, in codegen_node\n    return self.codegen_node_schedule(node_schedule, buf_accesses, numel, rnumel)\n  File \"/data/users/colinpeppler/pytorch/torch/_inductor/codegen/triton.py\", line 3602, in codegen_node_schedule\n    final_kernel.call_kernel(final_kernel.kernel_name)\n  File \"/data/users/colinpeppler/pytorch/torch/_inductor/codegen/triton.py\", line 3055, in call_kernel\n    grid = wrapper.generate_default_grid(name, grid)\n  File \"/data/users/colinpeppler/pytorch/torch/_inductor/codegen/cpp_wrapper_cuda.py\", line 174, in generate_default_grid\n    params is not None\nAssertionError: cuda kernel parameters for triton_poi_fused_add_0 should already exist at this moment, only found dict_keys(['Placeholder.DESCRIPTIVE_NAME', 'triton_poi_fused_add_mul_0', 'triton_poi_fused_pow_1'])\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/126622\nApproved by: https://github.com/chenyang78\nghstack dependencies: #125982",
    "label": "YES",
    "changes": [
        {
            "name": "test_max_autotune.py",
            "path": "test/inductor/test_max_autotune.py",
            "patches": [
                {
                    "old_start": 463,
                    "old_length": 6,
                    "new_start": 463,
                    "new_length": 27,
                    "hunk": "@@ -463,6 +463,27 @@ class TestMaxAutotune(TestCase):\n         self.assertEqual(fn(*inputs), fn_c(*inputs), atol=1e-2, rtol=1e-2)\n         self.assertEqual(counters[\"inductor\"][\"select_algorithm_precompile\"], 0)\n \n+    @skipIfRocm\n+    @fresh_inductor_cache()\n+    @config.patch(max_autotune=True, max_fusion_size=2)\n+    def test_jit_fusion_matches_aot_fusion(self):\n+        # In this example, AOTInductor's JIT-compile will fuse(buf1, buf2) due\n+        # to proximity, we want to make sure AOT-compile pass does the same.\n+        # AOT could do fuse(buf2, buf4) instead if buf3 was pushed to the end\n+        # of the V.graph.buffers list because fuse(buf2, buf4) would have a\n+        # better proximity score than fuse(buf1, buf2). This scenario is possible\n+        # since finalizing MultiTemplateBuffers needs to replace buffers.\n+        def fn(x, number):\n+            buf0 = x + x\n+            buf1 = number.item()\n+            buf2 = x * x\n+            buf3 = x @ x  # MultiTemplateBuffer\n+            buf4 = x**2\n+            return buf0, buf1, buf2, buf3, buf4\n+\n+        inputs = (torch.rand([256, 256], device=\"cuda\"), torch.tensor(3, device=\"cuda\"))\n+        torch._export.aot_compile(fn, args=inputs)\n+\n     @config.patch(autotune_local_cache=False, autotune_remote_cache=False)\n     def test_precompilations(self):\n         def fn(a, b, c):\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    @skipIfRocm\n+    @fresh_inductor_cache()\n+    @config.patch(max_autotune=True, max_fusion_size=2)\n+    def test_jit_fusion_matches_aot_fusion(self):\n+        # In this example, AOTInductor's JIT-compile will fuse(buf1, buf2) due\n+        # to proximity, we want to make sure AOT-compile pass does the same.\n+        # AOT could do fuse(buf2, buf4) instead if buf3 was pushed to the end\n+        # of the V.graph.buffers list because fuse(buf2, buf4) would have a\n+        # better proximity score than fuse(buf1, buf2). This scenario is possible\n+        # since finalizing MultiTemplateBuffers needs to replace buffers.\n+        def fn(x, number):\n+            buf0 = x + x\n+            buf1 = number.item()\n+            buf2 = x * x\n+            buf3 = x @ x  # MultiTemplateBuffer\n+            buf4 = x**2\n+            return buf0, buf1, buf2, buf3, buf4\n+\n+        inputs = (torch.rand([256, 256], device=\"cuda\"), torch.tensor(3, device=\"cuda\"))\n+        torch._export.aot_compile(fn, args=inputs)\n+\n",
            "whole_hunk": "@@ -463,6 +463,27 @@ class TestMaxAutotune(TestCase):\n         self.assertEqual(fn(*inputs), fn_c(*inputs), atol=1e-2, rtol=1e-2)\n         self.assertEqual(counters[\"inductor\"][\"select_algorithm_precompile\"], 0)\n \n+    @skipIfRocm\n+    @fresh_inductor_cache()\n+    @config.patch(max_autotune=True, max_fusion_size=2)\n+    def test_jit_fusion_matches_aot_fusion(self):\n+        # In this example, AOTInductor's JIT-compile will fuse(buf1, buf2) due\n+        # to proximity, we want to make sure AOT-compile pass does the same.\n+        # AOT could do fuse(buf2, buf4) instead if buf3 was pushed to the end\n+        # of the V.graph.buffers list because fuse(buf2, buf4) would have a\n+        # better proximity score than fuse(buf1, buf2). This scenario is possible\n+        # since finalizing MultiTemplateBuffers needs to replace buffers.\n+        def fn(x, number):\n+            buf0 = x + x\n+            buf1 = number.item()\n+            buf2 = x * x\n+            buf3 = x @ x  # MultiTemplateBuffer\n+            buf4 = x**2\n+            return buf0, buf1, buf2, buf3, buf4\n+\n+        inputs = (torch.rand([256, 256], device=\"cuda\"), torch.tensor(3, device=\"cuda\"))\n+        torch._export.aot_compile(fn, args=inputs)\n+\n     @config.patch(autotune_local_cache=False, autotune_remote_cache=False)\n     def test_precompilations(self):\n         def fn(a, b, c):\n"
        },
        {
            "name": "scheduler.py",
            "path": "torch/_inductor/scheduler.py",
            "patches": [
                {
                    "old_start": 1752,
                    "old_length": 7,
                    "new_start": 1752,
                    "new_length": 9,
                    "hunk": "@@ -1752,7 +1752,9 @@ class Scheduler:\n             del V.graph.name_to_buffer[replaced_name]\n             new_node.name = orig_name\n \n-            V.graph.buffers.remove(orig_node)\n+            orig = V.graph.buffers.index(orig_node)\n+            V.graph.buffers.remove(new_node)\n+            V.graph.buffers[orig] = new_node\n             V.graph.name_to_buffer[orig_name] = new_node\n \n         for i, node in enumerate(self.nodes):"
                }
            ],
            "whole_deleted": "-            V.graph.buffers.remove(orig_node)\n",
            "whole_added": "+            orig = V.graph.buffers.index(orig_node)\n+            V.graph.buffers.remove(new_node)\n+            V.graph.buffers[orig] = new_node\n",
            "whole_hunk": "@@ -1752,7 +1752,9 @@ class Scheduler:\n             del V.graph.name_to_buffer[replaced_name]\n             new_node.name = orig_name\n \n-            V.graph.buffers.remove(orig_node)\n+            orig = V.graph.buffers.index(orig_node)\n+            V.graph.buffers.remove(new_node)\n+            V.graph.buffers[orig] = new_node\n             V.graph.name_to_buffer[orig_name] = new_node\n \n         for i, node in enumerate(self.nodes):"
        }
    ]
},
{
    "Id": 371,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/d72d99e5917a3d26e304dfb22d7cc86c99466528",
    "date": "2023-12-20T12:16:07+00:00",
    "message": "Fix sparse compressed tensor invariants checks when nnz==0 (#115826)\n\nFixes https://github.com/pytorch/pytorch/issues/115755\n\nThis PR is a step toward deprecating `torch.empty(..., layout=<sparse compressed tensor layout>)` that usage should be minimized as it will produce invalid tensors, see also https://github.com/pytorch/pytorch/issues/90695 .\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115826\nApproved by: https://github.com/cpuhrsch, https://github.com/amjames",
    "label": "YES",
    "changes": [
        {
            "name": "TensorFactories.cpp",
            "path": "aten/src/ATen/native/TensorFactories.cpp",
            "patches": [
                {
                    "old_start": 8,
                    "old_length": 6,
                    "new_start": 8,
                    "new_length": 7,
                    "hunk": "@@ -8,6 +8,7 @@\n #include <ATen/ExpandUtils.h>\n #include <ATen/Parallel.h>\n #include <ATen/MapAllocator.h>\n+#include <ATen/SparseCsrTensorUtils.h>\n #include <ATen/TracerMode.h>\n #include <ATen/TensorOperators.h>\n #include <ATen/NamedTensorUtils.h>\n"
                },
                {
                    "old_start": 1276,
                    "old_length": 14,
                    "new_start": 1277,
                    "new_length": 51,
                    "hunk": "@@ -1276,14 +1277,51 @@ Tensor triu_indices_cpu(\n \n // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ zeros ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n+static Tensor zeros_sparse_compressed_symint(c10::SymIntArrayRef size,\n+    c10::optional<ScalarType> dtype,\n+    Layout layout,\n+    c10::optional<Device> device,\n+    c10::optional<bool> pin_memory) {\n+  check_size_nonnegative(size);\n+  TORCH_CHECK(size.size() >= 2, \"torch.zeros: Only batched sparse compressed (non-block) tensors are supported, but got size \", size);\n+  auto size_ = C10_AS_INTARRAYREF_SLOW(size);\n+  // torch.zeros cannot be used to create blocked tensors because its\n+  // API lacks a method to specify the block size.\n+  AT_DISPATCH_SPARSE_COMPRESSED_NONBLOCK_LAYOUTS(layout, \"zeros_sparse_compressed\", [&]{});\n+\n+  int64_t nnz = 0;\n+  auto compressed_indices_size = DimVector(size_.slice(0, size.size() - 2));\n+  auto plain_indices_and_values_size = DimVector(size_.slice(0, size.size() - 2));\n+  compressed_indices_size.push_back(size_[at::sparse_csr::compressedDimension(layout, size_)] + 1);\n+  plain_indices_and_values_size.push_back(nnz);\n+\n+  TensorOptions options = TensorOptions().dtype(ScalarType::Long).layout(Layout::Strided).device(device).pinned_memory(pin_memory);\n+  auto compressed_indices = at::empty(compressed_indices_size, options);\n+  compressed_indices.zero_();\n+  auto plain_indices = at::empty(plain_indices_and_values_size, options);\n+  auto values = at::empty(plain_indices_and_values_size, options.dtype(dtype));\n+\n+  return at::_sparse_compressed_tensor_unsafe(compressed_indices,\n+                                              plain_indices,\n+                                              values,\n+                                              size_,\n+                                              dtype,\n+                                              layout,\n+                                              device,\n+                                              pin_memory);\n+}\n+\n Tensor zeros_symint(SymIntArrayRef size,\n     c10::optional<ScalarType> dtype,\n     c10::optional<Layout> layout,\n     c10::optional<Device> device,\n     c10::optional<bool> pin_memory) {\n+  Layout layout_ = layout.value_or(Layout::Strided);\n+  if (at::sparse_csr::is_sparse_compressed(layout_)) {\n+    return zeros_sparse_compressed_symint(size, dtype, layout_, device, pin_memory);\n+  }\n   // See [Note: hacky wrapper removal for TensorOptions]\n   TensorOptions options = TensorOptions().dtype(dtype).layout(layout).device(device).pinned_memory(pin_memory);\n-\n   auto result = at::empty_symint(size, options);\n   return result.zero_();\n }\n"
                }
            ],
            "whole_deleted": "-\n",
            "whole_added": "+#include <ATen/SparseCsrTensorUtils.h>\n+static Tensor zeros_sparse_compressed_symint(c10::SymIntArrayRef size,\n+    c10::optional<ScalarType> dtype,\n+    Layout layout,\n+    c10::optional<Device> device,\n+    c10::optional<bool> pin_memory) {\n+  check_size_nonnegative(size);\n+  TORCH_CHECK(size.size() >= 2, \"torch.zeros: Only batched sparse compressed (non-block) tensors are supported, but got size \", size);\n+  auto size_ = C10_AS_INTARRAYREF_SLOW(size);\n+  // torch.zeros cannot be used to create blocked tensors because its\n+  // API lacks a method to specify the block size.\n+  AT_DISPATCH_SPARSE_COMPRESSED_NONBLOCK_LAYOUTS(layout, \"zeros_sparse_compressed\", [&]{});\n+\n+  int64_t nnz = 0;\n+  auto compressed_indices_size = DimVector(size_.slice(0, size.size() - 2));\n+  auto plain_indices_and_values_size = DimVector(size_.slice(0, size.size() - 2));\n+  compressed_indices_size.push_back(size_[at::sparse_csr::compressedDimension(layout, size_)] + 1);\n+  plain_indices_and_values_size.push_back(nnz);\n+\n+  TensorOptions options = TensorOptions().dtype(ScalarType::Long).layout(Layout::Strided).device(device).pinned_memory(pin_memory);\n+  auto compressed_indices = at::empty(compressed_indices_size, options);\n+  compressed_indices.zero_();\n+  auto plain_indices = at::empty(plain_indices_and_values_size, options);\n+  auto values = at::empty(plain_indices_and_values_size, options.dtype(dtype));\n+\n+  return at::_sparse_compressed_tensor_unsafe(compressed_indices,\n+                                              plain_indices,\n+                                              values,\n+                                              size_,\n+                                              dtype,\n+                                              layout,\n+                                              device,\n+                                              pin_memory);\n+}\n+\n+  Layout layout_ = layout.value_or(Layout::Strided);\n+  if (at::sparse_csr::is_sparse_compressed(layout_)) {\n+    return zeros_sparse_compressed_symint(size, dtype, layout_, device, pin_memory);\n+  }\n",
            "whole_hunk": "@@ -8,6 +8,7 @@\n #include <ATen/ExpandUtils.h>\n #include <ATen/Parallel.h>\n #include <ATen/MapAllocator.h>\n+#include <ATen/SparseCsrTensorUtils.h>\n #include <ATen/TracerMode.h>\n #include <ATen/TensorOperators.h>\n #include <ATen/NamedTensorUtils.h>\n@@ -1276,14 +1277,51 @@ Tensor triu_indices_cpu(\n \n // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ zeros ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n+static Tensor zeros_sparse_compressed_symint(c10::SymIntArrayRef size,\n+    c10::optional<ScalarType> dtype,\n+    Layout layout,\n+    c10::optional<Device> device,\n+    c10::optional<bool> pin_memory) {\n+  check_size_nonnegative(size);\n+  TORCH_CHECK(size.size() >= 2, \"torch.zeros: Only batched sparse compressed (non-block) tensors are supported, but got size \", size);\n+  auto size_ = C10_AS_INTARRAYREF_SLOW(size);\n+  // torch.zeros cannot be used to create blocked tensors because its\n+  // API lacks a method to specify the block size.\n+  AT_DISPATCH_SPARSE_COMPRESSED_NONBLOCK_LAYOUTS(layout, \"zeros_sparse_compressed\", [&]{});\n+\n+  int64_t nnz = 0;\n+  auto compressed_indices_size = DimVector(size_.slice(0, size.size() - 2));\n+  auto plain_indices_and_values_size = DimVector(size_.slice(0, size.size() - 2));\n+  compressed_indices_size.push_back(size_[at::sparse_csr::compressedDimension(layout, size_)] + 1);\n+  plain_indices_and_values_size.push_back(nnz);\n+\n+  TensorOptions options = TensorOptions().dtype(ScalarType::Long).layout(Layout::Strided).device(device).pinned_memory(pin_memory);\n+  auto compressed_indices = at::empty(compressed_indices_size, options);\n+  compressed_indices.zero_();\n+  auto plain_indices = at::empty(plain_indices_and_values_size, options);\n+  auto values = at::empty(plain_indices_and_values_size, options.dtype(dtype));\n+\n+  return at::_sparse_compressed_tensor_unsafe(compressed_indices,\n+                                              plain_indices,\n+                                              values,\n+                                              size_,\n+                                              dtype,\n+                                              layout,\n+                                              device,\n+                                              pin_memory);\n+}\n+\n Tensor zeros_symint(SymIntArrayRef size,\n     c10::optional<ScalarType> dtype,\n     c10::optional<Layout> layout,\n     c10::optional<Device> device,\n     c10::optional<bool> pin_memory) {\n+  Layout layout_ = layout.value_or(Layout::Strided);\n+  if (at::sparse_csr::is_sparse_compressed(layout_)) {\n+    return zeros_sparse_compressed_symint(size, dtype, layout_, device, pin_memory);\n+  }\n   // See [Note: hacky wrapper removal for TensorOptions]\n   TensorOptions options = TensorOptions().dtype(dtype).layout(layout).device(device).pinned_memory(pin_memory);\n-\n   auto result = at::empty_symint(size, options);\n   return result.zero_();\n }\n"
        },
        {
            "name": "SparseBlasImpl.cpp",
            "path": "aten/src/ATen/native/mkl/SparseBlasImpl.cpp",
            "patches": [
                {
                    "old_start": 316,
                    "old_length": 7,
                    "new_start": 316,
                    "new_length": 7,
                    "hunk": "@@ -316,7 +316,7 @@ void addmm_sparse_result(\n   }\n \n   // MKL doesn't have an interface to compute alpha*(A*B) + beta*C at once\n-  Tensor mat1_mat2 = at::empty(result.sizes(), result.options());\n+  Tensor mat1_mat2 = at::zeros(result.sizes(), result.options());\n   indices_to_mkl_compatible_inplace(mat1_mat2);\n \n   AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES(\n"
                }
            ],
            "whole_deleted": "-  Tensor mat1_mat2 = at::empty(result.sizes(), result.options());\n",
            "whole_added": "+  Tensor mat1_mat2 = at::zeros(result.sizes(), result.options());\n",
            "whole_hunk": "@@ -316,7 +316,7 @@ void addmm_sparse_result(\n   }\n \n   // MKL doesn't have an interface to compute alpha*(A*B) + beta*C at once\n-  Tensor mat1_mat2 = at::empty(result.sizes(), result.options());\n+  Tensor mat1_mat2 = at::zeros(result.sizes(), result.options());\n   indices_to_mkl_compatible_inplace(mat1_mat2);\n \n   AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES(\n"
        },
        {
            "name": "SparseCsrTensor.cpp",
            "path": "aten/src/ATen/native/sparse/SparseCsrTensor.cpp",
            "patches": [
                {
                    "old_start": 259,
                    "old_length": 16,
                    "new_start": 259,
                    "new_length": 13,
                    "hunk": "@@ -259,16 +259,13 @@ static void _validate_sparse_compressed_tensor_args_worker(const Tensor& compres\n       compressed_indices_type);\n \n   // Indices invariants\n-  if (plain_indices.numel() > 0) {\n-    at::_validate_compressed_sparse_indices(\n-        /*is_crow = */layout == kSparseCsr || layout == kSparseBsr,\n-        compressed_indices,\n-        plain_indices,\n-        compressed_dim_size,\n-        plain_dim_size,\n-        values_nnz\n-    );\n-  }\n+  at::_validate_compressed_sparse_indices(\n+      /*is_crow = */layout == kSparseCsr || layout == kSparseBsr,\n+      compressed_indices,\n+      plain_indices,\n+      compressed_dim_size,\n+      plain_dim_size,\n+      values_nnz);\n \n   // Device Invariants\n   // 4.1\n"
                },
                {
                    "old_start": 563,
                    "old_length": 6,
                    "new_start": 560,
                    "new_length": 10,
                    "hunk": "@@ -563,6 +560,10 @@ SPARSE_COMPRESSED_TENSOR(csc, kSparseCsc)\n SPARSE_COMPRESSED_TENSOR(bsr, kSparseBsr)\n SPARSE_COMPRESSED_TENSOR(bsc, kSparseBsc)\n \n+// Warning: ideally, torch.empty(..., layout=<sparse compressed\n+// format>) ought to be unsupported because it does not return a valid\n+// sparse compressed tensor without initialization of compressed\n+// indices. The implementation below is kept for BC.\n Tensor empty_sparse_compressed(\n     IntArrayRef size,\n     c10::optional<ScalarType> dtype,\n"
                },
                {
                    "old_start": 590,
                    "old_length": 16,
                    "new_start": 591,
                    "new_length": 25,
                    "hunk": "@@ -590,16 +591,25 @@ Tensor empty_sparse_compressed(\n   auto compressed_indices = at::empty(compressed_indices_size, options);\n   auto plain_indices = at::empty(plain_indices_and_values_size, options);\n   auto values = at::empty(plain_indices_and_values_size, options.dtype(dtype));\n-\n-  return at::_sparse_compressed_tensor_unsafe(\n-       compressed_indices,\n-       plain_indices,\n-       values,\n-       size,\n-       dtype,\n-       layout,\n-       device,\n-       pin_memory);\n+  // torch.empty on produces garbage so that the resulting empty\n+  // sparse compressed tensor may fail to satisfy the following\n+  // compressed sparse tensor invariants:\n+  //\n+  //   compressed_indices[..., 0] == 0\n+  //   compressed_indices[..., -1] == nnz.\n+  //   compressed_indices must be non-decreasing sequence\n+  //\n+  // Therefore, avoid using empty to create sparse compressed\n+  // tensors. Instead, use compressed sparse constructors directly or\n+  // other factory functions such as torch.zeros, etc.\n+  return at::_sparse_compressed_tensor_unsafe(compressed_indices,\n+                                              plain_indices,\n+                                              values,\n+                                              size,\n+                                              dtype,\n+                                              layout,\n+                                              device,\n+                                              pin_memory);\n }\n \n const Tensor& resize_sparse_csr_(\n"
                }
            ],
            "whole_deleted": "-  if (plain_indices.numel() > 0) {\n-    at::_validate_compressed_sparse_indices(\n-        /*is_crow = */layout == kSparseCsr || layout == kSparseBsr,\n-        compressed_indices,\n-        plain_indices,\n-        compressed_dim_size,\n-        plain_dim_size,\n-        values_nnz\n-    );\n-  }\n-\n-  return at::_sparse_compressed_tensor_unsafe(\n-       compressed_indices,\n-       plain_indices,\n-       values,\n-       size,\n-       dtype,\n-       layout,\n-       device,\n-       pin_memory);\n",
            "whole_added": "+  at::_validate_compressed_sparse_indices(\n+      /*is_crow = */layout == kSparseCsr || layout == kSparseBsr,\n+      compressed_indices,\n+      plain_indices,\n+      compressed_dim_size,\n+      plain_dim_size,\n+      values_nnz);\n+// Warning: ideally, torch.empty(..., layout=<sparse compressed\n+// format>) ought to be unsupported because it does not return a valid\n+// sparse compressed tensor without initialization of compressed\n+// indices. The implementation below is kept for BC.\n+  // torch.empty on produces garbage so that the resulting empty\n+  // sparse compressed tensor may fail to satisfy the following\n+  // compressed sparse tensor invariants:\n+  //\n+  //   compressed_indices[..., 0] == 0\n+  //   compressed_indices[..., -1] == nnz.\n+  //   compressed_indices must be non-decreasing sequence\n+  //\n+  // Therefore, avoid using empty to create sparse compressed\n+  // tensors. Instead, use compressed sparse constructors directly or\n+  // other factory functions such as torch.zeros, etc.\n+  return at::_sparse_compressed_tensor_unsafe(compressed_indices,\n+                                              plain_indices,\n+                                              values,\n+                                              size,\n+                                              dtype,\n+                                              layout,\n+                                              device,\n+                                              pin_memory);\n",
            "whole_hunk": "@@ -259,16 +259,13 @@ static void _validate_sparse_compressed_tensor_args_worker(const Tensor& compres\n       compressed_indices_type);\n \n   // Indices invariants\n-  if (plain_indices.numel() > 0) {\n-    at::_validate_compressed_sparse_indices(\n-        /*is_crow = */layout == kSparseCsr || layout == kSparseBsr,\n-        compressed_indices,\n-        plain_indices,\n-        compressed_dim_size,\n-        plain_dim_size,\n-        values_nnz\n-    );\n-  }\n+  at::_validate_compressed_sparse_indices(\n+      /*is_crow = */layout == kSparseCsr || layout == kSparseBsr,\n+      compressed_indices,\n+      plain_indices,\n+      compressed_dim_size,\n+      plain_dim_size,\n+      values_nnz);\n \n   // Device Invariants\n   // 4.1\n@@ -563,6 +560,10 @@ SPARSE_COMPRESSED_TENSOR(csc, kSparseCsc)\n SPARSE_COMPRESSED_TENSOR(bsr, kSparseBsr)\n SPARSE_COMPRESSED_TENSOR(bsc, kSparseBsc)\n \n+// Warning: ideally, torch.empty(..., layout=<sparse compressed\n+// format>) ought to be unsupported because it does not return a valid\n+// sparse compressed tensor without initialization of compressed\n+// indices. The implementation below is kept for BC.\n Tensor empty_sparse_compressed(\n     IntArrayRef size,\n     c10::optional<ScalarType> dtype,\n@@ -590,16 +591,25 @@ Tensor empty_sparse_compressed(\n   auto compressed_indices = at::empty(compressed_indices_size, options);\n   auto plain_indices = at::empty(plain_indices_and_values_size, options);\n   auto values = at::empty(plain_indices_and_values_size, options.dtype(dtype));\n-\n-  return at::_sparse_compressed_tensor_unsafe(\n-       compressed_indices,\n-       plain_indices,\n-       values,\n-       size,\n-       dtype,\n-       layout,\n-       device,\n-       pin_memory);\n+  // torch.empty on produces garbage so that the resulting empty\n+  // sparse compressed tensor may fail to satisfy the following\n+  // compressed sparse tensor invariants:\n+  //\n+  //   compressed_indices[..., 0] == 0\n+  //   compressed_indices[..., -1] == nnz.\n+  //   compressed_indices must be non-decreasing sequence\n+  //\n+  // Therefore, avoid using empty to create sparse compressed\n+  // tensors. Instead, use compressed sparse constructors directly or\n+  // other factory functions such as torch.zeros, etc.\n+  return at::_sparse_compressed_tensor_unsafe(compressed_indices,\n+                                              plain_indices,\n+                                              values,\n+                                              size,\n+                                              dtype,\n+                                              layout,\n+                                              device,\n+                                              pin_memory);\n }\n \n const Tensor& resize_sparse_csr_(\n"
        },
        {
            "name": "test_sparse_csr.py",
            "path": "test/test_sparse_csr.py",
            "patches": [
                {
                    "old_start": 306,
                    "old_length": 7,
                    "new_start": 306,
                    "new_length": 9,
                    "hunk": "@@ -306,7 +306,9 @@ class TestSparseCompressed(TestCase):\n         compressed_indices_mth, plain_indices_mth = sparse_compressed_indices_methods[layout]\n         for m, n, b in itertools.product(ns, ns, batch_shapes):\n             shape = (*b, m, n)\n-            result = torch.empty(shape, dtype=dtype, device=device, layout=layout)\n+            with torch.sparse.check_sparse_tensor_invariants(enable=False):\n+                # torch.empty may return invalid sparse compressed tensors\n+                result = torch.empty(shape, dtype=dtype, device=device, layout=layout)\n             self.assertEqual(result.shape, shape)\n             self.assertEqual(result.dtype, dtype)\n             self.assertEqual(result.device, torch.device(device))\n"
                },
                {
                    "old_start": 727,
                    "old_length": 6,
                    "new_start": 729,
                    "new_length": 13,
                    "hunk": "@@ -727,6 +729,13 @@ class TestSparseCompressed(TestCase):\n                    shape((2, 3)),\n                    r'`compressed_indices\\[..., 0\\] == 0` is not satisfied.')\n \n+            yield ('invalid compressed_indices[0] when nnz == 0',\n+                   tensor([1, 0], dtype=torch.int64),\n+                   tensor([], dtype=torch.int64),\n+                   values([1])[:0],\n+                   shape((1, 1)),\n+                   r'`compressed_indices\\[..., 0\\] == 0` is not satisfied.')\n+\n             yield ('invalid compressed_indices[-1]',\n                    tensor([0, 2, 5]),\n                    tensor([0, 1, 0, 2]),\n"
                },
                {
                    "old_start": 734,
                    "old_length": 6,
                    "new_start": 743,
                    "new_length": 13,
                    "hunk": "@@ -734,6 +743,13 @@ class TestSparseCompressed(TestCase):\n                    shape((2, 3)),\n                    r'`compressed_indices\\[..., -1\\] == nnz` is not satisfied.')\n \n+            yield ('invalid compressed_indices[-1] when nnz == 0',\n+                   tensor([0, 1], dtype=torch.int64),\n+                   tensor([], dtype=torch.int64),\n+                   values([1])[:0],\n+                   shape((1, 1)),\n+                   r'`compressed_indices\\[..., -1\\] == nnz` is not satisfied.')\n+\n             yield ('invalid compressed_indices.diff(dim=-1)',\n                    tensor([0, 0, 4]),\n                    tensor([0, 1, 0, 2]),"
                }
            ],
            "whole_deleted": "-            result = torch.empty(shape, dtype=dtype, device=device, layout=layout)\n",
            "whole_added": "+            with torch.sparse.check_sparse_tensor_invariants(enable=False):\n+                # torch.empty may return invalid sparse compressed tensors\n+                result = torch.empty(shape, dtype=dtype, device=device, layout=layout)\n+            yield ('invalid compressed_indices[0] when nnz == 0',\n+                   tensor([1, 0], dtype=torch.int64),\n+                   tensor([], dtype=torch.int64),\n+                   values([1])[:0],\n+                   shape((1, 1)),\n+                   r'`compressed_indices\\[..., 0\\] == 0` is not satisfied.')\n+\n+            yield ('invalid compressed_indices[-1] when nnz == 0',\n+                   tensor([0, 1], dtype=torch.int64),\n+                   tensor([], dtype=torch.int64),\n+                   values([1])[:0],\n+                   shape((1, 1)),\n+                   r'`compressed_indices\\[..., -1\\] == nnz` is not satisfied.')\n+\n",
            "whole_hunk": "@@ -306,7 +306,9 @@ class TestSparseCompressed(TestCase):\n         compressed_indices_mth, plain_indices_mth = sparse_compressed_indices_methods[layout]\n         for m, n, b in itertools.product(ns, ns, batch_shapes):\n             shape = (*b, m, n)\n-            result = torch.empty(shape, dtype=dtype, device=device, layout=layout)\n+            with torch.sparse.check_sparse_tensor_invariants(enable=False):\n+                # torch.empty may return invalid sparse compressed tensors\n+                result = torch.empty(shape, dtype=dtype, device=device, layout=layout)\n             self.assertEqual(result.shape, shape)\n             self.assertEqual(result.dtype, dtype)\n             self.assertEqual(result.device, torch.device(device))\n@@ -727,6 +729,13 @@ class TestSparseCompressed(TestCase):\n                    shape((2, 3)),\n                    r'`compressed_indices\\[..., 0\\] == 0` is not satisfied.')\n \n+            yield ('invalid compressed_indices[0] when nnz == 0',\n+                   tensor([1, 0], dtype=torch.int64),\n+                   tensor([], dtype=torch.int64),\n+                   values([1])[:0],\n+                   shape((1, 1)),\n+                   r'`compressed_indices\\[..., 0\\] == 0` is not satisfied.')\n+\n             yield ('invalid compressed_indices[-1]',\n                    tensor([0, 2, 5]),\n                    tensor([0, 1, 0, 2]),\n@@ -734,6 +743,13 @@ class TestSparseCompressed(TestCase):\n                    shape((2, 3)),\n                    r'`compressed_indices\\[..., -1\\] == nnz` is not satisfied.')\n \n+            yield ('invalid compressed_indices[-1] when nnz == 0',\n+                   tensor([0, 1], dtype=torch.int64),\n+                   tensor([], dtype=torch.int64),\n+                   values([1])[:0],\n+                   shape((1, 1)),\n+                   r'`compressed_indices\\[..., -1\\] == nnz` is not satisfied.')\n+\n             yield ('invalid compressed_indices.diff(dim=-1)',\n                    tensor([0, 0, 4]),\n                    tensor([0, 1, 0, 2]),"
        }
    ]
},
{
    "Id": 18,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/f422027fceb71b1f2f7d940f3ade32ced2ed0ba0",
    "date": "2024-07-12T23:06:52+00:00",
    "message": "fix torch.linalg.lstsq input check (#130612)\n\nFixes [#117236 ](https://github.com/pytorch/pytorch/issues/117236)\nThe current case does not meet the vector scenario requirements, and it lacks sufficient checks (relying solely on ```dim_diff``` is insufficient).  Consequently, it triggers an internal assertion error.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130612\nApproved by: https://github.com/lezcano",
    "label": "YES",
    "changes": [
        {
            "name": "BatchLinearAlgebra.cpp",
            "path": "aten/src/ATen/native/BatchLinearAlgebra.cpp",
            "patches": [
                {
                    "old_start": 3524,
                    "old_length": 10,
                    "new_start": 3524,
                    "new_length": 22,
                    "hunk": "@@ -3524,10 +3524,22 @@ std::tuple<Tensor&, Tensor&, Tensor&, Tensor&> linalg_lstsq_out(\n   TORCH_CHECK(\n       0 <= dim_diff && dim_diff <= 1,\n       \"torch.linalg.lstsq: input.dim() must be greater or equal to other.dim() and (input.dim() - other.dim()) <= 1\");\n-  Tensor other_2d = dim_diff ? other.unsqueeze(-1) : other;\n+\n+  // now check whether the provided output tensors can be used directly\n+\n+  // Two types of 'other' tensors are supported:\n+  // - 1-dimensional (1D) tensor or batch of 1D tensors (vector case)\n+  // - 2-dimensional (2D) tensor or batch of 2D tensors (matrix case)\n+  // original torch.lstsq supported only the matrix case, while NumPy works for both cases\n+  // for the batched input we need to be able to distinguish them\n+  // auto expected_batched_rhs_shape = IntArrayRef(input.sizes().data(), input.dim() - 1); // input.shape[:-1]\n+  // bool vector_case = other.dim() == 1 || (input.dim() - 1 == other.dim() && other.sizes().equals(expected_batched_rhs_shape));\n+\n+  bool vector_case = linalg_solve_is_vector_rhs(input, other);\n+  Tensor other_2d = vector_case ? other.unsqueeze(-1) : other;\n   TORCH_CHECK(\n       input.size(-2) == other_2d.size(-2),\n-      dim_diff ? \"torch.linalg.lstsq: input.size(-2) should match other.size(-1)\"\n+      vector_case ? \"torch.linalg.lstsq: input.size(-2) should match other.size(-1)\"\n                : \"torch.linalg.lstsq: input.size(-2) should match other.size(-2)\");\n \n   checkSameDevice(\"torch.linalg.lstsq\", other, input, \"other\");\n"
                },
                {
                    "old_start": 3561,
                    "old_length": 17,
                    "new_start": 3573,
                    "new_length": 6,
                    "hunk": "@@ -3561,17 +3573,6 @@ std::tuple<Tensor&, Tensor&, Tensor&, Tensor&> linalg_lstsq_out(\n \n   auto infos = at::zeros({std::max<int64_t>(1, batchCount(input))}, input.options().dtype(kInt));\n \n-  // now check whether the provided output tensors can be used directly\n-\n-  // Two types of 'other' tensors are supported:\n-  // - 1-dimensional (1D) tensor or batch of 1D tensors (vector case)\n-  // - 2-dimensional (2D) tensor or batch of 2D tensors (matrix case)\n-  // original torch.lstsq supported only the matrix case, while NumPy works for both cases\n-  // for the batched input we need to be able to distinguish them\n-  // auto expected_batched_rhs_shape = IntArrayRef(input.sizes().data(), input.dim() - 1); // input.shape[:-1]\n-  // bool vector_case = other.dim() == 1 || (input.dim() - 1 == other.dim() && other.sizes().equals(expected_batched_rhs_shape));\n-  bool vector_case = linalg_solve_is_vector_rhs(input, other);\n-\n   // provided output tensor can be used directly if:\n   // 1. the shape matches the expected shape\n   // 2. the dtype matches the expected dtype\n"
                }
            ],
            "whole_deleted": "-  Tensor other_2d = dim_diff ? other.unsqueeze(-1) : other;\n-      dim_diff ? \"torch.linalg.lstsq: input.size(-2) should match other.size(-1)\"\n-  // now check whether the provided output tensors can be used directly\n-\n-  // Two types of 'other' tensors are supported:\n-  // - 1-dimensional (1D) tensor or batch of 1D tensors (vector case)\n-  // - 2-dimensional (2D) tensor or batch of 2D tensors (matrix case)\n-  // original torch.lstsq supported only the matrix case, while NumPy works for both cases\n-  // for the batched input we need to be able to distinguish them\n-  // auto expected_batched_rhs_shape = IntArrayRef(input.sizes().data(), input.dim() - 1); // input.shape[:-1]\n-  // bool vector_case = other.dim() == 1 || (input.dim() - 1 == other.dim() && other.sizes().equals(expected_batched_rhs_shape));\n-  bool vector_case = linalg_solve_is_vector_rhs(input, other);\n-\n",
            "whole_added": "+\n+  // now check whether the provided output tensors can be used directly\n+\n+  // Two types of 'other' tensors are supported:\n+  // - 1-dimensional (1D) tensor or batch of 1D tensors (vector case)\n+  // - 2-dimensional (2D) tensor or batch of 2D tensors (matrix case)\n+  // original torch.lstsq supported only the matrix case, while NumPy works for both cases\n+  // for the batched input we need to be able to distinguish them\n+  // auto expected_batched_rhs_shape = IntArrayRef(input.sizes().data(), input.dim() - 1); // input.shape[:-1]\n+  // bool vector_case = other.dim() == 1 || (input.dim() - 1 == other.dim() && other.sizes().equals(expected_batched_rhs_shape));\n+\n+  bool vector_case = linalg_solve_is_vector_rhs(input, other);\n+  Tensor other_2d = vector_case ? other.unsqueeze(-1) : other;\n+      vector_case ? \"torch.linalg.lstsq: input.size(-2) should match other.size(-1)\"\n",
            "whole_hunk": "@@ -3524,10 +3524,22 @@ std::tuple<Tensor&, Tensor&, Tensor&, Tensor&> linalg_lstsq_out(\n   TORCH_CHECK(\n       0 <= dim_diff && dim_diff <= 1,\n       \"torch.linalg.lstsq: input.dim() must be greater or equal to other.dim() and (input.dim() - other.dim()) <= 1\");\n-  Tensor other_2d = dim_diff ? other.unsqueeze(-1) : other;\n+\n+  // now check whether the provided output tensors can be used directly\n+\n+  // Two types of 'other' tensors are supported:\n+  // - 1-dimensional (1D) tensor or batch of 1D tensors (vector case)\n+  // - 2-dimensional (2D) tensor or batch of 2D tensors (matrix case)\n+  // original torch.lstsq supported only the matrix case, while NumPy works for both cases\n+  // for the batched input we need to be able to distinguish them\n+  // auto expected_batched_rhs_shape = IntArrayRef(input.sizes().data(), input.dim() - 1); // input.shape[:-1]\n+  // bool vector_case = other.dim() == 1 || (input.dim() - 1 == other.dim() && other.sizes().equals(expected_batched_rhs_shape));\n+\n+  bool vector_case = linalg_solve_is_vector_rhs(input, other);\n+  Tensor other_2d = vector_case ? other.unsqueeze(-1) : other;\n   TORCH_CHECK(\n       input.size(-2) == other_2d.size(-2),\n-      dim_diff ? \"torch.linalg.lstsq: input.size(-2) should match other.size(-1)\"\n+      vector_case ? \"torch.linalg.lstsq: input.size(-2) should match other.size(-1)\"\n                : \"torch.linalg.lstsq: input.size(-2) should match other.size(-2)\");\n \n   checkSameDevice(\"torch.linalg.lstsq\", other, input, \"other\");\n@@ -3561,17 +3573,6 @@ std::tuple<Tensor&, Tensor&, Tensor&, Tensor&> linalg_lstsq_out(\n \n   auto infos = at::zeros({std::max<int64_t>(1, batchCount(input))}, input.options().dtype(kInt));\n \n-  // now check whether the provided output tensors can be used directly\n-\n-  // Two types of 'other' tensors are supported:\n-  // - 1-dimensional (1D) tensor or batch of 1D tensors (vector case)\n-  // - 2-dimensional (2D) tensor or batch of 2D tensors (matrix case)\n-  // original torch.lstsq supported only the matrix case, while NumPy works for both cases\n-  // for the batched input we need to be able to distinguish them\n-  // auto expected_batched_rhs_shape = IntArrayRef(input.sizes().data(), input.dim() - 1); // input.shape[:-1]\n-  // bool vector_case = other.dim() == 1 || (input.dim() - 1 == other.dim() && other.sizes().equals(expected_batched_rhs_shape));\n-  bool vector_case = linalg_solve_is_vector_rhs(input, other);\n-\n   // provided output tensor can be used directly if:\n   // 1. the shape matches the expected shape\n   // 2. the dtype matches the expected dtype\n"
        },
        {
            "name": "test_linalg.py",
            "path": "test/test_linalg.py",
            "patches": [
                {
                    "old_start": 425,
                    "old_length": 6,
                    "new_start": 425,
                    "new_length": 12,
                    "hunk": "@@ -425,6 +425,12 @@ class TestLinalg(TestCase):\n         with self.assertRaisesRegex(RuntimeError, r'input.size\\(-2\\) should match other.size\\(-2\\)'):\n             torch.linalg.lstsq(a, b.unsqueeze(-1))\n \n+        a = torch.randn(1, 1, 1, dtype=dtype, device=device)\n+        b = torch.randn(3, 1, dtype=dtype, device=device)\n+\n+        with self.assertRaisesRegex(RuntimeError, r'input.size\\(-2\\) should match other.size\\(-2\\)'):\n+            torch.linalg.lstsq(a, b)\n+\n         def complement_device(device):\n             if device == 'cpu' and torch.cuda.is_available():\n                 return 'cuda'"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        a = torch.randn(1, 1, 1, dtype=dtype, device=device)\n+        b = torch.randn(3, 1, dtype=dtype, device=device)\n+\n+        with self.assertRaisesRegex(RuntimeError, r'input.size\\(-2\\) should match other.size\\(-2\\)'):\n+            torch.linalg.lstsq(a, b)\n+\n",
            "whole_hunk": "@@ -425,6 +425,12 @@ class TestLinalg(TestCase):\n         with self.assertRaisesRegex(RuntimeError, r'input.size\\(-2\\) should match other.size\\(-2\\)'):\n             torch.linalg.lstsq(a, b.unsqueeze(-1))\n \n+        a = torch.randn(1, 1, 1, dtype=dtype, device=device)\n+        b = torch.randn(3, 1, dtype=dtype, device=device)\n+\n+        with self.assertRaisesRegex(RuntimeError, r'input.size\\(-2\\) should match other.size\\(-2\\)'):\n+            torch.linalg.lstsq(a, b)\n+\n         def complement_device(device):\n             if device == 'cpu' and torch.cuda.is_available():\n                 return 'cuda'"
        }
    ]
},
{
    "Id": 323,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/f1aef2c0941139fcdc8d59208718127959f6a706",
    "date": "2024-01-26T15:24:29+00:00",
    "message": "Don't check is_conj for `_refs.linalg.svd` (#117972)\n\nThe flag is not correctly set when PyTorch is compiled with GPU support resulting in failures in\n`test_ops.py::test_python_ref_meta__refs_linalg_svd_cpu_complex`.\n\nUse a similar approach to test_meta and skip the check for this function.\n\nWorkaround for #105068\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/117972\nApproved by: https://github.com/lezcano",
    "label": "YES",
    "changes": [
        {
            "name": "test_ops.py",
            "path": "test/test_ops.py",
            "patches": [
                {
                    "old_start": 315,
                    "old_length": 6,
                    "new_start": 315,
                    "new_length": 10,
                    "hunk": "@@ -315,6 +315,10 @@ class TestCommon(TestCase):\n     @ops(python_ref_db)\n     @skipIfTorchInductor(\"Takes too long for inductor\")\n     def test_python_ref_meta(self, device, dtype, op):\n+        CHECK_CONJ_SKIPS = {\n+            torch._refs.linalg.svd,\n+        }\n+\n         with FakeTensorMode() as mode:\n             pass\n \n"
                },
                {
                    "old_start": 341,
                    "old_length": 12,
                    "new_start": 345,
                    "new_length": 12,
                    "hunk": "@@ -341,12 +345,12 @@ class TestCommon(TestCase):\n \n             if isinstance(result, torch.Tensor):\n                 self.assertTrue(isinstance(meta_result, FakeTensor))\n-                prims.utils.compare_tensor_meta(result, meta_result)\n+                prims.utils.compare_tensor_meta(result, meta_result, check_conj=op.op not in CHECK_CONJ_SKIPS)\n             elif isinstance(result, Sequence):\n                 for a, b in zip(result, meta_result):\n                     if isinstance(a, torch.Tensor) or isinstance(b, torch.Tensor):\n                         self.assertTrue(isinstance(b, FakeTensor))\n-                        prims.utils.compare_tensor_meta(a, b)\n+                        prims.utils.compare_tensor_meta(a, b, check_conj=op.op not in CHECK_CONJ_SKIPS)\n \n     def _ref_test_helper(\n         self,\n"
                }
            ],
            "whole_deleted": "-                prims.utils.compare_tensor_meta(result, meta_result)\n-                        prims.utils.compare_tensor_meta(a, b)\n",
            "whole_added": "+        CHECK_CONJ_SKIPS = {\n+            torch._refs.linalg.svd,\n+        }\n+\n+                prims.utils.compare_tensor_meta(result, meta_result, check_conj=op.op not in CHECK_CONJ_SKIPS)\n+                        prims.utils.compare_tensor_meta(a, b, check_conj=op.op not in CHECK_CONJ_SKIPS)\n",
            "whole_hunk": "@@ -315,6 +315,10 @@ class TestCommon(TestCase):\n     @ops(python_ref_db)\n     @skipIfTorchInductor(\"Takes too long for inductor\")\n     def test_python_ref_meta(self, device, dtype, op):\n+        CHECK_CONJ_SKIPS = {\n+            torch._refs.linalg.svd,\n+        }\n+\n         with FakeTensorMode() as mode:\n             pass\n \n@@ -341,12 +345,12 @@ class TestCommon(TestCase):\n \n             if isinstance(result, torch.Tensor):\n                 self.assertTrue(isinstance(meta_result, FakeTensor))\n-                prims.utils.compare_tensor_meta(result, meta_result)\n+                prims.utils.compare_tensor_meta(result, meta_result, check_conj=op.op not in CHECK_CONJ_SKIPS)\n             elif isinstance(result, Sequence):\n                 for a, b in zip(result, meta_result):\n                     if isinstance(a, torch.Tensor) or isinstance(b, torch.Tensor):\n                         self.assertTrue(isinstance(b, FakeTensor))\n-                        prims.utils.compare_tensor_meta(a, b)\n+                        prims.utils.compare_tensor_meta(a, b, check_conj=op.op not in CHECK_CONJ_SKIPS)\n \n     def _ref_test_helper(\n         self,\n"
        },
        {
            "name": "__init__.py",
            "path": "torch/_prims_common/__init__.py",
            "patches": [
                {
                    "old_start": 130,
                    "old_length": 6,
                    "new_start": 130,
                    "new_length": 7,
                    "hunk": "@@ -130,6 +130,7 @@ def compare_tensor_meta(\n     check_strides=False,\n     *,\n     allow_rhs_unbacked=False,\n+    check_conj=True,\n ):\n     \"\"\"\n     Checks that two tensor likes have the same shape,\n"
                },
                {
                    "old_start": 171,
                    "old_length": 10,
                    "new_start": 172,
                    "new_length": 11,
                    "hunk": "@@ -171,10 +172,11 @@ def compare_tensor_meta(\n             msg = f\"Storage offset mismatch! Storage offsets are {a.storage_offset()} and {b.storage_offset()}!\"\n             raise RuntimeError(msg)\n \n-    if a.is_conj() != b.is_conj():\n-        raise RuntimeError(\n-            f\"Conj mismatch! is_conj is set to {a.is_conj()} and {b.is_conj()}\"\n-        )\n+    if check_conj:\n+        if a.is_conj() != b.is_conj():\n+            raise RuntimeError(\n+                f\"Conj mismatch! is_conj is set to {a.is_conj()} and {b.is_conj()}\"\n+            )\n \n     if a.is_neg() != b.is_neg():\n         raise RuntimeError("
                }
            ],
            "whole_deleted": "-    if a.is_conj() != b.is_conj():\n-        raise RuntimeError(\n-            f\"Conj mismatch! is_conj is set to {a.is_conj()} and {b.is_conj()}\"\n-        )\n",
            "whole_added": "+    check_conj=True,\n+    if check_conj:\n+        if a.is_conj() != b.is_conj():\n+            raise RuntimeError(\n+                f\"Conj mismatch! is_conj is set to {a.is_conj()} and {b.is_conj()}\"\n+            )\n",
            "whole_hunk": "@@ -130,6 +130,7 @@ def compare_tensor_meta(\n     check_strides=False,\n     *,\n     allow_rhs_unbacked=False,\n+    check_conj=True,\n ):\n     \"\"\"\n     Checks that two tensor likes have the same shape,\n@@ -171,10 +172,11 @@ def compare_tensor_meta(\n             msg = f\"Storage offset mismatch! Storage offsets are {a.storage_offset()} and {b.storage_offset()}!\"\n             raise RuntimeError(msg)\n \n-    if a.is_conj() != b.is_conj():\n-        raise RuntimeError(\n-            f\"Conj mismatch! is_conj is set to {a.is_conj()} and {b.is_conj()}\"\n-        )\n+    if check_conj:\n+        if a.is_conj() != b.is_conj():\n+            raise RuntimeError(\n+                f\"Conj mismatch! is_conj is set to {a.is_conj()} and {b.is_conj()}\"\n+            )\n \n     if a.is_neg() != b.is_neg():\n         raise RuntimeError("
        }
    ]
},
{
    "Id": 481,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/097defb1608827d82b18b27adeec0a98b72a9281",
    "date": "2023-10-12T03:37:18+00:00",
    "message": "[device mesh] only check when world size > num_devices per host (#111091)\n\nas titled\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111091\nApproved by: https://github.com/awgu, https://github.com/wz337\nghstack dependencies: #110898, #110900",
    "label": "NO",
    "changes": [
        {
            "name": "device_mesh.py",
            "path": "torch/distributed/_tensor/device_mesh.py",
            "patches": [
                {
                    "old_start": 188,
                    "old_length": 7,
                    "new_start": 188,
                    "new_length": 10,
                    "hunk": "@@ -188,7 +188,10 @@ class DeviceMesh:\n             # automatically set the current cuda/cuda-like device base on num of gpu devices available in each host\n             # NOTE: This device selection would only work for homogeneous hardware.\n             num_devices_per_host = device_handle.device_count()\n-            if world_size % num_devices_per_host != 0:\n+            if (\n+                world_size > num_devices_per_host\n+                and world_size % num_devices_per_host != 0\n+            ):\n                 raise RuntimeError(\n                     f\"DeviceMesh only support homogeneous hardware, but found \"\n                     f\"{world_size} ranks and {num_devices_per_host} {self.device_type} devices!\""
                }
            ],
            "whole_deleted": "-            if world_size % num_devices_per_host != 0:\n",
            "whole_added": "+            if (\n+                world_size > num_devices_per_host\n+                and world_size % num_devices_per_host != 0\n+            ):\n",
            "whole_hunk": "@@ -188,7 +188,10 @@ class DeviceMesh:\n             # automatically set the current cuda/cuda-like device base on num of gpu devices available in each host\n             # NOTE: This device selection would only work for homogeneous hardware.\n             num_devices_per_host = device_handle.device_count()\n-            if world_size % num_devices_per_host != 0:\n+            if (\n+                world_size > num_devices_per_host\n+                and world_size % num_devices_per_host != 0\n+            ):\n                 raise RuntimeError(\n                     f\"DeviceMesh only support homogeneous hardware, but found \"\n                     f\"{world_size} ranks and {num_devices_per_host} {self.device_type} devices!\""
        }
    ]
},
{
    "Id": 15,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/a7cfe40c9bb22eef588b1916a7f7e8b132919c8d",
    "date": "2024-07-15T18:52:55+00:00",
    "message": "[dtensor] Improve from_local API with run_check (#130289)\n\nas titled, this PR:\n1. switch `run_check` to be by default False and add extra doc/comments\n   about the correctness guarantee. Since I observed so many calls\nforget to use run_check=False, we should simply switch to not perform\nmetadata check and make our documentation explicit\n2. Implement metadata check by picking up the changes from https://github.com/pytorch/pytorch/pull/115229\n3. Improve the from_local documentation\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130289\nApproved by: https://github.com/awgu, https://github.com/wz337\nghstack dependencies: #130286, #130287, #130288",
    "label": "NO",
    "changes": [
        {
            "name": "test_dtensor.py",
            "path": "test/distributed/_tensor/test_dtensor.py",
            "patches": [
                {
                    "old_start": 822,
                    "old_length": 6,
                    "new_start": 822,
                    "new_length": 53,
                    "hunk": "@@ -822,6 +822,53 @@ class DTensorMeshTest(DTensorTestBase):\n             (numel_1_tensor + sharded_dtensor).to_local(), numel_1_tensor + local_tensor\n         )\n \n+    @with_comms\n+    def test_metadata_consistency_check(self):\n+        device_mesh = DeviceMesh(self.device_type, list(range(self.world_size)))\n+        placements = [Shard(0)]\n+\n+        # Create a local tensor with specific metadata and check dtype change\n+        local_tensor = torch.randn(3, 3, requires_grad=True, dtype=torch.float32)\n+\n+        if self.rank == 0:\n+            local_tensor = local_tensor.to(dtype=torch.float64)\n+\n+        with self.assertRaises(ValueError):\n+            DTensor.from_local(local_tensor, device_mesh, placements, run_check=True)\n+\n+        try:\n+            DTensor.from_local(local_tensor, device_mesh, placements, run_check=False)\n+        except ValueError:\n+            self.fail(\"Unexpected ValueError raised with run_check=False\")\n+\n+        # Create a local tensor with specific metadata and check requires_grad change\n+        local_tensor = torch.randn(3, 3, requires_grad=True, dtype=torch.float32)\n+\n+        if self.rank == 0:\n+            local_tensor.requires_grad = False\n+\n+        with self.assertRaises(ValueError):\n+            DTensor.from_local(local_tensor, device_mesh, placements, run_check=True)\n+\n+        try:\n+            DTensor.from_local(local_tensor, device_mesh, placements, run_check=False)\n+        except ValueError:\n+            self.fail(\"Unexpected ValueError raised with run_check=False\")\n+\n+        # Create a local tensor with specific metadata and check stride change\n+        local_tensor = torch.randn(3, 4, requires_grad=True, dtype=torch.float32)\n+\n+        if self.rank == 0:\n+            local_tensor = local_tensor.t()  # transpose changes the stride\n+\n+        with self.assertRaises(ValueError):\n+            DTensor.from_local(local_tensor, device_mesh, placements, run_check=True)\n+\n+        try:\n+            DTensor.from_local(local_tensor, device_mesh, placements, run_check=False)\n+        except ValueError:\n+            self.fail(\"Unexpected ValueError raised with run_check=False\")\n+\n \n class TestDTensorPlacementTypes(DTensorTestBase):\n     @property\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    @with_comms\n+    def test_metadata_consistency_check(self):\n+        device_mesh = DeviceMesh(self.device_type, list(range(self.world_size)))\n+        placements = [Shard(0)]\n+\n+        # Create a local tensor with specific metadata and check dtype change\n+        local_tensor = torch.randn(3, 3, requires_grad=True, dtype=torch.float32)\n+\n+        if self.rank == 0:\n+            local_tensor = local_tensor.to(dtype=torch.float64)\n+\n+        with self.assertRaises(ValueError):\n+            DTensor.from_local(local_tensor, device_mesh, placements, run_check=True)\n+\n+        try:\n+            DTensor.from_local(local_tensor, device_mesh, placements, run_check=False)\n+        except ValueError:\n+            self.fail(\"Unexpected ValueError raised with run_check=False\")\n+\n+        # Create a local tensor with specific metadata and check requires_grad change\n+        local_tensor = torch.randn(3, 3, requires_grad=True, dtype=torch.float32)\n+\n+        if self.rank == 0:\n+            local_tensor.requires_grad = False\n+\n+        with self.assertRaises(ValueError):\n+            DTensor.from_local(local_tensor, device_mesh, placements, run_check=True)\n+\n+        try:\n+            DTensor.from_local(local_tensor, device_mesh, placements, run_check=False)\n+        except ValueError:\n+            self.fail(\"Unexpected ValueError raised with run_check=False\")\n+\n+        # Create a local tensor with specific metadata and check stride change\n+        local_tensor = torch.randn(3, 4, requires_grad=True, dtype=torch.float32)\n+\n+        if self.rank == 0:\n+            local_tensor = local_tensor.t()  # transpose changes the stride\n+\n+        with self.assertRaises(ValueError):\n+            DTensor.from_local(local_tensor, device_mesh, placements, run_check=True)\n+\n+        try:\n+            DTensor.from_local(local_tensor, device_mesh, placements, run_check=False)\n+        except ValueError:\n+            self.fail(\"Unexpected ValueError raised with run_check=False\")\n+\n",
            "whole_hunk": "@@ -822,6 +822,53 @@ class DTensorMeshTest(DTensorTestBase):\n             (numel_1_tensor + sharded_dtensor).to_local(), numel_1_tensor + local_tensor\n         )\n \n+    @with_comms\n+    def test_metadata_consistency_check(self):\n+        device_mesh = DeviceMesh(self.device_type, list(range(self.world_size)))\n+        placements = [Shard(0)]\n+\n+        # Create a local tensor with specific metadata and check dtype change\n+        local_tensor = torch.randn(3, 3, requires_grad=True, dtype=torch.float32)\n+\n+        if self.rank == 0:\n+            local_tensor = local_tensor.to(dtype=torch.float64)\n+\n+        with self.assertRaises(ValueError):\n+            DTensor.from_local(local_tensor, device_mesh, placements, run_check=True)\n+\n+        try:\n+            DTensor.from_local(local_tensor, device_mesh, placements, run_check=False)\n+        except ValueError:\n+            self.fail(\"Unexpected ValueError raised with run_check=False\")\n+\n+        # Create a local tensor with specific metadata and check requires_grad change\n+        local_tensor = torch.randn(3, 3, requires_grad=True, dtype=torch.float32)\n+\n+        if self.rank == 0:\n+            local_tensor.requires_grad = False\n+\n+        with self.assertRaises(ValueError):\n+            DTensor.from_local(local_tensor, device_mesh, placements, run_check=True)\n+\n+        try:\n+            DTensor.from_local(local_tensor, device_mesh, placements, run_check=False)\n+        except ValueError:\n+            self.fail(\"Unexpected ValueError raised with run_check=False\")\n+\n+        # Create a local tensor with specific metadata and check stride change\n+        local_tensor = torch.randn(3, 4, requires_grad=True, dtype=torch.float32)\n+\n+        if self.rank == 0:\n+            local_tensor = local_tensor.t()  # transpose changes the stride\n+\n+        with self.assertRaises(ValueError):\n+            DTensor.from_local(local_tensor, device_mesh, placements, run_check=True)\n+\n+        try:\n+            DTensor.from_local(local_tensor, device_mesh, placements, run_check=False)\n+        except ValueError:\n+            self.fail(\"Unexpected ValueError raised with run_check=False\")\n+\n \n class TestDTensorPlacementTypes(DTensorTestBase):\n     @property\n"
        },
        {
            "name": "_collective_utils.py",
            "path": "torch/distributed/_tensor/_collective_utils.py",
            "patches": [
                {
                    "old_start": 193,
                    "old_length": 6,
                    "new_start": 193,
                    "new_length": 30,
                    "hunk": "@@ -193,6 +193,30 @@ def fill_empty_tensor_to_shards(\n     return shards\n \n \n+def check_tensor_meta(\n+    local_tensor, check_shape_stride=False\n+) -> Optional[\"placement_types.TensorMeta\"]:\n+    local_metadata = {\n+        \"dtype\": local_tensor.dtype,\n+        \"requires_grad\": local_tensor.requires_grad,\n+    }\n+\n+    if check_shape_stride:\n+        local_metadata.update(\n+            {\"shape\": local_tensor.shape, \"stride\": local_tensor.stride()}\n+        )\n+\n+    gathered_metadata = [None for _ in range(torch.distributed.get_world_size())]\n+    torch.distributed.all_gather_object(gathered_metadata, local_metadata)\n+\n+    # Check if metadata is consistent across ranks\n+    if not all(meta == local_metadata for meta in gathered_metadata):\n+        raise ValueError(\n+            \"Inconsistent tensor metadata (including shape and stride) across ranks.\"\n+        )\n+    return None\n+\n+\n def spec_to_bytes(spec: \"placement_types.DTensorSpec\") -> int:\n     assert spec.tensor_meta is not None, \"spec should have tensor meta defined!\"\n     return spec.tensor_meta.dtype.itemsize * math.prod(spec.shape)\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+def check_tensor_meta(\n+    local_tensor, check_shape_stride=False\n+) -> Optional[\"placement_types.TensorMeta\"]:\n+    local_metadata = {\n+        \"dtype\": local_tensor.dtype,\n+        \"requires_grad\": local_tensor.requires_grad,\n+    }\n+\n+    if check_shape_stride:\n+        local_metadata.update(\n+            {\"shape\": local_tensor.shape, \"stride\": local_tensor.stride()}\n+        )\n+\n+    gathered_metadata = [None for _ in range(torch.distributed.get_world_size())]\n+    torch.distributed.all_gather_object(gathered_metadata, local_metadata)\n+\n+    # Check if metadata is consistent across ranks\n+    if not all(meta == local_metadata for meta in gathered_metadata):\n+        raise ValueError(\n+            \"Inconsistent tensor metadata (including shape and stride) across ranks.\"\n+        )\n+    return None\n+\n+\n",
            "whole_hunk": "@@ -193,6 +193,30 @@ def fill_empty_tensor_to_shards(\n     return shards\n \n \n+def check_tensor_meta(\n+    local_tensor, check_shape_stride=False\n+) -> Optional[\"placement_types.TensorMeta\"]:\n+    local_metadata = {\n+        \"dtype\": local_tensor.dtype,\n+        \"requires_grad\": local_tensor.requires_grad,\n+    }\n+\n+    if check_shape_stride:\n+        local_metadata.update(\n+            {\"shape\": local_tensor.shape, \"stride\": local_tensor.stride()}\n+        )\n+\n+    gathered_metadata = [None for _ in range(torch.distributed.get_world_size())]\n+    torch.distributed.all_gather_object(gathered_metadata, local_metadata)\n+\n+    # Check if metadata is consistent across ranks\n+    if not all(meta == local_metadata for meta in gathered_metadata):\n+        raise ValueError(\n+            \"Inconsistent tensor metadata (including shape and stride) across ranks.\"\n+        )\n+    return None\n+\n+\n def spec_to_bytes(spec: \"placement_types.DTensorSpec\") -> int:\n     assert spec.tensor_meta is not None, \"spec should have tensor meta defined!\"\n     return spec.tensor_meta.dtype.itemsize * math.prod(spec.shape)\n"
        },
        {
            "name": "api.py",
            "path": "torch/distributed/_tensor/api.py",
            "patches": [
                {
                    "old_start": 8,
                    "old_length": 7,
                    "new_start": 8,
                    "new_length": 10,
                    "hunk": "@@ -8,7 +8,10 @@ import torch\n import torch.distributed._tensor._dispatch as op_dispatch\n import torch.distributed._tensor.random as random\n import torch.nn as nn\n-from torch.distributed._tensor._collective_utils import mesh_broadcast\n+from torch.distributed._tensor._collective_utils import (\n+    check_tensor_meta,\n+    mesh_broadcast,\n+)\n from torch.distributed._tensor._redistribute import (\n     Redistribute,\n     redistribute_local_tensor,\n"
                },
                {
                    "old_start": 141,
                    "old_length": 7,
                    "new_start": 144,
                    "new_length": 10,
                    "hunk": "@@ -141,7 +144,10 @@ class _FromTorchTensor(torch.autograd.Function):\n             # simply set the local tensor to an empty tensor\n             input = input.new_empty(0, requires_grad=input.requires_grad)\n         elif run_check:\n-            # TODO: by default check tensor metas across rank\n+            # TODO: support uneven sharding when global shape/stride not passed, by\n+            # building the global TensorMeta during check_tensor_meta\n+            check_shape_stride = not shape and not stride\n+            check_tensor_meta(input, check_shape_stride=check_shape_stride)\n             # TODO: See if we need to make this run_check logic\n             # have a corresponding backward.\n             for idx, placement in enumerate(placements):\n"
                },
                {
                    "old_start": 318,
                    "old_length": 13,
                    "new_start": 324,
                    "new_length": 13,
                    "hunk": "@@ -318,13 +324,13 @@ class DTensor(torch.Tensor):  # pyre-ignore[13]: pyre is bad at __new__\n         device_mesh: Optional[DeviceMesh] = None,\n         placements: Optional[Sequence[Placement]] = None,\n         *,\n-        run_check: bool = True,\n+        run_check: bool = False,\n         shape: Optional[torch.Size] = None,\n         stride: Optional[Tuple[int, ...]] = None,\n     ) -> \"DTensor\":\n         \"\"\"\n         Create a :class:`DTensor` from a local torch.Tensor on each rank\n-        according to the `device_mesh` and `placements` specified.\n+        according to the ``device_mesh`` and ``placements`` specified.\n \n         Args:\n             local_tensor (torch.Tensor): local torch.Tensor on each rank.\n"
                },
                {
                    "old_start": 333,
                    "old_length": 28,
                    "new_start": 339,
                    "new_length": 31,
                    "hunk": "@@ -333,28 +339,31 @@ class DTensor(torch.Tensor):  # pyre-ignore[13]: pyre is bad at __new__\n                 context manager, default: None\n             placements (List[:class:`Placement`], optional): the placements that\n                 describes how to place the local torch.Tensor on DeviceMesh, must\n-                have the same number of elements as `device_mesh.ndim`. If not\n-                specified, we will by default replicate the tensor across the\n-                `device_mesh` from the first rank of each dimension of the `device_mesh`.\n+                have the same number of elements as ``device_mesh.ndim``.\n \n         Keyword args:\n-            run_check (bool, optional): indicate whether to run check across ranks\n-                to check meta information and data. if have :class:`Replicate` in\n-                `placements`, the data on first rank of the device mesh dimension\n-                will be broadcasted to other ranks.\n+            run_check (bool, optional): at a cost of extra communications, perform\n+                sanity check across ranks to check each local tensor's meta information\n+                to ensure correctness. If have :class:`Replicate` in ``placements``, the\n+                data on first rank of the device mesh dimension will be broadcasted\n+                to other ranks. default: False\n             shape (torch.Size, optional): A List of int which specifies the size of\n                 DTensor which build on top of `local_tensor`. Note this needs to be\n-                provided if the shape of `local_tensor` are different across the ranks.\n-                If not provided, `shape` will be computed assuming the given distributed\n-                tensor is evenly sharded across ranks.\n+                provided if the shape of ``local_tensor`` are different across the ranks.\n+                If not provided, ``shape`` will be computed assuming the given distributed\n+                tensor is evenly sharded across ranks. default: None\n             stride (tuple, optional): A List of int which specifies the stride of DTensor.\n-                If not provided, `stride` will be computed assuming the given distributed\n-                tensor is evenly sharded across ranks.\n+                If not provided, ``stride`` will be computed assuming the given distributed\n+                tensor is evenly sharded across ranks. default: None\n \n         Returns:\n             A :class:`DTensor` object\n \n-        .. note:: `from_local` is differentiable, the `requires_grad` of the created\n+        .. note:: When ``run_check=False``, it is the user's responsibility to ensure the\n+            local tensor passed in is correct across ranks. If not, the behavior of the created\n+            DTensor is undefined.\n+\n+        .. note:: ``from_local`` is differentiable, the `requires_grad` of the created\n             `DTensor` object will depend on if `local_tensor` requires_grad or not.\n         \"\"\"\n         # if same shape/dtype, no need to run_check, if not, must allgather"
                }
            ],
            "whole_deleted": "-from torch.distributed._tensor._collective_utils import mesh_broadcast\n-            # TODO: by default check tensor metas across rank\n-        run_check: bool = True,\n-        according to the `device_mesh` and `placements` specified.\n-                have the same number of elements as `device_mesh.ndim`. If not\n-                specified, we will by default replicate the tensor across the\n-                `device_mesh` from the first rank of each dimension of the `device_mesh`.\n-            run_check (bool, optional): indicate whether to run check across ranks\n-                to check meta information and data. if have :class:`Replicate` in\n-                `placements`, the data on first rank of the device mesh dimension\n-                will be broadcasted to other ranks.\n-                provided if the shape of `local_tensor` are different across the ranks.\n-                If not provided, `shape` will be computed assuming the given distributed\n-                tensor is evenly sharded across ranks.\n-                If not provided, `stride` will be computed assuming the given distributed\n-                tensor is evenly sharded across ranks.\n-        .. note:: `from_local` is differentiable, the `requires_grad` of the created\n",
            "whole_added": "+from torch.distributed._tensor._collective_utils import (\n+    check_tensor_meta,\n+    mesh_broadcast,\n+)\n+            # TODO: support uneven sharding when global shape/stride not passed, by\n+            # building the global TensorMeta during check_tensor_meta\n+            check_shape_stride = not shape and not stride\n+            check_tensor_meta(input, check_shape_stride=check_shape_stride)\n+        run_check: bool = False,\n+        according to the ``device_mesh`` and ``placements`` specified.\n+                have the same number of elements as ``device_mesh.ndim``.\n+            run_check (bool, optional): at a cost of extra communications, perform\n+                sanity check across ranks to check each local tensor's meta information\n+                to ensure correctness. If have :class:`Replicate` in ``placements``, the\n+                data on first rank of the device mesh dimension will be broadcasted\n+                to other ranks. default: False\n+                provided if the shape of ``local_tensor`` are different across the ranks.\n+                If not provided, ``shape`` will be computed assuming the given distributed\n+                tensor is evenly sharded across ranks. default: None\n+                If not provided, ``stride`` will be computed assuming the given distributed\n+                tensor is evenly sharded across ranks. default: None\n+        .. note:: When ``run_check=False``, it is the user's responsibility to ensure the\n+            local tensor passed in is correct across ranks. If not, the behavior of the created\n+            DTensor is undefined.\n+\n+        .. note:: ``from_local`` is differentiable, the `requires_grad` of the created\n",
            "whole_hunk": "@@ -8,7 +8,10 @@ import torch\n import torch.distributed._tensor._dispatch as op_dispatch\n import torch.distributed._tensor.random as random\n import torch.nn as nn\n-from torch.distributed._tensor._collective_utils import mesh_broadcast\n+from torch.distributed._tensor._collective_utils import (\n+    check_tensor_meta,\n+    mesh_broadcast,\n+)\n from torch.distributed._tensor._redistribute import (\n     Redistribute,\n     redistribute_local_tensor,\n@@ -141,7 +144,10 @@ class _FromTorchTensor(torch.autograd.Function):\n             # simply set the local tensor to an empty tensor\n             input = input.new_empty(0, requires_grad=input.requires_grad)\n         elif run_check:\n-            # TODO: by default check tensor metas across rank\n+            # TODO: support uneven sharding when global shape/stride not passed, by\n+            # building the global TensorMeta during check_tensor_meta\n+            check_shape_stride = not shape and not stride\n+            check_tensor_meta(input, check_shape_stride=check_shape_stride)\n             # TODO: See if we need to make this run_check logic\n             # have a corresponding backward.\n             for idx, placement in enumerate(placements):\n@@ -318,13 +324,13 @@ class DTensor(torch.Tensor):  # pyre-ignore[13]: pyre is bad at __new__\n         device_mesh: Optional[DeviceMesh] = None,\n         placements: Optional[Sequence[Placement]] = None,\n         *,\n-        run_check: bool = True,\n+        run_check: bool = False,\n         shape: Optional[torch.Size] = None,\n         stride: Optional[Tuple[int, ...]] = None,\n     ) -> \"DTensor\":\n         \"\"\"\n         Create a :class:`DTensor` from a local torch.Tensor on each rank\n-        according to the `device_mesh` and `placements` specified.\n+        according to the ``device_mesh`` and ``placements`` specified.\n \n         Args:\n             local_tensor (torch.Tensor): local torch.Tensor on each rank.\n@@ -333,28 +339,31 @@ class DTensor(torch.Tensor):  # pyre-ignore[13]: pyre is bad at __new__\n                 context manager, default: None\n             placements (List[:class:`Placement`], optional): the placements that\n                 describes how to place the local torch.Tensor on DeviceMesh, must\n-                have the same number of elements as `device_mesh.ndim`. If not\n-                specified, we will by default replicate the tensor across the\n-                `device_mesh` from the first rank of each dimension of the `device_mesh`.\n+                have the same number of elements as ``device_mesh.ndim``.\n \n         Keyword args:\n-            run_check (bool, optional): indicate whether to run check across ranks\n-                to check meta information and data. if have :class:`Replicate` in\n-                `placements`, the data on first rank of the device mesh dimension\n-                will be broadcasted to other ranks.\n+            run_check (bool, optional): at a cost of extra communications, perform\n+                sanity check across ranks to check each local tensor's meta information\n+                to ensure correctness. If have :class:`Replicate` in ``placements``, the\n+                data on first rank of the device mesh dimension will be broadcasted\n+                to other ranks. default: False\n             shape (torch.Size, optional): A List of int which specifies the size of\n                 DTensor which build on top of `local_tensor`. Note this needs to be\n-                provided if the shape of `local_tensor` are different across the ranks.\n-                If not provided, `shape` will be computed assuming the given distributed\n-                tensor is evenly sharded across ranks.\n+                provided if the shape of ``local_tensor`` are different across the ranks.\n+                If not provided, ``shape`` will be computed assuming the given distributed\n+                tensor is evenly sharded across ranks. default: None\n             stride (tuple, optional): A List of int which specifies the stride of DTensor.\n-                If not provided, `stride` will be computed assuming the given distributed\n-                tensor is evenly sharded across ranks.\n+                If not provided, ``stride`` will be computed assuming the given distributed\n+                tensor is evenly sharded across ranks. default: None\n \n         Returns:\n             A :class:`DTensor` object\n \n-        .. note:: `from_local` is differentiable, the `requires_grad` of the created\n+        .. note:: When ``run_check=False``, it is the user's responsibility to ensure the\n+            local tensor passed in is correct across ranks. If not, the behavior of the created\n+            DTensor is undefined.\n+\n+        .. note:: ``from_local`` is differentiable, the `requires_grad` of the created\n             `DTensor` object will depend on if `local_tensor` requires_grad or not.\n         \"\"\"\n         # if same shape/dtype, no need to run_check, if not, must allgather"
        }
    ]
},
{
    "Id": 334,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/38827141681397564ff4ef0163dc1f0d17cd0708",
    "date": "2024-01-18T01:33:55+00:00",
    "message": "Fix check-labels.yml for ghstack PRs (#117680)\n\nOtherwise check-labels doesn't run on ghstack PRs, see https://github.com/pytorch/pytorch/pull/117609 for example: no Check Labels workflow run.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/117680\nApproved by: https://github.com/izaitsevfb",
    "label": "YES",
    "changes": [
        {
            "name": "check-labels.yml",
            "path": ".github/workflows/check-labels.yml",
            "patches": [
                {
                    "old_start": 9,
                    "old_length": 13,
                    "new_start": 9,
                    "new_length": 14,
                    "hunk": "@@ -9,13 +9,14 @@ on:\n   pull_request_target:\n     types: [opened, synchronize, reopened, labeled, unlabeled]\n     branches: [main]\n-    paths-ignore: [.github]\n \n-  # To allow testing PRs that change workflows.\n-  # May be triggered together with pull_request_target, it's OK.\n+  # To check labels on ghstack PRs.\n+  # Note: as pull_request doesn't trigger on PRs targeting main,\n+  # to test changes to the workflow itself one needs to create\n+  # a PR that targets a gh/**/base branch.\n   pull_request:\n     types: [opened, synchronize, reopened, labeled, unlabeled]\n-    paths: [.github]\n+    branches: [gh/**/base]\n \n   workflow_dispatch:\n \n"
                },
                {
                    "old_start": 26,
                    "old_length": 6,
                    "new_start": 27,
                    "new_length": 7,
                    "hunk": "@@ -26,6 +27,7 @@ concurrency:\n jobs:\n   check-labels:\n     name: Check labels\n+    if: github.repository_owner == 'pytorch'\n     runs-on: linux.20_04.4x\n     steps:\n       - name: Checkout PyTorch"
                }
            ],
            "whole_deleted": "-    paths-ignore: [.github]\n-  # To allow testing PRs that change workflows.\n-  # May be triggered together with pull_request_target, it's OK.\n-    paths: [.github]\n",
            "whole_added": "+  # To check labels on ghstack PRs.\n+  # Note: as pull_request doesn't trigger on PRs targeting main,\n+  # to test changes to the workflow itself one needs to create\n+  # a PR that targets a gh/**/base branch.\n+    branches: [gh/**/base]\n+    if: github.repository_owner == 'pytorch'\n",
            "whole_hunk": "@@ -9,13 +9,14 @@ on:\n   pull_request_target:\n     types: [opened, synchronize, reopened, labeled, unlabeled]\n     branches: [main]\n-    paths-ignore: [.github]\n \n-  # To allow testing PRs that change workflows.\n-  # May be triggered together with pull_request_target, it's OK.\n+  # To check labels on ghstack PRs.\n+  # Note: as pull_request doesn't trigger on PRs targeting main,\n+  # to test changes to the workflow itself one needs to create\n+  # a PR that targets a gh/**/base branch.\n   pull_request:\n     types: [opened, synchronize, reopened, labeled, unlabeled]\n-    paths: [.github]\n+    branches: [gh/**/base]\n \n   workflow_dispatch:\n \n@@ -26,6 +27,7 @@ concurrency:\n jobs:\n   check-labels:\n     name: Check labels\n+    if: github.repository_owner == 'pytorch'\n     runs-on: linux.20_04.4x\n     steps:\n       - name: Checkout PyTorch"
        }
    ]
},
{
    "Id": 39,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/2f219f7d79357a82c1caf2d4a8df540e97b56bce",
    "date": "2024-07-06T16:03:01+00:00",
    "message": "Enforce unused-{variable/function} checks to all torch targets (#130189)\n\nFixes #ISSUE_NUMBER\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130189\nApproved by: https://github.com/ezyang",
    "label": "NO",
    "changes": [
        {
            "name": "utils.h",
            "path": "aten/src/ATen/native/quantized/cudnn/utils.h",
            "patches": [
                {
                    "old_start": 195,
                    "old_length": 20,
                    "new_start": 195,
                    "new_length": 19,
                    "hunk": "@@ -195,20 +195,19 @@ struct PackedConvWeightCudnn : public ConvPackedParamsBase<kSpatialDim> {\n };\n \n namespace cudnn_utils {\n-namespace {\n \n // TODO: we can remove this function when cuDNN enables pass by value support for\n // pointwise multiplication operations. the only reason why we need this right now is\n // we use broadcasting scalar multiplication in conv, linear, and add ops, and cuDNN requires\n // the scalar to be a scalar tensor with the same number of dimensions (num_dim) as the tensor we're multiplying to\n-at::Tensor getRequantMultiplierTensor(double requant_multiplier, uint8_t num_dim) {\n+inline at::Tensor getRequantMultiplierTensor(double requant_multiplier, uint8_t num_dim) {\n   at::SmallVector<int64_t, 4> requantize_multiplier_tensor_size(num_dim, 1);\n   at::Tensor requantize_multiplier_tensor = at::empty(requantize_multiplier_tensor_size, at::device(at::kCUDA).dtype(at::kFloat));\n   requantize_multiplier_tensor.fill_(requant_multiplier);\n   return requantize_multiplier_tensor;\n }\n \n-uint8_t getAlignment(const at::Tensor &t) {\n+inline uint8_t getAlignment(const at::Tensor &t) {\n   // alignment are in bytes\n   uint8_t alignment = 1;\n   uintptr_t address = reinterpret_cast<uintptr_t>(t.data_ptr());\n"
                },
                {
                    "old_start": 225,
                    "old_length": 7,
                    "new_start": 224,
                    "new_length": 7,
                    "hunk": "@@ -225,7 +224,7 @@ uint8_t getAlignment(const at::Tensor &t) {\n // backend cudnn will no longer directly save to the tensor, allowing us to omit this tensor from the variant pack.\n // See third_party/cudnn_frontend/samples/fusion_sample.cpp for other examples\n \n-cudnn_frontend::Tensor getTensorDescriptor(const at::Tensor &t, int64_t id, uint8_t alignment, bool is_virtual = false) {\n+inline cudnn_frontend::Tensor getTensorDescriptor(const at::Tensor &t, int64_t id, uint8_t alignment, bool is_virtual = false) {\n   auto shape = t.sizes();\n   auto strides = t.strides();\n   if (is_virtual) {\n"
                },
                {
                    "old_start": 247,
                    "old_length": 7,
                    "new_start": 246,
                    "new_length": 7,
                    "hunk": "@@ -247,7 +246,7 @@ cudnn_frontend::Tensor getTensorDescriptor(const at::Tensor &t, int64_t id, uint\n     .build();\n }\n \n-cudnn_frontend::Tensor getTensorDescriptor(const c10::IntArrayRef& shape, const c10::IntArrayRef& strides, cudnnDataType_t cudnn_dtype, int64_t id, uint8_t alignment, bool is_virtual = false) {\n+inline cudnn_frontend::Tensor getTensorDescriptor(const c10::IntArrayRef& shape, const c10::IntArrayRef& strides, cudnnDataType_t cudnn_dtype, int64_t id, uint8_t alignment, bool is_virtual = false) {\n   if (is_virtual) {\n     return cudnn_frontend::TensorBuilder()\n       .setDim(shape.size(), shape.data())\n"
                },
                {
                    "old_start": 269,
                    "old_length": 7,
                    "new_start": 268,
                    "new_length": 7,
                    "hunk": "@@ -269,7 +268,7 @@ cudnn_frontend::Tensor getTensorDescriptor(const c10::IntArrayRef& shape, const\n \n // TODO: there is a table from input dtype to operator dtype, we can derive\n // the operator dtype based on input dtype\n-cudnn_frontend::PointWiseDesc_v8 getPointWiseMulDescriptor(cudnnDataType_t dataType) {\n+inline cudnn_frontend::PointWiseDesc_v8 getPointWiseMulDescriptor(cudnnDataType_t dataType) {\n   return cudnn_frontend::PointWiseDescBuilder()\n     .setMode(cudnnPointwiseMode_t::CUDNN_POINTWISE_MUL)\n     .setMathPrecision(dataType)\n"
                },
                {
                    "old_start": 278,
                    "old_length": 7,
                    "new_start": 277,
                    "new_length": 7,
                    "hunk": "@@ -278,7 +277,7 @@ cudnn_frontend::PointWiseDesc_v8 getPointWiseMulDescriptor(cudnnDataType_t dataT\n \n // TODO: there is a table from input dtype to operator dtype, we can derive\n // the operator dtype based on input dtype\n-cudnn_frontend::PointWiseDesc_v8 getPointWiseAddDescriptor(cudnnDataType_t dataType) {\n+inline cudnn_frontend::PointWiseDesc_v8 getPointWiseAddDescriptor(cudnnDataType_t dataType) {\n   return cudnn_frontend::PointWiseDescBuilder()\n     .setMode(cudnnPointwiseMode_t::CUDNN_POINTWISE_ADD)\n     .setMathPrecision(dataType)\n"
                },
                {
                    "old_start": 287,
                    "old_length": 7,
                    "new_start": 286,
                    "new_length": 7,
                    "hunk": "@@ -287,7 +286,7 @@ cudnn_frontend::PointWiseDesc_v8 getPointWiseAddDescriptor(cudnnDataType_t dataT\n \n // TODO: there is a table from input dtype to operator dtype, we can derive\n // the operator dtype based on input dtype\n-cudnn_frontend::PointWiseDesc_v8 getPointWiseReluDescriptor(cudnnDataType_t dataType) {\n+inline cudnn_frontend::PointWiseDesc_v8 getPointWiseReluDescriptor(cudnnDataType_t dataType) {\n   return cudnn_frontend::PointWiseDescBuilder()\n     .setMode(cudnnPointwiseMode_t::CUDNN_POINTWISE_RELU_FWD)\n     .setMathPrecision(dataType)\n"
                },
                {
                    "old_start": 295,
                    "old_length": 7,
                    "new_start": 294,
                    "new_length": 7,
                    "hunk": "@@ -295,7 +294,7 @@ cudnn_frontend::PointWiseDesc_v8 getPointWiseReluDescriptor(cudnnDataType_t data\n }\n \n \n-void filterEngineConfigs(\n+inline void filterEngineConfigs(\n   cudnn_frontend::EngineConfigList &from,\n   cudnn_frontend::EngineConfigList &to,\n   bool deterministic, bool allow_tf32, c10::ScalarType scalar_type)\n"
                },
                {
                    "old_start": 313,
                    "old_length": 41,
                    "new_start": 312,
                    "new_length": 6,
                    "hunk": "@@ -313,41 +312,6 @@ void filterEngineConfigs(\n   cudnn_frontend::filter(from, to, filter);\n }\n \n-\n-cudnn_frontend::ExecutionPlan get_execplan_from_heuristics_else_fall_back(cudnn_frontend::OperationGraph&& opGraph, cudnnHandle_t handle_) {\n-  auto heuristics = cudnn_frontend::EngineHeuristicsBuilder()\n-    .setOperationGraph(opGraph)\n-    .setHeurMode(CUDNN_HEUR_MODE_INSTANT)\n-    .build();\n-\n-  // std::cout << \"Heuristic has \" << heuristics.getEngineConfigCount() << \" configurations \" << std::endl;\n-  auto& engine_config = heuristics.getEngineConfig(heuristics.getEngineConfigCount());\n-\n-  // Try engine configs returned by the heuristics and pick up the first one that works.\n-  for (auto& ecfg : engine_config) {\n-    try {\n-      auto plan = cudnn_frontend::ExecutionPlanBuilder()\n-        .setHandle(handle_)\n-        .setEngineConfig(ecfg, opGraph.getTag())\n-        .build();\n-      return plan;\n-    } catch (cudnn_frontend::cudnnException& e) {\n-      continue;\n-    }\n-  }\n-\n-  {\n-    // std::cout << opGraph.describe() << \" has \" << total_engines << \" engines.\" << std::endl;\n-    auto engine = cudnn_frontend::EngineBuilder().setGlobalEngineIdx(0).setOperationGraph(opGraph).build();\n-    // std::cout << engine.describe() << std::endl;\n-\n-    auto engine_config = cudnn_frontend::EngineConfigBuilder().setEngine(engine).build();\n-    // std::cout << engine_config.describe() << std::endl;\n-\n-    return cudnn_frontend::ExecutionPlanBuilder().setHandle(handle_).setEngineConfig(engine_config).build();\n-  }\n-}\n-} // anonymous\n } // cudnn_utils\n \n #endif  // AT_CUDNN_ENABLED\n"
                }
            ],
            "whole_deleted": "-namespace {\n-at::Tensor getRequantMultiplierTensor(double requant_multiplier, uint8_t num_dim) {\n-uint8_t getAlignment(const at::Tensor &t) {\n-cudnn_frontend::Tensor getTensorDescriptor(const at::Tensor &t, int64_t id, uint8_t alignment, bool is_virtual = false) {\n-cudnn_frontend::Tensor getTensorDescriptor(const c10::IntArrayRef& shape, const c10::IntArrayRef& strides, cudnnDataType_t cudnn_dtype, int64_t id, uint8_t alignment, bool is_virtual = false) {\n-cudnn_frontend::PointWiseDesc_v8 getPointWiseMulDescriptor(cudnnDataType_t dataType) {\n-cudnn_frontend::PointWiseDesc_v8 getPointWiseAddDescriptor(cudnnDataType_t dataType) {\n-cudnn_frontend::PointWiseDesc_v8 getPointWiseReluDescriptor(cudnnDataType_t dataType) {\n-void filterEngineConfigs(\n-\n-cudnn_frontend::ExecutionPlan get_execplan_from_heuristics_else_fall_back(cudnn_frontend::OperationGraph&& opGraph, cudnnHandle_t handle_) {\n-  auto heuristics = cudnn_frontend::EngineHeuristicsBuilder()\n-    .setOperationGraph(opGraph)\n-    .setHeurMode(CUDNN_HEUR_MODE_INSTANT)\n-    .build();\n-\n-  // std::cout << \"Heuristic has \" << heuristics.getEngineConfigCount() << \" configurations \" << std::endl;\n-  auto& engine_config = heuristics.getEngineConfig(heuristics.getEngineConfigCount());\n-\n-  // Try engine configs returned by the heuristics and pick up the first one that works.\n-  for (auto& ecfg : engine_config) {\n-    try {\n-      auto plan = cudnn_frontend::ExecutionPlanBuilder()\n-        .setHandle(handle_)\n-        .setEngineConfig(ecfg, opGraph.getTag())\n-        .build();\n-      return plan;\n-    } catch (cudnn_frontend::cudnnException& e) {\n-      continue;\n-    }\n-  }\n-\n-  {\n-    // std::cout << opGraph.describe() << \" has \" << total_engines << \" engines.\" << std::endl;\n-    auto engine = cudnn_frontend::EngineBuilder().setGlobalEngineIdx(0).setOperationGraph(opGraph).build();\n-    // std::cout << engine.describe() << std::endl;\n-\n-    auto engine_config = cudnn_frontend::EngineConfigBuilder().setEngine(engine).build();\n-    // std::cout << engine_config.describe() << std::endl;\n-\n-    return cudnn_frontend::ExecutionPlanBuilder().setHandle(handle_).setEngineConfig(engine_config).build();\n-  }\n-}\n-} // anonymous\n",
            "whole_added": "+inline at::Tensor getRequantMultiplierTensor(double requant_multiplier, uint8_t num_dim) {\n+inline uint8_t getAlignment(const at::Tensor &t) {\n+inline cudnn_frontend::Tensor getTensorDescriptor(const at::Tensor &t, int64_t id, uint8_t alignment, bool is_virtual = false) {\n+inline cudnn_frontend::Tensor getTensorDescriptor(const c10::IntArrayRef& shape, const c10::IntArrayRef& strides, cudnnDataType_t cudnn_dtype, int64_t id, uint8_t alignment, bool is_virtual = false) {\n+inline cudnn_frontend::PointWiseDesc_v8 getPointWiseMulDescriptor(cudnnDataType_t dataType) {\n+inline cudnn_frontend::PointWiseDesc_v8 getPointWiseAddDescriptor(cudnnDataType_t dataType) {\n+inline cudnn_frontend::PointWiseDesc_v8 getPointWiseReluDescriptor(cudnnDataType_t dataType) {\n+inline void filterEngineConfigs(\n",
            "whole_hunk": "@@ -195,20 +195,19 @@ struct PackedConvWeightCudnn : public ConvPackedParamsBase<kSpatialDim> {\n };\n \n namespace cudnn_utils {\n-namespace {\n \n // TODO: we can remove this function when cuDNN enables pass by value support for\n // pointwise multiplication operations. the only reason why we need this right now is\n // we use broadcasting scalar multiplication in conv, linear, and add ops, and cuDNN requires\n // the scalar to be a scalar tensor with the same number of dimensions (num_dim) as the tensor we're multiplying to\n-at::Tensor getRequantMultiplierTensor(double requant_multiplier, uint8_t num_dim) {\n+inline at::Tensor getRequantMultiplierTensor(double requant_multiplier, uint8_t num_dim) {\n   at::SmallVector<int64_t, 4> requantize_multiplier_tensor_size(num_dim, 1);\n   at::Tensor requantize_multiplier_tensor = at::empty(requantize_multiplier_tensor_size, at::device(at::kCUDA).dtype(at::kFloat));\n   requantize_multiplier_tensor.fill_(requant_multiplier);\n   return requantize_multiplier_tensor;\n }\n \n-uint8_t getAlignment(const at::Tensor &t) {\n+inline uint8_t getAlignment(const at::Tensor &t) {\n   // alignment are in bytes\n   uint8_t alignment = 1;\n   uintptr_t address = reinterpret_cast<uintptr_t>(t.data_ptr());\n@@ -225,7 +224,7 @@ uint8_t getAlignment(const at::Tensor &t) {\n // backend cudnn will no longer directly save to the tensor, allowing us to omit this tensor from the variant pack.\n // See third_party/cudnn_frontend/samples/fusion_sample.cpp for other examples\n \n-cudnn_frontend::Tensor getTensorDescriptor(const at::Tensor &t, int64_t id, uint8_t alignment, bool is_virtual = false) {\n+inline cudnn_frontend::Tensor getTensorDescriptor(const at::Tensor &t, int64_t id, uint8_t alignment, bool is_virtual = false) {\n   auto shape = t.sizes();\n   auto strides = t.strides();\n   if (is_virtual) {\n@@ -247,7 +246,7 @@ cudnn_frontend::Tensor getTensorDescriptor(const at::Tensor &t, int64_t id, uint\n     .build();\n }\n \n-cudnn_frontend::Tensor getTensorDescriptor(const c10::IntArrayRef& shape, const c10::IntArrayRef& strides, cudnnDataType_t cudnn_dtype, int64_t id, uint8_t alignment, bool is_virtual = false) {\n+inline cudnn_frontend::Tensor getTensorDescriptor(const c10::IntArrayRef& shape, const c10::IntArrayRef& strides, cudnnDataType_t cudnn_dtype, int64_t id, uint8_t alignment, bool is_virtual = false) {\n   if (is_virtual) {\n     return cudnn_frontend::TensorBuilder()\n       .setDim(shape.size(), shape.data())\n@@ -269,7 +268,7 @@ cudnn_frontend::Tensor getTensorDescriptor(const c10::IntArrayRef& shape, const\n \n // TODO: there is a table from input dtype to operator dtype, we can derive\n // the operator dtype based on input dtype\n-cudnn_frontend::PointWiseDesc_v8 getPointWiseMulDescriptor(cudnnDataType_t dataType) {\n+inline cudnn_frontend::PointWiseDesc_v8 getPointWiseMulDescriptor(cudnnDataType_t dataType) {\n   return cudnn_frontend::PointWiseDescBuilder()\n     .setMode(cudnnPointwiseMode_t::CUDNN_POINTWISE_MUL)\n     .setMathPrecision(dataType)\n@@ -278,7 +277,7 @@ cudnn_frontend::PointWiseDesc_v8 getPointWiseMulDescriptor(cudnnDataType_t dataT\n \n // TODO: there is a table from input dtype to operator dtype, we can derive\n // the operator dtype based on input dtype\n-cudnn_frontend::PointWiseDesc_v8 getPointWiseAddDescriptor(cudnnDataType_t dataType) {\n+inline cudnn_frontend::PointWiseDesc_v8 getPointWiseAddDescriptor(cudnnDataType_t dataType) {\n   return cudnn_frontend::PointWiseDescBuilder()\n     .setMode(cudnnPointwiseMode_t::CUDNN_POINTWISE_ADD)\n     .setMathPrecision(dataType)\n@@ -287,7 +286,7 @@ cudnn_frontend::PointWiseDesc_v8 getPointWiseAddDescriptor(cudnnDataType_t dataT\n \n // TODO: there is a table from input dtype to operator dtype, we can derive\n // the operator dtype based on input dtype\n-cudnn_frontend::PointWiseDesc_v8 getPointWiseReluDescriptor(cudnnDataType_t dataType) {\n+inline cudnn_frontend::PointWiseDesc_v8 getPointWiseReluDescriptor(cudnnDataType_t dataType) {\n   return cudnn_frontend::PointWiseDescBuilder()\n     .setMode(cudnnPointwiseMode_t::CUDNN_POINTWISE_RELU_FWD)\n     .setMathPrecision(dataType)\n@@ -295,7 +294,7 @@ cudnn_frontend::PointWiseDesc_v8 getPointWiseReluDescriptor(cudnnDataType_t data\n }\n \n \n-void filterEngineConfigs(\n+inline void filterEngineConfigs(\n   cudnn_frontend::EngineConfigList &from,\n   cudnn_frontend::EngineConfigList &to,\n   bool deterministic, bool allow_tf32, c10::ScalarType scalar_type)\n@@ -313,41 +312,6 @@ void filterEngineConfigs(\n   cudnn_frontend::filter(from, to, filter);\n }\n \n-\n-cudnn_frontend::ExecutionPlan get_execplan_from_heuristics_else_fall_back(cudnn_frontend::OperationGraph&& opGraph, cudnnHandle_t handle_) {\n-  auto heuristics = cudnn_frontend::EngineHeuristicsBuilder()\n-    .setOperationGraph(opGraph)\n-    .setHeurMode(CUDNN_HEUR_MODE_INSTANT)\n-    .build();\n-\n-  // std::cout << \"Heuristic has \" << heuristics.getEngineConfigCount() << \" configurations \" << std::endl;\n-  auto& engine_config = heuristics.getEngineConfig(heuristics.getEngineConfigCount());\n-\n-  // Try engine configs returned by the heuristics and pick up the first one that works.\n-  for (auto& ecfg : engine_config) {\n-    try {\n-      auto plan = cudnn_frontend::ExecutionPlanBuilder()\n-        .setHandle(handle_)\n-        .setEngineConfig(ecfg, opGraph.getTag())\n-        .build();\n-      return plan;\n-    } catch (cudnn_frontend::cudnnException& e) {\n-      continue;\n-    }\n-  }\n-\n-  {\n-    // std::cout << opGraph.describe() << \" has \" << total_engines << \" engines.\" << std::endl;\n-    auto engine = cudnn_frontend::EngineBuilder().setGlobalEngineIdx(0).setOperationGraph(opGraph).build();\n-    // std::cout << engine.describe() << std::endl;\n-\n-    auto engine_config = cudnn_frontend::EngineConfigBuilder().setEngine(engine).build();\n-    // std::cout << engine_config.describe() << std::endl;\n-\n-    return cudnn_frontend::ExecutionPlanBuilder().setHandle(handle_).setEngineConfig(engine_config).build();\n-  }\n-}\n-} // anonymous\n } // cudnn_utils\n \n #endif  // AT_CUDNN_ENABLED\n"
        },
        {
            "name": "utils.cmake",
            "path": "cmake/public/utils.cmake",
            "patches": [
                {
                    "old_start": 405,
                    "old_length": 12,
                    "new_start": 405,
                    "new_length": 6,
                    "hunk": "@@ -405,12 +405,6 @@ endmacro()\n #   torch_compile_options(lib_name)\n function(torch_compile_options libname)\n   set_property(TARGET ${libname} PROPERTY CXX_STANDARD 17)\n-  set(private_compile_options \"\")\n-\n-  # ---[ Check if warnings should be errors.\n-  if(WERROR)\n-    list(APPEND private_compile_options -Werror)\n-  endif()\n \n   # until they can be unified, keep these lists synced with setup.py\n   if(MSVC)\n"
                },
                {
                    "old_start": 429,
                    "old_length": 7,
                    "new_start": 423,
                    "new_length": 7,
                    "hunk": "@@ -429,7 +423,7 @@ function(torch_compile_options libname)\n         /bigobj>\n       )\n   else()\n-    list(APPEND private_compile_options\n+    set(private_compile_options\n       -Wall\n       -Wextra\n       -Wdeprecated\n"
                },
                {
                    "old_start": 441,
                    "old_length": 17,
                    "new_start": 435,
                    "new_length": 13,
                    "hunk": "@@ -441,17 +435,13 @@ function(torch_compile_options libname)\n       -Wno-strict-overflow\n       -Wno-strict-aliasing\n       )\n-    if(\"${libname}\" STREQUAL \"torch_cpu\")\n-      list(APPEND private_compile_options -Wunused-function)\n-      list(APPEND private_compile_options -Wunused-variable)\n-      if(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n-        list(APPEND private_compile_options -Wunused-but-set-variable)\n-      endif()\n-      if(\"${CMAKE_CXX_COMPILER_ID}\" MATCHES \"Clang\")\n-        list(APPEND private_compile_options -Wunused-private-field)\n-      endif()\n-    else()\n-      list(APPEND private_compile_options -Wno-unused-function)\n+    list(APPEND private_compile_options -Wunused-function)\n+    list(APPEND private_compile_options -Wunused-variable)\n+    if(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n+      list(APPEND private_compile_options -Wunused-but-set-variable)\n+    endif()\n+    if(\"${CMAKE_CXX_COMPILER_ID}\" MATCHES \"Clang\")\n+      list(APPEND private_compile_options -Wunused-private-field)\n     endif()\n     if(NOT \"${CMAKE_CXX_COMPILER_ID}\" MATCHES \"Clang\")\n       list(APPEND private_compile_options\n"
                },
                {
                    "old_start": 462,
                    "old_length": 6,
                    "new_start": 452,
                    "new_length": 7,
                    "hunk": "@@ -462,6 +452,7 @@ function(torch_compile_options libname)\n \n     if(WERROR)\n       list(APPEND private_compile_options\n+        -Werror\n         -Wno-strict-overflow\n         -Werror=inconsistent-missing-override\n         -Werror=inconsistent-missing-destructor-override"
                }
            ],
            "whole_deleted": "-  set(private_compile_options \"\")\n-\n-  # ---[ Check if warnings should be errors.\n-  if(WERROR)\n-    list(APPEND private_compile_options -Werror)\n-  endif()\n-    list(APPEND private_compile_options\n-    if(\"${libname}\" STREQUAL \"torch_cpu\")\n-      list(APPEND private_compile_options -Wunused-function)\n-      list(APPEND private_compile_options -Wunused-variable)\n-      if(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n-        list(APPEND private_compile_options -Wunused-but-set-variable)\n-      endif()\n-      if(\"${CMAKE_CXX_COMPILER_ID}\" MATCHES \"Clang\")\n-        list(APPEND private_compile_options -Wunused-private-field)\n-      endif()\n-    else()\n-      list(APPEND private_compile_options -Wno-unused-function)\n",
            "whole_added": "+    set(private_compile_options\n+    list(APPEND private_compile_options -Wunused-function)\n+    list(APPEND private_compile_options -Wunused-variable)\n+    if(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n+      list(APPEND private_compile_options -Wunused-but-set-variable)\n+    endif()\n+    if(\"${CMAKE_CXX_COMPILER_ID}\" MATCHES \"Clang\")\n+      list(APPEND private_compile_options -Wunused-private-field)\n+        -Werror\n",
            "whole_hunk": "@@ -405,12 +405,6 @@ endmacro()\n #   torch_compile_options(lib_name)\n function(torch_compile_options libname)\n   set_property(TARGET ${libname} PROPERTY CXX_STANDARD 17)\n-  set(private_compile_options \"\")\n-\n-  # ---[ Check if warnings should be errors.\n-  if(WERROR)\n-    list(APPEND private_compile_options -Werror)\n-  endif()\n \n   # until they can be unified, keep these lists synced with setup.py\n   if(MSVC)\n@@ -429,7 +423,7 @@ function(torch_compile_options libname)\n         /bigobj>\n       )\n   else()\n-    list(APPEND private_compile_options\n+    set(private_compile_options\n       -Wall\n       -Wextra\n       -Wdeprecated\n@@ -441,17 +435,13 @@ function(torch_compile_options libname)\n       -Wno-strict-overflow\n       -Wno-strict-aliasing\n       )\n-    if(\"${libname}\" STREQUAL \"torch_cpu\")\n-      list(APPEND private_compile_options -Wunused-function)\n-      list(APPEND private_compile_options -Wunused-variable)\n-      if(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n-        list(APPEND private_compile_options -Wunused-but-set-variable)\n-      endif()\n-      if(\"${CMAKE_CXX_COMPILER_ID}\" MATCHES \"Clang\")\n-        list(APPEND private_compile_options -Wunused-private-field)\n-      endif()\n-    else()\n-      list(APPEND private_compile_options -Wno-unused-function)\n+    list(APPEND private_compile_options -Wunused-function)\n+    list(APPEND private_compile_options -Wunused-variable)\n+    if(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n+      list(APPEND private_compile_options -Wunused-but-set-variable)\n+    endif()\n+    if(\"${CMAKE_CXX_COMPILER_ID}\" MATCHES \"Clang\")\n+      list(APPEND private_compile_options -Wunused-private-field)\n     endif()\n     if(NOT \"${CMAKE_CXX_COMPILER_ID}\" MATCHES \"Clang\")\n       list(APPEND private_compile_options\n@@ -462,6 +452,7 @@ function(torch_compile_options libname)\n \n     if(WERROR)\n       list(APPEND private_compile_options\n+        -Werror\n         -Wno-strict-overflow\n         -Werror=inconsistent-missing-override\n         -Werror=inconsistent-missing-destructor-override"
        }
    ]
},
{
    "Id": 54,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/5c9d5272e43573269427cae5e38307faac3266f6",
    "date": "2024-07-02T08:45:59+00:00",
    "message": "fixes #124582 (#128483)\n\nadded check for existence of outputs requiring grad to make_graphed_callables.\n\nadded new test case, updated existing test case to include parameterless modules.\n\nFixes #124582\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128483\nApproved by: https://github.com/eqy, https://github.com/ezyang",
    "label": "YES",
    "changes": [
        {
            "name": "test_cuda.py",
            "path": "test/test_cuda.py",
            "patches": [
                {
                    "old_start": 3074,
                    "old_length": 13,
                    "new_start": 3074,
                    "new_length": 25,
                    "hunk": "@@ -3074,13 +3074,25 @@ exit(2)\n                 ).cuda()\n \n             def forward(self, x):\n-                return {\"output\": self.net_2(self.net_1(x))}\n+                return self.net_2(self.net_1(x))\n+\n+        class ParameterlessModule(torch.nn.Module):\n+            def forward(self, x):\n+                idx = (\n+                    torch.arange(x.size(0), device=x.device)\n+                    .view(-1, 1)\n+                    .repeat(1, x.size(1))\n+                )\n+                return {\"output\": torch.gather(x, 0, idx)}\n \n         models = []\n         for _ in range(2):\n             model_section1 = MLP1(D_in, H, H).cuda()\n             model_section2 = MLP2(H, H, D_out).cuda()\n-            models.append(torch.nn.Sequential(model_section1, model_section2))\n+            model_section3 = ParameterlessModule().cuda()\n+            models.append(\n+                torch.nn.Sequential(model_section1, model_section2, model_section3)\n+            )\n \n         model_graphed = models[0]\n         model_control = models[1]\n"
                },
                {
                    "old_start": 3092,
                    "old_length": 6,
                    "new_start": 3104,
                    "new_length": 7,
                    "hunk": "@@ -3092,6 +3104,7 @@ exit(2)\n \n         x = torch.randn(N, D_in, device=\"cuda\")\n         h = torch.randn(N, H, device=\"cuda\", requires_grad=True)\n+        h2 = torch.randn(N, D_out, device=\"cuda\", requires_grad=True)\n         unused_input = torch.randn(N, H, device=\"cuda\", requires_grad=True)\n         y_pred = torch.randn(N, D_out, device=\"cuda\", requires_grad=True)\n         y = torch.randn(N, D_out, device=\"cuda\")\n"
                },
                {
                    "old_start": 3104,
                    "old_length": 13,
                    "new_start": 3117,
                    "new_length": 21,
                    "hunk": "@@ -3104,13 +3117,21 @@ exit(2)\n             (\n                 model_graphed[0],\n                 model_graphed[1],\n+                model_graphed[2],\n                 relu_graphed,\n                 loss_fn_graphed,\n             ) = torch.cuda.make_graphed_callables(\n-                (model_graphed[0], model_graphed[1], relu_control, loss_fn_control),\n+                (\n+                    model_graphed[0],\n+                    model_graphed[1],\n+                    model_graphed[2],\n+                    relu_control,\n+                    loss_fn_control,\n+                ),\n                 (\n                     ({\"x\": x, \"unused_input\": unused_input},),\n                     (h,),\n+                    (h2,),\n                     (y_pred,),\n                     (y_pred, y),\n                 ),\n"
                },
                {
                    "old_start": 3149,
                    "old_length": 6,
                    "new_start": 3170,
                    "new_length": 84,
                    "hunk": "@@ -3149,6 +3170,84 @@ exit(2)\n             model_graphed({\"x\": real_inputs[0]}), model_control({\"x\": real_inputs[0]})\n         )\n \n+    @unittest.skipIf(\n+        not TEST_CUDA_GRAPH, \"CUDA >= 11.0 or ROCM >= 5.3 required for graphs\"\n+    )\n+    @parametrize(\n+        \"with_amp,cache_enabled,allow_unused_input\",\n+        [\n+            subtest((False, False, True), decorators=[skipIfRocm]),\n+            subtest((True, False, True), decorators=[skipIfRocm]),\n+            subtest((True, True, True), decorators=[unittest.expectedFailure]),\n+            subtest((False, False, False), decorators=[skipIfRocm]),\n+        ],\n+        name_fn=lambda x, y, z: \"{}{}{}\".format(\n+            {True: \"with_amp\", False: \"without_amp\"}[x],\n+            {True: \"_cache_enabled\", False: \"_cache_disabled\"}[y] if x else \"\",\n+            {True: \"_allow_unused_input\", False: \"_not_allow_unused_input\"}[z],\n+        ),\n+    )\n+    @serialTest()\n+    def test_graph_make_graphed_callables_parameterless_nograd_module(\n+        self, with_amp, cache_enabled, allow_unused_input\n+    ):\n+        torch.manual_seed(5)\n+        torch.cuda.manual_seed(5)\n+\n+        N, D_in, H, D_out = 640, 4096, 2048, 1024\n+\n+        class ParameterlessModule(torch.nn.Module):\n+            def forward(self, input_dict: dict):\n+                x = input_dict[\"x\"]\n+                idx = (\n+                    torch.arange(x.size(0), device=x.device)\n+                    .view(-1, 1)\n+                    .repeat(1, x.size(1))\n+                )\n+                return {\"output\": torch.gather(x, 0, idx)}\n+\n+        models = []\n+        for _ in range(2):\n+            model_section1 = ParameterlessModule().cuda()\n+            models.append(torch.nn.Sequential(model_section1))\n+\n+        model_graphed = models[0]\n+        model_control = models[1]\n+\n+        model_graphed.load_state_dict(model_control.state_dict())\n+\n+        x = torch.randn(N, D_in, device=\"cuda\", requires_grad=False)\n+        unused_input = torch.randn(N, H, device=\"cuda\", requires_grad=False)\n+        y_pred = torch.randn(N, D_in, device=\"cuda\", requires_grad=False)\n+        y = torch.randn(N, D_in, device=\"cuda\")\n+\n+        # This is a good stress test. It graphs four callables: two Modules and two python functions.\n+        with torch.cuda.amp.autocast(with_amp, cache_enabled=cache_enabled):\n+            model_graphed[0] = torch.cuda.make_graphed_callables(\n+                model_graphed[0],\n+                ({\"x\": x, \"unused_input\": unused_input},),\n+                allow_unused_input=allow_unused_input,\n+            )\n+\n+        real_inputs = [torch.rand_like(x, requires_grad=True) for _ in range(10)]\n+        real_targets = [torch.rand_like(y) for _ in range(10)]\n+\n+        for m in (model_graphed, model_control):\n+            # Resets RNC states before iterations for graphed and ungraphed models,\n+            # so dropout math should be bitwise identical for both.\n+            torch.manual_seed(5)\n+            torch.cuda.manual_seed(5)\n+            for data, target in zip(real_inputs, real_targets):\n+                with torch.cuda.amp.autocast(with_amp, cache_enabled=cache_enabled):\n+                    out = m({\"x\": data, \"unused_input\": unused_input})[\"output\"]\n+\n+        # We graphed the models in training mode. Eval should still run ungraphed.\n+        model_graphed.eval()\n+        model_control.eval()\n+        self.assertEqual(\n+            model_graphed({\"x\": real_inputs[0]}), model_control({\"x\": real_inputs[0]})\n+        )\n+\n     @unittest.skipIf(\n         not TEST_CUDA_GRAPH, \"CUDA >= 11.0 or ROCM >= 5.3 required for graphs\"\n     )\n"
                }
            ],
            "whole_deleted": "-                return {\"output\": self.net_2(self.net_1(x))}\n-            models.append(torch.nn.Sequential(model_section1, model_section2))\n-                (model_graphed[0], model_graphed[1], relu_control, loss_fn_control),\n",
            "whole_added": "+                return self.net_2(self.net_1(x))\n+\n+        class ParameterlessModule(torch.nn.Module):\n+            def forward(self, x):\n+                idx = (\n+                    torch.arange(x.size(0), device=x.device)\n+                    .view(-1, 1)\n+                    .repeat(1, x.size(1))\n+                )\n+                return {\"output\": torch.gather(x, 0, idx)}\n+            model_section3 = ParameterlessModule().cuda()\n+            models.append(\n+                torch.nn.Sequential(model_section1, model_section2, model_section3)\n+            )\n+        h2 = torch.randn(N, D_out, device=\"cuda\", requires_grad=True)\n+                model_graphed[2],\n+                (\n+                    model_graphed[0],\n+                    model_graphed[1],\n+                    model_graphed[2],\n+                    relu_control,\n+                    loss_fn_control,\n+                ),\n+                    (h2,),\n+    @unittest.skipIf(\n+        not TEST_CUDA_GRAPH, \"CUDA >= 11.0 or ROCM >= 5.3 required for graphs\"\n+    )\n+    @parametrize(\n+        \"with_amp,cache_enabled,allow_unused_input\",\n+        [\n+            subtest((False, False, True), decorators=[skipIfRocm]),\n+            subtest((True, False, True), decorators=[skipIfRocm]),\n+            subtest((True, True, True), decorators=[unittest.expectedFailure]),\n+            subtest((False, False, False), decorators=[skipIfRocm]),\n+        ],\n+        name_fn=lambda x, y, z: \"{}{}{}\".format(\n+            {True: \"with_amp\", False: \"without_amp\"}[x],\n+            {True: \"_cache_enabled\", False: \"_cache_disabled\"}[y] if x else \"\",\n+            {True: \"_allow_unused_input\", False: \"_not_allow_unused_input\"}[z],\n+        ),\n+    )\n+    @serialTest()\n+    def test_graph_make_graphed_callables_parameterless_nograd_module(\n+        self, with_amp, cache_enabled, allow_unused_input\n+    ):\n+        torch.manual_seed(5)\n+        torch.cuda.manual_seed(5)\n+\n+        N, D_in, H, D_out = 640, 4096, 2048, 1024\n+\n+        class ParameterlessModule(torch.nn.Module):\n+            def forward(self, input_dict: dict):\n+                x = input_dict[\"x\"]\n+                idx = (\n+                    torch.arange(x.size(0), device=x.device)\n+                    .view(-1, 1)\n+                    .repeat(1, x.size(1))\n+                )\n+                return {\"output\": torch.gather(x, 0, idx)}\n+\n+        models = []\n+        for _ in range(2):\n+            model_section1 = ParameterlessModule().cuda()\n+            models.append(torch.nn.Sequential(model_section1))\n+\n+        model_graphed = models[0]\n+        model_control = models[1]\n+\n+        model_graphed.load_state_dict(model_control.state_dict())\n+\n+        x = torch.randn(N, D_in, device=\"cuda\", requires_grad=False)\n+        unused_input = torch.randn(N, H, device=\"cuda\", requires_grad=False)\n+        y_pred = torch.randn(N, D_in, device=\"cuda\", requires_grad=False)\n+        y = torch.randn(N, D_in, device=\"cuda\")\n+\n+        # This is a good stress test. It graphs four callables: two Modules and two python functions.\n+        with torch.cuda.amp.autocast(with_amp, cache_enabled=cache_enabled):\n+            model_graphed[0] = torch.cuda.make_graphed_callables(\n+                model_graphed[0],\n+                ({\"x\": x, \"unused_input\": unused_input},),\n+                allow_unused_input=allow_unused_input,\n+            )\n+\n+        real_inputs = [torch.rand_like(x, requires_grad=True) for _ in range(10)]\n+        real_targets = [torch.rand_like(y) for _ in range(10)]\n+\n+        for m in (model_graphed, model_control):\n+            # Resets RNC states before iterations for graphed and ungraphed models,\n+            # so dropout math should be bitwise identical for both.\n+            torch.manual_seed(5)\n+            torch.cuda.manual_seed(5)\n+            for data, target in zip(real_inputs, real_targets):\n+                with torch.cuda.amp.autocast(with_amp, cache_enabled=cache_enabled):\n+                    out = m({\"x\": data, \"unused_input\": unused_input})[\"output\"]\n+\n+        # We graphed the models in training mode. Eval should still run ungraphed.\n+        model_graphed.eval()\n+        model_control.eval()\n+        self.assertEqual(\n+            model_graphed({\"x\": real_inputs[0]}), model_control({\"x\": real_inputs[0]})\n+        )\n+\n",
            "whole_hunk": "@@ -3074,13 +3074,25 @@ exit(2)\n                 ).cuda()\n \n             def forward(self, x):\n-                return {\"output\": self.net_2(self.net_1(x))}\n+                return self.net_2(self.net_1(x))\n+\n+        class ParameterlessModule(torch.nn.Module):\n+            def forward(self, x):\n+                idx = (\n+                    torch.arange(x.size(0), device=x.device)\n+                    .view(-1, 1)\n+                    .repeat(1, x.size(1))\n+                )\n+                return {\"output\": torch.gather(x, 0, idx)}\n \n         models = []\n         for _ in range(2):\n             model_section1 = MLP1(D_in, H, H).cuda()\n             model_section2 = MLP2(H, H, D_out).cuda()\n-            models.append(torch.nn.Sequential(model_section1, model_section2))\n+            model_section3 = ParameterlessModule().cuda()\n+            models.append(\n+                torch.nn.Sequential(model_section1, model_section2, model_section3)\n+            )\n \n         model_graphed = models[0]\n         model_control = models[1]\n@@ -3092,6 +3104,7 @@ exit(2)\n \n         x = torch.randn(N, D_in, device=\"cuda\")\n         h = torch.randn(N, H, device=\"cuda\", requires_grad=True)\n+        h2 = torch.randn(N, D_out, device=\"cuda\", requires_grad=True)\n         unused_input = torch.randn(N, H, device=\"cuda\", requires_grad=True)\n         y_pred = torch.randn(N, D_out, device=\"cuda\", requires_grad=True)\n         y = torch.randn(N, D_out, device=\"cuda\")\n@@ -3104,13 +3117,21 @@ exit(2)\n             (\n                 model_graphed[0],\n                 model_graphed[1],\n+                model_graphed[2],\n                 relu_graphed,\n                 loss_fn_graphed,\n             ) = torch.cuda.make_graphed_callables(\n-                (model_graphed[0], model_graphed[1], relu_control, loss_fn_control),\n+                (\n+                    model_graphed[0],\n+                    model_graphed[1],\n+                    model_graphed[2],\n+                    relu_control,\n+                    loss_fn_control,\n+                ),\n                 (\n                     ({\"x\": x, \"unused_input\": unused_input},),\n                     (h,),\n+                    (h2,),\n                     (y_pred,),\n                     (y_pred, y),\n                 ),\n@@ -3149,6 +3170,84 @@ exit(2)\n             model_graphed({\"x\": real_inputs[0]}), model_control({\"x\": real_inputs[0]})\n         )\n \n+    @unittest.skipIf(\n+        not TEST_CUDA_GRAPH, \"CUDA >= 11.0 or ROCM >= 5.3 required for graphs\"\n+    )\n+    @parametrize(\n+        \"with_amp,cache_enabled,allow_unused_input\",\n+        [\n+            subtest((False, False, True), decorators=[skipIfRocm]),\n+            subtest((True, False, True), decorators=[skipIfRocm]),\n+            subtest((True, True, True), decorators=[unittest.expectedFailure]),\n+            subtest((False, False, False), decorators=[skipIfRocm]),\n+        ],\n+        name_fn=lambda x, y, z: \"{}{}{}\".format(\n+            {True: \"with_amp\", False: \"without_amp\"}[x],\n+            {True: \"_cache_enabled\", False: \"_cache_disabled\"}[y] if x else \"\",\n+            {True: \"_allow_unused_input\", False: \"_not_allow_unused_input\"}[z],\n+        ),\n+    )\n+    @serialTest()\n+    def test_graph_make_graphed_callables_parameterless_nograd_module(\n+        self, with_amp, cache_enabled, allow_unused_input\n+    ):\n+        torch.manual_seed(5)\n+        torch.cuda.manual_seed(5)\n+\n+        N, D_in, H, D_out = 640, 4096, 2048, 1024\n+\n+        class ParameterlessModule(torch.nn.Module):\n+            def forward(self, input_dict: dict):\n+                x = input_dict[\"x\"]\n+                idx = (\n+                    torch.arange(x.size(0), device=x.device)\n+                    .view(-1, 1)\n+                    .repeat(1, x.size(1))\n+                )\n+                return {\"output\": torch.gather(x, 0, idx)}\n+\n+        models = []\n+        for _ in range(2):\n+            model_section1 = ParameterlessModule().cuda()\n+            models.append(torch.nn.Sequential(model_section1))\n+\n+        model_graphed = models[0]\n+        model_control = models[1]\n+\n+        model_graphed.load_state_dict(model_control.state_dict())\n+\n+        x = torch.randn(N, D_in, device=\"cuda\", requires_grad=False)\n+        unused_input = torch.randn(N, H, device=\"cuda\", requires_grad=False)\n+        y_pred = torch.randn(N, D_in, device=\"cuda\", requires_grad=False)\n+        y = torch.randn(N, D_in, device=\"cuda\")\n+\n+        # This is a good stress test. It graphs four callables: two Modules and two python functions.\n+        with torch.cuda.amp.autocast(with_amp, cache_enabled=cache_enabled):\n+            model_graphed[0] = torch.cuda.make_graphed_callables(\n+                model_graphed[0],\n+                ({\"x\": x, \"unused_input\": unused_input},),\n+                allow_unused_input=allow_unused_input,\n+            )\n+\n+        real_inputs = [torch.rand_like(x, requires_grad=True) for _ in range(10)]\n+        real_targets = [torch.rand_like(y) for _ in range(10)]\n+\n+        for m in (model_graphed, model_control):\n+            # Resets RNC states before iterations for graphed and ungraphed models,\n+            # so dropout math should be bitwise identical for both.\n+            torch.manual_seed(5)\n+            torch.cuda.manual_seed(5)\n+            for data, target in zip(real_inputs, real_targets):\n+                with torch.cuda.amp.autocast(with_amp, cache_enabled=cache_enabled):\n+                    out = m({\"x\": data, \"unused_input\": unused_input})[\"output\"]\n+\n+        # We graphed the models in training mode. Eval should still run ungraphed.\n+        model_graphed.eval()\n+        model_control.eval()\n+        self.assertEqual(\n+            model_graphed({\"x\": real_inputs[0]}), model_control({\"x\": real_inputs[0]})\n+        )\n+\n     @unittest.skipIf(\n         not TEST_CUDA_GRAPH, \"CUDA >= 11.0 or ROCM >= 5.3 required for graphs\"\n     )\n"
        },
        {
            "name": "graphs.py",
            "path": "torch/cuda/graphs.py",
            "patches": [
                {
                    "old_start": 318,
                    "old_length": 18,
                    "new_start": 318,
                    "new_length": 25,
                    "hunk": "@@ -318,18 +318,25 @@ def make_graphed_callables(\n         for func, args, static_input_surface in zip(\n             callables, sample_args, per_callable_static_input_surfaces\n         ):\n+            grad_inputs, outputs, outputs_grad = None, None, None\n             for _ in range(num_warmup_iters):\n                 outputs = torch.utils._pytree.tree_leaves(func(*args))\n-                grad_inputs = torch.autograd.grad(\n-                    outputs=tuple(o for o in outputs if o.requires_grad),\n-                    inputs=tuple(i for i in static_input_surface if i.requires_grad),\n-                    grad_outputs=tuple(\n-                        torch.empty_like(o) for o in outputs if o.requires_grad\n-                    ),\n-                    only_inputs=True,\n-                    allow_unused=allow_unused_input,\n-                )\n-            del outputs, grad_inputs  # type: ignore[possibly-undefined]\n+                outputs_grad = tuple(o for o in outputs if o.requires_grad)\n+                if len(outputs_grad) > 0:\n+                    grad_inputs = torch.autograd.grad(\n+                        outputs=outputs_grad,\n+                        inputs=tuple(\n+                            i for i in static_input_surface if i.requires_grad\n+                        ),\n+                        grad_outputs=tuple(\n+                            torch.empty_like(o) for o in outputs if o.requires_grad\n+                        ),\n+                        only_inputs=True,\n+                        allow_unused=allow_unused_input,\n+                    )\n+            for v in [outputs, outputs_grad, grad_inputs]:\n+                del v\n+\n     torch.cuda.synchronize()\n \n     # All captures here share a mempool. To avoid replays corrupting each other's memory,\n"
                },
                {
                    "old_start": 362,
                    "old_length": 14,
                    "new_start": 369,
                    "new_length": 17,
                    "hunk": "@@ -362,14 +369,17 @@ def make_graphed_callables(\n             torch.empty_like(o) if o.requires_grad else None for o in static_outputs\n         )\n \n-        with torch.cuda.graph(bwd_graph, pool=mempool):\n-            grad_inputs = torch.autograd.grad(\n-                outputs=tuple(o for o in static_outputs if o.requires_grad),\n-                inputs=tuple(i for i in static_input_surface if i.requires_grad),\n-                grad_outputs=tuple(o for o in static_grad_outputs if o is not None),\n-                only_inputs=True,\n-                allow_unused=allow_unused_input,\n-            )\n+        outputs_grad = tuple(o for o in static_outputs if o.requires_grad)\n+        grad_inputs = None\n+        if len(outputs_grad) > 0:\n+            with torch.cuda.graph(bwd_graph, pool=mempool):\n+                grad_inputs = torch.autograd.grad(\n+                    outputs=outputs_grad,\n+                    inputs=tuple(i for i in static_input_surface if i.requires_grad),\n+                    grad_outputs=tuple(o for o in static_grad_outputs if o is not None),\n+                    only_inputs=True,\n+                    allow_unused=allow_unused_input,\n+                )\n \n         # Constructs a tuple suitable for returning from Graphed.backward:\n         # Pads out the actually-needed grads with Nones in gradient slots for inputs that don't require grad.\n"
                },
                {
                    "old_start": 377,
                    "old_length": 7,
                    "new_start": 387,
                    "new_length": 7,
                    "hunk": "@@ -377,7 +387,7 @@ def make_graphed_callables(\n         static_grad_inputs = []\n         grad_idx = 0\n         for arg in static_input_surface:\n-            if arg.requires_grad:\n+            if arg.requires_grad and grad_inputs is not None:\n                 static_grad_inputs.append(grad_inputs[grad_idx])\n                 grad_idx += 1\n             else:"
                }
            ],
            "whole_deleted": "-                grad_inputs = torch.autograd.grad(\n-                    outputs=tuple(o for o in outputs if o.requires_grad),\n-                    inputs=tuple(i for i in static_input_surface if i.requires_grad),\n-                    grad_outputs=tuple(\n-                        torch.empty_like(o) for o in outputs if o.requires_grad\n-                    ),\n-                    only_inputs=True,\n-                    allow_unused=allow_unused_input,\n-                )\n-            del outputs, grad_inputs  # type: ignore[possibly-undefined]\n-        with torch.cuda.graph(bwd_graph, pool=mempool):\n-            grad_inputs = torch.autograd.grad(\n-                outputs=tuple(o for o in static_outputs if o.requires_grad),\n-                inputs=tuple(i for i in static_input_surface if i.requires_grad),\n-                grad_outputs=tuple(o for o in static_grad_outputs if o is not None),\n-                only_inputs=True,\n-                allow_unused=allow_unused_input,\n-            )\n-            if arg.requires_grad:\n",
            "whole_added": "+            grad_inputs, outputs, outputs_grad = None, None, None\n+                outputs_grad = tuple(o for o in outputs if o.requires_grad)\n+                if len(outputs_grad) > 0:\n+                    grad_inputs = torch.autograd.grad(\n+                        outputs=outputs_grad,\n+                        inputs=tuple(\n+                            i for i in static_input_surface if i.requires_grad\n+                        ),\n+                        grad_outputs=tuple(\n+                            torch.empty_like(o) for o in outputs if o.requires_grad\n+                        ),\n+                        only_inputs=True,\n+                        allow_unused=allow_unused_input,\n+                    )\n+            for v in [outputs, outputs_grad, grad_inputs]:\n+                del v\n+\n+        outputs_grad = tuple(o for o in static_outputs if o.requires_grad)\n+        grad_inputs = None\n+        if len(outputs_grad) > 0:\n+            with torch.cuda.graph(bwd_graph, pool=mempool):\n+                grad_inputs = torch.autograd.grad(\n+                    outputs=outputs_grad,\n+                    inputs=tuple(i for i in static_input_surface if i.requires_grad),\n+                    grad_outputs=tuple(o for o in static_grad_outputs if o is not None),\n+                    only_inputs=True,\n+                    allow_unused=allow_unused_input,\n+                )\n+            if arg.requires_grad and grad_inputs is not None:\n",
            "whole_hunk": "@@ -318,18 +318,25 @@ def make_graphed_callables(\n         for func, args, static_input_surface in zip(\n             callables, sample_args, per_callable_static_input_surfaces\n         ):\n+            grad_inputs, outputs, outputs_grad = None, None, None\n             for _ in range(num_warmup_iters):\n                 outputs = torch.utils._pytree.tree_leaves(func(*args))\n-                grad_inputs = torch.autograd.grad(\n-                    outputs=tuple(o for o in outputs if o.requires_grad),\n-                    inputs=tuple(i for i in static_input_surface if i.requires_grad),\n-                    grad_outputs=tuple(\n-                        torch.empty_like(o) for o in outputs if o.requires_grad\n-                    ),\n-                    only_inputs=True,\n-                    allow_unused=allow_unused_input,\n-                )\n-            del outputs, grad_inputs  # type: ignore[possibly-undefined]\n+                outputs_grad = tuple(o for o in outputs if o.requires_grad)\n+                if len(outputs_grad) > 0:\n+                    grad_inputs = torch.autograd.grad(\n+                        outputs=outputs_grad,\n+                        inputs=tuple(\n+                            i for i in static_input_surface if i.requires_grad\n+                        ),\n+                        grad_outputs=tuple(\n+                            torch.empty_like(o) for o in outputs if o.requires_grad\n+                        ),\n+                        only_inputs=True,\n+                        allow_unused=allow_unused_input,\n+                    )\n+            for v in [outputs, outputs_grad, grad_inputs]:\n+                del v\n+\n     torch.cuda.synchronize()\n \n     # All captures here share a mempool. To avoid replays corrupting each other's memory,\n@@ -362,14 +369,17 @@ def make_graphed_callables(\n             torch.empty_like(o) if o.requires_grad else None for o in static_outputs\n         )\n \n-        with torch.cuda.graph(bwd_graph, pool=mempool):\n-            grad_inputs = torch.autograd.grad(\n-                outputs=tuple(o for o in static_outputs if o.requires_grad),\n-                inputs=tuple(i for i in static_input_surface if i.requires_grad),\n-                grad_outputs=tuple(o for o in static_grad_outputs if o is not None),\n-                only_inputs=True,\n-                allow_unused=allow_unused_input,\n-            )\n+        outputs_grad = tuple(o for o in static_outputs if o.requires_grad)\n+        grad_inputs = None\n+        if len(outputs_grad) > 0:\n+            with torch.cuda.graph(bwd_graph, pool=mempool):\n+                grad_inputs = torch.autograd.grad(\n+                    outputs=outputs_grad,\n+                    inputs=tuple(i for i in static_input_surface if i.requires_grad),\n+                    grad_outputs=tuple(o for o in static_grad_outputs if o is not None),\n+                    only_inputs=True,\n+                    allow_unused=allow_unused_input,\n+                )\n \n         # Constructs a tuple suitable for returning from Graphed.backward:\n         # Pads out the actually-needed grads with Nones in gradient slots for inputs that don't require grad.\n@@ -377,7 +387,7 @@ def make_graphed_callables(\n         static_grad_inputs = []\n         grad_idx = 0\n         for arg in static_input_surface:\n-            if arg.requires_grad:\n+            if arg.requires_grad and grad_inputs is not None:\n                 static_grad_inputs.append(grad_inputs[grad_idx])\n                 grad_idx += 1\n             else:"
        }
    ]
},
{
    "Id": 533,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/90ef3b82d1ea823add8f49ea58bd0df057dd00e1",
    "date": "2023-09-01T02:14:18+00:00",
    "message": "[DeviceMesh] Add unique mesh_dim_name check in init_device_mesh() (#108326)\n\nEach mesh_dim_name in mesh_dim_names need to be unique. This PR adds check when calling init_device_mesh().\nPull Request resolved: https://github.com/pytorch/pytorch/pull/108326\nApproved by: https://github.com/wanchaol",
    "label": "NO",
    "changes": [
        {
            "name": "test_device_mesh.py",
            "path": "test/distributed/_tensor/test_device_mesh.py",
            "patches": [
                {
                    "old_start": 183,
                    "old_length": 6,
                    "new_start": 183,
                    "new_length": 30,
                    "hunk": "@@ -183,6 +183,30 @@ class InitDeviceMeshTest(DTensorTestBase):\n         two_d_mesh = init_device_mesh(self.device_type, mesh_shape)\n         self.assertEqual(two_d_mesh, ref_mesh)\n \n+    @with_comms\n+    def test_raises_duplicate_mesh_dim_names(self):\n+        with self.assertRaisesRegex(\n+            RuntimeError,\n+            \"Each mesh_dim_name must be uqique.\",\n+        ):\n+            mesn = init_device_mesh(\n+                self.device_type,\n+                (2, 4),\n+                mesh_dim_names=[\"dp\", \"dp\"],\n+            )\n+\n+    @with_comms\n+    def test_raises_mesh_shape_mesh_dim_names_mismatch(self):\n+        with self.assertRaisesRegex(\n+            RuntimeError,\n+            \"mesh_shape and mesh_dim_names should have same length!\",\n+        ):\n+            mesh = init_device_mesh(\n+                self.device_type,\n+                (8,),\n+                mesh_dim_names=[\"dp\", \"tp\"],\n+            )\n+\n \n class TestDeviceMeshGetItem(DTensorTestBase):\n     @property\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    @with_comms\n+    def test_raises_duplicate_mesh_dim_names(self):\n+        with self.assertRaisesRegex(\n+            RuntimeError,\n+            \"Each mesh_dim_name must be uqique.\",\n+        ):\n+            mesn = init_device_mesh(\n+                self.device_type,\n+                (2, 4),\n+                mesh_dim_names=[\"dp\", \"dp\"],\n+            )\n+\n+    @with_comms\n+    def test_raises_mesh_shape_mesh_dim_names_mismatch(self):\n+        with self.assertRaisesRegex(\n+            RuntimeError,\n+            \"mesh_shape and mesh_dim_names should have same length!\",\n+        ):\n+            mesh = init_device_mesh(\n+                self.device_type,\n+                (8,),\n+                mesh_dim_names=[\"dp\", \"tp\"],\n+            )\n+\n",
            "whole_hunk": "@@ -183,6 +183,30 @@ class InitDeviceMeshTest(DTensorTestBase):\n         two_d_mesh = init_device_mesh(self.device_type, mesh_shape)\n         self.assertEqual(two_d_mesh, ref_mesh)\n \n+    @with_comms\n+    def test_raises_duplicate_mesh_dim_names(self):\n+        with self.assertRaisesRegex(\n+            RuntimeError,\n+            \"Each mesh_dim_name must be uqique.\",\n+        ):\n+            mesn = init_device_mesh(\n+                self.device_type,\n+                (2, 4),\n+                mesh_dim_names=[\"dp\", \"dp\"],\n+            )\n+\n+    @with_comms\n+    def test_raises_mesh_shape_mesh_dim_names_mismatch(self):\n+        with self.assertRaisesRegex(\n+            RuntimeError,\n+            \"mesh_shape and mesh_dim_names should have same length!\",\n+        ):\n+            mesh = init_device_mesh(\n+                self.device_type,\n+                (8,),\n+                mesh_dim_names=[\"dp\", \"tp\"],\n+            )\n+\n \n class TestDeviceMeshGetItem(DTensorTestBase):\n     @property\n"
        },
        {
            "name": "device_mesh.py",
            "path": "torch/distributed/_tensor/device_mesh.py",
            "patches": [
                {
                    "old_start": 384,
                    "old_length": 10,
                    "new_start": 384,
                    "new_length": 18,
                    "hunk": "@@ -384,10 +384,18 @@ def init_device_mesh(\n         >>> one_d_mesh = init_device_mesh(\"cuda\", mesh_shape=(8,))\n         >>> two_d_mesh = init_device_mesh(\"cuda\", mesh_shape=(2, 8), mesh_dim_names=(\"dp\", \"tp\"))\n     \"\"\"\n-    if mesh_dim_names is not None and len(mesh_shape) != len(mesh_dim_names):\n-        raise RuntimeError(\n-            f\"Please provide a mesh_dim_name to each mesh_dim! Found {len(mesh_dim_names)} instead of {len(mesh_shape)}.\"\n-        )\n+    if mesh_dim_names is not None:\n+        if len(set(mesh_dim_names)) != len(mesh_dim_names):\n+            raise RuntimeError(\n+                \"Each mesh_dim_name must be uqique.\",\n+                f\"Found repeated mesh_dim_name in mesh_dim_names {mesh_dim_names}\",\n+            )\n+\n+        if len(mesh_shape) != len(mesh_dim_names):\n+            raise RuntimeError(\n+                \"mesh_shape and mesh_dim_names should have same length!\",\n+                f\"Found len(mesh_dim_names): {len(mesh_dim_names)} and len(mesh_shape):{len(mesh_shape)}.\",\n+            )\n \n     mesh = torch.arange(math.prod(mesh_shape)).view(mesh_shape)\n     device_mesh = DeviceMesh("
                }
            ],
            "whole_deleted": "-    if mesh_dim_names is not None and len(mesh_shape) != len(mesh_dim_names):\n-        raise RuntimeError(\n-            f\"Please provide a mesh_dim_name to each mesh_dim! Found {len(mesh_dim_names)} instead of {len(mesh_shape)}.\"\n-        )\n",
            "whole_added": "+    if mesh_dim_names is not None:\n+        if len(set(mesh_dim_names)) != len(mesh_dim_names):\n+            raise RuntimeError(\n+                \"Each mesh_dim_name must be uqique.\",\n+                f\"Found repeated mesh_dim_name in mesh_dim_names {mesh_dim_names}\",\n+            )\n+\n+        if len(mesh_shape) != len(mesh_dim_names):\n+            raise RuntimeError(\n+                \"mesh_shape and mesh_dim_names should have same length!\",\n+                f\"Found len(mesh_dim_names): {len(mesh_dim_names)} and len(mesh_shape):{len(mesh_shape)}.\",\n+            )\n",
            "whole_hunk": "@@ -384,10 +384,18 @@ def init_device_mesh(\n         >>> one_d_mesh = init_device_mesh(\"cuda\", mesh_shape=(8,))\n         >>> two_d_mesh = init_device_mesh(\"cuda\", mesh_shape=(2, 8), mesh_dim_names=(\"dp\", \"tp\"))\n     \"\"\"\n-    if mesh_dim_names is not None and len(mesh_shape) != len(mesh_dim_names):\n-        raise RuntimeError(\n-            f\"Please provide a mesh_dim_name to each mesh_dim! Found {len(mesh_dim_names)} instead of {len(mesh_shape)}.\"\n-        )\n+    if mesh_dim_names is not None:\n+        if len(set(mesh_dim_names)) != len(mesh_dim_names):\n+            raise RuntimeError(\n+                \"Each mesh_dim_name must be uqique.\",\n+                f\"Found repeated mesh_dim_name in mesh_dim_names {mesh_dim_names}\",\n+            )\n+\n+        if len(mesh_shape) != len(mesh_dim_names):\n+            raise RuntimeError(\n+                \"mesh_shape and mesh_dim_names should have same length!\",\n+                f\"Found len(mesh_dim_names): {len(mesh_dim_names)} and len(mesh_shape):{len(mesh_shape)}.\",\n+            )\n \n     mesh = torch.arange(math.prod(mesh_shape)).view(mesh_shape)\n     device_mesh = DeviceMesh("
        }
    ]
},
{
    "Id": 46,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/51fa0bd436cf627bd0c8ccf3a3a8b9c07d260622",
    "date": "2024-07-04T01:14:29+00:00",
    "message": "[pt2-bench] pass acc test if ref is NaN (#129996)\n\nI'm debugging the accuracy failure for training vision_maskrcnn.\n\nUnfortunately I could not succeed to run it locally (I've check pined commits for torchbenchmars/torchvision are correct, and reinstalled torchbenchmark for mask_rcnn). I get this error:\n```\neager run fail: AssertionError: targets should not be none when in training mode\n```\n(Command: time python benchmarks/dynamo/torchbench.py --backend inductor --amp --performance --training --only vision_maskrcnn )\n\nBut look at the log from the dashboard\n```\nE0623 19:17:59.085000 140114670171328 torch/_dynamo/utils.py:1468] RMSE (res-fp64): nan, (ref-fp64): nan and shape=torch.Size([1024, 256, 1, 1]). res.dtype: torch.float32, multiplier: 3.000000, tol: 0.001000\n```\n\nWe can see both the reference number and the pt2 number are NaN. I change torch._dynamo.utils.same to return true if both RMSE values are NaN.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/129996\nApproved by: https://github.com/jansel",
    "label": "YES",
    "changes": [
        {
            "name": "test_utils.py",
            "path": "test/dynamo/test_utils.py",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 19,
                    "hunk": "@@ -0,0 +1,19 @@\n+# Owner(s): [\"module: dynamo\"]\n+import torch\n+from torch._dynamo import utils\n+from torch._inductor.test_case import TestCase\n+\n+\n+class TestUtils(TestCase):\n+    def test_nan(self):\n+        a = torch.Tensor([float(\"nan\")])\n+        b = torch.Tensor([float(\"nan\")])\n+        fp64_ref = torch.DoubleTensor([5.0])\n+        res = utils.same(a, b, fp64_ref=fp64_ref, equal_nan=True)\n+        self.assertTrue(res)\n+\n+\n+if __name__ == \"__main__\":\n+    from torch._dynamo.test_case import run_tests\n+\n+    run_tests()\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+# Owner(s): [\"module: dynamo\"]\n+import torch\n+from torch._dynamo import utils\n+from torch._inductor.test_case import TestCase\n+\n+\n+class TestUtils(TestCase):\n+    def test_nan(self):\n+        a = torch.Tensor([float(\"nan\")])\n+        b = torch.Tensor([float(\"nan\")])\n+        fp64_ref = torch.DoubleTensor([5.0])\n+        res = utils.same(a, b, fp64_ref=fp64_ref, equal_nan=True)\n+        self.assertTrue(res)\n+\n+\n+if __name__ == \"__main__\":\n+    from torch._dynamo.test_case import run_tests\n+\n+    run_tests()\n",
            "whole_hunk": "@@ -0,0 +1,19 @@\n+# Owner(s): [\"module: dynamo\"]\n+import torch\n+from torch._dynamo import utils\n+from torch._inductor.test_case import TestCase\n+\n+\n+class TestUtils(TestCase):\n+    def test_nan(self):\n+        a = torch.Tensor([float(\"nan\")])\n+        b = torch.Tensor([float(\"nan\")])\n+        fp64_ref = torch.DoubleTensor([5.0])\n+        res = utils.same(a, b, fp64_ref=fp64_ref, equal_nan=True)\n+        self.assertTrue(res)\n+\n+\n+if __name__ == \"__main__\":\n+    from torch._dynamo.test_case import run_tests\n+\n+    run_tests()\n"
        },
        {
            "name": "utils.py",
            "path": "torch/_dynamo/utils.py",
            "patches": [
                {
                    "old_start": 1466,
                    "old_length": 6,
                    "new_start": 1466,
                    "new_length": 13,
                    "hunk": "@@ -1466,6 +1466,13 @@ def same(\n                     multiplier = 3.0\n \n                 passes_test = res_error <= (multiplier * ref_error + tol / 10.0)\n+                if (\n+                    not passes_test\n+                    and equal_nan\n+                    and math.isnan(ref_error)\n+                    and math.isnan(res_error)\n+                ):\n+                    passes_test = True\n                 if not passes_test:\n                     log_error(\n                         \"RMSE (res-fp64): %.5f, (ref-fp64): %.5f and shape=%s. res.dtype: %s, multiplier: %f, tol: %f\","
                }
            ],
            "whole_deleted": "",
            "whole_added": "+                if (\n+                    not passes_test\n+                    and equal_nan\n+                    and math.isnan(ref_error)\n+                    and math.isnan(res_error)\n+                ):\n+                    passes_test = True\n",
            "whole_hunk": "@@ -1466,6 +1466,13 @@ def same(\n                     multiplier = 3.0\n \n                 passes_test = res_error <= (multiplier * ref_error + tol / 10.0)\n+                if (\n+                    not passes_test\n+                    and equal_nan\n+                    and math.isnan(ref_error)\n+                    and math.isnan(res_error)\n+                ):\n+                    passes_test = True\n                 if not passes_test:\n                     log_error(\n                         \"RMSE (res-fp64): %.5f, (ref-fp64): %.5f and shape=%s. res.dtype: %s, multiplier: %f, tol: %f\","
        }
    ]
},
{
    "Id": 93,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/bb13fad7aa7754042efe6e9465410cf5e543a77e",
    "date": "2024-06-12T21:49:42+00:00",
    "message": "Share TCPStore by default when using c10d rdzv handler (#128096)\n\nSummary:\nNumber of features rely on TCP store as a control plane. By default TCPStore server is started on rank0 trainer and this can create a a race condition when rank0 may exit (error and graceful exit) and any other ranks reading/writing will fail.\n\nSolution: TCPStore server should outlive all the trainer processes. By moving the ownership TCPStore to torchelastic agent it naturally fixes the lifecycle of the server.\n\nStatic rendezvous in torchelastic does already support sharing of the TCPStore server. We are extending this to more commonly used c10d rendezvous handler.\n\nAny handler would like to manage tcp store has to:\n- Return true on `use_agent_store` property\n- `RendezvousInfo`.`RendezvousStoreInfo`#[`master_addr/master_port`] values refer to managed TCPStore (those are returned on `next_rendezvous` call)\n\nNote: in some instances users may want to use non-TCPStore based stores for the torchelastic rendezvous process, so the handler will need to create and hold a reference to TCPStore (as done in this change)\n\nTest Plan:\n`cat ~/workspace/dist-demo/stores.py`\n~~~\nimport torch\nimport logging\nimport sys\nimport torch.distributed as dist\nimport torch\n\nimport os\nimport time\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.StreamHandler(sys.stderr))\nlogger.setLevel(logging.INFO)\n\ndef _run_test(store):\n\n    if dist.get_rank() == 1:\n        logger.info(\"Rank %s is sleeping\", dist.get_rank())\n        time.sleep(5)\n        key = \"lookup_key\"\n        logger.info(\"Checking key %s in store on rank %s\", key, dist.get_rank())\n        store.check([key])\n    else:\n        logger.info(\"rank %s done\", dist.get_rank())\n\ndef main() -> None:\n    use_gpu = torch.cuda.is_available()\n    dist.init_process_group(backend=\"nccl\" if use_gpu else \"gloo\")\n    dist.barrier()\n\n    logger.info(f\"Hello World from rank {dist.get_rank()}\")\n\n    host = os.environ['MASTER_ADDR']\n    port = os.environ['MASTER_PORT']\n    world_size = os.environ['WORLD_SIZE']\n\n    logger.info(\"testing TCPStore\")\n    store = dist.TCPStore(\n        host_name=host, port=int(port), world_size=int(world_size),\n    )\n    _run_test(store)\n\nif __name__ == \"__main__\":\n    main()\n~~~\n\nWith the fix (TORCH_DISABLE_SHARE_RDZV_TCP_STORE=0 or just drop the option)\n~~~\n(pytorch_38) [kurman@devgpu011.cln5 ~/local/pytorch (main)]$ TORCH_DISABLE_SHARE_RDZV_TCP_STORE=0 python -m torch.distributed.run --rdzv-backend c10d --nproc-per-node 3 ~/workspace/dist-demo/stores.py\nmaster_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.\nWARNING:__main__:\n*****************************************\nSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\n*****************************************\nHello World from rank 1\nHello World from rank 2\nHello World from rank 0\ntesting TCPStore\ntesting TCPStore\ntesting TCPStore\nrank 2 done\nRank 1 is sleeping\nrank 0 done\nChecking key lookup_key in store on rank 1\n~~~\n\nTORCH_DISABLE_SHARE_RDZV_TCP_STORE=1\n~~~\n(pytorch_38) [kurman@devgpu011.cln5 ~/local/pytorch (main)]$ TORCH_DISABLE_SHARE_RDZV_TCP_STORE=1 python -m torch.distributed.run --rdzv-backend c10d --npro\nc-per-node 3 ~/workspace/dist-demo/stores.py\nmaster_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.\nWARNING:__main__:\n*****************************************\n\nSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\n*****************************************\nHello World from rank 0\nHello World from rank 2\nHello World from rank 1\ntesting TCPStore\ntesting TCPStore\ntesting TCPStore\nrank 0 done\nrank 2 done\nRank 1 is sleeping\nChecking key lookup_key in store on rank 1\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/home/kurman/workspace/dist-demo/stores.py\", line 46, in <module>\n[rank1]:     main()\n[rank1]:   File \"/home/kurman/workspace/dist-demo/stores.py\", line 42, in main\n[rank1]:     _run_test(store)\n[rank1]:   File \"/home/kurman/workspace/dist-demo/stores.py\", line 22, in _run_test\n[rank1]:     store.check([key])\n[rank1]: torch.distributed.DistNetworkError: Connection reset by peer\nE0605 17:40:22.853277 140249136719680 torch/distributed/elastic/multiprocessing/api.py:832] failed (exitcode: 1) local_rank: 1 (pid: 2279237) of binary: /home/kurman/.conda/envs/pytorch_38/bin/python\nTraceback (most recent call last):\n  File \"/home/kurman/.conda/envs/pytorch_38/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/home/kurman/.conda/envs/pytorch_38/lib/python3.8/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/data/users/kurman/pytorch/torch/distributed/run.py\", line 904, in <module>\n    main()\n  File \"/data/users/kurman/pytorch/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n    return f(*args, **kwargs)\n  File \"/data/users/kurman/pytorch/torch/distributed/run.py\", line 900, in main\n    run(args)\n  File \"/data/users/kurman/pytorch/torch/distributed/run.py\", line 891, in run\n    elastic_launch(\n  File \"/data/users/kurman/pytorch/torch/distributed/launcher/api.py\", line 132, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/data/users/kurman/pytorch/torch/distributed/launcher/api.py\", line 263, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:\n============================================================\n/home/kurman/workspace/dist-demo/stores.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2024-06-05_17:40:22\n  host      : devgpu011.cln5.facebook.com\n  rank      : 1 (local_rank: 1)\n  exitcode  : 1 (pid: 2279237)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\n~~~\n\nDifferential Revision: D58180193\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128096\nApproved by: https://github.com/shuqiangzhang",
    "label": "NO",
    "changes": [
        {
            "name": "dynamic_rendezvous_test.py",
            "path": "test/distributed/elastic/rendezvous/dynamic_rendezvous_test.py",
            "patches": [
                {
                    "old_start": 17,
                    "old_length": 7,
                    "new_start": 18,
                    "new_length": 9,
                    "hunk": "@@ -17,7 +18,9 @@ from base64 import b64encode\n from datetime import datetime, timedelta\n from typing import Callable, cast, Optional, Tuple\n from unittest import TestCase\n-from unittest.mock import call, MagicMock, Mock, patch\n+from unittest.mock import call, MagicMock, Mock, patch, PropertyMock\n+\n+import torch.distributed as dist\n \n from torch.distributed import HashStore, Store\n from torch.distributed.elastic.rendezvous import (\n"
                },
                {
                    "old_start": 26,
                    "old_length": 6,
                    "new_start": 29,
                    "new_length": 7,
                    "hunk": "@@ -26,6 +29,7 @@ from torch.distributed.elastic.rendezvous import (\n     RendezvousInfo,\n     RendezvousParameters,\n     RendezvousStateError,\n+    RendezvousStoreInfo,\n     RendezvousTimeoutError,\n )\n from torch.distributed.elastic.rendezvous.dynamic_rendezvous import (\n"
                },
                {
                    "old_start": 1169,
                    "old_length": 6,
                    "new_start": 1173,
                    "new_length": 16,
                    "hunk": "@@ -1169,6 +1173,16 @@ class DynamicRendezvousHandlerTest(TestCase):\n \n         self._state = self._state_holder.state\n \n+        self._tcp_store_mock = DummyStore()\n+\n+        patcher = patch.object(\n+            DynamicRendezvousHandler,\n+            \"_create_tcp_store_server\",\n+            return_value=self._tcp_store_mock,\n+        )\n+        patcher.start()\n+        self.addCleanup(patcher.stop)\n+\n     def _create_handler(self) -> DynamicRendezvousHandler:\n         settings = RendezvousSettings(\n             run_id=\"dummy_run_id\",\n"
                },
                {
                    "old_start": 1189,
                    "old_length": 6,
                    "new_start": 1203,
                    "new_length": 36,
                    "hunk": "@@ -1189,6 +1203,36 @@ class DynamicRendezvousHandlerTest(TestCase):\n             self._node, settings, \"dummy_backend\", self._store, self._state_holder\n         )\n \n+    def test_share_store_creates_tcp_store(self):\n+        handler = self._create_handler()\n+\n+        shared_store_info = RendezvousStoreInfo(\"host\", 54321)\n+        with patch.object(RendezvousStoreInfo, \"build\", return_value=shared_store_info):\n+            rdzv_info = handler.next_rendezvous()\n+            self.assertEqual(rdzv_info.bootstrap_store_info.master_addr, \"host\")\n+            self.assertEqual(rdzv_info.bootstrap_store_info.master_port, 54321)\n+        self.assertEqual(handler._shared_tcp_store_server, self._tcp_store_mock)\n+\n+        rdzv_info = handler.next_rendezvous()\n+        self.assertEqual(handler._shared_tcp_store_server, self._tcp_store_mock)\n+\n+    def test_share_store_when_tcp_store(self):\n+        handler = self._create_handler()\n+\n+        with patch.object(dist, \"PrefixStore\", new=Mock):\n+            handler._store = Mock(spec=dist.TCPStore)\n+            type(handler._store).host = PropertyMock(return_value=\"host\")\n+            type(handler._store).port = PropertyMock(return_value=54321)\n+            rdzv_info = handler.next_rendezvous()\n+            self.assertEqual(rdzv_info.bootstrap_store_info.master_addr, \"host\")\n+            self.assertEqual(rdzv_info.bootstrap_store_info.master_port, 54321)\n+            self.assertEqual(handler._shared_tcp_store_server, handler._store)\n+\n+            rdzv_info = handler.next_rendezvous()\n+            self.assertEqual(rdzv_info.bootstrap_store_info.master_addr, \"host\")\n+            self.assertEqual(rdzv_info.bootstrap_store_info.master_port, 54321)\n+            self.assertEqual(handler._shared_tcp_store_server, handler._store)\n+\n     @patch(\"torch.distributed.elastic.rendezvous.dynamic_rendezvous._delay\")\n     def test_next_rendezvous_skews_the_first_join_attempt(self, mock_delay) -> None:\n         for round, expected_call_count in [(0, True), (1, False)]:\n"
                },
                {
                    "old_start": 1717,
                    "old_length": 6,
                    "new_start": 1761,
                    "new_length": 93,
                    "hunk": "@@ -1717,6 +1761,93 @@ class IntegrationTest(TestCase):\n             lambda: len(pickle.loads(self._backend.get_state()[0]).wait_list) == 1\n         )\n \n+    def test_use_agent_store_is_true_by_default(self):\n+        handler = self._create_handler(\n+            min_nodes=1,\n+            max_nodes=2,\n+        )\n+\n+        self.assertTrue(handler.use_agent_store)\n+\n+    @patch.dict(os.environ, {\"TORCH_DISABLE_SHARE_RDZV_TCP_STORE\": \"1\"})\n+    def test_use_agent_store_is_disabled(self):\n+        handler = self._create_handler(\n+            min_nodes=1,\n+            max_nodes=2,\n+        )\n+\n+        self.assertFalse(handler.use_agent_store)\n+\n+    @patch.object(dist, \"PrefixStore\")\n+    def test_share_tcp_store_from_backend(self, prefix_store_class_mock):\n+        prefix_store = Mock(spec=dist.PrefixStore)\n+        prefix_store_class_mock.return_value = prefix_store\n+\n+        tcp_store = Mock(spec=dist.TCPStore)\n+        expected_addr = \"expected_address\"\n+        expected_port = 54321\n+        type(tcp_store).host = PropertyMock(return_value=expected_addr)\n+        type(tcp_store).port = PropertyMock(return_value=expected_port)\n+        # this will be injected\n+        self._store = tcp_store\n+\n+        handler1 = self._create_handler(min_nodes=2, max_nodes=2)\n+        handler2 = self._create_handler(min_nodes=2, max_nodes=2)\n+\n+        handler1_thread = _CapturingThread(target=handler1.next_rendezvous)\n+        handler2_thread = _CapturingThread(target=handler2.next_rendezvous)\n+\n+        handler1_thread.start()\n+        handler2_thread.start()\n+\n+        rdzv_info1: RendezvousInfo = handler1_thread.join()\n+        rdzv_info2: RendezvousInfo = handler2_thread.join()\n+\n+        self.assertEqual(rdzv_info1.store, prefix_store)\n+        self.assertEqual(rdzv_info2.store, prefix_store)\n+        prefix_store_class_mock.assert_called_with(\n+            \"torch.rendezvous.dummy_run_id.0\", tcp_store\n+        )\n+\n+        self.assertEqual(\n+            rdzv_info1.bootstrap_store_info, rdzv_info2.bootstrap_store_info\n+        )\n+\n+        self.assertEqual(rdzv_info1.bootstrap_store_info.master_addr, expected_addr)\n+        self.assertEqual(rdzv_info1.bootstrap_store_info.master_port, expected_port)\n+\n+    @patch.dict(os.environ, {\"TORCH_DISABLE_SHARE_RDZV_TCP_STORE\": \"1\"})\n+    @patch.object(dist, \"PrefixStore\")\n+    def test_share_tcp_store_is_disabled(self, prefix_store_class_mock):\n+        prefix_store = Mock()\n+        prefix_store_class_mock.return_value = prefix_store\n+\n+        prefix_store.set.return_value = None\n+        prefix_store.get.return_value = b\"123\"\n+        tcp_store = Mock(spec=dist.TCPStore)\n+        # this will be injected\n+        self._store = tcp_store\n+\n+        handler1 = self._create_handler(min_nodes=2, max_nodes=2)\n+        handler2 = self._create_handler(min_nodes=2, max_nodes=2)\n+\n+        handler1_thread = _CapturingThread(target=handler1.next_rendezvous)\n+        handler2_thread = _CapturingThread(target=handler2.next_rendezvous)\n+\n+        handler1_thread.start()\n+        handler2_thread.start()\n+\n+        rdzv_info1: RendezvousInfo = handler1_thread.join()\n+        rdzv_info2: RendezvousInfo = handler2_thread.join()\n+\n+        self.assertEqual(rdzv_info1.store, prefix_store)\n+        self.assertEqual(rdzv_info2.store, prefix_store)\n+        prefix_store_class_mock.assert_called_with(\n+            \"torch.rendezvous.dummy_run_id.0\", self._store\n+        )\n+        self.assertEqual(rdzv_info1.bootstrap_store_info.master_port, 123)\n+        self.assertEqual(rdzv_info2.bootstrap_store_info.master_port, 123)\n+\n \n class _InMemoryRendezvousBackend(RendezvousBackend):\n     def __init__(self):\n"
                }
            ],
            "whole_deleted": "-from unittest.mock import call, MagicMock, Mock, patch\n",
            "whole_added": "+from unittest.mock import call, MagicMock, Mock, patch, PropertyMock\n+\n+import torch.distributed as dist\n+    RendezvousStoreInfo,\n+        self._tcp_store_mock = DummyStore()\n+\n+        patcher = patch.object(\n+            DynamicRendezvousHandler,\n+            \"_create_tcp_store_server\",\n+            return_value=self._tcp_store_mock,\n+        )\n+        patcher.start()\n+        self.addCleanup(patcher.stop)\n+\n+    def test_share_store_creates_tcp_store(self):\n+        handler = self._create_handler()\n+\n+        shared_store_info = RendezvousStoreInfo(\"host\", 54321)\n+        with patch.object(RendezvousStoreInfo, \"build\", return_value=shared_store_info):\n+            rdzv_info = handler.next_rendezvous()\n+            self.assertEqual(rdzv_info.bootstrap_store_info.master_addr, \"host\")\n+            self.assertEqual(rdzv_info.bootstrap_store_info.master_port, 54321)\n+        self.assertEqual(handler._shared_tcp_store_server, self._tcp_store_mock)\n+\n+        rdzv_info = handler.next_rendezvous()\n+        self.assertEqual(handler._shared_tcp_store_server, self._tcp_store_mock)\n+\n+    def test_share_store_when_tcp_store(self):\n+        handler = self._create_handler()\n+\n+        with patch.object(dist, \"PrefixStore\", new=Mock):\n+            handler._store = Mock(spec=dist.TCPStore)\n+            type(handler._store).host = PropertyMock(return_value=\"host\")\n+            type(handler._store).port = PropertyMock(return_value=54321)\n+            rdzv_info = handler.next_rendezvous()\n+            self.assertEqual(rdzv_info.bootstrap_store_info.master_addr, \"host\")\n+            self.assertEqual(rdzv_info.bootstrap_store_info.master_port, 54321)\n+            self.assertEqual(handler._shared_tcp_store_server, handler._store)\n+\n+            rdzv_info = handler.next_rendezvous()\n+            self.assertEqual(rdzv_info.bootstrap_store_info.master_addr, \"host\")\n+            self.assertEqual(rdzv_info.bootstrap_store_info.master_port, 54321)\n+            self.assertEqual(handler._shared_tcp_store_server, handler._store)\n+\n+    def test_use_agent_store_is_true_by_default(self):\n+        handler = self._create_handler(\n+            min_nodes=1,\n+            max_nodes=2,\n+        )\n+\n+        self.assertTrue(handler.use_agent_store)\n+\n+    @patch.dict(os.environ, {\"TORCH_DISABLE_SHARE_RDZV_TCP_STORE\": \"1\"})\n+    def test_use_agent_store_is_disabled(self):\n+        handler = self._create_handler(\n+            min_nodes=1,\n+            max_nodes=2,\n+        )\n+\n+        self.assertFalse(handler.use_agent_store)\n+\n+    @patch.object(dist, \"PrefixStore\")\n+    def test_share_tcp_store_from_backend(self, prefix_store_class_mock):\n+        prefix_store = Mock(spec=dist.PrefixStore)\n+        prefix_store_class_mock.return_value = prefix_store\n+\n+        tcp_store = Mock(spec=dist.TCPStore)\n+        expected_addr = \"expected_address\"\n+        expected_port = 54321\n+        type(tcp_store).host = PropertyMock(return_value=expected_addr)\n+        type(tcp_store).port = PropertyMock(return_value=expected_port)\n+        # this will be injected\n+        self._store = tcp_store\n+\n+        handler1 = self._create_handler(min_nodes=2, max_nodes=2)\n+        handler2 = self._create_handler(min_nodes=2, max_nodes=2)\n+\n+        handler1_thread = _CapturingThread(target=handler1.next_rendezvous)\n+        handler2_thread = _CapturingThread(target=handler2.next_rendezvous)\n+\n+        handler1_thread.start()\n+        handler2_thread.start()\n+\n+        rdzv_info1: RendezvousInfo = handler1_thread.join()\n+        rdzv_info2: RendezvousInfo = handler2_thread.join()\n+\n+        self.assertEqual(rdzv_info1.store, prefix_store)\n+        self.assertEqual(rdzv_info2.store, prefix_store)\n+        prefix_store_class_mock.assert_called_with(\n+            \"torch.rendezvous.dummy_run_id.0\", tcp_store\n+        )\n+\n+        self.assertEqual(\n+            rdzv_info1.bootstrap_store_info, rdzv_info2.bootstrap_store_info\n+        )\n+\n+        self.assertEqual(rdzv_info1.bootstrap_store_info.master_addr, expected_addr)\n+        self.assertEqual(rdzv_info1.bootstrap_store_info.master_port, expected_port)\n+\n+    @patch.dict(os.environ, {\"TORCH_DISABLE_SHARE_RDZV_TCP_STORE\": \"1\"})\n+    @patch.object(dist, \"PrefixStore\")\n+    def test_share_tcp_store_is_disabled(self, prefix_store_class_mock):\n+        prefix_store = Mock()\n+        prefix_store_class_mock.return_value = prefix_store\n+\n+        prefix_store.set.return_value = None\n+        prefix_store.get.return_value = b\"123\"\n+        tcp_store = Mock(spec=dist.TCPStore)\n+        # this will be injected\n+        self._store = tcp_store\n+\n+        handler1 = self._create_handler(min_nodes=2, max_nodes=2)\n+        handler2 = self._create_handler(min_nodes=2, max_nodes=2)\n+\n+        handler1_thread = _CapturingThread(target=handler1.next_rendezvous)\n+        handler2_thread = _CapturingThread(target=handler2.next_rendezvous)\n+\n+        handler1_thread.start()\n+        handler2_thread.start()\n+\n+        rdzv_info1: RendezvousInfo = handler1_thread.join()\n+        rdzv_info2: RendezvousInfo = handler2_thread.join()\n+\n+        self.assertEqual(rdzv_info1.store, prefix_store)\n+        self.assertEqual(rdzv_info2.store, prefix_store)\n+        prefix_store_class_mock.assert_called_with(\n+            \"torch.rendezvous.dummy_run_id.0\", self._store\n+        )\n+        self.assertEqual(rdzv_info1.bootstrap_store_info.master_port, 123)\n+        self.assertEqual(rdzv_info2.bootstrap_store_info.master_port, 123)\n+\n",
            "whole_hunk": "@@ -17,7 +18,9 @@ from base64 import b64encode\n from datetime import datetime, timedelta\n from typing import Callable, cast, Optional, Tuple\n from unittest import TestCase\n-from unittest.mock import call, MagicMock, Mock, patch\n+from unittest.mock import call, MagicMock, Mock, patch, PropertyMock\n+\n+import torch.distributed as dist\n \n from torch.distributed import HashStore, Store\n from torch.distributed.elastic.rendezvous import (\n@@ -26,6 +29,7 @@ from torch.distributed.elastic.rendezvous import (\n     RendezvousInfo,\n     RendezvousParameters,\n     RendezvousStateError,\n+    RendezvousStoreInfo,\n     RendezvousTimeoutError,\n )\n from torch.distributed.elastic.rendezvous.dynamic_rendezvous import (\n@@ -1169,6 +1173,16 @@ class DynamicRendezvousHandlerTest(TestCase):\n \n         self._state = self._state_holder.state\n \n+        self._tcp_store_mock = DummyStore()\n+\n+        patcher = patch.object(\n+            DynamicRendezvousHandler,\n+            \"_create_tcp_store_server\",\n+            return_value=self._tcp_store_mock,\n+        )\n+        patcher.start()\n+        self.addCleanup(patcher.stop)\n+\n     def _create_handler(self) -> DynamicRendezvousHandler:\n         settings = RendezvousSettings(\n             run_id=\"dummy_run_id\",\n@@ -1189,6 +1203,36 @@ class DynamicRendezvousHandlerTest(TestCase):\n             self._node, settings, \"dummy_backend\", self._store, self._state_holder\n         )\n \n+    def test_share_store_creates_tcp_store(self):\n+        handler = self._create_handler()\n+\n+        shared_store_info = RendezvousStoreInfo(\"host\", 54321)\n+        with patch.object(RendezvousStoreInfo, \"build\", return_value=shared_store_info):\n+            rdzv_info = handler.next_rendezvous()\n+            self.assertEqual(rdzv_info.bootstrap_store_info.master_addr, \"host\")\n+            self.assertEqual(rdzv_info.bootstrap_store_info.master_port, 54321)\n+        self.assertEqual(handler._shared_tcp_store_server, self._tcp_store_mock)\n+\n+        rdzv_info = handler.next_rendezvous()\n+        self.assertEqual(handler._shared_tcp_store_server, self._tcp_store_mock)\n+\n+    def test_share_store_when_tcp_store(self):\n+        handler = self._create_handler()\n+\n+        with patch.object(dist, \"PrefixStore\", new=Mock):\n+            handler._store = Mock(spec=dist.TCPStore)\n+            type(handler._store).host = PropertyMock(return_value=\"host\")\n+            type(handler._store).port = PropertyMock(return_value=54321)\n+            rdzv_info = handler.next_rendezvous()\n+            self.assertEqual(rdzv_info.bootstrap_store_info.master_addr, \"host\")\n+            self.assertEqual(rdzv_info.bootstrap_store_info.master_port, 54321)\n+            self.assertEqual(handler._shared_tcp_store_server, handler._store)\n+\n+            rdzv_info = handler.next_rendezvous()\n+            self.assertEqual(rdzv_info.bootstrap_store_info.master_addr, \"host\")\n+            self.assertEqual(rdzv_info.bootstrap_store_info.master_port, 54321)\n+            self.assertEqual(handler._shared_tcp_store_server, handler._store)\n+\n     @patch(\"torch.distributed.elastic.rendezvous.dynamic_rendezvous._delay\")\n     def test_next_rendezvous_skews_the_first_join_attempt(self, mock_delay) -> None:\n         for round, expected_call_count in [(0, True), (1, False)]:\n@@ -1717,6 +1761,93 @@ class IntegrationTest(TestCase):\n             lambda: len(pickle.loads(self._backend.get_state()[0]).wait_list) == 1\n         )\n \n+    def test_use_agent_store_is_true_by_default(self):\n+        handler = self._create_handler(\n+            min_nodes=1,\n+            max_nodes=2,\n+        )\n+\n+        self.assertTrue(handler.use_agent_store)\n+\n+    @patch.dict(os.environ, {\"TORCH_DISABLE_SHARE_RDZV_TCP_STORE\": \"1\"})\n+    def test_use_agent_store_is_disabled(self):\n+        handler = self._create_handler(\n+            min_nodes=1,\n+            max_nodes=2,\n+        )\n+\n+        self.assertFalse(handler.use_agent_store)\n+\n+    @patch.object(dist, \"PrefixStore\")\n+    def test_share_tcp_store_from_backend(self, prefix_store_class_mock):\n+        prefix_store = Mock(spec=dist.PrefixStore)\n+        prefix_store_class_mock.return_value = prefix_store\n+\n+        tcp_store = Mock(spec=dist.TCPStore)\n+        expected_addr = \"expected_address\"\n+        expected_port = 54321\n+        type(tcp_store).host = PropertyMock(return_value=expected_addr)\n+        type(tcp_store).port = PropertyMock(return_value=expected_port)\n+        # this will be injected\n+        self._store = tcp_store\n+\n+        handler1 = self._create_handler(min_nodes=2, max_nodes=2)\n+        handler2 = self._create_handler(min_nodes=2, max_nodes=2)\n+\n+        handler1_thread = _CapturingThread(target=handler1.next_rendezvous)\n+        handler2_thread = _CapturingThread(target=handler2.next_rendezvous)\n+\n+        handler1_thread.start()\n+        handler2_thread.start()\n+\n+        rdzv_info1: RendezvousInfo = handler1_thread.join()\n+        rdzv_info2: RendezvousInfo = handler2_thread.join()\n+\n+        self.assertEqual(rdzv_info1.store, prefix_store)\n+        self.assertEqual(rdzv_info2.store, prefix_store)\n+        prefix_store_class_mock.assert_called_with(\n+            \"torch.rendezvous.dummy_run_id.0\", tcp_store\n+        )\n+\n+        self.assertEqual(\n+            rdzv_info1.bootstrap_store_info, rdzv_info2.bootstrap_store_info\n+        )\n+\n+        self.assertEqual(rdzv_info1.bootstrap_store_info.master_addr, expected_addr)\n+        self.assertEqual(rdzv_info1.bootstrap_store_info.master_port, expected_port)\n+\n+    @patch.dict(os.environ, {\"TORCH_DISABLE_SHARE_RDZV_TCP_STORE\": \"1\"})\n+    @patch.object(dist, \"PrefixStore\")\n+    def test_share_tcp_store_is_disabled(self, prefix_store_class_mock):\n+        prefix_store = Mock()\n+        prefix_store_class_mock.return_value = prefix_store\n+\n+        prefix_store.set.return_value = None\n+        prefix_store.get.return_value = b\"123\"\n+        tcp_store = Mock(spec=dist.TCPStore)\n+        # this will be injected\n+        self._store = tcp_store\n+\n+        handler1 = self._create_handler(min_nodes=2, max_nodes=2)\n+        handler2 = self._create_handler(min_nodes=2, max_nodes=2)\n+\n+        handler1_thread = _CapturingThread(target=handler1.next_rendezvous)\n+        handler2_thread = _CapturingThread(target=handler2.next_rendezvous)\n+\n+        handler1_thread.start()\n+        handler2_thread.start()\n+\n+        rdzv_info1: RendezvousInfo = handler1_thread.join()\n+        rdzv_info2: RendezvousInfo = handler2_thread.join()\n+\n+        self.assertEqual(rdzv_info1.store, prefix_store)\n+        self.assertEqual(rdzv_info2.store, prefix_store)\n+        prefix_store_class_mock.assert_called_with(\n+            \"torch.rendezvous.dummy_run_id.0\", self._store\n+        )\n+        self.assertEqual(rdzv_info1.bootstrap_store_info.master_port, 123)\n+        self.assertEqual(rdzv_info2.bootstrap_store_info.master_port, 123)\n+\n \n class _InMemoryRendezvousBackend(RendezvousBackend):\n     def __init__(self):\n"
        },
        {
            "name": "c10d_rendezvous_backend.py",
            "path": "torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py",
            "patches": [
                {
                    "old_start": 159,
                    "old_length": 6,
                    "new_start": 159,
                    "new_length": 7,
                    "hunk": "@@ -159,6 +159,7 @@ def _create_tcp_store(params: RendezvousParameters) -> TCPStore:\n                 host,\n                 port,\n                 is_master=is_server,\n+                multi_tenant=True,\n                 timeout=timedelta(seconds=read_timeout),\n                 use_libuv=use_libuv,\n             )\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+                multi_tenant=True,\n",
            "whole_hunk": "@@ -159,6 +159,7 @@ def _create_tcp_store(params: RendezvousParameters) -> TCPStore:\n                 host,\n                 port,\n                 is_master=is_server,\n+                multi_tenant=True,\n                 timeout=timedelta(seconds=read_timeout),\n                 use_libuv=use_libuv,\n             )\n"
        },
        {
            "name": "dynamic_rendezvous.py",
            "path": "torch/distributed/elastic/rendezvous/dynamic_rendezvous.py",
            "patches": [
                {
                    "old_start": 19,
                    "old_length": 7,
                    "new_start": 19,
                    "new_length": 9,
                    "hunk": "@@ -19,7 +19,9 @@ from datetime import datetime, timedelta\n from enum import Enum\n from typing import Any, Callable, cast, Dict, List, Optional, Set, Tuple\n \n-from torch.distributed import PrefixStore, Store\n+import torch.distributed as dist\n+\n+from torch.distributed import Store\n from torch.distributed.elastic.events import construct_and_record_rdzv_event, NodeState\n \n from .api import (\n"
                },
                {
                    "old_start": 1073,
                    "old_length": 6,
                    "new_start": 1075,
                    "new_length": 11,
                    "hunk": "@@ -1073,6 +1075,11 @@ class DynamicRendezvousHandler(RendezvousHandler):\n \n         self._keep_alive_timer = None\n \n+        # Cached shared store server reference\n+        self._shared_tcp_store_server: Optional[dist.Store] = None\n+\n+        self._bootstrap_store_info: Optional[RendezvousStoreInfo] = None\n+\n     def _record(\n         self,\n         message: str,\n"
                },
                {
                    "old_start": 1090,
                    "old_length": 6,
                    "new_start": 1097,
                    "new_length": 15,
                    "hunk": "@@ -1090,6 +1097,15 @@ class DynamicRendezvousHandler(RendezvousHandler):\n             rank=rank,\n         )\n \n+    def _create_tcp_store_server(self, bootstrap_store_info) -> dist.TCPStore:\n+        return dist.TCPStore(\n+            bootstrap_store_info.master_addr,\n+            bootstrap_store_info.master_port,\n+            is_master=True,\n+            multi_tenant=True,\n+            use_libuv=True,\n+        )\n+\n     @property\n     def settings(self) -> RendezvousSettings:\n         \"\"\"Get the settings of the rendezvous.\"\"\"\n"
                },
                {
                    "old_start": 1099,
                    "old_length": 6,
                    "new_start": 1115,
                    "new_length": 11,
                    "hunk": "@@ -1099,6 +1115,11 @@ class DynamicRendezvousHandler(RendezvousHandler):\n         \"\"\"See base class.\"\"\"\n         return self._backend_name\n \n+    @property\n+    def use_agent_store(self) -> bool:\n+        \"\"\"See base class.\"\"\"\n+        return os.getenv(\"TORCH_DISABLE_SHARE_RDZV_TCP_STORE\", \"0\") != \"1\"\n+\n     def next_rendezvous(self) -> RendezvousInfo:\n         \"\"\"See base class.\"\"\"\n         msg = (\n"
                },
                {
                    "old_start": 1147,
                    "old_length": 12,
                    "new_start": 1168,
                    "new_length": 39,
                    "hunk": "@@ -1147,12 +1168,39 @@ class DynamicRendezvousHandler(RendezvousHandler):\n         self._record(message=msg, rank=rank)\n         logger.info(msg)\n \n-        bootstrap_store_info = RendezvousStoreInfo.build(rank, store)\n+        # opt-out option of TCP store sharing\n+        if os.getenv(\"TORCH_DISABLE_SHARE_RDZV_TCP_STORE\", \"0\") == \"1\":\n+            bootstrap_store_info = RendezvousStoreInfo.build(rank, store)\n+            return RendezvousInfo(\n+                store,\n+                rank,\n+                world_size,\n+                bootstrap_store_info,\n+            )\n+\n+        if self._bootstrap_store_info is None:\n+            if isinstance(self._store, dist.TCPStore):\n+                addr = self._store.host\n+                port = self._store.port\n+                self._bootstrap_store_info = RendezvousStoreInfo(master_addr=addr, master_port=port)\n+                if rank == 0:\n+                    self._shared_tcp_store_server = self._store\n+            else:\n+                # If the store is not type of TCPStore start TCPStore server, which requries\n+                # bootstrapping info across ranks\n+                self._bootstrap_store_info = RendezvousStoreInfo.build(rank, store)\n+                if rank == 0:\n+                    self._shared_tcp_store_server = self._create_tcp_store_server(self._bootstrap_store_info)\n+\n+        assert self._bootstrap_store_info is not None\n+        if rank == 0:\n+            assert self._shared_tcp_store_server is not None\n+\n         return RendezvousInfo(\n             store,\n             rank,\n             world_size,\n-            bootstrap_store_info,\n+            self._bootstrap_store_info,  # type: ignore[assignment]\n         )\n \n     def is_closed(self) -> bool:\n"
                },
                {
                    "old_start": 1288,
                    "old_length": 10,
                    "new_start": 1336,
                    "new_length": 13,
                    "hunk": "@@ -1288,10 +1336,13 @@ class DynamicRendezvousHandler(RendezvousHandler):\n \n         return state.participants[self._this_node], len(state.participants)\n \n-    def _get_store(self) -> Store:\n+    def _wrap_store(self, store: Store) -> Store:\n         key_prefix = f\"torch.rendezvous.{self._settings.run_id}.{self._state_holder.state.round}\"\n \n-        return PrefixStore(key_prefix, self._store)\n+        return dist.PrefixStore(key_prefix, store)\n+\n+    def _get_store(self) -> Store:\n+        return self._wrap_store(self._store)\n \n     def _get_deadline(self, timeout: timedelta) -> float:\n         return time.monotonic() + timeout.total_seconds()"
                }
            ],
            "whole_deleted": "-from torch.distributed import PrefixStore, Store\n-        bootstrap_store_info = RendezvousStoreInfo.build(rank, store)\n-            bootstrap_store_info,\n-    def _get_store(self) -> Store:\n-        return PrefixStore(key_prefix, self._store)\n",
            "whole_added": "+import torch.distributed as dist\n+\n+from torch.distributed import Store\n+        # Cached shared store server reference\n+        self._shared_tcp_store_server: Optional[dist.Store] = None\n+\n+        self._bootstrap_store_info: Optional[RendezvousStoreInfo] = None\n+\n+    def _create_tcp_store_server(self, bootstrap_store_info) -> dist.TCPStore:\n+        return dist.TCPStore(\n+            bootstrap_store_info.master_addr,\n+            bootstrap_store_info.master_port,\n+            is_master=True,\n+            multi_tenant=True,\n+            use_libuv=True,\n+        )\n+\n+    @property\n+    def use_agent_store(self) -> bool:\n+        \"\"\"See base class.\"\"\"\n+        return os.getenv(\"TORCH_DISABLE_SHARE_RDZV_TCP_STORE\", \"0\") != \"1\"\n+\n+        # opt-out option of TCP store sharing\n+        if os.getenv(\"TORCH_DISABLE_SHARE_RDZV_TCP_STORE\", \"0\") == \"1\":\n+            bootstrap_store_info = RendezvousStoreInfo.build(rank, store)\n+            return RendezvousInfo(\n+                store,\n+                rank,\n+                world_size,\n+                bootstrap_store_info,\n+            )\n+\n+        if self._bootstrap_store_info is None:\n+            if isinstance(self._store, dist.TCPStore):\n+                addr = self._store.host\n+                port = self._store.port\n+                self._bootstrap_store_info = RendezvousStoreInfo(master_addr=addr, master_port=port)\n+                if rank == 0:\n+                    self._shared_tcp_store_server = self._store\n+            else:\n+                # If the store is not type of TCPStore start TCPStore server, which requries\n+                # bootstrapping info across ranks\n+                self._bootstrap_store_info = RendezvousStoreInfo.build(rank, store)\n+                if rank == 0:\n+                    self._shared_tcp_store_server = self._create_tcp_store_server(self._bootstrap_store_info)\n+\n+        assert self._bootstrap_store_info is not None\n+        if rank == 0:\n+            assert self._shared_tcp_store_server is not None\n+\n+            self._bootstrap_store_info,  # type: ignore[assignment]\n+    def _wrap_store(self, store: Store) -> Store:\n+        return dist.PrefixStore(key_prefix, store)\n+\n+    def _get_store(self) -> Store:\n+        return self._wrap_store(self._store)\n",
            "whole_hunk": "@@ -19,7 +19,9 @@ from datetime import datetime, timedelta\n from enum import Enum\n from typing import Any, Callable, cast, Dict, List, Optional, Set, Tuple\n \n-from torch.distributed import PrefixStore, Store\n+import torch.distributed as dist\n+\n+from torch.distributed import Store\n from torch.distributed.elastic.events import construct_and_record_rdzv_event, NodeState\n \n from .api import (\n@@ -1073,6 +1075,11 @@ class DynamicRendezvousHandler(RendezvousHandler):\n \n         self._keep_alive_timer = None\n \n+        # Cached shared store server reference\n+        self._shared_tcp_store_server: Optional[dist.Store] = None\n+\n+        self._bootstrap_store_info: Optional[RendezvousStoreInfo] = None\n+\n     def _record(\n         self,\n         message: str,\n@@ -1090,6 +1097,15 @@ class DynamicRendezvousHandler(RendezvousHandler):\n             rank=rank,\n         )\n \n+    def _create_tcp_store_server(self, bootstrap_store_info) -> dist.TCPStore:\n+        return dist.TCPStore(\n+            bootstrap_store_info.master_addr,\n+            bootstrap_store_info.master_port,\n+            is_master=True,\n+            multi_tenant=True,\n+            use_libuv=True,\n+        )\n+\n     @property\n     def settings(self) -> RendezvousSettings:\n         \"\"\"Get the settings of the rendezvous.\"\"\"\n@@ -1099,6 +1115,11 @@ class DynamicRendezvousHandler(RendezvousHandler):\n         \"\"\"See base class.\"\"\"\n         return self._backend_name\n \n+    @property\n+    def use_agent_store(self) -> bool:\n+        \"\"\"See base class.\"\"\"\n+        return os.getenv(\"TORCH_DISABLE_SHARE_RDZV_TCP_STORE\", \"0\") != \"1\"\n+\n     def next_rendezvous(self) -> RendezvousInfo:\n         \"\"\"See base class.\"\"\"\n         msg = (\n@@ -1147,12 +1168,39 @@ class DynamicRendezvousHandler(RendezvousHandler):\n         self._record(message=msg, rank=rank)\n         logger.info(msg)\n \n-        bootstrap_store_info = RendezvousStoreInfo.build(rank, store)\n+        # opt-out option of TCP store sharing\n+        if os.getenv(\"TORCH_DISABLE_SHARE_RDZV_TCP_STORE\", \"0\") == \"1\":\n+            bootstrap_store_info = RendezvousStoreInfo.build(rank, store)\n+            return RendezvousInfo(\n+                store,\n+                rank,\n+                world_size,\n+                bootstrap_store_info,\n+            )\n+\n+        if self._bootstrap_store_info is None:\n+            if isinstance(self._store, dist.TCPStore):\n+                addr = self._store.host\n+                port = self._store.port\n+                self._bootstrap_store_info = RendezvousStoreInfo(master_addr=addr, master_port=port)\n+                if rank == 0:\n+                    self._shared_tcp_store_server = self._store\n+            else:\n+                # If the store is not type of TCPStore start TCPStore server, which requries\n+                # bootstrapping info across ranks\n+                self._bootstrap_store_info = RendezvousStoreInfo.build(rank, store)\n+                if rank == 0:\n+                    self._shared_tcp_store_server = self._create_tcp_store_server(self._bootstrap_store_info)\n+\n+        assert self._bootstrap_store_info is not None\n+        if rank == 0:\n+            assert self._shared_tcp_store_server is not None\n+\n         return RendezvousInfo(\n             store,\n             rank,\n             world_size,\n-            bootstrap_store_info,\n+            self._bootstrap_store_info,  # type: ignore[assignment]\n         )\n \n     def is_closed(self) -> bool:\n@@ -1288,10 +1336,13 @@ class DynamicRendezvousHandler(RendezvousHandler):\n \n         return state.participants[self._this_node], len(state.participants)\n \n-    def _get_store(self) -> Store:\n+    def _wrap_store(self, store: Store) -> Store:\n         key_prefix = f\"torch.rendezvous.{self._settings.run_id}.{self._state_holder.state.round}\"\n \n-        return PrefixStore(key_prefix, self._store)\n+        return dist.PrefixStore(key_prefix, store)\n+\n+    def _get_store(self) -> Store:\n+        return self._wrap_store(self._store)\n \n     def _get_deadline(self, timeout: timedelta) -> float:\n         return time.monotonic() + timeout.total_seconds()"
        }
    ]
},
{
    "Id": 477,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/5caf2e55d4c59d3562d5e3e91c22698451edcb80",
    "date": "2023-10-14T20:59:28+00:00",
    "message": "[FSDP] fix: fix for fsdp zero2 validation error (#110139)\n\n# Problem\nWhen sharding_strategy is set to SHARD_GRAD_OP and forward_prefetch is turned on, the validation after the train has an incorrect weight shape.\n<img width=\"1508\" alt=\"image\" src=\"https://github.com/pytorch/pytorch/assets/41232043/57a9c3bb-cb5c-46df-ac26-922740686f9e\">\n\n# Analyze\nWhen using `SHARD_GRAD_OP`, the `free_unsharded_flat_param` in `_post_forward_reshard` is often False, so it does not set the handle's `_prefetched` flag to False after the forward.\n\nThe normal train phase sets this flag to False in the `_post_backward_final_callback`, and the validation phase doesn't execute the hook, so after the first iter of the validation is done, the flag of the handle of the prefetched will remain True.\n\nThis will cause the handle to skip the `_unshard` in the next `_pre_forward_unshard`, and the `_prefetch_handle` will not do a prefetch, which will result in an incorrect weight shape.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110139\nApproved by: https://github.com/awgu",
    "label": "YES",
    "changes": [
        {
            "name": "test_fsdp_misc.py",
            "path": "test/distributed/fsdp/test_fsdp_misc.py",
            "patches": [
                {
                    "old_start": 143,
                    "old_length": 6,
                    "new_start": 143,
                    "new_length": 109,
                    "hunk": "@@ -143,6 +143,109 @@ class TestFSDPMiscMultiProcess(FSDPTest):\n             nested_wrapped_module, torch.device(\"cuda\", torch.cuda.current_device())\n         )\n \n+    @skip_if_lt_x_gpu(2)\n+    def test_fsdp_zero2_eval_with_prefetch(self):\n+        # Test FSDP validation with SHARD_GRAD_OP and forward_prefetch\n+\n+        class Mnist(nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.conv1 = nn.Conv2d(1, 32, 3, 1)\n+                self.conv2 = nn.Conv2d(32, 64, 3, 1)\n+                self.dropout1 = nn.Dropout(0.25)\n+                self.dropout2 = nn.Dropout(0.5)\n+                self.fc1 = nn.Linear(9216, 128)\n+                self.fc2 = nn.Linear(128, 10)\n+                self.ln = nn.LayerNorm(9216)\n+\n+            def forward(self, x, y):\n+                x = self.conv1(x)\n+                x = torch.nn.functional.relu(x)\n+                x = self.conv2(x)\n+                x = torch.nn.functional.relu(x)\n+                x = torch.nn.functional.max_pool2d(x, 2)\n+                x = self.dropout1(x)\n+                x = torch.flatten(x, 1)\n+                x = self.ln(x)\n+                x = self.fc1(x)\n+                x = torch.nn.functional.relu(x)\n+                x = self.dropout2(x)\n+                x = self.fc2(x)\n+                output = torch.nn.functional.log_softmax(x, dim=1)\n+                loss = torch.nn.functional.cross_entropy(output, y)\n+                return loss\n+\n+        model = Mnist().cuda()\n+        model1 = Mnist().cuda()\n+        model1.load_state_dict(model.state_dict())\n+        fsdp_model = FSDP(\n+            model,\n+            sharding_strategy=ShardingStrategy.SHARD_GRAD_OP,\n+            forward_prefetch=True,\n+            use_orig_params=True,\n+            auto_wrap_policy=ModuleWrapPolicy([nn.Linear, nn.Conv2d]),\n+        )\n+        ddp_model = torch.nn.parallel.DistributedDataParallel(\n+            model1,\n+        )\n+\n+        fsdp_opt = torch.optim.SGD(fsdp_model.parameters(), lr=1e-4)\n+        ddp_opt = torch.optim.SGD(ddp_model.parameters(), lr=1e-4)\n+\n+        seed = self.rank + 20231010\n+        torch.manual_seed(seed)\n+        torch.cuda.manual_seed(seed)\n+\n+        losses = []\n+        grads = []\n+        for i in range(5):\n+            x = torch.randn(8, 1, 28, 28, device=\"cuda\").requires_grad_()\n+            y = torch.randint(low=0, high=9, size=(8,), device=\"cuda\")\n+            for model, opt in ((fsdp_model, fsdp_opt), (ddp_model, ddp_opt)):\n+                seed = self.rank + i\n+                torch.manual_seed(seed)\n+                torch.cuda.manual_seed(seed)\n+                loss = model(x, y).sum()\n+                losses.append(loss)\n+                loss.backward()\n+                opt.step()\n+                grads.append(x.grad)\n+                opt.zero_grad()\n+            assert torch.allclose(losses[0], losses[1])\n+            assert torch.allclose(grads[0], grads[1])\n+            losses.clear()\n+            grads.clear()\n+\n+        with torch.no_grad():\n+            fsdp_model.eval()\n+            ddp_model.eval()\n+            for _ in range(5):\n+                x = torch.randn(8, 1, 28, 28, device=\"cuda\").requires_grad_()\n+                y = torch.randint(low=0, high=9, size=(8,), device=\"cuda\")\n+                fsdp_loss = fsdp_model(x, y)\n+                ddp_loss = ddp_model(x, y)\n+                assert torch.allclose(fsdp_loss, ddp_loss)\n+\n+        fsdp_model.train()\n+        ddp_model.train()\n+        for i in range(5):\n+            x = torch.randn(8, 1, 28, 28, device=\"cuda\").requires_grad_()\n+            y = torch.randint(low=0, high=9, size=(8,), device=\"cuda\")\n+            for model, opt in ((fsdp_model, fsdp_opt), (ddp_model, ddp_opt)):\n+                seed = self.rank + i\n+                torch.manual_seed(seed)\n+                torch.cuda.manual_seed(seed)\n+                loss = model(x, y).sum()\n+                losses.append(loss)\n+                loss.backward()\n+                opt.step()\n+                grads.append(x.grad)\n+                opt.zero_grad()\n+            assert torch.allclose(losses[0], losses[1])\n+            assert torch.allclose(grads[0], grads[1])\n+            losses.clear()\n+            grads.clear()\n+\n     @skip_if_lt_x_gpu(2)\n     @parametrize(\"use_second_layer\", [True, False])\n     @parametrize(\"sharding_strategy\", [ShardingStrategy.NO_SHARD, None])\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    @skip_if_lt_x_gpu(2)\n+    def test_fsdp_zero2_eval_with_prefetch(self):\n+        # Test FSDP validation with SHARD_GRAD_OP and forward_prefetch\n+\n+        class Mnist(nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.conv1 = nn.Conv2d(1, 32, 3, 1)\n+                self.conv2 = nn.Conv2d(32, 64, 3, 1)\n+                self.dropout1 = nn.Dropout(0.25)\n+                self.dropout2 = nn.Dropout(0.5)\n+                self.fc1 = nn.Linear(9216, 128)\n+                self.fc2 = nn.Linear(128, 10)\n+                self.ln = nn.LayerNorm(9216)\n+\n+            def forward(self, x, y):\n+                x = self.conv1(x)\n+                x = torch.nn.functional.relu(x)\n+                x = self.conv2(x)\n+                x = torch.nn.functional.relu(x)\n+                x = torch.nn.functional.max_pool2d(x, 2)\n+                x = self.dropout1(x)\n+                x = torch.flatten(x, 1)\n+                x = self.ln(x)\n+                x = self.fc1(x)\n+                x = torch.nn.functional.relu(x)\n+                x = self.dropout2(x)\n+                x = self.fc2(x)\n+                output = torch.nn.functional.log_softmax(x, dim=1)\n+                loss = torch.nn.functional.cross_entropy(output, y)\n+                return loss\n+\n+        model = Mnist().cuda()\n+        model1 = Mnist().cuda()\n+        model1.load_state_dict(model.state_dict())\n+        fsdp_model = FSDP(\n+            model,\n+            sharding_strategy=ShardingStrategy.SHARD_GRAD_OP,\n+            forward_prefetch=True,\n+            use_orig_params=True,\n+            auto_wrap_policy=ModuleWrapPolicy([nn.Linear, nn.Conv2d]),\n+        )\n+        ddp_model = torch.nn.parallel.DistributedDataParallel(\n+            model1,\n+        )\n+\n+        fsdp_opt = torch.optim.SGD(fsdp_model.parameters(), lr=1e-4)\n+        ddp_opt = torch.optim.SGD(ddp_model.parameters(), lr=1e-4)\n+\n+        seed = self.rank + 20231010\n+        torch.manual_seed(seed)\n+        torch.cuda.manual_seed(seed)\n+\n+        losses = []\n+        grads = []\n+        for i in range(5):\n+            x = torch.randn(8, 1, 28, 28, device=\"cuda\").requires_grad_()\n+            y = torch.randint(low=0, high=9, size=(8,), device=\"cuda\")\n+            for model, opt in ((fsdp_model, fsdp_opt), (ddp_model, ddp_opt)):\n+                seed = self.rank + i\n+                torch.manual_seed(seed)\n+                torch.cuda.manual_seed(seed)\n+                loss = model(x, y).sum()\n+                losses.append(loss)\n+                loss.backward()\n+                opt.step()\n+                grads.append(x.grad)\n+                opt.zero_grad()\n+            assert torch.allclose(losses[0], losses[1])\n+            assert torch.allclose(grads[0], grads[1])\n+            losses.clear()\n+            grads.clear()\n+\n+        with torch.no_grad():\n+            fsdp_model.eval()\n+            ddp_model.eval()\n+            for _ in range(5):\n+                x = torch.randn(8, 1, 28, 28, device=\"cuda\").requires_grad_()\n+                y = torch.randint(low=0, high=9, size=(8,), device=\"cuda\")\n+                fsdp_loss = fsdp_model(x, y)\n+                ddp_loss = ddp_model(x, y)\n+                assert torch.allclose(fsdp_loss, ddp_loss)\n+\n+        fsdp_model.train()\n+        ddp_model.train()\n+        for i in range(5):\n+            x = torch.randn(8, 1, 28, 28, device=\"cuda\").requires_grad_()\n+            y = torch.randint(low=0, high=9, size=(8,), device=\"cuda\")\n+            for model, opt in ((fsdp_model, fsdp_opt), (ddp_model, ddp_opt)):\n+                seed = self.rank + i\n+                torch.manual_seed(seed)\n+                torch.cuda.manual_seed(seed)\n+                loss = model(x, y).sum()\n+                losses.append(loss)\n+                loss.backward()\n+                opt.step()\n+                grads.append(x.grad)\n+                opt.zero_grad()\n+            assert torch.allclose(losses[0], losses[1])\n+            assert torch.allclose(grads[0], grads[1])\n+            losses.clear()\n+            grads.clear()\n+\n",
            "whole_hunk": "@@ -143,6 +143,109 @@ class TestFSDPMiscMultiProcess(FSDPTest):\n             nested_wrapped_module, torch.device(\"cuda\", torch.cuda.current_device())\n         )\n \n+    @skip_if_lt_x_gpu(2)\n+    def test_fsdp_zero2_eval_with_prefetch(self):\n+        # Test FSDP validation with SHARD_GRAD_OP and forward_prefetch\n+\n+        class Mnist(nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.conv1 = nn.Conv2d(1, 32, 3, 1)\n+                self.conv2 = nn.Conv2d(32, 64, 3, 1)\n+                self.dropout1 = nn.Dropout(0.25)\n+                self.dropout2 = nn.Dropout(0.5)\n+                self.fc1 = nn.Linear(9216, 128)\n+                self.fc2 = nn.Linear(128, 10)\n+                self.ln = nn.LayerNorm(9216)\n+\n+            def forward(self, x, y):\n+                x = self.conv1(x)\n+                x = torch.nn.functional.relu(x)\n+                x = self.conv2(x)\n+                x = torch.nn.functional.relu(x)\n+                x = torch.nn.functional.max_pool2d(x, 2)\n+                x = self.dropout1(x)\n+                x = torch.flatten(x, 1)\n+                x = self.ln(x)\n+                x = self.fc1(x)\n+                x = torch.nn.functional.relu(x)\n+                x = self.dropout2(x)\n+                x = self.fc2(x)\n+                output = torch.nn.functional.log_softmax(x, dim=1)\n+                loss = torch.nn.functional.cross_entropy(output, y)\n+                return loss\n+\n+        model = Mnist().cuda()\n+        model1 = Mnist().cuda()\n+        model1.load_state_dict(model.state_dict())\n+        fsdp_model = FSDP(\n+            model,\n+            sharding_strategy=ShardingStrategy.SHARD_GRAD_OP,\n+            forward_prefetch=True,\n+            use_orig_params=True,\n+            auto_wrap_policy=ModuleWrapPolicy([nn.Linear, nn.Conv2d]),\n+        )\n+        ddp_model = torch.nn.parallel.DistributedDataParallel(\n+            model1,\n+        )\n+\n+        fsdp_opt = torch.optim.SGD(fsdp_model.parameters(), lr=1e-4)\n+        ddp_opt = torch.optim.SGD(ddp_model.parameters(), lr=1e-4)\n+\n+        seed = self.rank + 20231010\n+        torch.manual_seed(seed)\n+        torch.cuda.manual_seed(seed)\n+\n+        losses = []\n+        grads = []\n+        for i in range(5):\n+            x = torch.randn(8, 1, 28, 28, device=\"cuda\").requires_grad_()\n+            y = torch.randint(low=0, high=9, size=(8,), device=\"cuda\")\n+            for model, opt in ((fsdp_model, fsdp_opt), (ddp_model, ddp_opt)):\n+                seed = self.rank + i\n+                torch.manual_seed(seed)\n+                torch.cuda.manual_seed(seed)\n+                loss = model(x, y).sum()\n+                losses.append(loss)\n+                loss.backward()\n+                opt.step()\n+                grads.append(x.grad)\n+                opt.zero_grad()\n+            assert torch.allclose(losses[0], losses[1])\n+            assert torch.allclose(grads[0], grads[1])\n+            losses.clear()\n+            grads.clear()\n+\n+        with torch.no_grad():\n+            fsdp_model.eval()\n+            ddp_model.eval()\n+            for _ in range(5):\n+                x = torch.randn(8, 1, 28, 28, device=\"cuda\").requires_grad_()\n+                y = torch.randint(low=0, high=9, size=(8,), device=\"cuda\")\n+                fsdp_loss = fsdp_model(x, y)\n+                ddp_loss = ddp_model(x, y)\n+                assert torch.allclose(fsdp_loss, ddp_loss)\n+\n+        fsdp_model.train()\n+        ddp_model.train()\n+        for i in range(5):\n+            x = torch.randn(8, 1, 28, 28, device=\"cuda\").requires_grad_()\n+            y = torch.randint(low=0, high=9, size=(8,), device=\"cuda\")\n+            for model, opt in ((fsdp_model, fsdp_opt), (ddp_model, ddp_opt)):\n+                seed = self.rank + i\n+                torch.manual_seed(seed)\n+                torch.cuda.manual_seed(seed)\n+                loss = model(x, y).sum()\n+                losses.append(loss)\n+                loss.backward()\n+                opt.step()\n+                grads.append(x.grad)\n+                opt.zero_grad()\n+            assert torch.allclose(losses[0], losses[1])\n+            assert torch.allclose(grads[0], grads[1])\n+            losses.clear()\n+            grads.clear()\n+\n     @skip_if_lt_x_gpu(2)\n     @parametrize(\"use_second_layer\", [True, False])\n     @parametrize(\"sharding_strategy\", [ShardingStrategy.NO_SHARD, None])\n"
        },
        {
            "name": "_runtime_utils.py",
            "path": "torch/distributed/fsdp/_runtime_utils.py",
            "patches": [
                {
                    "old_start": 599,
                    "old_length": 6,
                    "new_start": 599,
                    "new_length": 7,
                    "hunk": "@@ -599,6 +599,7 @@ def _root_pre_forward(\n                     handles.append(fsdp_state._handle)\n             for handle in handles:\n                 handle._needs_pre_forward_unshard = True\n+                handle._prefetched = False\n         _wait_for_computation_stream(\n             state._device_handle.current_stream(),\n             state._unshard_stream,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+                handle._prefetched = False\n",
            "whole_hunk": "@@ -599,6 +599,7 @@ def _root_pre_forward(\n                     handles.append(fsdp_state._handle)\n             for handle in handles:\n                 handle._needs_pre_forward_unshard = True\n+                handle._prefetched = False\n         _wait_for_computation_stream(\n             state._device_handle.current_stream(),\n             state._unshard_stream,"
        }
    ]
},
{
    "Id": 363,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/b5e83b8c50bb883c1749c2edf8add5bb37e284c4",
    "date": "2023-12-28T15:02:29+00:00",
    "message": "Fix edge case for size 1 channels dim in AdaptiveMaxPool (#116482)\n\nFixes https://github.com/pytorch/pytorch/issues/107842\n\nUnlike `AdaptiveAvgPool`, `AdaptiveMaxPool` does not have a CUDA kernel for ChannelsLast. We workaround this by calling `contiguous()` on the input. However, there is an edge case when the channels dimension has size 1.\n\n```python\n>>> t = torch.randn(2, 1, 3, 3)\n>>> t.stride()\n(9, 9, 3, 1)\n>>> t_c =  t.to(memory_format=torch.channels_last)\n>>> t_c.stride()\n(9, 1, 3, 1)  # (CHW, 1, CW, C)\n>>> t_c.is_contiguous()\nTrue  # contiguity check doesn't check strides for singleton dimensions\n```\n\nSince the CUDA kernel treats the batch,`B`, and  channels,`C`, dimensions as implicitly flattened and increments the data pointer for `input` to the start of the next plane using\n\nhttps://github.com/pytorch/pytorch/blob/669b182d33f5e1368d8de6d86b891f65480c9b22/aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu#L67\n\nIf our input falls into the aforementioned edge case, the `data_ptr` will not be incremented correctly. The simple fix for this is to calculate the stride for the channels dimension using $\\prod_{i > 1}size(i)$\n\nAnalogous fix for the 3D case.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116482\nApproved by: https://github.com/albanD",
    "label": "YES",
    "changes": [
        {
            "name": "AdaptiveMaxPooling2d.cu",
            "path": "aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu",
            "patches": [
                {
                    "old_start": 268,
                    "old_length": 7,
                    "new_start": 268,
                    "new_length": 11,
                    "hunk": "@@ -268,7 +268,11 @@ const Tensor& indices) {\n     int64_t isizeH = input_.size(2);\n     int64_t isizeW = input_.size(3);\n \n-    int64_t istrideD = input_.stride(1);\n+    // In the kernel, the batch and channel dimensions are treated as if they\n+    // are flattened and istrideD is used as the stride of this flattened dim\n+    // Handle the edge case where input_.size(1) == 1, where despite passing the\n+    // contiguity check the stride might not be H * W\n+    int64_t istrideD = isizeH * isizeW;\n     int64_t istrideH = input_.stride(2);\n     int64_t istrideW = input_.stride(3);\n \n"
                }
            ],
            "whole_deleted": "-    int64_t istrideD = input_.stride(1);\n",
            "whole_added": "+    // In the kernel, the batch and channel dimensions are treated as if they\n+    // are flattened and istrideD is used as the stride of this flattened dim\n+    // Handle the edge case where input_.size(1) == 1, where despite passing the\n+    // contiguity check the stride might not be H * W\n+    int64_t istrideD = isizeH * isizeW;\n",
            "whole_hunk": "@@ -268,7 +268,11 @@ const Tensor& indices) {\n     int64_t isizeH = input_.size(2);\n     int64_t isizeW = input_.size(3);\n \n-    int64_t istrideD = input_.stride(1);\n+    // In the kernel, the batch and channel dimensions are treated as if they\n+    // are flattened and istrideD is used as the stride of this flattened dim\n+    // Handle the edge case where input_.size(1) == 1, where despite passing the\n+    // contiguity check the stride might not be H * W\n+    int64_t istrideD = isizeH * isizeW;\n     int64_t istrideH = input_.stride(2);\n     int64_t istrideW = input_.stride(3);\n \n"
        },
        {
            "name": "AdaptiveMaxPooling3d.cu",
            "path": "aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu",
            "patches": [
                {
                    "old_start": 346,
                    "old_length": 7,
                    "new_start": 346,
                    "new_length": 11,
                    "hunk": "@@ -346,7 +346,11 @@ TORCH_IMPL_FUNC(adaptive_max_pool3d_out_cuda)\n     isizeH = input_.size(3);\n     isizeW = input_.size(4);\n \n-    istrideD = input_.stride(1);\n+    // In the kernel, the batch and channel dimensions are treated as if they\n+    // are flattened and istrideD is used as the stride of this flattened dim\n+    // Handle the edge case where input_.size(1) == 1, where despite passing the\n+    // contiguity check the stride might not be T * H * W\n+    istrideD = isizeT * isizeH * isizeW;\n     istrideT = input_.stride(2);\n     istrideH = input_.stride(3);\n     istrideW = input_.stride(4);\n"
                }
            ],
            "whole_deleted": "-    istrideD = input_.stride(1);\n",
            "whole_added": "+    // In the kernel, the batch and channel dimensions are treated as if they\n+    // are flattened and istrideD is used as the stride of this flattened dim\n+    // Handle the edge case where input_.size(1) == 1, where despite passing the\n+    // contiguity check the stride might not be T * H * W\n+    istrideD = isizeT * isizeH * isizeW;\n",
            "whole_hunk": "@@ -346,7 +346,11 @@ TORCH_IMPL_FUNC(adaptive_max_pool3d_out_cuda)\n     isizeH = input_.size(3);\n     isizeW = input_.size(4);\n \n-    istrideD = input_.stride(1);\n+    // In the kernel, the batch and channel dimensions are treated as if they\n+    // are flattened and istrideD is used as the stride of this flattened dim\n+    // Handle the edge case where input_.size(1) == 1, where despite passing the\n+    // contiguity check the stride might not be T * H * W\n+    istrideD = isizeT * isizeH * isizeW;\n     istrideT = input_.stride(2);\n     istrideH = input_.stride(3);\n     istrideW = input_.stride(4);\n"
        },
        {
            "name": "test_pooling.py",
            "path": "test/nn/test_pooling.py",
            "patches": [
                {
                    "old_start": 1072,
                    "old_length": 38,
                    "new_start": 1072,
                    "new_length": 48,
                    "hunk": "@@ -1072,38 +1072,48 @@ torch.cuda.synchronize()\n \n     @dtypes(torch.float, torch.double)\n     def test_adaptive_pooling_max_nhwc(self, device, dtype):\n-        def helper(n, c, h, w, output_height, output_width, contig):\n-            input = torch.randint(1, 10, (n, c, h, w), device=device, dtype=dtype)\n-            input = input.contiguous(memory_format=torch.channels_last)\n-            grad = torch.randint(1, 10, (4, 8, output_height, output_width), device=device, dtype=dtype)\n-            grad = grad.contiguous(memory_format=torch.channels_last)\n+        def helper(input_size, output_plane_size, contig):\n+            n_plane_dims = len(output_plane_size)\n+            mod = torch.nn.AdaptiveMaxPool2d if n_plane_dims == 2 else torch.nn.AdaptiveMaxPool3d\n+            channels_last = torch.channels_last if n_plane_dims == 2 else torch.channels_last_3d\n+            output_size = input_size[:2] + output_plane_size\n+            input = torch.randint(1, 10, input_size, device=device, dtype=dtype)\n+            input = input.contiguous(memory_format=channels_last)\n+            grad = torch.randint(1, 10, output_size, device=device, dtype=dtype)\n+            grad = grad.contiguous(memory_format=channels_last)\n             if not contig:\n-                input = input[:, ::2, :, :]\n-                grad = grad[:, ::2, :, :]\n+                input = input[:, ::2]\n+                grad = grad[:, ::2]\n             input.requires_grad_(True)\n-            pool = torch.nn.AdaptiveMaxPool2d((output_height, output_width), return_indices=True).to(device)\n+            pool = mod(output_plane_size, return_indices=True).to(device)\n \n             ref_input = input.detach().clone().contiguous().requires_grad_(True)\n             ref_grad = grad.detach().clone().contiguous()\n-            ref_pool = torch.nn.AdaptiveMaxPool2d((output_height, output_width), return_indices=True).to(device)\n+            ref_pool = mod(output_plane_size, return_indices=True).to(device)\n \n             out, ind = pool(input)\n             out.backward(grad)\n             ref_out, ref_ind = ref_pool(ref_input)\n             ref_out.backward(ref_grad)\n \n-            self.assertTrue(out.is_contiguous(memory_format=torch.channels_last))\n+            # channels_last_3d case does not return channels_last_3d outputs\n+            if n_plane_dims == 2:\n+                self.assertTrue(out.is_contiguous(memory_format=channels_last))\n+                self.assertTrue(ind.is_contiguous(memory_format=channels_last))\n             self.assertTrue(ref_out.is_contiguous())\n-            self.assertTrue(ind.is_contiguous(memory_format=torch.channels_last))\n             self.assertTrue(ref_ind.is_contiguous())\n             self.assertEqual(out, ref_out)\n             self.assertEqual(ind, ref_ind)\n             self.assertEqual(input.grad, ref_input.grad)\n \n         for contig in [True, False]:\n-            helper(4, 8, 10, 10, 7, 7, contig)\n-            helper(4, 8, 9, 14, 5, 8, contig)\n-            helper(4, 8, 11, 11, 1, 1, contig)\n+            helper((4, 8, 10, 10), (7, 7), contig)\n+            helper((4, 8, 9, 14), (5, 8), contig)\n+            helper((4, 8, 11, 11), (1, 1), contig)\n+            helper((2, 1, 3, 3), (1, 1), contig)\n+            helper((4, 8, 10, 10, 10), (7, 7, 7), contig)\n+            helper((4, 8, 11, 11, 11), (1, 1, 1), contig)\n+            helper((2, 1, 3, 3, 3), (1, 1, 1), contig)\n \n     @dtypes(torch.float, torch.double)\n     def test_pooling_max_nhwc(self, device, dtype):"
                }
            ],
            "whole_deleted": "-        def helper(n, c, h, w, output_height, output_width, contig):\n-            input = torch.randint(1, 10, (n, c, h, w), device=device, dtype=dtype)\n-            input = input.contiguous(memory_format=torch.channels_last)\n-            grad = torch.randint(1, 10, (4, 8, output_height, output_width), device=device, dtype=dtype)\n-            grad = grad.contiguous(memory_format=torch.channels_last)\n-                input = input[:, ::2, :, :]\n-                grad = grad[:, ::2, :, :]\n-            pool = torch.nn.AdaptiveMaxPool2d((output_height, output_width), return_indices=True).to(device)\n-            ref_pool = torch.nn.AdaptiveMaxPool2d((output_height, output_width), return_indices=True).to(device)\n-            self.assertTrue(out.is_contiguous(memory_format=torch.channels_last))\n-            self.assertTrue(ind.is_contiguous(memory_format=torch.channels_last))\n-            helper(4, 8, 10, 10, 7, 7, contig)\n-            helper(4, 8, 9, 14, 5, 8, contig)\n-            helper(4, 8, 11, 11, 1, 1, contig)\n",
            "whole_added": "+        def helper(input_size, output_plane_size, contig):\n+            n_plane_dims = len(output_plane_size)\n+            mod = torch.nn.AdaptiveMaxPool2d if n_plane_dims == 2 else torch.nn.AdaptiveMaxPool3d\n+            channels_last = torch.channels_last if n_plane_dims == 2 else torch.channels_last_3d\n+            output_size = input_size[:2] + output_plane_size\n+            input = torch.randint(1, 10, input_size, device=device, dtype=dtype)\n+            input = input.contiguous(memory_format=channels_last)\n+            grad = torch.randint(1, 10, output_size, device=device, dtype=dtype)\n+            grad = grad.contiguous(memory_format=channels_last)\n+                input = input[:, ::2]\n+                grad = grad[:, ::2]\n+            pool = mod(output_plane_size, return_indices=True).to(device)\n+            ref_pool = mod(output_plane_size, return_indices=True).to(device)\n+            # channels_last_3d case does not return channels_last_3d outputs\n+            if n_plane_dims == 2:\n+                self.assertTrue(out.is_contiguous(memory_format=channels_last))\n+                self.assertTrue(ind.is_contiguous(memory_format=channels_last))\n+            helper((4, 8, 10, 10), (7, 7), contig)\n+            helper((4, 8, 9, 14), (5, 8), contig)\n+            helper((4, 8, 11, 11), (1, 1), contig)\n+            helper((2, 1, 3, 3), (1, 1), contig)\n+            helper((4, 8, 10, 10, 10), (7, 7, 7), contig)\n+            helper((4, 8, 11, 11, 11), (1, 1, 1), contig)\n+            helper((2, 1, 3, 3, 3), (1, 1, 1), contig)\n",
            "whole_hunk": "@@ -1072,38 +1072,48 @@ torch.cuda.synchronize()\n \n     @dtypes(torch.float, torch.double)\n     def test_adaptive_pooling_max_nhwc(self, device, dtype):\n-        def helper(n, c, h, w, output_height, output_width, contig):\n-            input = torch.randint(1, 10, (n, c, h, w), device=device, dtype=dtype)\n-            input = input.contiguous(memory_format=torch.channels_last)\n-            grad = torch.randint(1, 10, (4, 8, output_height, output_width), device=device, dtype=dtype)\n-            grad = grad.contiguous(memory_format=torch.channels_last)\n+        def helper(input_size, output_plane_size, contig):\n+            n_plane_dims = len(output_plane_size)\n+            mod = torch.nn.AdaptiveMaxPool2d if n_plane_dims == 2 else torch.nn.AdaptiveMaxPool3d\n+            channels_last = torch.channels_last if n_plane_dims == 2 else torch.channels_last_3d\n+            output_size = input_size[:2] + output_plane_size\n+            input = torch.randint(1, 10, input_size, device=device, dtype=dtype)\n+            input = input.contiguous(memory_format=channels_last)\n+            grad = torch.randint(1, 10, output_size, device=device, dtype=dtype)\n+            grad = grad.contiguous(memory_format=channels_last)\n             if not contig:\n-                input = input[:, ::2, :, :]\n-                grad = grad[:, ::2, :, :]\n+                input = input[:, ::2]\n+                grad = grad[:, ::2]\n             input.requires_grad_(True)\n-            pool = torch.nn.AdaptiveMaxPool2d((output_height, output_width), return_indices=True).to(device)\n+            pool = mod(output_plane_size, return_indices=True).to(device)\n \n             ref_input = input.detach().clone().contiguous().requires_grad_(True)\n             ref_grad = grad.detach().clone().contiguous()\n-            ref_pool = torch.nn.AdaptiveMaxPool2d((output_height, output_width), return_indices=True).to(device)\n+            ref_pool = mod(output_plane_size, return_indices=True).to(device)\n \n             out, ind = pool(input)\n             out.backward(grad)\n             ref_out, ref_ind = ref_pool(ref_input)\n             ref_out.backward(ref_grad)\n \n-            self.assertTrue(out.is_contiguous(memory_format=torch.channels_last))\n+            # channels_last_3d case does not return channels_last_3d outputs\n+            if n_plane_dims == 2:\n+                self.assertTrue(out.is_contiguous(memory_format=channels_last))\n+                self.assertTrue(ind.is_contiguous(memory_format=channels_last))\n             self.assertTrue(ref_out.is_contiguous())\n-            self.assertTrue(ind.is_contiguous(memory_format=torch.channels_last))\n             self.assertTrue(ref_ind.is_contiguous())\n             self.assertEqual(out, ref_out)\n             self.assertEqual(ind, ref_ind)\n             self.assertEqual(input.grad, ref_input.grad)\n \n         for contig in [True, False]:\n-            helper(4, 8, 10, 10, 7, 7, contig)\n-            helper(4, 8, 9, 14, 5, 8, contig)\n-            helper(4, 8, 11, 11, 1, 1, contig)\n+            helper((4, 8, 10, 10), (7, 7), contig)\n+            helper((4, 8, 9, 14), (5, 8), contig)\n+            helper((4, 8, 11, 11), (1, 1), contig)\n+            helper((2, 1, 3, 3), (1, 1), contig)\n+            helper((4, 8, 10, 10, 10), (7, 7, 7), contig)\n+            helper((4, 8, 11, 11, 11), (1, 1, 1), contig)\n+            helper((2, 1, 3, 3, 3), (1, 1, 1), contig)\n \n     @dtypes(torch.float, torch.double)\n     def test_pooling_max_nhwc(self, device, dtype):"
        }
    ]
},
{
    "Id": 429,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/ae2c219de2fd032036aa1d2a04101f1c23fd5bbe",
    "date": "2023-11-10T20:46:13+00:00",
    "message": "Revert \"[BE] Remove stale CUDA version check from cpp_extension.py (#113447)\"\n\nThis reverts commit 7ccca60927cdccde63d6a1d40480950f24e9877a.\n\nReverted https://github.com/pytorch/pytorch/pull/113447 on behalf of https://github.com/malfet due to Broke ROCM ([comment](https://github.com/pytorch/pytorch/pull/113447#issuecomment-1806407892))",
    "label": "YES",
    "changes": [
        {
            "name": "cpp_extension.py",
            "path": "torch/utils/cpp_extension.py",
            "patches": [
                {
                    "old_start": 2344,
                    "old_length": 12,
                    "new_start": 2344,
                    "new_length": 17,
                    "hunk": "@@ -2344,12 +2344,17 @@ def _write_ninja_file(path,\n     if with_cuda:\n         cuda_compile_rule = ['rule cuda_compile']\n         nvcc_gendeps = ''\n-        cuda_compile_rule.append('  depfile = $out.d')\n-        cuda_compile_rule.append('  deps = gcc')\n-        # Note: non-system deps with nvcc are only supported\n-        # on Linux so use --generate-dependencies-with-compile\n-        # to make this work on Windows too.\n-        nvcc_gendeps = '--generate-dependencies-with-compile --dependency-output $out.d'\n+        # --generate-dependencies-with-compile was added in CUDA 10.2.\n+        # Compilation will work on earlier CUDA versions but header file\n+        # dependencies are not correctly computed.\n+        required_cuda_version = '11.0'\n+        if torch.version.cuda is not None and TorchVersion(torch.version.cuda) >= required_cuda_version:\n+            cuda_compile_rule.append('  depfile = $out.d')\n+            cuda_compile_rule.append('  deps = gcc')\n+            # Note: non-system deps with nvcc are only supported\n+            # on Linux so use --generate-dependencies-with-compile\n+            # to make this work on Windows too.\n+            nvcc_gendeps = '--generate-dependencies-with-compile --dependency-output $out.d'\n         cuda_compile_rule.append(\n             f'  command = $nvcc {nvcc_gendeps} $cuda_cflags -c $in -o $out $cuda_post_cflags')\n "
                }
            ],
            "whole_deleted": "-        cuda_compile_rule.append('  depfile = $out.d')\n-        cuda_compile_rule.append('  deps = gcc')\n-        # Note: non-system deps with nvcc are only supported\n-        # on Linux so use --generate-dependencies-with-compile\n-        # to make this work on Windows too.\n-        nvcc_gendeps = '--generate-dependencies-with-compile --dependency-output $out.d'\n",
            "whole_added": "+        # --generate-dependencies-with-compile was added in CUDA 10.2.\n+        # Compilation will work on earlier CUDA versions but header file\n+        # dependencies are not correctly computed.\n+        required_cuda_version = '11.0'\n+        if torch.version.cuda is not None and TorchVersion(torch.version.cuda) >= required_cuda_version:\n+            cuda_compile_rule.append('  depfile = $out.d')\n+            cuda_compile_rule.append('  deps = gcc')\n+            # Note: non-system deps with nvcc are only supported\n+            # on Linux so use --generate-dependencies-with-compile\n+            # to make this work on Windows too.\n+            nvcc_gendeps = '--generate-dependencies-with-compile --dependency-output $out.d'\n",
            "whole_hunk": "@@ -2344,12 +2344,17 @@ def _write_ninja_file(path,\n     if with_cuda:\n         cuda_compile_rule = ['rule cuda_compile']\n         nvcc_gendeps = ''\n-        cuda_compile_rule.append('  depfile = $out.d')\n-        cuda_compile_rule.append('  deps = gcc')\n-        # Note: non-system deps with nvcc are only supported\n-        # on Linux so use --generate-dependencies-with-compile\n-        # to make this work on Windows too.\n-        nvcc_gendeps = '--generate-dependencies-with-compile --dependency-output $out.d'\n+        # --generate-dependencies-with-compile was added in CUDA 10.2.\n+        # Compilation will work on earlier CUDA versions but header file\n+        # dependencies are not correctly computed.\n+        required_cuda_version = '11.0'\n+        if torch.version.cuda is not None and TorchVersion(torch.version.cuda) >= required_cuda_version:\n+            cuda_compile_rule.append('  depfile = $out.d')\n+            cuda_compile_rule.append('  deps = gcc')\n+            # Note: non-system deps with nvcc are only supported\n+            # on Linux so use --generate-dependencies-with-compile\n+            # to make this work on Windows too.\n+            nvcc_gendeps = '--generate-dependencies-with-compile --dependency-output $out.d'\n         cuda_compile_rule.append(\n             f'  command = $nvcc {nvcc_gendeps} $cuda_cflags -c $in -o $out $cuda_post_cflags')\n "
        }
    ]
},
{
    "Id": 472,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/04b04c068659127a53d659c44b0dd75fa9fd5887",
    "date": "2023-10-16T21:16:17+00:00",
    "message": "[Compiled Autograd] Turn accumulate_grad into an op (#111271)\n\nRather than baking the behavior of `AccumulateGrad` nodes into the generated graph (either as `+=`, or as a return value of the graph).  This creates a new `accumulate_grad_` dispatcher op that is included in the generated graph like:\n```\ndef forward(self, inputs, sizes, hooks):\n    getitem = inputs[0]\n    getitem_1 = inputs[1]\n    getitem_2 = inputs[2]\n    getitem_3 = inputs[3]\n    getitem_4 = inputs[4]\n    getitem_5 = inputs[5]\n    getitem_6 = inputs[6]\n    getitem_7 = inputs[7]\n    getitem_8 = inputs[8]\n    getitem_9 = inputs[9];  inputs = None\n    expand = torch.ops.aten.expand.default(getitem, [2, 4]);  getitem = None\n    threshold_backward = torch.ops.aten.threshold_backward.default(expand, getitem_1, 0);  expand = getitem_1 = None\n    t = torch.ops.aten.t.default(getitem_3);  getitem_3 = None\n    mm = torch.ops.aten.mm.default(threshold_backward, t);  t = None\n    t_1 = torch.ops.aten.t.default(threshold_backward)\n    mm_1 = torch.ops.aten.mm.default(t_1, getitem_2);  t_1 = getitem_2 = None\n    t_2 = torch.ops.aten.t.default(mm_1);  mm_1 = None\n    sum_1 = torch.ops.aten.sum.dim_IntList(threshold_backward, [0], True);  threshold_backward = None\n    view = torch.ops.aten.view.default(sum_1, [4]);  sum_1 = None\n    t_3 = torch.ops.aten.t.default(t_2);  t_2 = None\n    accumulate_grad_ = torch.ops.inductor.accumulate_grad_.default(getitem_4, t_3);  getitem_4 = t_3 = None\n    threshold_backward_1 = torch.ops.aten.threshold_backward.default(mm, getitem_5, 0);  mm = getitem_5 = None\n    t_4 = torch.ops.aten.t.default(threshold_backward_1)\n    mm_2 = torch.ops.aten.mm.default(t_4, getitem_6);  t_4 = getitem_6 = None\n    t_5 = torch.ops.aten.t.default(mm_2);  mm_2 = None\n    sum_2 = torch.ops.aten.sum.dim_IntList(threshold_backward_1, [0], True);  threshold_backward_1 = None\n    view_1 = torch.ops.aten.view.default(sum_2, [4]);  sum_2 = None\n    t_6 = torch.ops.aten.t.default(t_5);  t_5 = None\n    accumulate_grad__1 = torch.ops.inductor.accumulate_grad_.default(getitem_7, t_6);  getitem_7 = t_6 = None\n    accumulate_grad__2 = torch.ops.inductor.accumulate_grad_.default(getitem_8, view_1);  getitem_8 = view_1 = None\n    accumulate_grad__3 = torch.ops.inductor.accumulate_grad_.default(getitem_9, view);  getitem_9 = view = None\n    return []\n\n```\n\nThe motivation here is `AccumulateGrad` nodes are causing trouble in FSDP tracing, since FSDP is in-place resizing parameters and parameter storage in hooks.  We will model this mutation in dynamo, but not during the initial compiled autograd capture.  This allows us to bypass failing shape checks in the initial capture.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111271\nApproved by: https://github.com/voznesenskym",
    "label": "NO",
    "changes": [
        {
            "name": "wrapper.py",
            "path": "torch/_inductor/codegen/wrapper.py",
            "patches": [
                {
                    "old_start": 349,
                    "old_length": 6,
                    "new_start": 349,
                    "new_length": 7,
                    "hunk": "@@ -349,6 +349,7 @@ class WrapperCodeGen(CodeGen):\n                 from torch._inductor.select_algorithm import extern_kernels\n \n                 aten = torch.ops.aten\n+                inductor_ops = torch.ops.inductor\n                 assert_size_stride = torch._C._dynamo.guards.assert_size_stride\n                 reinterpret_tensor = torch.ops.inductor._reinterpret_tensor\n                 async_compile = AsyncCompile()\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+                inductor_ops = torch.ops.inductor\n",
            "whole_hunk": "@@ -349,6 +349,7 @@ class WrapperCodeGen(CodeGen):\n                 from torch._inductor.select_algorithm import extern_kernels\n \n                 aten = torch.ops.aten\n+                inductor_ops = torch.ops.inductor\n                 assert_size_stride = torch._C._dynamo.guards.assert_size_stride\n                 reinterpret_tensor = torch.ops.inductor._reinterpret_tensor\n                 async_compile = AsyncCompile()\n"
        },
        {
            "name": "ir.py",
            "path": "torch/_inductor/ir.py",
            "patches": [
                {
                    "old_start": 3602,
                    "old_length": 6,
                    "new_start": 3602,
                    "new_length": 33,
                    "hunk": "@@ -3602,6 +3602,33 @@ class InplaceBernoulliFallback(ExternKernel):\n         self.name = V.graph.register_buffer(self)\n \n \n+class AccumulateGrad(ExternKernel):\n+    \"\"\"\n+    This needs to be a custom class to handle mutation properly\n+    \"\"\"\n+\n+    kernel = \"inductor_ops.accumulate_grad_\"\n+\n+    def codegen(self, wrapper):\n+        (variable, new_grad) = (t.codegen_reference() for t in self.inputs)\n+        wrapper.writeline(f\"{self.kernel}({variable}, {new_grad})\")\n+\n+    def should_allocate(self):\n+        return False\n+\n+    def get_mutation_names(self):\n+        assert isinstance(self.layout, MutationLayout)\n+        return (self.layout.target.get_name(),)\n+\n+    def __init__(self, variable, new_grad):\n+        super().__init__(\n+            None,\n+            MutationLayout(variable),\n+            self.unwrap_storage([variable, new_grad]),\n+        )\n+        self.name = V.graph.register_buffer(self)\n+\n+\n class ScatterFallback(ExternKernel):\n     \"\"\"\n     This needs to be a custom class to handle mutation properly.\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+class AccumulateGrad(ExternKernel):\n+    \"\"\"\n+    This needs to be a custom class to handle mutation properly\n+    \"\"\"\n+\n+    kernel = \"inductor_ops.accumulate_grad_\"\n+\n+    def codegen(self, wrapper):\n+        (variable, new_grad) = (t.codegen_reference() for t in self.inputs)\n+        wrapper.writeline(f\"{self.kernel}({variable}, {new_grad})\")\n+\n+    def should_allocate(self):\n+        return False\n+\n+    def get_mutation_names(self):\n+        assert isinstance(self.layout, MutationLayout)\n+        return (self.layout.target.get_name(),)\n+\n+    def __init__(self, variable, new_grad):\n+        super().__init__(\n+            None,\n+            MutationLayout(variable),\n+            self.unwrap_storage([variable, new_grad]),\n+        )\n+        self.name = V.graph.register_buffer(self)\n+\n+\n",
            "whole_hunk": "@@ -3602,6 +3602,33 @@ class InplaceBernoulliFallback(ExternKernel):\n         self.name = V.graph.register_buffer(self)\n \n \n+class AccumulateGrad(ExternKernel):\n+    \"\"\"\n+    This needs to be a custom class to handle mutation properly\n+    \"\"\"\n+\n+    kernel = \"inductor_ops.accumulate_grad_\"\n+\n+    def codegen(self, wrapper):\n+        (variable, new_grad) = (t.codegen_reference() for t in self.inputs)\n+        wrapper.writeline(f\"{self.kernel}({variable}, {new_grad})\")\n+\n+    def should_allocate(self):\n+        return False\n+\n+    def get_mutation_names(self):\n+        assert isinstance(self.layout, MutationLayout)\n+        return (self.layout.target.get_name(),)\n+\n+    def __init__(self, variable, new_grad):\n+        super().__init__(\n+            None,\n+            MutationLayout(variable),\n+            self.unwrap_storage([variable, new_grad]),\n+        )\n+        self.name = V.graph.register_buffer(self)\n+\n+\n class ScatterFallback(ExternKernel):\n     \"\"\"\n     This needs to be a custom class to handle mutation properly.\n"
        },
        {
            "name": "lowering.py",
            "path": "torch/_inductor/lowering.py",
            "patches": [
                {
                    "old_start": 4840,
                    "old_length": 6,
                    "new_start": 4840,
                    "new_length": 15,
                    "hunk": "@@ -4840,6 +4840,15 @@ def _realize(x):\n     return clone(x)\n \n \n+@register_lowering(torch.ops.inductor.accumulate_grad_)\n+def accumulate_grad_(variable, new_grad):\n+    # TODO(jansel): decompose into `variable.grad += new_grad` when variable.grad is defined\n+    variable.realize()\n+    new_grad.realize()\n+    ir.AccumulateGrad(variable, new_grad)\n+    return variable\n+\n+\n try:\n     import torch.distributed._functional_collectives\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+@register_lowering(torch.ops.inductor.accumulate_grad_)\n+def accumulate_grad_(variable, new_grad):\n+    # TODO(jansel): decompose into `variable.grad += new_grad` when variable.grad is defined\n+    variable.realize()\n+    new_grad.realize()\n+    ir.AccumulateGrad(variable, new_grad)\n+    return variable\n+\n+\n",
            "whole_hunk": "@@ -4840,6 +4840,15 @@ def _realize(x):\n     return clone(x)\n \n \n+@register_lowering(torch.ops.inductor.accumulate_grad_)\n+def accumulate_grad_(variable, new_grad):\n+    # TODO(jansel): decompose into `variable.grad += new_grad` when variable.grad is defined\n+    variable.realize()\n+    new_grad.realize()\n+    ir.AccumulateGrad(variable, new_grad)\n+    return variable\n+\n+\n try:\n     import torch.distributed._functional_collectives\n \n"
        },
        {
            "name": "accumulate_grad.cpp",
            "path": "torch/csrc/autograd/functions/accumulate_grad.cpp",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 5,
                    "new_start": 1,
                    "new_length": 6,
                    "hunk": "@@ -1,5 +1,6 @@\n #include <torch/csrc/autograd/functions/accumulate_grad.h>\n \n+#include <ATen/core/dispatch/Dispatcher.h>\n #include <torch/csrc/autograd/functions/basic_ops.h>\n #include <torch/csrc/autograd/functions/tensor.h>\n #include <torch/csrc/autograd/functions/utils.h>\n"
                },
                {
                    "old_start": 83,
                    "old_length": 14,
                    "new_start": 84,
                    "new_length": 12,
                    "hunk": "@@ -83,14 +84,12 @@ variable_list AccumulateGrad::apply_with_saved(\n   at::Tensor grad_copy = variable.grad();\n   saved.before(variable_copy);\n   saved.before(grad_copy);\n-  accumulateGrad(\n-      variable_copy,\n-      grad_copy,\n-      grads[0],\n-      0 /* num_expected_refs, 0 disables aliased reuse */,\n-      [&saved, this](const at::Tensor& grad_update) {\n-        saved.assign_mutable_grad(variable, grad_update);\n-      });\n+  variable_copy.mutable_grad() = grad_copy;\n+  // op is intentionally static\n+  static auto op = c10::Dispatcher::singleton()\n+                       .findSchemaOrThrow(\"inductor::accumulate_grad_\", \"\")\n+                       .typed<void(const at::Tensor&, const at::Tensor&)>();\n+  op.call(variable_copy, grads[0]);\n   saved.after(variable_copy);\n   saved.after(grad_copy);\n \n"
                }
            ],
            "whole_deleted": "-  accumulateGrad(\n-      variable_copy,\n-      grad_copy,\n-      grads[0],\n-      0 /* num_expected_refs, 0 disables aliased reuse */,\n-      [&saved, this](const at::Tensor& grad_update) {\n-        saved.assign_mutable_grad(variable, grad_update);\n-      });\n",
            "whole_added": "+#include <ATen/core/dispatch/Dispatcher.h>\n+  variable_copy.mutable_grad() = grad_copy;\n+  // op is intentionally static\n+  static auto op = c10::Dispatcher::singleton()\n+                       .findSchemaOrThrow(\"inductor::accumulate_grad_\", \"\")\n+                       .typed<void(const at::Tensor&, const at::Tensor&)>();\n+  op.call(variable_copy, grads[0]);\n",
            "whole_hunk": "@@ -1,5 +1,6 @@\n #include <torch/csrc/autograd/functions/accumulate_grad.h>\n \n+#include <ATen/core/dispatch/Dispatcher.h>\n #include <torch/csrc/autograd/functions/basic_ops.h>\n #include <torch/csrc/autograd/functions/tensor.h>\n #include <torch/csrc/autograd/functions/utils.h>\n@@ -83,14 +84,12 @@ variable_list AccumulateGrad::apply_with_saved(\n   at::Tensor grad_copy = variable.grad();\n   saved.before(variable_copy);\n   saved.before(grad_copy);\n-  accumulateGrad(\n-      variable_copy,\n-      grad_copy,\n-      grads[0],\n-      0 /* num_expected_refs, 0 disables aliased reuse */,\n-      [&saved, this](const at::Tensor& grad_update) {\n-        saved.assign_mutable_grad(variable, grad_update);\n-      });\n+  variable_copy.mutable_grad() = grad_copy;\n+  // op is intentionally static\n+  static auto op = c10::Dispatcher::singleton()\n+                       .findSchemaOrThrow(\"inductor::accumulate_grad_\", \"\")\n+                       .typed<void(const at::Tensor&, const at::Tensor&)>();\n+  op.call(variable_copy, grads[0]);\n   saved.after(variable_copy);\n   saved.after(grad_copy);\n \n"
        },
        {
            "name": "compiled_autograd.h",
            "path": "torch/csrc/dynamo/compiled_autograd.h",
            "patches": [
                {
                    "old_start": 469,
                    "old_length": 7,
                    "new_start": 469,
                    "new_length": 6,
                    "hunk": "@@ -469,7 +469,6 @@ struct TraceState {\n   size_t sym_sizes_index;\n   std::vector<c10::optional<c10::SymInt>> sym_sizes;\n   variable_list outputs;\n-  std::vector<size_t> output_grad_targets;\n };\n \n class SwapSavedVariables {\n"
                },
                {
                    "old_start": 620,
                    "old_length": 17,
                    "new_start": 619,
                    "new_length": 6,
                    "hunk": "@@ -620,17 +619,6 @@ class SwapSavedVariables {\n   NO_OP_VISIT(double);\n #undef NO_OP_VISIT\n \n-  // record the need to run `dst.mutable_grad() = src` after the graph\n-  // dst is a real tensor, src is a fake tensor\n-  void assign_mutable_grad(const at::Tensor& dst, const at::Tensor& src) {\n-    const TensorArg& arg = compiler.tensor_args.lookup(dst);\n-    TORCH_INTERNAL_ASSERT(arg.defined());\n-    TORCH_INTERNAL_ASSERT(\n-        state.outputs.size() == state.output_grad_targets.size());\n-    state.outputs.emplace_back(src);\n-    state.output_grad_targets.emplace_back(arg.index());\n-  }\n-\n   SwapSavedVariables(AutogradCompilerCall& c, TraceState& s)\n       : compiler(c), state(s) {}\n \n"
                }
            ],
            "whole_deleted": "-  std::vector<size_t> output_grad_targets;\n-  // record the need to run `dst.mutable_grad() = src` after the graph\n-  // dst is a real tensor, src is a fake tensor\n-  void assign_mutable_grad(const at::Tensor& dst, const at::Tensor& src) {\n-    const TensorArg& arg = compiler.tensor_args.lookup(dst);\n-    TORCH_INTERNAL_ASSERT(arg.defined());\n-    TORCH_INTERNAL_ASSERT(\n-        state.outputs.size() == state.output_grad_targets.size());\n-    state.outputs.emplace_back(src);\n-    state.output_grad_targets.emplace_back(arg.index());\n-  }\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -469,7 +469,6 @@ struct TraceState {\n   size_t sym_sizes_index;\n   std::vector<c10::optional<c10::SymInt>> sym_sizes;\n   variable_list outputs;\n-  std::vector<size_t> output_grad_targets;\n };\n \n class SwapSavedVariables {\n@@ -620,17 +619,6 @@ class SwapSavedVariables {\n   NO_OP_VISIT(double);\n #undef NO_OP_VISIT\n \n-  // record the need to run `dst.mutable_grad() = src` after the graph\n-  // dst is a real tensor, src is a fake tensor\n-  void assign_mutable_grad(const at::Tensor& dst, const at::Tensor& src) {\n-    const TensorArg& arg = compiler.tensor_args.lookup(dst);\n-    TORCH_INTERNAL_ASSERT(arg.defined());\n-    TORCH_INTERNAL_ASSERT(\n-        state.outputs.size() == state.output_grad_targets.size());\n-    state.outputs.emplace_back(src);\n-    state.output_grad_targets.emplace_back(arg.index());\n-  }\n-\n   SwapSavedVariables(AutogradCompilerCall& c, TraceState& s)\n       : compiler(c), state(s) {}\n \n"
        },
        {
            "name": "python_compiled_autograd.cpp",
            "path": "torch/csrc/dynamo/python_compiled_autograd.cpp",
            "patches": [
                {
                    "old_start": 104,
                    "old_length": 7,
                    "new_start": 104,
                    "new_length": 6,
                    "hunk": "@@ -104,7 +104,6 @@ struct CacheNode {\n     next.clear();\n     key_storage.clear();\n     expected_sizes.clear();\n-    output_grad_targets.clear();\n     compiled_fn = nullptr;\n   }\n \n"
                },
                {
                    "old_start": 208,
                    "old_length": 9,
                    "new_start": 207,
                    "new_length": 6,
                    "hunk": "@@ -208,9 +207,6 @@ struct CacheNode {\n   std::vector<SizeInput> expected_sizes;\n \n   THPObjectPtr compiled_fn;\n-  // Maps each return value of compiled_fn to an input index.  After the graph\n-  // runs we do: `inputs[output_grad_targets[i]].mutable_grad() = outputs[i]`\n-  std::vector<size_t> output_grad_targets;\n };\n \n struct InputBuffers : public std::unordered_map<Node*, InputBuffer> {\n"
                },
                {
                    "old_start": 463,
                    "old_length": 7,
                    "new_start": 459,
                    "new_length": 6,
                    "hunk": "@@ -463,7 +459,6 @@ variable_list compiled_autograd(\n     }\n \n     cache->compiled_fn = check(call_end_capture(py_compiler, state.outputs));\n-    cache->output_grad_targets = std::move(state.output_grad_targets);\n     state.debug_asserts();\n   } // End cache miss region\n \n"
                },
                {
                    "old_start": 482,
                    "old_length": 21,
                    "new_start": 477,
                    "new_length": 8,
                    "hunk": "@@ -482,21 +477,8 @@ variable_list compiled_autograd(\n   THPObjectPtr pyresult(check(PyObject_CallFunctionObjArgs(\n       cache->compiled_fn.get(), inputs.get(), sizes.get(), hooks.get(), NULL)));\n   variable_list outputs = THPVariable_UnpackList(pyresult);\n-  if (accumulate_grad) {\n-    TORCH_INTERNAL_ASSERT(outputs.size() == cache->output_grad_targets.size());\n-    for (const auto i : c10::irange(outputs.size())) {\n-      // Here we set the `var.grad = ...` for each call to\n-      // `saved.assign_mutable_grad(var, ...)`.  For the case on inplace grad\n-      // accumuation there will be an `add_` op in the graph and no return\n-      // value.\n-      compiler_call.tensor_args.inputs[cache->output_grad_targets[i]]\n-          .mutable_grad() = outputs[i];\n-    }\n-    return variable_list();\n-  } else {\n-    TORCH_INTERNAL_ASSERT(outputs.size() == output_edges.size());\n-    return outputs;\n-  }\n+  TORCH_INTERNAL_ASSERT(outputs.size() == output_edges.size());\n+  return outputs;\n }\n \n static PyObject* set_autograd_compiler(PyObject* dummy, PyObject* args) {\n"
                }
            ],
            "whole_deleted": "-    output_grad_targets.clear();\n-  // Maps each return value of compiled_fn to an input index.  After the graph\n-  // runs we do: `inputs[output_grad_targets[i]].mutable_grad() = outputs[i]`\n-  std::vector<size_t> output_grad_targets;\n-    cache->output_grad_targets = std::move(state.output_grad_targets);\n-  if (accumulate_grad) {\n-    TORCH_INTERNAL_ASSERT(outputs.size() == cache->output_grad_targets.size());\n-    for (const auto i : c10::irange(outputs.size())) {\n-      // Here we set the `var.grad = ...` for each call to\n-      // `saved.assign_mutable_grad(var, ...)`.  For the case on inplace grad\n-      // accumuation there will be an `add_` op in the graph and no return\n-      // value.\n-      compiler_call.tensor_args.inputs[cache->output_grad_targets[i]]\n-          .mutable_grad() = outputs[i];\n-    }\n-    return variable_list();\n-  } else {\n-    TORCH_INTERNAL_ASSERT(outputs.size() == output_edges.size());\n-    return outputs;\n-  }\n",
            "whole_added": "+  TORCH_INTERNAL_ASSERT(outputs.size() == output_edges.size());\n+  return outputs;\n",
            "whole_hunk": "@@ -104,7 +104,6 @@ struct CacheNode {\n     next.clear();\n     key_storage.clear();\n     expected_sizes.clear();\n-    output_grad_targets.clear();\n     compiled_fn = nullptr;\n   }\n \n@@ -208,9 +207,6 @@ struct CacheNode {\n   std::vector<SizeInput> expected_sizes;\n \n   THPObjectPtr compiled_fn;\n-  // Maps each return value of compiled_fn to an input index.  After the graph\n-  // runs we do: `inputs[output_grad_targets[i]].mutable_grad() = outputs[i]`\n-  std::vector<size_t> output_grad_targets;\n };\n \n struct InputBuffers : public std::unordered_map<Node*, InputBuffer> {\n@@ -463,7 +459,6 @@ variable_list compiled_autograd(\n     }\n \n     cache->compiled_fn = check(call_end_capture(py_compiler, state.outputs));\n-    cache->output_grad_targets = std::move(state.output_grad_targets);\n     state.debug_asserts();\n   } // End cache miss region\n \n@@ -482,21 +477,8 @@ variable_list compiled_autograd(\n   THPObjectPtr pyresult(check(PyObject_CallFunctionObjArgs(\n       cache->compiled_fn.get(), inputs.get(), sizes.get(), hooks.get(), NULL)));\n   variable_list outputs = THPVariable_UnpackList(pyresult);\n-  if (accumulate_grad) {\n-    TORCH_INTERNAL_ASSERT(outputs.size() == cache->output_grad_targets.size());\n-    for (const auto i : c10::irange(outputs.size())) {\n-      // Here we set the `var.grad = ...` for each call to\n-      // `saved.assign_mutable_grad(var, ...)`.  For the case on inplace grad\n-      // accumuation there will be an `add_` op in the graph and no return\n-      // value.\n-      compiler_call.tensor_args.inputs[cache->output_grad_targets[i]]\n-          .mutable_grad() = outputs[i];\n-    }\n-    return variable_list();\n-  } else {\n-    TORCH_INTERNAL_ASSERT(outputs.size() == output_edges.size());\n-    return outputs;\n-  }\n+  TORCH_INTERNAL_ASSERT(outputs.size() == output_edges.size());\n+  return outputs;\n }\n \n static PyObject* set_autograd_compiler(PyObject* dummy, PyObject* args) {\n"
        },
        {
            "name": "inductor_ops.cpp",
            "path": "torch/csrc/inductor/inductor_ops.cpp",
            "patches": [
                {
                    "old_start": 4,
                    "old_length": 6,
                    "new_start": 4,
                    "new_length": 7,
                    "hunk": "@@ -4,6 +4,7 @@\n #include <ATen/ops/mm.h>\n #endif\n \n+#include <torch/csrc/autograd/functions/accumulate_grad.h>\n #include <torch/csrc/inductor/inductor_ops.h>\n #include <torch/library.h>\n \n"
                },
                {
                    "old_start": 38,
                    "old_length": 6,
                    "new_start": 39,
                    "new_length": 23,
                    "hunk": "@@ -38,6 +39,23 @@ Tensor _reinterpret_tensor(\n   return self_;\n }\n \n+static void accumulate_grad_(const Tensor& variable, const Tensor& new_grad) {\n+  at::Tensor& grad = variable.mutable_grad();\n+  if (new_grad.device() != kMeta) {\n+    torch::autograd::AccumulateGrad::accumulateGrad(\n+        variable,\n+        grad,\n+        new_grad,\n+        1 /* num_expected_refs */,\n+        [&grad](at::Tensor&& grad_update) { grad = std::move(grad_update); });\n+  } else {\n+    // no shape checking for `device=\"meta\"` to workaround FSDP inplace mutation\n+    if (!grad.defined()) {\n+      grad = new_grad;\n+    }\n+  }\n+}\n+\n TORCH_LIBRARY_FRAGMENT(inductor, m) {\n   m.def(\n       \"_mm_plus_mm(Tensor a, Tensor b, Tensor c, Tensor d, Tensor(t!) out) -> Tensor(t!)\",\n"
                },
                {
                    "old_start": 45,
                    "old_length": 6,
                    "new_start": 63,
                    "new_length": 9,
                    "hunk": "@@ -45,6 +63,9 @@ TORCH_LIBRARY_FRAGMENT(inductor, m) {\n   m.def(\n       \"_reinterpret_tensor(Tensor self, int[] size, int[] stride, int offset_increment=0) -> Tensor\",\n       _reinterpret_tensor);\n+  m.def(\n+      \"accumulate_grad_(Tensor variable, Tensor new_grad) -> ()\",\n+      dispatch(c10::DispatchKey::CompositeExplicitAutograd, accumulate_grad_));\n }\n \n } // namespace inductor\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include <torch/csrc/autograd/functions/accumulate_grad.h>\n+static void accumulate_grad_(const Tensor& variable, const Tensor& new_grad) {\n+  at::Tensor& grad = variable.mutable_grad();\n+  if (new_grad.device() != kMeta) {\n+    torch::autograd::AccumulateGrad::accumulateGrad(\n+        variable,\n+        grad,\n+        new_grad,\n+        1 /* num_expected_refs */,\n+        [&grad](at::Tensor&& grad_update) { grad = std::move(grad_update); });\n+  } else {\n+    // no shape checking for `device=\"meta\"` to workaround FSDP inplace mutation\n+    if (!grad.defined()) {\n+      grad = new_grad;\n+    }\n+  }\n+}\n+\n+  m.def(\n+      \"accumulate_grad_(Tensor variable, Tensor new_grad) -> ()\",\n+      dispatch(c10::DispatchKey::CompositeExplicitAutograd, accumulate_grad_));\n",
            "whole_hunk": "@@ -4,6 +4,7 @@\n #include <ATen/ops/mm.h>\n #endif\n \n+#include <torch/csrc/autograd/functions/accumulate_grad.h>\n #include <torch/csrc/inductor/inductor_ops.h>\n #include <torch/library.h>\n \n@@ -38,6 +39,23 @@ Tensor _reinterpret_tensor(\n   return self_;\n }\n \n+static void accumulate_grad_(const Tensor& variable, const Tensor& new_grad) {\n+  at::Tensor& grad = variable.mutable_grad();\n+  if (new_grad.device() != kMeta) {\n+    torch::autograd::AccumulateGrad::accumulateGrad(\n+        variable,\n+        grad,\n+        new_grad,\n+        1 /* num_expected_refs */,\n+        [&grad](at::Tensor&& grad_update) { grad = std::move(grad_update); });\n+  } else {\n+    // no shape checking for `device=\"meta\"` to workaround FSDP inplace mutation\n+    if (!grad.defined()) {\n+      grad = new_grad;\n+    }\n+  }\n+}\n+\n TORCH_LIBRARY_FRAGMENT(inductor, m) {\n   m.def(\n       \"_mm_plus_mm(Tensor a, Tensor b, Tensor c, Tensor d, Tensor(t!) out) -> Tensor(t!)\",\n@@ -45,6 +63,9 @@ TORCH_LIBRARY_FRAGMENT(inductor, m) {\n   m.def(\n       \"_reinterpret_tensor(Tensor self, int[] size, int[] stride, int offset_increment=0) -> Tensor\",\n       _reinterpret_tensor);\n+  m.def(\n+      \"accumulate_grad_(Tensor variable, Tensor new_grad) -> ()\",\n+      dispatch(c10::DispatchKey::CompositeExplicitAutograd, accumulate_grad_));\n }\n \n } // namespace inductor\n"
        },
        {
            "name": "node.py",
            "path": "torch/fx/node.py",
            "patches": [
                {
                    "old_start": 39,
                    "old_length": 7,
                    "new_start": 39,
                    "new_length": 9,
                    "hunk": "@@ -39,7 +39,9 @@ _side_effectful_functions: Set[Callable] = {\n     _ops.aten.sym_constrain_range_for_size.default,\n     _ops.profiler._record_function_enter,\n     _ops.profiler._record_function_enter_new,\n-    _ops.profiler._record_function_exit}\n+    _ops.profiler._record_function_exit,\n+    _ops.inductor.accumulate_grad_.default,\n+}\n \n \n @compatibility(is_backward_compatible=False)"
                }
            ],
            "whole_deleted": "-    _ops.profiler._record_function_exit}\n",
            "whole_added": "+    _ops.profiler._record_function_exit,\n+    _ops.inductor.accumulate_grad_.default,\n+}\n",
            "whole_hunk": "@@ -39,7 +39,9 @@ _side_effectful_functions: Set[Callable] = {\n     _ops.aten.sym_constrain_range_for_size.default,\n     _ops.profiler._record_function_enter,\n     _ops.profiler._record_function_enter_new,\n-    _ops.profiler._record_function_exit}\n+    _ops.profiler._record_function_exit,\n+    _ops.inductor.accumulate_grad_.default,\n+}\n \n \n @compatibility(is_backward_compatible=False)"
        }
    ]
},
{
    "Id": 248,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/272cf29e4d0c9de3fb1631f766e8bb55d65944d6",
    "date": "2024-03-11T22:34:42+00:00",
    "message": "[FSDP2][BE] Refactored `check_1d_sharded_parity` to use mesh (#121357)\n\nEventually, we should just have one unified way to check for parity between a `DTensor`-sharded model and a replicated model. This PR is a small refactor to work toward that. One current gap to use this `check_sharded_parity` function for 2D is that FSDP's `(Shard(0), Shard(0))` layout differs from that of the `DTensor` APIs since FSDP shards on dim-0 after TP shards on dim-0.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/121357\nApproved by: https://github.com/weifengpy\nghstack dependencies: #121360",
    "label": "NO",
    "changes": [
        {
            "name": "test_fully_shard_autograd.py",
            "path": "test/distributed/_composable/fsdp/test_fully_shard_autograd.py",
            "patches": [
                {
                    "old_start": 13,
                    "old_length": 7,
                    "new_start": 13,
                    "new_length": 7,
                    "hunk": "@@ -13,7 +13,7 @@ from torch.distributed._composable.fsdp import fully_shard\n from torch.nn.parallel.scatter_gather import _is_namedtuple\n from torch.testing._internal.common_distributed import skip_if_lt_x_gpu\n from torch.testing._internal.common_fsdp import (\n-    check_1d_sharded_parity,\n+    check_sharded_parity,\n     DoubleLinear,\n     FSDPTest,\n )\n"
                },
                {
                    "old_start": 77,
                    "old_length": 7,
                    "new_start": 77,
                    "new_length": 7,
                    "hunk": "@@ -77,7 +77,7 @@ class TestFullyShardAutograd(FSDPTest):\n             self.assertEqual(loss, ref_loss)\n             optim.zero_grad(set_to_none=(iter_idx % 2))\n             ref_optim.zero_grad(set_to_none=(iter_idx % 2))\n-            check_1d_sharded_parity(self, ref_model, model)\n+            check_sharded_parity(self, ref_model, model)\n \n     @skip_if_lt_x_gpu(2)\n     def test_unused_forward_module(self):\n"
                },
                {
                    "old_start": 116,
                    "old_length": 7,
                    "new_start": 116,
                    "new_length": 7,
                    "hunk": "@@ -116,7 +116,7 @@ class TestFullyShardAutograd(FSDPTest):\n             self._reduce_1d_partial_grads(ref_model)\n             dist.all_reduce(losses[1])  # partial -> replicated\n             self.assertEqual(losses[0], losses[1])\n-            check_1d_sharded_parity(self, ref_model, model)\n+            check_sharded_parity(self, ref_model, model)\n             for _optim in (optim, ref_optim):\n                 _optim.step()\n                 _optim.zero_grad(set_to_none=(iter_idx % 2))\n"
                },
                {
                    "old_start": 226,
                    "old_length": 7,
                    "new_start": 226,
                    "new_length": 7,
                    "hunk": "@@ -226,7 +226,7 @@ class TestFullyShardAutograd(FSDPTest):\n             self._reduce_1d_partial_grads(ref_model)\n             dist.all_reduce(losses[1])  # partial -> replicated\n             self.assertEqual(losses[0], losses[1])\n-            check_1d_sharded_parity(self, ref_model, model)\n+            check_sharded_parity(self, ref_model, model)\n             for _optim in (optim, ref_optim):\n                 _optim.step()\n                 _optim.zero_grad(set_to_none=(iter_idx % 2))\n"
                }
            ],
            "whole_deleted": "-    check_1d_sharded_parity,\n-            check_1d_sharded_parity(self, ref_model, model)\n-            check_1d_sharded_parity(self, ref_model, model)\n-            check_1d_sharded_parity(self, ref_model, model)\n",
            "whole_added": "+    check_sharded_parity,\n+            check_sharded_parity(self, ref_model, model)\n+            check_sharded_parity(self, ref_model, model)\n+            check_sharded_parity(self, ref_model, model)\n",
            "whole_hunk": "@@ -13,7 +13,7 @@ from torch.distributed._composable.fsdp import fully_shard\n from torch.nn.parallel.scatter_gather import _is_namedtuple\n from torch.testing._internal.common_distributed import skip_if_lt_x_gpu\n from torch.testing._internal.common_fsdp import (\n-    check_1d_sharded_parity,\n+    check_sharded_parity,\n     DoubleLinear,\n     FSDPTest,\n )\n@@ -77,7 +77,7 @@ class TestFullyShardAutograd(FSDPTest):\n             self.assertEqual(loss, ref_loss)\n             optim.zero_grad(set_to_none=(iter_idx % 2))\n             ref_optim.zero_grad(set_to_none=(iter_idx % 2))\n-            check_1d_sharded_parity(self, ref_model, model)\n+            check_sharded_parity(self, ref_model, model)\n \n     @skip_if_lt_x_gpu(2)\n     def test_unused_forward_module(self):\n@@ -116,7 +116,7 @@ class TestFullyShardAutograd(FSDPTest):\n             self._reduce_1d_partial_grads(ref_model)\n             dist.all_reduce(losses[1])  # partial -> replicated\n             self.assertEqual(losses[0], losses[1])\n-            check_1d_sharded_parity(self, ref_model, model)\n+            check_sharded_parity(self, ref_model, model)\n             for _optim in (optim, ref_optim):\n                 _optim.step()\n                 _optim.zero_grad(set_to_none=(iter_idx % 2))\n@@ -226,7 +226,7 @@ class TestFullyShardAutograd(FSDPTest):\n             self._reduce_1d_partial_grads(ref_model)\n             dist.all_reduce(losses[1])  # partial -> replicated\n             self.assertEqual(losses[0], losses[1])\n-            check_1d_sharded_parity(self, ref_model, model)\n+            check_sharded_parity(self, ref_model, model)\n             for _optim in (optim, ref_optim):\n                 _optim.step()\n                 _optim.zero_grad(set_to_none=(iter_idx % 2))\n"
        },
        {
            "name": "test_fully_shard_frozen.py",
            "path": "test/distributed/_composable/fsdp/test_fully_shard_frozen.py",
            "patches": [
                {
                    "old_start": 17,
                    "old_length": 7,
                    "new_start": 17,
                    "new_length": 7,
                    "hunk": "@@ -17,7 +17,7 @@ from torch.distributed._composable.fsdp._fsdp_param_group import (\n )\n from torch.testing._internal.common_distributed import skip_if_lt_x_gpu\n from torch.testing._internal.common_fsdp import (\n-    check_1d_sharded_parity,\n+    check_sharded_parity,\n     FSDPTest,\n     MLP,\n     patch_reduce_scatter,\n"
                },
                {
                    "old_start": 123,
                    "old_length": 7,
                    "new_start": 123,
                    "new_length": 7,
                    "hunk": "@@ -123,7 +123,7 @@ class TestFullyShardFrozen(FSDPTest):\n                     losses.append(_model(inp).sum())\n                     losses[-1].backward()\n                     _optim.step()\n-                check_1d_sharded_parity(self, ref_model, model)\n+                check_sharded_parity(self, ref_model, model)\n                 self.assertEqual(losses[0], losses[1])\n                 # Check that the post-backward hooks ran through the autograd\n                 # backward, not the final callback (except possibly that of the\n"
                }
            ],
            "whole_deleted": "-    check_1d_sharded_parity,\n-                check_1d_sharded_parity(self, ref_model, model)\n",
            "whole_added": "+    check_sharded_parity,\n+                check_sharded_parity(self, ref_model, model)\n",
            "whole_hunk": "@@ -17,7 +17,7 @@ from torch.distributed._composable.fsdp._fsdp_param_group import (\n )\n from torch.testing._internal.common_distributed import skip_if_lt_x_gpu\n from torch.testing._internal.common_fsdp import (\n-    check_1d_sharded_parity,\n+    check_sharded_parity,\n     FSDPTest,\n     MLP,\n     patch_reduce_scatter,\n@@ -123,7 +123,7 @@ class TestFullyShardFrozen(FSDPTest):\n                     losses.append(_model(inp).sum())\n                     losses[-1].backward()\n                     _optim.step()\n-                check_1d_sharded_parity(self, ref_model, model)\n+                check_sharded_parity(self, ref_model, model)\n                 self.assertEqual(losses[0], losses[1])\n                 # Check that the post-backward hooks ran through the autograd\n                 # backward, not the final callback (except possibly that of the\n"
        },
        {
            "name": "test_fully_shard_mixed_precision.py",
            "path": "test/distributed/_composable/fsdp/test_fully_shard_mixed_precision.py",
            "patches": [
                {
                    "old_start": 15,
                    "old_length": 7,
                    "new_start": 15,
                    "new_length": 7,
                    "hunk": "@@ -15,7 +15,7 @@ from torch.testing._internal.common_distributed import (\n     skip_if_lt_x_gpu,\n )\n from torch.testing._internal.common_fsdp import (\n-    check_1d_sharded_parity,\n+    check_sharded_parity,\n     FSDPTest,\n     FSDPTestMultiThread,\n     MLP,\n"
                },
                {
                    "old_start": 108,
                    "old_length": 7,
                    "new_start": 108,
                    "new_length": 7,
                    "hunk": "@@ -108,7 +108,7 @@ class TestFullyShardMixedPrecisionTraining(FSDPTest):\n                 param_bf16.detach().copy_(param_fp32)\n \n             self.assertEqual(fsdp_loss, ref_loss)\n-            check_1d_sharded_parity(self, ref_model, model)\n+            check_sharded_parity(self, ref_model, model)\n \n     @skip_if_lt_x_gpu(2)\n     @requires_nccl_version((2, 10), \"Need NCCL 2.10+ for bf16 collectives\")\n"
                },
                {
                    "old_start": 160,
                    "old_length": 7,
                    "new_start": 160,
                    "new_length": 7,
                    "hunk": "@@ -160,7 +160,7 @@ class TestFullyShardMixedPrecisionTraining(FSDPTest):\n                 param_bf16.detach().copy_(param_fp32)\n \n             self.assertEqual(fsdp_loss, ref_loss)\n-            check_1d_sharded_parity(self, ref_model, model)\n+            check_sharded_parity(self, ref_model, model)\n \n     def _test_reduce_dtype_bf16_reduce(self, reshard_after_forward: Union[bool, int]):\n         param_dtype, reduce_dtype = torch.float32, torch.bfloat16\n"
                },
                {
                    "old_start": 195,
                    "old_length": 7,
                    "new_start": 195,
                    "new_length": 7,
                    "hunk": "@@ -195,7 +195,7 @@ class TestFullyShardMixedPrecisionTraining(FSDPTest):\n             ref_optim.step()  # fp32 optimizer step\n \n             self.assertEqual(fsdp_loss, ref_loss)\n-            check_1d_sharded_parity(self, ref_model, model)\n+            check_sharded_parity(self, ref_model, model)\n \n \n class TestFullyShardMixedPrecisionCasts(FSDPTestMultiThread):\n"
                }
            ],
            "whole_deleted": "-    check_1d_sharded_parity,\n-            check_1d_sharded_parity(self, ref_model, model)\n-            check_1d_sharded_parity(self, ref_model, model)\n-            check_1d_sharded_parity(self, ref_model, model)\n",
            "whole_added": "+    check_sharded_parity,\n+            check_sharded_parity(self, ref_model, model)\n+            check_sharded_parity(self, ref_model, model)\n+            check_sharded_parity(self, ref_model, model)\n",
            "whole_hunk": "@@ -15,7 +15,7 @@ from torch.testing._internal.common_distributed import (\n     skip_if_lt_x_gpu,\n )\n from torch.testing._internal.common_fsdp import (\n-    check_1d_sharded_parity,\n+    check_sharded_parity,\n     FSDPTest,\n     FSDPTestMultiThread,\n     MLP,\n@@ -108,7 +108,7 @@ class TestFullyShardMixedPrecisionTraining(FSDPTest):\n                 param_bf16.detach().copy_(param_fp32)\n \n             self.assertEqual(fsdp_loss, ref_loss)\n-            check_1d_sharded_parity(self, ref_model, model)\n+            check_sharded_parity(self, ref_model, model)\n \n     @skip_if_lt_x_gpu(2)\n     @requires_nccl_version((2, 10), \"Need NCCL 2.10+ for bf16 collectives\")\n@@ -160,7 +160,7 @@ class TestFullyShardMixedPrecisionTraining(FSDPTest):\n                 param_bf16.detach().copy_(param_fp32)\n \n             self.assertEqual(fsdp_loss, ref_loss)\n-            check_1d_sharded_parity(self, ref_model, model)\n+            check_sharded_parity(self, ref_model, model)\n \n     def _test_reduce_dtype_bf16_reduce(self, reshard_after_forward: Union[bool, int]):\n         param_dtype, reduce_dtype = torch.float32, torch.bfloat16\n@@ -195,7 +195,7 @@ class TestFullyShardMixedPrecisionTraining(FSDPTest):\n             ref_optim.step()  # fp32 optimizer step\n \n             self.assertEqual(fsdp_loss, ref_loss)\n-            check_1d_sharded_parity(self, ref_model, model)\n+            check_sharded_parity(self, ref_model, model)\n \n \n class TestFullyShardMixedPrecisionCasts(FSDPTestMultiThread):\n"
        },
        {
            "name": "test_fully_shard_training.py",
            "path": "test/distributed/_composable/fsdp/test_fully_shard_training.py",
            "patches": [
                {
                    "old_start": 26,
                    "old_length": 7,
                    "new_start": 26,
                    "new_length": 7,
                    "hunk": "@@ -26,7 +26,7 @@ from torch.distributed.tensor.parallel import (\n from torch.testing._internal.common_cuda import TEST_CUDA\n from torch.testing._internal.common_distributed import skip_if_lt_x_gpu\n from torch.testing._internal.common_fsdp import (\n-    check_1d_sharded_parity,\n+    check_sharded_parity,\n     FSDPTest,\n     FSDPTestMultiThread,\n     MLP,\n"
                },
                {
                    "old_start": 210,
                    "old_length": 7,
                    "new_start": 210,
                    "new_length": 7,
                    "hunk": "@@ -210,7 +210,7 @@ class TestFullyShardCastAfterInit(FSDPTestMultiThread):\n         for param in model.parameters():\n             self.assertEqual(param.dtype, dtype)\n         optim = torch.optim.Adam(model.parameters(), lr=1e-2, foreach=True)\n-        check_1d_sharded_parity(self, ref_model, model)\n+        check_sharded_parity(self, ref_model, model)\n         torch.manual_seed(42 + self.rank + 1)\n         inp = torch.randn((2, mlp_dim), device=\"cuda\", dtype=dtype)\n         for iter_idx in range(10):\n"
                },
                {
                    "old_start": 219,
                    "old_length": 7,
                    "new_start": 219,
                    "new_length": 7,
                    "hunk": "@@ -219,7 +219,7 @@ class TestFullyShardCastAfterInit(FSDPTestMultiThread):\n                 losses.append(_model(inp).sum())\n                 losses[-1].backward()\n             self.assertEqual(losses[0], losses[1])\n-            check_1d_sharded_parity(self, ref_model, model)\n+            check_sharded_parity(self, ref_model, model)\n             for _optim in (ref_optim, optim):\n                 _optim.step()\n                 _optim.zero_grad(set_to_none=(iter_idx % 2 == 0))\n"
                },
                {
                    "old_start": 528,
                    "old_length": 7,
                    "new_start": 528,
                    "new_length": 7,
                    "hunk": "@@ -528,7 +528,7 @@ class TestFullyShard1DTrainingCompose(FSDPTest):\n         # Reuse the same input across iterations to avoid loss explosion from\n         # trying to learn from random inputs\n         inp = torch.randint(0, vocab_size, (3, 64), device=\"cuda\")\n-        check_1d_sharded_parity(\n+        check_sharded_parity(\n             self, ref_model, model, prefixes_to_ignore=prefixes_to_ignore\n         )\n         for iter_idx in range(10):\n"
                },
                {
                    "old_start": 537,
                    "old_length": 14,
                    "new_start": 537,
                    "new_length": 14,
                    "hunk": "@@ -537,14 +537,14 @@ class TestFullyShard1DTrainingCompose(FSDPTest):\n                 torch.manual_seed(iter_idx + 1)  # for dropout determinism\n                 losses.append(_model(inp).sum())\n                 losses[-1].backward()\n-            check_1d_sharded_parity(\n+            check_sharded_parity(\n                 self, ref_model, model, prefixes_to_ignore=prefixes_to_ignore\n             )\n             self.assertEqual(losses[0], losses[1])\n             for _optim in (ref_optim, optim):\n                 _optim.step()\n                 _optim.zero_grad(set_to_none=(iter_idx % 2 == 0))\n-            check_1d_sharded_parity(\n+            check_sharded_parity(\n                 self, ref_model, model, prefixes_to_ignore=prefixes_to_ignore\n             )\n \n"
                },
                {
                    "old_start": 697,
                    "old_length": 7,
                    "new_start": 697,
                    "new_length": 7,
                    "hunk": "@@ -697,7 +697,7 @@ class TestFullyShardGradientAccumulation(FSDPTest):\n             for param in ref_model.parameters():\n                 if param.grad is not None:\n                     param.grad.div_(self.world_size)\n-            check_1d_sharded_parity(self, ref_model, model)\n+            check_sharded_parity(self, ref_model, model)\n             for _optim in (optim, ref_optim):\n                 _optim.step()\n                 # When `set_to_none=False`, we are exercising mixing\n"
                }
            ],
            "whole_deleted": "-    check_1d_sharded_parity,\n-        check_1d_sharded_parity(self, ref_model, model)\n-            check_1d_sharded_parity(self, ref_model, model)\n-        check_1d_sharded_parity(\n-            check_1d_sharded_parity(\n-            check_1d_sharded_parity(\n-            check_1d_sharded_parity(self, ref_model, model)\n",
            "whole_added": "+    check_sharded_parity,\n+        check_sharded_parity(self, ref_model, model)\n+            check_sharded_parity(self, ref_model, model)\n+        check_sharded_parity(\n+            check_sharded_parity(\n+            check_sharded_parity(\n+            check_sharded_parity(self, ref_model, model)\n",
            "whole_hunk": "@@ -26,7 +26,7 @@ from torch.distributed.tensor.parallel import (\n from torch.testing._internal.common_cuda import TEST_CUDA\n from torch.testing._internal.common_distributed import skip_if_lt_x_gpu\n from torch.testing._internal.common_fsdp import (\n-    check_1d_sharded_parity,\n+    check_sharded_parity,\n     FSDPTest,\n     FSDPTestMultiThread,\n     MLP,\n@@ -210,7 +210,7 @@ class TestFullyShardCastAfterInit(FSDPTestMultiThread):\n         for param in model.parameters():\n             self.assertEqual(param.dtype, dtype)\n         optim = torch.optim.Adam(model.parameters(), lr=1e-2, foreach=True)\n-        check_1d_sharded_parity(self, ref_model, model)\n+        check_sharded_parity(self, ref_model, model)\n         torch.manual_seed(42 + self.rank + 1)\n         inp = torch.randn((2, mlp_dim), device=\"cuda\", dtype=dtype)\n         for iter_idx in range(10):\n@@ -219,7 +219,7 @@ class TestFullyShardCastAfterInit(FSDPTestMultiThread):\n                 losses.append(_model(inp).sum())\n                 losses[-1].backward()\n             self.assertEqual(losses[0], losses[1])\n-            check_1d_sharded_parity(self, ref_model, model)\n+            check_sharded_parity(self, ref_model, model)\n             for _optim in (ref_optim, optim):\n                 _optim.step()\n                 _optim.zero_grad(set_to_none=(iter_idx % 2 == 0))\n@@ -528,7 +528,7 @@ class TestFullyShard1DTrainingCompose(FSDPTest):\n         # Reuse the same input across iterations to avoid loss explosion from\n         # trying to learn from random inputs\n         inp = torch.randint(0, vocab_size, (3, 64), device=\"cuda\")\n-        check_1d_sharded_parity(\n+        check_sharded_parity(\n             self, ref_model, model, prefixes_to_ignore=prefixes_to_ignore\n         )\n         for iter_idx in range(10):\n@@ -537,14 +537,14 @@ class TestFullyShard1DTrainingCompose(FSDPTest):\n                 torch.manual_seed(iter_idx + 1)  # for dropout determinism\n                 losses.append(_model(inp).sum())\n                 losses[-1].backward()\n-            check_1d_sharded_parity(\n+            check_sharded_parity(\n                 self, ref_model, model, prefixes_to_ignore=prefixes_to_ignore\n             )\n             self.assertEqual(losses[0], losses[1])\n             for _optim in (ref_optim, optim):\n                 _optim.step()\n                 _optim.zero_grad(set_to_none=(iter_idx % 2 == 0))\n-            check_1d_sharded_parity(\n+            check_sharded_parity(\n                 self, ref_model, model, prefixes_to_ignore=prefixes_to_ignore\n             )\n \n@@ -697,7 +697,7 @@ class TestFullyShardGradientAccumulation(FSDPTest):\n             for param in ref_model.parameters():\n                 if param.grad is not None:\n                     param.grad.div_(self.world_size)\n-            check_1d_sharded_parity(self, ref_model, model)\n+            check_sharded_parity(self, ref_model, model)\n             for _optim in (optim, ref_optim):\n                 _optim.step()\n                 # When `set_to_none=False`, we are exercising mixing\n"
        },
        {
            "name": "common_fsdp.py",
            "path": "torch/testing/_internal/common_fsdp.py",
            "patches": [
                {
                    "old_start": 32,
                    "old_length": 7,
                    "new_start": 32,
                    "new_length": 7,
                    "hunk": "@@ -32,7 +32,7 @@ from torch.distributed._composable.fsdp._fsdp_param_group import (\n     FSDPParamGroup,\n     RegisterPostBackwardFunction,\n )\n-from torch.distributed._tensor import DTensor\n+from torch.distributed._tensor import distribute_tensor, DTensor, Shard\n from torch.distributed.fsdp import CPUOffload, FullyShardedDataParallel as FSDP\n from torch.distributed.fsdp._common_utils import TrainingState\n from torch.distributed.fsdp._init_utils import NO_RESHARD_AFTER_FORWARD_STRATEGIES\n"
                },
                {
                    "old_start": 957,
                    "old_length": 16,
                    "new_start": 957,
                    "new_length": 12,
                    "hunk": "@@ -957,16 +957,12 @@ def reduce_scatter_with_assert(\n     return orig_reduce_scatter(*args, **kwargs)\n \n \n-def check_1d_sharded_parity(\n+def check_sharded_parity(\n     cls,  # unit test class\n     replicated_module: nn.Module,\n     sharded_module: nn.Module,\n-    group: Optional[dist.ProcessGroup] = None,\n-    check_grads: bool = True,\n     prefixes_to_ignore: Tuple[str, ...] = (),\n ):\n-    group = group or dist.distributed_c10d._get_default_group()\n-    rank, world_size = group.rank(), group.size()\n     for (replicated_name, replicated_param), (sharded_name, sharded_param) in zip(\n         replicated_module.named_parameters(), sharded_module.named_parameters()\n     ):\n"
                },
                {
                    "old_start": 976,
                    "old_length": 18,
                    "new_start": 972,
                    "new_length": 22,
                    "hunk": "@@ -976,18 +972,22 @@ def check_1d_sharded_parity(\n         cls.assertEqual(replicated_name, clean_sharded_name)\n         cls.assertIsInstance(sharded_param, DTensor)\n         assert isinstance(sharded_param, DTensor)  # mypy\n-        param_chunks = torch.chunk(replicated_param, world_size, dim=0)\n-        cls.assertEqual(sharded_param._local_tensor, param_chunks[rank])\n-        if not check_grads:\n-            continue\n+        mesh, placements = sharded_param.device_mesh, sharded_param.placements\n+        if tuple(placements) == (Shard(0), Shard(0)):\n+            raise AssertionError(\n+                \"FSDP's (Shard(0), Shard(0)) layout differs from distribute_tensor(), \"\n+                \"so we cannot check for equality using it\"\n+            )\n+        sharded_ref_param = distribute_tensor(replicated_param, mesh, placements)\n+        cls.assertEqual(sharded_param.to_local(), sharded_ref_param.to_local())\n         if replicated_param.grad is None:\n             cls.assertIsNone(sharded_param.grad)\n             continue\n         cls.assertIsNotNone(sharded_param.grad)\n-        grad_chunks = torch.chunk(replicated_param.grad, world_size, dim=0)\n+        sharded_ref_grad = distribute_tensor(replicated_param.grad, mesh, placements)\n         cls.assertIsInstance(sharded_param.grad, DTensor)\n         assert isinstance(sharded_param.grad, DTensor)  # mypy\n-        cls.assertEqual(sharded_param.grad._local_tensor, grad_chunks[rank])\n+        cls.assertEqual(sharded_param.grad.to_local(), sharded_ref_grad.to_local())\n \n \n def run_subtests("
                }
            ],
            "whole_deleted": "-from torch.distributed._tensor import DTensor\n-def check_1d_sharded_parity(\n-    group: Optional[dist.ProcessGroup] = None,\n-    check_grads: bool = True,\n-    group = group or dist.distributed_c10d._get_default_group()\n-    rank, world_size = group.rank(), group.size()\n-        param_chunks = torch.chunk(replicated_param, world_size, dim=0)\n-        cls.assertEqual(sharded_param._local_tensor, param_chunks[rank])\n-        if not check_grads:\n-            continue\n-        grad_chunks = torch.chunk(replicated_param.grad, world_size, dim=0)\n-        cls.assertEqual(sharded_param.grad._local_tensor, grad_chunks[rank])\n",
            "whole_added": "+from torch.distributed._tensor import distribute_tensor, DTensor, Shard\n+def check_sharded_parity(\n+        mesh, placements = sharded_param.device_mesh, sharded_param.placements\n+        if tuple(placements) == (Shard(0), Shard(0)):\n+            raise AssertionError(\n+                \"FSDP's (Shard(0), Shard(0)) layout differs from distribute_tensor(), \"\n+                \"so we cannot check for equality using it\"\n+            )\n+        sharded_ref_param = distribute_tensor(replicated_param, mesh, placements)\n+        cls.assertEqual(sharded_param.to_local(), sharded_ref_param.to_local())\n+        sharded_ref_grad = distribute_tensor(replicated_param.grad, mesh, placements)\n+        cls.assertEqual(sharded_param.grad.to_local(), sharded_ref_grad.to_local())\n",
            "whole_hunk": "@@ -32,7 +32,7 @@ from torch.distributed._composable.fsdp._fsdp_param_group import (\n     FSDPParamGroup,\n     RegisterPostBackwardFunction,\n )\n-from torch.distributed._tensor import DTensor\n+from torch.distributed._tensor import distribute_tensor, DTensor, Shard\n from torch.distributed.fsdp import CPUOffload, FullyShardedDataParallel as FSDP\n from torch.distributed.fsdp._common_utils import TrainingState\n from torch.distributed.fsdp._init_utils import NO_RESHARD_AFTER_FORWARD_STRATEGIES\n@@ -957,16 +957,12 @@ def reduce_scatter_with_assert(\n     return orig_reduce_scatter(*args, **kwargs)\n \n \n-def check_1d_sharded_parity(\n+def check_sharded_parity(\n     cls,  # unit test class\n     replicated_module: nn.Module,\n     sharded_module: nn.Module,\n-    group: Optional[dist.ProcessGroup] = None,\n-    check_grads: bool = True,\n     prefixes_to_ignore: Tuple[str, ...] = (),\n ):\n-    group = group or dist.distributed_c10d._get_default_group()\n-    rank, world_size = group.rank(), group.size()\n     for (replicated_name, replicated_param), (sharded_name, sharded_param) in zip(\n         replicated_module.named_parameters(), sharded_module.named_parameters()\n     ):\n@@ -976,18 +972,22 @@ def check_1d_sharded_parity(\n         cls.assertEqual(replicated_name, clean_sharded_name)\n         cls.assertIsInstance(sharded_param, DTensor)\n         assert isinstance(sharded_param, DTensor)  # mypy\n-        param_chunks = torch.chunk(replicated_param, world_size, dim=0)\n-        cls.assertEqual(sharded_param._local_tensor, param_chunks[rank])\n-        if not check_grads:\n-            continue\n+        mesh, placements = sharded_param.device_mesh, sharded_param.placements\n+        if tuple(placements) == (Shard(0), Shard(0)):\n+            raise AssertionError(\n+                \"FSDP's (Shard(0), Shard(0)) layout differs from distribute_tensor(), \"\n+                \"so we cannot check for equality using it\"\n+            )\n+        sharded_ref_param = distribute_tensor(replicated_param, mesh, placements)\n+        cls.assertEqual(sharded_param.to_local(), sharded_ref_param.to_local())\n         if replicated_param.grad is None:\n             cls.assertIsNone(sharded_param.grad)\n             continue\n         cls.assertIsNotNone(sharded_param.grad)\n-        grad_chunks = torch.chunk(replicated_param.grad, world_size, dim=0)\n+        sharded_ref_grad = distribute_tensor(replicated_param.grad, mesh, placements)\n         cls.assertIsInstance(sharded_param.grad, DTensor)\n         assert isinstance(sharded_param.grad, DTensor)  # mypy\n-        cls.assertEqual(sharded_param.grad._local_tensor, grad_chunks[rank])\n+        cls.assertEqual(sharded_param.grad.to_local(), sharded_ref_grad.to_local())\n \n \n def run_subtests("
        }
    ]
},
{
    "Id": 548,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/d2215f14ba1cd45c79f778a804a6125f140461b3",
    "date": "2023-08-22T12:38:05+00:00",
    "message": "Fix: transactional translation validation insertion. (#107523)\n\nThis PR fixes transactional behavior of translation validation insertion.\n\nPreviously, this transactional behavior was implemented by removing the FX node if any\nissues occurred until the end of `evaluate_expr`. However, since we cache FX nodes, we\nmight end up removing something that wasn't inserted in the same function call.\n\n**Solution:** when creating an FX node for `call_function`, we also return whether this is\na fresh FX node or not. Then, we can appropriately handle each case.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/107523\nApproved by: https://github.com/ezyang",
    "label": "YES",
    "changes": [
        {
            "name": "symbolic_shapes.py",
            "path": "torch/fx/experimental/symbolic_shapes.py",
            "patches": [
                {
                    "old_start": 1328,
                    "old_length": 7,
                    "new_start": 1328,
                    "new_length": 7,
                    "hunk": "@@ -1328,7 +1328,7 @@ def _make_node_magic(method, func):\n \n         # Create a FX node that corresponds to the operation being applied to\n         # this node.\n-        fx_node = self.shape_env.create_fx_call_function(op, (self.fx_node, other.fx_node))\n+        fx_node, _ = self.shape_env.create_fx_call_function(op, (self.fx_node, other.fx_node))\n         return SymNode(out, self.shape_env, pytype, out_hint, fx_node=fx_node)\n \n     def unary_magic_impl(self):\n"
                },
                {
                    "old_start": 1358,
                    "old_length": 7,
                    "new_start": 1358,
                    "new_length": 7,
                    "hunk": "@@ -1358,7 +1358,7 @@ def _make_node_magic(method, func):\n         else:\n             pytype = self.pytype\n \n-        fx_node = self.shape_env.create_fx_call_function(op, (self.fx_node,))\n+        fx_node, _ = self.shape_env.create_fx_call_function(op, (self.fx_node,))\n         return SymNode(out, self.shape_env, pytype, out_hint, fx_node=fx_node)\n \n     if method in unary_magic_methods:\n"
                },
                {
                    "old_start": 2137,
                    "old_length": 9,
                    "new_start": 2137,
                    "new_length": 11,
                    "hunk": "@@ -2137,9 +2137,11 @@ class ShapeEnv:\n             self,\n             op: Callable,\n             args: Tuple,\n-    ) -> Optional[torch.fx.Node]:\n+    ) -> Tuple[Optional[torch.fx.Node], bool]:\n         # Cache this tuple in order to avoid duplicated nodes.\n         node_key = (op, args)\n+        # Flags whether the returned node was cached or not.\n+        fresh = False\n \n         if _translation_validation_enabled() and node_key not in self.fx_node_cache:\n             from torch.fx.experimental.validator import z3op\n"
                },
                {
                    "old_start": 2149,
                    "old_length": 7,
                    "new_start": 2151,
                    "new_length": 9,
                    "hunk": "@@ -2149,7 +2151,9 @@ class ShapeEnv:\n                 # We check if we are not mixing SymNode that should not be ignored\n                 # (fx_node is not None) with those that should (fx_node is None).\n                 assert all(not isinstance(a, torch.fx.Node) for a in args)\n-                return None\n+                return None, fresh\n+\n+            fresh = True\n \n             # If translation validation is enabled, all arguments must have its\n             # own FX node.\n"
                },
                {
                    "old_start": 2157,
                    "old_length": 7,
                    "new_start": 2161,
                    "new_length": 7,
                    "hunk": "@@ -2157,7 +2161,7 @@ class ShapeEnv:\n             lifted_op = z3op(op, self.validator)\n             self.fx_node_cache[node_key] = self.graph.call_function(lifted_op, args)\n \n-        return self.fx_node_cache.get(node_key, None)\n+        return self.fx_node_cache.get(node_key, None), fresh\n \n     def create_fx_placeholder_and_z3var(\n             self,\n"
                },
                {
                    "old_start": 3385,
                    "old_length": 19,
                    "new_start": 3389,
                    "new_length": 20,
                    "hunk": "@@ -3385,19 +3389,20 @@ class ShapeEnv:\n         # If all of the above check, we create an FX node representing the\n         # actual expression to be guarded.\n         node = None\n+        fresh = False\n         if (\n                 _translation_validation_enabled()\n                 and fx_node is not None\n                 and not self._suppress_guards_tls()\n         ):\n             if concrete_val is sympy.true:\n-                node = self.create_fx_call_function(torch._assert, (fx_node,))\n+                node, fresh = self.create_fx_call_function(torch._assert, (fx_node,))\n             elif concrete_val is sympy.false:\n-                neg = self.create_fx_call_function(operator.not_, (fx_node,))\n-                node = self.create_fx_call_function(torch._assert, (neg,))\n+                neg, _ = self.create_fx_call_function(operator.not_, (fx_node,))\n+                node, fresh = self.create_fx_call_function(torch._assert, (neg,))\n             else:\n-                eql = self.create_fx_call_function(operator.eq, (fx_node, concrete_val))\n-                node = self.create_fx_call_function(torch._assert, (eql,))\n+                eql, _ = self.create_fx_call_function(operator.eq, (fx_node, concrete_val))\n+                node, fresh = self.create_fx_call_function(torch._assert, (eql,))\n \n         # After creating the FX node corresponding to orig_expr, we must make sure that\n         # no error will be raised until the end of this function.\n"
                },
                {
                    "old_start": 3469,
                    "old_length": 7,
                    "new_start": 3474,
                    "new_length": 8,
                    "hunk": "@@ -3469,7 +3474,8 @@ class ShapeEnv:\n                 guard = ShapeGuard(g, stack)\n                 self.guards.append(guard)\n         except Exception:\n-            self.remove_fx_node(node)\n+            if fresh:\n+                self.remove_fx_node(node)\n             raise\n         else:\n             if not self._suppress_guards_tls():"
                }
            ],
            "whole_deleted": "-        fx_node = self.shape_env.create_fx_call_function(op, (self.fx_node, other.fx_node))\n-        fx_node = self.shape_env.create_fx_call_function(op, (self.fx_node,))\n-    ) -> Optional[torch.fx.Node]:\n-                return None\n-        return self.fx_node_cache.get(node_key, None)\n-                node = self.create_fx_call_function(torch._assert, (fx_node,))\n-                neg = self.create_fx_call_function(operator.not_, (fx_node,))\n-                node = self.create_fx_call_function(torch._assert, (neg,))\n-                eql = self.create_fx_call_function(operator.eq, (fx_node, concrete_val))\n-                node = self.create_fx_call_function(torch._assert, (eql,))\n-            self.remove_fx_node(node)\n",
            "whole_added": "+        fx_node, _ = self.shape_env.create_fx_call_function(op, (self.fx_node, other.fx_node))\n+        fx_node, _ = self.shape_env.create_fx_call_function(op, (self.fx_node,))\n+    ) -> Tuple[Optional[torch.fx.Node], bool]:\n+        # Flags whether the returned node was cached or not.\n+        fresh = False\n+                return None, fresh\n+\n+            fresh = True\n+        return self.fx_node_cache.get(node_key, None), fresh\n+        fresh = False\n+                node, fresh = self.create_fx_call_function(torch._assert, (fx_node,))\n+                neg, _ = self.create_fx_call_function(operator.not_, (fx_node,))\n+                node, fresh = self.create_fx_call_function(torch._assert, (neg,))\n+                eql, _ = self.create_fx_call_function(operator.eq, (fx_node, concrete_val))\n+                node, fresh = self.create_fx_call_function(torch._assert, (eql,))\n+            if fresh:\n+                self.remove_fx_node(node)\n",
            "whole_hunk": "@@ -1328,7 +1328,7 @@ def _make_node_magic(method, func):\n \n         # Create a FX node that corresponds to the operation being applied to\n         # this node.\n-        fx_node = self.shape_env.create_fx_call_function(op, (self.fx_node, other.fx_node))\n+        fx_node, _ = self.shape_env.create_fx_call_function(op, (self.fx_node, other.fx_node))\n         return SymNode(out, self.shape_env, pytype, out_hint, fx_node=fx_node)\n \n     def unary_magic_impl(self):\n@@ -1358,7 +1358,7 @@ def _make_node_magic(method, func):\n         else:\n             pytype = self.pytype\n \n-        fx_node = self.shape_env.create_fx_call_function(op, (self.fx_node,))\n+        fx_node, _ = self.shape_env.create_fx_call_function(op, (self.fx_node,))\n         return SymNode(out, self.shape_env, pytype, out_hint, fx_node=fx_node)\n \n     if method in unary_magic_methods:\n@@ -2137,9 +2137,11 @@ class ShapeEnv:\n             self,\n             op: Callable,\n             args: Tuple,\n-    ) -> Optional[torch.fx.Node]:\n+    ) -> Tuple[Optional[torch.fx.Node], bool]:\n         # Cache this tuple in order to avoid duplicated nodes.\n         node_key = (op, args)\n+        # Flags whether the returned node was cached or not.\n+        fresh = False\n \n         if _translation_validation_enabled() and node_key not in self.fx_node_cache:\n             from torch.fx.experimental.validator import z3op\n@@ -2149,7 +2151,9 @@ class ShapeEnv:\n                 # We check if we are not mixing SymNode that should not be ignored\n                 # (fx_node is not None) with those that should (fx_node is None).\n                 assert all(not isinstance(a, torch.fx.Node) for a in args)\n-                return None\n+                return None, fresh\n+\n+            fresh = True\n \n             # If translation validation is enabled, all arguments must have its\n             # own FX node.\n@@ -2157,7 +2161,7 @@ class ShapeEnv:\n             lifted_op = z3op(op, self.validator)\n             self.fx_node_cache[node_key] = self.graph.call_function(lifted_op, args)\n \n-        return self.fx_node_cache.get(node_key, None)\n+        return self.fx_node_cache.get(node_key, None), fresh\n \n     def create_fx_placeholder_and_z3var(\n             self,\n@@ -3385,19 +3389,20 @@ class ShapeEnv:\n         # If all of the above check, we create an FX node representing the\n         # actual expression to be guarded.\n         node = None\n+        fresh = False\n         if (\n                 _translation_validation_enabled()\n                 and fx_node is not None\n                 and not self._suppress_guards_tls()\n         ):\n             if concrete_val is sympy.true:\n-                node = self.create_fx_call_function(torch._assert, (fx_node,))\n+                node, fresh = self.create_fx_call_function(torch._assert, (fx_node,))\n             elif concrete_val is sympy.false:\n-                neg = self.create_fx_call_function(operator.not_, (fx_node,))\n-                node = self.create_fx_call_function(torch._assert, (neg,))\n+                neg, _ = self.create_fx_call_function(operator.not_, (fx_node,))\n+                node, fresh = self.create_fx_call_function(torch._assert, (neg,))\n             else:\n-                eql = self.create_fx_call_function(operator.eq, (fx_node, concrete_val))\n-                node = self.create_fx_call_function(torch._assert, (eql,))\n+                eql, _ = self.create_fx_call_function(operator.eq, (fx_node, concrete_val))\n+                node, fresh = self.create_fx_call_function(torch._assert, (eql,))\n \n         # After creating the FX node corresponding to orig_expr, we must make sure that\n         # no error will be raised until the end of this function.\n@@ -3469,7 +3474,8 @@ class ShapeEnv:\n                 guard = ShapeGuard(g, stack)\n                 self.guards.append(guard)\n         except Exception:\n-            self.remove_fx_node(node)\n+            if fresh:\n+                self.remove_fx_node(node)\n             raise\n         else:\n             if not self._suppress_guards_tls():"
        }
    ]
},
{
    "Id": 405,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/0f887a6d1a62449c92ad22b7659c471797ed3762",
    "date": "2023-11-22T18:05:33+00:00",
    "message": "limit fused kernel num args. (#113131)\n\nFixes #97361\n\nWhen fused kernel more than 1024 parameters, it should throw error from ctypes.\nLimit args number is should be a mechanism to protect stack memory. As we known, CPP is passing args via stack memory, and stack memory has size limitation.\n\nCode change:\n\n1. cpp backend will check the fused nodes' args number, if it is reach the limitation. It will status flush status to ready.\n2. scheduler will check `ready_to_flush` API and help backend flush codegen.\n3. Add `ready_to_flush` API to `BaseScheduling`, Triton backend will return False due to not support it yet.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113131\nApproved by: https://github.com/jgong5, https://github.com/mlazos",
    "label": "YES",
    "changes": [
        {
            "name": "test_torchinductor.py",
            "path": "test/inductor/test_torchinductor.py",
            "patches": [
                {
                    "old_start": 7713,
                    "old_length": 6,
                    "new_start": 7713,
                    "new_length": 24,
                    "hunk": "@@ -7713,6 +7713,24 @@ class CommonTemplate:\n         b = torch.randn(65, 2**24, device=self.device)\n         fn(a, b)\n \n+    def test_fuse_large_params(self):\n+        def pt2_optimizer_step(optimizer):\n+            @torch.compile()\n+            def f():\n+                optimizer.step()\n+\n+            f()\n+\n+        params = [\n+            torch.rand(10, 10, dtype=torch.float32, device=self.device)\n+            for _ in range(194)\n+        ]\n+        for p in params:\n+            p.grad = torch.rand_like(p)\n+\n+        o = torch.optim.AdamW(params)\n+        pt2_optimizer_step(o)\n+\n     def test_adaptive_avg_pool1d_argmax(self):\n         # https://github.com/pytorch/pytorch/issues/113013\n         def fn(x):\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    def test_fuse_large_params(self):\n+        def pt2_optimizer_step(optimizer):\n+            @torch.compile()\n+            def f():\n+                optimizer.step()\n+\n+            f()\n+\n+        params = [\n+            torch.rand(10, 10, dtype=torch.float32, device=self.device)\n+            for _ in range(194)\n+        ]\n+        for p in params:\n+            p.grad = torch.rand_like(p)\n+\n+        o = torch.optim.AdamW(params)\n+        pt2_optimizer_step(o)\n+\n",
            "whole_hunk": "@@ -7713,6 +7713,24 @@ class CommonTemplate:\n         b = torch.randn(65, 2**24, device=self.device)\n         fn(a, b)\n \n+    def test_fuse_large_params(self):\n+        def pt2_optimizer_step(optimizer):\n+            @torch.compile()\n+            def f():\n+                optimizer.step()\n+\n+            f()\n+\n+        params = [\n+            torch.rand(10, 10, dtype=torch.float32, device=self.device)\n+            for _ in range(194)\n+        ]\n+        for p in params:\n+            p.grad = torch.rand_like(p)\n+\n+        o = torch.optim.AdamW(params)\n+        pt2_optimizer_step(o)\n+\n     def test_adaptive_avg_pool1d_argmax(self):\n         # https://github.com/pytorch/pytorch/issues/113013\n         def fn(x):\n"
        },
        {
            "name": "cpp.py",
            "path": "torch/_inductor/codegen/cpp.py",
            "patches": [
                {
                    "old_start": 2919,
                    "old_length": 9,
                    "new_start": 2919,
                    "new_length": 18,
                    "hunk": "@@ -2919,9 +2919,18 @@ class CppKernelProxy(CppKernel):\n \n \n class CppScheduling(BaseScheduling):\n+    # ctypes limits the number of args to 1024, refer to:\n+    # https://github.com/python/cpython/commit/a285af7e626d1b81cf09f8b2bf7656f100bc1237\n+    # We set a conservative threshold here.\n+    MAX_FUSED_KERNEL_ARGS_NUM = 500\n+\n     def __init__(self, scheduler):\n         self.scheduler = scheduler\n         self.get_kernel_group()\n+        self._ready_to_flush = False\n+\n+    def _set_flush_status(self, status: bool):\n+        self._ready_to_flush = status\n \n     def group_fn(self, sizes):\n         return tuple(tuple(map(V.graph.sizevars.simplify, s)) for s in sizes)\n"
                },
                {
                    "old_start": 2968,
                    "old_length": 12,
                    "new_start": 2977,
                    "new_length": 23,
                    "hunk": "@@ -2968,12 +2977,23 @@ class CppScheduling(BaseScheduling):\n \n         kernel_group.finalize_kernel(cpp_kernel_proxy, nodes)\n \n+        args_num = self._get_scheduled_num_args()\n+        if args_num > CppScheduling.MAX_FUSED_KERNEL_ARGS_NUM:\n+            self._set_flush_status(True)\n+\n+    def _get_scheduled_num_args(self):\n+        return self.kernel_group.get_num_args()\n+\n+    def ready_to_flush(self):\n+        return self._ready_to_flush\n+\n     def codegen_sync(self):\n         pass\n \n     def flush(self):\n         self.kernel_group.codegen_define_and_call(V.graph.wrapper_code)\n         self.get_kernel_group()\n+        self._set_flush_status(False)\n \n \n class KernelGroup:\n"
                },
                {
                    "old_start": 2995,
                    "old_length": 6,
                    "new_start": 3015,
                    "new_length": 11,
                    "hunk": "@@ -2995,6 +3015,11 @@ class KernelGroup:\n         ws = self.ws\n         new_kernel.codegen_loops(code, ws)\n \n+    def get_num_args(self):\n+        arg_defs, call_args, arg_types = self.args.cpp_argdefs()\n+        args_num = len(arg_defs)\n+        return args_num\n+\n     def codegen_define_and_call(self, wrapper):\n         self.stack.close()\n         if not self.scheduled_nodes:\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    # ctypes limits the number of args to 1024, refer to:\n+    # https://github.com/python/cpython/commit/a285af7e626d1b81cf09f8b2bf7656f100bc1237\n+    # We set a conservative threshold here.\n+    MAX_FUSED_KERNEL_ARGS_NUM = 500\n+\n+        self._ready_to_flush = False\n+\n+    def _set_flush_status(self, status: bool):\n+        self._ready_to_flush = status\n+        args_num = self._get_scheduled_num_args()\n+        if args_num > CppScheduling.MAX_FUSED_KERNEL_ARGS_NUM:\n+            self._set_flush_status(True)\n+\n+    def _get_scheduled_num_args(self):\n+        return self.kernel_group.get_num_args()\n+\n+    def ready_to_flush(self):\n+        return self._ready_to_flush\n+\n+        self._set_flush_status(False)\n+    def get_num_args(self):\n+        arg_defs, call_args, arg_types = self.args.cpp_argdefs()\n+        args_num = len(arg_defs)\n+        return args_num\n+\n",
            "whole_hunk": "@@ -2919,9 +2919,18 @@ class CppKernelProxy(CppKernel):\n \n \n class CppScheduling(BaseScheduling):\n+    # ctypes limits the number of args to 1024, refer to:\n+    # https://github.com/python/cpython/commit/a285af7e626d1b81cf09f8b2bf7656f100bc1237\n+    # We set a conservative threshold here.\n+    MAX_FUSED_KERNEL_ARGS_NUM = 500\n+\n     def __init__(self, scheduler):\n         self.scheduler = scheduler\n         self.get_kernel_group()\n+        self._ready_to_flush = False\n+\n+    def _set_flush_status(self, status: bool):\n+        self._ready_to_flush = status\n \n     def group_fn(self, sizes):\n         return tuple(tuple(map(V.graph.sizevars.simplify, s)) for s in sizes)\n@@ -2968,12 +2977,23 @@ class CppScheduling(BaseScheduling):\n \n         kernel_group.finalize_kernel(cpp_kernel_proxy, nodes)\n \n+        args_num = self._get_scheduled_num_args()\n+        if args_num > CppScheduling.MAX_FUSED_KERNEL_ARGS_NUM:\n+            self._set_flush_status(True)\n+\n+    def _get_scheduled_num_args(self):\n+        return self.kernel_group.get_num_args()\n+\n+    def ready_to_flush(self):\n+        return self._ready_to_flush\n+\n     def codegen_sync(self):\n         pass\n \n     def flush(self):\n         self.kernel_group.codegen_define_and_call(V.graph.wrapper_code)\n         self.get_kernel_group()\n+        self._set_flush_status(False)\n \n \n class KernelGroup:\n@@ -2995,6 +3015,11 @@ class KernelGroup:\n         ws = self.ws\n         new_kernel.codegen_loops(code, ws)\n \n+    def get_num_args(self):\n+        arg_defs, call_args, arg_types = self.args.cpp_argdefs()\n+        args_num = len(arg_defs)\n+        return args_num\n+\n     def codegen_define_and_call(self, wrapper):\n         self.stack.close()\n         if not self.scheduled_nodes:\n"
        },
        {
            "name": "triton.py",
            "path": "torch/_inductor/codegen/triton.py",
            "patches": [
                {
                    "old_start": 2859,
                    "old_length": 6,
                    "new_start": 2859,
                    "new_length": 9,
                    "hunk": "@@ -2859,6 +2859,9 @@ class TritonScheduling(BaseScheduling):\n     def flush(self):\n         pass\n \n+    def ready_to_flush(self) -> bool:\n+        return False\n+\n     def benchmark_fused_nodes(self, nodes):\n         _, (numel, rnumel) = max(nodes, key=lambda x: int(x.is_reduction())).group\n         node_schedule = self.generate_node_schedule(nodes, numel, rnumel)\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    def ready_to_flush(self) -> bool:\n+        return False\n+\n",
            "whole_hunk": "@@ -2859,6 +2859,9 @@ class TritonScheduling(BaseScheduling):\n     def flush(self):\n         pass\n \n+    def ready_to_flush(self) -> bool:\n+        return False\n+\n     def benchmark_fused_nodes(self, nodes):\n         _, (numel, rnumel) = max(nodes, key=lambda x: int(x.is_reduction())).group\n         node_schedule = self.generate_node_schedule(nodes, numel, rnumel)\n"
        },
        {
            "name": "scheduler.py",
            "path": "torch/_inductor/scheduler.py",
            "patches": [
                {
                    "old_start": 2194,
                    "old_length": 6,
                    "new_start": 2194,
                    "new_length": 11,
                    "hunk": "@@ -2194,6 +2194,11 @@ class Scheduler:\n \n             self.available_buffer_names.update(node.get_names())\n \n+            if not isinstance(node, NopKernelSchedulerNode):\n+                device = node.get_device()\n+                if self.get_backend(device).ready_to_flush():\n+                    self.flush()\n+\n         self.flush()\n \n     def is_unaligned_buffer(self, buf_name):\n"
                },
                {
                    "old_start": 2250,
                    "old_length": 6,
                    "new_start": 2255,
                    "new_length": 13,
                    "hunk": "@@ -2250,6 +2255,13 @@ class BaseScheduling:\n         \"\"\"\n         raise NotImplementedError()\n \n+    def ready_to_flush(self) -> bool:\n+        \"\"\"\n+        Check whether the backend is requesting the scheduler to flush the generated kernel.\n+        If not supported, please return False.\n+        \"\"\"\n+        return False\n+\n     def flush(self):\n         \"\"\"\n         Flush the generated kernel and python wrapper code to the source code file."
                }
            ],
            "whole_deleted": "",
            "whole_added": "+            if not isinstance(node, NopKernelSchedulerNode):\n+                device = node.get_device()\n+                if self.get_backend(device).ready_to_flush():\n+                    self.flush()\n+\n+    def ready_to_flush(self) -> bool:\n+        \"\"\"\n+        Check whether the backend is requesting the scheduler to flush the generated kernel.\n+        If not supported, please return False.\n+        \"\"\"\n+        return False\n+\n",
            "whole_hunk": "@@ -2194,6 +2194,11 @@ class Scheduler:\n \n             self.available_buffer_names.update(node.get_names())\n \n+            if not isinstance(node, NopKernelSchedulerNode):\n+                device = node.get_device()\n+                if self.get_backend(device).ready_to_flush():\n+                    self.flush()\n+\n         self.flush()\n \n     def is_unaligned_buffer(self, buf_name):\n@@ -2250,6 +2255,13 @@ class BaseScheduling:\n         \"\"\"\n         raise NotImplementedError()\n \n+    def ready_to_flush(self) -> bool:\n+        \"\"\"\n+        Check whether the backend is requesting the scheduler to flush the generated kernel.\n+        If not supported, please return False.\n+        \"\"\"\n+        return False\n+\n     def flush(self):\n         \"\"\"\n         Flush the generated kernel and python wrapper code to the source code file."
        }
    ]
},
{
    "Id": 365,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/44b98c09ca90ce4529d1a9792b01a0e8afe5345e",
    "date": "2023-12-27T00:08:31+00:00",
    "message": "[BE] migrate all assertRaises tests to OptimizerInfo test_errors (#116315)\n\nRemoves a part of the sparse adam test and the following three tests: `test_fused_optimizer_raises`, `test_duplicate_params_across_param_groups`, `test_duplicate_params_in_one_param_group`\n\n```\n(pytorch-3.10) [janeyx@devgpu023.odn1 ~/local/pytorch (d2d129de)]$ python test/test_optim.py -k test_fused_optimizer_raises -k test_duplicate_params_across_param_groups -k test_duplicate_params_in_one_param_group\n/home/janeyx/.conda/envs/pytorch-3.10/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n...\n----------------------------------------------------------------------\nRan 3 tests in 0.023s\n\nOK\n```\n\nIncreases coverage by testing the duplicate param tests on ALL the optims instead of just one each. Also fixes SparseAdam bug which was accidentally calling torch.unbind through list instead of putting params in a list. This bug was caught by migrating the weird warning stuff to just one easy warning context manager, which checks that nothing else gets raised.\n\nThe new test_errors does not run slower than before, overhead is still king:\n```\n(pytorch-3.10) [janeyx@devgpu023.odn1 ~/local/pytorch (d2d129de)]$ python test/test_optim.py -k test_errors\n/home/janeyx/.conda/envs/pytorch-3.10/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n..........................\n----------------------------------------------------------------------\nRan 26 tests in 10.337s\n\nOK\n```\n\nCompared to test_errors BEFORE my commit :p\n```\n(pytorch-3.10) [janeyx@devgpu023.odn1 ~/local/pytorch (b47aa696)]$ python test/test_optim.py -k test_errors\n/home/janeyx/.conda/envs/pytorch-3.10/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n.............sssssssssssss\n----------------------------------------------------------------------\nRan 26 tests in 11.980s\n\nOK (skipped=13)\n(pytorch-3.10) [janeyx@devgpu023.odn1 ~/local/pytorch (b47aa696)]$\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116315\nApproved by: https://github.com/mikaylagawarecki",
    "label": "NO",
    "changes": [
        {
            "name": "test_optim.py",
            "path": "test/optim/test_optim.py",
            "patches": [
                {
                    "old_start": 906,
                    "old_length": 14,
                    "new_start": 906,
                    "new_length": 6,
                    "hunk": "@@ -906,14 +906,6 @@ class TestOptim(TestCase):\n             sparse_only=True,\n             maximize=True,\n         )\n-        import warnings\n-        with warnings.catch_warnings(record=True) as ws:\n-            SparseAdam(torch.zeros(3))\n-            self.assertEqual(len(ws), 1)\n-            for warning in ws:\n-                self.assertEqual(len(warning.message.args), 1)\n-                self.assertRegex(warning.message.args[0],\n-                                 \"Passing in a raw Tensor as ``params`` to SparseAdam \")\n \n     # ROCm precision is too low to pass this test\n     def test_adadelta(self):\n"
                },
                {
                    "old_start": 1438,
                    "old_length": 20,
                    "new_start": 1430,
                    "new_length": 6,
                    "hunk": "@@ -1438,20 +1430,6 @@ class TestOptim(TestCase):\n         self.assertEqual(type(res1), type(res2))\n \n \n-    def test_duplicate_params_in_one_param_group(self):\n-        param = Parameter(torch.randn(1))\n-        with self.assertWarnsOnceRegex(UserWarning, '.*a parameter group with duplicate parameters.*'):\n-            Adamax([param, param], lr=0.01)\n-\n-    def test_duplicate_params_across_param_groups(self):\n-        param = Parameter(torch.randn(1))\n-        self.assertRaisesRegex(\n-            ValueError,\n-            'some parameters appear in more than one parameter group',\n-            lambda: Adadelta([{'params': param}, {'params': param}])\n-        )\n-\n-\n     def test_fused_optimizer_does_not_step_if_foundinf(self):\n         if not torch.cuda.is_available():\n             self.skipTest(\"CUDA is required.\")\n"
                },
                {
                    "old_start": 1621,
                    "old_length": 14,
                    "new_start": 1599,
                    "new_length": 6,
                    "hunk": "@@ -1621,14 +1599,6 @@ class TestOptim(TestCase):\n         opt2.step()\n         self.assertListEqual(data, [0, 1, 2, 5, 0, 1, 2, 5, 0, 1, 2, 5])\n \n-    def test_fused_optimizer_raises(self):\n-        if not torch.cuda.is_available():\n-            self.skipTest(\"Requires CUDA devices\")\n-        for optimizer_ctor in (Adam, AdamW):\n-            with self.assertRaisesRegex(RuntimeError, \"`fused` and `foreach` cannot be `True` together.\"):\n-                optimizer_ctor([torch.empty((), device=\"cuda\")], foreach=True, fused=True)\n-            with self.assertRaisesRegex(RuntimeError, \"`fused` does not support `differentiable`\"):\n-                optimizer_ctor([torch.empty((), device=\"cuda\")], differentiable=True, fused=True)\n \n     @staticmethod\n     def _state_dict_pre_hook(optimizer: Optimizer) -> None:\n"
                }
            ],
            "whole_deleted": "-        import warnings\n-        with warnings.catch_warnings(record=True) as ws:\n-            SparseAdam(torch.zeros(3))\n-            self.assertEqual(len(ws), 1)\n-            for warning in ws:\n-                self.assertEqual(len(warning.message.args), 1)\n-                self.assertRegex(warning.message.args[0],\n-                                 \"Passing in a raw Tensor as ``params`` to SparseAdam \")\n-    def test_duplicate_params_in_one_param_group(self):\n-        param = Parameter(torch.randn(1))\n-        with self.assertWarnsOnceRegex(UserWarning, '.*a parameter group with duplicate parameters.*'):\n-            Adamax([param, param], lr=0.01)\n-\n-    def test_duplicate_params_across_param_groups(self):\n-        param = Parameter(torch.randn(1))\n-        self.assertRaisesRegex(\n-            ValueError,\n-            'some parameters appear in more than one parameter group',\n-            lambda: Adadelta([{'params': param}, {'params': param}])\n-        )\n-\n-\n-    def test_fused_optimizer_raises(self):\n-        if not torch.cuda.is_available():\n-            self.skipTest(\"Requires CUDA devices\")\n-        for optimizer_ctor in (Adam, AdamW):\n-            with self.assertRaisesRegex(RuntimeError, \"`fused` and `foreach` cannot be `True` together.\"):\n-                optimizer_ctor([torch.empty((), device=\"cuda\")], foreach=True, fused=True)\n-            with self.assertRaisesRegex(RuntimeError, \"`fused` does not support `differentiable`\"):\n-                optimizer_ctor([torch.empty((), device=\"cuda\")], differentiable=True, fused=True)\n",
            "whole_added": "",
            "whole_hunk": "@@ -906,14 +906,6 @@ class TestOptim(TestCase):\n             sparse_only=True,\n             maximize=True,\n         )\n-        import warnings\n-        with warnings.catch_warnings(record=True) as ws:\n-            SparseAdam(torch.zeros(3))\n-            self.assertEqual(len(ws), 1)\n-            for warning in ws:\n-                self.assertEqual(len(warning.message.args), 1)\n-                self.assertRegex(warning.message.args[0],\n-                                 \"Passing in a raw Tensor as ``params`` to SparseAdam \")\n \n     # ROCm precision is too low to pass this test\n     def test_adadelta(self):\n@@ -1438,20 +1430,6 @@ class TestOptim(TestCase):\n         self.assertEqual(type(res1), type(res2))\n \n \n-    def test_duplicate_params_in_one_param_group(self):\n-        param = Parameter(torch.randn(1))\n-        with self.assertWarnsOnceRegex(UserWarning, '.*a parameter group with duplicate parameters.*'):\n-            Adamax([param, param], lr=0.01)\n-\n-    def test_duplicate_params_across_param_groups(self):\n-        param = Parameter(torch.randn(1))\n-        self.assertRaisesRegex(\n-            ValueError,\n-            'some parameters appear in more than one parameter group',\n-            lambda: Adadelta([{'params': param}, {'params': param}])\n-        )\n-\n-\n     def test_fused_optimizer_does_not_step_if_foundinf(self):\n         if not torch.cuda.is_available():\n             self.skipTest(\"CUDA is required.\")\n@@ -1621,14 +1599,6 @@ class TestOptim(TestCase):\n         opt2.step()\n         self.assertListEqual(data, [0, 1, 2, 5, 0, 1, 2, 5, 0, 1, 2, 5])\n \n-    def test_fused_optimizer_raises(self):\n-        if not torch.cuda.is_available():\n-            self.skipTest(\"Requires CUDA devices\")\n-        for optimizer_ctor in (Adam, AdamW):\n-            with self.assertRaisesRegex(RuntimeError, \"`fused` and `foreach` cannot be `True` together.\"):\n-                optimizer_ctor([torch.empty((), device=\"cuda\")], foreach=True, fused=True)\n-            with self.assertRaisesRegex(RuntimeError, \"`fused` does not support `differentiable`\"):\n-                optimizer_ctor([torch.empty((), device=\"cuda\")], differentiable=True, fused=True)\n \n     @staticmethod\n     def _state_dict_pre_hook(optimizer: Optimizer) -> None:\n"
        },
        {
            "name": "test_optim.py",
            "path": "test/test_optim.py",
            "patches": [
                {
                    "old_start": 26,
                    "old_length": 7,
                    "new_start": 26,
                    "new_length": 6,
                    "hunk": "@@ -26,7 +26,6 @@ class TestOptimRenewed(TestCase):\n             self.assertFalse(any(f for f in global_cliquey_flags if f in optim_input.kwargs))\n \n \n-    @onlyCPU\n     @optims([optim for optim in optim_db if optim.optim_error_inputs_func is not None])\n     def test_errors(self, device, dtype, optim_info):\n         optim_cls = optim_info.optim_cls\n"
                },
                {
                    "old_start": 36,
                    "old_length": 12,
                    "new_start": 35,
                    "new_length": 20,
                    "hunk": "@@ -36,12 +35,20 @@ class TestOptimRenewed(TestCase):\n             optim_input = error_input.optimizer_error_input\n             params, kwargs = optim_input.params, optim_input.kwargs\n             if error_input.error_on == OptimizerErrorEnum.CONSTRUCTION_ERROR:\n-                with self.assertRaisesRegex(error_input.error_type, error_input.error_regex):\n-                    optim_cls(params, **kwargs)\n+                if issubclass(error_input.error_type, Warning):\n+                    with self.assertWarnsRegex(error_input.error_type, error_input.error_regex):\n+                        optim_cls(params, **kwargs)\n+                else:\n+                    with self.assertRaisesRegex(error_input.error_type, error_input.error_regex):\n+                        optim_cls(params, **kwargs)\n             elif error_input.error_on == OptimizerErrorEnum.STEP_ERROR:\n                 optim = optim_cls(params, **kwargs)\n-                with self.assertRaisesRegex(error_input.error_type, error_input.error_regex):\n-                    optim.step()\n+                if issubclass(error_input.error_type, Warning):\n+                    with self.assertWarnsRegex(error_input.error_type, error_input.error_regex):\n+                        optim.step()\n+                else:\n+                    with self.assertRaisesRegex(error_input.error_type, error_input.error_regex):\n+                        optim.step()\n             else:\n                 raise NotImplementedError(f\"Unknown error type {error_input.error_on}\")\n \n"
                }
            ],
            "whole_deleted": "-    @onlyCPU\n-                with self.assertRaisesRegex(error_input.error_type, error_input.error_regex):\n-                    optim_cls(params, **kwargs)\n-                with self.assertRaisesRegex(error_input.error_type, error_input.error_regex):\n-                    optim.step()\n",
            "whole_added": "+                if issubclass(error_input.error_type, Warning):\n+                    with self.assertWarnsRegex(error_input.error_type, error_input.error_regex):\n+                        optim_cls(params, **kwargs)\n+                else:\n+                    with self.assertRaisesRegex(error_input.error_type, error_input.error_regex):\n+                        optim_cls(params, **kwargs)\n+                if issubclass(error_input.error_type, Warning):\n+                    with self.assertWarnsRegex(error_input.error_type, error_input.error_regex):\n+                        optim.step()\n+                else:\n+                    with self.assertRaisesRegex(error_input.error_type, error_input.error_regex):\n+                        optim.step()\n",
            "whole_hunk": "@@ -26,7 +26,6 @@ class TestOptimRenewed(TestCase):\n             self.assertFalse(any(f for f in global_cliquey_flags if f in optim_input.kwargs))\n \n \n-    @onlyCPU\n     @optims([optim for optim in optim_db if optim.optim_error_inputs_func is not None])\n     def test_errors(self, device, dtype, optim_info):\n         optim_cls = optim_info.optim_cls\n@@ -36,12 +35,20 @@ class TestOptimRenewed(TestCase):\n             optim_input = error_input.optimizer_error_input\n             params, kwargs = optim_input.params, optim_input.kwargs\n             if error_input.error_on == OptimizerErrorEnum.CONSTRUCTION_ERROR:\n-                with self.assertRaisesRegex(error_input.error_type, error_input.error_regex):\n-                    optim_cls(params, **kwargs)\n+                if issubclass(error_input.error_type, Warning):\n+                    with self.assertWarnsRegex(error_input.error_type, error_input.error_regex):\n+                        optim_cls(params, **kwargs)\n+                else:\n+                    with self.assertRaisesRegex(error_input.error_type, error_input.error_regex):\n+                        optim_cls(params, **kwargs)\n             elif error_input.error_on == OptimizerErrorEnum.STEP_ERROR:\n                 optim = optim_cls(params, **kwargs)\n-                with self.assertRaisesRegex(error_input.error_type, error_input.error_regex):\n-                    optim.step()\n+                if issubclass(error_input.error_type, Warning):\n+                    with self.assertWarnsRegex(error_input.error_type, error_input.error_regex):\n+                        optim.step()\n+                else:\n+                    with self.assertRaisesRegex(error_input.error_type, error_input.error_regex):\n+                        optim.step()\n             else:\n                 raise NotImplementedError(f\"Unknown error type {error_input.error_on}\")\n \n"
        },
        {
            "name": "optimizer.py",
            "path": "torch/optim/optimizer.py",
            "patches": [
                {
                    "old_start": 260,
                    "old_length": 6,
                    "new_start": 260,
                    "new_length": 7,
                    "hunk": "@@ -260,6 +260,7 @@ class Optimizer:\n                                \"is deprecated. In the future, this will raise an error. \"\n                                \"Please wrap your Tensor in an iterable instead.\"),\n                               FutureWarning)\n+                params = [params]\n             else:\n                 raise TypeError(\"params argument given to the optimizer should be \"\n                                 \"an iterable of Tensors or dicts, but got \" +\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+                params = [params]\n",
            "whole_hunk": "@@ -260,6 +260,7 @@ class Optimizer:\n                                \"is deprecated. In the future, this will raise an error. \"\n                                \"Please wrap your Tensor in an iterable instead.\"),\n                               FutureWarning)\n+                params = [params]\n             else:\n                 raise TypeError(\"params argument given to the optimizer should be \"\n                                 \"an iterable of Tensors or dicts, but got \" +\n"
        },
        {
            "name": "common_optimizers.py",
            "path": "torch/testing/_internal/common_optimizers.py",
            "patches": [
                {
                    "old_start": 190,
                    "old_length": 6,
                    "new_start": 190,
                    "new_length": 43,
                    "hunk": "@@ -190,6 +190,43 @@ class optims(_TestParametrizer):\n                 raise ex\n \n \n+# Helper function for generating error inputs for all optimizers, used below.\n+def get_error_inputs_for_all_optims(device, dtype):\n+    if str(device) == \"cpu\":\n+        sample_param = Parameter(torch.randn(1, device=device, dtype=dtype))\n+        return [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=sample_param,\n+                    kwargs={},\n+                    desc=\"invalid param type\",\n+                ),\n+                error_type=TypeError,\n+                error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n+            ),\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=[sample_param, sample_param],\n+                    kwargs={},\n+                    desc=\"a param group cannot have duplicate parameters\",\n+                ),\n+                error_type=UserWarning,\n+                error_regex=\".*a parameter group with duplicate parameters.*\",\n+            ),\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=[{\"params\": sample_param}, {\"params\": sample_param}],\n+                    kwargs={},\n+                    desc=\"duplicate parameters should not occur across param groups either\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"some parameters appear in more than one parameter group\",\n+            ),\n+        ]\n+    else:\n+        return []\n+\n+\n # ------------------------------------------------------------------------------------------\n # NOTE: [optimizer kwarg categories]\n # We categorize optimizer kwargs as 3 types:\n"
                },
                {
                    "old_start": 237,
                    "old_length": 26,
                    "new_start": 274,
                    "new_length": 20,
                    "hunk": "@@ -237,26 +274,20 @@ def optim_inputs_func_adadelta(device=None):\n \n \n def optim_error_inputs_func_adadelta(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, rho=1.1),\n-                desc=\"rho should be between 0 and 1\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid rho value: 1.1\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, rho=1.1),\n+                    desc=\"rho should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid rho value: 1.1\",\n             ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n+        ]\n+    return error_inputs\n \n \n def optim_inputs_func_adagrad(device=None):\n"
                },
                {
                    "old_start": 284,
                    "old_length": 26,
                    "new_start": 315,
                    "new_length": 20,
                    "hunk": "@@ -284,26 +315,20 @@ def optim_inputs_func_adagrad(device=None):\n \n \n def optim_error_inputs_func_adagrad(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, lr_decay=-0.5),\n-                desc=\"lr_decay must be bigger than 0\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid lr_decay value: -0.5\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, lr_decay=-0.5),\n+                    desc=\"lr_decay must be bigger than 0\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid lr_decay value: -0.5\",\n             ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n+        ]\n+    return error_inputs\n \n \n # TODO: consider tensor LR! See multi_tensor_optimizer_configs in test_optim.py --> tensor LR should work\n"
                },
                {
                    "old_start": 341,
                    "old_length": 44,
                    "new_start": 366,
                    "new_length": 60,
                    "hunk": "@@ -341,44 +366,60 @@ def optim_inputs_func_adam(device=None):\n \n \n def optim_error_inputs_func_adam(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n-                desc=\"beta1 should be between 0 and 1\",\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n+                    desc=\"beta1 should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid beta parameter at index 0: 1.0\",\n             ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid beta parameter at index 0: 1.0\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, weight_decay=-1),\n-                desc=\"weight_decay should > 0\",\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, weight_decay=-1),\n+                    desc=\"weight_decay should > 0\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid weight_decay value: -1\",\n             ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid weight_decay value: -1\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=torch.tensor(0.001), foreach=True),\n-                desc=\"lr as Tensor doesn't work with foreach & not capturable\",\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=torch.tensor(0.001), foreach=True),\n+                    desc=\"lr as Tensor doesn't work with foreach & not capturable\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"lr as a Tensor is not supported for capturable=False and foreach=True\",\n             ),\n-            error_type=ValueError,\n-            error_regex=\"lr as a Tensor is not supported for capturable=False and foreach=True\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n+        ]\n+    if str(device) == \"cuda\":\n+        sample_tensor = torch.empty((), device=device, dtype=dtype)\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=[sample_tensor],\n+                    kwargs={\"foreach\": True, \"fused\": True},\n+                    desc=\"`fused` and `foreach` cannot be `True` together\",\n+                ),\n+                error_type=RuntimeError,\n+                error_regex=\"`fused` and `foreach` cannot be `True` together\",\n             ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=[sample_tensor],\n+                    kwargs={\"fused\": True, \"differentiable\": True},\n+                    desc=\"`fused` does not support `differentiable`\",\n+                ),\n+                error_type=RuntimeError,\n+                error_regex=\"`fused` does not support `differentiable`\",\n+            ),\n+        ]\n+    return error_inputs\n \n \n def optim_inputs_func_adamax(device=None):\n"
                },
                {
                    "old_start": 397,
                    "old_length": 26,
                    "new_start": 438,
                    "new_length": 20,
                    "hunk": "@@ -397,26 +438,20 @@ def optim_inputs_func_adamax(device=None):\n \n \n def optim_error_inputs_func_adamax(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, betas=(0.0, 1.0)),\n-                desc=\"beta2 should be between 0 and 1\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid beta parameter at index 1: 1.0\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, betas=(0.0, 1.0)),\n+                    desc=\"beta2 should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid beta parameter at index 1: 1.0\",\n             ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n+        ]\n+    return error_inputs\n \n \n def optim_inputs_func_adamw(device=None):\n"
                },
                {
                    "old_start": 444,
                    "old_length": 26,
                    "new_start": 479,
                    "new_length": 20,
                    "hunk": "@@ -444,26 +479,20 @@ def optim_inputs_func_asgd(device=None):\n \n \n def optim_error_inputs_func_asgd(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, weight_decay=-0.5),\n-                desc=\"weight_decay should > 0\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid weight_decay value: -0.5\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, weight_decay=-0.5),\n+                    desc=\"weight_decay should > 0\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid weight_decay value: -0.5\",\n             ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n+        ]\n+    return error_inputs\n \n \n def optim_inputs_func_lbfgs(device=None):\n"
                },
                {
                    "old_start": 482,
                    "old_length": 17,
                    "new_start": 511,
                    "new_length": 7,
                    "hunk": "@@ -482,17 +511,7 @@ def optim_inputs_func_lbfgs(device=None):\n \n \n def optim_error_inputs_func_lbfgs(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n-            ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n+    return get_error_inputs_for_all_optims(device, dtype)\n \n \n # Weird story bro, NAdam and RAdam do not have maximize.\n"
                },
                {
                    "old_start": 526,
                    "old_length": 35,
                    "new_start": 545,
                    "new_length": 29,
                    "hunk": "@@ -526,35 +545,29 @@ def optim_inputs_func_nadam(device=None):\n \n \n def optim_error_inputs_func_nadam(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n-                desc=\"beta1 should be between 0 and 1\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid beta parameter at index 0: 1.0\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, momentum_decay=-0.2),\n-                desc=\"momentum_decay should > 0\",\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n+                    desc=\"beta1 should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid beta parameter at index 0: 1.0\",\n             ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid momentum_decay value: -0.2\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, momentum_decay=-0.2),\n+                    desc=\"momentum_decay should > 0\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid momentum_decay value: -0.2\",\n             ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n+        ]\n+    return error_inputs\n \n \n # Weird story bro, NAdam and RAdam do not have maximize.\n"
                },
                {
                    "old_start": 575,
                    "old_length": 35,
                    "new_start": 588,
                    "new_length": 29,
                    "hunk": "@@ -575,35 +588,29 @@ def optim_inputs_func_radam(device=None):\n \n \n def optim_error_inputs_func_radam(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n-                desc=\"beta1 should be between 0 and 1\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid beta parameter at index 0: 1.0\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, weight_decay=-1),\n-                desc=\"weight_decay should > 0\",\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n+                    desc=\"beta1 should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid beta parameter at index 0: 1.0\",\n             ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid weight_decay value: -1\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, weight_decay=-1),\n+                    desc=\"weight_decay should > 0\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid weight_decay value: -1\",\n             ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n+        ]\n+    return error_inputs\n \n \n def optim_inputs_func_rmsprop(device=None):\n"
                },
                {
                    "old_start": 637,
                    "old_length": 26,
                    "new_start": 644,
                    "new_length": 20,
                    "hunk": "@@ -637,26 +644,20 @@ def optim_inputs_func_rmsprop(device=None):\n \n \n def optim_error_inputs_func_rmsprop(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, momentum=-1.0),\n-                desc=\"momentum should be between 0 and 1\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid momentum value: -1.0\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, momentum=-1.0),\n+                    desc=\"momentum should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid momentum value: -1.0\",\n             ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n+        ]\n+    return error_inputs\n \n \n def optim_inputs_func_rprop(device=None):\n"
                },
                {
                    "old_start": 676,
                    "old_length": 26,
                    "new_start": 677,
                    "new_length": 20,
                    "hunk": "@@ -676,26 +677,20 @@ def optim_inputs_func_rprop(device=None):\n \n \n def optim_error_inputs_func_rprop(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, etas=(1.0, 0.5)),\n-                desc=\"0 < eta1 < 1 < eta2\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid eta values: 1.0, 0.5\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, etas=(1.0, 0.5)),\n+                    desc=\"0 < eta1 < 1 < eta2\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid eta values: 1.0, 0.5\",\n             ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n+        ]\n+    return error_inputs\n \n \n def optim_inputs_func_sgd(device=None):\n"
                },
                {
                    "old_start": 728,
                    "old_length": 26,
                    "new_start": 723,
                    "new_length": 20,
                    "hunk": "@@ -728,26 +723,20 @@ def optim_inputs_func_sgd(device=None):\n \n \n def optim_error_inputs_func_sgd(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, momentum=-0.5),\n-                desc=\"momentum should be between 0 and 1\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid momentum value: -0.5\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, momentum=-0.5),\n+                    desc=\"momentum should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid momentum value: -0.5\",\n             ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n+        ]\n+    return error_inputs\n \n \n def optim_inputs_func_sparseadam(device=None):\n"
                },
                {
                    "old_start": 761,
                    "old_length": 45,
                    "new_start": 750,
                    "new_length": 61,
                    "hunk": "@@ -761,45 +750,61 @@ def optim_inputs_func_sparseadam(device=None):\n \n \n def optim_error_inputs_func_sparseadam(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n-                desc=\"beta1 should be between 0 and 1\",\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+\n+    if str(device) == \"cpu\":\n+        # SparseAdam raises a warning and not an error for the first entry. We\n+        # update it here:\n+        error_inputs[0].error_type = FutureWarning\n+        error_inputs[\n+            0\n+        ].error_regex = \"Passing in a raw Tensor as ``params`` to SparseAdam\"\n+\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n+                    desc=\"beta1 should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid beta parameter at index 0: 1.0\",\n             ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid beta parameter at index 0: 1.0\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=[\n-                    torch.zeros(3, layout=torch.sparse_coo, device=device, dtype=dtype)\n-                ],\n-                kwargs={},\n-                desc=\"dense params required\",\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=[\n+                        torch.zeros(\n+                            3, layout=torch.sparse_coo, device=device, dtype=dtype\n+                        )\n+                    ],\n+                    kwargs={},\n+                    desc=\"dense params required\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"SparseAdam requires dense parameter tensors\",\n             ),\n-            error_type=ValueError,\n-            error_regex=\"SparseAdam requires dense parameter tensors\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=[\n-                    {\n-                        \"params\": [\n-                            torch.zeros(\n-                                3, layout=torch.sparse_coo, device=device, dtype=dtype\n-                            )\n-                        ]\n-                    }\n-                ],\n-                kwargs={},\n-                desc=\"dense params required in param_groups\",\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=[\n+                        {\n+                            \"params\": [\n+                                torch.zeros(\n+                                    3,\n+                                    layout=torch.sparse_coo,\n+                                    device=device,\n+                                    dtype=dtype,\n+                                )\n+                            ]\n+                        }\n+                    ],\n+                    kwargs={},\n+                    desc=\"dense params required in param_groups\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"SparseAdam requires dense parameter tensors\",\n             ),\n-            error_type=ValueError,\n-            error_regex=\"SparseAdam requires dense parameter tensors\",\n-        ),\n-    ]\n+        ]\n+    return error_inputs\n \n \n # Database of OptimizerInfo entries in alphabetical order."
                }
            ],
            "whole_deleted": "-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, rho=1.1),\n-                desc=\"rho should be between 0 and 1\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid rho value: 1.1\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, lr_decay=-0.5),\n-                desc=\"lr_decay must be bigger than 0\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid lr_decay value: -0.5\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n-                desc=\"beta1 should be between 0 and 1\",\n-            error_type=ValueError,\n-            error_regex=\"Invalid beta parameter at index 0: 1.0\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, weight_decay=-1),\n-                desc=\"weight_decay should > 0\",\n-            error_type=ValueError,\n-            error_regex=\"Invalid weight_decay value: -1\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=torch.tensor(0.001), foreach=True),\n-                desc=\"lr as Tensor doesn't work with foreach & not capturable\",\n-            error_type=ValueError,\n-            error_regex=\"lr as a Tensor is not supported for capturable=False and foreach=True\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, betas=(0.0, 1.0)),\n-                desc=\"beta2 should be between 0 and 1\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid beta parameter at index 1: 1.0\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, weight_decay=-0.5),\n-                desc=\"weight_decay should > 0\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid weight_decay value: -0.5\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n-            ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n-                desc=\"beta1 should be between 0 and 1\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid beta parameter at index 0: 1.0\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, momentum_decay=-0.2),\n-                desc=\"momentum_decay should > 0\",\n-            error_type=ValueError,\n-            error_regex=\"Invalid momentum_decay value: -0.2\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n-                desc=\"beta1 should be between 0 and 1\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid beta parameter at index 0: 1.0\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, weight_decay=-1),\n-                desc=\"weight_decay should > 0\",\n-            error_type=ValueError,\n-            error_regex=\"Invalid weight_decay value: -1\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, momentum=-1.0),\n-                desc=\"momentum should be between 0 and 1\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid momentum value: -1.0\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, etas=(1.0, 0.5)),\n-                desc=\"0 < eta1 < 1 < eta2\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid eta values: 1.0, 0.5\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, momentum=-0.5),\n-                desc=\"momentum should be between 0 and 1\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid momentum value: -0.5\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n-                desc=\"beta1 should be between 0 and 1\",\n-            error_type=ValueError,\n-            error_regex=\"Invalid beta parameter at index 0: 1.0\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=[\n-                    torch.zeros(3, layout=torch.sparse_coo, device=device, dtype=dtype)\n-                ],\n-                kwargs={},\n-                desc=\"dense params required\",\n-            error_type=ValueError,\n-            error_regex=\"SparseAdam requires dense parameter tensors\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=[\n-                    {\n-                        \"params\": [\n-                            torch.zeros(\n-                                3, layout=torch.sparse_coo, device=device, dtype=dtype\n-                            )\n-                        ]\n-                    }\n-                ],\n-                kwargs={},\n-                desc=\"dense params required in param_groups\",\n-            error_type=ValueError,\n-            error_regex=\"SparseAdam requires dense parameter tensors\",\n-        ),\n-    ]\n",
            "whole_added": "+# Helper function for generating error inputs for all optimizers, used below.\n+def get_error_inputs_for_all_optims(device, dtype):\n+    if str(device) == \"cpu\":\n+        sample_param = Parameter(torch.randn(1, device=device, dtype=dtype))\n+        return [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=sample_param,\n+                    kwargs={},\n+                    desc=\"invalid param type\",\n+                ),\n+                error_type=TypeError,\n+                error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n+            ),\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=[sample_param, sample_param],\n+                    kwargs={},\n+                    desc=\"a param group cannot have duplicate parameters\",\n+                ),\n+                error_type=UserWarning,\n+                error_regex=\".*a parameter group with duplicate parameters.*\",\n+            ),\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=[{\"params\": sample_param}, {\"params\": sample_param}],\n+                    kwargs={},\n+                    desc=\"duplicate parameters should not occur across param groups either\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"some parameters appear in more than one parameter group\",\n+            ),\n+        ]\n+    else:\n+        return []\n+\n+\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, rho=1.1),\n+                    desc=\"rho should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid rho value: 1.1\",\n+        ]\n+    return error_inputs\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, lr_decay=-0.5),\n+                    desc=\"lr_decay must be bigger than 0\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid lr_decay value: -0.5\",\n+        ]\n+    return error_inputs\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n+                    desc=\"beta1 should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid beta parameter at index 0: 1.0\",\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, weight_decay=-1),\n+                    desc=\"weight_decay should > 0\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid weight_decay value: -1\",\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=torch.tensor(0.001), foreach=True),\n+                    desc=\"lr as Tensor doesn't work with foreach & not capturable\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"lr as a Tensor is not supported for capturable=False and foreach=True\",\n+        ]\n+    if str(device) == \"cuda\":\n+        sample_tensor = torch.empty((), device=device, dtype=dtype)\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=[sample_tensor],\n+                    kwargs={\"foreach\": True, \"fused\": True},\n+                    desc=\"`fused` and `foreach` cannot be `True` together\",\n+                ),\n+                error_type=RuntimeError,\n+                error_regex=\"`fused` and `foreach` cannot be `True` together\",\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=[sample_tensor],\n+                    kwargs={\"fused\": True, \"differentiable\": True},\n+                    desc=\"`fused` does not support `differentiable`\",\n+                ),\n+                error_type=RuntimeError,\n+                error_regex=\"`fused` does not support `differentiable`\",\n+            ),\n+        ]\n+    return error_inputs\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, betas=(0.0, 1.0)),\n+                    desc=\"beta2 should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid beta parameter at index 1: 1.0\",\n+        ]\n+    return error_inputs\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, weight_decay=-0.5),\n+                    desc=\"weight_decay should > 0\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid weight_decay value: -0.5\",\n+        ]\n+    return error_inputs\n+    return get_error_inputs_for_all_optims(device, dtype)\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n+                    desc=\"beta1 should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid beta parameter at index 0: 1.0\",\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, momentum_decay=-0.2),\n+                    desc=\"momentum_decay should > 0\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid momentum_decay value: -0.2\",\n+        ]\n+    return error_inputs\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n+                    desc=\"beta1 should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid beta parameter at index 0: 1.0\",\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, weight_decay=-1),\n+                    desc=\"weight_decay should > 0\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid weight_decay value: -1\",\n+        ]\n+    return error_inputs\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, momentum=-1.0),\n+                    desc=\"momentum should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid momentum value: -1.0\",\n+        ]\n+    return error_inputs\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, etas=(1.0, 0.5)),\n+                    desc=\"0 < eta1 < 1 < eta2\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid eta values: 1.0, 0.5\",\n+        ]\n+    return error_inputs\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, momentum=-0.5),\n+                    desc=\"momentum should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid momentum value: -0.5\",\n+        ]\n+    return error_inputs\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+\n+    if str(device) == \"cpu\":\n+        # SparseAdam raises a warning and not an error for the first entry. We\n+        # update it here:\n+        error_inputs[0].error_type = FutureWarning\n+        error_inputs[\n+            0\n+        ].error_regex = \"Passing in a raw Tensor as ``params`` to SparseAdam\"\n+\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n+                    desc=\"beta1 should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid beta parameter at index 0: 1.0\",\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=[\n+                        torch.zeros(\n+                            3, layout=torch.sparse_coo, device=device, dtype=dtype\n+                        )\n+                    ],\n+                    kwargs={},\n+                    desc=\"dense params required\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"SparseAdam requires dense parameter tensors\",\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=[\n+                        {\n+                            \"params\": [\n+                                torch.zeros(\n+                                    3,\n+                                    layout=torch.sparse_coo,\n+                                    device=device,\n+                                    dtype=dtype,\n+                                )\n+                            ]\n+                        }\n+                    ],\n+                    kwargs={},\n+                    desc=\"dense params required in param_groups\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"SparseAdam requires dense parameter tensors\",\n+        ]\n+    return error_inputs\n",
            "whole_hunk": "@@ -190,6 +190,43 @@ class optims(_TestParametrizer):\n                 raise ex\n \n \n+# Helper function for generating error inputs for all optimizers, used below.\n+def get_error_inputs_for_all_optims(device, dtype):\n+    if str(device) == \"cpu\":\n+        sample_param = Parameter(torch.randn(1, device=device, dtype=dtype))\n+        return [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=sample_param,\n+                    kwargs={},\n+                    desc=\"invalid param type\",\n+                ),\n+                error_type=TypeError,\n+                error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n+            ),\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=[sample_param, sample_param],\n+                    kwargs={},\n+                    desc=\"a param group cannot have duplicate parameters\",\n+                ),\n+                error_type=UserWarning,\n+                error_regex=\".*a parameter group with duplicate parameters.*\",\n+            ),\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=[{\"params\": sample_param}, {\"params\": sample_param}],\n+                    kwargs={},\n+                    desc=\"duplicate parameters should not occur across param groups either\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"some parameters appear in more than one parameter group\",\n+            ),\n+        ]\n+    else:\n+        return []\n+\n+\n # ------------------------------------------------------------------------------------------\n # NOTE: [optimizer kwarg categories]\n # We categorize optimizer kwargs as 3 types:\n@@ -237,26 +274,20 @@ def optim_inputs_func_adadelta(device=None):\n \n \n def optim_error_inputs_func_adadelta(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, rho=1.1),\n-                desc=\"rho should be between 0 and 1\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid rho value: 1.1\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, rho=1.1),\n+                    desc=\"rho should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid rho value: 1.1\",\n             ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n+        ]\n+    return error_inputs\n \n \n def optim_inputs_func_adagrad(device=None):\n@@ -284,26 +315,20 @@ def optim_inputs_func_adagrad(device=None):\n \n \n def optim_error_inputs_func_adagrad(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, lr_decay=-0.5),\n-                desc=\"lr_decay must be bigger than 0\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid lr_decay value: -0.5\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, lr_decay=-0.5),\n+                    desc=\"lr_decay must be bigger than 0\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid lr_decay value: -0.5\",\n             ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n+        ]\n+    return error_inputs\n \n \n # TODO: consider tensor LR! See multi_tensor_optimizer_configs in test_optim.py --> tensor LR should work\n@@ -341,44 +366,60 @@ def optim_inputs_func_adam(device=None):\n \n \n def optim_error_inputs_func_adam(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n-                desc=\"beta1 should be between 0 and 1\",\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n+                    desc=\"beta1 should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid beta parameter at index 0: 1.0\",\n             ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid beta parameter at index 0: 1.0\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, weight_decay=-1),\n-                desc=\"weight_decay should > 0\",\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, weight_decay=-1),\n+                    desc=\"weight_decay should > 0\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid weight_decay value: -1\",\n             ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid weight_decay value: -1\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=torch.tensor(0.001), foreach=True),\n-                desc=\"lr as Tensor doesn't work with foreach & not capturable\",\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=torch.tensor(0.001), foreach=True),\n+                    desc=\"lr as Tensor doesn't work with foreach & not capturable\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"lr as a Tensor is not supported for capturable=False and foreach=True\",\n             ),\n-            error_type=ValueError,\n-            error_regex=\"lr as a Tensor is not supported for capturable=False and foreach=True\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n+        ]\n+    if str(device) == \"cuda\":\n+        sample_tensor = torch.empty((), device=device, dtype=dtype)\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=[sample_tensor],\n+                    kwargs={\"foreach\": True, \"fused\": True},\n+                    desc=\"`fused` and `foreach` cannot be `True` together\",\n+                ),\n+                error_type=RuntimeError,\n+                error_regex=\"`fused` and `foreach` cannot be `True` together\",\n             ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=[sample_tensor],\n+                    kwargs={\"fused\": True, \"differentiable\": True},\n+                    desc=\"`fused` does not support `differentiable`\",\n+                ),\n+                error_type=RuntimeError,\n+                error_regex=\"`fused` does not support `differentiable`\",\n+            ),\n+        ]\n+    return error_inputs\n \n \n def optim_inputs_func_adamax(device=None):\n@@ -397,26 +438,20 @@ def optim_inputs_func_adamax(device=None):\n \n \n def optim_error_inputs_func_adamax(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, betas=(0.0, 1.0)),\n-                desc=\"beta2 should be between 0 and 1\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid beta parameter at index 1: 1.0\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, betas=(0.0, 1.0)),\n+                    desc=\"beta2 should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid beta parameter at index 1: 1.0\",\n             ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n+        ]\n+    return error_inputs\n \n \n def optim_inputs_func_adamw(device=None):\n@@ -444,26 +479,20 @@ def optim_inputs_func_asgd(device=None):\n \n \n def optim_error_inputs_func_asgd(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, weight_decay=-0.5),\n-                desc=\"weight_decay should > 0\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid weight_decay value: -0.5\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, weight_decay=-0.5),\n+                    desc=\"weight_decay should > 0\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid weight_decay value: -0.5\",\n             ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n+        ]\n+    return error_inputs\n \n \n def optim_inputs_func_lbfgs(device=None):\n@@ -482,17 +511,7 @@ def optim_inputs_func_lbfgs(device=None):\n \n \n def optim_error_inputs_func_lbfgs(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n-            ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n+    return get_error_inputs_for_all_optims(device, dtype)\n \n \n # Weird story bro, NAdam and RAdam do not have maximize.\n@@ -526,35 +545,29 @@ def optim_inputs_func_nadam(device=None):\n \n \n def optim_error_inputs_func_nadam(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n-                desc=\"beta1 should be between 0 and 1\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid beta parameter at index 0: 1.0\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, momentum_decay=-0.2),\n-                desc=\"momentum_decay should > 0\",\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n+                    desc=\"beta1 should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid beta parameter at index 0: 1.0\",\n             ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid momentum_decay value: -0.2\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, momentum_decay=-0.2),\n+                    desc=\"momentum_decay should > 0\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid momentum_decay value: -0.2\",\n             ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n+        ]\n+    return error_inputs\n \n \n # Weird story bro, NAdam and RAdam do not have maximize.\n@@ -575,35 +588,29 @@ def optim_inputs_func_radam(device=None):\n \n \n def optim_error_inputs_func_radam(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n-                desc=\"beta1 should be between 0 and 1\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid beta parameter at index 0: 1.0\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, weight_decay=-1),\n-                desc=\"weight_decay should > 0\",\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n+                    desc=\"beta1 should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid beta parameter at index 0: 1.0\",\n             ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid weight_decay value: -1\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, weight_decay=-1),\n+                    desc=\"weight_decay should > 0\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid weight_decay value: -1\",\n             ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n+        ]\n+    return error_inputs\n \n \n def optim_inputs_func_rmsprop(device=None):\n@@ -637,26 +644,20 @@ def optim_inputs_func_rmsprop(device=None):\n \n \n def optim_error_inputs_func_rmsprop(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, momentum=-1.0),\n-                desc=\"momentum should be between 0 and 1\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid momentum value: -1.0\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, momentum=-1.0),\n+                    desc=\"momentum should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid momentum value: -1.0\",\n             ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n+        ]\n+    return error_inputs\n \n \n def optim_inputs_func_rprop(device=None):\n@@ -676,26 +677,20 @@ def optim_inputs_func_rprop(device=None):\n \n \n def optim_error_inputs_func_rprop(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, etas=(1.0, 0.5)),\n-                desc=\"0 < eta1 < 1 < eta2\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid eta values: 1.0, 0.5\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, etas=(1.0, 0.5)),\n+                    desc=\"0 < eta1 < 1 < eta2\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid eta values: 1.0, 0.5\",\n             ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n+        ]\n+    return error_inputs\n \n \n def optim_inputs_func_sgd(device=None):\n@@ -728,26 +723,20 @@ def optim_inputs_func_sgd(device=None):\n \n \n def optim_error_inputs_func_sgd(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, momentum=-0.5),\n-                desc=\"momentum should be between 0 and 1\",\n-            ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid momentum value: -0.5\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=Parameter(torch.randn(1, device=device, dtype=dtype)),\n-                kwargs={},\n-                desc=\"invalid param type\",\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+    if str(device) == \"cpu\":\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, momentum=-0.5),\n+                    desc=\"momentum should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid momentum value: -0.5\",\n             ),\n-            error_type=TypeError,\n-            error_regex=\"params argument given to the optimizer should be an iterable of Tensors or dicts\",\n-        ),\n-    ]\n+        ]\n+    return error_inputs\n \n \n def optim_inputs_func_sparseadam(device=None):\n@@ -761,45 +750,61 @@ def optim_inputs_func_sparseadam(device=None):\n \n \n def optim_error_inputs_func_sparseadam(device, dtype):\n-    return [\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=None,\n-                kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n-                desc=\"beta1 should be between 0 and 1\",\n+    error_inputs = get_error_inputs_for_all_optims(device, dtype)\n+\n+    if str(device) == \"cpu\":\n+        # SparseAdam raises a warning and not an error for the first entry. We\n+        # update it here:\n+        error_inputs[0].error_type = FutureWarning\n+        error_inputs[\n+            0\n+        ].error_regex = \"Passing in a raw Tensor as ``params`` to SparseAdam\"\n+\n+        error_inputs += [\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=None,\n+                    kwargs=dict(lr=1e-2, betas=(1.0, 0.0)),\n+                    desc=\"beta1 should be between 0 and 1\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"Invalid beta parameter at index 0: 1.0\",\n             ),\n-            error_type=ValueError,\n-            error_regex=\"Invalid beta parameter at index 0: 1.0\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=[\n-                    torch.zeros(3, layout=torch.sparse_coo, device=device, dtype=dtype)\n-                ],\n-                kwargs={},\n-                desc=\"dense params required\",\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=[\n+                        torch.zeros(\n+                            3, layout=torch.sparse_coo, device=device, dtype=dtype\n+                        )\n+                    ],\n+                    kwargs={},\n+                    desc=\"dense params required\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"SparseAdam requires dense parameter tensors\",\n             ),\n-            error_type=ValueError,\n-            error_regex=\"SparseAdam requires dense parameter tensors\",\n-        ),\n-        ErrorOptimizerInput(\n-            OptimizerInput(\n-                params=[\n-                    {\n-                        \"params\": [\n-                            torch.zeros(\n-                                3, layout=torch.sparse_coo, device=device, dtype=dtype\n-                            )\n-                        ]\n-                    }\n-                ],\n-                kwargs={},\n-                desc=\"dense params required in param_groups\",\n+            ErrorOptimizerInput(\n+                OptimizerInput(\n+                    params=[\n+                        {\n+                            \"params\": [\n+                                torch.zeros(\n+                                    3,\n+                                    layout=torch.sparse_coo,\n+                                    device=device,\n+                                    dtype=dtype,\n+                                )\n+                            ]\n+                        }\n+                    ],\n+                    kwargs={},\n+                    desc=\"dense params required in param_groups\",\n+                ),\n+                error_type=ValueError,\n+                error_regex=\"SparseAdam requires dense parameter tensors\",\n             ),\n-            error_type=ValueError,\n-            error_regex=\"SparseAdam requires dense parameter tensors\",\n-        ),\n-    ]\n+        ]\n+    return error_inputs\n \n \n # Database of OptimizerInfo entries in alphabetical order."
        }
    ]
},
{
    "Id": 335,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/29fa6fbc4eda6c02ecdfd73b74a8702187c4fc44",
    "date": "2024-01-17T18:41:42+00:00",
    "message": "[Dynamo] Fix a corner case of reinplace_inplaceable_ops pass for triton kernels (#117612)\n\nSummary:\nWe saw the following failure when compiling custom triton kernels:\n```\nRuntimeError: Argument 'getitem_22' of Node 'triton_kernel_wrapper_functional_proxy_3' was used before it has been defined! Please check that Nodes in the graph are topologically ordered\n```\nThe root-cause is when doing the replacement, the replacement is replaced by another replacement. The fix will keep finding the replacement until it is not replaced\n\nTest Plan:\n\nAdded a test case\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/117612\nApproved by: https://github.com/aakhundov",
    "label": "YES",
    "changes": [
        {
            "name": "test_triton_kernels.py",
            "path": "test/dynamo/test_triton_kernels.py",
            "patches": [
                {
                    "old_start": 476,
                    "old_length": 6,
                    "new_start": 476,
                    "new_length": 26,
                    "hunk": "@@ -476,6 +476,26 @@ def forward(self, x_1, output_1):\n         compiled_result = torch.compile(call_triton)(t1, t2)\n         self.assertEqual(torch_result, compiled_result)\n \n+    @requires_cuda()\n+    @skipIfRocm\n+    def test_triton_kernel_reinplace_inplaceable_pass(self):\n+        def call_triton(\n+            x: torch.Tensor,\n+            y: torch.Tensor,\n+        ):\n+            output = torch.zeros_like(x)\n+            n_elements = output.numel()\n+            grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n+            add_kernel_autotuned[grid](x, y, output, n_elements)\n+            add_kernel_autotuned[grid](output, x, output, n_elements)\n+            return output\n+\n+        t1 = torch.rand(5, device=\"cuda\")\n+        t2 = torch.rand(5, device=\"cuda\")\n+        torch_result = call_triton(t1, t2)\n+        compiled_result = torch.compile(call_triton)(t1, t2)\n+        self.assertEqual(torch_result, compiled_result)\n+\n     @requires_cuda()\n     @common_utils.parametrize(\"grad\", [False, True])\n     def test_triton_kernel_multi_kernel(self, grad):\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    @requires_cuda()\n+    @skipIfRocm\n+    def test_triton_kernel_reinplace_inplaceable_pass(self):\n+        def call_triton(\n+            x: torch.Tensor,\n+            y: torch.Tensor,\n+        ):\n+            output = torch.zeros_like(x)\n+            n_elements = output.numel()\n+            grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n+            add_kernel_autotuned[grid](x, y, output, n_elements)\n+            add_kernel_autotuned[grid](output, x, output, n_elements)\n+            return output\n+\n+        t1 = torch.rand(5, device=\"cuda\")\n+        t2 = torch.rand(5, device=\"cuda\")\n+        torch_result = call_triton(t1, t2)\n+        compiled_result = torch.compile(call_triton)(t1, t2)\n+        self.assertEqual(torch_result, compiled_result)\n+\n",
            "whole_hunk": "@@ -476,6 +476,26 @@ def forward(self, x_1, output_1):\n         compiled_result = torch.compile(call_triton)(t1, t2)\n         self.assertEqual(torch_result, compiled_result)\n \n+    @requires_cuda()\n+    @skipIfRocm\n+    def test_triton_kernel_reinplace_inplaceable_pass(self):\n+        def call_triton(\n+            x: torch.Tensor,\n+            y: torch.Tensor,\n+        ):\n+            output = torch.zeros_like(x)\n+            n_elements = output.numel()\n+            grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n+            add_kernel_autotuned[grid](x, y, output, n_elements)\n+            add_kernel_autotuned[grid](output, x, output, n_elements)\n+            return output\n+\n+        t1 = torch.rand(5, device=\"cuda\")\n+        t2 = torch.rand(5, device=\"cuda\")\n+        torch_result = call_triton(t1, t2)\n+        compiled_result = torch.compile(call_triton)(t1, t2)\n+        self.assertEqual(torch_result, compiled_result)\n+\n     @requires_cuda()\n     @common_utils.parametrize(\"grad\", [False, True])\n     def test_triton_kernel_multi_kernel(self, grad):\n"
        },
        {
            "name": "post_grad.py",
            "path": "torch/_inductor/fx_passes/post_grad.py",
            "patches": [
                {
                    "old_start": 3,
                    "old_length": 7,
                    "new_start": 3,
                    "new_length": 7,
                    "hunk": "@@ -3,7 +3,7 @@ import itertools\n import logging\n import operator\n from collections import Counter, defaultdict, namedtuple\n-from typing import Any, Dict, List, Optional, Set, Tuple, Union\n+from typing import Any, Dict, List, Optional, Set, Union\n \n from sympy import Expr\n \n"
                },
                {
                    "old_start": 748,
                    "old_length": 7,
                    "new_start": 748,
                    "new_length": 7,
                    "hunk": "@@ -748,7 +748,7 @@ def reinplace_inplaceable_ops(graph):\n                 node, shared_view_nodes, copy_node=None\n             )\n \n-    replace_list: List[Tuple[Any, Any]] = []\n+    replace_dict: Dict[torch.fx.Node, torch.fx.Node] = {}\n     for node in graph.nodes:\n         if (inplaceable_op := inplaceable_ops.get(node.target, None)) is not None:\n             mutated_arg = node.args[inplaceable_op.mutated_arg]\n"
                },
                {
                    "old_start": 775,
                    "old_length": 7,
                    "new_start": 775,
                    "new_length": 7,
                    "hunk": "@@ -775,7 +775,7 @@ def reinplace_inplaceable_ops(graph):\n                         graph.erase_node(copy_node)\n                     for user in node.users:\n                         if user.target == operator.getitem and user.args[1] == arg:\n-                            replace_list.append((user, mutated_arg))\n+                            replace_dict[user] = mutated_arg\n                 else:\n                     tensors_to_clone.append(arg)\n             kwargs = dict(node.kwargs)\n"
                },
                {
                    "old_start": 795,
                    "old_length": 7,
                    "new_start": 795,
                    "new_length": 12,
                    "hunk": "@@ -795,7 +795,12 @@ def reinplace_inplaceable_ops(graph):\n                     graph.erase_node(copy_node)\n \n                 node.target = inplaceable_op.inplace_op\n-    for node, replacement in replace_list:\n+\n+    for node, replacement in replace_dict.items():\n+        while replacement in replace_dict:\n+            replacement = replace_dict[replacement]\n+        replace_dict[node] = replacement\n+\n         node.replace_all_uses_with(replacement)\n         graph.erase_node(node)\n "
                }
            ],
            "whole_deleted": "-from typing import Any, Dict, List, Optional, Set, Tuple, Union\n-    replace_list: List[Tuple[Any, Any]] = []\n-                            replace_list.append((user, mutated_arg))\n-    for node, replacement in replace_list:\n",
            "whole_added": "+from typing import Any, Dict, List, Optional, Set, Union\n+    replace_dict: Dict[torch.fx.Node, torch.fx.Node] = {}\n+                            replace_dict[user] = mutated_arg\n+\n+    for node, replacement in replace_dict.items():\n+        while replacement in replace_dict:\n+            replacement = replace_dict[replacement]\n+        replace_dict[node] = replacement\n+\n",
            "whole_hunk": "@@ -3,7 +3,7 @@ import itertools\n import logging\n import operator\n from collections import Counter, defaultdict, namedtuple\n-from typing import Any, Dict, List, Optional, Set, Tuple, Union\n+from typing import Any, Dict, List, Optional, Set, Union\n \n from sympy import Expr\n \n@@ -748,7 +748,7 @@ def reinplace_inplaceable_ops(graph):\n                 node, shared_view_nodes, copy_node=None\n             )\n \n-    replace_list: List[Tuple[Any, Any]] = []\n+    replace_dict: Dict[torch.fx.Node, torch.fx.Node] = {}\n     for node in graph.nodes:\n         if (inplaceable_op := inplaceable_ops.get(node.target, None)) is not None:\n             mutated_arg = node.args[inplaceable_op.mutated_arg]\n@@ -775,7 +775,7 @@ def reinplace_inplaceable_ops(graph):\n                         graph.erase_node(copy_node)\n                     for user in node.users:\n                         if user.target == operator.getitem and user.args[1] == arg:\n-                            replace_list.append((user, mutated_arg))\n+                            replace_dict[user] = mutated_arg\n                 else:\n                     tensors_to_clone.append(arg)\n             kwargs = dict(node.kwargs)\n@@ -795,7 +795,12 @@ def reinplace_inplaceable_ops(graph):\n                     graph.erase_node(copy_node)\n \n                 node.target = inplaceable_op.inplace_op\n-    for node, replacement in replace_list:\n+\n+    for node, replacement in replace_dict.items():\n+        while replacement in replace_dict:\n+            replacement = replace_dict[replacement]\n+        replace_dict[node] = replacement\n+\n         node.replace_all_uses_with(replacement)\n         graph.erase_node(node)\n "
        }
    ]
},
{
    "Id": 361,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/de4d48df34c0e8bcd03d96bcdfd12e263e9889ff",
    "date": "2023-12-29T21:58:25+00:00",
    "message": "[c10d] Fix timeout dump path write path overlap when there are multiple PGs (#116218)\n\nBasically we observed that if there are multiple PGs and if the timeout happens on one of the subPG, we somehow use the local rank in the dump file. We realize that:\n1. For setting the timeout signal in the store, any watchdog thread from any PG can do that.\n2. For checking and dump, only the watchdog thread of default PG which we will always create and contain all ranks (no file name conflict) is needed here because the store signal and dump debug info are all global.\n3. Since dump is global, we want to avoid the case when ranks from sub-PG pollute logs from global ranks (local rank 0 vs global rank 0). So that we use global ranks here to initialize debug info writer. (Down the road, we are thinking about making it a singleton so that user only register it once for multi-PG case.)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116218\nApproved by: https://github.com/wconstab",
    "label": "YES",
    "changes": [
        {
            "name": "ProcessGroupNCCL.cpp",
            "path": "torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp",
            "patches": [
                {
                    "old_start": 799,
                    "old_length": 6,
                    "new_start": 799,
                    "new_length": 7,
                    "hunk": "@@ -799,6 +799,7 @@ ProcessGroupNCCL::ProcessGroupNCCL(\n   std::string nccl_debug = getCvarString({\"NCCL_DEBUG\"}, OFF.c_str());\n   LOG(INFO) << logPrefix() << \"ProcessGroupNCCL initialization options: \"\n             << \"NCCL version: \" << getNcclVersion() << \", size: \" << size\n+            << \", global rank: \" << globalRank()\n             << \", TORCH_NCCL_ASYNC_ERROR_HANDLING: \" << asyncErrorHandling_\n             << \", TORCH_NCCL_DUMP_ON_TIMEOUT: \" << dumpOnTimeout_\n             << \", TORCH_NCCL_DESYNC_DEBUG: \" << desyncDebug_\n"
                },
                {
                    "old_start": 1211,
                    "old_length": 7,
                    "new_start": 1212,
                    "new_length": 7,
                    "hunk": "@@ -1211,7 +1212,7 @@ bool ProcessGroupNCCL::dumpDebuggingInfo() {\n     if (debugInfoWriter_ == nullptr) {\n       // Dump the trace blob into local disk as a fallback.\n       std::unique_ptr<DebugInfoWriter> debugInfoWriterPtr =\n-          std::make_unique<DebugInfoWriter>(rank_);\n+          std::make_unique<DebugInfoWriter>(globalRank());\n       registerDebugInfoWriter(std::move(debugInfoWriterPtr));\n     }\n     debugInfoWriter_->write(ncclTrace);\n"
                },
                {
                    "old_start": 1440,
                    "old_length": 6,
                    "new_start": 1441,
                    "new_length": 11,
                    "hunk": "@@ -1440,6 +1441,11 @@ const std::string& ProcessGroupNCCL::logPrefix() const {\n   return prefix;\n }\n \n+const int& ProcessGroupNCCL::globalRank() const {\n+  static int globalRank = rank_;\n+  return globalRank;\n+}\n+\n void ProcessGroupNCCL::watchdogHandler() {\n   bool done = false;\n   lastWorkListUpdateTime_ = std::chrono::steady_clock::now();\n"
                },
                {
                    "old_start": 1466,
                    "old_length": 10,
                    "new_start": 1472,
                    "new_length": 12,
                    "hunk": "@@ -1466,10 +1472,12 @@ void ProcessGroupNCCL::watchdogHandler() {\n     // Bump up heart beat by one.\n     heartbeat_++;\n \n-    // poll store to see if some ranks have flagged a timeout when\n+    // Assuming that we always init a process group containing all ranks,\n+    // we only use the watchdog thread to listen for the global signal to dump\n+    // and abort. We poll store to see if some ranks have flagged a timeout when\n     // we haven't polled for `heartbeat_timeout` seconds and there haven't\n     // any work added or removed for `watchdog_timeout` seconds.\n-    if (dumpOnTimeout_) {\n+    if (dumpOnTimeout_ && uid_ == 0) {\n       auto currentTime = std::chrono::steady_clock::now();\n       auto timeSinceLastWorkListUpdate =\n           std::chrono::duration_cast<std::chrono::milliseconds>(\n"
                }
            ],
            "whole_deleted": "-          std::make_unique<DebugInfoWriter>(rank_);\n-    // poll store to see if some ranks have flagged a timeout when\n-    if (dumpOnTimeout_) {\n",
            "whole_added": "+            << \", global rank: \" << globalRank()\n+          std::make_unique<DebugInfoWriter>(globalRank());\n+const int& ProcessGroupNCCL::globalRank() const {\n+  static int globalRank = rank_;\n+  return globalRank;\n+}\n+\n+    // Assuming that we always init a process group containing all ranks,\n+    // we only use the watchdog thread to listen for the global signal to dump\n+    // and abort. We poll store to see if some ranks have flagged a timeout when\n+    if (dumpOnTimeout_ && uid_ == 0) {\n",
            "whole_hunk": "@@ -799,6 +799,7 @@ ProcessGroupNCCL::ProcessGroupNCCL(\n   std::string nccl_debug = getCvarString({\"NCCL_DEBUG\"}, OFF.c_str());\n   LOG(INFO) << logPrefix() << \"ProcessGroupNCCL initialization options: \"\n             << \"NCCL version: \" << getNcclVersion() << \", size: \" << size\n+            << \", global rank: \" << globalRank()\n             << \", TORCH_NCCL_ASYNC_ERROR_HANDLING: \" << asyncErrorHandling_\n             << \", TORCH_NCCL_DUMP_ON_TIMEOUT: \" << dumpOnTimeout_\n             << \", TORCH_NCCL_DESYNC_DEBUG: \" << desyncDebug_\n@@ -1211,7 +1212,7 @@ bool ProcessGroupNCCL::dumpDebuggingInfo() {\n     if (debugInfoWriter_ == nullptr) {\n       // Dump the trace blob into local disk as a fallback.\n       std::unique_ptr<DebugInfoWriter> debugInfoWriterPtr =\n-          std::make_unique<DebugInfoWriter>(rank_);\n+          std::make_unique<DebugInfoWriter>(globalRank());\n       registerDebugInfoWriter(std::move(debugInfoWriterPtr));\n     }\n     debugInfoWriter_->write(ncclTrace);\n@@ -1440,6 +1441,11 @@ const std::string& ProcessGroupNCCL::logPrefix() const {\n   return prefix;\n }\n \n+const int& ProcessGroupNCCL::globalRank() const {\n+  static int globalRank = rank_;\n+  return globalRank;\n+}\n+\n void ProcessGroupNCCL::watchdogHandler() {\n   bool done = false;\n   lastWorkListUpdateTime_ = std::chrono::steady_clock::now();\n@@ -1466,10 +1472,12 @@ void ProcessGroupNCCL::watchdogHandler() {\n     // Bump up heart beat by one.\n     heartbeat_++;\n \n-    // poll store to see if some ranks have flagged a timeout when\n+    // Assuming that we always init a process group containing all ranks,\n+    // we only use the watchdog thread to listen for the global signal to dump\n+    // and abort. We poll store to see if some ranks have flagged a timeout when\n     // we haven't polled for `heartbeat_timeout` seconds and there haven't\n     // any work added or removed for `watchdog_timeout` seconds.\n-    if (dumpOnTimeout_) {\n+    if (dumpOnTimeout_ && uid_ == 0) {\n       auto currentTime = std::chrono::steady_clock::now();\n       auto timeSinceLastWorkListUpdate =\n           std::chrono::duration_cast<std::chrono::milliseconds>(\n"
        },
        {
            "name": "ProcessGroupNCCL.hpp",
            "path": "torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp",
            "patches": [
                {
                    "old_start": 719,
                    "old_length": 6,
                    "new_start": 719,
                    "new_length": 12,
                    "hunk": "@@ -719,6 +719,12 @@ class TORCH_API ProcessGroupNCCL : public Backend {\n   // disambiguating logs\n   const std::string& logPrefix() const;\n \n+  // Returns the global rank of the device. This function assumes that users\n+  // always create a default global process group(PG) which includes all\n+  // devices. It is called in the constructor of ProcessGroupNCCL, so it always\n+  // return the rank_ of the the very first PG created, aka, default global PG.\n+  const int& globalRank() const;\n+\n  protected:\n   // Function that runs as part of a separate thread aside from watchdog\n   // thread because we need to check the heartbeat from watchdog thread"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  // Returns the global rank of the device. This function assumes that users\n+  // always create a default global process group(PG) which includes all\n+  // devices. It is called in the constructor of ProcessGroupNCCL, so it always\n+  // return the rank_ of the the very first PG created, aka, default global PG.\n+  const int& globalRank() const;\n+\n",
            "whole_hunk": "@@ -719,6 +719,12 @@ class TORCH_API ProcessGroupNCCL : public Backend {\n   // disambiguating logs\n   const std::string& logPrefix() const;\n \n+  // Returns the global rank of the device. This function assumes that users\n+  // always create a default global process group(PG) which includes all\n+  // devices. It is called in the constructor of ProcessGroupNCCL, so it always\n+  // return the rank_ of the the very first PG created, aka, default global PG.\n+  const int& globalRank() const;\n+\n  protected:\n   // Function that runs as part of a separate thread aside from watchdog\n   // thread because we need to check the heartbeat from watchdog thread"
        }
    ]
},
{
    "Id": 310,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/8b00e5aa12cfbc29909a8697180621e7cd5d92d3",
    "date": "2024-02-02T19:10:11+00:00",
    "message": "[FSDP2] Added pre/post-backward (#118004)\n\nThis PR adds the pre- and post-backward logic:\n- **Pre-backward hook:** `FSDPState` and `FSDPParamGroup` define this, and `FSDPState` is responsible for registering since its pre-backward should run even if the `FSDPState` does not manage any parameters (in case it is the root).\n- **Post-backward hook:** Only `FSDParamGroup` defines this since the post-backward hook reshards parameters and reduce-scatters gradients (functionality only needed with managed parameters). The `FSDPParamGroup` is responsible for registering this.\n- **Post-backward final callback:** `FSDPState` defines this, and each `FSDPParamGroup` defines a `finalize_backward()` to call in the final callback.\n\n### Pre-Backward\n\nThe pre-backward hook is registered on the module outputs (that require gradient), and it should run when the first such output has its gradient computed. The hook may run multiple times per backward, once per module forward. Specifically, there will be one `(pre-backward, post-backward)` interval for each of the module's `forward()` calls. This is contrast with the existing FSDP semantics, which only defines a single `(pre-backward, post-backward)` interval that is equivalent to the union of this FSDP's `(pre-backward, post-backward)` intervals. This avoids spiking memory from having multiple modules not resharding and avoids some autograd edge cases.\n\nWe implement the pre-backward hook by having a flag that is set upon the 1st calls to disable subsequent calls. This flag could be maintained by FSDP, but for a cleaner design, we augment `register_multi_grad_hook` with a `mode=\"any\"` option and use that instead.\n\n### Post-Backward\n\nThe post-backward hook is equivalent to a module full backward hook (`nn.Module.register_full_backward_hook`) except it adds pytree logic to work with data structures other than just flat `Tensor` args passed to `nn.Module.forward`. If we were to use `register_full_backward_hook`, then the hook could fire early (before all gradients for the module have been computed). Most internal models use custom data structures as `forward` inputs, and they find that unifying under pytree is an acceptable solution.\n\nUnlike existing FSDP, we are able to reshard the parameters in the post-backward hook _before_ 'concatenating' the autograd-computed gradients, achieving a lower peak memory usage. (Existing FSDP has `SplitWithSizesBackward` that calls a `CatArrayBatched`, and here we have the reduce-scatter copy-in.)\n\n### Final Callback\nThe final callback runs as a queued callback to the autograd engine, meaning that it runs at the end of backward.\n\nIn the future, if we do not want to wait for the reduce-scatter (or similar for CPU offloading), we can augment the final callback. The code is written such that each reduce-scatter can be waited on separately (via CUDA event).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/118004\nApproved by: https://github.com/weifengpy, https://github.com/wanchaol\nghstack dependencies: #117950, #117955, #117973, #117975",
    "label": "NO",
    "changes": [
        {
            "name": "test_fully_shard_collectives.py",
            "path": "test/distributed/_composable/fsdp/test_fully_shard_collectives.py",
            "patches": [
                {
                    "old_start": 166,
                    "old_length": 6,
                    "new_start": 166,
                    "new_length": 7,
                    "hunk": "@@ -166,6 +166,7 @@ class TestFullyShardCollectives(FSDPTestMultiThread):\n         orig_params = self._init_params(param_sizes)\n         fsdp_param_group = self._init_fsdp_param_group(orig_params)\n         fsdp_params = fsdp_param_group.fsdp_params\n+        fsdp_param_group.comm_ctx.init()\n \n         # - Run one unshard to initialize metadata\n         fsdp_param_group.unshard()\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        fsdp_param_group.comm_ctx.init()\n",
            "whole_hunk": "@@ -166,6 +166,7 @@ class TestFullyShardCollectives(FSDPTestMultiThread):\n         orig_params = self._init_params(param_sizes)\n         fsdp_param_group = self._init_fsdp_param_group(orig_params)\n         fsdp_params = fsdp_param_group.fsdp_params\n+        fsdp_param_group.comm_ctx.init()\n \n         # - Run one unshard to initialize metadata\n         fsdp_param_group.unshard()\n"
        },
        {
            "name": "test_fully_shard_init.py",
            "path": "test/distributed/_composable/fsdp/test_fully_shard_init.py",
            "patches": [
                {
                    "old_start": 372,
                    "old_length": 7,
                    "new_start": 372,
                    "new_length": 7,
                    "hunk": "@@ -372,7 +372,7 @@ class TestFullyShardLazyInit(FSDPTestMultiThread):\n         self.assertFalse(model0_in_proj_state._is_root)\n         self.assertFalse(model0_out_proj_state._is_root)\n \n-        all_states = root_state._all_states\n+        all_states = root_state._state_ctx.all_states\n         self.assertEqual(len(all_states), 3)\n         self.assertEqual(\n             all_states, [root_state, model0_in_proj_state, model0_out_proj_state]\n"
                },
                {
                    "old_start": 436,
                    "old_length": 6,
                    "new_start": 436,
                    "new_length": 22,
                    "hunk": "@@ -436,6 +436,22 @@ class TestFullyShardLazyInit(FSDPTestMultiThread):\n             model0_out_proj_param_fqns, {\"0.out_proj.weight\", \"0.out_proj.bias\"}\n         )\n \n+    @unittest.skipIf(not TEST_CUDA, \"no cuda\")\n+    def test_fully_shard_double_lazy_init(self):\n+        model = nn.Sequential(MLP(8), MLP(8))\n+        fully_shard(model[0].in_proj)\n+        fully_shard(model[0].out_proj)\n+        fully_shard(model)\n+        root_state = fully_shard.state(model)\n+        model0_in_proj_state = fully_shard.state(model[0].in_proj)\n+        model0_in_proj_state._lazy_init()\n+        regex = (\n+            \"FSDP state has already been lazily initialized for 0.in_proj\\n\"\n+            \"FSDP requires running forward through the root module first\"\n+        )\n+        with self.assertRaisesRegex(RuntimeError, regex):\n+            root_state._lazy_init()\n+\n \n if __name__ == \"__main__\":\n     run_tests()\n"
                }
            ],
            "whole_deleted": "-        all_states = root_state._all_states\n",
            "whole_added": "+        all_states = root_state._state_ctx.all_states\n+    @unittest.skipIf(not TEST_CUDA, \"no cuda\")\n+    def test_fully_shard_double_lazy_init(self):\n+        model = nn.Sequential(MLP(8), MLP(8))\n+        fully_shard(model[0].in_proj)\n+        fully_shard(model[0].out_proj)\n+        fully_shard(model)\n+        root_state = fully_shard.state(model)\n+        model0_in_proj_state = fully_shard.state(model[0].in_proj)\n+        model0_in_proj_state._lazy_init()\n+        regex = (\n+            \"FSDP state has already been lazily initialized for 0.in_proj\\n\"\n+            \"FSDP requires running forward through the root module first\"\n+        )\n+        with self.assertRaisesRegex(RuntimeError, regex):\n+            root_state._lazy_init()\n+\n",
            "whole_hunk": "@@ -372,7 +372,7 @@ class TestFullyShardLazyInit(FSDPTestMultiThread):\n         self.assertFalse(model0_in_proj_state._is_root)\n         self.assertFalse(model0_out_proj_state._is_root)\n \n-        all_states = root_state._all_states\n+        all_states = root_state._state_ctx.all_states\n         self.assertEqual(len(all_states), 3)\n         self.assertEqual(\n             all_states, [root_state, model0_in_proj_state, model0_out_proj_state]\n@@ -436,6 +436,22 @@ class TestFullyShardLazyInit(FSDPTestMultiThread):\n             model0_out_proj_param_fqns, {\"0.out_proj.weight\", \"0.out_proj.bias\"}\n         )\n \n+    @unittest.skipIf(not TEST_CUDA, \"no cuda\")\n+    def test_fully_shard_double_lazy_init(self):\n+        model = nn.Sequential(MLP(8), MLP(8))\n+        fully_shard(model[0].in_proj)\n+        fully_shard(model[0].out_proj)\n+        fully_shard(model)\n+        root_state = fully_shard.state(model)\n+        model0_in_proj_state = fully_shard.state(model[0].in_proj)\n+        model0_in_proj_state._lazy_init()\n+        regex = (\n+            \"FSDP state has already been lazily initialized for 0.in_proj\\n\"\n+            \"FSDP requires running forward through the root module first\"\n+        )\n+        with self.assertRaisesRegex(RuntimeError, regex):\n+            root_state._lazy_init()\n+\n \n if __name__ == \"__main__\":\n     run_tests()\n"
        },
        {
            "name": "test_fully_shard_training.py",
            "path": "test/distributed/_composable/fsdp/test_fully_shard_training.py",
            "patches": [
                {
                    "old_start": 5,
                    "old_length": 14,
                    "new_start": 5,
                    "new_length": 22,
                    "hunk": "@@ -5,14 +5,22 @@ import unittest\n from typing import List, Tuple\n \n import torch\n+import torch.distributed as dist\n import torch.nn as nn\n from torch.distributed._composable import replicate\n from torch.distributed._composable.fsdp import fully_shard\n from torch.distributed._tensor import DTensor\n+from torch.distributed.device_mesh import init_device_mesh\n from torch.testing._internal.common_cuda import TEST_CUDA\n from torch.testing._internal.common_distributed import skip_if_lt_x_gpu\n-from torch.testing._internal.common_fsdp import FSDPTest, FSDPTestMultiThread, MLP\n-from torch.testing._internal.common_utils import run_tests\n+from torch.testing._internal.common_fsdp import (\n+    FSDPTest,\n+    FSDPTestMultiThread,\n+    MLP,\n+    patch_all_gather,\n+    patch_reduce_scatter,\n+)\n+from torch.testing._internal.common_utils import get_cycles_per_ms, run_tests\n \n \n class TestFullyShardForwardInputs(FSDPTestMultiThread):\n"
                },
                {
                    "old_start": 74,
                    "old_length": 6,
                    "new_start": 82,
                    "new_length": 27,
                    "hunk": "@@ -74,6 +82,27 @@ class TestFullyShardRegisteredParams(FSDPTestMultiThread):\n         model(inp)\n         self._assert_dtensor_params(model)\n \n+    @unittest.skipIf(not TEST_CUDA, \"no cuda\")\n+    def test_sharded_params_registered_after_backward(self):\n+        \"\"\"Tests that the sharded parameters are registered after forward.\"\"\"\n+        device = torch.device(\"cuda\", 0)\n+        # Single FSDP group\n+        model = MLP(8, device)\n+        fully_shard(model)\n+        inp = torch.randn((2, 8), device=\"cuda\")\n+        self._assert_dtensor_params(model)\n+        model(inp).sum().backward()\n+        self._assert_dtensor_params(model)\n+\n+        # Multiple FSDP groups\n+        model = MLP(8, device)\n+        fully_shard(model.in_proj)\n+        fully_shard(model.out_proj)\n+        fully_shard(model)\n+        self._assert_dtensor_params(model)\n+        model(inp).sum().backward()\n+        self._assert_dtensor_params(model)\n+\n     def _assert_dtensor_params(self, module: nn.Module):\n         for param in module.parameters():\n             self.assertIsInstance(param, DTensor)\n"
                },
                {
                    "old_start": 97,
                    "old_length": 9,
                    "new_start": 126,
                    "new_length": 7,
                    "hunk": "@@ -97,9 +126,7 @@ class TestFullyShard1DTrainingCore(FSDPTest):\n     def _test_train_parity_single_group(self, lin_shapes: List[Tuple[int, int]]):\n         torch.manual_seed(42)\n         model = nn.Sequential(\n-            nn.Linear(*lin_shapes[0], bias=True),\n-            nn.ReLU(),\n-            nn.Linear(*lin_shapes[1], bias=True),\n+            nn.Linear(*lin_shapes[0]), nn.ReLU(), nn.Linear(*lin_shapes[1])\n         )\n         ref_model = copy.deepcopy(model).cuda()\n         replicate(ref_model, device_ids=[self.rank])\n"
                },
                {
                    "old_start": 111,
                    "old_length": 10,
                    "new_start": 138,
                    "new_length": 142,
                    "hunk": "@@ -111,10 +138,142 @@ class TestFullyShard1DTrainingCore(FSDPTest):\n         for iter_idx in range(10):\n             losses: List[torch.Tensor] = []\n             for _model, _optim in ((ref_model, ref_optim), (model, optim)):\n-                # TODO: Test forward only for now\n+                _optim.zero_grad(set_to_none=(iter_idx % 2 == 0))\n                 losses.append(_model(*inp).sum())\n+                losses[-1].backward()\n+                _optim.step()\n             self.assertEqual(losses[0], losses[1])\n \n+    @skip_if_lt_x_gpu(2)\n+    def test_train_parity_multi_group(self):\n+        \"\"\"\n+        Tests train parity against DDP when using multiple parameter groups for\n+        communication (for communication and computation overlap plus memory\n+        reduction).\n+        \"\"\"\n+        self.run_subtests(\n+            {\n+                \"device_type\": [\"cuda\"],\n+                \"delay_after_forward\": [False, True],\n+                \"delay_before_all_gather\": [False, True],\n+                \"delay_before_reduce_scatter\": [False, True],\n+                \"delay_before_optim\": [False, True],\n+            },\n+            self._test_train_parity_multi_group,\n+        )\n+\n+    def _test_train_parity_multi_group(\n+        self,\n+        device_type: str,\n+        delay_after_forward: bool,\n+        delay_before_all_gather: bool,\n+        delay_before_reduce_scatter: bool,\n+        delay_before_optim: bool,\n+    ):\n+        # Only test individual delays or all four delays to save test time\n+        if (\n+            delay_after_forward\n+            + delay_before_all_gather\n+            + delay_before_reduce_scatter\n+            + delay_before_optim\n+            in (2, 3)\n+        ):\n+            return\n+        assert device_type in (\"cuda\", \"cpu\"), f\"{device_type}\"\n+        torch.manual_seed(42)\n+        lin_dim = 32\n+        model = nn.Sequential(*[MLP(lin_dim, torch.device(\"cpu\")) for _ in range(3)])\n+        ref_model = copy.deepcopy(model)\n+        if device_type == \"cuda\":\n+            replicate(ref_model.cuda(), device_ids=[self.rank])\n+        else:\n+            gloo_pg = dist.new_group(backend=\"gloo\")\n+            replicate(ref_model, process_group=gloo_pg)\n+        ref_optim = torch.optim.Adam(ref_model.parameters(), lr=1e-2)\n+        mesh = init_device_mesh(device_type, (self.world_size,))\n+        for mlp in model:\n+            fully_shard(mlp, mesh=mesh)\n+        fully_shard(model, mesh=mesh)\n+        optim = torch.optim.Adam(model.parameters(), lr=1e-2)\n+\n+        delay_in_ms = 100\n+        orig_all_gather = dist.all_gather_into_tensor\n+        orig_reduce_scatter = dist.reduce_scatter_tensor\n+\n+        def delayed_all_gather(*args, **kwargs):\n+            if delay_before_all_gather:\n+                torch.cuda._sleep(int(delay_in_ms * get_cycles_per_ms()))\n+            return orig_all_gather(*args, **kwargs)\n+\n+        def delayed_reduce_scatter(*args, **kwargs):\n+            if delay_before_reduce_scatter:\n+                torch.cuda._sleep(int(delay_in_ms * get_cycles_per_ms()))\n+            return orig_reduce_scatter(*args, **kwargs)\n+\n+        torch.manual_seed(42 + self.rank + 1)\n+        with patch_all_gather(delayed_all_gather), patch_reduce_scatter(\n+            delayed_reduce_scatter\n+        ):\n+            for iter_idx in range(10):\n+                inp = torch.randn((8, lin_dim), device=torch.device(device_type))\n+                losses: List[torch.Tensor] = []\n+                for _model, _optim in ((ref_model, ref_optim), (model, optim)):\n+                    _optim.zero_grad(set_to_none=(iter_idx % 2 == 0))\n+                    losses.append(_model(inp).sum())\n+                    if _model is model and delay_after_forward:\n+                        torch.cuda._sleep(int(delay_in_ms * get_cycles_per_ms()))\n+                    losses[-1].backward()\n+                    if _model is model and delay_before_optim:\n+                        torch.cuda._sleep(int(delay_in_ms * get_cycles_per_ms()))\n+                    _optim.step()\n+                self.assertEqual(losses[0], losses[1])\n+\n+    @skip_if_lt_x_gpu(2)\n+    def test_non_root_forward_backward(self):\n+        \"\"\"\n+        Tests running forward/backward through the root and then through a\n+        non-root. The non-root needs to synchronize streams/queue the callback.\n+        \"\"\"\n+        torch.manual_seed(42)\n+        lin_dim = 32\n+        model = nn.Sequential(*[MLP(lin_dim, torch.device(\"cpu\")) for _ in range(3)])\n+        ref_model = copy.deepcopy(model).cuda()\n+        ref_optim = torch.optim.Adam(ref_model.parameters(), lr=1e-2)\n+        for mlp in model:\n+            fully_shard(mlp)\n+        fully_shard(model)\n+        optim = torch.optim.Adam(model.parameters(), lr=1e-2, foreach=True)\n+        torch.manual_seed(42 + self.rank)\n+        inp = torch.randn((8, lin_dim), device=torch.device(\"cuda\"))\n+\n+        ref_root_loss = ref_model(inp).sum()\n+        ref_root_loss.backward()\n+        for param in ref_model.parameters():\n+            dist.all_reduce(param.grad)\n+            param.grad.detach().div_(self.world_size)\n+        ref_optim.step()\n+        ref_optim.zero_grad()\n+        ref_nonroot_loss = ref_model[0](inp).sum()\n+        ref_nonroot_loss.backward()\n+        for param in ref_model.parameters():\n+            if param.grad is not None:\n+                dist.all_reduce(param.grad)\n+                param.grad.detach().div_(self.world_size)\n+        ref_optim.step()\n+\n+        root_loss = model(inp).sum()\n+        root_loss.backward()\n+        torch.cuda._sleep(int(100 * get_cycles_per_ms()))\n+        optim.step()\n+        optim.zero_grad()\n+        nonroot_loss = model[0](inp).sum()\n+        nonroot_loss.backward()\n+        optim.step()\n+\n+        self.assertEqual(ref_root_loss, root_loss)\n+        self.assertEqual(ref_nonroot_loss, nonroot_loss)\n+        self.assertEqual(ref_model(inp).sum(), model(inp).sum())\n+\n \n if __name__ == \"__main__\":\n     run_tests()\n"
                }
            ],
            "whole_deleted": "-from torch.testing._internal.common_fsdp import FSDPTest, FSDPTestMultiThread, MLP\n-from torch.testing._internal.common_utils import run_tests\n-            nn.Linear(*lin_shapes[0], bias=True),\n-            nn.ReLU(),\n-            nn.Linear(*lin_shapes[1], bias=True),\n-                # TODO: Test forward only for now\n",
            "whole_added": "+import torch.distributed as dist\n+from torch.distributed.device_mesh import init_device_mesh\n+from torch.testing._internal.common_fsdp import (\n+    FSDPTest,\n+    FSDPTestMultiThread,\n+    MLP,\n+    patch_all_gather,\n+    patch_reduce_scatter,\n+)\n+from torch.testing._internal.common_utils import get_cycles_per_ms, run_tests\n+    @unittest.skipIf(not TEST_CUDA, \"no cuda\")\n+    def test_sharded_params_registered_after_backward(self):\n+        \"\"\"Tests that the sharded parameters are registered after forward.\"\"\"\n+        device = torch.device(\"cuda\", 0)\n+        # Single FSDP group\n+        model = MLP(8, device)\n+        fully_shard(model)\n+        inp = torch.randn((2, 8), device=\"cuda\")\n+        self._assert_dtensor_params(model)\n+        model(inp).sum().backward()\n+        self._assert_dtensor_params(model)\n+\n+        # Multiple FSDP groups\n+        model = MLP(8, device)\n+        fully_shard(model.in_proj)\n+        fully_shard(model.out_proj)\n+        fully_shard(model)\n+        self._assert_dtensor_params(model)\n+        model(inp).sum().backward()\n+        self._assert_dtensor_params(model)\n+\n+            nn.Linear(*lin_shapes[0]), nn.ReLU(), nn.Linear(*lin_shapes[1])\n+                _optim.zero_grad(set_to_none=(iter_idx % 2 == 0))\n+                losses[-1].backward()\n+                _optim.step()\n+    @skip_if_lt_x_gpu(2)\n+    def test_train_parity_multi_group(self):\n+        \"\"\"\n+        Tests train parity against DDP when using multiple parameter groups for\n+        communication (for communication and computation overlap plus memory\n+        reduction).\n+        \"\"\"\n+        self.run_subtests(\n+            {\n+                \"device_type\": [\"cuda\"],\n+                \"delay_after_forward\": [False, True],\n+                \"delay_before_all_gather\": [False, True],\n+                \"delay_before_reduce_scatter\": [False, True],\n+                \"delay_before_optim\": [False, True],\n+            },\n+            self._test_train_parity_multi_group,\n+        )\n+\n+    def _test_train_parity_multi_group(\n+        self,\n+        device_type: str,\n+        delay_after_forward: bool,\n+        delay_before_all_gather: bool,\n+        delay_before_reduce_scatter: bool,\n+        delay_before_optim: bool,\n+    ):\n+        # Only test individual delays or all four delays to save test time\n+        if (\n+            delay_after_forward\n+            + delay_before_all_gather\n+            + delay_before_reduce_scatter\n+            + delay_before_optim\n+            in (2, 3)\n+        ):\n+            return\n+        assert device_type in (\"cuda\", \"cpu\"), f\"{device_type}\"\n+        torch.manual_seed(42)\n+        lin_dim = 32\n+        model = nn.Sequential(*[MLP(lin_dim, torch.device(\"cpu\")) for _ in range(3)])\n+        ref_model = copy.deepcopy(model)\n+        if device_type == \"cuda\":\n+            replicate(ref_model.cuda(), device_ids=[self.rank])\n+        else:\n+            gloo_pg = dist.new_group(backend=\"gloo\")\n+            replicate(ref_model, process_group=gloo_pg)\n+        ref_optim = torch.optim.Adam(ref_model.parameters(), lr=1e-2)\n+        mesh = init_device_mesh(device_type, (self.world_size,))\n+        for mlp in model:\n+            fully_shard(mlp, mesh=mesh)\n+        fully_shard(model, mesh=mesh)\n+        optim = torch.optim.Adam(model.parameters(), lr=1e-2)\n+\n+        delay_in_ms = 100\n+        orig_all_gather = dist.all_gather_into_tensor\n+        orig_reduce_scatter = dist.reduce_scatter_tensor\n+\n+        def delayed_all_gather(*args, **kwargs):\n+            if delay_before_all_gather:\n+                torch.cuda._sleep(int(delay_in_ms * get_cycles_per_ms()))\n+            return orig_all_gather(*args, **kwargs)\n+\n+        def delayed_reduce_scatter(*args, **kwargs):\n+            if delay_before_reduce_scatter:\n+                torch.cuda._sleep(int(delay_in_ms * get_cycles_per_ms()))\n+            return orig_reduce_scatter(*args, **kwargs)\n+\n+        torch.manual_seed(42 + self.rank + 1)\n+        with patch_all_gather(delayed_all_gather), patch_reduce_scatter(\n+            delayed_reduce_scatter\n+        ):\n+            for iter_idx in range(10):\n+                inp = torch.randn((8, lin_dim), device=torch.device(device_type))\n+                losses: List[torch.Tensor] = []\n+                for _model, _optim in ((ref_model, ref_optim), (model, optim)):\n+                    _optim.zero_grad(set_to_none=(iter_idx % 2 == 0))\n+                    losses.append(_model(inp).sum())\n+                    if _model is model and delay_after_forward:\n+                        torch.cuda._sleep(int(delay_in_ms * get_cycles_per_ms()))\n+                    losses[-1].backward()\n+                    if _model is model and delay_before_optim:\n+                        torch.cuda._sleep(int(delay_in_ms * get_cycles_per_ms()))\n+                    _optim.step()\n+                self.assertEqual(losses[0], losses[1])\n+\n+    @skip_if_lt_x_gpu(2)\n+    def test_non_root_forward_backward(self):\n+        \"\"\"\n+        Tests running forward/backward through the root and then through a\n+        non-root. The non-root needs to synchronize streams/queue the callback.\n+        \"\"\"\n+        torch.manual_seed(42)\n+        lin_dim = 32\n+        model = nn.Sequential(*[MLP(lin_dim, torch.device(\"cpu\")) for _ in range(3)])\n+        ref_model = copy.deepcopy(model).cuda()\n+        ref_optim = torch.optim.Adam(ref_model.parameters(), lr=1e-2)\n+        for mlp in model:\n+            fully_shard(mlp)\n+        fully_shard(model)\n+        optim = torch.optim.Adam(model.parameters(), lr=1e-2, foreach=True)\n+        torch.manual_seed(42 + self.rank)\n+        inp = torch.randn((8, lin_dim), device=torch.device(\"cuda\"))\n+\n+        ref_root_loss = ref_model(inp).sum()\n+        ref_root_loss.backward()\n+        for param in ref_model.parameters():\n+            dist.all_reduce(param.grad)\n+            param.grad.detach().div_(self.world_size)\n+        ref_optim.step()\n+        ref_optim.zero_grad()\n+        ref_nonroot_loss = ref_model[0](inp).sum()\n+        ref_nonroot_loss.backward()\n+        for param in ref_model.parameters():\n+            if param.grad is not None:\n+                dist.all_reduce(param.grad)\n+                param.grad.detach().div_(self.world_size)\n+        ref_optim.step()\n+\n+        root_loss = model(inp).sum()\n+        root_loss.backward()\n+        torch.cuda._sleep(int(100 * get_cycles_per_ms()))\n+        optim.step()\n+        optim.zero_grad()\n+        nonroot_loss = model[0](inp).sum()\n+        nonroot_loss.backward()\n+        optim.step()\n+\n+        self.assertEqual(ref_root_loss, root_loss)\n+        self.assertEqual(ref_nonroot_loss, nonroot_loss)\n+        self.assertEqual(ref_model(inp).sum(), model(inp).sum())\n+\n",
            "whole_hunk": "@@ -5,14 +5,22 @@ import unittest\n from typing import List, Tuple\n \n import torch\n+import torch.distributed as dist\n import torch.nn as nn\n from torch.distributed._composable import replicate\n from torch.distributed._composable.fsdp import fully_shard\n from torch.distributed._tensor import DTensor\n+from torch.distributed.device_mesh import init_device_mesh\n from torch.testing._internal.common_cuda import TEST_CUDA\n from torch.testing._internal.common_distributed import skip_if_lt_x_gpu\n-from torch.testing._internal.common_fsdp import FSDPTest, FSDPTestMultiThread, MLP\n-from torch.testing._internal.common_utils import run_tests\n+from torch.testing._internal.common_fsdp import (\n+    FSDPTest,\n+    FSDPTestMultiThread,\n+    MLP,\n+    patch_all_gather,\n+    patch_reduce_scatter,\n+)\n+from torch.testing._internal.common_utils import get_cycles_per_ms, run_tests\n \n \n class TestFullyShardForwardInputs(FSDPTestMultiThread):\n@@ -74,6 +82,27 @@ class TestFullyShardRegisteredParams(FSDPTestMultiThread):\n         model(inp)\n         self._assert_dtensor_params(model)\n \n+    @unittest.skipIf(not TEST_CUDA, \"no cuda\")\n+    def test_sharded_params_registered_after_backward(self):\n+        \"\"\"Tests that the sharded parameters are registered after forward.\"\"\"\n+        device = torch.device(\"cuda\", 0)\n+        # Single FSDP group\n+        model = MLP(8, device)\n+        fully_shard(model)\n+        inp = torch.randn((2, 8), device=\"cuda\")\n+        self._assert_dtensor_params(model)\n+        model(inp).sum().backward()\n+        self._assert_dtensor_params(model)\n+\n+        # Multiple FSDP groups\n+        model = MLP(8, device)\n+        fully_shard(model.in_proj)\n+        fully_shard(model.out_proj)\n+        fully_shard(model)\n+        self._assert_dtensor_params(model)\n+        model(inp).sum().backward()\n+        self._assert_dtensor_params(model)\n+\n     def _assert_dtensor_params(self, module: nn.Module):\n         for param in module.parameters():\n             self.assertIsInstance(param, DTensor)\n@@ -97,9 +126,7 @@ class TestFullyShard1DTrainingCore(FSDPTest):\n     def _test_train_parity_single_group(self, lin_shapes: List[Tuple[int, int]]):\n         torch.manual_seed(42)\n         model = nn.Sequential(\n-            nn.Linear(*lin_shapes[0], bias=True),\n-            nn.ReLU(),\n-            nn.Linear(*lin_shapes[1], bias=True),\n+            nn.Linear(*lin_shapes[0]), nn.ReLU(), nn.Linear(*lin_shapes[1])\n         )\n         ref_model = copy.deepcopy(model).cuda()\n         replicate(ref_model, device_ids=[self.rank])\n@@ -111,10 +138,142 @@ class TestFullyShard1DTrainingCore(FSDPTest):\n         for iter_idx in range(10):\n             losses: List[torch.Tensor] = []\n             for _model, _optim in ((ref_model, ref_optim), (model, optim)):\n-                # TODO: Test forward only for now\n+                _optim.zero_grad(set_to_none=(iter_idx % 2 == 0))\n                 losses.append(_model(*inp).sum())\n+                losses[-1].backward()\n+                _optim.step()\n             self.assertEqual(losses[0], losses[1])\n \n+    @skip_if_lt_x_gpu(2)\n+    def test_train_parity_multi_group(self):\n+        \"\"\"\n+        Tests train parity against DDP when using multiple parameter groups for\n+        communication (for communication and computation overlap plus memory\n+        reduction).\n+        \"\"\"\n+        self.run_subtests(\n+            {\n+                \"device_type\": [\"cuda\"],\n+                \"delay_after_forward\": [False, True],\n+                \"delay_before_all_gather\": [False, True],\n+                \"delay_before_reduce_scatter\": [False, True],\n+                \"delay_before_optim\": [False, True],\n+            },\n+            self._test_train_parity_multi_group,\n+        )\n+\n+    def _test_train_parity_multi_group(\n+        self,\n+        device_type: str,\n+        delay_after_forward: bool,\n+        delay_before_all_gather: bool,\n+        delay_before_reduce_scatter: bool,\n+        delay_before_optim: bool,\n+    ):\n+        # Only test individual delays or all four delays to save test time\n+        if (\n+            delay_after_forward\n+            + delay_before_all_gather\n+            + delay_before_reduce_scatter\n+            + delay_before_optim\n+            in (2, 3)\n+        ):\n+            return\n+        assert device_type in (\"cuda\", \"cpu\"), f\"{device_type}\"\n+        torch.manual_seed(42)\n+        lin_dim = 32\n+        model = nn.Sequential(*[MLP(lin_dim, torch.device(\"cpu\")) for _ in range(3)])\n+        ref_model = copy.deepcopy(model)\n+        if device_type == \"cuda\":\n+            replicate(ref_model.cuda(), device_ids=[self.rank])\n+        else:\n+            gloo_pg = dist.new_group(backend=\"gloo\")\n+            replicate(ref_model, process_group=gloo_pg)\n+        ref_optim = torch.optim.Adam(ref_model.parameters(), lr=1e-2)\n+        mesh = init_device_mesh(device_type, (self.world_size,))\n+        for mlp in model:\n+            fully_shard(mlp, mesh=mesh)\n+        fully_shard(model, mesh=mesh)\n+        optim = torch.optim.Adam(model.parameters(), lr=1e-2)\n+\n+        delay_in_ms = 100\n+        orig_all_gather = dist.all_gather_into_tensor\n+        orig_reduce_scatter = dist.reduce_scatter_tensor\n+\n+        def delayed_all_gather(*args, **kwargs):\n+            if delay_before_all_gather:\n+                torch.cuda._sleep(int(delay_in_ms * get_cycles_per_ms()))\n+            return orig_all_gather(*args, **kwargs)\n+\n+        def delayed_reduce_scatter(*args, **kwargs):\n+            if delay_before_reduce_scatter:\n+                torch.cuda._sleep(int(delay_in_ms * get_cycles_per_ms()))\n+            return orig_reduce_scatter(*args, **kwargs)\n+\n+        torch.manual_seed(42 + self.rank + 1)\n+        with patch_all_gather(delayed_all_gather), patch_reduce_scatter(\n+            delayed_reduce_scatter\n+        ):\n+            for iter_idx in range(10):\n+                inp = torch.randn((8, lin_dim), device=torch.device(device_type))\n+                losses: List[torch.Tensor] = []\n+                for _model, _optim in ((ref_model, ref_optim), (model, optim)):\n+                    _optim.zero_grad(set_to_none=(iter_idx % 2 == 0))\n+                    losses.append(_model(inp).sum())\n+                    if _model is model and delay_after_forward:\n+                        torch.cuda._sleep(int(delay_in_ms * get_cycles_per_ms()))\n+                    losses[-1].backward()\n+                    if _model is model and delay_before_optim:\n+                        torch.cuda._sleep(int(delay_in_ms * get_cycles_per_ms()))\n+                    _optim.step()\n+                self.assertEqual(losses[0], losses[1])\n+\n+    @skip_if_lt_x_gpu(2)\n+    def test_non_root_forward_backward(self):\n+        \"\"\"\n+        Tests running forward/backward through the root and then through a\n+        non-root. The non-root needs to synchronize streams/queue the callback.\n+        \"\"\"\n+        torch.manual_seed(42)\n+        lin_dim = 32\n+        model = nn.Sequential(*[MLP(lin_dim, torch.device(\"cpu\")) for _ in range(3)])\n+        ref_model = copy.deepcopy(model).cuda()\n+        ref_optim = torch.optim.Adam(ref_model.parameters(), lr=1e-2)\n+        for mlp in model:\n+            fully_shard(mlp)\n+        fully_shard(model)\n+        optim = torch.optim.Adam(model.parameters(), lr=1e-2, foreach=True)\n+        torch.manual_seed(42 + self.rank)\n+        inp = torch.randn((8, lin_dim), device=torch.device(\"cuda\"))\n+\n+        ref_root_loss = ref_model(inp).sum()\n+        ref_root_loss.backward()\n+        for param in ref_model.parameters():\n+            dist.all_reduce(param.grad)\n+            param.grad.detach().div_(self.world_size)\n+        ref_optim.step()\n+        ref_optim.zero_grad()\n+        ref_nonroot_loss = ref_model[0](inp).sum()\n+        ref_nonroot_loss.backward()\n+        for param in ref_model.parameters():\n+            if param.grad is not None:\n+                dist.all_reduce(param.grad)\n+                param.grad.detach().div_(self.world_size)\n+        ref_optim.step()\n+\n+        root_loss = model(inp).sum()\n+        root_loss.backward()\n+        torch.cuda._sleep(int(100 * get_cycles_per_ms()))\n+        optim.step()\n+        optim.zero_grad()\n+        nonroot_loss = model[0](inp).sum()\n+        nonroot_loss.backward()\n+        optim.step()\n+\n+        self.assertEqual(ref_root_loss, root_loss)\n+        self.assertEqual(ref_nonroot_loss, nonroot_loss)\n+        self.assertEqual(ref_model(inp).sum(), model(inp).sum())\n+\n \n if __name__ == \"__main__\":\n     run_tests()\n"
        },
        {
            "name": "_fsdp_collectives.py",
            "path": "torch/distributed/_composable/fsdp/_fsdp_collectives.py",
            "patches": [
                {
                    "old_start": 19,
                    "old_length": 25,
                    "new_start": 18,
                    "new_length": 6,
                    "hunk": "@@ -19,25 +18,6 @@ class AllGatherResult(NamedTuple):\n     all_gather_input_numels: List[int]\n \n \n-class AllGatherState(NamedTuple):\n-    all_gather_result: AllGatherResult\n-    event: torch.cuda.Event  # copy-out\n-\n-\n-class AllGatherStateHolder:\n-    def __init__(self):\n-        self._state: Optional[AllGatherState] = None\n-\n-    def put(self, state: AllGatherState) -> None:\n-        assert self._state is None, \"Expects to hold only one all-gather state\"\n-        self._state = state\n-\n-    def pop(self) -> Optional[AllGatherState]:\n-        state = self._state\n-        self._state = None\n-        return state\n-\n-\n @torch.no_grad()\n def foreach_all_gather(\n     fsdp_params: List[FSDPParam],\n"
                }
            ],
            "whole_deleted": "-class AllGatherState(NamedTuple):\n-    all_gather_result: AllGatherResult\n-    event: torch.cuda.Event  # copy-out\n-\n-\n-class AllGatherStateHolder:\n-    def __init__(self):\n-        self._state: Optional[AllGatherState] = None\n-\n-    def put(self, state: AllGatherState) -> None:\n-        assert self._state is None, \"Expects to hold only one all-gather state\"\n-        self._state = state\n-\n-    def pop(self) -> Optional[AllGatherState]:\n-        state = self._state\n-        self._state = None\n-        return state\n-\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -19,25 +18,6 @@ class AllGatherResult(NamedTuple):\n     all_gather_input_numels: List[int]\n \n \n-class AllGatherState(NamedTuple):\n-    all_gather_result: AllGatherResult\n-    event: torch.cuda.Event  # copy-out\n-\n-\n-class AllGatherStateHolder:\n-    def __init__(self):\n-        self._state: Optional[AllGatherState] = None\n-\n-    def put(self, state: AllGatherState) -> None:\n-        assert self._state is None, \"Expects to hold only one all-gather state\"\n-        self._state = state\n-\n-    def pop(self) -> Optional[AllGatherState]:\n-        state = self._state\n-        self._state = None\n-        return state\n-\n-\n @torch.no_grad()\n def foreach_all_gather(\n     fsdp_params: List[FSDPParam],\n"
        },
        {
            "name": "_fsdp_param.py",
            "path": "torch/distributed/_composable/fsdp/_fsdp_param.py",
            "patches": [
                {
                    "old_start": 6,
                    "old_length": 6,
                    "new_start": 6,
                    "new_length": 7,
                    "hunk": "@@ -6,6 +6,7 @@ import torch\n import torch.nn as nn\n \n from torch._prims_common import make_contiguous_strides_for\n+from torch.distributed._functional_collectives import AsyncCollectiveTensor\n from torch.distributed._tensor import DTensor, Placement, Replicate, Shard\n from torch.distributed._tensor.device_mesh import _mesh_resources\n from torch.distributed._tensor.placement_types import DTensorSpec\n"
                },
                {
                    "old_start": 286,
                    "old_length": 6,
                    "new_start": 287,
                    "new_length": 24,
                    "hunk": "@@ -286,6 +287,24 @@ class FSDPParam:\n             return self._sharded_param_data\n         return torch.empty(0)  # mypy\n \n+    @property\n+    def unsharded_param(self) -> nn.Parameter:  # ND\n+        self._assert_in_states(ShardedState.UNSHARDED)\n+        return self._unsharded_param\n+\n+    @property\n+    def unsharded_grad_data(self) -> torch.Tensor:\n+        grad = self.unsharded_param.grad\n+        assert grad is not None, \"Expects unsharded_param.grad to not be None\"\n+        return self._get_grad_inner_tensor(grad)\n+\n+    def _get_grad_inner_tensor(self, grad: torch.Tensor) -> torch.Tensor:\n+        if self.is_dtensor:\n+            if isinstance(grad, AsyncCollectiveTensor):\n+                grad = grad.wait()\n+            grad = cast(DTensor, grad)._local_tensor\n+        return grad\n+\n     def _assert_in_states(self, *states: ShardedState) -> None:\n         if self.sharded_state not in states:\n             _raise_assert_with_print(\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+from torch.distributed._functional_collectives import AsyncCollectiveTensor\n+    @property\n+    def unsharded_param(self) -> nn.Parameter:  # ND\n+        self._assert_in_states(ShardedState.UNSHARDED)\n+        return self._unsharded_param\n+\n+    @property\n+    def unsharded_grad_data(self) -> torch.Tensor:\n+        grad = self.unsharded_param.grad\n+        assert grad is not None, \"Expects unsharded_param.grad to not be None\"\n+        return self._get_grad_inner_tensor(grad)\n+\n+    def _get_grad_inner_tensor(self, grad: torch.Tensor) -> torch.Tensor:\n+        if self.is_dtensor:\n+            if isinstance(grad, AsyncCollectiveTensor):\n+                grad = grad.wait()\n+            grad = cast(DTensor, grad)._local_tensor\n+        return grad\n+\n",
            "whole_hunk": "@@ -6,6 +6,7 @@ import torch\n import torch.nn as nn\n \n from torch._prims_common import make_contiguous_strides_for\n+from torch.distributed._functional_collectives import AsyncCollectiveTensor\n from torch.distributed._tensor import DTensor, Placement, Replicate, Shard\n from torch.distributed._tensor.device_mesh import _mesh_resources\n from torch.distributed._tensor.placement_types import DTensorSpec\n@@ -286,6 +287,24 @@ class FSDPParam:\n             return self._sharded_param_data\n         return torch.empty(0)  # mypy\n \n+    @property\n+    def unsharded_param(self) -> nn.Parameter:  # ND\n+        self._assert_in_states(ShardedState.UNSHARDED)\n+        return self._unsharded_param\n+\n+    @property\n+    def unsharded_grad_data(self) -> torch.Tensor:\n+        grad = self.unsharded_param.grad\n+        assert grad is not None, \"Expects unsharded_param.grad to not be None\"\n+        return self._get_grad_inner_tensor(grad)\n+\n+    def _get_grad_inner_tensor(self, grad: torch.Tensor) -> torch.Tensor:\n+        if self.is_dtensor:\n+            if isinstance(grad, AsyncCollectiveTensor):\n+                grad = grad.wait()\n+            grad = cast(DTensor, grad)._local_tensor\n+        return grad\n+\n     def _assert_in_states(self, *states: ShardedState) -> None:\n         if self.sharded_state not in states:\n             _raise_assert_with_print(\n"
        },
        {
            "name": "_fsdp_param_group.py",
            "path": "torch/distributed/_composable/fsdp/_fsdp_param_group.py",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 21,
                    "new_start": 1,
                    "new_length": 69,
                    "hunk": "@@ -1,21 +1,69 @@\n-from typing import Any, Dict, List, Optional, Tuple\n+from typing import Any, Dict, List, NamedTuple, Optional, Tuple\n \n import torch\n import torch.distributed as dist\n import torch.nn as nn\n \n from torch.distributed.fsdp._common_utils import _named_parameters_with_duplicates\n+from torch.utils._pytree import tree_flatten, tree_unflatten\n from ._fsdp_collectives import (\n     AllGatherResult,\n-    AllGatherState,\n-    AllGatherStateHolder,\n     foreach_all_gather,\n     foreach_all_gather_copy_out,\n+    foreach_reduce_scatter,\n )\n from ._fsdp_common import FSDPMeshInfo, HSDPMeshInfo, TrainingState\n from ._fsdp_param import FSDPParam, ParamModuleInfo, ShardedState\n \n \n+\"\"\"\n+[Note: Overlapping all-gather copy-in and all-gather]\n+For implicit forward prefetching, we want to overlap the next copy-in with the\n+current all-gather. We do so using a separate copy-in stream. However, since\n+we have the all-gather input as a view into the output, we must make sure to\n+copy into different memory from the current all-gather's output. Thus, we keep\n+a reference to the current all-gather's output and have the next FSDP parameter\n+group free it after its copy-in. Finally, we have the last FSDP state flush the\n+reference to avoid holding onto memory after forward.\n+\"\"\"\n+\n+\n+class FSDPCommContext:\n+    \"\"\"This has the communication state shared across FSDP states/parameter groups.\"\"\"\n+\n+    def init(self):\n+        # Setting the all-gather/reduce-scatter streams to be higher priority\n+        # can help avoid some issues where their copies in/out are delayed and\n+        # block computation\n+        high_priority = -1\n+        # All-gather state and copy-in stream allow overlapping the next\n+        # copy-in with the current all-gather in forward; copy-in overlaps with\n+        # reduce-scatter in backward without the separate copy-in stream\n+        self.all_gather_copy_in_stream = torch.cuda.Stream(priority=high_priority)\n+        self.all_gather_state: Optional[AllGatherState] = None\n+        # All-gather stream allows overlapping next all-gather with current\n+        # forward compute\n+        self.all_gather_stream = torch.cuda.Stream(priority=high_priority)\n+        # Reduce-scatter stream gives separate execution \"thread\" for post-\n+        # backward logic like pre/post-gradient division and reduce-scatter\n+        self.reduce_scatter_stream = torch.cuda.Stream(priority=high_priority)\n+\n+    def get_all_gather_streams(\n+        self, training_state: TrainingState\n+    ) -> Tuple[torch.cuda.Stream, torch.cuda.Stream]:\n+        if training_state in (TrainingState.FORWARD, TrainingState.PRE_BACKWARD):\n+            # Use separate streams for implicit prefetching\n+            return self.all_gather_copy_in_stream, self.all_gather_stream\n+        current_stream = torch.cuda.current_stream()\n+        return current_stream, current_stream\n+\n+\n+# See [Note: Overlapping all-gather copy-in and all-gather]\n+class AllGatherState(NamedTuple):\n+    all_gather_result: AllGatherResult\n+    event: torch.cuda.Event  # all-gather copy-out\n+\n+\n class FSDPParamGroup:\n     \"\"\"This class represents a parameter group to communicate together.\"\"\"\n \n"
                },
                {
                    "old_start": 41,
                    "old_length": 16,
                    "new_start": 89,
                    "new_length": 16,
                    "hunk": "@@ -41,16 +89,16 @@ class FSDPParamGroup:\n         self._module_fqn: Optional[str] = None  # prefixed from root module\n \n         # - Communication and communication/computation overlap\n-        default_stream = torch.cuda.current_stream()\n-        self.default_stream: torch.cuda.Stream = default_stream\n-        self.all_gather_copy_in_stream: torch.cuda.Stream = default_stream\n-        self.all_gather_stream: torch.cuda.Stream = default_stream\n-        self.all_gather_state = AllGatherStateHolder()\n+        self.comm_ctx = FSDPCommContext()\n         self._init_grad_divide_factors()\n \n         # - CUDA events for stream synchronization\n         # Holds the all-gather output buffer, sync objects, and metadata\n         self._all_gather_result: Optional[AllGatherResult] = None\n+        # Holds the reduce-scatter view-out CUDA event that marks the end of\n+        # the group's post-backward (e.g. reduce-scatter and div), which should\n+        # be waited on at the end of backward\n+        self._reduce_scatter_view_out_event: Optional[torch.cuda.Event] = None\n \n     # Initialization #\n     def _init_mp_dtypes(self) -> None:\n"
                },
                {
                    "old_start": 94,
                    "old_length": 8,
                    "new_start": 142,
                    "new_length": 7,
                    "hunk": "@@ -94,8 +142,7 @@ class FSDPParamGroup:\n             self.fsdp_params,\n             self._all_gather_process_group,\n             async_op,\n-            self._all_gather_copy_in_stream_for_unshard,\n-            self._all_gather_stream_for_unshard,\n+            *self.comm_ctx.get_all_gather_streams(self._training_state),\n             self.device,\n             self._param_dtype,\n         )\n"
                },
                {
                    "old_start": 112,
                    "old_length": 9,
                    "new_start": 159,
                    "new_length": 9,
                    "hunk": "@@ -112,9 +159,9 @@ class FSDPParamGroup:\n         if not self._all_gather_result:\n             return  # no preceding unshard\n         if self._training_state == TrainingState.FORWARD:  # implicit prefetch\n-            if prev_all_gather_state := self.all_gather_state.pop():\n+            if prev_all_gather_state := self.comm_ctx.all_gather_state:\n                 self._wait_all_gather_streams_on_event(prev_all_gather_state.event)\n-                del prev_all_gather_state  # free\n+                self.comm_ctx.all_gather_state = None  # free the all-gather result\n         foreach_all_gather_copy_out(\n             self._all_gather_result, self.fsdp_params, self._all_gather_process_group\n         )\n"
                },
                {
                    "old_start": 124,
                    "old_length": 16,
                    "new_start": 171,
                    "new_length": 16,
                    "hunk": "@@ -124,16 +171,16 @@ class FSDPParamGroup:\n         all_gather_copy_out_event = torch.cuda.Event()\n         all_gather_copy_out_event.record()\n         if self._training_state == TrainingState.FORWARD:\n-            self.all_gather_state.put(\n-                AllGatherState(self._all_gather_result, all_gather_copy_out_event)\n+            self.comm_ctx.all_gather_state = AllGatherState(\n+                self._all_gather_result, all_gather_copy_out_event\n             )\n         else:\n             self._wait_all_gather_streams_on_event(all_gather_copy_out_event)\n         self._all_gather_result = None  # free unless saved in `all_gather_state`\n \n     def _wait_all_gather_streams_on_event(self, event: torch.cuda.Event):\n-        self.all_gather_copy_in_stream.wait_event(event)\n-        self.all_gather_stream.wait_event(event)\n+        self.comm_ctx.all_gather_copy_in_stream.wait_event(event)\n+        self.comm_ctx.all_gather_stream.wait_event(event)\n \n     def reshard(self):\n         self._to_sharded()\n"
                },
                {
                    "old_start": 145,
                    "old_length": 6,
                    "new_start": 192,
                    "new_length": 7,
                    "hunk": "@@ -145,6 +192,7 @@ class FSDPParamGroup:\n             self._training_state = TrainingState.FORWARD\n             self.unshard()\n             self.wait_for_unshard()\n+            args, kwargs = self._register_post_backward_hook(args, kwargs)\n             return args, kwargs\n \n     def post_forward(self, module: nn.Module, input: Any, output: Any):\n"
                },
                {
                    "old_start": 153,
                    "old_length": 6,
                    "new_start": 201,
                    "new_length": 50,
                    "hunk": "@@ -153,6 +201,50 @@ class FSDPParamGroup:\n             self._training_state = TrainingState.IDLE\n             return output\n \n+    def pre_backward(self, *unused: Any):\n+        with torch.profiler.record_function(\"FSDP::pre_backward\"):\n+            self._training_state = TrainingState.PRE_BACKWARD\n+            self.unshard()  # no-op if prefetched\n+            self.wait_for_unshard()\n+\n+    def _post_backward(self, *unused: Any):\n+        self._training_state = TrainingState.POST_BACKWARD\n+        with torch.profiler.record_function(\"FSDP::post_backward_reshard\"):\n+            # Save the autograd-computed gradients before resharding to only\n+            # access the unsharded parameters when their data is present\n+            fsdp_params_with_grad: List[FSDPParam] = []\n+            unsharded_grads: List[torch.Tensor] = []\n+            for fsdp_param in self.fsdp_params:\n+                if fsdp_param.unsharded_param.grad is not None:\n+                    fsdp_params_with_grad.append(fsdp_param)\n+                    unsharded_grads.append(fsdp_param.unsharded_grad_data)\n+                    fsdp_param.unsharded_param.grad = None\n+            self.reshard()\n+        if len(fsdp_params_with_grad) == 0:\n+            return\n+        with torch.profiler.record_function(\"FSDP::post_backward_reduce\"):\n+            self._reduce_scatter_view_out_event = foreach_reduce_scatter(\n+                fsdp_params_with_grad,\n+                unsharded_grads,\n+                self._reduce_scatter_process_group,\n+                self.comm_ctx.reduce_scatter_stream,\n+                self._orig_dtype,\n+                self._orig_dtype,\n+                self.device,\n+                self._grad_predivide_factor,\n+                self._grad_postdivide_factor,\n+            )\n+\n+    def finalize_backward(self):\n+        if self._sharded_state == ShardedState.UNSHARDED:\n+            # Run post-backward here since the forward inputs did not require\n+            # gradient, so the post-backward hook did not run\n+            self._post_backward()\n+        if self._reduce_scatter_view_out_event is not None:\n+            torch.cuda.current_stream().wait_event(self._reduce_scatter_view_out_event)\n+            self._reduce_scatter_view_out_event = None\n+        self._training_state = TrainingState.IDLE\n+\n     # Utilities #\n     def _to_sharded(self):\n         if self._sharded_state != ShardedState.SHARDED:\n"
                },
                {
                    "old_start": 166,
                    "old_length": 6,
                    "new_start": 258,
                    "new_length": 32,
                    "hunk": "@@ -166,6 +258,32 @@ class FSDPParamGroup:\n                 fsdp_param.to_unsharded()\n             self._sharded_state = ShardedState.UNSHARDED\n \n+    # Hook Registration #\n+    def _register_post_backward_hook(\n+        self, args: Tuple[Any, ...], kwargs: Dict[str, Any]\n+    ) -> Tuple[Tuple[Any, ...], Dict[str, Any]]:\n+        if not torch.is_grad_enabled():\n+            return args, kwargs\n+        args_list, args_spec = tree_flatten(args)\n+        kwargs_list, kwargs_spec = tree_flatten(kwargs)\n+        args_kwargs_list = list(args_list) + list(kwargs_list)\n+        inp_tensor_indices: List[int] = []\n+        inp_tensors: List[torch.Tensor] = []\n+        for i, obj in enumerate(args_kwargs_list):\n+            if torch.is_tensor(obj) and obj.requires_grad:\n+                inp_tensor_indices.append(i)\n+                inp_tensors.append(obj)\n+        if len(inp_tensors) == 0:\n+            return args, kwargs  # no tensors that require gradients\n+        inp_tensors = RegisterPostBackwardFunction.apply(self, *inp_tensors)\n+        for inp_tensor_idx, inp_tensor in zip(inp_tensor_indices, inp_tensors):\n+            args_kwargs_list[inp_tensor_idx] = inp_tensor\n+        args_list = args_kwargs_list[: len(args_list)]\n+        kwargs_list = args_kwargs_list[len(args_list) :]\n+        args = tree_unflatten(args_list, args_spec)\n+        kwargs = tree_unflatten(kwargs_list, kwargs_spec)\n+        return args, kwargs\n+\n     # Properties #\n     @property\n     def _all_gather_process_group(self) -> dist.ProcessGroup:\n"
                },
                {
                    "old_start": 174,
                    "old_length": 33,
                    "new_start": 292,
                    "new_length": 18,
                    "hunk": "@@ -174,33 +292,18 @@ class FSDPParamGroup:\n         return mesh_info.shard_process_group\n \n     @property\n-    def _use_all_gather_stream(self) -> bool:\n-        return self._training_state in (\n-            TrainingState.FORWARD,\n-            TrainingState.PRE_BACKWARD,\n-        )\n-\n-    @property\n-    def _all_gather_copy_in_stream_for_unshard(self) -> torch.cuda.Stream:\n-        if self._use_all_gather_stream:\n-            return self.all_gather_copy_in_stream\n-        return self.default_stream\n-\n-    @property\n-    def _all_gather_stream_for_unshard(self) -> torch.cuda.Stream:\n-        if self._use_all_gather_stream:\n-            return self.all_gather_stream\n-        return self.default_stream\n+    def _reduce_scatter_process_group(self) -> dist.ProcessGroup:\n+        mesh_info = self.mesh_info\n+        assert isinstance(mesh_info, FSDPMeshInfo)\n+        return mesh_info.shard_process_group\n \n \n def _get_param_module_infos(\n     params: List[nn.Parameter], module: nn.Module\n ) -> List[ParamModuleInfo]:\n     \"\"\"\n-    Shared parameter:\n-        lin1.weight = lin2.weight\n-    Shared module:\n-        mlp.lin1 = mlp.lin2\n+    Shared parameter: lin1.weight = lin2.weight\n+    Shared module: mlp.lin1 = mlp.lin2\n     We do not remove duplicates when traversing both modules and parameters to\n     find shared modules' parameters and shared parameters within a module.\n     \"\"\"\n"
                },
                {
                    "old_start": 219,
                    "old_length": 3,
                    "new_start": 322,
                    "new_length": 16,
                    "hunk": "@@ -219,3 +322,16 @@ def _get_param_module_infos(\n     if len(param_to_module_info) != len(params):\n         raise AssertionError(f\"Some parameters are not in the module tree of {module}\")\n     return [param_to_module_info[param] for param in params]\n+\n+\n+class RegisterPostBackwardFunction(torch.autograd.Function):\n+    @staticmethod\n+    def forward(ctx, param_group: FSDPParamGroup, *inputs: torch.Tensor):\n+        # All tensors in `inputs` should require gradient\n+        ctx.param_group = param_group\n+        return inputs\n+\n+    @staticmethod\n+    def backward(ctx, *grads: torch.Tensor):\n+        ctx.param_group._post_backward()\n+        return (None,) + grads\n"
                }
            ],
            "whole_deleted": "-from typing import Any, Dict, List, Optional, Tuple\n-    AllGatherState,\n-    AllGatherStateHolder,\n-        default_stream = torch.cuda.current_stream()\n-        self.default_stream: torch.cuda.Stream = default_stream\n-        self.all_gather_copy_in_stream: torch.cuda.Stream = default_stream\n-        self.all_gather_stream: torch.cuda.Stream = default_stream\n-        self.all_gather_state = AllGatherStateHolder()\n-            self._all_gather_copy_in_stream_for_unshard,\n-            self._all_gather_stream_for_unshard,\n-            if prev_all_gather_state := self.all_gather_state.pop():\n-                del prev_all_gather_state  # free\n-            self.all_gather_state.put(\n-                AllGatherState(self._all_gather_result, all_gather_copy_out_event)\n-        self.all_gather_copy_in_stream.wait_event(event)\n-        self.all_gather_stream.wait_event(event)\n-    def _use_all_gather_stream(self) -> bool:\n-        return self._training_state in (\n-            TrainingState.FORWARD,\n-            TrainingState.PRE_BACKWARD,\n-        )\n-\n-    @property\n-    def _all_gather_copy_in_stream_for_unshard(self) -> torch.cuda.Stream:\n-        if self._use_all_gather_stream:\n-            return self.all_gather_copy_in_stream\n-        return self.default_stream\n-\n-    @property\n-    def _all_gather_stream_for_unshard(self) -> torch.cuda.Stream:\n-        if self._use_all_gather_stream:\n-            return self.all_gather_stream\n-        return self.default_stream\n-    Shared parameter:\n-        lin1.weight = lin2.weight\n-    Shared module:\n-        mlp.lin1 = mlp.lin2\n",
            "whole_added": "+from typing import Any, Dict, List, NamedTuple, Optional, Tuple\n+from torch.utils._pytree import tree_flatten, tree_unflatten\n+    foreach_reduce_scatter,\n+\"\"\"\n+[Note: Overlapping all-gather copy-in and all-gather]\n+For implicit forward prefetching, we want to overlap the next copy-in with the\n+current all-gather. We do so using a separate copy-in stream. However, since\n+we have the all-gather input as a view into the output, we must make sure to\n+copy into different memory from the current all-gather's output. Thus, we keep\n+a reference to the current all-gather's output and have the next FSDP parameter\n+group free it after its copy-in. Finally, we have the last FSDP state flush the\n+reference to avoid holding onto memory after forward.\n+\"\"\"\n+\n+\n+class FSDPCommContext:\n+    \"\"\"This has the communication state shared across FSDP states/parameter groups.\"\"\"\n+\n+    def init(self):\n+        # Setting the all-gather/reduce-scatter streams to be higher priority\n+        # can help avoid some issues where their copies in/out are delayed and\n+        # block computation\n+        high_priority = -1\n+        # All-gather state and copy-in stream allow overlapping the next\n+        # copy-in with the current all-gather in forward; copy-in overlaps with\n+        # reduce-scatter in backward without the separate copy-in stream\n+        self.all_gather_copy_in_stream = torch.cuda.Stream(priority=high_priority)\n+        self.all_gather_state: Optional[AllGatherState] = None\n+        # All-gather stream allows overlapping next all-gather with current\n+        # forward compute\n+        self.all_gather_stream = torch.cuda.Stream(priority=high_priority)\n+        # Reduce-scatter stream gives separate execution \"thread\" for post-\n+        # backward logic like pre/post-gradient division and reduce-scatter\n+        self.reduce_scatter_stream = torch.cuda.Stream(priority=high_priority)\n+\n+    def get_all_gather_streams(\n+        self, training_state: TrainingState\n+    ) -> Tuple[torch.cuda.Stream, torch.cuda.Stream]:\n+        if training_state in (TrainingState.FORWARD, TrainingState.PRE_BACKWARD):\n+            # Use separate streams for implicit prefetching\n+            return self.all_gather_copy_in_stream, self.all_gather_stream\n+        current_stream = torch.cuda.current_stream()\n+        return current_stream, current_stream\n+\n+\n+# See [Note: Overlapping all-gather copy-in and all-gather]\n+class AllGatherState(NamedTuple):\n+    all_gather_result: AllGatherResult\n+    event: torch.cuda.Event  # all-gather copy-out\n+\n+\n+        self.comm_ctx = FSDPCommContext()\n+        # Holds the reduce-scatter view-out CUDA event that marks the end of\n+        # the group's post-backward (e.g. reduce-scatter and div), which should\n+        # be waited on at the end of backward\n+        self._reduce_scatter_view_out_event: Optional[torch.cuda.Event] = None\n+            *self.comm_ctx.get_all_gather_streams(self._training_state),\n+            if prev_all_gather_state := self.comm_ctx.all_gather_state:\n+                self.comm_ctx.all_gather_state = None  # free the all-gather result\n+            self.comm_ctx.all_gather_state = AllGatherState(\n+                self._all_gather_result, all_gather_copy_out_event\n+        self.comm_ctx.all_gather_copy_in_stream.wait_event(event)\n+        self.comm_ctx.all_gather_stream.wait_event(event)\n+            args, kwargs = self._register_post_backward_hook(args, kwargs)\n+    def pre_backward(self, *unused: Any):\n+        with torch.profiler.record_function(\"FSDP::pre_backward\"):\n+            self._training_state = TrainingState.PRE_BACKWARD\n+            self.unshard()  # no-op if prefetched\n+            self.wait_for_unshard()\n+\n+    def _post_backward(self, *unused: Any):\n+        self._training_state = TrainingState.POST_BACKWARD\n+        with torch.profiler.record_function(\"FSDP::post_backward_reshard\"):\n+            # Save the autograd-computed gradients before resharding to only\n+            # access the unsharded parameters when their data is present\n+            fsdp_params_with_grad: List[FSDPParam] = []\n+            unsharded_grads: List[torch.Tensor] = []\n+            for fsdp_param in self.fsdp_params:\n+                if fsdp_param.unsharded_param.grad is not None:\n+                    fsdp_params_with_grad.append(fsdp_param)\n+                    unsharded_grads.append(fsdp_param.unsharded_grad_data)\n+                    fsdp_param.unsharded_param.grad = None\n+            self.reshard()\n+        if len(fsdp_params_with_grad) == 0:\n+            return\n+        with torch.profiler.record_function(\"FSDP::post_backward_reduce\"):\n+            self._reduce_scatter_view_out_event = foreach_reduce_scatter(\n+                fsdp_params_with_grad,\n+                unsharded_grads,\n+                self._reduce_scatter_process_group,\n+                self.comm_ctx.reduce_scatter_stream,\n+                self._orig_dtype,\n+                self._orig_dtype,\n+                self.device,\n+                self._grad_predivide_factor,\n+                self._grad_postdivide_factor,\n+            )\n+\n+    def finalize_backward(self):\n+        if self._sharded_state == ShardedState.UNSHARDED:\n+            # Run post-backward here since the forward inputs did not require\n+            # gradient, so the post-backward hook did not run\n+            self._post_backward()\n+        if self._reduce_scatter_view_out_event is not None:\n+            torch.cuda.current_stream().wait_event(self._reduce_scatter_view_out_event)\n+            self._reduce_scatter_view_out_event = None\n+        self._training_state = TrainingState.IDLE\n+\n+    # Hook Registration #\n+    def _register_post_backward_hook(\n+        self, args: Tuple[Any, ...], kwargs: Dict[str, Any]\n+    ) -> Tuple[Tuple[Any, ...], Dict[str, Any]]:\n+        if not torch.is_grad_enabled():\n+            return args, kwargs\n+        args_list, args_spec = tree_flatten(args)\n+        kwargs_list, kwargs_spec = tree_flatten(kwargs)\n+        args_kwargs_list = list(args_list) + list(kwargs_list)\n+        inp_tensor_indices: List[int] = []\n+        inp_tensors: List[torch.Tensor] = []\n+        for i, obj in enumerate(args_kwargs_list):\n+            if torch.is_tensor(obj) and obj.requires_grad:\n+                inp_tensor_indices.append(i)\n+                inp_tensors.append(obj)\n+        if len(inp_tensors) == 0:\n+            return args, kwargs  # no tensors that require gradients\n+        inp_tensors = RegisterPostBackwardFunction.apply(self, *inp_tensors)\n+        for inp_tensor_idx, inp_tensor in zip(inp_tensor_indices, inp_tensors):\n+            args_kwargs_list[inp_tensor_idx] = inp_tensor\n+        args_list = args_kwargs_list[: len(args_list)]\n+        kwargs_list = args_kwargs_list[len(args_list) :]\n+        args = tree_unflatten(args_list, args_spec)\n+        kwargs = tree_unflatten(kwargs_list, kwargs_spec)\n+        return args, kwargs\n+\n+    def _reduce_scatter_process_group(self) -> dist.ProcessGroup:\n+        mesh_info = self.mesh_info\n+        assert isinstance(mesh_info, FSDPMeshInfo)\n+        return mesh_info.shard_process_group\n+    Shared parameter: lin1.weight = lin2.weight\n+    Shared module: mlp.lin1 = mlp.lin2\n+\n+\n+class RegisterPostBackwardFunction(torch.autograd.Function):\n+    @staticmethod\n+    def forward(ctx, param_group: FSDPParamGroup, *inputs: torch.Tensor):\n+        # All tensors in `inputs` should require gradient\n+        ctx.param_group = param_group\n+        return inputs\n+\n+    @staticmethod\n+    def backward(ctx, *grads: torch.Tensor):\n+        ctx.param_group._post_backward()\n+        return (None,) + grads\n",
            "whole_hunk": "@@ -1,21 +1,69 @@\n-from typing import Any, Dict, List, Optional, Tuple\n+from typing import Any, Dict, List, NamedTuple, Optional, Tuple\n \n import torch\n import torch.distributed as dist\n import torch.nn as nn\n \n from torch.distributed.fsdp._common_utils import _named_parameters_with_duplicates\n+from torch.utils._pytree import tree_flatten, tree_unflatten\n from ._fsdp_collectives import (\n     AllGatherResult,\n-    AllGatherState,\n-    AllGatherStateHolder,\n     foreach_all_gather,\n     foreach_all_gather_copy_out,\n+    foreach_reduce_scatter,\n )\n from ._fsdp_common import FSDPMeshInfo, HSDPMeshInfo, TrainingState\n from ._fsdp_param import FSDPParam, ParamModuleInfo, ShardedState\n \n \n+\"\"\"\n+[Note: Overlapping all-gather copy-in and all-gather]\n+For implicit forward prefetching, we want to overlap the next copy-in with the\n+current all-gather. We do so using a separate copy-in stream. However, since\n+we have the all-gather input as a view into the output, we must make sure to\n+copy into different memory from the current all-gather's output. Thus, we keep\n+a reference to the current all-gather's output and have the next FSDP parameter\n+group free it after its copy-in. Finally, we have the last FSDP state flush the\n+reference to avoid holding onto memory after forward.\n+\"\"\"\n+\n+\n+class FSDPCommContext:\n+    \"\"\"This has the communication state shared across FSDP states/parameter groups.\"\"\"\n+\n+    def init(self):\n+        # Setting the all-gather/reduce-scatter streams to be higher priority\n+        # can help avoid some issues where their copies in/out are delayed and\n+        # block computation\n+        high_priority = -1\n+        # All-gather state and copy-in stream allow overlapping the next\n+        # copy-in with the current all-gather in forward; copy-in overlaps with\n+        # reduce-scatter in backward without the separate copy-in stream\n+        self.all_gather_copy_in_stream = torch.cuda.Stream(priority=high_priority)\n+        self.all_gather_state: Optional[AllGatherState] = None\n+        # All-gather stream allows overlapping next all-gather with current\n+        # forward compute\n+        self.all_gather_stream = torch.cuda.Stream(priority=high_priority)\n+        # Reduce-scatter stream gives separate execution \"thread\" for post-\n+        # backward logic like pre/post-gradient division and reduce-scatter\n+        self.reduce_scatter_stream = torch.cuda.Stream(priority=high_priority)\n+\n+    def get_all_gather_streams(\n+        self, training_state: TrainingState\n+    ) -> Tuple[torch.cuda.Stream, torch.cuda.Stream]:\n+        if training_state in (TrainingState.FORWARD, TrainingState.PRE_BACKWARD):\n+            # Use separate streams for implicit prefetching\n+            return self.all_gather_copy_in_stream, self.all_gather_stream\n+        current_stream = torch.cuda.current_stream()\n+        return current_stream, current_stream\n+\n+\n+# See [Note: Overlapping all-gather copy-in and all-gather]\n+class AllGatherState(NamedTuple):\n+    all_gather_result: AllGatherResult\n+    event: torch.cuda.Event  # all-gather copy-out\n+\n+\n class FSDPParamGroup:\n     \"\"\"This class represents a parameter group to communicate together.\"\"\"\n \n@@ -41,16 +89,16 @@ class FSDPParamGroup:\n         self._module_fqn: Optional[str] = None  # prefixed from root module\n \n         # - Communication and communication/computation overlap\n-        default_stream = torch.cuda.current_stream()\n-        self.default_stream: torch.cuda.Stream = default_stream\n-        self.all_gather_copy_in_stream: torch.cuda.Stream = default_stream\n-        self.all_gather_stream: torch.cuda.Stream = default_stream\n-        self.all_gather_state = AllGatherStateHolder()\n+        self.comm_ctx = FSDPCommContext()\n         self._init_grad_divide_factors()\n \n         # - CUDA events for stream synchronization\n         # Holds the all-gather output buffer, sync objects, and metadata\n         self._all_gather_result: Optional[AllGatherResult] = None\n+        # Holds the reduce-scatter view-out CUDA event that marks the end of\n+        # the group's post-backward (e.g. reduce-scatter and div), which should\n+        # be waited on at the end of backward\n+        self._reduce_scatter_view_out_event: Optional[torch.cuda.Event] = None\n \n     # Initialization #\n     def _init_mp_dtypes(self) -> None:\n@@ -94,8 +142,7 @@ class FSDPParamGroup:\n             self.fsdp_params,\n             self._all_gather_process_group,\n             async_op,\n-            self._all_gather_copy_in_stream_for_unshard,\n-            self._all_gather_stream_for_unshard,\n+            *self.comm_ctx.get_all_gather_streams(self._training_state),\n             self.device,\n             self._param_dtype,\n         )\n@@ -112,9 +159,9 @@ class FSDPParamGroup:\n         if not self._all_gather_result:\n             return  # no preceding unshard\n         if self._training_state == TrainingState.FORWARD:  # implicit prefetch\n-            if prev_all_gather_state := self.all_gather_state.pop():\n+            if prev_all_gather_state := self.comm_ctx.all_gather_state:\n                 self._wait_all_gather_streams_on_event(prev_all_gather_state.event)\n-                del prev_all_gather_state  # free\n+                self.comm_ctx.all_gather_state = None  # free the all-gather result\n         foreach_all_gather_copy_out(\n             self._all_gather_result, self.fsdp_params, self._all_gather_process_group\n         )\n@@ -124,16 +171,16 @@ class FSDPParamGroup:\n         all_gather_copy_out_event = torch.cuda.Event()\n         all_gather_copy_out_event.record()\n         if self._training_state == TrainingState.FORWARD:\n-            self.all_gather_state.put(\n-                AllGatherState(self._all_gather_result, all_gather_copy_out_event)\n+            self.comm_ctx.all_gather_state = AllGatherState(\n+                self._all_gather_result, all_gather_copy_out_event\n             )\n         else:\n             self._wait_all_gather_streams_on_event(all_gather_copy_out_event)\n         self._all_gather_result = None  # free unless saved in `all_gather_state`\n \n     def _wait_all_gather_streams_on_event(self, event: torch.cuda.Event):\n-        self.all_gather_copy_in_stream.wait_event(event)\n-        self.all_gather_stream.wait_event(event)\n+        self.comm_ctx.all_gather_copy_in_stream.wait_event(event)\n+        self.comm_ctx.all_gather_stream.wait_event(event)\n \n     def reshard(self):\n         self._to_sharded()\n@@ -145,6 +192,7 @@ class FSDPParamGroup:\n             self._training_state = TrainingState.FORWARD\n             self.unshard()\n             self.wait_for_unshard()\n+            args, kwargs = self._register_post_backward_hook(args, kwargs)\n             return args, kwargs\n \n     def post_forward(self, module: nn.Module, input: Any, output: Any):\n@@ -153,6 +201,50 @@ class FSDPParamGroup:\n             self._training_state = TrainingState.IDLE\n             return output\n \n+    def pre_backward(self, *unused: Any):\n+        with torch.profiler.record_function(\"FSDP::pre_backward\"):\n+            self._training_state = TrainingState.PRE_BACKWARD\n+            self.unshard()  # no-op if prefetched\n+            self.wait_for_unshard()\n+\n+    def _post_backward(self, *unused: Any):\n+        self._training_state = TrainingState.POST_BACKWARD\n+        with torch.profiler.record_function(\"FSDP::post_backward_reshard\"):\n+            # Save the autograd-computed gradients before resharding to only\n+            # access the unsharded parameters when their data is present\n+            fsdp_params_with_grad: List[FSDPParam] = []\n+            unsharded_grads: List[torch.Tensor] = []\n+            for fsdp_param in self.fsdp_params:\n+                if fsdp_param.unsharded_param.grad is not None:\n+                    fsdp_params_with_grad.append(fsdp_param)\n+                    unsharded_grads.append(fsdp_param.unsharded_grad_data)\n+                    fsdp_param.unsharded_param.grad = None\n+            self.reshard()\n+        if len(fsdp_params_with_grad) == 0:\n+            return\n+        with torch.profiler.record_function(\"FSDP::post_backward_reduce\"):\n+            self._reduce_scatter_view_out_event = foreach_reduce_scatter(\n+                fsdp_params_with_grad,\n+                unsharded_grads,\n+                self._reduce_scatter_process_group,\n+                self.comm_ctx.reduce_scatter_stream,\n+                self._orig_dtype,\n+                self._orig_dtype,\n+                self.device,\n+                self._grad_predivide_factor,\n+                self._grad_postdivide_factor,\n+            )\n+\n+    def finalize_backward(self):\n+        if self._sharded_state == ShardedState.UNSHARDED:\n+            # Run post-backward here since the forward inputs did not require\n+            # gradient, so the post-backward hook did not run\n+            self._post_backward()\n+        if self._reduce_scatter_view_out_event is not None:\n+            torch.cuda.current_stream().wait_event(self._reduce_scatter_view_out_event)\n+            self._reduce_scatter_view_out_event = None\n+        self._training_state = TrainingState.IDLE\n+\n     # Utilities #\n     def _to_sharded(self):\n         if self._sharded_state != ShardedState.SHARDED:\n@@ -166,6 +258,32 @@ class FSDPParamGroup:\n                 fsdp_param.to_unsharded()\n             self._sharded_state = ShardedState.UNSHARDED\n \n+    # Hook Registration #\n+    def _register_post_backward_hook(\n+        self, args: Tuple[Any, ...], kwargs: Dict[str, Any]\n+    ) -> Tuple[Tuple[Any, ...], Dict[str, Any]]:\n+        if not torch.is_grad_enabled():\n+            return args, kwargs\n+        args_list, args_spec = tree_flatten(args)\n+        kwargs_list, kwargs_spec = tree_flatten(kwargs)\n+        args_kwargs_list = list(args_list) + list(kwargs_list)\n+        inp_tensor_indices: List[int] = []\n+        inp_tensors: List[torch.Tensor] = []\n+        for i, obj in enumerate(args_kwargs_list):\n+            if torch.is_tensor(obj) and obj.requires_grad:\n+                inp_tensor_indices.append(i)\n+                inp_tensors.append(obj)\n+        if len(inp_tensors) == 0:\n+            return args, kwargs  # no tensors that require gradients\n+        inp_tensors = RegisterPostBackwardFunction.apply(self, *inp_tensors)\n+        for inp_tensor_idx, inp_tensor in zip(inp_tensor_indices, inp_tensors):\n+            args_kwargs_list[inp_tensor_idx] = inp_tensor\n+        args_list = args_kwargs_list[: len(args_list)]\n+        kwargs_list = args_kwargs_list[len(args_list) :]\n+        args = tree_unflatten(args_list, args_spec)\n+        kwargs = tree_unflatten(kwargs_list, kwargs_spec)\n+        return args, kwargs\n+\n     # Properties #\n     @property\n     def _all_gather_process_group(self) -> dist.ProcessGroup:\n@@ -174,33 +292,18 @@ class FSDPParamGroup:\n         return mesh_info.shard_process_group\n \n     @property\n-    def _use_all_gather_stream(self) -> bool:\n-        return self._training_state in (\n-            TrainingState.FORWARD,\n-            TrainingState.PRE_BACKWARD,\n-        )\n-\n-    @property\n-    def _all_gather_copy_in_stream_for_unshard(self) -> torch.cuda.Stream:\n-        if self._use_all_gather_stream:\n-            return self.all_gather_copy_in_stream\n-        return self.default_stream\n-\n-    @property\n-    def _all_gather_stream_for_unshard(self) -> torch.cuda.Stream:\n-        if self._use_all_gather_stream:\n-            return self.all_gather_stream\n-        return self.default_stream\n+    def _reduce_scatter_process_group(self) -> dist.ProcessGroup:\n+        mesh_info = self.mesh_info\n+        assert isinstance(mesh_info, FSDPMeshInfo)\n+        return mesh_info.shard_process_group\n \n \n def _get_param_module_infos(\n     params: List[nn.Parameter], module: nn.Module\n ) -> List[ParamModuleInfo]:\n     \"\"\"\n-    Shared parameter:\n-        lin1.weight = lin2.weight\n-    Shared module:\n-        mlp.lin1 = mlp.lin2\n+    Shared parameter: lin1.weight = lin2.weight\n+    Shared module: mlp.lin1 = mlp.lin2\n     We do not remove duplicates when traversing both modules and parameters to\n     find shared modules' parameters and shared parameters within a module.\n     \"\"\"\n@@ -219,3 +322,16 @@ def _get_param_module_infos(\n     if len(param_to_module_info) != len(params):\n         raise AssertionError(f\"Some parameters are not in the module tree of {module}\")\n     return [param_to_module_info[param] for param in params]\n+\n+\n+class RegisterPostBackwardFunction(torch.autograd.Function):\n+    @staticmethod\n+    def forward(ctx, param_group: FSDPParamGroup, *inputs: torch.Tensor):\n+        # All tensors in `inputs` should require gradient\n+        ctx.param_group = param_group\n+        return inputs\n+\n+    @staticmethod\n+    def backward(ctx, *grads: torch.Tensor):\n+        ctx.param_group._post_backward()\n+        return (None,) + grads\n"
        },
        {
            "name": "_fsdp_state.py",
            "path": "torch/distributed/_composable/fsdp/_fsdp_state.py",
            "patches": [
                {
                    "old_start": 2,
                    "old_length": 49,
                    "new_start": 2,
                    "new_length": 58,
                    "hunk": "@@ -2,49 +2,58 @@ from typing import Any, Dict, List, Optional, Tuple\n \n import torch\n import torch.nn as nn\n-\n+from torch.autograd import Variable\n+from torch.autograd.graph import register_multi_grad_hook\n from torch.distributed._composable_state import (\n     _get_module_state,\n     _insert_module_state,\n     _State,\n )\n from torch.distributed.utils import _to_kwargs\n+from torch.utils._pytree import tree_flatten\n from torch.utils.hooks import RemovableHandle\n-from ._fsdp_collectives import AllGatherStateHolder\n from ._fsdp_common import TrainingState\n from ._fsdp_param import FSDPParam\n+from ._fsdp_param_group import FSDPCommContext, FSDPParamGroup\n+\n+\n+class FSDPStateContext:\n+    \"\"\"This has state shared across FSDP states.\"\"\"\n \n-from ._fsdp_param_group import FSDPParamGroup\n+    def __init__(self):\n+        # All FSDP states in the root state's module tree\n+        self.all_states: List[FSDPState] = []\n+        # Iteration's forward root runs the once-per-forward logic; this root\n+        # may not be the overall root set by lazy initialization in cases where\n+        # only a submodule runs forward (e.g. encoder-only for eval)\n+        self.iter_forward_root: Optional[FSDPState] = None\n+        # Final callback should only be queued once per backward\n+        self.post_backward_final_callback_queued: bool = False\n \n \n class FSDPState(_State):\n     _module: nn.Module  # permit ref cycle since module and state lifetimes are 1:1\n     _device: torch.device\n-    _default_stream: torch.cuda.Stream\n-    _all_gather_copy_in_stream: torch.cuda.Stream\n-    _all_gather_stream: torch.cuda.Stream\n-    # For overlapping current copy-out and next all-gather in forward\n-    _all_gather_state: AllGatherStateHolder\n \n     def __init__(self):\n         super().__init__()\n         self._fsdp_param_group: Optional[FSDPParamGroup] = None\n-        self._is_root: Optional[bool] = None\n+        self._is_root: Optional[bool] = None  # root set during lazy init\n+        self._state_ctx = FSDPStateContext()\n+        self._comm_ctx = FSDPCommContext()\n         self._training_state: TrainingState = TrainingState.IDLE\n         self._pre_forward_hook_handle: Optional[RemovableHandle] = None\n-\n-        # Attributes only used on the root state:\n-        self._all_states: List[FSDPState] = []\n+        self._pre_backward_hook_handles: List[RemovableHandle] = []\n \n     # Define a separate init since `__init__` is called in the contract\n     def init(self, module: nn.Module, device: torch.device) -> None:\n         _insert_module_state(module, self)\n         self._module = module\n         self._device = device\n-        self._pre_forward_hook_handle = self._module.register_forward_pre_hook(\n+        self._pre_forward_hook_handle = module.register_forward_pre_hook(\n             self._pre_forward, prepend=True, with_kwargs=True\n         )\n-        self._post_forward_hook_handle = self._module.register_forward_hook(\n+        self._post_forward_hook_handle = module.register_forward_hook(\n             self._post_forward, prepend=False\n         )\n \n"
                },
                {
                    "old_start": 52,
                    "old_length": 9,
                    "new_start": 61,
                    "new_length": 14,
                    "hunk": "@@ -52,9 +61,14 @@ class FSDPState(_State):\n         self, module: nn.Module, args: Tuple[Any, ...], kwargs: Dict[str, Any]\n     ) -> Tuple[Tuple[Any, ...], Dict[str, Any]]:\n         self._lazy_init()\n-        if not self._is_root:\n+        if self._state_ctx.iter_forward_root is not None:\n             return args, kwargs\n+        self._state_ctx.iter_forward_root = self\n         with torch.profiler.record_function(\"FSDP::root_pre_forward\"):\n+            # Wait for optimizer before implicitly prefetched all-gathers\n+            current_stream = torch.cuda.current_stream()\n+            self._comm_ctx.all_gather_copy_in_stream.wait_stream(current_stream)\n+            self._comm_ctx.all_gather_stream.wait_stream(current_stream)\n             if self._device.type == \"cuda\":\n                 with torch.profiler.record_function(\"FSDP::inputs_to_device\"):\n                     args_tuple, kwargs_tuple = _to_kwargs(\n"
                },
                {
                    "old_start": 64,
                    "old_length": 35,
                    "new_start": 78,
                    "new_length": 38,
                    "hunk": "@@ -64,35 +78,38 @@ class FSDPState(_State):\n         return args, kwargs\n \n     def _lazy_init(self) -> None:\n+        \"\"\"\n+        Lazy initialization represents when all modules' parallelisms have\n+        finalized (e.g. FSDP has been applied to all desired modules). This\n+        means that we can determine which state is the root, and we do so by\n+        the 1st state to run forward.\n+        \"\"\"\n         if self._is_root is not None:\n             return  # no-op: already initialized\n         self._is_root = True\n         root_module = self._module\n-        for module in root_module.modules():\n-            if (state := _get_module_fsdp_state(module)) is not None:\n-                if module is not root_module:\n-                    state._is_root = False\n-                self._all_states.append(state)\n+        for module_name, module in root_module.named_modules():\n+            if (state := _get_module_fsdp_state(module)) is None:\n+                continue\n+            if module is not root_module:\n+                if state._is_root is not None:\n+                    raise RuntimeError(\n+                        \"FSDP state has already been lazily initialized for \"\n+                        f\"{module_name}\\nFSDP requires running forward through \"\n+                        \"the root module first\"\n+                    )\n+                state._is_root = False\n+            self._state_ctx.all_states.append(state)\n         self._init_fqns()\n         self._init_shared_state()\n \n     def _init_shared_state(self) -> None:\n-        # Setting the all-gather/reduce-scatter streams to be higher priority\n-        # can help avoid some issues where their copies in/out are delayed and\n-        # block computation\n-        high_priority = -1\n-        self._default_stream = torch.cuda.current_stream()\n-        self._all_gather_copy_in_stream = torch.cuda.Stream(priority=high_priority)\n-        self._all_gather_stream = torch.cuda.Stream(priority=high_priority)\n-        self._all_gather_state = AllGatherStateHolder()\n-        for state in self._all_states:\n+        self._comm_ctx.init()\n+        for state in self._state_ctx.all_states:\n+            state._state_ctx = self._state_ctx\n+            state._comm_ctx = self._comm_ctx\n             if fsdp_param_group := state._fsdp_param_group:\n-                fsdp_param_group.default_stream = self._default_stream\n-                fsdp_param_group.all_gather_copy_in_stream = (\n-                    self._all_gather_copy_in_stream\n-                )\n-                fsdp_param_group.all_gather_stream = self._all_gather_stream\n-                fsdp_param_group.all_gather_state = self._all_gather_state\n+                fsdp_param_group.comm_ctx = self._comm_ctx\n \n     def _init_fqns(self) -> None:\n         \"\"\"Sets module and parameter FQN attributes for debugging.\"\"\"\n"
                },
                {
                    "old_start": 100,
                    "old_length": 7,
                    "new_start": 117,
                    "new_length": 7,
                    "hunk": "@@ -100,7 +117,7 @@ class FSDPState(_State):\n         root_module = self._module\n         param_to_fsdp_param: Dict[nn.Parameter, FSDPParam] = {}\n         module_to_fsdp_param_group: Dict[nn.Module, FSDPParamGroup] = {}\n-        for state in self._all_states:\n+        for state in self._state_ctx.all_states:\n             if fsdp_param_group := state._fsdp_param_group:\n                 for fsdp_param in fsdp_param_group.fsdp_params:\n                     param_to_fsdp_param[fsdp_param.sharded_param] = fsdp_param\n"
                },
                {
                    "old_start": 124,
                    "old_length": 11,
                    "new_start": 141,
                    "new_length": 55,
                    "hunk": "@@ -124,11 +141,55 @@ class FSDPState(_State):\n     def _post_forward(self, module: nn.Module, input: Any, output: Any) -> Any:\n         if self._fsdp_param_group:\n             output = self._fsdp_param_group.post_forward(module, input, output)\n+        output = self._register_pre_backward_hook(output)\n         self._training_state = TrainingState.IDLE\n-        if self._is_root and (all_gather_state := self._all_gather_state.pop()):\n-            self._all_gather_copy_in_stream.wait_event(all_gather_state.event)\n-            self._all_gather_stream.wait_event(all_gather_state.event)\n-            del all_gather_state  # free\n+        if self._state_ctx.iter_forward_root is self:\n+            if all_gather_state := self._comm_ctx.all_gather_state:\n+                # Free the last all-gather result if needed; refer to\n+                # [Note: Overlapping all-gather copy-in and all-gather]\n+                self._comm_ctx.all_gather_copy_in_stream.wait_event(\n+                    all_gather_state.event\n+                )\n+                self._comm_ctx.all_gather_stream.wait_event(all_gather_state.event)\n+                self._comm_ctx.all_gather_state = None  # free the all-gather result\n+            self._state_ctx.iter_forward_root = None\n+\n+    def _pre_backward(self, *unused: Any) -> None:\n+        self._training_state = TrainingState.PRE_BACKWARD\n+        self._register_root_post_backward_final_callback()\n+        if self._fsdp_param_group:\n+            self._fsdp_param_group.pre_backward(*unused)\n+\n+    def _root_post_backward_final_callback(self) -> None:\n+        with torch.profiler.record_function(\"FSDP::root_post_backward_callback\"):\n+            self._training_state = TrainingState.IDLE\n+            for state in self._state_ctx.all_states:\n+                state._training_state = TrainingState.IDLE\n+                if state._fsdp_param_group:\n+                    state._fsdp_param_group.finalize_backward()\n+            self._state_ctx.post_backward_final_callback_queued = False\n+            for handle in self._pre_backward_hook_handles:\n+                handle.remove()\n+            self._pre_backward_hook_handles.clear()\n+\n+    def _register_pre_backward_hook(self, output: Any) -> Any:\n+        if not torch.is_grad_enabled():\n+            return output\n+\n+        flat_outputs, _ = tree_flatten(output)\n+        tensors = tuple(t for t in flat_outputs if t.requires_grad)\n+        if tensors:\n+            handle = register_multi_grad_hook(tensors, self._pre_backward, mode=\"any\")\n+            self._pre_backward_hook_handles.append(handle)\n+        return output\n+\n+    def _register_root_post_backward_final_callback(self):\n+        if self._state_ctx.post_backward_final_callback_queued:\n+            return\n+        self._state_ctx.post_backward_final_callback_queued = True\n+        Variable._execution_engine.queue_callback(\n+            self._root_post_backward_final_callback\n+        )\n \n \n def _get_module_fsdp_state(module: nn.Module) -> Optional[FSDPState]:\n"
                }
            ],
            "whole_deleted": "-\n-from ._fsdp_collectives import AllGatherStateHolder\n-from ._fsdp_param_group import FSDPParamGroup\n-    _default_stream: torch.cuda.Stream\n-    _all_gather_copy_in_stream: torch.cuda.Stream\n-    _all_gather_stream: torch.cuda.Stream\n-    # For overlapping current copy-out and next all-gather in forward\n-    _all_gather_state: AllGatherStateHolder\n-        self._is_root: Optional[bool] = None\n-\n-        # Attributes only used on the root state:\n-        self._all_states: List[FSDPState] = []\n-        self._pre_forward_hook_handle = self._module.register_forward_pre_hook(\n-        self._post_forward_hook_handle = self._module.register_forward_hook(\n-        if not self._is_root:\n-        for module in root_module.modules():\n-            if (state := _get_module_fsdp_state(module)) is not None:\n-                if module is not root_module:\n-                    state._is_root = False\n-                self._all_states.append(state)\n-        # Setting the all-gather/reduce-scatter streams to be higher priority\n-        # can help avoid some issues where their copies in/out are delayed and\n-        # block computation\n-        high_priority = -1\n-        self._default_stream = torch.cuda.current_stream()\n-        self._all_gather_copy_in_stream = torch.cuda.Stream(priority=high_priority)\n-        self._all_gather_stream = torch.cuda.Stream(priority=high_priority)\n-        self._all_gather_state = AllGatherStateHolder()\n-        for state in self._all_states:\n-                fsdp_param_group.default_stream = self._default_stream\n-                fsdp_param_group.all_gather_copy_in_stream = (\n-                    self._all_gather_copy_in_stream\n-                )\n-                fsdp_param_group.all_gather_stream = self._all_gather_stream\n-                fsdp_param_group.all_gather_state = self._all_gather_state\n-        for state in self._all_states:\n-        if self._is_root and (all_gather_state := self._all_gather_state.pop()):\n-            self._all_gather_copy_in_stream.wait_event(all_gather_state.event)\n-            self._all_gather_stream.wait_event(all_gather_state.event)\n-            del all_gather_state  # free\n",
            "whole_added": "+from torch.autograd import Variable\n+from torch.autograd.graph import register_multi_grad_hook\n+from torch.utils._pytree import tree_flatten\n+from ._fsdp_param_group import FSDPCommContext, FSDPParamGroup\n+\n+\n+class FSDPStateContext:\n+    \"\"\"This has state shared across FSDP states.\"\"\"\n+    def __init__(self):\n+        # All FSDP states in the root state's module tree\n+        self.all_states: List[FSDPState] = []\n+        # Iteration's forward root runs the once-per-forward logic; this root\n+        # may not be the overall root set by lazy initialization in cases where\n+        # only a submodule runs forward (e.g. encoder-only for eval)\n+        self.iter_forward_root: Optional[FSDPState] = None\n+        # Final callback should only be queued once per backward\n+        self.post_backward_final_callback_queued: bool = False\n+        self._is_root: Optional[bool] = None  # root set during lazy init\n+        self._state_ctx = FSDPStateContext()\n+        self._comm_ctx = FSDPCommContext()\n+        self._pre_backward_hook_handles: List[RemovableHandle] = []\n+        self._pre_forward_hook_handle = module.register_forward_pre_hook(\n+        self._post_forward_hook_handle = module.register_forward_hook(\n+        if self._state_ctx.iter_forward_root is not None:\n+        self._state_ctx.iter_forward_root = self\n+            # Wait for optimizer before implicitly prefetched all-gathers\n+            current_stream = torch.cuda.current_stream()\n+            self._comm_ctx.all_gather_copy_in_stream.wait_stream(current_stream)\n+            self._comm_ctx.all_gather_stream.wait_stream(current_stream)\n+        \"\"\"\n+        Lazy initialization represents when all modules' parallelisms have\n+        finalized (e.g. FSDP has been applied to all desired modules). This\n+        means that we can determine which state is the root, and we do so by\n+        the 1st state to run forward.\n+        \"\"\"\n+        for module_name, module in root_module.named_modules():\n+            if (state := _get_module_fsdp_state(module)) is None:\n+                continue\n+            if module is not root_module:\n+                if state._is_root is not None:\n+                    raise RuntimeError(\n+                        \"FSDP state has already been lazily initialized for \"\n+                        f\"{module_name}\\nFSDP requires running forward through \"\n+                        \"the root module first\"\n+                    )\n+                state._is_root = False\n+            self._state_ctx.all_states.append(state)\n+        self._comm_ctx.init()\n+        for state in self._state_ctx.all_states:\n+            state._state_ctx = self._state_ctx\n+            state._comm_ctx = self._comm_ctx\n+                fsdp_param_group.comm_ctx = self._comm_ctx\n+        for state in self._state_ctx.all_states:\n+        output = self._register_pre_backward_hook(output)\n+        if self._state_ctx.iter_forward_root is self:\n+            if all_gather_state := self._comm_ctx.all_gather_state:\n+                # Free the last all-gather result if needed; refer to\n+                # [Note: Overlapping all-gather copy-in and all-gather]\n+                self._comm_ctx.all_gather_copy_in_stream.wait_event(\n+                    all_gather_state.event\n+                )\n+                self._comm_ctx.all_gather_stream.wait_event(all_gather_state.event)\n+                self._comm_ctx.all_gather_state = None  # free the all-gather result\n+            self._state_ctx.iter_forward_root = None\n+\n+    def _pre_backward(self, *unused: Any) -> None:\n+        self._training_state = TrainingState.PRE_BACKWARD\n+        self._register_root_post_backward_final_callback()\n+        if self._fsdp_param_group:\n+            self._fsdp_param_group.pre_backward(*unused)\n+\n+    def _root_post_backward_final_callback(self) -> None:\n+        with torch.profiler.record_function(\"FSDP::root_post_backward_callback\"):\n+            self._training_state = TrainingState.IDLE\n+            for state in self._state_ctx.all_states:\n+                state._training_state = TrainingState.IDLE\n+                if state._fsdp_param_group:\n+                    state._fsdp_param_group.finalize_backward()\n+            self._state_ctx.post_backward_final_callback_queued = False\n+            for handle in self._pre_backward_hook_handles:\n+                handle.remove()\n+            self._pre_backward_hook_handles.clear()\n+\n+    def _register_pre_backward_hook(self, output: Any) -> Any:\n+        if not torch.is_grad_enabled():\n+            return output\n+\n+        flat_outputs, _ = tree_flatten(output)\n+        tensors = tuple(t for t in flat_outputs if t.requires_grad)\n+        if tensors:\n+            handle = register_multi_grad_hook(tensors, self._pre_backward, mode=\"any\")\n+            self._pre_backward_hook_handles.append(handle)\n+        return output\n+\n+    def _register_root_post_backward_final_callback(self):\n+        if self._state_ctx.post_backward_final_callback_queued:\n+            return\n+        self._state_ctx.post_backward_final_callback_queued = True\n+        Variable._execution_engine.queue_callback(\n+            self._root_post_backward_final_callback\n+        )\n",
            "whole_hunk": "@@ -2,49 +2,58 @@ from typing import Any, Dict, List, Optional, Tuple\n \n import torch\n import torch.nn as nn\n-\n+from torch.autograd import Variable\n+from torch.autograd.graph import register_multi_grad_hook\n from torch.distributed._composable_state import (\n     _get_module_state,\n     _insert_module_state,\n     _State,\n )\n from torch.distributed.utils import _to_kwargs\n+from torch.utils._pytree import tree_flatten\n from torch.utils.hooks import RemovableHandle\n-from ._fsdp_collectives import AllGatherStateHolder\n from ._fsdp_common import TrainingState\n from ._fsdp_param import FSDPParam\n+from ._fsdp_param_group import FSDPCommContext, FSDPParamGroup\n+\n+\n+class FSDPStateContext:\n+    \"\"\"This has state shared across FSDP states.\"\"\"\n \n-from ._fsdp_param_group import FSDPParamGroup\n+    def __init__(self):\n+        # All FSDP states in the root state's module tree\n+        self.all_states: List[FSDPState] = []\n+        # Iteration's forward root runs the once-per-forward logic; this root\n+        # may not be the overall root set by lazy initialization in cases where\n+        # only a submodule runs forward (e.g. encoder-only for eval)\n+        self.iter_forward_root: Optional[FSDPState] = None\n+        # Final callback should only be queued once per backward\n+        self.post_backward_final_callback_queued: bool = False\n \n \n class FSDPState(_State):\n     _module: nn.Module  # permit ref cycle since module and state lifetimes are 1:1\n     _device: torch.device\n-    _default_stream: torch.cuda.Stream\n-    _all_gather_copy_in_stream: torch.cuda.Stream\n-    _all_gather_stream: torch.cuda.Stream\n-    # For overlapping current copy-out and next all-gather in forward\n-    _all_gather_state: AllGatherStateHolder\n \n     def __init__(self):\n         super().__init__()\n         self._fsdp_param_group: Optional[FSDPParamGroup] = None\n-        self._is_root: Optional[bool] = None\n+        self._is_root: Optional[bool] = None  # root set during lazy init\n+        self._state_ctx = FSDPStateContext()\n+        self._comm_ctx = FSDPCommContext()\n         self._training_state: TrainingState = TrainingState.IDLE\n         self._pre_forward_hook_handle: Optional[RemovableHandle] = None\n-\n-        # Attributes only used on the root state:\n-        self._all_states: List[FSDPState] = []\n+        self._pre_backward_hook_handles: List[RemovableHandle] = []\n \n     # Define a separate init since `__init__` is called in the contract\n     def init(self, module: nn.Module, device: torch.device) -> None:\n         _insert_module_state(module, self)\n         self._module = module\n         self._device = device\n-        self._pre_forward_hook_handle = self._module.register_forward_pre_hook(\n+        self._pre_forward_hook_handle = module.register_forward_pre_hook(\n             self._pre_forward, prepend=True, with_kwargs=True\n         )\n-        self._post_forward_hook_handle = self._module.register_forward_hook(\n+        self._post_forward_hook_handle = module.register_forward_hook(\n             self._post_forward, prepend=False\n         )\n \n@@ -52,9 +61,14 @@ class FSDPState(_State):\n         self, module: nn.Module, args: Tuple[Any, ...], kwargs: Dict[str, Any]\n     ) -> Tuple[Tuple[Any, ...], Dict[str, Any]]:\n         self._lazy_init()\n-        if not self._is_root:\n+        if self._state_ctx.iter_forward_root is not None:\n             return args, kwargs\n+        self._state_ctx.iter_forward_root = self\n         with torch.profiler.record_function(\"FSDP::root_pre_forward\"):\n+            # Wait for optimizer before implicitly prefetched all-gathers\n+            current_stream = torch.cuda.current_stream()\n+            self._comm_ctx.all_gather_copy_in_stream.wait_stream(current_stream)\n+            self._comm_ctx.all_gather_stream.wait_stream(current_stream)\n             if self._device.type == \"cuda\":\n                 with torch.profiler.record_function(\"FSDP::inputs_to_device\"):\n                     args_tuple, kwargs_tuple = _to_kwargs(\n@@ -64,35 +78,38 @@ class FSDPState(_State):\n         return args, kwargs\n \n     def _lazy_init(self) -> None:\n+        \"\"\"\n+        Lazy initialization represents when all modules' parallelisms have\n+        finalized (e.g. FSDP has been applied to all desired modules). This\n+        means that we can determine which state is the root, and we do so by\n+        the 1st state to run forward.\n+        \"\"\"\n         if self._is_root is not None:\n             return  # no-op: already initialized\n         self._is_root = True\n         root_module = self._module\n-        for module in root_module.modules():\n-            if (state := _get_module_fsdp_state(module)) is not None:\n-                if module is not root_module:\n-                    state._is_root = False\n-                self._all_states.append(state)\n+        for module_name, module in root_module.named_modules():\n+            if (state := _get_module_fsdp_state(module)) is None:\n+                continue\n+            if module is not root_module:\n+                if state._is_root is not None:\n+                    raise RuntimeError(\n+                        \"FSDP state has already been lazily initialized for \"\n+                        f\"{module_name}\\nFSDP requires running forward through \"\n+                        \"the root module first\"\n+                    )\n+                state._is_root = False\n+            self._state_ctx.all_states.append(state)\n         self._init_fqns()\n         self._init_shared_state()\n \n     def _init_shared_state(self) -> None:\n-        # Setting the all-gather/reduce-scatter streams to be higher priority\n-        # can help avoid some issues where their copies in/out are delayed and\n-        # block computation\n-        high_priority = -1\n-        self._default_stream = torch.cuda.current_stream()\n-        self._all_gather_copy_in_stream = torch.cuda.Stream(priority=high_priority)\n-        self._all_gather_stream = torch.cuda.Stream(priority=high_priority)\n-        self._all_gather_state = AllGatherStateHolder()\n-        for state in self._all_states:\n+        self._comm_ctx.init()\n+        for state in self._state_ctx.all_states:\n+            state._state_ctx = self._state_ctx\n+            state._comm_ctx = self._comm_ctx\n             if fsdp_param_group := state._fsdp_param_group:\n-                fsdp_param_group.default_stream = self._default_stream\n-                fsdp_param_group.all_gather_copy_in_stream = (\n-                    self._all_gather_copy_in_stream\n-                )\n-                fsdp_param_group.all_gather_stream = self._all_gather_stream\n-                fsdp_param_group.all_gather_state = self._all_gather_state\n+                fsdp_param_group.comm_ctx = self._comm_ctx\n \n     def _init_fqns(self) -> None:\n         \"\"\"Sets module and parameter FQN attributes for debugging.\"\"\"\n@@ -100,7 +117,7 @@ class FSDPState(_State):\n         root_module = self._module\n         param_to_fsdp_param: Dict[nn.Parameter, FSDPParam] = {}\n         module_to_fsdp_param_group: Dict[nn.Module, FSDPParamGroup] = {}\n-        for state in self._all_states:\n+        for state in self._state_ctx.all_states:\n             if fsdp_param_group := state._fsdp_param_group:\n                 for fsdp_param in fsdp_param_group.fsdp_params:\n                     param_to_fsdp_param[fsdp_param.sharded_param] = fsdp_param\n@@ -124,11 +141,55 @@ class FSDPState(_State):\n     def _post_forward(self, module: nn.Module, input: Any, output: Any) -> Any:\n         if self._fsdp_param_group:\n             output = self._fsdp_param_group.post_forward(module, input, output)\n+        output = self._register_pre_backward_hook(output)\n         self._training_state = TrainingState.IDLE\n-        if self._is_root and (all_gather_state := self._all_gather_state.pop()):\n-            self._all_gather_copy_in_stream.wait_event(all_gather_state.event)\n-            self._all_gather_stream.wait_event(all_gather_state.event)\n-            del all_gather_state  # free\n+        if self._state_ctx.iter_forward_root is self:\n+            if all_gather_state := self._comm_ctx.all_gather_state:\n+                # Free the last all-gather result if needed; refer to\n+                # [Note: Overlapping all-gather copy-in and all-gather]\n+                self._comm_ctx.all_gather_copy_in_stream.wait_event(\n+                    all_gather_state.event\n+                )\n+                self._comm_ctx.all_gather_stream.wait_event(all_gather_state.event)\n+                self._comm_ctx.all_gather_state = None  # free the all-gather result\n+            self._state_ctx.iter_forward_root = None\n+\n+    def _pre_backward(self, *unused: Any) -> None:\n+        self._training_state = TrainingState.PRE_BACKWARD\n+        self._register_root_post_backward_final_callback()\n+        if self._fsdp_param_group:\n+            self._fsdp_param_group.pre_backward(*unused)\n+\n+    def _root_post_backward_final_callback(self) -> None:\n+        with torch.profiler.record_function(\"FSDP::root_post_backward_callback\"):\n+            self._training_state = TrainingState.IDLE\n+            for state in self._state_ctx.all_states:\n+                state._training_state = TrainingState.IDLE\n+                if state._fsdp_param_group:\n+                    state._fsdp_param_group.finalize_backward()\n+            self._state_ctx.post_backward_final_callback_queued = False\n+            for handle in self._pre_backward_hook_handles:\n+                handle.remove()\n+            self._pre_backward_hook_handles.clear()\n+\n+    def _register_pre_backward_hook(self, output: Any) -> Any:\n+        if not torch.is_grad_enabled():\n+            return output\n+\n+        flat_outputs, _ = tree_flatten(output)\n+        tensors = tuple(t for t in flat_outputs if t.requires_grad)\n+        if tensors:\n+            handle = register_multi_grad_hook(tensors, self._pre_backward, mode=\"any\")\n+            self._pre_backward_hook_handles.append(handle)\n+        return output\n+\n+    def _register_root_post_backward_final_callback(self):\n+        if self._state_ctx.post_backward_final_callback_queued:\n+            return\n+        self._state_ctx.post_backward_final_callback_queued = True\n+        Variable._execution_engine.queue_callback(\n+            self._root_post_backward_final_callback\n+        )\n \n \n def _get_module_fsdp_state(module: nn.Module) -> Optional[FSDPState]:\n"
        },
        {
            "name": "common_fsdp.py",
            "path": "torch/testing/_internal/common_fsdp.py",
            "patches": [
                {
                    "old_start": 2,
                    "old_length": 6,
                    "new_start": 2,
                    "new_length": 7,
                    "hunk": "@@ -2,6 +2,7 @@\n \n # Owner(s): [\"oncall: distributed\"]\n \n+import contextlib\n import itertools\n import os\n import re\n"
                },
                {
                    "old_start": 846,
                    "old_length": 6,
                    "new_start": 847,
                    "new_length": 26,
                    "hunk": "@@ -846,6 +847,26 @@ class MLP(nn.Module):\n         return z\n \n \n+@contextlib.contextmanager\n+def patch_all_gather(new_all_gather_into_tensor: Callable):\n+    orig_all_gather = dist.all_gather_into_tensor\n+    dist.all_gather_into_tensor = new_all_gather_into_tensor\n+    try:\n+        yield\n+    finally:\n+        dist.all_gather_into_tensor = orig_all_gather\n+\n+\n+@contextlib.contextmanager\n+def patch_reduce_scatter(new_reduce_scatter_tensor: Callable):\n+    orig_reduce_scatter = dist.reduce_scatter_tensor\n+    dist.reduce_scatter_tensor = new_reduce_scatter_tensor\n+    try:\n+        yield\n+    finally:\n+        dist.reduce_scatter_tensor = orig_reduce_scatter\n+\n+\n def run_subtests(\n     cls_inst,\n     subtest_config: Dict[str, List[Any]],"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+import contextlib\n+@contextlib.contextmanager\n+def patch_all_gather(new_all_gather_into_tensor: Callable):\n+    orig_all_gather = dist.all_gather_into_tensor\n+    dist.all_gather_into_tensor = new_all_gather_into_tensor\n+    try:\n+        yield\n+    finally:\n+        dist.all_gather_into_tensor = orig_all_gather\n+\n+\n+@contextlib.contextmanager\n+def patch_reduce_scatter(new_reduce_scatter_tensor: Callable):\n+    orig_reduce_scatter = dist.reduce_scatter_tensor\n+    dist.reduce_scatter_tensor = new_reduce_scatter_tensor\n+    try:\n+        yield\n+    finally:\n+        dist.reduce_scatter_tensor = orig_reduce_scatter\n+\n+\n",
            "whole_hunk": "@@ -2,6 +2,7 @@\n \n # Owner(s): [\"oncall: distributed\"]\n \n+import contextlib\n import itertools\n import os\n import re\n@@ -846,6 +847,26 @@ class MLP(nn.Module):\n         return z\n \n \n+@contextlib.contextmanager\n+def patch_all_gather(new_all_gather_into_tensor: Callable):\n+    orig_all_gather = dist.all_gather_into_tensor\n+    dist.all_gather_into_tensor = new_all_gather_into_tensor\n+    try:\n+        yield\n+    finally:\n+        dist.all_gather_into_tensor = orig_all_gather\n+\n+\n+@contextlib.contextmanager\n+def patch_reduce_scatter(new_reduce_scatter_tensor: Callable):\n+    orig_reduce_scatter = dist.reduce_scatter_tensor\n+    dist.reduce_scatter_tensor = new_reduce_scatter_tensor\n+    try:\n+        yield\n+    finally:\n+        dist.reduce_scatter_tensor = orig_reduce_scatter\n+\n+\n def run_subtests(\n     cls_inst,\n     subtest_config: Dict[str, List[Any]],"
        }
    ]
},
{
    "Id": 402,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/bc34f02c38485ff6b5d1255012c1dd2b8c59810d",
    "date": "2023-11-29T00:14:02+00:00",
    "message": "[BE][Easy]: Apply RUF019: remove duplicate checks for dict access (#114478)\n\nApplies RUF019 nightly preview rule to the codebase\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114478\nApproved by: https://github.com/mikaylagawarecki",
    "label": "NO",
    "changes": [
        {
            "name": "test_optim.py",
            "path": "test/optim/test_optim.py",
            "patches": [
                {
                    "old_start": 851,
                    "old_length": 7,
                    "new_start": 851,
                    "new_length": 7,
                    "hunk": "@@ -851,7 +851,7 @@ class TestOptim(TestCase):\n             st_max_mem, mt_max_mem = max_mems\n             intermediate_size = nparams * param.nelement() * param.element_size()\n             nintermediates = 1  # we expect a budget of 1 intermediate most of the time\n-            if (('capturable' in kwargs_with_flags and kwargs_with_flags['capturable']) or\n+            if (kwargs_with_flags.get('capturable') or\n                     optimizer_constructor.__name__ in [\"Adadelta\", \"ASGD\"]):\n                 # with capturable in Adam(W), we have 2 extra intermediates for the bias_corrections\n                 # with Adadelta, we have 2 extra for (acc_delta + eps) and (square_avg + eps)\n"
                }
            ],
            "whole_deleted": "-            if (('capturable' in kwargs_with_flags and kwargs_with_flags['capturable']) or\n",
            "whole_added": "+            if (kwargs_with_flags.get('capturable') or\n",
            "whole_hunk": "@@ -851,7 +851,7 @@ class TestOptim(TestCase):\n             st_max_mem, mt_max_mem = max_mems\n             intermediate_size = nparams * param.nelement() * param.element_size()\n             nintermediates = 1  # we expect a budget of 1 intermediate most of the time\n-            if (('capturable' in kwargs_with_flags and kwargs_with_flags['capturable']) or\n+            if (kwargs_with_flags.get('capturable') or\n                     optimizer_constructor.__name__ in [\"Adadelta\", \"ASGD\"]):\n                 # with capturable in Adam(W), we have 2 extra intermediates for the bias_corrections\n                 # with Adadelta, we have 2 extra for (acc_delta + eps) and (square_avg + eps)\n"
        },
        {
            "name": "_script.py",
            "path": "torch/jit/_script.py",
            "patches": [
                {
                    "old_start": 454,
                    "old_length": 7,
                    "new_start": 454,
                    "new_length": 7,
                    "hunk": "@@ -454,7 +454,7 @@ if _enabled:\n             self.__dict__[\"_initializing\"] = False\n \n         def __getattr__(self, attr):\n-            if \"_initializing\" in self.__dict__ and self.__dict__[\"_initializing\"]:\n+            if self.__dict__.get(\"_initializing\"):\n                 return super().__getattr__(attr)  # type: ignore[misc]\n \n             if attr in self._props:\n"
                },
                {
                    "old_start": 463,
                    "old_length": 7,
                    "new_start": 463,
                    "new_length": 7,
                    "hunk": "@@ -463,7 +463,7 @@ if _enabled:\n             return getattr(self._c, attr)\n \n         def __setattr__(self, attr, value):\n-            if \"_initializing\" in self.__dict__ and self.__dict__[\"_initializing\"]:\n+            if self.__dict__.get(\"_initializing\"):\n                 return super().__setattr__(attr, value)\n \n             if attr in self._props:"
                }
            ],
            "whole_deleted": "-            if \"_initializing\" in self.__dict__ and self.__dict__[\"_initializing\"]:\n-            if \"_initializing\" in self.__dict__ and self.__dict__[\"_initializing\"]:\n",
            "whole_added": "+            if self.__dict__.get(\"_initializing\"):\n+            if self.__dict__.get(\"_initializing\"):\n",
            "whole_hunk": "@@ -454,7 +454,7 @@ if _enabled:\n             self.__dict__[\"_initializing\"] = False\n \n         def __getattr__(self, attr):\n-            if \"_initializing\" in self.__dict__ and self.__dict__[\"_initializing\"]:\n+            if self.__dict__.get(\"_initializing\"):\n                 return super().__getattr__(attr)  # type: ignore[misc]\n \n             if attr in self._props:\n@@ -463,7 +463,7 @@ if _enabled:\n             return getattr(self._c, attr)\n \n         def __setattr__(self, attr, value):\n-            if \"_initializing\" in self.__dict__ and self.__dict__[\"_initializing\"]:\n+            if self.__dict__.get(\"_initializing\"):\n                 return super().__setattr__(attr, value)\n \n             if attr in self._props:"
        }
    ]
},
{
    "Id": 69,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/1bb1e3463c95c75a8555fd4c796dad8a2762d667",
    "date": "2024-06-25T04:19:44+00:00",
    "message": "Fix allowlisting of builtins for weights_only unpickler (#129244)\n\nSince we use [`DEFAULT_PROTOCOL=2`](https://github.com/pytorch/pytorch/blob/main/torch/serialization.py#L62), some functions/classes that were renamed from python 2-->3 will be pickled with their python2 name. This PR ensures that when a mod `GLOBAL <python2_mod>.<python2_name> ` is encountered, [following the strategy used by pickle](https://github.com/python/cpython/blob/main/Lib/pickle.py#L1590C13-L1593C63) it is properly mapped to `<python3_mod>.<python3_name>`.\n\nThis fix ensures that `add_safe_globals` works properly for such functions/classes (i.e. users will allowlist the python3 func and the weights_only unpickler will do the appropriate translation when checking whether a class was allowlisted).\n\nAn example is as follows:\n`__builtin__` was named to `builtins`, see the [release notes for Python 3.0](https://docs.python.org/3/whatsnew/3.0.html)\n\n> Renamed module `__builtin__` to [`builtins`](https://docs.python.org/3/library/builtins.html#module-builtins) (removing the underscores, adding an \u2018s\u2019). The __builtins__ variable found in most global namespaces is unchanged. To modify a builtin, you should use [builtins](https://docs.python.org/3/library/builtins.html#module-builtins), not `__builtins__`!\n\nHowever, since we use [`DEFAULT_PROTOCOL=2`](https://github.com/pytorch/pytorch/blob/main/torch/serialization.py#L62), builtins will be pickled with their module string as `__builtin__`.\n\n```python\n>>> import pickle\n>>> import pickletools\n>>> print.__module__\n'builtins'\n>>> with open('print.pkl', 'wb') as f:\n>>>      pickle.dump(print, f, protocol=2) # 2 because this is the default protocol used by pytorch\n>>> with open('print.pkl', 'rb') as f:\n>>>     pickletools.dis(f)\n0: \\x80 PROTO      2\n2: c    GLOBAL     '__builtin__ print' # pickle saves the module string as __builtin__ !!! :(\n21: q    BINPUT     0\n23: .    STOP\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/129244\nApproved by: https://github.com/albanD",
    "label": "YES",
    "changes": [
        {
            "name": "test_serialization.py",
            "path": "test/test_serialization.py",
            "patches": [
                {
                    "old_start": 1040,
                    "old_length": 8,
                    "new_start": 1040,
                    "new_length": 14,
                    "hunk": "@@ -1040,8 +1040,14 @@ class TestSerialization(TestCase, SerializationMixin):\n             self.assertIsNone(torch.load(f, weights_only=False))\n             f.seek(0)\n             # Safe load should assert\n-            with self.assertRaisesRegex(pickle.UnpicklingError, \"Unsupported global: GLOBAL __builtin__.print\"):\n+            with self.assertRaisesRegex(pickle.UnpicklingError, \"Unsupported global: GLOBAL builtins.print\"):\n                 torch.load(f, weights_only=True)\n+            try:\n+                torch.serialization.add_safe_globals([print])\n+                f.seek(0)\n+                torch.load(f, weights_only=True)\n+            finally:\n+                torch.serialization.clear_safe_globals()\n \n     @parametrize('weights_only', (False, True))\n     def test_serialization_math_bits(self, weights_only):\n"
                }
            ],
            "whole_deleted": "-            with self.assertRaisesRegex(pickle.UnpicklingError, \"Unsupported global: GLOBAL __builtin__.print\"):\n",
            "whole_added": "+            with self.assertRaisesRegex(pickle.UnpicklingError, \"Unsupported global: GLOBAL builtins.print\"):\n+            try:\n+                torch.serialization.add_safe_globals([print])\n+                f.seek(0)\n+                torch.load(f, weights_only=True)\n+            finally:\n+                torch.serialization.clear_safe_globals()\n",
            "whole_hunk": "@@ -1040,8 +1040,14 @@ class TestSerialization(TestCase, SerializationMixin):\n             self.assertIsNone(torch.load(f, weights_only=False))\n             f.seek(0)\n             # Safe load should assert\n-            with self.assertRaisesRegex(pickle.UnpicklingError, \"Unsupported global: GLOBAL __builtin__.print\"):\n+            with self.assertRaisesRegex(pickle.UnpicklingError, \"Unsupported global: GLOBAL builtins.print\"):\n                 torch.load(f, weights_only=True)\n+            try:\n+                torch.serialization.add_safe_globals([print])\n+                f.seek(0)\n+                torch.load(f, weights_only=True)\n+            finally:\n+                torch.serialization.clear_safe_globals()\n \n     @parametrize('weights_only', (False, True))\n     def test_serialization_math_bits(self, weights_only):\n"
        },
        {
            "name": "_weights_only_unpickler.py",
            "path": "torch/_weights_only_unpickler.py",
            "patches": [
                {
                    "old_start": 23,
                    "old_length": 6,
                    "new_start": 23,
                    "new_length": 7,
                    "hunk": "@@ -23,6 +23,7 @@\n # weights = torch.load(buf, weights_only = True)\n \n import functools as _functools\n+import warnings\n from collections import Counter, OrderedDict\n from pickle import (\n     APPEND,\n"
                },
                {
                    "old_start": 67,
                    "old_length": 6,
                    "new_start": 68,
                    "new_length": 16,
                    "hunk": "@@ -67,6 +68,16 @@ from struct import unpack\n from sys import maxsize\n from typing import Any, Dict, List\n \n+try:\n+    # We rely on this module in private cPython which provides dicts of\n+    # modules/functions that had their names changed from Python 2 to 3\n+    has_compat_pickle = True\n+    from _compat_pickle import IMPORT_MAPPING, NAME_MAPPING\n+except ImportError:\n+    # To prevent warning on import torch, we warn in the Unpickler.load below\n+    has_compat_pickle = False\n+    IMPORT_MAPPING, NAME_MAPPING = dict(), dict()\n+\n import torch\n \n _marked_safe_globals_list: List[Any] = []\n"
                },
                {
                    "old_start": 97,
                    "old_length": 7,
                    "new_start": 108,
                    "new_length": 8,
                    "hunk": "@@ -97,7 +108,8 @@ def _clear_safe_globals():\n def _get_user_allowed_globals():\n     rc: Dict[str, Any] = {}\n     for f in _marked_safe_globals_list:\n-        rc[f\"{f.__module__}.{f.__name__}\"] = f\n+        module, name = f.__module__, f.__name__\n+        rc[f\"{module}.{name}\"] = f\n     return rc\n \n \n"
                },
                {
                    "old_start": 170,
                    "old_length": 12,
                    "new_start": 182,
                    "new_length": 20,
                    "hunk": "@@ -170,12 +182,20 @@ class Unpickler:\n         self.readline = file.readline\n         self.read = file.read\n         self.memo: Dict[int, Any] = {}\n+        self.proto: int = -1\n \n     def load(self):\n         \"\"\"Read a pickled object representation from the open file.\n \n         Return the reconstituted object hierarchy specified in the file.\n         \"\"\"\n+        if not has_compat_pickle:\n+            warnings.warn(\n+                \"Could not import IMPORT_MAPPING and NAME_MAPPING from _compat_pickle. \"\n+                \"If the default `pickle_protocol` was used at `torch.save` time, any functions or \"\n+                \"classes that are in these maps might not behave correctly if allowlisted via \"\n+                \"`torch.serialization.add_safe_globals()`.\"\n+            )\n         self.metastack = []\n         self.stack: List[Any] = []\n         self.append = self.stack.append\n"
                },
                {
                    "old_start": 190,
                    "old_length": 6,
                    "new_start": 210,
                    "new_length": 13,
                    "hunk": "@@ -190,6 +210,13 @@ class Unpickler:\n             if key[0] == GLOBAL[0]:\n                 module = readline()[:-1].decode(\"utf-8\")\n                 name = readline()[:-1].decode(\"utf-8\")\n+                # Patch since torch.save default protocol is 2\n+                # users will be running this code in python > 3\n+                if self.proto == 2 and has_compat_pickle:\n+                    if (module, name) in NAME_MAPPING:\n+                        module, name = NAME_MAPPING[(module, name)]\n+                    elif module in IMPORT_MAPPING:\n+                        module = IMPORT_MAPPING[module]\n                 full_path = f\"{module}.{name}\"\n                 if full_path in _get_allowed_globals():\n                     self.append(_get_allowed_globals()[full_path])\n"
                },
                {
                    "old_start": 334,
                    "old_length": 8,
                    "new_start": 361,
                    "new_length": 14,
                    "hunk": "@@ -334,8 +361,14 @@ class Unpickler:\n                 self.append(decode_long(data))\n             # First and last deserializer ops\n             elif key[0] == PROTO[0]:\n-                # Read and ignore proto version\n-                read(1)[0]\n+                self.proto = read(1)[0]\n+                if self.proto != 2:\n+                    warnings.warn(\n+                        f\"Detected pickle protocol {self.proto} in the checkpoint, which was \"\n+                        \"not the default pickle protocol used by `torch.load` (2). The weights_only \"\n+                        \"Unpickler might not support all instructions implemented by this protocol, \"\n+                        \"please file an issue for adding support if you encounter this.\"\n+                    )\n             elif key[0] == STOP[0]:\n                 rc = self.stack.pop()\n                 return rc"
                }
            ],
            "whole_deleted": "-        rc[f\"{f.__module__}.{f.__name__}\"] = f\n-                # Read and ignore proto version\n-                read(1)[0]\n",
            "whole_added": "+import warnings\n+try:\n+    # We rely on this module in private cPython which provides dicts of\n+    # modules/functions that had their names changed from Python 2 to 3\n+    has_compat_pickle = True\n+    from _compat_pickle import IMPORT_MAPPING, NAME_MAPPING\n+except ImportError:\n+    # To prevent warning on import torch, we warn in the Unpickler.load below\n+    has_compat_pickle = False\n+    IMPORT_MAPPING, NAME_MAPPING = dict(), dict()\n+\n+        module, name = f.__module__, f.__name__\n+        rc[f\"{module}.{name}\"] = f\n+        self.proto: int = -1\n+        if not has_compat_pickle:\n+            warnings.warn(\n+                \"Could not import IMPORT_MAPPING and NAME_MAPPING from _compat_pickle. \"\n+                \"If the default `pickle_protocol` was used at `torch.save` time, any functions or \"\n+                \"classes that are in these maps might not behave correctly if allowlisted via \"\n+                \"`torch.serialization.add_safe_globals()`.\"\n+            )\n+                # Patch since torch.save default protocol is 2\n+                # users will be running this code in python > 3\n+                if self.proto == 2 and has_compat_pickle:\n+                    if (module, name) in NAME_MAPPING:\n+                        module, name = NAME_MAPPING[(module, name)]\n+                    elif module in IMPORT_MAPPING:\n+                        module = IMPORT_MAPPING[module]\n+                self.proto = read(1)[0]\n+                if self.proto != 2:\n+                    warnings.warn(\n+                        f\"Detected pickle protocol {self.proto} in the checkpoint, which was \"\n+                        \"not the default pickle protocol used by `torch.load` (2). The weights_only \"\n+                        \"Unpickler might not support all instructions implemented by this protocol, \"\n+                        \"please file an issue for adding support if you encounter this.\"\n+                    )\n",
            "whole_hunk": "@@ -23,6 +23,7 @@\n # weights = torch.load(buf, weights_only = True)\n \n import functools as _functools\n+import warnings\n from collections import Counter, OrderedDict\n from pickle import (\n     APPEND,\n@@ -67,6 +68,16 @@ from struct import unpack\n from sys import maxsize\n from typing import Any, Dict, List\n \n+try:\n+    # We rely on this module in private cPython which provides dicts of\n+    # modules/functions that had their names changed from Python 2 to 3\n+    has_compat_pickle = True\n+    from _compat_pickle import IMPORT_MAPPING, NAME_MAPPING\n+except ImportError:\n+    # To prevent warning on import torch, we warn in the Unpickler.load below\n+    has_compat_pickle = False\n+    IMPORT_MAPPING, NAME_MAPPING = dict(), dict()\n+\n import torch\n \n _marked_safe_globals_list: List[Any] = []\n@@ -97,7 +108,8 @@ def _clear_safe_globals():\n def _get_user_allowed_globals():\n     rc: Dict[str, Any] = {}\n     for f in _marked_safe_globals_list:\n-        rc[f\"{f.__module__}.{f.__name__}\"] = f\n+        module, name = f.__module__, f.__name__\n+        rc[f\"{module}.{name}\"] = f\n     return rc\n \n \n@@ -170,12 +182,20 @@ class Unpickler:\n         self.readline = file.readline\n         self.read = file.read\n         self.memo: Dict[int, Any] = {}\n+        self.proto: int = -1\n \n     def load(self):\n         \"\"\"Read a pickled object representation from the open file.\n \n         Return the reconstituted object hierarchy specified in the file.\n         \"\"\"\n+        if not has_compat_pickle:\n+            warnings.warn(\n+                \"Could not import IMPORT_MAPPING and NAME_MAPPING from _compat_pickle. \"\n+                \"If the default `pickle_protocol` was used at `torch.save` time, any functions or \"\n+                \"classes that are in these maps might not behave correctly if allowlisted via \"\n+                \"`torch.serialization.add_safe_globals()`.\"\n+            )\n         self.metastack = []\n         self.stack: List[Any] = []\n         self.append = self.stack.append\n@@ -190,6 +210,13 @@ class Unpickler:\n             if key[0] == GLOBAL[0]:\n                 module = readline()[:-1].decode(\"utf-8\")\n                 name = readline()[:-1].decode(\"utf-8\")\n+                # Patch since torch.save default protocol is 2\n+                # users will be running this code in python > 3\n+                if self.proto == 2 and has_compat_pickle:\n+                    if (module, name) in NAME_MAPPING:\n+                        module, name = NAME_MAPPING[(module, name)]\n+                    elif module in IMPORT_MAPPING:\n+                        module = IMPORT_MAPPING[module]\n                 full_path = f\"{module}.{name}\"\n                 if full_path in _get_allowed_globals():\n                     self.append(_get_allowed_globals()[full_path])\n@@ -334,8 +361,14 @@ class Unpickler:\n                 self.append(decode_long(data))\n             # First and last deserializer ops\n             elif key[0] == PROTO[0]:\n-                # Read and ignore proto version\n-                read(1)[0]\n+                self.proto = read(1)[0]\n+                if self.proto != 2:\n+                    warnings.warn(\n+                        f\"Detected pickle protocol {self.proto} in the checkpoint, which was \"\n+                        \"not the default pickle protocol used by `torch.load` (2). The weights_only \"\n+                        \"Unpickler might not support all instructions implemented by this protocol, \"\n+                        \"please file an issue for adding support if you encounter this.\"\n+                    )\n             elif key[0] == STOP[0]:\n                 rc = self.stack.pop()\n                 return rc"
        }
    ]
},
{
    "Id": 261,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/69cedc16c593e777cd36f064dd862c78d6291c0b",
    "date": "2024-03-06T21:55:34+00:00",
    "message": "Add padding dimension checks and tests (#121298)\n\nFixes #121093\n\nPreviously, calling the following functions with invalid padding dimensions would cause a segmentation fault:\n```\ntorch._C._nn.replication_pad1d, torch._C._nn.replication_pad3d, torch._C._nn.replication_pad3d\n```\n\nTo fix, added condition checking to raise a runtime error with a debug message instead, specifying the correct dimensions necessary.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/121298\nApproved by: https://github.com/mikaylagawarecki",
    "label": "YES",
    "changes": [
        {
            "name": "ReplicationPadding.cpp",
            "path": "aten/src/ATen/native/ReplicationPadding.cpp",
            "patches": [
                {
                    "old_start": 25,
                    "old_length": 6,
                    "new_start": 25,
                    "new_length": 7,
                    "hunk": "@@ -25,6 +25,7 @@ namespace at::meta {\n TORCH_META_FUNC(replication_pad1d) (\n   const Tensor& input, IntArrayRef paddingSize  // no out argument!\n ) {\n+  TORCH_CHECK(paddingSize.size() == 2, \"padding size is expected to be 2\");\n \n   int64_t dimw = 1;\n   int64_t dimslices = 0;\n"
                },
                {
                    "old_start": 85,
                    "old_length": 6,
                    "new_start": 86,
                    "new_length": 7,
                    "hunk": "@@ -85,6 +86,7 @@ TORCH_META_FUNC(replication_pad1d_backward) (\n TORCH_META_FUNC(replication_pad2d) (\n   const Tensor& input, IntArrayRef paddingSize\n ) {\n+  TORCH_CHECK(paddingSize.size() == 4, \"padding size is expected to be 4\");\n   int64_t pad_l = paddingSize[0];\n   int64_t pad_r = paddingSize[1];\n   int64_t pad_t = paddingSize[2];\n"
                },
                {
                    "old_start": 124,
                    "old_length": 6,
                    "new_start": 126,
                    "new_length": 7,
                    "hunk": "@@ -124,6 +126,7 @@ TORCH_META_FUNC(replication_pad2d) (\n TORCH_META_FUNC(replication_pad3d) (\n   const Tensor& input, IntArrayRef paddingSize\n ) {\n+  TORCH_CHECK(paddingSize.size() == 6, \"padding size is expected to be 6\");\n   int64_t pleft = paddingSize[0];\n   int64_t pright = paddingSize[1];\n   int64_t ptop = paddingSize[2];\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  TORCH_CHECK(paddingSize.size() == 2, \"padding size is expected to be 2\");\n+  TORCH_CHECK(paddingSize.size() == 4, \"padding size is expected to be 4\");\n+  TORCH_CHECK(paddingSize.size() == 6, \"padding size is expected to be 6\");\n",
            "whole_hunk": "@@ -25,6 +25,7 @@ namespace at::meta {\n TORCH_META_FUNC(replication_pad1d) (\n   const Tensor& input, IntArrayRef paddingSize  // no out argument!\n ) {\n+  TORCH_CHECK(paddingSize.size() == 2, \"padding size is expected to be 2\");\n \n   int64_t dimw = 1;\n   int64_t dimslices = 0;\n@@ -85,6 +86,7 @@ TORCH_META_FUNC(replication_pad1d_backward) (\n TORCH_META_FUNC(replication_pad2d) (\n   const Tensor& input, IntArrayRef paddingSize\n ) {\n+  TORCH_CHECK(paddingSize.size() == 4, \"padding size is expected to be 4\");\n   int64_t pad_l = paddingSize[0];\n   int64_t pad_r = paddingSize[1];\n   int64_t pad_t = paddingSize[2];\n@@ -124,6 +126,7 @@ TORCH_META_FUNC(replication_pad2d) (\n TORCH_META_FUNC(replication_pad3d) (\n   const Tensor& input, IntArrayRef paddingSize\n ) {\n+  TORCH_CHECK(paddingSize.size() == 6, \"padding size is expected to be 6\");\n   int64_t pleft = paddingSize[0];\n   int64_t pright = paddingSize[1];\n   int64_t ptop = paddingSize[2];\n"
        },
        {
            "name": "test_nn.py",
            "path": "test/test_nn.py",
            "patches": [
                {
                    "old_start": 8481,
                    "old_length": 6,
                    "new_start": 8481,
                    "new_length": 15,
                    "hunk": "@@ -8481,6 +8481,15 @@ class TestNNDeviceType(NNTestCase):\n             inp = torch.randn(3, 0, 10, 10, 10, device=device, dtype=dtype)\n             mod(inp)\n \n+        with self.assertRaisesRegex(RuntimeError, 'padding size is expected to be 2'):\n+            torch._C._nn.replication_pad1d(torch.randn([2]), padding=[])\n+\n+        with self.assertRaisesRegex(RuntimeError, 'padding size is expected to be 4'):\n+            torch._C._nn.replication_pad2d(torch.randn([2]), padding=[])\n+\n+        with self.assertRaisesRegex(RuntimeError, 'padding size is expected to be 6'):\n+            torch._C._nn.replication_pad3d(torch.randn([2]), padding=[])\n+\n     def test_ReplicationPad1d_large(self, device):\n         shapes = ([2, 65736, 4], [65736, 2, 4])\n         pl, pr = 3, 4"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        with self.assertRaisesRegex(RuntimeError, 'padding size is expected to be 2'):\n+            torch._C._nn.replication_pad1d(torch.randn([2]), padding=[])\n+\n+        with self.assertRaisesRegex(RuntimeError, 'padding size is expected to be 4'):\n+            torch._C._nn.replication_pad2d(torch.randn([2]), padding=[])\n+\n+        with self.assertRaisesRegex(RuntimeError, 'padding size is expected to be 6'):\n+            torch._C._nn.replication_pad3d(torch.randn([2]), padding=[])\n+\n",
            "whole_hunk": "@@ -8481,6 +8481,15 @@ class TestNNDeviceType(NNTestCase):\n             inp = torch.randn(3, 0, 10, 10, 10, device=device, dtype=dtype)\n             mod(inp)\n \n+        with self.assertRaisesRegex(RuntimeError, 'padding size is expected to be 2'):\n+            torch._C._nn.replication_pad1d(torch.randn([2]), padding=[])\n+\n+        with self.assertRaisesRegex(RuntimeError, 'padding size is expected to be 4'):\n+            torch._C._nn.replication_pad2d(torch.randn([2]), padding=[])\n+\n+        with self.assertRaisesRegex(RuntimeError, 'padding size is expected to be 6'):\n+            torch._C._nn.replication_pad3d(torch.randn([2]), padding=[])\n+\n     def test_ReplicationPad1d_large(self, device):\n         shapes = ([2, 65736, 4], [65736, 2, 4])\n         pl, pr = 3, 4"
        }
    ]
},
{
    "Id": 516,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/4ee179c9528c8c6aae17a01f2b0d7e8235219219",
    "date": "2023-09-16T00:07:19+00:00",
    "message": "Fix `ConstantVariable` init method if NumPy is missing (#109388)\n\nBy adding `np is not None` check before `isinstance(value, np.number)`\n\nPartially addresses https://github.com/pytorch/pytorch/issues/109387\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109388\nApproved by: https://github.com/ezyang",
    "label": "YES",
    "changes": [
        {
            "name": "constant.py",
            "path": "torch/_dynamo/variables/constant.py",
            "patches": [
                {
                    "old_start": 33,
                    "old_length": 7,
                    "new_start": 33,
                    "new_length": 7,
                    "hunk": "@@ -33,7 +33,7 @@ class ConstantVariable(VariableTracker):\n             for disallowed_type, reason in _type_to_assert_reason.items():\n                 assert not isinstance(value, disallowed_type), reason\n \n-        if isinstance(value, np.number):\n+        if np is not None and isinstance(value, np.number):\n             self.value = value.item()\n         else:\n             self.value = value"
                }
            ],
            "whole_deleted": "-        if isinstance(value, np.number):\n",
            "whole_added": "+        if np is not None and isinstance(value, np.number):\n",
            "whole_hunk": "@@ -33,7 +33,7 @@ class ConstantVariable(VariableTracker):\n             for disallowed_type, reason in _type_to_assert_reason.items():\n                 assert not isinstance(value, disallowed_type), reason\n \n-        if isinstance(value, np.number):\n+        if np is not None and isinstance(value, np.number):\n             self.value = value.item()\n         else:\n             self.value = value"
        }
    ]
},
{
    "Id": 436,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/addb8e29cd842e1a290cb0b55662ee0423ab2498",
    "date": "2023-11-09T06:12:13+00:00",
    "message": "Enable 2d + AC torch.compile (#112536)\n\nThis PR enables AC + torch.compile to work with FSDP + TP, the fix to\nhigh order op path is that we need to check both tensor and tensor\nsubclass bases to make sourceless builder\n\nNOTE: selective AC + 2D is still not working, need to fix this\nseparately\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/112536\nApproved by: https://github.com/yf225",
    "label": "NO",
    "changes": [
        {
            "name": "test_dtensor_compile.py",
            "path": "test/distributed/_tensor/test_dtensor_compile.py",
            "patches": [
                {
                    "old_start": 15,
                    "old_length": 6,
                    "new_start": 15,
                    "new_length": 10,
                    "hunk": "@@ -15,6 +15,10 @@ from torch.distributed._tensor import (\n     Replicate,\n     Shard,\n )\n+from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n+    checkpoint_wrapper,\n+    CheckpointImpl,\n+)\n from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n from torch.distributed.tensor.parallel import (\n     ColwiseParallel,\n"
                },
                {
                    "old_start": 34,
                    "old_length": 6,
                    "new_start": 38,
                    "new_length": 7,
                    "hunk": "@@ -34,6 +38,7 @@ from torch.testing._internal.distributed._tensor.common_dtensor import (\n     with_comms,\n )\n from torch.testing._internal.distributed.fake_pg import FakeStore\n+from torch.utils.checkpoint import checkpoint\n \n \n class SimpleModel(nn.Module):\n"
                },
                {
                    "old_start": 280,
                    "old_length": 12,
                    "new_start": 285,
                    "new_length": 61,
                    "hunk": "@@ -280,12 +285,61 @@ class TestDTensorCompileE2E(DTensorTestBase):\n \n         self.assertEqual(out, compiled_output)\n \n+    @with_comms\n+    @skip_if_lt_x_gpu(4)\n+    def test_2d_fsdp_tp_ac_compile(self):\n+        dp_degree = 2\n+        tp_degree = self.world_size // dp_degree\n+        model = SimpleModel(self.device_type)\n+        model_copy = copy.deepcopy(model)\n+\n+        # 2-D mesh is [dp, tp]\n+        mesh_2d = init_device_mesh(\n+            \"cuda\", mesh_shape=(dp_degree, tp_degree), mesh_dim_names=(\"dp\", \"tp\")\n+        )\n+\n+        inp = torch.rand(20, 10, device=self.device_type)\n+        parallelize_plan = {\n+            \"mlp_0.net1\": ColwiseParallel(),\n+            \"mlp_0.net2\": RowwiseParallel(),\n+            \"mlp_1.net1\": ColwiseParallel(),\n+            \"mlp_1.net2\": RowwiseParallel(),\n+        }\n+        tp_model = parallelize_module(model, mesh_2d[\"tp\"], parallelize_plan)\n+        tp_model = checkpoint_wrapper(\n+            tp_model,\n+            checkpoint_impl=CheckpointImpl.NO_REENTRANT,\n+            checkpoint_fn=checkpoint,\n+            use_reentrant=False,\n+        )\n+        eager_2d = FSDP(tp_model, device_mesh=mesh_2d[\"dp\"], use_orig_params=True)\n+\n+        tp_model2 = parallelize_module(model_copy, mesh_2d[\"tp\"], parallelize_plan)\n+        fsdp_2d = FSDP(\n+            tp_model2,\n+            device_mesh=mesh_2d[\"dp\"],\n+            use_orig_params=True,\n+        )\n+        # TODO: once aot autograd support is ready we can just use default backend\n+        compiled_2d = torch.compile(fsdp_2d, backend=\"aot_eager\")\n+\n+        # forward pass\n+        out = eager_2d(inp)\n+        compiled_output = compiled_2d(inp)\n+        self.assertEqual(out, compiled_output)\n+\n+        # backward pass\n+        out.sum().backward()\n+        compiled_output.sum().backward()\n+\n+        # compare the gradients:\n+        for n, p in zip(fsdp_2d.parameters(), compiled_2d.parameters()):\n+            self.assertEqual(n.grad, p.grad)\n+\n     @with_comms\n     @skip_if_lt_x_gpu(4)\n     def test_compile_dtensor_redistribute_backward(self):\n         mesh = DeviceMesh(device_type=\"cuda\", mesh=torch.arange(self.world_size))\n-        #            device_type=\"cuda\",\n-        #            mesh=torch.arange(0, self.world_size).view(data_parallel_size, -1),\n \n         def fn(x, y):\n             dt = DTensor.from_local(x.reshape(2, 4), mesh, [Shard(0)], run_check=False)\n"
                }
            ],
            "whole_deleted": "-        #            device_type=\"cuda\",\n-        #            mesh=torch.arange(0, self.world_size).view(data_parallel_size, -1),\n",
            "whole_added": "+from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n+    checkpoint_wrapper,\n+    CheckpointImpl,\n+)\n+from torch.utils.checkpoint import checkpoint\n+    @with_comms\n+    @skip_if_lt_x_gpu(4)\n+    def test_2d_fsdp_tp_ac_compile(self):\n+        dp_degree = 2\n+        tp_degree = self.world_size // dp_degree\n+        model = SimpleModel(self.device_type)\n+        model_copy = copy.deepcopy(model)\n+\n+        # 2-D mesh is [dp, tp]\n+        mesh_2d = init_device_mesh(\n+            \"cuda\", mesh_shape=(dp_degree, tp_degree), mesh_dim_names=(\"dp\", \"tp\")\n+        )\n+\n+        inp = torch.rand(20, 10, device=self.device_type)\n+        parallelize_plan = {\n+            \"mlp_0.net1\": ColwiseParallel(),\n+            \"mlp_0.net2\": RowwiseParallel(),\n+            \"mlp_1.net1\": ColwiseParallel(),\n+            \"mlp_1.net2\": RowwiseParallel(),\n+        }\n+        tp_model = parallelize_module(model, mesh_2d[\"tp\"], parallelize_plan)\n+        tp_model = checkpoint_wrapper(\n+            tp_model,\n+            checkpoint_impl=CheckpointImpl.NO_REENTRANT,\n+            checkpoint_fn=checkpoint,\n+            use_reentrant=False,\n+        )\n+        eager_2d = FSDP(tp_model, device_mesh=mesh_2d[\"dp\"], use_orig_params=True)\n+\n+        tp_model2 = parallelize_module(model_copy, mesh_2d[\"tp\"], parallelize_plan)\n+        fsdp_2d = FSDP(\n+            tp_model2,\n+            device_mesh=mesh_2d[\"dp\"],\n+            use_orig_params=True,\n+        )\n+        # TODO: once aot autograd support is ready we can just use default backend\n+        compiled_2d = torch.compile(fsdp_2d, backend=\"aot_eager\")\n+\n+        # forward pass\n+        out = eager_2d(inp)\n+        compiled_output = compiled_2d(inp)\n+        self.assertEqual(out, compiled_output)\n+\n+        # backward pass\n+        out.sum().backward()\n+        compiled_output.sum().backward()\n+\n+        # compare the gradients:\n+        for n, p in zip(fsdp_2d.parameters(), compiled_2d.parameters()):\n+            self.assertEqual(n.grad, p.grad)\n+\n",
            "whole_hunk": "@@ -15,6 +15,10 @@ from torch.distributed._tensor import (\n     Replicate,\n     Shard,\n )\n+from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n+    checkpoint_wrapper,\n+    CheckpointImpl,\n+)\n from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n from torch.distributed.tensor.parallel import (\n     ColwiseParallel,\n@@ -34,6 +38,7 @@ from torch.testing._internal.distributed._tensor.common_dtensor import (\n     with_comms,\n )\n from torch.testing._internal.distributed.fake_pg import FakeStore\n+from torch.utils.checkpoint import checkpoint\n \n \n class SimpleModel(nn.Module):\n@@ -280,12 +285,61 @@ class TestDTensorCompileE2E(DTensorTestBase):\n \n         self.assertEqual(out, compiled_output)\n \n+    @with_comms\n+    @skip_if_lt_x_gpu(4)\n+    def test_2d_fsdp_tp_ac_compile(self):\n+        dp_degree = 2\n+        tp_degree = self.world_size // dp_degree\n+        model = SimpleModel(self.device_type)\n+        model_copy = copy.deepcopy(model)\n+\n+        # 2-D mesh is [dp, tp]\n+        mesh_2d = init_device_mesh(\n+            \"cuda\", mesh_shape=(dp_degree, tp_degree), mesh_dim_names=(\"dp\", \"tp\")\n+        )\n+\n+        inp = torch.rand(20, 10, device=self.device_type)\n+        parallelize_plan = {\n+            \"mlp_0.net1\": ColwiseParallel(),\n+            \"mlp_0.net2\": RowwiseParallel(),\n+            \"mlp_1.net1\": ColwiseParallel(),\n+            \"mlp_1.net2\": RowwiseParallel(),\n+        }\n+        tp_model = parallelize_module(model, mesh_2d[\"tp\"], parallelize_plan)\n+        tp_model = checkpoint_wrapper(\n+            tp_model,\n+            checkpoint_impl=CheckpointImpl.NO_REENTRANT,\n+            checkpoint_fn=checkpoint,\n+            use_reentrant=False,\n+        )\n+        eager_2d = FSDP(tp_model, device_mesh=mesh_2d[\"dp\"], use_orig_params=True)\n+\n+        tp_model2 = parallelize_module(model_copy, mesh_2d[\"tp\"], parallelize_plan)\n+        fsdp_2d = FSDP(\n+            tp_model2,\n+            device_mesh=mesh_2d[\"dp\"],\n+            use_orig_params=True,\n+        )\n+        # TODO: once aot autograd support is ready we can just use default backend\n+        compiled_2d = torch.compile(fsdp_2d, backend=\"aot_eager\")\n+\n+        # forward pass\n+        out = eager_2d(inp)\n+        compiled_output = compiled_2d(inp)\n+        self.assertEqual(out, compiled_output)\n+\n+        # backward pass\n+        out.sum().backward()\n+        compiled_output.sum().backward()\n+\n+        # compare the gradients:\n+        for n, p in zip(fsdp_2d.parameters(), compiled_2d.parameters()):\n+            self.assertEqual(n.grad, p.grad)\n+\n     @with_comms\n     @skip_if_lt_x_gpu(4)\n     def test_compile_dtensor_redistribute_backward(self):\n         mesh = DeviceMesh(device_type=\"cuda\", mesh=torch.arange(self.world_size))\n-        #            device_type=\"cuda\",\n-        #            mesh=torch.arange(0, self.world_size).view(data_parallel_size, -1),\n \n         def fn(x, y):\n             dt = DTensor.from_local(x.reshape(2, 4), mesh, [Shard(0)], run_check=False)\n"
        },
        {
            "name": "test_misc.py",
            "path": "test/dynamo/test_misc.py",
            "patches": [
                {
                    "old_start": 7156,
                    "old_length": 6,
                    "new_start": 7156,
                    "new_length": 27,
                    "hunk": "@@ -7156,6 +7156,27 @@ def ___make_guard_fn():\n         self.assertEqual(counter.frame_count, 1)\n         self.assertEqual(counter.op_count, 18)\n \n+    def test_tracing_py_tree_tensor_subclass(self):\n+        import torch.utils._pytree as pytree\n+        from torch.testing._internal.two_tensor import TwoTensor\n+        from torch.utils.checkpoint import checkpoint\n+\n+        def fn(xs):\n+            nested_xs = [[xs]]\n+            flat_xs, spec = pytree.tree_flatten(xs)\n+            return flat_xs[0].clone()\n+\n+        # use checkpoint to trigger a \"sourceless\" tensor subclass\n+        def checkpoint_fn(xs):\n+            return checkpoint(fn, xs)\n+\n+        xs = TwoTensor(torch.ones(2, 2), torch.ones(2, 2))\n+\n+        counter = CompileCounter()\n+        torch._dynamo.optimize(counter, nopython=True)(checkpoint_fn)(xs)\n+        self.assertEqual(counter.frame_count, 1)\n+        self.assertEqual(counter.op_count, 2)\n+\n     def test_tracing_tree_map_only(self):\n         import torch.utils._pytree as pytree\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    def test_tracing_py_tree_tensor_subclass(self):\n+        import torch.utils._pytree as pytree\n+        from torch.testing._internal.two_tensor import TwoTensor\n+        from torch.utils.checkpoint import checkpoint\n+\n+        def fn(xs):\n+            nested_xs = [[xs]]\n+            flat_xs, spec = pytree.tree_flatten(xs)\n+            return flat_xs[0].clone()\n+\n+        # use checkpoint to trigger a \"sourceless\" tensor subclass\n+        def checkpoint_fn(xs):\n+            return checkpoint(fn, xs)\n+\n+        xs = TwoTensor(torch.ones(2, 2), torch.ones(2, 2))\n+\n+        counter = CompileCounter()\n+        torch._dynamo.optimize(counter, nopython=True)(checkpoint_fn)(xs)\n+        self.assertEqual(counter.frame_count, 1)\n+        self.assertEqual(counter.op_count, 2)\n+\n",
            "whole_hunk": "@@ -7156,6 +7156,27 @@ def ___make_guard_fn():\n         self.assertEqual(counter.frame_count, 1)\n         self.assertEqual(counter.op_count, 18)\n \n+    def test_tracing_py_tree_tensor_subclass(self):\n+        import torch.utils._pytree as pytree\n+        from torch.testing._internal.two_tensor import TwoTensor\n+        from torch.utils.checkpoint import checkpoint\n+\n+        def fn(xs):\n+            nested_xs = [[xs]]\n+            flat_xs, spec = pytree.tree_flatten(xs)\n+            return flat_xs[0].clone()\n+\n+        # use checkpoint to trigger a \"sourceless\" tensor subclass\n+        def checkpoint_fn(xs):\n+            return checkpoint(fn, xs)\n+\n+        xs = TwoTensor(torch.ones(2, 2), torch.ones(2, 2))\n+\n+        counter = CompileCounter()\n+        torch._dynamo.optimize(counter, nopython=True)(checkpoint_fn)(xs)\n+        self.assertEqual(counter.frame_count, 1)\n+        self.assertEqual(counter.op_count, 2)\n+\n     def test_tracing_tree_map_only(self):\n         import torch.utils._pytree as pytree\n \n"
        },
        {
            "name": "builtin.py",
            "path": "torch/_dynamo/variables/builtin.py",
            "patches": [
                {
                    "old_start": 1080,
                    "old_length": 7,
                    "new_start": 1080,
                    "new_length": 9,
                    "hunk": "@@ -1080,7 +1080,9 @@ class BuiltinVariable(VariableTracker):\n                             for i, b in enumerate(bases)\n                         ]\n                     elif len(bases) == 1 and (\n-                        bases[0] is object or bases[0] is torch._C.TensorBase\n+                        bases[0] is object\n+                        or bases[0] is torch._C.TensorBase\n+                        or bases[0] is torch.Tensor\n                     ):\n                         tuple_args = [SourcelessBuilder()(tx, bases[0])]\n                     else:"
                }
            ],
            "whole_deleted": "-                        bases[0] is object or bases[0] is torch._C.TensorBase\n",
            "whole_added": "+                        bases[0] is object\n+                        or bases[0] is torch._C.TensorBase\n+                        or bases[0] is torch.Tensor\n",
            "whole_hunk": "@@ -1080,7 +1080,9 @@ class BuiltinVariable(VariableTracker):\n                             for i, b in enumerate(bases)\n                         ]\n                     elif len(bases) == 1 and (\n-                        bases[0] is object or bases[0] is torch._C.TensorBase\n+                        bases[0] is object\n+                        or bases[0] is torch._C.TensorBase\n+                        or bases[0] is torch.Tensor\n                     ):\n                         tuple_args = [SourcelessBuilder()(tx, bases[0])]\n                     else:"
        }
    ]
},
{
    "Id": 182,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/b4a008209a451d6fde94f09601ad179a274c315c",
    "date": "2024-04-27T18:35:35+00:00",
    "message": "Expose tensor check from guard for reusing (#124836)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/124836\nApproved by: https://github.com/jgong5, https://github.com/jansel, https://github.com/desertfire",
    "label": "NO",
    "changes": [
        {
            "name": "cache_entry.cpp",
            "path": "torch/csrc/dynamo/cache_entry.cpp",
            "patches": [
                {
                    "old_start": 10,
                    "old_length": 7,
                    "new_start": 10,
                    "new_length": 8,
                    "hunk": "@@ -10,7 +10,8 @@ CacheEntry::CacheEntry(const py::handle& guarded_code, PyObject* backend) {\n   this->backend = backend;\n   // TODO - clean this up when enable_cpp_guard_manager is True by default\n   if (py::hasattr(this->check_fn, \"root\")) {\n-    this->root_mgr = convert_to_root_guard_manager(this->check_fn.attr(\"root\"));\n+    this->root_mgr = torch::dynamo::convert_to_root_guard_manager(\n+        this->check_fn.attr(\"root\"));\n   }\n }\n \n"
                }
            ],
            "whole_deleted": "-    this->root_mgr = convert_to_root_guard_manager(this->check_fn.attr(\"root\"));\n",
            "whole_added": "+    this->root_mgr = torch::dynamo::convert_to_root_guard_manager(\n+        this->check_fn.attr(\"root\"));\n",
            "whole_hunk": "@@ -10,7 +10,8 @@ CacheEntry::CacheEntry(const py::handle& guarded_code, PyObject* backend) {\n   this->backend = backend;\n   // TODO - clean this up when enable_cpp_guard_manager is True by default\n   if (py::hasattr(this->check_fn, \"root\")) {\n-    this->root_mgr = convert_to_root_guard_manager(this->check_fn.attr(\"root\"));\n+    this->root_mgr = torch::dynamo::convert_to_root_guard_manager(\n+        this->check_fn.attr(\"root\"));\n   }\n }\n \n"
        },
        {
            "name": "extra_state.cpp",
            "path": "torch/csrc/dynamo/extra_state.cpp",
            "patches": [
                {
                    "old_start": 97,
                    "old_length": 7,
                    "new_start": 97,
                    "new_length": 8,
                    "hunk": "@@ -97,7 +97,8 @@ PyObject* lookup(\n         // TODO(anijain2305) - Clean this up when enable_cpp_guard_manager is\n         // True by default\n         if (cache_entry.root_mgr != nullptr) {\n-          valid = run_root_guard_manager(cache_entry.root_mgr, f_locals);\n+          valid = torch::dynamo::run_root_guard_manager(\n+              cache_entry.root_mgr, f_locals);\n         } else {\n           valid = cache_entry.check_fn(locals).cast<bool>();\n         }\n"
                }
            ],
            "whole_deleted": "-          valid = run_root_guard_manager(cache_entry.root_mgr, f_locals);\n",
            "whole_added": "+          valid = torch::dynamo::run_root_guard_manager(\n+              cache_entry.root_mgr, f_locals);\n",
            "whole_hunk": "@@ -97,7 +97,8 @@ PyObject* lookup(\n         // TODO(anijain2305) - Clean this up when enable_cpp_guard_manager is\n         // True by default\n         if (cache_entry.root_mgr != nullptr) {\n-          valid = run_root_guard_manager(cache_entry.root_mgr, f_locals);\n+          valid = torch::dynamo::run_root_guard_manager(\n+              cache_entry.root_mgr, f_locals);\n         } else {\n           valid = cache_entry.check_fn(locals).cast<bool>();\n         }\n"
        },
        {
            "name": "guards.cpp",
            "path": "torch/csrc/dynamo/guards.cpp",
            "patches": [
                {
                    "old_start": 40,
                    "old_length": 6,
                    "new_start": 40,
                    "new_length": 8,
                    "hunk": "@@ -40,6 +40,8 @@ typedef struct {\n \n #endif // IS_PYTHON_3_12_PLUS\n \n+namespace torch::dynamo {\n+\n // Macro to skip addition of duplicate guards like EQUALS_MATCH\n #define SKIP_IF_GUARD_ALREADY_PRESENT(name) \\\n   if (self.is_leaf_guard_present(name)) {   \\\n"
                },
                {
                    "old_start": 47,
                    "old_length": 149,
                    "new_start": 49,
                    "new_length": 131,
                    "hunk": "@@ -47,149 +49,131 @@ typedef struct {\n   }                                         \\\n   self.insert_leaf_guard(name);\n \n-namespace {\n-\n-struct LocalState {\n-  // TLS state that changes operators\n-  c10::impl::LocalDispatchKeySet dispatch_modifier;\n-  bool grad_mode_enabled;\n-\n-  at::DispatchKeySet apply(at::DispatchKeySet ks) const {\n-    return (ks | dispatch_modifier.included_) - dispatch_modifier.excluded_;\n-  }\n-\n-  LocalState()\n-      : dispatch_modifier(c10::impl::tls_local_dispatch_key_set()),\n-        grad_mode_enabled(at::GradMode::is_enabled()) {}\n-};\n+TensorCheck::TensorCheck(\n+    const LocalState& state,\n+    PyTypeObject* pt,\n+    const at::Tensor& v,\n+    std::vector<std::optional<c10::SymInt>> dynamic_dims_sizes,\n+    std::vector<std::optional<c10::SymInt>> dynamic_dims_strides)\n+    : pytype(pt),\n+      dispatch_key_(state.apply(v.key_set()).raw_repr()),\n+      dtype_(v.dtype().toScalarType()),\n+      device_index_(v.device().index()),\n+      requires_grad_(v.requires_grad()),\n+      sizes_(std::move(dynamic_dims_sizes)),\n+      strides_(std::move(dynamic_dims_strides)),\n+      dim_(static_cast<int64_t>(sizes_.size())) {\n+  // TODO(voz): In cases where sizes_ and strides_ are fully dynamic, should\n+  // we just treat this as optional?\n+}\n \n-class TensorCheck {\n- public:\n-  TensorCheck(\n-      const LocalState& state,\n-      PyTypeObject* pt,\n-      const at::Tensor& v,\n-      std::vector<std::optional<c10::SymInt>> dynamic_dims_sizes,\n-      std::vector<std::optional<c10::SymInt>> dynamic_dims_strides)\n-      : pytype(pt),\n-        dispatch_key_(state.apply(v.key_set()).raw_repr()),\n-        dtype_(v.dtype().toScalarType()),\n-        device_index_(v.device().index()),\n-        requires_grad_(v.requires_grad()),\n-        sizes_(std::move(dynamic_dims_sizes)),\n-        strides_(std::move(dynamic_dims_strides)),\n-        dim_(static_cast<int64_t>(sizes_.size())) {\n-    // TODO(voz): In cases where sizes_ and strides_ are fully dynamic, should\n-    // we just treat this as optional?\n-  }\n-\n-  // See note in guards.py [Note - On Export Tensor Guards]\n-  // Logic parallel to here must be maintained in python\n-  bool check(const LocalState& state, const at::Tensor& v) {\n-    if (dispatch_key_ != state.apply(v.key_set()).raw_repr() ||\n-        dtype_ != v.dtype().toScalarType() ||\n-        device_index_ != v.device().index() ||\n-        requires_grad_ != v.requires_grad()) {\n-      return false;\n-    }\n-    auto ndim = v.ndimension();\n-    if (ndim != dim_) {\n-      return false;\n-    }\n-    const auto& sizes = v.sym_sizes();\n-    const auto& strides = v.sym_strides();\n-    for (auto i : c10::irange(ndim)) {\n-      auto known_size = sizes_[i];\n-      auto known_stride = strides_[i];\n-      if (known_size.has_value()) {\n-        if (known_size.value() != sizes[i]) {\n-          return false;\n-        }\n+TensorCheck::TensorCheck(\n+    const LocalState& state,\n+    PyTypeObject* pt,\n+    uint64_t dispatch_key,\n+    at::ScalarType dtype,\n+    at::DeviceIndex device_index,\n+    std::vector<std::optional<c10::SymInt>> dynamic_dims_sizes,\n+    std::vector<std::optional<c10::SymInt>> dynamic_dims_strides)\n+    : pytype(pt),\n+      dispatch_key_(dispatch_key),\n+      dtype_(dtype),\n+      device_index_(device_index),\n+      requires_grad_(false),\n+      sizes_(std::move(dynamic_dims_sizes)),\n+      strides_(std::move(dynamic_dims_strides)),\n+      dim_(static_cast<int64_t>(sizes_.size())) {}\n+\n+// See note in guards.py [Note - On Export Tensor Guards]\n+// Logic parallel to here must be maintained in python\n+bool TensorCheck::check(const LocalState& state, const at::Tensor& v) {\n+  if (dispatch_key_ != state.apply(v.key_set()).raw_repr() ||\n+      dtype_ != v.dtype().toScalarType() ||\n+      device_index_ != v.device().index() ||\n+      requires_grad_ != v.requires_grad()) {\n+    return false;\n+  }\n+  auto ndim = v.ndimension();\n+  if (ndim != dim_) {\n+    return false;\n+  }\n+  const auto& sizes = v.sym_sizes();\n+  const auto& strides = v.sym_strides();\n+  for (auto i : c10::irange(ndim)) {\n+    auto known_size = sizes_[i];\n+    auto known_stride = strides_[i];\n+    if (known_size.has_value()) {\n+      if (known_size.value() != sizes[i]) {\n+        return false;\n       }\n-      if (known_stride.has_value()) {\n-        if (known_stride.value() != strides[i]) {\n-          return false;\n-        }\n+    }\n+    if (known_stride.has_value()) {\n+      if (known_stride.value() != strides[i]) {\n+        return false;\n       }\n     }\n-    return true;\n   }\n+  return true;\n+}\n \n-  std::string check_verbose(\n-      const LocalState& state,\n-      const at::Tensor& v,\n-      const std::string& tensor_name) {\n-    std::stringstream fail_reason;\n-    fail_reason << \"tensor '\" << tensor_name << \"' \";\n-    if (dispatch_key_ != state.apply(v.key_set()).raw_repr()) {\n-      // return fmt::format(\"tensor dispatch key mismatch. expected {}, actual\n-      // {}\", dispatch_key_, state.apply(v.key_set()).raw_repr());\n-      fail_reason << \"dispatch key set mismatch. expected \"\n-                  << c10::DispatchKeySet(\n-                         c10::DispatchKeySet::RAW, dispatch_key_)\n-                  << \", actual \" << state.apply(v.key_set());\n-      return fail_reason.str();\n-    } else if (dtype_ != v.dtype().toScalarType()) {\n-      // return fmt::format(\"tensor dtype mismatch. expected {}, actual {}\",\n-      // dtype_, v.dtype().toScalarType());\n-      fail_reason << \"dtype mismatch. expected \" << dtype_ << \", actual \"\n-                  << v.dtype().toScalarType();\n-      return fail_reason.str();\n-    } else if (device_index_ != v.device().index()) {\n-      fail_reason\n-          << \"Tensor device index mismatch. Expected device index to be \"\n-          << device_index_ << \", actual \" << v.device().index();\n-      return fail_reason.str();\n-    } else if (requires_grad_ != v.requires_grad()) {\n-      // return fmt::format(\"tensor requires_grad mismatch. expected {}\",\n-      // requires_grad_);\n-      fail_reason << \"requires_grad mismatch. expected requires_grad=\"\n-                  << requires_grad_;\n+std::string TensorCheck::check_verbose(\n+    const LocalState& state,\n+    const at::Tensor& v,\n+    const std::string& tensor_name) {\n+  std::stringstream fail_reason;\n+  fail_reason << \"tensor '\" << tensor_name << \"' \";\n+  if (dispatch_key_ != state.apply(v.key_set()).raw_repr()) {\n+    // return fmt::format(\"tensor dispatch key mismatch. expected {}, actual\n+    // {}\", dispatch_key_, state.apply(v.key_set()).raw_repr());\n+    fail_reason << \"dispatch key set mismatch. expected \"\n+                << c10::DispatchKeySet(c10::DispatchKeySet::RAW, dispatch_key_)\n+                << \", actual \" << state.apply(v.key_set());\n+    return fail_reason.str();\n+  } else if (dtype_ != v.dtype().toScalarType()) {\n+    // return fmt::format(\"tensor dtype mismatch. expected {}, actual {}\",\n+    // dtype_, v.dtype().toScalarType());\n+    fail_reason << \"dtype mismatch. expected \" << dtype_ << \", actual \"\n+                << v.dtype().toScalarType();\n+    return fail_reason.str();\n+  } else if (device_index_ != v.device().index()) {\n+    fail_reason << \"Tensor device index mismatch. Expected device index to be \"\n+                << device_index_ << \", actual \" << v.device().index();\n+    return fail_reason.str();\n+  } else if (requires_grad_ != v.requires_grad()) {\n+    // return fmt::format(\"tensor requires_grad mismatch. expected {}\",\n+    // requires_grad_);\n+    fail_reason << \"requires_grad mismatch. expected requires_grad=\"\n+                << requires_grad_;\n+    return fail_reason.str();\n+  }\n+  auto ndim = v.ndimension();\n+  if (ndim != dim_) {\n+    // return fmt::format(\"tensor rank mismatch. expected {}, actual {}\",\n+    // sizes_.size(), ndim);\n+    fail_reason << \"rank mismatch. expected \" << sizes_.size() << \", actual \"\n+                << ndim;\n+    return fail_reason.str();\n+  }\n+  const auto& sizes = v.sym_sizes();\n+  const auto& strides = v.sym_strides();\n+  for (auto i : c10::irange(ndim)) {\n+    auto known_size = sizes_[i];\n+    auto known_stride = strides_[i];\n+    if (known_size.has_value() && (known_size.value() != sizes[i])) {\n+      fail_reason << \"size mismatch at index \" << i << \". expected \"\n+                  << known_size.value() << \", actual \" << sizes[i];\n       return fail_reason.str();\n     }\n-    auto ndim = v.ndimension();\n-    if (ndim != dim_) {\n-      // return fmt::format(\"tensor rank mismatch. expected {}, actual {}\",\n-      // sizes_.size(), ndim);\n-      fail_reason << \"rank mismatch. expected \" << sizes_.size() << \", actual \"\n-                  << ndim;\n+    if (known_stride.has_value() && known_stride.value() != strides[i]) {\n+      fail_reason << \"stride mismatch at index \" << i << \". expected \"\n+                  << known_stride.value() << \", actual \" << strides[i];\n       return fail_reason.str();\n     }\n-    const auto& sizes = v.sym_sizes();\n-    const auto& strides = v.sym_strides();\n-    for (auto i : c10::irange(ndim)) {\n-      auto known_size = sizes_[i];\n-      auto known_stride = strides_[i];\n-      if (known_size.has_value() && (known_size.value() != sizes[i])) {\n-        fail_reason << \"size mismatch at index \" << i << \". expected \"\n-                    << known_size.value() << \", actual \" << sizes[i];\n-        return fail_reason.str();\n-      }\n-      if (known_stride.has_value() && known_stride.value() != strides[i]) {\n-        fail_reason << \"stride mismatch at index \" << i << \". expected \"\n-                    << known_stride.value() << \", actual \" << strides[i];\n-        return fail_reason.str();\n-      }\n-    }\n-    return \"\";\n   }\n+  return \"\";\n+}\n \n-  PyTypeObject* pytype;\n-\n- private:\n-  uint64_t dispatch_key_; // DispatchKeySet includes device/layout\n-  at::ScalarType dtype_;\n-  // Note(voz): While dispatch_key_ is sufficiently representative of a device\n-  // In that keys are more granular AND device specific - they do not\n-  // necessarily capture device indices correctly.\n-  at::DeviceIndex device_index_;\n-  bool requires_grad_;\n-  // NB: These are unset if dynamic shapes is enabled.\n-  std::vector<std::optional<c10::SymInt>> sizes_;\n-  std::vector<std::optional<c10::SymInt>> strides_;\n-  // Not strictly required for dense tensors, but nested tensors need it.\n-  int64_t dim_;\n-};\n+namespace {\n \n typedef std::vector<TensorCheck> ChecksList;\n \n"
                },
                {
                    "old_start": 3949,
                    "old_length": 3,
                    "new_start": 3933,
                    "new_length": 5,
                    "hunk": "@@ -3949,3 +3933,5 @@ PyObject* torch_c_dynamo_guards_init() {\n \n   return m;\n }\n+\n+} // namespace torch::dynamo\n"
                }
            ],
            "whole_deleted": "-namespace {\n-\n-struct LocalState {\n-  // TLS state that changes operators\n-  c10::impl::LocalDispatchKeySet dispatch_modifier;\n-  bool grad_mode_enabled;\n-\n-  at::DispatchKeySet apply(at::DispatchKeySet ks) const {\n-    return (ks | dispatch_modifier.included_) - dispatch_modifier.excluded_;\n-  }\n-\n-  LocalState()\n-      : dispatch_modifier(c10::impl::tls_local_dispatch_key_set()),\n-        grad_mode_enabled(at::GradMode::is_enabled()) {}\n-};\n-class TensorCheck {\n- public:\n-  TensorCheck(\n-      const LocalState& state,\n-      PyTypeObject* pt,\n-      const at::Tensor& v,\n-      std::vector<std::optional<c10::SymInt>> dynamic_dims_sizes,\n-      std::vector<std::optional<c10::SymInt>> dynamic_dims_strides)\n-      : pytype(pt),\n-        dispatch_key_(state.apply(v.key_set()).raw_repr()),\n-        dtype_(v.dtype().toScalarType()),\n-        device_index_(v.device().index()),\n-        requires_grad_(v.requires_grad()),\n-        sizes_(std::move(dynamic_dims_sizes)),\n-        strides_(std::move(dynamic_dims_strides)),\n-        dim_(static_cast<int64_t>(sizes_.size())) {\n-    // TODO(voz): In cases where sizes_ and strides_ are fully dynamic, should\n-    // we just treat this as optional?\n-  }\n-\n-  // See note in guards.py [Note - On Export Tensor Guards]\n-  // Logic parallel to here must be maintained in python\n-  bool check(const LocalState& state, const at::Tensor& v) {\n-    if (dispatch_key_ != state.apply(v.key_set()).raw_repr() ||\n-        dtype_ != v.dtype().toScalarType() ||\n-        device_index_ != v.device().index() ||\n-        requires_grad_ != v.requires_grad()) {\n-      return false;\n-    }\n-    auto ndim = v.ndimension();\n-    if (ndim != dim_) {\n-      return false;\n-    }\n-    const auto& sizes = v.sym_sizes();\n-    const auto& strides = v.sym_strides();\n-    for (auto i : c10::irange(ndim)) {\n-      auto known_size = sizes_[i];\n-      auto known_stride = strides_[i];\n-      if (known_size.has_value()) {\n-        if (known_size.value() != sizes[i]) {\n-          return false;\n-        }\n-      if (known_stride.has_value()) {\n-        if (known_stride.value() != strides[i]) {\n-          return false;\n-        }\n-    return true;\n-  std::string check_verbose(\n-      const LocalState& state,\n-      const at::Tensor& v,\n-      const std::string& tensor_name) {\n-    std::stringstream fail_reason;\n-    fail_reason << \"tensor '\" << tensor_name << \"' \";\n-    if (dispatch_key_ != state.apply(v.key_set()).raw_repr()) {\n-      // return fmt::format(\"tensor dispatch key mismatch. expected {}, actual\n-      // {}\", dispatch_key_, state.apply(v.key_set()).raw_repr());\n-      fail_reason << \"dispatch key set mismatch. expected \"\n-                  << c10::DispatchKeySet(\n-                         c10::DispatchKeySet::RAW, dispatch_key_)\n-                  << \", actual \" << state.apply(v.key_set());\n-      return fail_reason.str();\n-    } else if (dtype_ != v.dtype().toScalarType()) {\n-      // return fmt::format(\"tensor dtype mismatch. expected {}, actual {}\",\n-      // dtype_, v.dtype().toScalarType());\n-      fail_reason << \"dtype mismatch. expected \" << dtype_ << \", actual \"\n-                  << v.dtype().toScalarType();\n-      return fail_reason.str();\n-    } else if (device_index_ != v.device().index()) {\n-      fail_reason\n-          << \"Tensor device index mismatch. Expected device index to be \"\n-          << device_index_ << \", actual \" << v.device().index();\n-      return fail_reason.str();\n-    } else if (requires_grad_ != v.requires_grad()) {\n-      // return fmt::format(\"tensor requires_grad mismatch. expected {}\",\n-      // requires_grad_);\n-      fail_reason << \"requires_grad mismatch. expected requires_grad=\"\n-                  << requires_grad_;\n-    auto ndim = v.ndimension();\n-    if (ndim != dim_) {\n-      // return fmt::format(\"tensor rank mismatch. expected {}, actual {}\",\n-      // sizes_.size(), ndim);\n-      fail_reason << \"rank mismatch. expected \" << sizes_.size() << \", actual \"\n-                  << ndim;\n-    const auto& sizes = v.sym_sizes();\n-    const auto& strides = v.sym_strides();\n-    for (auto i : c10::irange(ndim)) {\n-      auto known_size = sizes_[i];\n-      auto known_stride = strides_[i];\n-      if (known_size.has_value() && (known_size.value() != sizes[i])) {\n-        fail_reason << \"size mismatch at index \" << i << \". expected \"\n-                    << known_size.value() << \", actual \" << sizes[i];\n-        return fail_reason.str();\n-      }\n-      if (known_stride.has_value() && known_stride.value() != strides[i]) {\n-        fail_reason << \"stride mismatch at index \" << i << \". expected \"\n-                    << known_stride.value() << \", actual \" << strides[i];\n-        return fail_reason.str();\n-      }\n-    }\n-    return \"\";\n-  PyTypeObject* pytype;\n-\n- private:\n-  uint64_t dispatch_key_; // DispatchKeySet includes device/layout\n-  at::ScalarType dtype_;\n-  // Note(voz): While dispatch_key_ is sufficiently representative of a device\n-  // In that keys are more granular AND device specific - they do not\n-  // necessarily capture device indices correctly.\n-  at::DeviceIndex device_index_;\n-  bool requires_grad_;\n-  // NB: These are unset if dynamic shapes is enabled.\n-  std::vector<std::optional<c10::SymInt>> sizes_;\n-  std::vector<std::optional<c10::SymInt>> strides_;\n-  // Not strictly required for dense tensors, but nested tensors need it.\n-  int64_t dim_;\n-};\n",
            "whole_added": "+namespace torch::dynamo {\n+\n+TensorCheck::TensorCheck(\n+    const LocalState& state,\n+    PyTypeObject* pt,\n+    const at::Tensor& v,\n+    std::vector<std::optional<c10::SymInt>> dynamic_dims_sizes,\n+    std::vector<std::optional<c10::SymInt>> dynamic_dims_strides)\n+    : pytype(pt),\n+      dispatch_key_(state.apply(v.key_set()).raw_repr()),\n+      dtype_(v.dtype().toScalarType()),\n+      device_index_(v.device().index()),\n+      requires_grad_(v.requires_grad()),\n+      sizes_(std::move(dynamic_dims_sizes)),\n+      strides_(std::move(dynamic_dims_strides)),\n+      dim_(static_cast<int64_t>(sizes_.size())) {\n+  // TODO(voz): In cases where sizes_ and strides_ are fully dynamic, should\n+  // we just treat this as optional?\n+}\n+TensorCheck::TensorCheck(\n+    const LocalState& state,\n+    PyTypeObject* pt,\n+    uint64_t dispatch_key,\n+    at::ScalarType dtype,\n+    at::DeviceIndex device_index,\n+    std::vector<std::optional<c10::SymInt>> dynamic_dims_sizes,\n+    std::vector<std::optional<c10::SymInt>> dynamic_dims_strides)\n+    : pytype(pt),\n+      dispatch_key_(dispatch_key),\n+      dtype_(dtype),\n+      device_index_(device_index),\n+      requires_grad_(false),\n+      sizes_(std::move(dynamic_dims_sizes)),\n+      strides_(std::move(dynamic_dims_strides)),\n+      dim_(static_cast<int64_t>(sizes_.size())) {}\n+\n+// See note in guards.py [Note - On Export Tensor Guards]\n+// Logic parallel to here must be maintained in python\n+bool TensorCheck::check(const LocalState& state, const at::Tensor& v) {\n+  if (dispatch_key_ != state.apply(v.key_set()).raw_repr() ||\n+      dtype_ != v.dtype().toScalarType() ||\n+      device_index_ != v.device().index() ||\n+      requires_grad_ != v.requires_grad()) {\n+    return false;\n+  }\n+  auto ndim = v.ndimension();\n+  if (ndim != dim_) {\n+    return false;\n+  }\n+  const auto& sizes = v.sym_sizes();\n+  const auto& strides = v.sym_strides();\n+  for (auto i : c10::irange(ndim)) {\n+    auto known_size = sizes_[i];\n+    auto known_stride = strides_[i];\n+    if (known_size.has_value()) {\n+      if (known_size.value() != sizes[i]) {\n+        return false;\n+    }\n+    if (known_stride.has_value()) {\n+      if (known_stride.value() != strides[i]) {\n+        return false;\n+  return true;\n+}\n+std::string TensorCheck::check_verbose(\n+    const LocalState& state,\n+    const at::Tensor& v,\n+    const std::string& tensor_name) {\n+  std::stringstream fail_reason;\n+  fail_reason << \"tensor '\" << tensor_name << \"' \";\n+  if (dispatch_key_ != state.apply(v.key_set()).raw_repr()) {\n+    // return fmt::format(\"tensor dispatch key mismatch. expected {}, actual\n+    // {}\", dispatch_key_, state.apply(v.key_set()).raw_repr());\n+    fail_reason << \"dispatch key set mismatch. expected \"\n+                << c10::DispatchKeySet(c10::DispatchKeySet::RAW, dispatch_key_)\n+                << \", actual \" << state.apply(v.key_set());\n+    return fail_reason.str();\n+  } else if (dtype_ != v.dtype().toScalarType()) {\n+    // return fmt::format(\"tensor dtype mismatch. expected {}, actual {}\",\n+    // dtype_, v.dtype().toScalarType());\n+    fail_reason << \"dtype mismatch. expected \" << dtype_ << \", actual \"\n+                << v.dtype().toScalarType();\n+    return fail_reason.str();\n+  } else if (device_index_ != v.device().index()) {\n+    fail_reason << \"Tensor device index mismatch. Expected device index to be \"\n+                << device_index_ << \", actual \" << v.device().index();\n+    return fail_reason.str();\n+  } else if (requires_grad_ != v.requires_grad()) {\n+    // return fmt::format(\"tensor requires_grad mismatch. expected {}\",\n+    // requires_grad_);\n+    fail_reason << \"requires_grad mismatch. expected requires_grad=\"\n+                << requires_grad_;\n+    return fail_reason.str();\n+  }\n+  auto ndim = v.ndimension();\n+  if (ndim != dim_) {\n+    // return fmt::format(\"tensor rank mismatch. expected {}, actual {}\",\n+    // sizes_.size(), ndim);\n+    fail_reason << \"rank mismatch. expected \" << sizes_.size() << \", actual \"\n+                << ndim;\n+    return fail_reason.str();\n+  }\n+  const auto& sizes = v.sym_sizes();\n+  const auto& strides = v.sym_strides();\n+  for (auto i : c10::irange(ndim)) {\n+    auto known_size = sizes_[i];\n+    auto known_stride = strides_[i];\n+    if (known_size.has_value() && (known_size.value() != sizes[i])) {\n+      fail_reason << \"size mismatch at index \" << i << \". expected \"\n+                  << known_size.value() << \", actual \" << sizes[i];\n+    if (known_stride.has_value() && known_stride.value() != strides[i]) {\n+      fail_reason << \"stride mismatch at index \" << i << \". expected \"\n+                  << known_stride.value() << \", actual \" << strides[i];\n+  return \"\";\n+}\n+namespace {\n+\n+} // namespace torch::dynamo\n",
            "whole_hunk": "@@ -40,6 +40,8 @@ typedef struct {\n \n #endif // IS_PYTHON_3_12_PLUS\n \n+namespace torch::dynamo {\n+\n // Macro to skip addition of duplicate guards like EQUALS_MATCH\n #define SKIP_IF_GUARD_ALREADY_PRESENT(name) \\\n   if (self.is_leaf_guard_present(name)) {   \\\n@@ -47,149 +49,131 @@ typedef struct {\n   }                                         \\\n   self.insert_leaf_guard(name);\n \n-namespace {\n-\n-struct LocalState {\n-  // TLS state that changes operators\n-  c10::impl::LocalDispatchKeySet dispatch_modifier;\n-  bool grad_mode_enabled;\n-\n-  at::DispatchKeySet apply(at::DispatchKeySet ks) const {\n-    return (ks | dispatch_modifier.included_) - dispatch_modifier.excluded_;\n-  }\n-\n-  LocalState()\n-      : dispatch_modifier(c10::impl::tls_local_dispatch_key_set()),\n-        grad_mode_enabled(at::GradMode::is_enabled()) {}\n-};\n+TensorCheck::TensorCheck(\n+    const LocalState& state,\n+    PyTypeObject* pt,\n+    const at::Tensor& v,\n+    std::vector<std::optional<c10::SymInt>> dynamic_dims_sizes,\n+    std::vector<std::optional<c10::SymInt>> dynamic_dims_strides)\n+    : pytype(pt),\n+      dispatch_key_(state.apply(v.key_set()).raw_repr()),\n+      dtype_(v.dtype().toScalarType()),\n+      device_index_(v.device().index()),\n+      requires_grad_(v.requires_grad()),\n+      sizes_(std::move(dynamic_dims_sizes)),\n+      strides_(std::move(dynamic_dims_strides)),\n+      dim_(static_cast<int64_t>(sizes_.size())) {\n+  // TODO(voz): In cases where sizes_ and strides_ are fully dynamic, should\n+  // we just treat this as optional?\n+}\n \n-class TensorCheck {\n- public:\n-  TensorCheck(\n-      const LocalState& state,\n-      PyTypeObject* pt,\n-      const at::Tensor& v,\n-      std::vector<std::optional<c10::SymInt>> dynamic_dims_sizes,\n-      std::vector<std::optional<c10::SymInt>> dynamic_dims_strides)\n-      : pytype(pt),\n-        dispatch_key_(state.apply(v.key_set()).raw_repr()),\n-        dtype_(v.dtype().toScalarType()),\n-        device_index_(v.device().index()),\n-        requires_grad_(v.requires_grad()),\n-        sizes_(std::move(dynamic_dims_sizes)),\n-        strides_(std::move(dynamic_dims_strides)),\n-        dim_(static_cast<int64_t>(sizes_.size())) {\n-    // TODO(voz): In cases where sizes_ and strides_ are fully dynamic, should\n-    // we just treat this as optional?\n-  }\n-\n-  // See note in guards.py [Note - On Export Tensor Guards]\n-  // Logic parallel to here must be maintained in python\n-  bool check(const LocalState& state, const at::Tensor& v) {\n-    if (dispatch_key_ != state.apply(v.key_set()).raw_repr() ||\n-        dtype_ != v.dtype().toScalarType() ||\n-        device_index_ != v.device().index() ||\n-        requires_grad_ != v.requires_grad()) {\n-      return false;\n-    }\n-    auto ndim = v.ndimension();\n-    if (ndim != dim_) {\n-      return false;\n-    }\n-    const auto& sizes = v.sym_sizes();\n-    const auto& strides = v.sym_strides();\n-    for (auto i : c10::irange(ndim)) {\n-      auto known_size = sizes_[i];\n-      auto known_stride = strides_[i];\n-      if (known_size.has_value()) {\n-        if (known_size.value() != sizes[i]) {\n-          return false;\n-        }\n+TensorCheck::TensorCheck(\n+    const LocalState& state,\n+    PyTypeObject* pt,\n+    uint64_t dispatch_key,\n+    at::ScalarType dtype,\n+    at::DeviceIndex device_index,\n+    std::vector<std::optional<c10::SymInt>> dynamic_dims_sizes,\n+    std::vector<std::optional<c10::SymInt>> dynamic_dims_strides)\n+    : pytype(pt),\n+      dispatch_key_(dispatch_key),\n+      dtype_(dtype),\n+      device_index_(device_index),\n+      requires_grad_(false),\n+      sizes_(std::move(dynamic_dims_sizes)),\n+      strides_(std::move(dynamic_dims_strides)),\n+      dim_(static_cast<int64_t>(sizes_.size())) {}\n+\n+// See note in guards.py [Note - On Export Tensor Guards]\n+// Logic parallel to here must be maintained in python\n+bool TensorCheck::check(const LocalState& state, const at::Tensor& v) {\n+  if (dispatch_key_ != state.apply(v.key_set()).raw_repr() ||\n+      dtype_ != v.dtype().toScalarType() ||\n+      device_index_ != v.device().index() ||\n+      requires_grad_ != v.requires_grad()) {\n+    return false;\n+  }\n+  auto ndim = v.ndimension();\n+  if (ndim != dim_) {\n+    return false;\n+  }\n+  const auto& sizes = v.sym_sizes();\n+  const auto& strides = v.sym_strides();\n+  for (auto i : c10::irange(ndim)) {\n+    auto known_size = sizes_[i];\n+    auto known_stride = strides_[i];\n+    if (known_size.has_value()) {\n+      if (known_size.value() != sizes[i]) {\n+        return false;\n       }\n-      if (known_stride.has_value()) {\n-        if (known_stride.value() != strides[i]) {\n-          return false;\n-        }\n+    }\n+    if (known_stride.has_value()) {\n+      if (known_stride.value() != strides[i]) {\n+        return false;\n       }\n     }\n-    return true;\n   }\n+  return true;\n+}\n \n-  std::string check_verbose(\n-      const LocalState& state,\n-      const at::Tensor& v,\n-      const std::string& tensor_name) {\n-    std::stringstream fail_reason;\n-    fail_reason << \"tensor '\" << tensor_name << \"' \";\n-    if (dispatch_key_ != state.apply(v.key_set()).raw_repr()) {\n-      // return fmt::format(\"tensor dispatch key mismatch. expected {}, actual\n-      // {}\", dispatch_key_, state.apply(v.key_set()).raw_repr());\n-      fail_reason << \"dispatch key set mismatch. expected \"\n-                  << c10::DispatchKeySet(\n-                         c10::DispatchKeySet::RAW, dispatch_key_)\n-                  << \", actual \" << state.apply(v.key_set());\n-      return fail_reason.str();\n-    } else if (dtype_ != v.dtype().toScalarType()) {\n-      // return fmt::format(\"tensor dtype mismatch. expected {}, actual {}\",\n-      // dtype_, v.dtype().toScalarType());\n-      fail_reason << \"dtype mismatch. expected \" << dtype_ << \", actual \"\n-                  << v.dtype().toScalarType();\n-      return fail_reason.str();\n-    } else if (device_index_ != v.device().index()) {\n-      fail_reason\n-          << \"Tensor device index mismatch. Expected device index to be \"\n-          << device_index_ << \", actual \" << v.device().index();\n-      return fail_reason.str();\n-    } else if (requires_grad_ != v.requires_grad()) {\n-      // return fmt::format(\"tensor requires_grad mismatch. expected {}\",\n-      // requires_grad_);\n-      fail_reason << \"requires_grad mismatch. expected requires_grad=\"\n-                  << requires_grad_;\n+std::string TensorCheck::check_verbose(\n+    const LocalState& state,\n+    const at::Tensor& v,\n+    const std::string& tensor_name) {\n+  std::stringstream fail_reason;\n+  fail_reason << \"tensor '\" << tensor_name << \"' \";\n+  if (dispatch_key_ != state.apply(v.key_set()).raw_repr()) {\n+    // return fmt::format(\"tensor dispatch key mismatch. expected {}, actual\n+    // {}\", dispatch_key_, state.apply(v.key_set()).raw_repr());\n+    fail_reason << \"dispatch key set mismatch. expected \"\n+                << c10::DispatchKeySet(c10::DispatchKeySet::RAW, dispatch_key_)\n+                << \", actual \" << state.apply(v.key_set());\n+    return fail_reason.str();\n+  } else if (dtype_ != v.dtype().toScalarType()) {\n+    // return fmt::format(\"tensor dtype mismatch. expected {}, actual {}\",\n+    // dtype_, v.dtype().toScalarType());\n+    fail_reason << \"dtype mismatch. expected \" << dtype_ << \", actual \"\n+                << v.dtype().toScalarType();\n+    return fail_reason.str();\n+  } else if (device_index_ != v.device().index()) {\n+    fail_reason << \"Tensor device index mismatch. Expected device index to be \"\n+                << device_index_ << \", actual \" << v.device().index();\n+    return fail_reason.str();\n+  } else if (requires_grad_ != v.requires_grad()) {\n+    // return fmt::format(\"tensor requires_grad mismatch. expected {}\",\n+    // requires_grad_);\n+    fail_reason << \"requires_grad mismatch. expected requires_grad=\"\n+                << requires_grad_;\n+    return fail_reason.str();\n+  }\n+  auto ndim = v.ndimension();\n+  if (ndim != dim_) {\n+    // return fmt::format(\"tensor rank mismatch. expected {}, actual {}\",\n+    // sizes_.size(), ndim);\n+    fail_reason << \"rank mismatch. expected \" << sizes_.size() << \", actual \"\n+                << ndim;\n+    return fail_reason.str();\n+  }\n+  const auto& sizes = v.sym_sizes();\n+  const auto& strides = v.sym_strides();\n+  for (auto i : c10::irange(ndim)) {\n+    auto known_size = sizes_[i];\n+    auto known_stride = strides_[i];\n+    if (known_size.has_value() && (known_size.value() != sizes[i])) {\n+      fail_reason << \"size mismatch at index \" << i << \". expected \"\n+                  << known_size.value() << \", actual \" << sizes[i];\n       return fail_reason.str();\n     }\n-    auto ndim = v.ndimension();\n-    if (ndim != dim_) {\n-      // return fmt::format(\"tensor rank mismatch. expected {}, actual {}\",\n-      // sizes_.size(), ndim);\n-      fail_reason << \"rank mismatch. expected \" << sizes_.size() << \", actual \"\n-                  << ndim;\n+    if (known_stride.has_value() && known_stride.value() != strides[i]) {\n+      fail_reason << \"stride mismatch at index \" << i << \". expected \"\n+                  << known_stride.value() << \", actual \" << strides[i];\n       return fail_reason.str();\n     }\n-    const auto& sizes = v.sym_sizes();\n-    const auto& strides = v.sym_strides();\n-    for (auto i : c10::irange(ndim)) {\n-      auto known_size = sizes_[i];\n-      auto known_stride = strides_[i];\n-      if (known_size.has_value() && (known_size.value() != sizes[i])) {\n-        fail_reason << \"size mismatch at index \" << i << \". expected \"\n-                    << known_size.value() << \", actual \" << sizes[i];\n-        return fail_reason.str();\n-      }\n-      if (known_stride.has_value() && known_stride.value() != strides[i]) {\n-        fail_reason << \"stride mismatch at index \" << i << \". expected \"\n-                    << known_stride.value() << \", actual \" << strides[i];\n-        return fail_reason.str();\n-      }\n-    }\n-    return \"\";\n   }\n+  return \"\";\n+}\n \n-  PyTypeObject* pytype;\n-\n- private:\n-  uint64_t dispatch_key_; // DispatchKeySet includes device/layout\n-  at::ScalarType dtype_;\n-  // Note(voz): While dispatch_key_ is sufficiently representative of a device\n-  // In that keys are more granular AND device specific - they do not\n-  // necessarily capture device indices correctly.\n-  at::DeviceIndex device_index_;\n-  bool requires_grad_;\n-  // NB: These are unset if dynamic shapes is enabled.\n-  std::vector<std::optional<c10::SymInt>> sizes_;\n-  std::vector<std::optional<c10::SymInt>> strides_;\n-  // Not strictly required for dense tensors, but nested tensors need it.\n-  int64_t dim_;\n-};\n+namespace {\n \n typedef std::vector<TensorCheck> ChecksList;\n \n@@ -3949,3 +3933,5 @@ PyObject* torch_c_dynamo_guards_init() {\n \n   return m;\n }\n+\n+} // namespace torch::dynamo\n"
        },
        {
            "name": "guards.h",
            "path": "torch/csrc/dynamo/guards.h",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 10,
                    "new_start": 1,
                    "new_length": 79,
                    "hunk": "@@ -1,10 +1,79 @@\n #pragma once\n+#include <c10/core/GradMode.h>\n #include <torch/csrc/python_headers.h>\n #include <torch/csrc/utils/pybind.h>\n \n+namespace torch::dynamo {\n+\n PyObject* torch_c_dynamo_guards_init();\n \n // interfaces for extra_state and eval_frame.c because RootGuardManager class is\n // not visible there.\n void* convert_to_root_guard_manager(py::object root);\n bool run_root_guard_manager(void* root, PyObject* f_locals);\n+\n+struct LocalState {\n+  // TLS state that changes operators\n+  c10::impl::LocalDispatchKeySet dispatch_modifier;\n+  c10::DispatchKeySet override_dispatch_key_set;\n+  bool grad_mode_enabled;\n+\n+  at::DispatchKeySet apply(at::DispatchKeySet ks) const {\n+    if (override_dispatch_key_set.empty()) {\n+      return (ks | dispatch_modifier.included_) - dispatch_modifier.excluded_;\n+    } else {\n+      return override_dispatch_key_set;\n+    }\n+  }\n+\n+  LocalState()\n+      : dispatch_modifier(c10::impl::tls_local_dispatch_key_set()),\n+        grad_mode_enabled(at::GradMode::is_enabled()) {}\n+\n+  void overrideDispatchKeySet(c10::DispatchKeySet ks) {\n+    override_dispatch_key_set = ks;\n+  }\n+};\n+\n+class TensorCheck {\n+ public:\n+  TensorCheck(\n+      const LocalState& state,\n+      PyTypeObject* pt,\n+      const at::Tensor& v,\n+      std::vector<std::optional<c10::SymInt>> dynamic_dims_sizes,\n+      std::vector<std::optional<c10::SymInt>> dynamic_dims_strides);\n+\n+  TensorCheck(\n+      const LocalState& state,\n+      PyTypeObject* pt,\n+      uint64_t dispatch_key,\n+      at::ScalarType dtype,\n+      at::DeviceIndex device_index,\n+      std::vector<std::optional<c10::SymInt>> dynamic_dims_sizes,\n+      std::vector<std::optional<c10::SymInt>> dynamic_dims_strides);\n+\n+  bool check(const LocalState& state, const at::Tensor& v);\n+  std::string check_verbose(\n+      const LocalState& state,\n+      const at::Tensor& v,\n+      const std::string& tensor_name);\n+\n+  PyTypeObject* pytype;\n+\n+ private:\n+  uint64_t dispatch_key_; // DispatchKeySet includes device/layout\n+  at::ScalarType dtype_;\n+  // Note(voz): While dispatch_key_ is sufficiently representative of a device\n+  // In that keys are more granular AND device specific - they do not\n+  // necessarily capture device indices correctly.\n+  at::DeviceIndex device_index_;\n+  bool requires_grad_;\n+  // NB: These are unset if dynamic shapes is enabled.\n+  std::vector<std::optional<c10::SymInt>> sizes_;\n+  std::vector<std::optional<c10::SymInt>> strides_;\n+  // Not strictly required for dense tensors, but nested tensors need it.\n+  int64_t dim_;\n+};\n+\n+} // namespace torch::dynamo"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include <c10/core/GradMode.h>\n+namespace torch::dynamo {\n+\n+\n+struct LocalState {\n+  // TLS state that changes operators\n+  c10::impl::LocalDispatchKeySet dispatch_modifier;\n+  c10::DispatchKeySet override_dispatch_key_set;\n+  bool grad_mode_enabled;\n+\n+  at::DispatchKeySet apply(at::DispatchKeySet ks) const {\n+    if (override_dispatch_key_set.empty()) {\n+      return (ks | dispatch_modifier.included_) - dispatch_modifier.excluded_;\n+    } else {\n+      return override_dispatch_key_set;\n+    }\n+  }\n+\n+  LocalState()\n+      : dispatch_modifier(c10::impl::tls_local_dispatch_key_set()),\n+        grad_mode_enabled(at::GradMode::is_enabled()) {}\n+\n+  void overrideDispatchKeySet(c10::DispatchKeySet ks) {\n+    override_dispatch_key_set = ks;\n+  }\n+};\n+\n+class TensorCheck {\n+ public:\n+  TensorCheck(\n+      const LocalState& state,\n+      PyTypeObject* pt,\n+      const at::Tensor& v,\n+      std::vector<std::optional<c10::SymInt>> dynamic_dims_sizes,\n+      std::vector<std::optional<c10::SymInt>> dynamic_dims_strides);\n+\n+  TensorCheck(\n+      const LocalState& state,\n+      PyTypeObject* pt,\n+      uint64_t dispatch_key,\n+      at::ScalarType dtype,\n+      at::DeviceIndex device_index,\n+      std::vector<std::optional<c10::SymInt>> dynamic_dims_sizes,\n+      std::vector<std::optional<c10::SymInt>> dynamic_dims_strides);\n+\n+  bool check(const LocalState& state, const at::Tensor& v);\n+  std::string check_verbose(\n+      const LocalState& state,\n+      const at::Tensor& v,\n+      const std::string& tensor_name);\n+\n+  PyTypeObject* pytype;\n+\n+ private:\n+  uint64_t dispatch_key_; // DispatchKeySet includes device/layout\n+  at::ScalarType dtype_;\n+  // Note(voz): While dispatch_key_ is sufficiently representative of a device\n+  // In that keys are more granular AND device specific - they do not\n+  // necessarily capture device indices correctly.\n+  at::DeviceIndex device_index_;\n+  bool requires_grad_;\n+  // NB: These are unset if dynamic shapes is enabled.\n+  std::vector<std::optional<c10::SymInt>> sizes_;\n+  std::vector<std::optional<c10::SymInt>> strides_;\n+  // Not strictly required for dense tensors, but nested tensors need it.\n+  int64_t dim_;\n+};\n+\n+} // namespace torch::dynamo\n",
            "whole_hunk": "@@ -1,10 +1,79 @@\n #pragma once\n+#include <c10/core/GradMode.h>\n #include <torch/csrc/python_headers.h>\n #include <torch/csrc/utils/pybind.h>\n \n+namespace torch::dynamo {\n+\n PyObject* torch_c_dynamo_guards_init();\n \n // interfaces for extra_state and eval_frame.c because RootGuardManager class is\n // not visible there.\n void* convert_to_root_guard_manager(py::object root);\n bool run_root_guard_manager(void* root, PyObject* f_locals);\n+\n+struct LocalState {\n+  // TLS state that changes operators\n+  c10::impl::LocalDispatchKeySet dispatch_modifier;\n+  c10::DispatchKeySet override_dispatch_key_set;\n+  bool grad_mode_enabled;\n+\n+  at::DispatchKeySet apply(at::DispatchKeySet ks) const {\n+    if (override_dispatch_key_set.empty()) {\n+      return (ks | dispatch_modifier.included_) - dispatch_modifier.excluded_;\n+    } else {\n+      return override_dispatch_key_set;\n+    }\n+  }\n+\n+  LocalState()\n+      : dispatch_modifier(c10::impl::tls_local_dispatch_key_set()),\n+        grad_mode_enabled(at::GradMode::is_enabled()) {}\n+\n+  void overrideDispatchKeySet(c10::DispatchKeySet ks) {\n+    override_dispatch_key_set = ks;\n+  }\n+};\n+\n+class TensorCheck {\n+ public:\n+  TensorCheck(\n+      const LocalState& state,\n+      PyTypeObject* pt,\n+      const at::Tensor& v,\n+      std::vector<std::optional<c10::SymInt>> dynamic_dims_sizes,\n+      std::vector<std::optional<c10::SymInt>> dynamic_dims_strides);\n+\n+  TensorCheck(\n+      const LocalState& state,\n+      PyTypeObject* pt,\n+      uint64_t dispatch_key,\n+      at::ScalarType dtype,\n+      at::DeviceIndex device_index,\n+      std::vector<std::optional<c10::SymInt>> dynamic_dims_sizes,\n+      std::vector<std::optional<c10::SymInt>> dynamic_dims_strides);\n+\n+  bool check(const LocalState& state, const at::Tensor& v);\n+  std::string check_verbose(\n+      const LocalState& state,\n+      const at::Tensor& v,\n+      const std::string& tensor_name);\n+\n+  PyTypeObject* pytype;\n+\n+ private:\n+  uint64_t dispatch_key_; // DispatchKeySet includes device/layout\n+  at::ScalarType dtype_;\n+  // Note(voz): While dispatch_key_ is sufficiently representative of a device\n+  // In that keys are more granular AND device specific - they do not\n+  // necessarily capture device indices correctly.\n+  at::DeviceIndex device_index_;\n+  bool requires_grad_;\n+  // NB: These are unset if dynamic shapes is enabled.\n+  std::vector<std::optional<c10::SymInt>> sizes_;\n+  std::vector<std::optional<c10::SymInt>> strides_;\n+  // Not strictly required for dense tensors, but nested tensors need it.\n+  int64_t dim_;\n+};\n+\n+} // namespace torch::dynamo"
        }
    ]
},
{
    "Id": 124,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/045309aa3575b86eb6fd259bfaa336f723eb7b2f",
    "date": "2024-05-28T17:56:13+00:00",
    "message": "[MPS] Enable toch.mm and friends for complex dtypes (#127241)\n\n- Add `supportedFloatingOrComplexType`\n- Change dtype check to those\n- Extend low-precision fp32 list to complex types\n- Mark conv2d as supported now, as it was failing due to the tighter accuracy constrains than the same op for float32 dtype\n\nFixes https://github.com/pytorch/pytorch/issues/127178\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/127241\nApproved by: https://github.com/janeyx99",
    "label": "YES",
    "changes": [
        {
            "name": "OperationUtils.h",
            "path": "aten/src/ATen/native/mps/OperationUtils.h",
            "patches": [
                {
                    "old_start": 419,
                    "old_length": 6,
                    "new_start": 419,
                    "new_length": 17,
                    "hunk": "@@ -419,6 +419,17 @@ inline bool supportedFloatingType(const Tensor& t) {\n   return supportedFloatingType(t.scalar_type());\n }\n \n+inline bool supportedFloatingOrComplexType(ScalarType dtype) {\n+  if (dtype == kComplexFloat || dtype == kComplexHalf) {\n+    return supportsComplex();\n+  }\n+  return supportedFloatingType(dtype);\n+}\n+inline bool supportedFloatingOrComplexType(const Tensor& t) {\n+  return supportedFloatingOrComplexType(t.scalar_type());\n+}\n+\n+\n inline bool needsGather(const Tensor& t) {\n   return !t.is_contiguous() || t.storage_offset();\n }\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+inline bool supportedFloatingOrComplexType(ScalarType dtype) {\n+  if (dtype == kComplexFloat || dtype == kComplexHalf) {\n+    return supportsComplex();\n+  }\n+  return supportedFloatingType(dtype);\n+}\n+inline bool supportedFloatingOrComplexType(const Tensor& t) {\n+  return supportedFloatingOrComplexType(t.scalar_type());\n+}\n+\n+\n",
            "whole_hunk": "@@ -419,6 +419,17 @@ inline bool supportedFloatingType(const Tensor& t) {\n   return supportedFloatingType(t.scalar_type());\n }\n \n+inline bool supportedFloatingOrComplexType(ScalarType dtype) {\n+  if (dtype == kComplexFloat || dtype == kComplexHalf) {\n+    return supportsComplex();\n+  }\n+  return supportedFloatingType(dtype);\n+}\n+inline bool supportedFloatingOrComplexType(const Tensor& t) {\n+  return supportedFloatingOrComplexType(t.scalar_type());\n+}\n+\n+\n inline bool needsGather(const Tensor& t) {\n   return !t.is_contiguous() || t.storage_offset();\n }\n"
        },
        {
            "name": "Linear.mm",
            "path": "aten/src/ATen/native/mps/operations/Linear.mm",
            "patches": [
                {
                    "old_start": 15,
                    "old_length": 16,
                    "new_start": 15,
                    "new_length": 16,
                    "hunk": "@@ -15,16 +15,16 @@ Tensor _mps_linear(const Tensor& input, const Tensor& weight_arg, const c10::opt\n \n   auto weight = (weight_arg.dim() == 1) ? weight_arg.view({1, weight_arg.size(0)}) : weight_arg;\n \n-  TORCH_CHECK(supportedFloatingType(input), \"MPS device does not support linear for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(input), \"MPS device does not support linear for non-float inputs\");\n   TORCH_CHECK(input.is_mps(), \"Tensor for argument input is on \", input.device(), \" but expected on mps\");\n-  TORCH_CHECK(supportedFloatingType(weight_arg), \"MPS device does not support linear for non-float weights\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(weight_arg), \"MPS device does not support linear for non-float weights\");\n   TORCH_CHECK(weight_arg.is_mps(), \"Tensor for argument weight is on \", weight_arg.device(), \" but expected on mps\");\n \n   const Tensor& bias = *(at::borrow_from_optional_tensor(bias_opt));\n   const bool is_bias_defined = bias.defined();\n   if (is_bias_defined) {\n     TORCH_CHECK(bias.is_mps(), \"Tensor for argument bias is on \", bias.device(), \" but expected on mps\");\n-    TORCH_CHECK(supportedFloatingType(bias), \"MPS device does not support linear for non-float bias\");\n+    TORCH_CHECK(supportedFloatingOrComplexType(bias), \"MPS device does not support linear for non-float bias\");\n   }\n \n   auto input_size = input.sizes();\n"
                },
                {
                    "old_start": 128,
                    "old_length": 11,
                    "new_start": 128,
                    "new_length": 12,
                    "hunk": "@@ -128,11 +128,12 @@ Tensor _mps_linear(const Tensor& input, const Tensor& weight_arg, const c10::opt\n \n static Tensor _mps_linear_backward_input(IntArrayRef input_size, const Tensor& grad_output, const Tensor& weight) {\n   TORCH_CHECK(grad_output.is_mps(), \"mps_linear_backward: grad_output needs to be mps layout\");\n-  TORCH_CHECK(weight.device().is_mps() && supportedFloatingType(weight),\n+  TORCH_CHECK(weight.device().is_mps() && supportedFloatingOrComplexType(weight),\n               \"mps_linear_backward: unsupported weights data type: \",\n               weight.scalar_type());\n \n-  TORCH_CHECK(supportedFloatingType(grad_output), \"MPS device does not support linear backward for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(grad_output),\n+              \"MPS device does not support linear backward for non-float inputs\");\n \n   const Tensor weight_reshaped = weight.is_contiguous() ? weight : weight.contiguous();\n \n"
                },
                {
                    "old_start": 193,
                    "old_length": 7,
                    "new_start": 194,
                    "new_length": 8,
                    "hunk": "@@ -193,7 +194,8 @@ static std::tuple<Tensor, Tensor> _mps_linear_backward_weights(const Tensor& gra\n   TORCH_CHECK(grad_output.is_mps() && input.is_mps(),\n               \"_mps_linear_backward: grad_output and input needs to be mps layout\");\n \n-  TORCH_CHECK(supportedFloatingType(grad_output), \"MPS device does not support linear backward for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(grad_output),\n+              \"MPS device does not support linear backward for non-float inputs\");\n \n   struct CachedGraph : public MPSCachedGraph {\n     CachedGraph(MPSGraph* graph) : MPSCachedGraph(graph) {}\n"
                }
            ],
            "whole_deleted": "-  TORCH_CHECK(supportedFloatingType(input), \"MPS device does not support linear for non-float inputs\");\n-  TORCH_CHECK(supportedFloatingType(weight_arg), \"MPS device does not support linear for non-float weights\");\n-    TORCH_CHECK(supportedFloatingType(bias), \"MPS device does not support linear for non-float bias\");\n-  TORCH_CHECK(weight.device().is_mps() && supportedFloatingType(weight),\n-  TORCH_CHECK(supportedFloatingType(grad_output), \"MPS device does not support linear backward for non-float inputs\");\n-  TORCH_CHECK(supportedFloatingType(grad_output), \"MPS device does not support linear backward for non-float inputs\");\n",
            "whole_added": "+  TORCH_CHECK(supportedFloatingOrComplexType(input), \"MPS device does not support linear for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(weight_arg), \"MPS device does not support linear for non-float weights\");\n+    TORCH_CHECK(supportedFloatingOrComplexType(bias), \"MPS device does not support linear for non-float bias\");\n+  TORCH_CHECK(weight.device().is_mps() && supportedFloatingOrComplexType(weight),\n+  TORCH_CHECK(supportedFloatingOrComplexType(grad_output),\n+              \"MPS device does not support linear backward for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(grad_output),\n+              \"MPS device does not support linear backward for non-float inputs\");\n",
            "whole_hunk": "@@ -15,16 +15,16 @@ Tensor _mps_linear(const Tensor& input, const Tensor& weight_arg, const c10::opt\n \n   auto weight = (weight_arg.dim() == 1) ? weight_arg.view({1, weight_arg.size(0)}) : weight_arg;\n \n-  TORCH_CHECK(supportedFloatingType(input), \"MPS device does not support linear for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(input), \"MPS device does not support linear for non-float inputs\");\n   TORCH_CHECK(input.is_mps(), \"Tensor for argument input is on \", input.device(), \" but expected on mps\");\n-  TORCH_CHECK(supportedFloatingType(weight_arg), \"MPS device does not support linear for non-float weights\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(weight_arg), \"MPS device does not support linear for non-float weights\");\n   TORCH_CHECK(weight_arg.is_mps(), \"Tensor for argument weight is on \", weight_arg.device(), \" but expected on mps\");\n \n   const Tensor& bias = *(at::borrow_from_optional_tensor(bias_opt));\n   const bool is_bias_defined = bias.defined();\n   if (is_bias_defined) {\n     TORCH_CHECK(bias.is_mps(), \"Tensor for argument bias is on \", bias.device(), \" but expected on mps\");\n-    TORCH_CHECK(supportedFloatingType(bias), \"MPS device does not support linear for non-float bias\");\n+    TORCH_CHECK(supportedFloatingOrComplexType(bias), \"MPS device does not support linear for non-float bias\");\n   }\n \n   auto input_size = input.sizes();\n@@ -128,11 +128,12 @@ Tensor _mps_linear(const Tensor& input, const Tensor& weight_arg, const c10::opt\n \n static Tensor _mps_linear_backward_input(IntArrayRef input_size, const Tensor& grad_output, const Tensor& weight) {\n   TORCH_CHECK(grad_output.is_mps(), \"mps_linear_backward: grad_output needs to be mps layout\");\n-  TORCH_CHECK(weight.device().is_mps() && supportedFloatingType(weight),\n+  TORCH_CHECK(weight.device().is_mps() && supportedFloatingOrComplexType(weight),\n               \"mps_linear_backward: unsupported weights data type: \",\n               weight.scalar_type());\n \n-  TORCH_CHECK(supportedFloatingType(grad_output), \"MPS device does not support linear backward for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(grad_output),\n+              \"MPS device does not support linear backward for non-float inputs\");\n \n   const Tensor weight_reshaped = weight.is_contiguous() ? weight : weight.contiguous();\n \n@@ -193,7 +194,8 @@ static std::tuple<Tensor, Tensor> _mps_linear_backward_weights(const Tensor& gra\n   TORCH_CHECK(grad_output.is_mps() && input.is_mps(),\n               \"_mps_linear_backward: grad_output and input needs to be mps layout\");\n \n-  TORCH_CHECK(supportedFloatingType(grad_output), \"MPS device does not support linear backward for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(grad_output),\n+              \"MPS device does not support linear backward for non-float inputs\");\n \n   struct CachedGraph : public MPSCachedGraph {\n     CachedGraph(MPSGraph* graph) : MPSCachedGraph(graph) {}\n"
        },
        {
            "name": "LinearAlgebra.mm",
            "path": "aten/src/ATen/native/mps/operations/LinearAlgebra.mm",
            "patches": [
                {
                    "old_start": 131,
                    "old_length": 7,
                    "new_start": 131,
                    "new_length": 7,
                    "hunk": "@@ -131,7 +131,7 @@ static Tensor& mm_out_mps_impl(const Tensor& self, const Tensor& other, Tensor&\n   using namespace mps;\n   using CachedGraph = MPSBinaryCachedGraph;\n   TORCH_CHECK(self.dim() == 2 && other.dim() == 2, \"tensors must be 2-D\");\n-  TORCH_CHECK(supportedFloatingType(self), \"MPS device does not support mm for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(self), \"MPS device does not support mm for non-float inputs\");\n \n   TensorArg args[]{{output, \"out\", 0}, {self, \"mat1\", 1}, {other, \"mat2\", 2}};\n   checkAllSameGPU(\"mm\", args);\n"
                },
                {
                    "old_start": 185,
                    "old_length": 7,
                    "new_start": 185,
                    "new_length": 8,
                    "hunk": "@@ -185,7 +185,8 @@ static Tensor& addbmm_or_baddbmm_out_mps_impl(const Tensor& input,\n   TORCH_CHECK(batch2.is_mps());\n   TORCH_CHECK(result.is_mps());\n \n-  TORCH_CHECK(supportedFloatingType(batch1), \"MPS device does not support addbmm or baddbmm for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(batch1),\n+              \"MPS device does not support addbmm or baddbmm for non-float inputs\");\n \n   TORCH_CHECK(batch1.dim() == 3, \"batch1 must be a 3D tensor\");\n   TORCH_CHECK(batch2.dim() == 3, \"batch2 must be a 3D tensor\");\n"
                },
                {
                    "old_start": 292,
                    "old_length": 7,
                    "new_start": 293,
                    "new_length": 7,
                    "hunk": "@@ -292,7 +293,7 @@ static Tensor& addmm_out_mps_impl(const Tensor& bias,\n \n   TORCH_CHECK(output.is_mps());\n   TORCH_CHECK(self.dim() == 2 && other.dim() == 2, \"tensors must be 2-D\");\n-  TORCH_CHECK(supportedFloatingType(self), \"MPS device does not support addmm for non-float input\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(self), \"MPS device does not support addmm for non-float input\");\n \n   TensorArg args[]{{output, \"out\", 0}, {bias, \"self\", 1}, {self, \"mat1\", 2}, {other, \"mat2\", 3}};\n   checkAllSameGPU(__func__, args);\n"
                },
                {
                    "old_start": 387,
                    "old_length": 7,
                    "new_start": 388,
                    "new_length": 7,
                    "hunk": "@@ -387,7 +388,7 @@ static Tensor& addmm_out_mps_impl(const Tensor& bias,\n static Tensor& bmm_out_mps_impl(const Tensor& batch1, const Tensor& batch2, Tensor& result) {\n   using namespace mps;\n \n-  TORCH_CHECK(supportedFloatingType(batch1), \"MPS device does not support bmm for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(batch1), \"MPS device does not support bmm for non-float inputs\");\n \n   if (batch1.numel() == 0 || batch2.numel() == 0) {\n     result.zero_();\n"
                },
                {
                    "old_start": 567,
                    "old_length": 7,
                    "new_start": 568,
                    "new_length": 7,
                    "hunk": "@@ -567,7 +568,7 @@ Tensor& addr_out_mps(const Tensor& self,\n \n   TORCH_CHECK(result.is_mps());\n   TORCH_CHECK(vec1.dim() == 1 && vec2.dim() == 1, \"tensors must be 1-D\");\n-  TORCH_CHECK(supportedFloatingType(vec1), \"MPS device does not support addr for non-float input\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(vec1), \"MPS device does not support addr for non-float input\");\n \n   TensorArg args[]{{result, \"out\", 0}, {self, \"self\", 1}, {vec1, \"vec1\", 2}, {vec2, \"vec2\", 3}};\n   checkAllSameGPU(__func__, args);\n"
                }
            ],
            "whole_deleted": "-  TORCH_CHECK(supportedFloatingType(self), \"MPS device does not support mm for non-float inputs\");\n-  TORCH_CHECK(supportedFloatingType(batch1), \"MPS device does not support addbmm or baddbmm for non-float inputs\");\n-  TORCH_CHECK(supportedFloatingType(self), \"MPS device does not support addmm for non-float input\");\n-  TORCH_CHECK(supportedFloatingType(batch1), \"MPS device does not support bmm for non-float inputs\");\n-  TORCH_CHECK(supportedFloatingType(vec1), \"MPS device does not support addr for non-float input\");\n",
            "whole_added": "+  TORCH_CHECK(supportedFloatingOrComplexType(self), \"MPS device does not support mm for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(batch1),\n+              \"MPS device does not support addbmm or baddbmm for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(self), \"MPS device does not support addmm for non-float input\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(batch1), \"MPS device does not support bmm for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(vec1), \"MPS device does not support addr for non-float input\");\n",
            "whole_hunk": "@@ -131,7 +131,7 @@ static Tensor& mm_out_mps_impl(const Tensor& self, const Tensor& other, Tensor&\n   using namespace mps;\n   using CachedGraph = MPSBinaryCachedGraph;\n   TORCH_CHECK(self.dim() == 2 && other.dim() == 2, \"tensors must be 2-D\");\n-  TORCH_CHECK(supportedFloatingType(self), \"MPS device does not support mm for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(self), \"MPS device does not support mm for non-float inputs\");\n \n   TensorArg args[]{{output, \"out\", 0}, {self, \"mat1\", 1}, {other, \"mat2\", 2}};\n   checkAllSameGPU(\"mm\", args);\n@@ -185,7 +185,8 @@ static Tensor& addbmm_or_baddbmm_out_mps_impl(const Tensor& input,\n   TORCH_CHECK(batch2.is_mps());\n   TORCH_CHECK(result.is_mps());\n \n-  TORCH_CHECK(supportedFloatingType(batch1), \"MPS device does not support addbmm or baddbmm for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(batch1),\n+              \"MPS device does not support addbmm or baddbmm for non-float inputs\");\n \n   TORCH_CHECK(batch1.dim() == 3, \"batch1 must be a 3D tensor\");\n   TORCH_CHECK(batch2.dim() == 3, \"batch2 must be a 3D tensor\");\n@@ -292,7 +293,7 @@ static Tensor& addmm_out_mps_impl(const Tensor& bias,\n \n   TORCH_CHECK(output.is_mps());\n   TORCH_CHECK(self.dim() == 2 && other.dim() == 2, \"tensors must be 2-D\");\n-  TORCH_CHECK(supportedFloatingType(self), \"MPS device does not support addmm for non-float input\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(self), \"MPS device does not support addmm for non-float input\");\n \n   TensorArg args[]{{output, \"out\", 0}, {bias, \"self\", 1}, {self, \"mat1\", 2}, {other, \"mat2\", 3}};\n   checkAllSameGPU(__func__, args);\n@@ -387,7 +388,7 @@ static Tensor& addmm_out_mps_impl(const Tensor& bias,\n static Tensor& bmm_out_mps_impl(const Tensor& batch1, const Tensor& batch2, Tensor& result) {\n   using namespace mps;\n \n-  TORCH_CHECK(supportedFloatingType(batch1), \"MPS device does not support bmm for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(batch1), \"MPS device does not support bmm for non-float inputs\");\n \n   if (batch1.numel() == 0 || batch2.numel() == 0) {\n     result.zero_();\n@@ -567,7 +568,7 @@ Tensor& addr_out_mps(const Tensor& self,\n \n   TORCH_CHECK(result.is_mps());\n   TORCH_CHECK(vec1.dim() == 1 && vec2.dim() == 1, \"tensors must be 1-D\");\n-  TORCH_CHECK(supportedFloatingType(vec1), \"MPS device does not support addr for non-float input\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(vec1), \"MPS device does not support addr for non-float input\");\n \n   TensorArg args[]{{result, \"out\", 0}, {self, \"self\", 1}, {vec1, \"vec1\", 2}, {vec2, \"vec2\", 3}};\n   checkAllSameGPU(__func__, args);\n"
        },
        {
            "name": "test_mps.py",
            "path": "test/test_mps.py",
            "patches": [
                {
                    "old_start": 297,
                    "old_length": 7,
                    "new_start": 297,
                    "new_length": 9,
                    "hunk": "@@ -297,7 +297,9 @@ def mps_ops_modifier(ops):\n         'narrow',\n         'narrow_copy',\n         'nn.functional.conv1d',\n+        'nn.functional.conv2d',\n         'nn.functional.conv_transpose1d',\n+        'nn.functional.conv_transpose2d',\n         'nn.functional.feature_alpha_dropoutwithout_train',\n         'nn.functional.padcircular',\n         'nn.functional.unfold',\n"
                },
                {
                    "old_start": 347,
                    "old_length": 6,
                    "new_start": 349,
                    "new_length": 7,
                    "hunk": "@@ -347,6 +349,7 @@ def mps_ops_modifier(ops):\n \n     AFTER_MACOS_14_0_SUPPORTED_COMPLEX_OPS = {\n         '__rdiv__',\n+        '__rmatmul__',\n         '_chunk_cat',\n         'acos',\n         'acosh',\n"
                },
                {
                    "old_start": 355,
                    "old_length": 16,
                    "new_start": 358,
                    "new_length": 20,
                    "hunk": "@@ -355,16 +358,20 @@ def mps_ops_modifier(ops):\n         'any',\n         'addcdiv',\n         'addcmul',\n+        'addmmdecomposed',\n+        'addmv',\n         'asin',\n         'atan',\n         'atanh',\n         'bfloat16',\n+        'bmm',\n         'bool',\n         'cartesian_prod',\n         'cat',\n         'char',\n         'column_stack',\n         'combinations',\n+        'corrcoef',\n         'constant_pad_nd',\n         'cos',\n         'cosh',\n"
                },
                {
                    "old_start": 374,
                    "old_length": 6,
                    "new_start": 381,
                    "new_length": 7,
                    "hunk": "@@ -374,6 +381,7 @@ def mps_ops_modifier(ops):\n         'divno_rounding_mode',\n         'dot',\n         'dstack',\n+        'einsum',\n         'eq',\n         'equal',\n         'exp2',\n"
                },
                {
                    "old_start": 400,
                    "old_length": 10,
                    "new_start": 408,
                    "new_length": 13,
                    "hunk": "@@ -400,10 +408,13 @@ def mps_ops_modifier(ops):\n         'gradient',\n         'half',\n         'hstack',\n+        'inner',\n         'int',\n         'isclose',\n         'isnan',\n         'ldexp',\n+        'linalg.multi_dot',\n+        'linalg.pinv',\n         'log10',\n         'log1p',\n         'log2',\n"
                },
                {
                    "old_start": 419,
                    "old_length": 7,
                    "new_start": 430,
                    "new_length": 10,
                    "hunk": "@@ -419,7 +430,10 @@ def mps_ops_modifier(ops):\n         'masked.std',\n         'masked.sum',\n         'masked.var',\n+        'matmul',\n         'mean',\n+        'mm',\n+        'mv',\n         'ne',\n         'neg',\n         'nn.functional.padconstant',\n"
                },
                {
                    "old_start": 430,
                    "old_length": 6,
                    "new_start": 444,
                    "new_length": 7,
                    "hunk": "@@ -430,6 +444,7 @@ def mps_ops_modifier(ops):\n         'nn.functional.rms_norm',\n         'nn.functional.softsign',\n         'nn.functional.tanhshrink',\n+        'pinverse',\n         'prod',\n         'reciprocal',\n         'roll',\n"
                },
                {
                    "old_start": 447,
                    "old_length": 6,
                    "new_start": 462,
                    "new_length": 7,
                    "hunk": "@@ -447,6 +462,7 @@ def mps_ops_modifier(ops):\n         'sum_to_size',\n         'tan',\n         'tanh',\n+        'tensordot',\n         'trace',\n         'trapz',\n         'trapezoid',\n"
                },
                {
                    "old_start": 11758,
                    "old_length": 7,
                    "new_start": 11774,
                    "new_length": 7,
                    "hunk": "@@ -11758,7 +11774,7 @@ class TestConsistency(TestCaseMPS):\n     }\n \n     def _compute_tolerances(self, op, dtype):\n-        if (op.name in self.FP32_LOW_PRECISION_LIST) and dtype == torch.float32:\n+        if (op.name in self.FP32_LOW_PRECISION_LIST) and dtype in [torch.float32, torch.complex64]:\n             return (1e-4, 3e-5)\n \n         if op.name in self.FP16_LOW_PRECISION_LIST and dtype == torch.float16:"
                }
            ],
            "whole_deleted": "-        if (op.name in self.FP32_LOW_PRECISION_LIST) and dtype == torch.float32:\n",
            "whole_added": "+        'nn.functional.conv2d',\n+        'nn.functional.conv_transpose2d',\n+        '__rmatmul__',\n+        'addmmdecomposed',\n+        'addmv',\n+        'bmm',\n+        'corrcoef',\n+        'einsum',\n+        'inner',\n+        'linalg.multi_dot',\n+        'linalg.pinv',\n+        'matmul',\n+        'mm',\n+        'mv',\n+        'pinverse',\n+        'tensordot',\n+        if (op.name in self.FP32_LOW_PRECISION_LIST) and dtype in [torch.float32, torch.complex64]:\n",
            "whole_hunk": "@@ -297,7 +297,9 @@ def mps_ops_modifier(ops):\n         'narrow',\n         'narrow_copy',\n         'nn.functional.conv1d',\n+        'nn.functional.conv2d',\n         'nn.functional.conv_transpose1d',\n+        'nn.functional.conv_transpose2d',\n         'nn.functional.feature_alpha_dropoutwithout_train',\n         'nn.functional.padcircular',\n         'nn.functional.unfold',\n@@ -347,6 +349,7 @@ def mps_ops_modifier(ops):\n \n     AFTER_MACOS_14_0_SUPPORTED_COMPLEX_OPS = {\n         '__rdiv__',\n+        '__rmatmul__',\n         '_chunk_cat',\n         'acos',\n         'acosh',\n@@ -355,16 +358,20 @@ def mps_ops_modifier(ops):\n         'any',\n         'addcdiv',\n         'addcmul',\n+        'addmmdecomposed',\n+        'addmv',\n         'asin',\n         'atan',\n         'atanh',\n         'bfloat16',\n+        'bmm',\n         'bool',\n         'cartesian_prod',\n         'cat',\n         'char',\n         'column_stack',\n         'combinations',\n+        'corrcoef',\n         'constant_pad_nd',\n         'cos',\n         'cosh',\n@@ -374,6 +381,7 @@ def mps_ops_modifier(ops):\n         'divno_rounding_mode',\n         'dot',\n         'dstack',\n+        'einsum',\n         'eq',\n         'equal',\n         'exp2',\n@@ -400,10 +408,13 @@ def mps_ops_modifier(ops):\n         'gradient',\n         'half',\n         'hstack',\n+        'inner',\n         'int',\n         'isclose',\n         'isnan',\n         'ldexp',\n+        'linalg.multi_dot',\n+        'linalg.pinv',\n         'log10',\n         'log1p',\n         'log2',\n@@ -419,7 +430,10 @@ def mps_ops_modifier(ops):\n         'masked.std',\n         'masked.sum',\n         'masked.var',\n+        'matmul',\n         'mean',\n+        'mm',\n+        'mv',\n         'ne',\n         'neg',\n         'nn.functional.padconstant',\n@@ -430,6 +444,7 @@ def mps_ops_modifier(ops):\n         'nn.functional.rms_norm',\n         'nn.functional.softsign',\n         'nn.functional.tanhshrink',\n+        'pinverse',\n         'prod',\n         'reciprocal',\n         'roll',\n@@ -447,6 +462,7 @@ def mps_ops_modifier(ops):\n         'sum_to_size',\n         'tan',\n         'tanh',\n+        'tensordot',\n         'trace',\n         'trapz',\n         'trapezoid',\n@@ -11758,7 +11774,7 @@ class TestConsistency(TestCaseMPS):\n     }\n \n     def _compute_tolerances(self, op, dtype):\n-        if (op.name in self.FP32_LOW_PRECISION_LIST) and dtype == torch.float32:\n+        if (op.name in self.FP32_LOW_PRECISION_LIST) and dtype in [torch.float32, torch.complex64]:\n             return (1e-4, 3e-5)\n \n         if op.name in self.FP16_LOW_PRECISION_LIST and dtype == torch.float16:"
        }
    ]
},
{
    "Id": 399,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/5262484ecefb03da62ed36053c9a61c1ca938ede",
    "date": "2023-11-30T02:10:56+00:00",
    "message": "[easy][aotinductor] fix typos & add static typing (#114728)\n\n```\n// check all references\n$ grep -rl 'cpp_kernel_overlad_name' *\nir.py\n```\n\n```\n$ lintrunner --take MYPYINDUCTOR torch/_inductor/codegen/wrapper.py torch/_inductor/ir.py\nok No lint issues.\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114728\nApproved by: https://github.com/Skylion007, https://github.com/chenyang78",
    "label": "NO",
    "changes": [
        {
            "name": "wrapper.py",
            "path": "torch/_inductor/codegen/wrapper.py",
            "patches": [
                {
                    "old_start": 71,
                    "old_length": 7,
                    "new_start": 71,
                    "new_length": 7,
                    "hunk": "@@ -71,7 +71,7 @@ def is_float(s: str):\n     return True\n \n \n-def convert_arg_type(python_type):\n+def convert_arg_type(python_type: str):\n     from .cpp import CONTAINER_PYTHON_TO_CPP, PYTHON_TO_CPP\n \n     if python_type == \"Tensor\":\n"
                },
                {
                    "old_start": 95,
                    "old_length": 7,
                    "new_start": 95,
                    "new_length": 7,
                    "hunk": "@@ -95,7 +95,7 @@ def convert_arg_type(python_type):\n     raise AssertionError(f\"unsupport python_type: {python_type}\")\n \n \n-def convert_return_type(python_type):\n+def convert_return_type(python_type: str):\n     # TODO: support alias\n     python_to_cpp = {\n         \"Tensor\": \"at::Tensor\",\n"
                },
                {
                    "old_start": 113,
                    "old_length": 12,
                    "new_start": 113,
                    "new_length": 12,
                    "hunk": "@@ -113,12 +113,12 @@ def get_cpp_op_schema(kernel):\n     arg_names = [x.name for x in kernel._schema.arguments]\n     returns = [repr(x.real_type) for x in kernel._schema.returns]\n \n-    num_retunrs = len(returns)\n-    assert num_retunrs > 0, \"must have at least one return value\"\n+    num_returns = len(returns)\n+    assert num_returns > 0, \"must have at least one return value\"\n \n-    if num_retunrs == 1:\n+    if num_returns == 1:\n         cpp_return_value = convert_return_type(returns[0])\n-    elif num_retunrs > 1:\n+    elif num_returns > 1:\n         tuple_returns = \", \".join([convert_return_type(r) for r in returns])\n         cpp_return_value = f\"std::tuple<{tuple_returns}>\"\n \n"
                }
            ],
            "whole_deleted": "-def convert_arg_type(python_type):\n-def convert_return_type(python_type):\n-    num_retunrs = len(returns)\n-    assert num_retunrs > 0, \"must have at least one return value\"\n-    if num_retunrs == 1:\n-    elif num_retunrs > 1:\n",
            "whole_added": "+def convert_arg_type(python_type: str):\n+def convert_return_type(python_type: str):\n+    num_returns = len(returns)\n+    assert num_returns > 0, \"must have at least one return value\"\n+    if num_returns == 1:\n+    elif num_returns > 1:\n",
            "whole_hunk": "@@ -71,7 +71,7 @@ def is_float(s: str):\n     return True\n \n \n-def convert_arg_type(python_type):\n+def convert_arg_type(python_type: str):\n     from .cpp import CONTAINER_PYTHON_TO_CPP, PYTHON_TO_CPP\n \n     if python_type == \"Tensor\":\n@@ -95,7 +95,7 @@ def convert_arg_type(python_type):\n     raise AssertionError(f\"unsupport python_type: {python_type}\")\n \n \n-def convert_return_type(python_type):\n+def convert_return_type(python_type: str):\n     # TODO: support alias\n     python_to_cpp = {\n         \"Tensor\": \"at::Tensor\",\n@@ -113,12 +113,12 @@ def get_cpp_op_schema(kernel):\n     arg_names = [x.name for x in kernel._schema.arguments]\n     returns = [repr(x.real_type) for x in kernel._schema.returns]\n \n-    num_retunrs = len(returns)\n-    assert num_retunrs > 0, \"must have at least one return value\"\n+    num_returns = len(returns)\n+    assert num_returns > 0, \"must have at least one return value\"\n \n-    if num_retunrs == 1:\n+    if num_returns == 1:\n         cpp_return_value = convert_return_type(returns[0])\n-    elif num_retunrs > 1:\n+    elif num_returns > 1:\n         tuple_returns = \", \".join([convert_return_type(r) for r in returns])\n         cpp_return_value = f\"std::tuple<{tuple_returns}>\"\n \n"
        },
        {
            "name": "ir.py",
            "path": "torch/_inductor/ir.py",
            "patches": [
                {
                    "old_start": 4292,
                    "old_length": 9,
                    "new_start": 4292,
                    "new_length": 9,
                    "hunk": "@@ -4292,9 +4292,9 @@ class FallbackKernel(ExternKernelAlloc):\n         ), f\"{kernel.__name__} with alias_info returns is not supported with cpp_wrapper\"\n \n         self.cpp_kernel = kernel._schema.name\n-        self.cpp_kernel_overlad_name = kernel._schema.overload_name\n+        self.cpp_kernel_overload_name = kernel._schema.overload_name\n         self.cpp_kernel_key = (\n-            f\"{self.cpp_kernel.replace('::', '_')}_{self.cpp_kernel_overlad_name}\"\n+            f\"{self.cpp_kernel.replace('::', '_')}_{self.cpp_kernel_overload_name}\"\n         )\n \n         self.cpp_op_schema = get_cpp_op_schema(kernel)\n"
                },
                {
                    "old_start": 4523,
                    "old_length": 7,
                    "new_start": 4523,
                    "new_length": 7,
                    "hunk": "@@ -4523,7 +4523,7 @@ class FallbackKernel(ExternKernelAlloc):\n                 args,\n                 self.cpp_op_schema,\n                 self.cpp_kernel_key,\n-                self.cpp_kernel_overlad_name,\n+                self.cpp_kernel_overload_name,\n                 self.op_overload,\n                 exported_args,\n                 self.outputs,\n"
                },
                {
                    "old_start": 5000,
                    "old_length": 7,
                    "new_start": 5000,
                    "new_length": 7,
                    "hunk": "@@ -5000,7 +5000,7 @@ class ConvolutionBinary(ExternKernelAlloc):\n             kernel=\"torch.ops.mkldnn._convolution_pointwise.binary\",\n             cpp_kernel=\"mkldnn::_convolution_pointwise\",\n         )\n-        self.cpp_kernel_overlad_name = \"binary\"\n+        self.cpp_kernel_overload_name = \"binary\"\n         self.cpp_kernel_key = \"convolution_pointwise_binary\"\n         self.cpp_op_schema = \"\"\"\n             at::Tensor(\n"
                },
                {
                    "old_start": 5026,
                    "old_length": 7,
                    "new_start": 5026,
                    "new_length": 7,
                    "hunk": "@@ -5026,7 +5026,7 @@ class ConvolutionBinary(ExternKernelAlloc):\n             self.codegen_args(),\n             self.cpp_op_schema,\n             self.cpp_kernel_key,\n-            self.cpp_kernel_overlad_name,\n+            self.cpp_kernel_overload_name,\n         )\n         if isinstance(self.layout, Layout):\n             self.codegen_size_asserts(wrapper)\n"
                },
                {
                    "old_start": 5090,
                    "old_length": 7,
                    "new_start": 5090,
                    "new_length": 7,
                    "hunk": "@@ -5090,7 +5090,7 @@ class ConvolutionBinaryInplace(ExternKernelAlloc):\n             kernel=\"torch.ops.mkldnn._convolution_pointwise_.binary\",\n             cpp_kernel=\"mkldnn::_convolution_pointwise_\",\n         )\n-        self.cpp_kernel_overlad_name = \"binary\"\n+        self.cpp_kernel_overload_name = \"binary\"\n         self.cpp_kernel_key = \"convolution_pointwise_binary_\"\n         # TODO: op.call: input[0] should be at::Tensor&\n         self.cpp_op_schema = \"\"\"\n"
                },
                {
                    "old_start": 5116,
                    "old_length": 7,
                    "new_start": 5116,
                    "new_length": 7,
                    "hunk": "@@ -5116,7 +5116,7 @@ class ConvolutionBinaryInplace(ExternKernelAlloc):\n             self.codegen_args(),\n             self.cpp_op_schema,\n             self.cpp_kernel_key,\n-            self.cpp_kernel_overlad_name,\n+            self.cpp_kernel_overload_name,\n         )\n \n     def get_mutation_names(self):\n"
                },
                {
                    "old_start": 5304,
                    "old_length": 7,
                    "new_start": 5304,
                    "new_length": 7,
                    "hunk": "@@ -5304,7 +5304,7 @@ class LinearBinary(ExternKernelAlloc):\n             kernel=\"torch.ops.mkldnn._linear_pointwise.binary\",\n             cpp_kernel=\"mkldnn::_linear_pointwise\",\n         )\n-        self.cpp_kernel_overlad_name = \"binary\"\n+        self.cpp_kernel_overload_name = \"binary\"\n         self.cpp_kernel_key = \"linear_pointwise_binary\"\n         self.cpp_op_schema = \"\"\"\n             at::Tensor(\n"
                },
                {
                    "old_start": 5322,
                    "old_length": 7,
                    "new_start": 5322,
                    "new_length": 7,
                    "hunk": "@@ -5322,7 +5322,7 @@ class LinearBinary(ExternKernelAlloc):\n             self.codegen_args(),\n             self.cpp_op_schema,\n             self.cpp_kernel_key,\n-            self.cpp_kernel_overlad_name,\n+            self.cpp_kernel_overload_name,\n         )\n \n     @classmethod\n"
                },
                {
                    "old_start": 5745,
                    "old_length": 7,
                    "new_start": 5745,
                    "new_length": 7,
                    "hunk": "@@ -5745,7 +5745,7 @@ class QConvPointWiseBinaryPT2E(ExternKernelAlloc):\n             kernel=\"torch.ops.onednn.qconv2d_pointwise.binary\",\n             cpp_kernel=\"onednn::qconv2d_pointwise\",\n         )\n-        self.cpp_kernel_overlad_name = \"binary\"\n+        self.cpp_kernel_overload_name = \"binary\"\n         self.cpp_kernel_key = \"qconv2d_pointwise_binary\"\n         self.cpp_op_schema = \"\"\"\n             at::Tensor(\n"
                },
                {
                    "old_start": 5830,
                    "old_length": 7,
                    "new_start": 5830,
                    "new_length": 7,
                    "hunk": "@@ -5830,7 +5830,7 @@ class QConvPointWiseBinaryPT2E(ExternKernelAlloc):\n             conv_args,\n             self.cpp_op_schema,\n             self.cpp_kernel_key,\n-            self.cpp_kernel_overlad_name,\n+            self.cpp_kernel_overload_name,\n         )\n         if isinstance(self.layout, Layout):\n             self.codegen_size_asserts(wrapper)\n"
                },
                {
                    "old_start": 7015,
                    "old_length": 9,
                    "new_start": 7015,
                    "new_length": 9,
                    "hunk": "@@ -7015,9 +7015,9 @@ class _CollectiveKernel(FallbackKernel):\n         from .codegen.wrapper import get_cpp_op_schema\n \n         self.cpp_kernel = kernel._schema.name\n-        self.cpp_kernel_overlad_name = kernel._schema.overload_name\n+        self.cpp_kernel_overload_name = kernel._schema.overload_name\n         self.cpp_kernel_key = (\n-            f\"{self.cpp_kernel.replace('::', '_')}_{self.cpp_kernel_overlad_name}\"\n+            f\"{self.cpp_kernel.replace('::', '_')}_{self.cpp_kernel_overload_name}\"\n         )\n \n         self.cpp_op_schema = get_cpp_op_schema(kernel)"
                }
            ],
            "whole_deleted": "-        self.cpp_kernel_overlad_name = kernel._schema.overload_name\n-            f\"{self.cpp_kernel.replace('::', '_')}_{self.cpp_kernel_overlad_name}\"\n-                self.cpp_kernel_overlad_name,\n-        self.cpp_kernel_overlad_name = \"binary\"\n-            self.cpp_kernel_overlad_name,\n-        self.cpp_kernel_overlad_name = \"binary\"\n-            self.cpp_kernel_overlad_name,\n-        self.cpp_kernel_overlad_name = \"binary\"\n-            self.cpp_kernel_overlad_name,\n-        self.cpp_kernel_overlad_name = \"binary\"\n-            self.cpp_kernel_overlad_name,\n-        self.cpp_kernel_overlad_name = kernel._schema.overload_name\n-            f\"{self.cpp_kernel.replace('::', '_')}_{self.cpp_kernel_overlad_name}\"\n",
            "whole_added": "+        self.cpp_kernel_overload_name = kernel._schema.overload_name\n+            f\"{self.cpp_kernel.replace('::', '_')}_{self.cpp_kernel_overload_name}\"\n+                self.cpp_kernel_overload_name,\n+        self.cpp_kernel_overload_name = \"binary\"\n+            self.cpp_kernel_overload_name,\n+        self.cpp_kernel_overload_name = \"binary\"\n+            self.cpp_kernel_overload_name,\n+        self.cpp_kernel_overload_name = \"binary\"\n+            self.cpp_kernel_overload_name,\n+        self.cpp_kernel_overload_name = \"binary\"\n+            self.cpp_kernel_overload_name,\n+        self.cpp_kernel_overload_name = kernel._schema.overload_name\n+            f\"{self.cpp_kernel.replace('::', '_')}_{self.cpp_kernel_overload_name}\"\n",
            "whole_hunk": "@@ -4292,9 +4292,9 @@ class FallbackKernel(ExternKernelAlloc):\n         ), f\"{kernel.__name__} with alias_info returns is not supported with cpp_wrapper\"\n \n         self.cpp_kernel = kernel._schema.name\n-        self.cpp_kernel_overlad_name = kernel._schema.overload_name\n+        self.cpp_kernel_overload_name = kernel._schema.overload_name\n         self.cpp_kernel_key = (\n-            f\"{self.cpp_kernel.replace('::', '_')}_{self.cpp_kernel_overlad_name}\"\n+            f\"{self.cpp_kernel.replace('::', '_')}_{self.cpp_kernel_overload_name}\"\n         )\n \n         self.cpp_op_schema = get_cpp_op_schema(kernel)\n@@ -4523,7 +4523,7 @@ class FallbackKernel(ExternKernelAlloc):\n                 args,\n                 self.cpp_op_schema,\n                 self.cpp_kernel_key,\n-                self.cpp_kernel_overlad_name,\n+                self.cpp_kernel_overload_name,\n                 self.op_overload,\n                 exported_args,\n                 self.outputs,\n@@ -5000,7 +5000,7 @@ class ConvolutionBinary(ExternKernelAlloc):\n             kernel=\"torch.ops.mkldnn._convolution_pointwise.binary\",\n             cpp_kernel=\"mkldnn::_convolution_pointwise\",\n         )\n-        self.cpp_kernel_overlad_name = \"binary\"\n+        self.cpp_kernel_overload_name = \"binary\"\n         self.cpp_kernel_key = \"convolution_pointwise_binary\"\n         self.cpp_op_schema = \"\"\"\n             at::Tensor(\n@@ -5026,7 +5026,7 @@ class ConvolutionBinary(ExternKernelAlloc):\n             self.codegen_args(),\n             self.cpp_op_schema,\n             self.cpp_kernel_key,\n-            self.cpp_kernel_overlad_name,\n+            self.cpp_kernel_overload_name,\n         )\n         if isinstance(self.layout, Layout):\n             self.codegen_size_asserts(wrapper)\n@@ -5090,7 +5090,7 @@ class ConvolutionBinaryInplace(ExternKernelAlloc):\n             kernel=\"torch.ops.mkldnn._convolution_pointwise_.binary\",\n             cpp_kernel=\"mkldnn::_convolution_pointwise_\",\n         )\n-        self.cpp_kernel_overlad_name = \"binary\"\n+        self.cpp_kernel_overload_name = \"binary\"\n         self.cpp_kernel_key = \"convolution_pointwise_binary_\"\n         # TODO: op.call: input[0] should be at::Tensor&\n         self.cpp_op_schema = \"\"\"\n@@ -5116,7 +5116,7 @@ class ConvolutionBinaryInplace(ExternKernelAlloc):\n             self.codegen_args(),\n             self.cpp_op_schema,\n             self.cpp_kernel_key,\n-            self.cpp_kernel_overlad_name,\n+            self.cpp_kernel_overload_name,\n         )\n \n     def get_mutation_names(self):\n@@ -5304,7 +5304,7 @@ class LinearBinary(ExternKernelAlloc):\n             kernel=\"torch.ops.mkldnn._linear_pointwise.binary\",\n             cpp_kernel=\"mkldnn::_linear_pointwise\",\n         )\n-        self.cpp_kernel_overlad_name = \"binary\"\n+        self.cpp_kernel_overload_name = \"binary\"\n         self.cpp_kernel_key = \"linear_pointwise_binary\"\n         self.cpp_op_schema = \"\"\"\n             at::Tensor(\n@@ -5322,7 +5322,7 @@ class LinearBinary(ExternKernelAlloc):\n             self.codegen_args(),\n             self.cpp_op_schema,\n             self.cpp_kernel_key,\n-            self.cpp_kernel_overlad_name,\n+            self.cpp_kernel_overload_name,\n         )\n \n     @classmethod\n@@ -5745,7 +5745,7 @@ class QConvPointWiseBinaryPT2E(ExternKernelAlloc):\n             kernel=\"torch.ops.onednn.qconv2d_pointwise.binary\",\n             cpp_kernel=\"onednn::qconv2d_pointwise\",\n         )\n-        self.cpp_kernel_overlad_name = \"binary\"\n+        self.cpp_kernel_overload_name = \"binary\"\n         self.cpp_kernel_key = \"qconv2d_pointwise_binary\"\n         self.cpp_op_schema = \"\"\"\n             at::Tensor(\n@@ -5830,7 +5830,7 @@ class QConvPointWiseBinaryPT2E(ExternKernelAlloc):\n             conv_args,\n             self.cpp_op_schema,\n             self.cpp_kernel_key,\n-            self.cpp_kernel_overlad_name,\n+            self.cpp_kernel_overload_name,\n         )\n         if isinstance(self.layout, Layout):\n             self.codegen_size_asserts(wrapper)\n@@ -7015,9 +7015,9 @@ class _CollectiveKernel(FallbackKernel):\n         from .codegen.wrapper import get_cpp_op_schema\n \n         self.cpp_kernel = kernel._schema.name\n-        self.cpp_kernel_overlad_name = kernel._schema.overload_name\n+        self.cpp_kernel_overload_name = kernel._schema.overload_name\n         self.cpp_kernel_key = (\n-            f\"{self.cpp_kernel.replace('::', '_')}_{self.cpp_kernel_overlad_name}\"\n+            f\"{self.cpp_kernel.replace('::', '_')}_{self.cpp_kernel_overload_name}\"\n         )\n \n         self.cpp_op_schema = get_cpp_op_schema(kernel)"
        }
    ]
},
{
    "Id": 225,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/2cd3ef47779d9c4337dc0edba50ac84ffdce0ede",
    "date": "2024-03-30T07:32:32+00:00",
    "message": "Check scale dtype for fake_quantize_per_channel_affine_cachemask (#120987)\n\nFixes #120903\n\nScale for fake quant is assumed FP32 but not checked. If scales of double dtype are passed in, an internal error is raised: `TORCH_INTERNAL_ASSERT(!needs_dynamic_casting<func_t>::check(iter));` in aten/src/ATen/native/cpu/Loops.h\nThis PR adds a check of scale dtype.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/120987\nApproved by: https://github.com/jgong5, https://github.com/jerryzh168",
    "label": "YES",
    "changes": [
        {
            "name": "FakeQuantPerChannelAffine.cpp",
            "path": "aten/src/ATen/native/quantized/FakeQuantPerChannelAffine.cpp",
            "patches": [
                {
                    "old_start": 48,
                    "old_length": 6,
                    "new_start": 48,
                    "new_length": 8,
                    "hunk": "@@ -48,6 +48,8 @@ std::tuple<Tensor, Tensor> fake_quantize_per_channel_affine_cachemask(\n     int64_t axis,\n     int64_t quant_min,\n     int64_t quant_max) {\n+  TORCH_CHECK(scale.scalar_type() == ScalarType::Float,\n+              \"Scale must be Float, found \", scale.scalar_type());\n   TORCH_CHECK(zero_point.scalar_type() == ScalarType::Int || zero_point.scalar_type() == ScalarType::Float || zero_point.scalar_type() == ScalarType::Half,\n               \"Zero-point must be Int32, Float or Half, found \", zero_point.scalar_type());\n   TORCH_CHECK(scale.dim() == 1, \"scale should be a 1-D tensor\");\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  TORCH_CHECK(scale.scalar_type() == ScalarType::Float,\n+              \"Scale must be Float, found \", scale.scalar_type());\n",
            "whole_hunk": "@@ -48,6 +48,8 @@ std::tuple<Tensor, Tensor> fake_quantize_per_channel_affine_cachemask(\n     int64_t axis,\n     int64_t quant_min,\n     int64_t quant_max) {\n+  TORCH_CHECK(scale.scalar_type() == ScalarType::Float,\n+              \"Scale must be Float, found \", scale.scalar_type());\n   TORCH_CHECK(zero_point.scalar_type() == ScalarType::Int || zero_point.scalar_type() == ScalarType::Float || zero_point.scalar_type() == ScalarType::Half,\n               \"Zero-point must be Int32, Float or Half, found \", zero_point.scalar_type());\n   TORCH_CHECK(scale.dim() == 1, \"scale should be a 1-D tensor\");\n"
        },
        {
            "name": "test_workflow_ops.py",
            "path": "test/quantization/core/test_workflow_ops.py",
            "patches": [
                {
                    "old_start": 30,
                    "old_length": 7,
                    "new_start": 30,
                    "new_length": 7,
                    "hunk": "@@ -30,7 +30,7 @@ from hypothesis import strategies as st\n import torch.testing._internal.hypothesis_utils as hu\n hu.assert_deadline_disabled()\n from torch.testing._internal.common_cuda import TEST_CUDA\n-from torch.testing._internal.common_utils import TestCase\n+from torch.testing._internal.common_utils import TestCase, skipIfTorchDynamo\n \n # Reference method for fake quantize\n # Note: because scale/zero_point are left as float in the actual kernel, this mimics how fake_quant works for float16/64\n"
                },
                {
                    "old_start": 1013,
                    "old_length": 6,
                    "new_start": 1013,
                    "new_length": 30,
                    "hunk": "@@ -1013,6 +1013,30 @@ class TestFakeQuantizeOps(TestCase):\n                         Y, Y_prime, \"Difference found between dequant+quant_per_channel and fake_quantize_per_channel\")\n                 self.assertTrue(test_was_run)\n \n+    @skipIfTorchDynamo(\"Not a suitable test for TorchDynamo\")\n+    def test_fake_quantize_per_channel_affine_scale_dtypes(self):\n+        \"\"\"\n+        Ensure the error message is more helpful\n+        \"\"\"\n+        dtype_list = [torch.float, torch.float64, torch.bfloat16, torch.half]\n+        for scale_dtype in dtype_list:\n+            input = torch.randn(3, 4, 5, 6)\n+            scale = torch.Tensor([0.1, 0.2, 0.3, 0.4]).to(scale_dtype)\n+            zero_point = torch.tensor([1, 2, 3, 4], dtype=torch.int32)\n+            axis = 1\n+            quant_min = 0\n+            quant_max = 255\n+            if scale_dtype != torch.float:\n+                with self.assertRaises(RuntimeError):\n+                    torch.fake_quantize_per_channel_affine(\n+                        input, scale, zero_point, axis, quant_min, quant_max\n+                    )\n+            else:\n+                torch.fake_quantize_per_channel_affine(\n+                    input, scale, zero_point, axis, quant_min, quant_max\n+                )\n+\n+\n class TestFusedObsFakeQuant(TestCase):\n     @given(device=st.sampled_from(['cpu', 'cuda'] if torch.cuda.is_available() else ['cpu']),\n            symmetric_quant=st.booleans())"
                }
            ],
            "whole_deleted": "-from torch.testing._internal.common_utils import TestCase\n",
            "whole_added": "+from torch.testing._internal.common_utils import TestCase, skipIfTorchDynamo\n+    @skipIfTorchDynamo(\"Not a suitable test for TorchDynamo\")\n+    def test_fake_quantize_per_channel_affine_scale_dtypes(self):\n+        \"\"\"\n+        Ensure the error message is more helpful\n+        \"\"\"\n+        dtype_list = [torch.float, torch.float64, torch.bfloat16, torch.half]\n+        for scale_dtype in dtype_list:\n+            input = torch.randn(3, 4, 5, 6)\n+            scale = torch.Tensor([0.1, 0.2, 0.3, 0.4]).to(scale_dtype)\n+            zero_point = torch.tensor([1, 2, 3, 4], dtype=torch.int32)\n+            axis = 1\n+            quant_min = 0\n+            quant_max = 255\n+            if scale_dtype != torch.float:\n+                with self.assertRaises(RuntimeError):\n+                    torch.fake_quantize_per_channel_affine(\n+                        input, scale, zero_point, axis, quant_min, quant_max\n+                    )\n+            else:\n+                torch.fake_quantize_per_channel_affine(\n+                    input, scale, zero_point, axis, quant_min, quant_max\n+                )\n+\n+\n",
            "whole_hunk": "@@ -30,7 +30,7 @@ from hypothesis import strategies as st\n import torch.testing._internal.hypothesis_utils as hu\n hu.assert_deadline_disabled()\n from torch.testing._internal.common_cuda import TEST_CUDA\n-from torch.testing._internal.common_utils import TestCase\n+from torch.testing._internal.common_utils import TestCase, skipIfTorchDynamo\n \n # Reference method for fake quantize\n # Note: because scale/zero_point are left as float in the actual kernel, this mimics how fake_quant works for float16/64\n@@ -1013,6 +1013,30 @@ class TestFakeQuantizeOps(TestCase):\n                         Y, Y_prime, \"Difference found between dequant+quant_per_channel and fake_quantize_per_channel\")\n                 self.assertTrue(test_was_run)\n \n+    @skipIfTorchDynamo(\"Not a suitable test for TorchDynamo\")\n+    def test_fake_quantize_per_channel_affine_scale_dtypes(self):\n+        \"\"\"\n+        Ensure the error message is more helpful\n+        \"\"\"\n+        dtype_list = [torch.float, torch.float64, torch.bfloat16, torch.half]\n+        for scale_dtype in dtype_list:\n+            input = torch.randn(3, 4, 5, 6)\n+            scale = torch.Tensor([0.1, 0.2, 0.3, 0.4]).to(scale_dtype)\n+            zero_point = torch.tensor([1, 2, 3, 4], dtype=torch.int32)\n+            axis = 1\n+            quant_min = 0\n+            quant_max = 255\n+            if scale_dtype != torch.float:\n+                with self.assertRaises(RuntimeError):\n+                    torch.fake_quantize_per_channel_affine(\n+                        input, scale, zero_point, axis, quant_min, quant_max\n+                    )\n+            else:\n+                torch.fake_quantize_per_channel_affine(\n+                    input, scale, zero_point, axis, quant_min, quant_max\n+                )\n+\n+\n class TestFusedObsFakeQuant(TestCase):\n     @given(device=st.sampled_from(['cpu', 'cuda'] if torch.cuda.is_available() else ['cpu']),\n            symmetric_quant=st.booleans())"
        }
    ]
},
{
    "Id": 235,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/8013c4409f3a37452e71ebb2e2a135cc40978b3e",
    "date": "2024-03-22T20:03:38+00:00",
    "message": "[inductor] config to control whether we assume inputs are aligned (#122158)\n\n**Motivation**: https://github.com/pytorch/pytorch/issues/112771\n\n**Summary**: Inductor generates triton that assumes that inputs are going to be 16-byte aligned. If the inputs aren't aligned, Inductor clones the inputs. This PR introduces a config option to not do this: when assume_aligned_inputs=False, Inductor will _not_ pass inputs as being divisible_by_16, and Inductor will not make clones. This an can generate code that might be a bit slower, but this tradeoff can be worth it in some scenarios where you might otherwise make a lot of clones.\n\nIdeally, we could do this on a per-tensor basis. But this would be a lot of work, and attempts to add guards on storage offsets to do this automatically have run into issues: recompilations and excessive time to generate/check guards.\n\n**Tests** https://github.com/pytorch/pytorch/pull/122159 flips this to False. It didn't run through all errors, but the ones we see are all expected failures: divisible_by_16 changes; triton kernel caching fails if we call the same triton kernel multiple times (this makes sense because the first call will have unaligned inputs, but subsequent calls have aligned inputs); and some xfailed tests start passing.\n\n**Alternatives/RFC**:\n* Is this the right thing to do with cudagraphs?\n* Elias and Jason mentioned that we probably still want to make clones if we're dealing with unaligned inputs to matmuls. Is this something we should add in this config option? (In the use case I'm targeting, it seems like we don't need this optimization right now)\n\nDifferential Revision: [D55079094](https://our.internmc.facebook.com/intern/diff/D55079094)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/122158\nApproved by: https://github.com/ezyang",
    "label": "NO",
    "changes": [
        {
            "name": "test_torchinductor.py",
            "path": "test/inductor/test_torchinductor.py",
            "patches": [
                {
                    "old_start": 9395,
                    "old_length": 6,
                    "new_start": 9395,
                    "new_length": 25,
                    "hunk": "@@ -9395,6 +9395,25 @@ if HAS_GPU and RUN_GPU and not TEST_WITH_ASAN:\n             self.assertEqual(arguments_that_are_divisible_by_16_in_kernel1, (0, 1))\n             torch._dynamo.reset()\n \n+        @config.patch(assume_aligned_inputs=False)\n+        def test_config_option_dont_assume_alignment(self):\n+            def fn(x: torch.Tensor) -> torch.Tensor:\n+                return x.sin() + x.cos()\n+\n+            for offset in (0, 1, 2):\n+                base = torch.randn(64 * 64 + 64, device=GPU_TYPE)\n+                inps = torch.as_strided(base, (64, 64), (64, 1), offset)\n+                torch._dynamo.reset()\n+                kernels = self.get_kernels(fn, [inps])\n+                arguments_that_are_divisible_by_16 = (\n+                    kernels[0].triton_meta[\"configs\"][0].divisible_by_16\n+                )\n+\n+                #             NO_ALIGN ALIGN     ALIGN\n+                # def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr)\n+\n+                self.assertEqual(arguments_that_are_divisible_by_16, (1, 2))\n+\n         def test_optimize_indexing_dtype(self):\n             def fn(x: torch.Tensor) -> torch.Tensor:\n                 return aten.upsample_bilinear2d.vec(x, None, True, [2.0, 2.0])\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        @config.patch(assume_aligned_inputs=False)\n+        def test_config_option_dont_assume_alignment(self):\n+            def fn(x: torch.Tensor) -> torch.Tensor:\n+                return x.sin() + x.cos()\n+\n+            for offset in (0, 1, 2):\n+                base = torch.randn(64 * 64 + 64, device=GPU_TYPE)\n+                inps = torch.as_strided(base, (64, 64), (64, 1), offset)\n+                torch._dynamo.reset()\n+                kernels = self.get_kernels(fn, [inps])\n+                arguments_that_are_divisible_by_16 = (\n+                    kernels[0].triton_meta[\"configs\"][0].divisible_by_16\n+                )\n+\n+                #             NO_ALIGN ALIGN     ALIGN\n+                # def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr)\n+\n+                self.assertEqual(arguments_that_are_divisible_by_16, (1, 2))\n+\n",
            "whole_hunk": "@@ -9395,6 +9395,25 @@ if HAS_GPU and RUN_GPU and not TEST_WITH_ASAN:\n             self.assertEqual(arguments_that_are_divisible_by_16_in_kernel1, (0, 1))\n             torch._dynamo.reset()\n \n+        @config.patch(assume_aligned_inputs=False)\n+        def test_config_option_dont_assume_alignment(self):\n+            def fn(x: torch.Tensor) -> torch.Tensor:\n+                return x.sin() + x.cos()\n+\n+            for offset in (0, 1, 2):\n+                base = torch.randn(64 * 64 + 64, device=GPU_TYPE)\n+                inps = torch.as_strided(base, (64, 64), (64, 1), offset)\n+                torch._dynamo.reset()\n+                kernels = self.get_kernels(fn, [inps])\n+                arguments_that_are_divisible_by_16 = (\n+                    kernels[0].triton_meta[\"configs\"][0].divisible_by_16\n+                )\n+\n+                #             NO_ALIGN ALIGN     ALIGN\n+                # def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr)\n+\n+                self.assertEqual(arguments_that_are_divisible_by_16, (1, 2))\n+\n         def test_optimize_indexing_dtype(self):\n             def fn(x: torch.Tensor) -> torch.Tensor:\n                 return aten.upsample_bilinear2d.vec(x, None, True, [2.0, 2.0])\n"
        },
        {
            "name": "compile_fx.py",
            "path": "torch/_inductor/compile_fx.py",
            "patches": [
                {
                    "old_start": 805,
                    "old_length": 7,
                    "new_start": 805,
                    "new_length": 10,
                    "hunk": "@@ -805,7 +805,10 @@ def align_inputs(\n     inputs: List[torch.Tensor],\n     static_input_idxs: Sequence[int] = (),\n ):\n-    inputs_to_check = get_input_idxs_to_check(inputs, static_input_idxs)\n+    if config.assume_aligned_inputs:\n+        inputs_to_check = get_input_idxs_to_check(inputs, static_input_idxs)\n+    else:\n+        inputs_to_check = []\n     return align_inputs_from_check_idxs(model, inputs_to_check)\n \n \n"
                }
            ],
            "whole_deleted": "-    inputs_to_check = get_input_idxs_to_check(inputs, static_input_idxs)\n",
            "whole_added": "+    if config.assume_aligned_inputs:\n+        inputs_to_check = get_input_idxs_to_check(inputs, static_input_idxs)\n+    else:\n+        inputs_to_check = []\n",
            "whole_hunk": "@@ -805,7 +805,10 @@ def align_inputs(\n     inputs: List[torch.Tensor],\n     static_input_idxs: Sequence[int] = (),\n ):\n-    inputs_to_check = get_input_idxs_to_check(inputs, static_input_idxs)\n+    if config.assume_aligned_inputs:\n+        inputs_to_check = get_input_idxs_to_check(inputs, static_input_idxs)\n+    else:\n+        inputs_to_check = []\n     return align_inputs_from_check_idxs(model, inputs_to_check)\n \n \n"
        },
        {
            "name": "config.py",
            "path": "torch/_inductor/config.py",
            "patches": [
                {
                    "old_start": 463,
                    "old_length": 6,
                    "new_start": 463,
                    "new_length": 11,
                    "hunk": "@@ -463,6 +463,11 @@ use_minimal_arrayref_interface: bool = False\n # decompose some memory bound matmul/bmm to mul\n decompose_mem_bound_mm: bool = False\n \n+# assume_aligned_inputs means that we assume that inputs will be aligned; we generate\n+# code using this assumption, and clone tensors before use if they aren't aligned.\n+# In the common case, most inputs will be aligned.\n+assume_aligned_inputs: bool = True\n+\n \n # config specific to codegen/cpp.py\n class cpp:\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+# assume_aligned_inputs means that we assume that inputs will be aligned; we generate\n+# code using this assumption, and clone tensors before use if they aren't aligned.\n+# In the common case, most inputs will be aligned.\n+assume_aligned_inputs: bool = True\n+\n",
            "whole_hunk": "@@ -463,6 +463,11 @@ use_minimal_arrayref_interface: bool = False\n # decompose some memory bound matmul/bmm to mul\n decompose_mem_bound_mm: bool = False\n \n+# assume_aligned_inputs means that we assume that inputs will be aligned; we generate\n+# code using this assumption, and clone tensors before use if they aren't aligned.\n+# In the common case, most inputs will be aligned.\n+assume_aligned_inputs: bool = True\n+\n \n # config specific to codegen/cpp.py\n class cpp:\n"
        },
        {
            "name": "scheduler.py",
            "path": "torch/_inductor/scheduler.py",
            "patches": [
                {
                    "old_start": 2450,
                    "old_length": 8,
                    "new_start": 2450,
                    "new_length": 10,
                    "hunk": "@@ -2450,8 +2450,10 @@ class Scheduler:\n         self.flush()\n \n     def is_unaligned_buffer(self, buf_name):\n-        if buf_name in V.graph.graph_inputs or buf_name in V.graph.constants:\n-            # all graph inputs or constants are assumed to be aligned\n+        if buf_name in V.graph.graph_inputs:\n+            return not config.assume_aligned_inputs\n+        if buf_name in V.graph.constants:\n+            # all constants are assumed to be aligned\n             return False\n         node = self.name_to_node[buf_name]\n         layout = node.node.get_layout()"
                }
            ],
            "whole_deleted": "-        if buf_name in V.graph.graph_inputs or buf_name in V.graph.constants:\n-            # all graph inputs or constants are assumed to be aligned\n",
            "whole_added": "+        if buf_name in V.graph.graph_inputs:\n+            return not config.assume_aligned_inputs\n+        if buf_name in V.graph.constants:\n+            # all constants are assumed to be aligned\n",
            "whole_hunk": "@@ -2450,8 +2450,10 @@ class Scheduler:\n         self.flush()\n \n     def is_unaligned_buffer(self, buf_name):\n-        if buf_name in V.graph.graph_inputs or buf_name in V.graph.constants:\n-            # all graph inputs or constants are assumed to be aligned\n+        if buf_name in V.graph.graph_inputs:\n+            return not config.assume_aligned_inputs\n+        if buf_name in V.graph.constants:\n+            # all constants are assumed to be aligned\n             return False\n         node = self.name_to_node[buf_name]\n         layout = node.node.get_layout()"
        }
    ]
},
{
    "Id": 176,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/394ec2da300e8933d8184ba862daed3a115d7fd9",
    "date": "2024-05-03T04:51:10+00:00",
    "message": "Remove GPU Check from Basic Chrome Trace test (#125430)\n\nSummary: Remove the check to make sure all GPU labels are enumerated when CUDA is available. There are some systems where CUDA is available but we do not print any GPU labels (because GPU is not available).\n\nTest Plan: Test in regression with ciflow/periodic label\n\nDifferential Revision: D56906893\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125430\nApproved by: https://github.com/izaitsevfb",
    "label": "YES",
    "changes": [
        {
            "name": "test_profiler.py",
            "path": "test/profiler/test_profiler.py",
            "patches": [
                {
                    "old_start": 1702,
                    "old_length": 9,
                    "new_start": 1702,
                    "new_length": 7,
                    "hunk": "@@ -1702,9 +1702,7 @@ assert KinetoStepTracker.current_step() == initial_step + 2 * niters\n                     == 0x1000000 + int(gpu_value.split()[1])\n                 )\n \n-        # only check that gpu labels are present if cuda available\n-        if cuda_available:\n-            self._check_all_gpu_present(gpu_dict, MAX_GPU_COUNT)\n+        # TODO add checking gpu count if cpuOnly_ is true or not\n \n     def _test_chrome_trace_basic_helper(self, with_cuda=False):\n         if with_cuda:"
                }
            ],
            "whole_deleted": "-        # only check that gpu labels are present if cuda available\n-        if cuda_available:\n-            self._check_all_gpu_present(gpu_dict, MAX_GPU_COUNT)\n",
            "whole_added": "+        # TODO add checking gpu count if cpuOnly_ is true or not\n",
            "whole_hunk": "@@ -1702,9 +1702,7 @@ assert KinetoStepTracker.current_step() == initial_step + 2 * niters\n                     == 0x1000000 + int(gpu_value.split()[1])\n                 )\n \n-        # only check that gpu labels are present if cuda available\n-        if cuda_available:\n-            self._check_all_gpu_present(gpu_dict, MAX_GPU_COUNT)\n+        # TODO add checking gpu count if cpuOnly_ is true or not\n \n     def _test_chrome_trace_basic_helper(self, with_cuda=False):\n         if with_cuda:"
        }
    ]
},
{
    "Id": 536,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/283ce12aa9c4ab0f734dba63007b2c6de7c655b8",
    "date": "2023-08-30T02:53:30+00:00",
    "message": "Add channels_last3d support for mkldnn conv and mkldnn deconv (#95271)\n\n### Motivation\n\n- Add channels_last3d support for mkldnn conv and mkldnn deconv.\n- Use `ideep::convolution_transpose_forward::compute_v3` instead of `ideep::convolution_transpose_forward::compute`.  compute_v3 uses `is_channels_last` to notify ideep whether to go CL or not to align with the memory format check of PyTorch.\n\n### Testing\n1 socket (28 cores):\n\n- memory format: torch.contiguous_format\n\nmodule | shape | forward / ms | backward / ms\n-- | -- | -- | --\nconv3d | input size: (32, 32, 10, 100, 100), weight size: (32, 32, 3, 3, 3) | 64.56885 | 150.1796\nconv3d | input size: (32, 16, 10, 200, 200), weight size: (16, 16, 3, 3, 3) | 100.6754 | 231.8883\nconv3d | input size: (16, 4, 5, 300, 300), weight size: (4, 4, 3, 3, 3) | 19.31751 | 68.31131\n\nmodule | shape | forward / ms | backward / ms\n-- | -- | -- | --\nConvTranspose3d | input size: (32, 32, 10, 100, 100), weight size: (32, 32, 3, 3, 3) | 122.7646 | 207.5125\nConvTranspose3d | input size: (32, 16, 10, 200, 200), weight size: (16, 16, 3, 3, 3) | 202.4542 | 368.5492\nConvTranspose3d | input size: (16, 4, 5, 300, 300), weight size: (4, 4, 3, 3, 3) | 122.959 | 84.62577\n\n- memory format: torch.channels_last_3d\n\nmodule | shape | forward / ms | backward / ms\n-- | -- | -- | --\nconv3d | input size: (32, 32, 10, 100, 100), weight size: (32, 32, 3, 3, 3) | 40.06993 | 114.317\nconv3d | input size: (32, 16, 10, 200, 200), weight size: (16, 16, 3, 3, 3 | 49.08249 | 133.4079\nconv3d | input size: (16, 4, 5, 300, 300), weight size: (4, 4, 3, 3, 3) | 5.873911 | 17.58647\n\nmodule | shape | forward / ms | backward / ms\n-- | -- | -- | --\nConvTranspose3d | input size: (32, 32, 10, 100, 100), weight size: (32, 32, 3, 3, 3) | 88.4246 | 208.2269\nConvTranspose3d | input size: (32, 16, 10, 200, 200), weight size: (16, 16, 3, 3, 3 | 140.0725 | 270.4172\nConvTranspose3d | input size: (16, 4, 5, 300, 300), weight size: (4, 4, 3, 3, 3) | 23.0223 | 37.16972\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/95271\nApproved by: https://github.com/jgong5, https://github.com/cpuhrsch",
    "label": "NO",
    "changes": [
        {
            "name": "ConvUtils.h",
            "path": "aten/src/ATen/native/ConvUtils.h",
            "patches": [
                {
                    "old_start": 389,
                    "old_length": 8,
                    "new_start": 389,
                    "new_length": 9,
                    "hunk": "@@ -389,8 +389,9 @@ static inline bool mkldnn_conv_use_channels_last(const at::Tensor& input, const\n       (input_memory_format  == at::MemoryFormat::ChannelsLast) ||\n       (weight_memory_format == at::MemoryFormat::ChannelsLast);\n \n-  // TODO: add channels last 3d support\n-  bool can_use_mkldnn_channels_last_3d = false;\n+  bool can_use_mkldnn_channels_last_3d =\n+      (input_memory_format  == at::MemoryFormat::ChannelsLast3d) ||\n+      (weight_memory_format == at::MemoryFormat::ChannelsLast3d);\n \n   return can_use_mkldnn_channels_last_2d || can_use_mkldnn_channels_last_3d;\n }\n"
                }
            ],
            "whole_deleted": "-  // TODO: add channels last 3d support\n-  bool can_use_mkldnn_channels_last_3d = false;\n",
            "whole_added": "+  bool can_use_mkldnn_channels_last_3d =\n+      (input_memory_format  == at::MemoryFormat::ChannelsLast3d) ||\n+      (weight_memory_format == at::MemoryFormat::ChannelsLast3d);\n",
            "whole_hunk": "@@ -389,8 +389,9 @@ static inline bool mkldnn_conv_use_channels_last(const at::Tensor& input, const\n       (input_memory_format  == at::MemoryFormat::ChannelsLast) ||\n       (weight_memory_format == at::MemoryFormat::ChannelsLast);\n \n-  // TODO: add channels last 3d support\n-  bool can_use_mkldnn_channels_last_3d = false;\n+  bool can_use_mkldnn_channels_last_3d =\n+      (input_memory_format  == at::MemoryFormat::ChannelsLast3d) ||\n+      (weight_memory_format == at::MemoryFormat::ChannelsLast3d);\n \n   return can_use_mkldnn_channels_last_2d || can_use_mkldnn_channels_last_3d;\n }\n"
        },
        {
            "name": "Convolution.cpp",
            "path": "aten/src/ATen/native/Convolution.cpp",
            "patches": [
                {
                    "old_start": 508,
                    "old_length": 9,
                    "new_start": 508,
                    "new_length": 6,
                    "hunk": "@@ -508,9 +508,6 @@ struct ConvParams {\n     if (transposed && is_output_padding_big()) {\n       return false;\n     }\n-    if (transposed && groups > 1 && at::symint::size<T>(input, 1) == groups) {\n-      return false;\n-    }\n     if (input.device().is_cpu() && input.scalar_type() == kBFloat16 && mkldnn_bf16_device_check()) {\n       return true;\n     }\n"
                }
            ],
            "whole_deleted": "-    if (transposed && groups > 1 && at::symint::size<T>(input, 1) == groups) {\n-      return false;\n-    }\n",
            "whole_added": "",
            "whole_hunk": "@@ -508,9 +508,6 @@ struct ConvParams {\n     if (transposed && is_output_padding_big()) {\n       return false;\n     }\n-    if (transposed && groups > 1 && at::symint::size<T>(input, 1) == groups) {\n-      return false;\n-    }\n     if (input.device().is_cpu() && input.scalar_type() == kBFloat16 && mkldnn_bf16_device_check()) {\n       return true;\n     }\n"
        },
        {
            "name": "Conv.cpp",
            "path": "aten/src/ATen/native/mkldnn/Conv.cpp",
            "patches": [
                {
                    "old_start": 727,
                    "old_length": 7,
                    "new_start": 727,
                    "new_length": 7,
                    "hunk": "@@ -727,7 +727,7 @@ Tensor _mkldnn_convolution_transpose(\n \n   if (bias.defined()) {\n     const ideep::tensor b = itensor_from_tensor(bias);\n-    ideep::convolution_transpose_forward::compute(\n+    ideep::convolution_transpose_forward::compute_v3(\n         x,\n         w,\n         b,\n"
                },
                {
                    "old_start": 738,
                    "old_length": 9,
                    "new_start": 738,
                    "new_length": 10,
                    "hunk": "@@ -738,9 +738,10 @@ Tensor _mkldnn_convolution_transpose(\n         padding_r(padding_expanded, output_padding_expanded),\n         dilation.vec(),\n         groups,\n+        use_channels_last,\n         op_attr);\n   } else {\n-    ideep::convolution_transpose_forward::compute(\n+    ideep::convolution_transpose_forward::compute_v3(\n         x,\n         w,\n         output_sizes,\n"
                },
                {
                    "old_start": 750,
                    "old_length": 6,
                    "new_start": 751,
                    "new_length": 7,
                    "hunk": "@@ -750,6 +751,7 @@ Tensor _mkldnn_convolution_transpose(\n         padding_r(padding_expanded, output_padding_expanded),\n         dilation.vec(),\n         groups,\n+        use_channels_last,\n         op_attr);\n   }\n   if (input.is_mkldnn()) {\n"
                },
                {
                    "old_start": 988,
                    "old_length": 7,
                    "new_start": 990,
                    "new_length": 7,
                    "hunk": "@@ -988,7 +990,7 @@ Tensor mkldnn_convolution_transpose_backward_input(\n     grad_input.resize_(input_size, memory_format);\n     grad_x = itensor_from_tensor(grad_input);\n   }\n-  ideep::convolution_transpose_backward_data::compute(\n+  ideep::convolution_transpose_backward_data::compute_v3(\n       grad_y,\n       w,\n       input_size.vec(),\n"
                },
                {
                    "old_start": 997,
                    "old_length": 7,
                    "new_start": 999,
                    "new_length": 8,
                    "hunk": "@@ -997,7 +999,8 @@ Tensor mkldnn_convolution_transpose_backward_input(\n       padding.vec(),\n       padding_r(padding, output_padding),\n       dilation.vec(),\n-      groups);\n+      groups,\n+      is_channels_last);\n \n   if (grad_output.is_mkldnn()) {\n     return MKLDNNTensor(grad_x, grad_output.options());\n"
                },
                {
                    "old_start": 1024,
                    "old_length": 7,
                    "new_start": 1027,
                    "new_length": 7,
                    "hunk": "@@ -1024,7 +1027,7 @@ std::tuple<Tensor,Tensor> mkldnn_convolution_transpose_backward_weights(\n \n   ideep::tensor grad_w, grad_b;\n   if (bias_defined) {\n-    ideep::convolution_transpose_backward_weights::compute(\n+    ideep::convolution_transpose_backward_weights::compute_v3(\n         x,\n         grad_y,\n         weight_size.vec(),\n"
                },
                {
                    "old_start": 1034,
                    "old_length": 9,
                    "new_start": 1037,
                    "new_length": 10,
                    "hunk": "@@ -1034,9 +1037,10 @@ std::tuple<Tensor,Tensor> mkldnn_convolution_transpose_backward_weights(\n         padding.vec(),\n         padding_r(padding, output_padding),\n         dilation.vec(),\n-        groups);\n+        groups,\n+        is_channels_last);\n   } else {\n-    ideep::convolution_transpose_backward_weights::compute(\n+    ideep::convolution_transpose_backward_weights::compute_v3(\n         x,\n         grad_y,\n         weight_size.vec(),\n"
                },
                {
                    "old_start": 1045,
                    "old_length": 7,
                    "new_start": 1049,
                    "new_length": 8,
                    "hunk": "@@ -1045,7 +1049,8 @@ std::tuple<Tensor,Tensor> mkldnn_convolution_transpose_backward_weights(\n         padding.vec(),\n         padding_r(padding, output_padding),\n         dilation.vec(),\n-        groups);\n+        groups,\n+        is_channels_last);\n   }\n \n   if (!is_channels_last) {\n"
                },
                {
                    "old_start": 1061,
                    "old_length": 13,
                    "new_start": 1066,
                    "new_length": 15,
                    "hunk": "@@ -1061,13 +1066,15 @@ std::tuple<Tensor,Tensor> mkldnn_convolution_transpose_backward_weights(\n }\n \n std::tuple<Tensor, Tensor, Tensor> mkldnn_convolution_transpose_backward(\n-    const Tensor& input, const Tensor& grad_output_t, const Tensor& weight,\n+    const Tensor& input_t, const Tensor& grad_output_t, const Tensor& weight_t,\n     IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups,\n     std::array<bool,3> output_mask)\n {\n-  bool is_channels_last = mkldnn_conv_use_channels_last(input, weight);\n-  auto memory_format = mkldnn_convolution_memory_format(input.ndimension(), is_channels_last);\n+  bool is_channels_last = mkldnn_conv_use_channels_last(input_t, weight_t);\n+  auto memory_format = mkldnn_convolution_memory_format(input_t.ndimension(), is_channels_last);\n   Tensor grad_output = grad_output_t.is_mkldnn() ? grad_output_t : grad_output_t.contiguous(memory_format);\n+  auto input = input_t.is_mkldnn() ? input_t : input_t.contiguous(memory_format);\n+  auto weight = weight_t.is_mkldnn() ? weight_t : weight_t.contiguous(memory_format);\n   int64_t dim = input.ndimension() - 2;\n   const auto padding_expanded = expand_param_if_needed(padding, \"padding\", dim);\n   const auto stride_expanded = expand_param_if_needed(stride, \"stride\", dim);\n"
                }
            ],
            "whole_deleted": "-    ideep::convolution_transpose_forward::compute(\n-    ideep::convolution_transpose_forward::compute(\n-  ideep::convolution_transpose_backward_data::compute(\n-      groups);\n-    ideep::convolution_transpose_backward_weights::compute(\n-        groups);\n-    ideep::convolution_transpose_backward_weights::compute(\n-        groups);\n-    const Tensor& input, const Tensor& grad_output_t, const Tensor& weight,\n-  bool is_channels_last = mkldnn_conv_use_channels_last(input, weight);\n-  auto memory_format = mkldnn_convolution_memory_format(input.ndimension(), is_channels_last);\n",
            "whole_added": "+    ideep::convolution_transpose_forward::compute_v3(\n+        use_channels_last,\n+    ideep::convolution_transpose_forward::compute_v3(\n+        use_channels_last,\n+  ideep::convolution_transpose_backward_data::compute_v3(\n+      groups,\n+      is_channels_last);\n+    ideep::convolution_transpose_backward_weights::compute_v3(\n+        groups,\n+        is_channels_last);\n+    ideep::convolution_transpose_backward_weights::compute_v3(\n+        groups,\n+        is_channels_last);\n+    const Tensor& input_t, const Tensor& grad_output_t, const Tensor& weight_t,\n+  bool is_channels_last = mkldnn_conv_use_channels_last(input_t, weight_t);\n+  auto memory_format = mkldnn_convolution_memory_format(input_t.ndimension(), is_channels_last);\n+  auto input = input_t.is_mkldnn() ? input_t : input_t.contiguous(memory_format);\n+  auto weight = weight_t.is_mkldnn() ? weight_t : weight_t.contiguous(memory_format);\n",
            "whole_hunk": "@@ -727,7 +727,7 @@ Tensor _mkldnn_convolution_transpose(\n \n   if (bias.defined()) {\n     const ideep::tensor b = itensor_from_tensor(bias);\n-    ideep::convolution_transpose_forward::compute(\n+    ideep::convolution_transpose_forward::compute_v3(\n         x,\n         w,\n         b,\n@@ -738,9 +738,10 @@ Tensor _mkldnn_convolution_transpose(\n         padding_r(padding_expanded, output_padding_expanded),\n         dilation.vec(),\n         groups,\n+        use_channels_last,\n         op_attr);\n   } else {\n-    ideep::convolution_transpose_forward::compute(\n+    ideep::convolution_transpose_forward::compute_v3(\n         x,\n         w,\n         output_sizes,\n@@ -750,6 +751,7 @@ Tensor _mkldnn_convolution_transpose(\n         padding_r(padding_expanded, output_padding_expanded),\n         dilation.vec(),\n         groups,\n+        use_channels_last,\n         op_attr);\n   }\n   if (input.is_mkldnn()) {\n@@ -988,7 +990,7 @@ Tensor mkldnn_convolution_transpose_backward_input(\n     grad_input.resize_(input_size, memory_format);\n     grad_x = itensor_from_tensor(grad_input);\n   }\n-  ideep::convolution_transpose_backward_data::compute(\n+  ideep::convolution_transpose_backward_data::compute_v3(\n       grad_y,\n       w,\n       input_size.vec(),\n@@ -997,7 +999,8 @@ Tensor mkldnn_convolution_transpose_backward_input(\n       padding.vec(),\n       padding_r(padding, output_padding),\n       dilation.vec(),\n-      groups);\n+      groups,\n+      is_channels_last);\n \n   if (grad_output.is_mkldnn()) {\n     return MKLDNNTensor(grad_x, grad_output.options());\n@@ -1024,7 +1027,7 @@ std::tuple<Tensor,Tensor> mkldnn_convolution_transpose_backward_weights(\n \n   ideep::tensor grad_w, grad_b;\n   if (bias_defined) {\n-    ideep::convolution_transpose_backward_weights::compute(\n+    ideep::convolution_transpose_backward_weights::compute_v3(\n         x,\n         grad_y,\n         weight_size.vec(),\n@@ -1034,9 +1037,10 @@ std::tuple<Tensor,Tensor> mkldnn_convolution_transpose_backward_weights(\n         padding.vec(),\n         padding_r(padding, output_padding),\n         dilation.vec(),\n-        groups);\n+        groups,\n+        is_channels_last);\n   } else {\n-    ideep::convolution_transpose_backward_weights::compute(\n+    ideep::convolution_transpose_backward_weights::compute_v3(\n         x,\n         grad_y,\n         weight_size.vec(),\n@@ -1045,7 +1049,8 @@ std::tuple<Tensor,Tensor> mkldnn_convolution_transpose_backward_weights(\n         padding.vec(),\n         padding_r(padding, output_padding),\n         dilation.vec(),\n-        groups);\n+        groups,\n+        is_channels_last);\n   }\n \n   if (!is_channels_last) {\n@@ -1061,13 +1066,15 @@ std::tuple<Tensor,Tensor> mkldnn_convolution_transpose_backward_weights(\n }\n \n std::tuple<Tensor, Tensor, Tensor> mkldnn_convolution_transpose_backward(\n-    const Tensor& input, const Tensor& grad_output_t, const Tensor& weight,\n+    const Tensor& input_t, const Tensor& grad_output_t, const Tensor& weight_t,\n     IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups,\n     std::array<bool,3> output_mask)\n {\n-  bool is_channels_last = mkldnn_conv_use_channels_last(input, weight);\n-  auto memory_format = mkldnn_convolution_memory_format(input.ndimension(), is_channels_last);\n+  bool is_channels_last = mkldnn_conv_use_channels_last(input_t, weight_t);\n+  auto memory_format = mkldnn_convolution_memory_format(input_t.ndimension(), is_channels_last);\n   Tensor grad_output = grad_output_t.is_mkldnn() ? grad_output_t : grad_output_t.contiguous(memory_format);\n+  auto input = input_t.is_mkldnn() ? input_t : input_t.contiguous(memory_format);\n+  auto weight = weight_t.is_mkldnn() ? weight_t : weight_t.contiguous(memory_format);\n   int64_t dim = input.ndimension() - 2;\n   const auto padding_expanded = expand_param_if_needed(padding, \"padding\", dim);\n   const auto stride_expanded = expand_param_if_needed(stride, \"stride\", dim);\n"
        },
        {
            "name": "test_mkldnn.py",
            "path": "test/test_mkldnn.py",
            "patches": [
                {
                    "old_start": 321,
                    "old_length": 22,
                    "new_start": 321,
                    "new_length": 25,
                    "hunk": "@@ -321,22 +321,25 @@ class TestMkldnn(TestCase):\n     def test_conv3d_bf16(self):\n         self._test_conv_bf16_base(dim=3)\n \n-    def _test_conv2d_nhwc_base(self, conv_module, weight_memory_format, dtype):\n-        input_shapes = (55, 55)\n+    def _test_conv_deconv_nhwc_base(self, conv_module, weight_memory_format, dtype, prec=None):\n+        input_shapes = {2: (55, 55), 3: (14, 14, 14)}\n         options = itertools.product([True, False], [True, False], [1, 2], [1, 4])\n+        if conv_module in [torch.nn.Conv2d, torch.nn.ConvTranspose2d]:\n+            cl_format = torch.channels_last\n+            input_shape = input_shapes[2]\n+        elif conv_module in [torch.nn.Conv3d, torch.nn.ConvTranspose3d]:\n+            cl_format = torch.channels_last_3d\n+            input_shape = input_shapes[3]\n+\n         for train, bias, dilation, groups in options:\n             N = torch.randint(3, 10, (1,)).item()\n             M = torch.randint(1, 3, (1,)).item() * groups\n             C = torch.randint(1, 3, (1,)).item() * groups\n-            x_shape = (N, C) + input_shapes\n+            x_shape = (N, C) + input_shape\n             x = torch.randn(x_shape, dtype=dtype)\n \n-            # TODO: remove this when group depthwise is supported:\n-            if conv_module is torch.nn.ConvTranspose2d and groups > 1 and C == groups:\n-                continue\n-\n-            # conv1: mkldnn conv in contiguous memory format (nchw)\n-            # conv2: mkldnn conv in channels last memory format (nhwc)\n+            # conv1: mkldnn conv/deconv in contiguous memory format (nchw)\n+            # conv2: mkldnn conv/deconv in channels last memory format (nhwc)\n             conv1 = conv_module(in_channels=C,\n                                 out_channels=M,\n                                 kernel_size=3,\n"
                },
                {
                    "old_start": 347,
                    "old_length": 46,
                    "new_start": 350,
                    "new_length": 67,
                    "hunk": "@@ -347,46 +350,67 @@ class TestMkldnn(TestCase):\n                                 groups=groups).to(dtype=dtype)\n             conv2 = copy.deepcopy(conv1).to(memory_format=weight_memory_format)\n             x1 = x.clone()\n-            x2 = x.clone().to(memory_format=torch.channels_last)\n+            x2 = x.clone().to(memory_format=cl_format)\n             if train:\n                 x1.requires_grad_()\n                 x2.requires_grad_()\n             y1 = conv1(x1)\n             y2 = conv2(x2)\n-            self.assertEqual(y1, y2)\n+            self.assertEqual(y1, y2, atol=prec, rtol=prec)\n+\n             if train:\n                 y1.sum().backward()\n                 y2.sum().backward()\n-                self.assertTrue(x2.grad.is_contiguous(memory_format=torch.channels_last))\n+                self.assertTrue(x2.grad.is_contiguous(memory_format=cl_format))\n                 self.assertEqual(conv1.weight.grad,\n                                  conv2.weight.grad,\n                                  atol=1e-3,\n                                  rtol=1e-3)\n                 if bias:\n-                    self.assertEqual(conv1.bias.grad, conv2.bias.grad)\n-                self.assertEqual(x1.grad, x2.grad)\n+                    self.assertEqual(conv1.bias.grad, conv2.bias.grad, atol=prec, rtol=prec)\n+                self.assertEqual(x1.grad, x2.grad, atol=prec, rtol=prec)\n \n-    def test_conv2d_nhwc(self):\n-        self._test_conv2d_nhwc_base(torch.nn.Conv2d, torch.contiguous_format, dtype=torch.float32)\n-        self._test_conv2d_nhwc_base(torch.nn.Conv2d, torch.channels_last, dtype=torch.float32)\n+    def test_conv_nhwc_fp32(self):\n+        self._test_conv_deconv_nhwc_base(torch.nn.Conv2d, torch.contiguous_format, dtype=torch.float32)\n+        self._test_conv_deconv_nhwc_base(torch.nn.Conv2d, torch.channels_last, dtype=torch.float32)\n+        self._test_conv_deconv_nhwc_base(torch.nn.Conv3d, torch.contiguous_format, dtype=torch.float32)\n+        self._test_conv_deconv_nhwc_base(torch.nn.Conv3d, torch.channels_last_3d, dtype=torch.float32)\n \n     @unittest.skipIf(IS_WINDOWS, \"Limit support for bf16 path\")\n-    def test_conv2d_nhwc_bf16(self):\n+    def test_conv_nhwc_bf16(self):\n         # when torch.ops.mkldnn._is_mkldnn_bf16_supported() returns false, bf16 CPU conv will fall back to thnn impl\n         if torch.ops.mkldnn._is_mkldnn_bf16_supported():\n-            self._test_conv2d_nhwc_base(torch.nn.Conv2d, torch.contiguous_format, dtype=torch.bfloat16)\n-            self._test_conv2d_nhwc_base(torch.nn.Conv2d, torch.channels_last, dtype=torch.bfloat16)\n-\n-    def test_conv_transpose2d_nhwc(self):\n-        self._test_conv2d_nhwc_base(torch.nn.ConvTranspose2d, torch.contiguous_format, dtype=torch.float32)\n-        self._test_conv2d_nhwc_base(torch.nn.ConvTranspose2d, torch.channels_last, dtype=torch.float32)\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv2d, torch.contiguous_format, dtype=torch.bfloat16)\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv2d, torch.channels_last, dtype=torch.bfloat16)\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv3d, torch.contiguous_format, dtype=torch.bfloat16)\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv3d, torch.channels_last_3d, dtype=torch.bfloat16)\n+        # test fall back to thnn impl\n+        with torch.backends.mkldnn.flags(enabled=False):\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv2d, torch.contiguous_format, dtype=torch.bfloat16, prec=1e-2)\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv2d, torch.channels_last, dtype=torch.bfloat16, prec=1e-2)\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv3d, torch.contiguous_format, dtype=torch.bfloat16, prec=1e-3)\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv3d, torch.channels_last_3d, dtype=torch.bfloat16, prec=1e-3)\n+\n+    def test_conv_transpose_nhwc_fp32(self):\n+        self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose2d, torch.contiguous_format, dtype=torch.float32)\n+        self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose2d, torch.channels_last, dtype=torch.float32)\n+        self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose3d, torch.contiguous_format, dtype=torch.float32)\n+        self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose3d, torch.channels_last_3d, dtype=torch.float32)\n \n     @unittest.skipIf(IS_WINDOWS, \"Limit support for bf16 path\")\n-    def test_conv_transpose2d_nhwc_bf16(self):\n+    def test_conv_transpose_nhwc_bf16(self):\n         # when torch.ops.mkldnn._is_mkldnn_bf16_supported() returns false, bf16 CPU conv will fall back to thnn impl\n         if torch.ops.mkldnn._is_mkldnn_bf16_supported():\n-            self._test_conv2d_nhwc_base(torch.nn.ConvTranspose2d, torch.contiguous_format, dtype=torch.bfloat16)\n-            self._test_conv2d_nhwc_base(torch.nn.ConvTranspose2d, torch.channels_last, dtype=torch.bfloat16)\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose2d, torch.contiguous_format, dtype=torch.bfloat16)\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose2d, torch.channels_last, dtype=torch.bfloat16)\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose3d, torch.contiguous_format, dtype=torch.bfloat16)\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose3d, torch.channels_last_3d, dtype=torch.bfloat16)\n+        # test fall back to thnn impl\n+        with torch.backends.mkldnn.flags(enabled=False):\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose2d, torch.contiguous_format, dtype=torch.bfloat16, prec=2e-2)\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose2d, torch.channels_last, dtype=torch.bfloat16, prec=2e-2)\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose3d, torch.contiguous_format, dtype=torch.bfloat16, prec=1e-3)\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose3d, torch.channels_last_3d, dtype=torch.bfloat16, prec=1e-3)\n \n     def _test_conv_transpose_base(self, dim):\n         conv_module = {\n"
                }
            ],
            "whole_deleted": "-    def _test_conv2d_nhwc_base(self, conv_module, weight_memory_format, dtype):\n-        input_shapes = (55, 55)\n-            x_shape = (N, C) + input_shapes\n-            # TODO: remove this when group depthwise is supported:\n-            if conv_module is torch.nn.ConvTranspose2d and groups > 1 and C == groups:\n-                continue\n-\n-            # conv1: mkldnn conv in contiguous memory format (nchw)\n-            # conv2: mkldnn conv in channels last memory format (nhwc)\n-            x2 = x.clone().to(memory_format=torch.channels_last)\n-            self.assertEqual(y1, y2)\n-                self.assertTrue(x2.grad.is_contiguous(memory_format=torch.channels_last))\n-                    self.assertEqual(conv1.bias.grad, conv2.bias.grad)\n-                self.assertEqual(x1.grad, x2.grad)\n-    def test_conv2d_nhwc(self):\n-        self._test_conv2d_nhwc_base(torch.nn.Conv2d, torch.contiguous_format, dtype=torch.float32)\n-        self._test_conv2d_nhwc_base(torch.nn.Conv2d, torch.channels_last, dtype=torch.float32)\n-    def test_conv2d_nhwc_bf16(self):\n-            self._test_conv2d_nhwc_base(torch.nn.Conv2d, torch.contiguous_format, dtype=torch.bfloat16)\n-            self._test_conv2d_nhwc_base(torch.nn.Conv2d, torch.channels_last, dtype=torch.bfloat16)\n-\n-    def test_conv_transpose2d_nhwc(self):\n-        self._test_conv2d_nhwc_base(torch.nn.ConvTranspose2d, torch.contiguous_format, dtype=torch.float32)\n-        self._test_conv2d_nhwc_base(torch.nn.ConvTranspose2d, torch.channels_last, dtype=torch.float32)\n-    def test_conv_transpose2d_nhwc_bf16(self):\n-            self._test_conv2d_nhwc_base(torch.nn.ConvTranspose2d, torch.contiguous_format, dtype=torch.bfloat16)\n-            self._test_conv2d_nhwc_base(torch.nn.ConvTranspose2d, torch.channels_last, dtype=torch.bfloat16)\n",
            "whole_added": "+    def _test_conv_deconv_nhwc_base(self, conv_module, weight_memory_format, dtype, prec=None):\n+        input_shapes = {2: (55, 55), 3: (14, 14, 14)}\n+        if conv_module in [torch.nn.Conv2d, torch.nn.ConvTranspose2d]:\n+            cl_format = torch.channels_last\n+            input_shape = input_shapes[2]\n+        elif conv_module in [torch.nn.Conv3d, torch.nn.ConvTranspose3d]:\n+            cl_format = torch.channels_last_3d\n+            input_shape = input_shapes[3]\n+\n+            x_shape = (N, C) + input_shape\n+            # conv1: mkldnn conv/deconv in contiguous memory format (nchw)\n+            # conv2: mkldnn conv/deconv in channels last memory format (nhwc)\n+            x2 = x.clone().to(memory_format=cl_format)\n+            self.assertEqual(y1, y2, atol=prec, rtol=prec)\n+\n+                self.assertTrue(x2.grad.is_contiguous(memory_format=cl_format))\n+                    self.assertEqual(conv1.bias.grad, conv2.bias.grad, atol=prec, rtol=prec)\n+                self.assertEqual(x1.grad, x2.grad, atol=prec, rtol=prec)\n+    def test_conv_nhwc_fp32(self):\n+        self._test_conv_deconv_nhwc_base(torch.nn.Conv2d, torch.contiguous_format, dtype=torch.float32)\n+        self._test_conv_deconv_nhwc_base(torch.nn.Conv2d, torch.channels_last, dtype=torch.float32)\n+        self._test_conv_deconv_nhwc_base(torch.nn.Conv3d, torch.contiguous_format, dtype=torch.float32)\n+        self._test_conv_deconv_nhwc_base(torch.nn.Conv3d, torch.channels_last_3d, dtype=torch.float32)\n+    def test_conv_nhwc_bf16(self):\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv2d, torch.contiguous_format, dtype=torch.bfloat16)\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv2d, torch.channels_last, dtype=torch.bfloat16)\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv3d, torch.contiguous_format, dtype=torch.bfloat16)\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv3d, torch.channels_last_3d, dtype=torch.bfloat16)\n+        # test fall back to thnn impl\n+        with torch.backends.mkldnn.flags(enabled=False):\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv2d, torch.contiguous_format, dtype=torch.bfloat16, prec=1e-2)\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv2d, torch.channels_last, dtype=torch.bfloat16, prec=1e-2)\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv3d, torch.contiguous_format, dtype=torch.bfloat16, prec=1e-3)\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv3d, torch.channels_last_3d, dtype=torch.bfloat16, prec=1e-3)\n+\n+    def test_conv_transpose_nhwc_fp32(self):\n+        self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose2d, torch.contiguous_format, dtype=torch.float32)\n+        self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose2d, torch.channels_last, dtype=torch.float32)\n+        self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose3d, torch.contiguous_format, dtype=torch.float32)\n+        self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose3d, torch.channels_last_3d, dtype=torch.float32)\n+    def test_conv_transpose_nhwc_bf16(self):\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose2d, torch.contiguous_format, dtype=torch.bfloat16)\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose2d, torch.channels_last, dtype=torch.bfloat16)\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose3d, torch.contiguous_format, dtype=torch.bfloat16)\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose3d, torch.channels_last_3d, dtype=torch.bfloat16)\n+        # test fall back to thnn impl\n+        with torch.backends.mkldnn.flags(enabled=False):\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose2d, torch.contiguous_format, dtype=torch.bfloat16, prec=2e-2)\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose2d, torch.channels_last, dtype=torch.bfloat16, prec=2e-2)\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose3d, torch.contiguous_format, dtype=torch.bfloat16, prec=1e-3)\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose3d, torch.channels_last_3d, dtype=torch.bfloat16, prec=1e-3)\n",
            "whole_hunk": "@@ -321,22 +321,25 @@ class TestMkldnn(TestCase):\n     def test_conv3d_bf16(self):\n         self._test_conv_bf16_base(dim=3)\n \n-    def _test_conv2d_nhwc_base(self, conv_module, weight_memory_format, dtype):\n-        input_shapes = (55, 55)\n+    def _test_conv_deconv_nhwc_base(self, conv_module, weight_memory_format, dtype, prec=None):\n+        input_shapes = {2: (55, 55), 3: (14, 14, 14)}\n         options = itertools.product([True, False], [True, False], [1, 2], [1, 4])\n+        if conv_module in [torch.nn.Conv2d, torch.nn.ConvTranspose2d]:\n+            cl_format = torch.channels_last\n+            input_shape = input_shapes[2]\n+        elif conv_module in [torch.nn.Conv3d, torch.nn.ConvTranspose3d]:\n+            cl_format = torch.channels_last_3d\n+            input_shape = input_shapes[3]\n+\n         for train, bias, dilation, groups in options:\n             N = torch.randint(3, 10, (1,)).item()\n             M = torch.randint(1, 3, (1,)).item() * groups\n             C = torch.randint(1, 3, (1,)).item() * groups\n-            x_shape = (N, C) + input_shapes\n+            x_shape = (N, C) + input_shape\n             x = torch.randn(x_shape, dtype=dtype)\n \n-            # TODO: remove this when group depthwise is supported:\n-            if conv_module is torch.nn.ConvTranspose2d and groups > 1 and C == groups:\n-                continue\n-\n-            # conv1: mkldnn conv in contiguous memory format (nchw)\n-            # conv2: mkldnn conv in channels last memory format (nhwc)\n+            # conv1: mkldnn conv/deconv in contiguous memory format (nchw)\n+            # conv2: mkldnn conv/deconv in channels last memory format (nhwc)\n             conv1 = conv_module(in_channels=C,\n                                 out_channels=M,\n                                 kernel_size=3,\n@@ -347,46 +350,67 @@ class TestMkldnn(TestCase):\n                                 groups=groups).to(dtype=dtype)\n             conv2 = copy.deepcopy(conv1).to(memory_format=weight_memory_format)\n             x1 = x.clone()\n-            x2 = x.clone().to(memory_format=torch.channels_last)\n+            x2 = x.clone().to(memory_format=cl_format)\n             if train:\n                 x1.requires_grad_()\n                 x2.requires_grad_()\n             y1 = conv1(x1)\n             y2 = conv2(x2)\n-            self.assertEqual(y1, y2)\n+            self.assertEqual(y1, y2, atol=prec, rtol=prec)\n+\n             if train:\n                 y1.sum().backward()\n                 y2.sum().backward()\n-                self.assertTrue(x2.grad.is_contiguous(memory_format=torch.channels_last))\n+                self.assertTrue(x2.grad.is_contiguous(memory_format=cl_format))\n                 self.assertEqual(conv1.weight.grad,\n                                  conv2.weight.grad,\n                                  atol=1e-3,\n                                  rtol=1e-3)\n                 if bias:\n-                    self.assertEqual(conv1.bias.grad, conv2.bias.grad)\n-                self.assertEqual(x1.grad, x2.grad)\n+                    self.assertEqual(conv1.bias.grad, conv2.bias.grad, atol=prec, rtol=prec)\n+                self.assertEqual(x1.grad, x2.grad, atol=prec, rtol=prec)\n \n-    def test_conv2d_nhwc(self):\n-        self._test_conv2d_nhwc_base(torch.nn.Conv2d, torch.contiguous_format, dtype=torch.float32)\n-        self._test_conv2d_nhwc_base(torch.nn.Conv2d, torch.channels_last, dtype=torch.float32)\n+    def test_conv_nhwc_fp32(self):\n+        self._test_conv_deconv_nhwc_base(torch.nn.Conv2d, torch.contiguous_format, dtype=torch.float32)\n+        self._test_conv_deconv_nhwc_base(torch.nn.Conv2d, torch.channels_last, dtype=torch.float32)\n+        self._test_conv_deconv_nhwc_base(torch.nn.Conv3d, torch.contiguous_format, dtype=torch.float32)\n+        self._test_conv_deconv_nhwc_base(torch.nn.Conv3d, torch.channels_last_3d, dtype=torch.float32)\n \n     @unittest.skipIf(IS_WINDOWS, \"Limit support for bf16 path\")\n-    def test_conv2d_nhwc_bf16(self):\n+    def test_conv_nhwc_bf16(self):\n         # when torch.ops.mkldnn._is_mkldnn_bf16_supported() returns false, bf16 CPU conv will fall back to thnn impl\n         if torch.ops.mkldnn._is_mkldnn_bf16_supported():\n-            self._test_conv2d_nhwc_base(torch.nn.Conv2d, torch.contiguous_format, dtype=torch.bfloat16)\n-            self._test_conv2d_nhwc_base(torch.nn.Conv2d, torch.channels_last, dtype=torch.bfloat16)\n-\n-    def test_conv_transpose2d_nhwc(self):\n-        self._test_conv2d_nhwc_base(torch.nn.ConvTranspose2d, torch.contiguous_format, dtype=torch.float32)\n-        self._test_conv2d_nhwc_base(torch.nn.ConvTranspose2d, torch.channels_last, dtype=torch.float32)\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv2d, torch.contiguous_format, dtype=torch.bfloat16)\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv2d, torch.channels_last, dtype=torch.bfloat16)\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv3d, torch.contiguous_format, dtype=torch.bfloat16)\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv3d, torch.channels_last_3d, dtype=torch.bfloat16)\n+        # test fall back to thnn impl\n+        with torch.backends.mkldnn.flags(enabled=False):\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv2d, torch.contiguous_format, dtype=torch.bfloat16, prec=1e-2)\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv2d, torch.channels_last, dtype=torch.bfloat16, prec=1e-2)\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv3d, torch.contiguous_format, dtype=torch.bfloat16, prec=1e-3)\n+            self._test_conv_deconv_nhwc_base(torch.nn.Conv3d, torch.channels_last_3d, dtype=torch.bfloat16, prec=1e-3)\n+\n+    def test_conv_transpose_nhwc_fp32(self):\n+        self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose2d, torch.contiguous_format, dtype=torch.float32)\n+        self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose2d, torch.channels_last, dtype=torch.float32)\n+        self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose3d, torch.contiguous_format, dtype=torch.float32)\n+        self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose3d, torch.channels_last_3d, dtype=torch.float32)\n \n     @unittest.skipIf(IS_WINDOWS, \"Limit support for bf16 path\")\n-    def test_conv_transpose2d_nhwc_bf16(self):\n+    def test_conv_transpose_nhwc_bf16(self):\n         # when torch.ops.mkldnn._is_mkldnn_bf16_supported() returns false, bf16 CPU conv will fall back to thnn impl\n         if torch.ops.mkldnn._is_mkldnn_bf16_supported():\n-            self._test_conv2d_nhwc_base(torch.nn.ConvTranspose2d, torch.contiguous_format, dtype=torch.bfloat16)\n-            self._test_conv2d_nhwc_base(torch.nn.ConvTranspose2d, torch.channels_last, dtype=torch.bfloat16)\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose2d, torch.contiguous_format, dtype=torch.bfloat16)\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose2d, torch.channels_last, dtype=torch.bfloat16)\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose3d, torch.contiguous_format, dtype=torch.bfloat16)\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose3d, torch.channels_last_3d, dtype=torch.bfloat16)\n+        # test fall back to thnn impl\n+        with torch.backends.mkldnn.flags(enabled=False):\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose2d, torch.contiguous_format, dtype=torch.bfloat16, prec=2e-2)\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose2d, torch.channels_last, dtype=torch.bfloat16, prec=2e-2)\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose3d, torch.contiguous_format, dtype=torch.bfloat16, prec=1e-3)\n+            self._test_conv_deconv_nhwc_base(torch.nn.ConvTranspose3d, torch.channels_last_3d, dtype=torch.bfloat16, prec=1e-3)\n \n     def _test_conv_transpose_base(self, dim):\n         conv_module = {\n"
        },
        {
            "name": "test_mkldnn_fusion.py",
            "path": "test/test_mkldnn_fusion.py",
            "patches": [
                {
                    "old_start": 268,
                    "old_length": 7,
                    "new_start": 268,
                    "new_length": 7,
                    "hunk": "@@ -268,7 +268,7 @@ class TestMkldnnFusion(JitTestCase):\n                 x = self.binary(x, other)\n                 return x\n \n-        input_shapes = {2: (112, 112), 3: (55, 55, 55)}\n+        input_shapes = {2: (112, 112), 3: (22, 22, 22)}\n         for pointwise_name, pointwise_fn in self._binary_list().items():\n             for dim in [2, 3]:\n                 channels_last = torch.channels_last if dim == 2 else torch.channels_last_3d\n"
                },
                {
                    "old_start": 301,
                    "old_length": 7,
                    "new_start": 301,
                    "new_length": 7,
                    "hunk": "@@ -301,7 +301,7 @@ class TestMkldnnFusion(JitTestCase):\n                             self.assertEqual(ref, other)\n                             self.assertEqual(ref, fused_inplace)\n \n-                        self.assertEqual(ref, fused)\n+                        self.assertEqual(ref, fused, atol=5e-4, rtol=5e-4)\n \n \n     def test_linear_binary_fusion_ops(self):"
                }
            ],
            "whole_deleted": "-        input_shapes = {2: (112, 112), 3: (55, 55, 55)}\n-                        self.assertEqual(ref, fused)\n",
            "whole_added": "+        input_shapes = {2: (112, 112), 3: (22, 22, 22)}\n+                        self.assertEqual(ref, fused, atol=5e-4, rtol=5e-4)\n",
            "whole_hunk": "@@ -268,7 +268,7 @@ class TestMkldnnFusion(JitTestCase):\n                 x = self.binary(x, other)\n                 return x\n \n-        input_shapes = {2: (112, 112), 3: (55, 55, 55)}\n+        input_shapes = {2: (112, 112), 3: (22, 22, 22)}\n         for pointwise_name, pointwise_fn in self._binary_list().items():\n             for dim in [2, 3]:\n                 channels_last = torch.channels_last if dim == 2 else torch.channels_last_3d\n@@ -301,7 +301,7 @@ class TestMkldnnFusion(JitTestCase):\n                             self.assertEqual(ref, other)\n                             self.assertEqual(ref, fused_inplace)\n \n-                        self.assertEqual(ref, fused)\n+                        self.assertEqual(ref, fused, atol=5e-4, rtol=5e-4)\n \n \n     def test_linear_binary_fusion_ops(self):"
        }
    ]
},
{
    "Id": 305,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/b816760a2f27adafb0b1dac4c032a2e97c690b29",
    "date": "2024-02-05T20:29:25+00:00",
    "message": "More progress on type checking ValueRanges (#118870)\n\nType checking Python is a pain. Here are my learnings:\n\n* The types for heavily polymorphic code is going to be verbose, no way around it. I originally was hoping I could lean on polymorphism with a bounded TypeVar to compactly write signatures for many of the ValueRanges methods, but I ran into some unworkaroundable mypy bugs. Writing out all the types explicitly and using `@overload` liberally works pretty well, so I think I recommend people do that instead of trying to do fancy things.\n* Sympy is missing annotations for assumptions, because they are all metaprogrammed. I don't really relish maintaining a typeshed for sympy, so I wrote a small mypy plugin to add them in.\n* GADT style refinement is... just not a good idea in practice. Mypy easily gets confused whether or not a return value from a refined section is allowed for the outer return type. So many of these have been replaced with less informative implementation types and more informative external types via overloads. Hopefully this is good for use sites.\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/118870\nApproved by: https://github.com/Skylion007, https://github.com/albanD",
    "label": "NO",
    "changes": [
        {
            "name": "mypy.ini",
            "path": "mypy.ini",
            "patches": [
                {
                    "old_start": 2,
                    "old_length": 7,
                    "new_start": 2,
                    "new_length": 7,
                    "hunk": "@@ -2,7 +2,7 @@\n # test_run_mypy in test/test_type_hints.py uses this string)\n \n [mypy]\n-plugins = mypy_plugins/check_mypy_version.py, numpy.typing.mypy_plugin\n+plugins = mypy_plugins/check_mypy_version.py, mypy_plugins/sympy_mypy_plugin.py, numpy.typing.mypy_plugin\n \n cache_dir = .mypy_cache/normal\n allow_redefinition = True\n"
                }
            ],
            "whole_deleted": "-plugins = mypy_plugins/check_mypy_version.py, numpy.typing.mypy_plugin\n",
            "whole_added": "+plugins = mypy_plugins/check_mypy_version.py, mypy_plugins/sympy_mypy_plugin.py, numpy.typing.mypy_plugin\n",
            "whole_hunk": "@@ -2,7 +2,7 @@\n # test_run_mypy in test/test_type_hints.py uses this string)\n \n [mypy]\n-plugins = mypy_plugins/check_mypy_version.py, numpy.typing.mypy_plugin\n+plugins = mypy_plugins/check_mypy_version.py, mypy_plugins/sympy_mypy_plugin.py, numpy.typing.mypy_plugin\n \n cache_dir = .mypy_cache/normal\n allow_redefinition = True\n"
        },
        {
            "name": "sympy_mypy_plugin.py",
            "path": "mypy_plugins/sympy_mypy_plugin.py",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 59,
                    "hunk": "@@ -0,0 +1,59 @@\n+from mypy.plugin import Plugin\n+from mypy.plugins.common import add_attribute_to_class\n+from mypy.types import NoneType, UnionType\n+\n+\n+class SympyPlugin(Plugin):\n+    def get_base_class_hook(self, fullname: str):\n+        if fullname == \"sympy.core.basic.Basic\":\n+            return add_assumptions\n+        return None\n+\n+\n+def add_assumptions(ctx) -> None:\n+    # Generated by list(sys.modules['sympy.core.assumptions']._assume_defined)\n+    # (do not import sympy to speedup mypy plugin load time)\n+    assumptions = [\n+        \"hermitian\",\n+        \"prime\",\n+        \"noninteger\",\n+        \"negative\",\n+        \"antihermitian\",\n+        \"infinite\",\n+        \"finite\",\n+        \"irrational\",\n+        \"extended_positive\",\n+        \"nonpositive\",\n+        \"odd\",\n+        \"algebraic\",\n+        \"integer\",\n+        \"rational\",\n+        \"extended_real\",\n+        \"nonnegative\",\n+        \"transcendental\",\n+        \"extended_nonzero\",\n+        \"extended_negative\",\n+        \"composite\",\n+        \"complex\",\n+        \"imaginary\",\n+        \"nonzero\",\n+        \"zero\",\n+        \"even\",\n+        \"positive\",\n+        \"polar\",\n+        \"extended_nonpositive\",\n+        \"extended_nonnegative\",\n+        \"real\",\n+        \"commutative\",\n+    ]\n+    for a in assumptions:\n+        add_attribute_to_class(\n+            ctx.api,\n+            ctx.cls,\n+            f\"is_{a}\",\n+            UnionType([ctx.api.named_type(\"builtins.bool\"), NoneType()]),\n+        )\n+\n+\n+def plugin(version: str):\n+    return SympyPlugin\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+from mypy.plugin import Plugin\n+from mypy.plugins.common import add_attribute_to_class\n+from mypy.types import NoneType, UnionType\n+\n+\n+class SympyPlugin(Plugin):\n+    def get_base_class_hook(self, fullname: str):\n+        if fullname == \"sympy.core.basic.Basic\":\n+            return add_assumptions\n+        return None\n+\n+\n+def add_assumptions(ctx) -> None:\n+    # Generated by list(sys.modules['sympy.core.assumptions']._assume_defined)\n+    # (do not import sympy to speedup mypy plugin load time)\n+    assumptions = [\n+        \"hermitian\",\n+        \"prime\",\n+        \"noninteger\",\n+        \"negative\",\n+        \"antihermitian\",\n+        \"infinite\",\n+        \"finite\",\n+        \"irrational\",\n+        \"extended_positive\",\n+        \"nonpositive\",\n+        \"odd\",\n+        \"algebraic\",\n+        \"integer\",\n+        \"rational\",\n+        \"extended_real\",\n+        \"nonnegative\",\n+        \"transcendental\",\n+        \"extended_nonzero\",\n+        \"extended_negative\",\n+        \"composite\",\n+        \"complex\",\n+        \"imaginary\",\n+        \"nonzero\",\n+        \"zero\",\n+        \"even\",\n+        \"positive\",\n+        \"polar\",\n+        \"extended_nonpositive\",\n+        \"extended_nonnegative\",\n+        \"real\",\n+        \"commutative\",\n+    ]\n+    for a in assumptions:\n+        add_attribute_to_class(\n+            ctx.api,\n+            ctx.cls,\n+            f\"is_{a}\",\n+            UnionType([ctx.api.named_type(\"builtins.bool\"), NoneType()]),\n+        )\n+\n+\n+def plugin(version: str):\n+    return SympyPlugin\n",
            "whole_hunk": "@@ -0,0 +1,59 @@\n+from mypy.plugin import Plugin\n+from mypy.plugins.common import add_attribute_to_class\n+from mypy.types import NoneType, UnionType\n+\n+\n+class SympyPlugin(Plugin):\n+    def get_base_class_hook(self, fullname: str):\n+        if fullname == \"sympy.core.basic.Basic\":\n+            return add_assumptions\n+        return None\n+\n+\n+def add_assumptions(ctx) -> None:\n+    # Generated by list(sys.modules['sympy.core.assumptions']._assume_defined)\n+    # (do not import sympy to speedup mypy plugin load time)\n+    assumptions = [\n+        \"hermitian\",\n+        \"prime\",\n+        \"noninteger\",\n+        \"negative\",\n+        \"antihermitian\",\n+        \"infinite\",\n+        \"finite\",\n+        \"irrational\",\n+        \"extended_positive\",\n+        \"nonpositive\",\n+        \"odd\",\n+        \"algebraic\",\n+        \"integer\",\n+        \"rational\",\n+        \"extended_real\",\n+        \"nonnegative\",\n+        \"transcendental\",\n+        \"extended_nonzero\",\n+        \"extended_negative\",\n+        \"composite\",\n+        \"complex\",\n+        \"imaginary\",\n+        \"nonzero\",\n+        \"zero\",\n+        \"even\",\n+        \"positive\",\n+        \"polar\",\n+        \"extended_nonpositive\",\n+        \"extended_nonnegative\",\n+        \"real\",\n+        \"commutative\",\n+    ]\n+    for a in assumptions:\n+        add_attribute_to_class(\n+            ctx.api,\n+            ctx.cls,\n+            f\"is_{a}\",\n+            UnionType([ctx.api.named_type(\"builtins.bool\"), NoneType()]),\n+        )\n+\n+\n+def plugin(version: str):\n+    return SympyPlugin\n"
        },
        {
            "name": "test_typing.py",
            "path": "test/test_typing.py",
            "patches": [
                {
                    "old_start": 67,
                    "old_length": 7,
                    "new_start": 67,
                    "new_length": 7,
                    "hunk": "@@ -67,7 +67,7 @@ def _run_mypy() -> Dict[str, List[str]]:\n                 directory,\n             ]\n         )\n-        assert not stderr, directory\n+        assert not stderr, stderr\n         stdout = stdout.replace(\"*\", \"\")\n \n         # Parse the output\n"
                }
            ],
            "whole_deleted": "-        assert not stderr, directory\n",
            "whole_added": "+        assert not stderr, stderr\n",
            "whole_hunk": "@@ -67,7 +67,7 @@ def _run_mypy() -> Dict[str, List[str]]:\n                 directory,\n             ]\n         )\n-        assert not stderr, directory\n+        assert not stderr, stderr\n         stdout = stdout.replace(\"*\", \"\")\n \n         # Parse the output\n"
        },
        {
            "name": "value_ranges.py",
            "path": "torch/utils/_sympy/value_ranges.py",
            "patches": [
                {
                    "old_start": 8,
                    "old_length": 7,
                    "new_start": 8,
                    "new_length": 7,
                    "hunk": "@@ -8,7 +8,7 @@ import operator\n import math\n import logging\n import torch\n-from typing import Dict, Optional, SupportsFloat, TypeVar, Generic, cast, Union\n+from typing import Dict, Optional, SupportsFloat, TypeVar, Generic, Union, overload, Callable, TYPE_CHECKING\n from typing_extensions import TypeGuard\n \n from torch._prims_common import dtype_to_type\n"
                },
                {
                    "old_start": 70,
                    "old_length": 8,
                    "new_start": 70,
                    "new_length": 25,
                    "hunk": "@@ -70,8 +70,25 @@ def vr_is_expr(vr: ValueRanges[_T]) -> TypeGuard[ValueRanges[sympy.Expr]]:\n     return not vr.is_bool\n \n \n+ExprIn = Union[int, float, sympy.Expr]\n+BoolIn = Union[bool, SympyBoolean]\n+AllIn = Union[ExprIn, BoolIn]\n+ExprFn = Callable[[sympy.Expr], sympy.Expr]\n+ExprFn2 = Callable[[sympy.Expr, sympy.Expr], sympy.Expr]\n+BoolFn = Callable[[SympyBoolean], SympyBoolean]\n+BoolFn2 = Callable[[SympyBoolean, SympyBoolean], SympyBoolean]\n+AllFn = Union[ExprFn, BoolFn]\n+AllFn2 = Union[ExprFn2, BoolFn2]\n+\n+\n @dataclasses.dataclass(frozen=True)\n class ValueRanges(Generic[_T]):\n+    if TYPE_CHECKING:\n+        # ruff doesn't understand circular references but mypy does\n+        ExprVR = ValueRanges[sympy.Expr]  # noqa: F821\n+        BoolVR = ValueRanges[SympyBoolean]  # noqa: F821\n+        AllVR = Union[ExprVR, BoolVR]\n+\n     # Although the type signature here suggests you can pass any\n     # sympy expression, in practice the analysis here only works\n     # with constant sympy expressions\n"
                },
                {
                    "old_start": 79,
                    "old_length": 7,
                    "new_start": 96,
                    "new_length": 15,
                    "hunk": "@@ -79,7 +96,15 @@ class ValueRanges(Generic[_T]):\n     upper: _T\n     is_bool: bool\n \n-    def __init__(self, lower: Union[_T, bool, int, float], upper: Union[_T, bool, int, float]) -> None:\n+    @overload\n+    def __init__(self: ValueRanges[sympy.Expr], lower: ExprIn, upper: ExprIn) -> None:\n+        ...\n+\n+    @overload\n+    def __init__(self: ValueRanges[SympyBoolean], lower: BoolIn, upper: BoolIn) -> None:\n+        ...\n+\n+    def __init__(self, lower: AllIn, upper: AllIn) -> None:\n         lower = simple_sympify(lower)\n         upper = simple_sympify(upper)\n         # TODO: when the bounds have free variables, this may be\n"
                },
                {
                    "old_start": 92,
                    "old_length": 15,
                    "new_start": 117,
                    "new_length": 15,
                    "hunk": "@@ -92,15 +117,15 @@ class ValueRanges(Generic[_T]):\n         object.__setattr__(self, \"is_bool\", isinstance(lower, SympyBoolean))\n         assert isinstance(upper, SympyBoolean) == self.is_bool\n \n-    def boolify(self):\n-        if self.is_bool:\n+    def boolify(self) -> ValueRanges[SympyBoolean]:\n+        if vr_is_bool(self):\n             return self\n         elif self == ValueRanges.unknown():\n             return ValueRanges.unknown_bool()\n         else:\n             raise AssertionError(f\"not bool like {self}\")\n \n-    def __contains__(self, x):\n+    def __contains__(self, x: AllIn) -> bool:\n         x = simple_sympify(x)\n         return sympy_generic_le(self.lower, x) and sympy_generic_le(x, self.upper)\n \n"
                },
                {
                    "old_start": 109,
                    "old_length": 30,
                    "new_start": 134,
                    "new_length": 42,
                    "hunk": "@@ -109,30 +134,42 @@ class ValueRanges(Generic[_T]):\n         return self & other\n \n     # Intersection\n-    def __and__(self: ValueRanges[_T], other: ValueRanges[_T]) -> ValueRanges[_T]:\n+    @overload\n+    def __and__(self: ValueRanges[sympy.Expr], other: ValueRanges[sympy.Expr]) -> ValueRanges[sympy.Expr]:\n+        ...\n+\n+    @overload\n+    def __and__(self: ValueRanges[SympyBoolean], other: ValueRanges[SympyBoolean]) -> ValueRanges[SympyBoolean]:\n+        ...\n+\n+    def __and__(self: AllVR, other: AllVR) -> AllVR:\n         if other == ValueRanges.unknown():\n             return self\n         if self == ValueRanges.unknown():\n             return other\n         assert self.is_bool == other.is_bool, (self, other)\n-        if vr_is_bool(self):\n-            return cast(ValueRanges[_T], ValueRanges(sympy.Or(self.lower, other.lower), sympy.And(self.upper, other.upper)))\n-        elif vr_is_expr(self):\n-            return cast(ValueRanges[_T], ValueRanges(sympy.Max(self.lower, other.lower), sympy.Min(self.upper, other.upper)))\n+        if self.is_bool:\n+            return ValueRanges(sympy.Or(self.lower, other.lower), sympy.And(self.upper, other.upper))\n         else:\n-            raise AssertionError(\"impossible\")\n+            return ValueRanges(sympy.Max(self.lower, other.lower), sympy.Min(self.upper, other.upper))\n \n     # Union\n-    def __or__(self, other) -> ValueRanges:\n+    @overload\n+    def __or__(self: ValueRanges[sympy.Expr], other: ValueRanges[sympy.Expr]) -> ValueRanges[sympy.Expr]:\n+        ...\n+\n+    @overload\n+    def __or__(self: ValueRanges[SympyBoolean], other: ValueRanges[SympyBoolean]) -> ValueRanges[SympyBoolean]:\n+        ...\n+\n+    def __or__(self: AllVR, other: AllVR) -> AllVR:\n         if ValueRanges.unknown() in (self, other):\n             return ValueRanges.unknown()\n         assert self.is_bool == other.is_bool, (self, other)\n-        if vr_is_bool(self):\n-            return cast(ValueRanges[_T], ValueRanges(sympy.And(self.lower, other.lower), sympy.Or(self.upper, other.upper)))\n-        elif vr_is_expr(self):\n-            return cast(ValueRanges[_T], ValueRanges(sympy.Min(self.lower, other.lower), sympy.Max(self.upper, other.upper)))\n+        if self.is_bool:\n+            return ValueRanges(sympy.And(self.lower, other.lower), sympy.Or(self.upper, other.upper))\n         else:\n-            raise AssertionError(\"impossible\")\n+            return ValueRanges(sympy.Min(self.lower, other.lower), sympy.Max(self.upper, other.upper))\n \n     def is_singleton(self) -> bool:\n         return self.lower == self.upper\n"
                },
                {
                    "old_start": 146,
                    "old_length": 43,
                    "new_start": 183,
                    "new_length": 76,
                    "hunk": "@@ -146,43 +183,76 @@ class ValueRanges(Generic[_T]):\n     def unknown_bool() -> ValueRanges[SympyBoolean]:\n         return ValueRanges(sympy.false, sympy.true)\n \n-    @classmethod\n-    def wrap(cls, arg):\n+    @overload\n+    @staticmethod\n+    # work around the fact that bool and int overlap\n+    def wrap(arg: Union[ExprIn, ExprVR]) -> ExprVR:  # type: ignore[overload-overlap]\n+        ...\n+\n+    @overload\n+    @staticmethod\n+    def wrap(arg: Union[BoolIn, BoolVR]) -> BoolVR:\n+        ...\n+\n+    @staticmethod\n+    def wrap(arg: Union[AllIn, AllVR]) -> AllVR:\n         if isinstance(arg, ValueRanges):\n             return arg\n-        return ValueRanges(arg, arg)\n+        # arg is either ExprIn or BoolIn, but we don't know it here\n+        return ValueRanges(arg, arg)  # type: ignore[arg-type]\n \n-    @classmethod\n-    def increasing_map(cls, x, fn):\n+    @staticmethod\n+    def increasing_map(x: Union[ExprIn, ExprVR], fn: ExprFn) -> ExprVR:\n         \"\"\"Increasing: x <= y => f(x) <= f(y).\"\"\"\n-        x = cls.wrap(x)\n+        x = ValueRanges.wrap(x)\n         return ValueRanges(fn(x.lower), fn(x.upper))\n \n-    @classmethod\n-    def decreasing_map(cls, x, fn):\n+    @overload\n+    @staticmethod\n+    def decreasing_map(x: Union[ExprIn, ExprVR], fn: ExprFn) -> ExprVR:\n+        ...\n+\n+    @overload\n+    @staticmethod\n+    def decreasing_map(x: Union[BoolIn, BoolVR], fn: BoolFn) -> BoolVR:\n+        ...\n+\n+    @staticmethod\n+    def decreasing_map(x: Union[AllIn, AllVR], fn: AllFn) -> AllVR:\n         \"\"\"Decreasing: x <= y => f(x) >= f(y).\"\"\"\n-        x = cls.wrap(x)\n-        return ValueRanges(fn(x.upper), fn(x.lower))\n+        x = ValueRanges.wrap(x)\n+        # consistently either Expr or Bool, but we don't know it here\n+        return ValueRanges(fn(x.upper), fn(x.lower))  # type: ignore[arg-type]\n \n-    @classmethod\n-    def monotone_map(cls, x, fn):\n+    @staticmethod\n+    def monotone_map(x: Union[ExprIn, ExprVR], fn: ExprFn) -> ExprVR:\n         \"\"\"It's increasing or decreasing.\"\"\"\n-        x = cls.wrap(x)\n+        x = ValueRanges.wrap(x)\n         l = fn(x.lower)\n         u = fn(x.upper)\n         return ValueRanges(min(l, u), max(l, u))\n \n-    @classmethod\n-    def convex_min_zero_map(cls, x, fn):\n+    @staticmethod\n+    def convex_min_zero_map(x: Union[ExprIn, ExprVR], fn: ExprFn) -> ExprVR:\n         \"\"\"Fn is convex and has a minimum at 0.\"\"\"\n         x = ValueRanges.wrap(x)\n         if 0 in x:\n             return ValueRanges(0, max(fn(x.lower), fn(x.upper)))\n         else:\n-            return cls.monotone_map(x, fn)\n+            return ValueRanges.monotone_map(x, fn)\n \n-    @classmethod\n-    def coordinatewise_increasing_map(cls, x, y, fn):\n+    @overload\n+    @staticmethod\n+    def coordinatewise_increasing_map(x: Union[ExprIn, ExprVR], y: Union[ExprIn, ExprVR], fn: ExprFn2) -> ExprVR:\n+        ...\n+\n+    @overload\n+    @staticmethod\n+    def coordinatewise_increasing_map(x: Union[BoolIn, BoolVR], y: Union[BoolIn, BoolVR], fn: BoolFn2) -> BoolVR:\n+        ...\n+\n+    @staticmethod\n+    def coordinatewise_increasing_map(x: Union[AllIn, AllVR], y: Union[AllIn, AllVR], fn: AllFn2) -> AllVR:\n         \"\"\"\n         It's increasing on each coordinate.\n \n"
                },
                {
                    "old_start": 190,
                    "old_length": 10,
                    "new_start": 260,
                    "new_length": 10,
                    "hunk": "@@ -190,10 +260,10 @@ class ValueRanges(Generic[_T]):\n         For every 1 <= i <= n and x_i <= y_i we have that\n         f(x1, .., xn) <= f(x1, , yi, ..., xn)\n         \"\"\"\n-        x, y = cls.wrap(x), cls.wrap(y)\n+        x, y = ValueRanges.wrap(x), ValueRanges.wrap(y)\n         return ValueRanges(\n-            fn(x.lower, y.lower),\n-            fn(x.upper, y.upper),\n+            fn(x.lower, y.lower),  # type: ignore[arg-type]\n+            fn(x.upper, y.upper),  # type: ignore[arg-type]\n         )\n \n     @classmethod\n"
                },
                {
                    "old_start": 450,
                    "old_length": 7,
                    "new_start": 520,
                    "new_length": 7,
                    "hunk": "@@ -450,7 +520,7 @@ class SymPyValueRangeAnalysis:\n         b = ValueRanges.wrap(b)\n \n         # Performs upcasting first\n-        def fn_(x, y):\n+        def fn_(x: sympy.Expr, y: sympy.Expr) -> sympy.Expr:\n             # Poorman's version of upcasting in Sympy\n             # Inf is not a float...\n             if x.is_Integer and y.is_Integer:"
                }
            ],
            "whole_deleted": "-from typing import Dict, Optional, SupportsFloat, TypeVar, Generic, cast, Union\n-    def __init__(self, lower: Union[_T, bool, int, float], upper: Union[_T, bool, int, float]) -> None:\n-    def boolify(self):\n-        if self.is_bool:\n-    def __contains__(self, x):\n-    def __and__(self: ValueRanges[_T], other: ValueRanges[_T]) -> ValueRanges[_T]:\n-        if vr_is_bool(self):\n-            return cast(ValueRanges[_T], ValueRanges(sympy.Or(self.lower, other.lower), sympy.And(self.upper, other.upper)))\n-        elif vr_is_expr(self):\n-            return cast(ValueRanges[_T], ValueRanges(sympy.Max(self.lower, other.lower), sympy.Min(self.upper, other.upper)))\n-            raise AssertionError(\"impossible\")\n-    def __or__(self, other) -> ValueRanges:\n-        if vr_is_bool(self):\n-            return cast(ValueRanges[_T], ValueRanges(sympy.And(self.lower, other.lower), sympy.Or(self.upper, other.upper)))\n-        elif vr_is_expr(self):\n-            return cast(ValueRanges[_T], ValueRanges(sympy.Min(self.lower, other.lower), sympy.Max(self.upper, other.upper)))\n-            raise AssertionError(\"impossible\")\n-    @classmethod\n-    def wrap(cls, arg):\n-        return ValueRanges(arg, arg)\n-    @classmethod\n-    def increasing_map(cls, x, fn):\n-        x = cls.wrap(x)\n-    @classmethod\n-    def decreasing_map(cls, x, fn):\n-        x = cls.wrap(x)\n-        return ValueRanges(fn(x.upper), fn(x.lower))\n-    @classmethod\n-    def monotone_map(cls, x, fn):\n-        x = cls.wrap(x)\n-    @classmethod\n-    def convex_min_zero_map(cls, x, fn):\n-            return cls.monotone_map(x, fn)\n-    @classmethod\n-    def coordinatewise_increasing_map(cls, x, y, fn):\n-        x, y = cls.wrap(x), cls.wrap(y)\n-            fn(x.lower, y.lower),\n-            fn(x.upper, y.upper),\n-        def fn_(x, y):\n",
            "whole_added": "+from typing import Dict, Optional, SupportsFloat, TypeVar, Generic, Union, overload, Callable, TYPE_CHECKING\n+ExprIn = Union[int, float, sympy.Expr]\n+BoolIn = Union[bool, SympyBoolean]\n+AllIn = Union[ExprIn, BoolIn]\n+ExprFn = Callable[[sympy.Expr], sympy.Expr]\n+ExprFn2 = Callable[[sympy.Expr, sympy.Expr], sympy.Expr]\n+BoolFn = Callable[[SympyBoolean], SympyBoolean]\n+BoolFn2 = Callable[[SympyBoolean, SympyBoolean], SympyBoolean]\n+AllFn = Union[ExprFn, BoolFn]\n+AllFn2 = Union[ExprFn2, BoolFn2]\n+\n+\n+    if TYPE_CHECKING:\n+        # ruff doesn't understand circular references but mypy does\n+        ExprVR = ValueRanges[sympy.Expr]  # noqa: F821\n+        BoolVR = ValueRanges[SympyBoolean]  # noqa: F821\n+        AllVR = Union[ExprVR, BoolVR]\n+\n+    @overload\n+    def __init__(self: ValueRanges[sympy.Expr], lower: ExprIn, upper: ExprIn) -> None:\n+        ...\n+\n+    @overload\n+    def __init__(self: ValueRanges[SympyBoolean], lower: BoolIn, upper: BoolIn) -> None:\n+        ...\n+\n+    def __init__(self, lower: AllIn, upper: AllIn) -> None:\n+    def boolify(self) -> ValueRanges[SympyBoolean]:\n+        if vr_is_bool(self):\n+    def __contains__(self, x: AllIn) -> bool:\n+    @overload\n+    def __and__(self: ValueRanges[sympy.Expr], other: ValueRanges[sympy.Expr]) -> ValueRanges[sympy.Expr]:\n+        ...\n+\n+    @overload\n+    def __and__(self: ValueRanges[SympyBoolean], other: ValueRanges[SympyBoolean]) -> ValueRanges[SympyBoolean]:\n+        ...\n+\n+    def __and__(self: AllVR, other: AllVR) -> AllVR:\n+        if self.is_bool:\n+            return ValueRanges(sympy.Or(self.lower, other.lower), sympy.And(self.upper, other.upper))\n+            return ValueRanges(sympy.Max(self.lower, other.lower), sympy.Min(self.upper, other.upper))\n+    @overload\n+    def __or__(self: ValueRanges[sympy.Expr], other: ValueRanges[sympy.Expr]) -> ValueRanges[sympy.Expr]:\n+        ...\n+\n+    @overload\n+    def __or__(self: ValueRanges[SympyBoolean], other: ValueRanges[SympyBoolean]) -> ValueRanges[SympyBoolean]:\n+        ...\n+\n+    def __or__(self: AllVR, other: AllVR) -> AllVR:\n+        if self.is_bool:\n+            return ValueRanges(sympy.And(self.lower, other.lower), sympy.Or(self.upper, other.upper))\n+            return ValueRanges(sympy.Min(self.lower, other.lower), sympy.Max(self.upper, other.upper))\n+    @overload\n+    @staticmethod\n+    # work around the fact that bool and int overlap\n+    def wrap(arg: Union[ExprIn, ExprVR]) -> ExprVR:  # type: ignore[overload-overlap]\n+        ...\n+\n+    @overload\n+    @staticmethod\n+    def wrap(arg: Union[BoolIn, BoolVR]) -> BoolVR:\n+        ...\n+\n+    @staticmethod\n+    def wrap(arg: Union[AllIn, AllVR]) -> AllVR:\n+        # arg is either ExprIn or BoolIn, but we don't know it here\n+        return ValueRanges(arg, arg)  # type: ignore[arg-type]\n+    @staticmethod\n+    def increasing_map(x: Union[ExprIn, ExprVR], fn: ExprFn) -> ExprVR:\n+        x = ValueRanges.wrap(x)\n+    @overload\n+    @staticmethod\n+    def decreasing_map(x: Union[ExprIn, ExprVR], fn: ExprFn) -> ExprVR:\n+        ...\n+\n+    @overload\n+    @staticmethod\n+    def decreasing_map(x: Union[BoolIn, BoolVR], fn: BoolFn) -> BoolVR:\n+        ...\n+\n+    @staticmethod\n+    def decreasing_map(x: Union[AllIn, AllVR], fn: AllFn) -> AllVR:\n+        x = ValueRanges.wrap(x)\n+        # consistently either Expr or Bool, but we don't know it here\n+        return ValueRanges(fn(x.upper), fn(x.lower))  # type: ignore[arg-type]\n+    @staticmethod\n+    def monotone_map(x: Union[ExprIn, ExprVR], fn: ExprFn) -> ExprVR:\n+        x = ValueRanges.wrap(x)\n+    @staticmethod\n+    def convex_min_zero_map(x: Union[ExprIn, ExprVR], fn: ExprFn) -> ExprVR:\n+            return ValueRanges.monotone_map(x, fn)\n+    @overload\n+    @staticmethod\n+    def coordinatewise_increasing_map(x: Union[ExprIn, ExprVR], y: Union[ExprIn, ExprVR], fn: ExprFn2) -> ExprVR:\n+        ...\n+\n+    @overload\n+    @staticmethod\n+    def coordinatewise_increasing_map(x: Union[BoolIn, BoolVR], y: Union[BoolIn, BoolVR], fn: BoolFn2) -> BoolVR:\n+        ...\n+\n+    @staticmethod\n+    def coordinatewise_increasing_map(x: Union[AllIn, AllVR], y: Union[AllIn, AllVR], fn: AllFn2) -> AllVR:\n+        x, y = ValueRanges.wrap(x), ValueRanges.wrap(y)\n+            fn(x.lower, y.lower),  # type: ignore[arg-type]\n+            fn(x.upper, y.upper),  # type: ignore[arg-type]\n+        def fn_(x: sympy.Expr, y: sympy.Expr) -> sympy.Expr:\n",
            "whole_hunk": "@@ -8,7 +8,7 @@ import operator\n import math\n import logging\n import torch\n-from typing import Dict, Optional, SupportsFloat, TypeVar, Generic, cast, Union\n+from typing import Dict, Optional, SupportsFloat, TypeVar, Generic, Union, overload, Callable, TYPE_CHECKING\n from typing_extensions import TypeGuard\n \n from torch._prims_common import dtype_to_type\n@@ -70,8 +70,25 @@ def vr_is_expr(vr: ValueRanges[_T]) -> TypeGuard[ValueRanges[sympy.Expr]]:\n     return not vr.is_bool\n \n \n+ExprIn = Union[int, float, sympy.Expr]\n+BoolIn = Union[bool, SympyBoolean]\n+AllIn = Union[ExprIn, BoolIn]\n+ExprFn = Callable[[sympy.Expr], sympy.Expr]\n+ExprFn2 = Callable[[sympy.Expr, sympy.Expr], sympy.Expr]\n+BoolFn = Callable[[SympyBoolean], SympyBoolean]\n+BoolFn2 = Callable[[SympyBoolean, SympyBoolean], SympyBoolean]\n+AllFn = Union[ExprFn, BoolFn]\n+AllFn2 = Union[ExprFn2, BoolFn2]\n+\n+\n @dataclasses.dataclass(frozen=True)\n class ValueRanges(Generic[_T]):\n+    if TYPE_CHECKING:\n+        # ruff doesn't understand circular references but mypy does\n+        ExprVR = ValueRanges[sympy.Expr]  # noqa: F821\n+        BoolVR = ValueRanges[SympyBoolean]  # noqa: F821\n+        AllVR = Union[ExprVR, BoolVR]\n+\n     # Although the type signature here suggests you can pass any\n     # sympy expression, in practice the analysis here only works\n     # with constant sympy expressions\n@@ -79,7 +96,15 @@ class ValueRanges(Generic[_T]):\n     upper: _T\n     is_bool: bool\n \n-    def __init__(self, lower: Union[_T, bool, int, float], upper: Union[_T, bool, int, float]) -> None:\n+    @overload\n+    def __init__(self: ValueRanges[sympy.Expr], lower: ExprIn, upper: ExprIn) -> None:\n+        ...\n+\n+    @overload\n+    def __init__(self: ValueRanges[SympyBoolean], lower: BoolIn, upper: BoolIn) -> None:\n+        ...\n+\n+    def __init__(self, lower: AllIn, upper: AllIn) -> None:\n         lower = simple_sympify(lower)\n         upper = simple_sympify(upper)\n         # TODO: when the bounds have free variables, this may be\n@@ -92,15 +117,15 @@ class ValueRanges(Generic[_T]):\n         object.__setattr__(self, \"is_bool\", isinstance(lower, SympyBoolean))\n         assert isinstance(upper, SympyBoolean) == self.is_bool\n \n-    def boolify(self):\n-        if self.is_bool:\n+    def boolify(self) -> ValueRanges[SympyBoolean]:\n+        if vr_is_bool(self):\n             return self\n         elif self == ValueRanges.unknown():\n             return ValueRanges.unknown_bool()\n         else:\n             raise AssertionError(f\"not bool like {self}\")\n \n-    def __contains__(self, x):\n+    def __contains__(self, x: AllIn) -> bool:\n         x = simple_sympify(x)\n         return sympy_generic_le(self.lower, x) and sympy_generic_le(x, self.upper)\n \n@@ -109,30 +134,42 @@ class ValueRanges(Generic[_T]):\n         return self & other\n \n     # Intersection\n-    def __and__(self: ValueRanges[_T], other: ValueRanges[_T]) -> ValueRanges[_T]:\n+    @overload\n+    def __and__(self: ValueRanges[sympy.Expr], other: ValueRanges[sympy.Expr]) -> ValueRanges[sympy.Expr]:\n+        ...\n+\n+    @overload\n+    def __and__(self: ValueRanges[SympyBoolean], other: ValueRanges[SympyBoolean]) -> ValueRanges[SympyBoolean]:\n+        ...\n+\n+    def __and__(self: AllVR, other: AllVR) -> AllVR:\n         if other == ValueRanges.unknown():\n             return self\n         if self == ValueRanges.unknown():\n             return other\n         assert self.is_bool == other.is_bool, (self, other)\n-        if vr_is_bool(self):\n-            return cast(ValueRanges[_T], ValueRanges(sympy.Or(self.lower, other.lower), sympy.And(self.upper, other.upper)))\n-        elif vr_is_expr(self):\n-            return cast(ValueRanges[_T], ValueRanges(sympy.Max(self.lower, other.lower), sympy.Min(self.upper, other.upper)))\n+        if self.is_bool:\n+            return ValueRanges(sympy.Or(self.lower, other.lower), sympy.And(self.upper, other.upper))\n         else:\n-            raise AssertionError(\"impossible\")\n+            return ValueRanges(sympy.Max(self.lower, other.lower), sympy.Min(self.upper, other.upper))\n \n     # Union\n-    def __or__(self, other) -> ValueRanges:\n+    @overload\n+    def __or__(self: ValueRanges[sympy.Expr], other: ValueRanges[sympy.Expr]) -> ValueRanges[sympy.Expr]:\n+        ...\n+\n+    @overload\n+    def __or__(self: ValueRanges[SympyBoolean], other: ValueRanges[SympyBoolean]) -> ValueRanges[SympyBoolean]:\n+        ...\n+\n+    def __or__(self: AllVR, other: AllVR) -> AllVR:\n         if ValueRanges.unknown() in (self, other):\n             return ValueRanges.unknown()\n         assert self.is_bool == other.is_bool, (self, other)\n-        if vr_is_bool(self):\n-            return cast(ValueRanges[_T], ValueRanges(sympy.And(self.lower, other.lower), sympy.Or(self.upper, other.upper)))\n-        elif vr_is_expr(self):\n-            return cast(ValueRanges[_T], ValueRanges(sympy.Min(self.lower, other.lower), sympy.Max(self.upper, other.upper)))\n+        if self.is_bool:\n+            return ValueRanges(sympy.And(self.lower, other.lower), sympy.Or(self.upper, other.upper))\n         else:\n-            raise AssertionError(\"impossible\")\n+            return ValueRanges(sympy.Min(self.lower, other.lower), sympy.Max(self.upper, other.upper))\n \n     def is_singleton(self) -> bool:\n         return self.lower == self.upper\n@@ -146,43 +183,76 @@ class ValueRanges(Generic[_T]):\n     def unknown_bool() -> ValueRanges[SympyBoolean]:\n         return ValueRanges(sympy.false, sympy.true)\n \n-    @classmethod\n-    def wrap(cls, arg):\n+    @overload\n+    @staticmethod\n+    # work around the fact that bool and int overlap\n+    def wrap(arg: Union[ExprIn, ExprVR]) -> ExprVR:  # type: ignore[overload-overlap]\n+        ...\n+\n+    @overload\n+    @staticmethod\n+    def wrap(arg: Union[BoolIn, BoolVR]) -> BoolVR:\n+        ...\n+\n+    @staticmethod\n+    def wrap(arg: Union[AllIn, AllVR]) -> AllVR:\n         if isinstance(arg, ValueRanges):\n             return arg\n-        return ValueRanges(arg, arg)\n+        # arg is either ExprIn or BoolIn, but we don't know it here\n+        return ValueRanges(arg, arg)  # type: ignore[arg-type]\n \n-    @classmethod\n-    def increasing_map(cls, x, fn):\n+    @staticmethod\n+    def increasing_map(x: Union[ExprIn, ExprVR], fn: ExprFn) -> ExprVR:\n         \"\"\"Increasing: x <= y => f(x) <= f(y).\"\"\"\n-        x = cls.wrap(x)\n+        x = ValueRanges.wrap(x)\n         return ValueRanges(fn(x.lower), fn(x.upper))\n \n-    @classmethod\n-    def decreasing_map(cls, x, fn):\n+    @overload\n+    @staticmethod\n+    def decreasing_map(x: Union[ExprIn, ExprVR], fn: ExprFn) -> ExprVR:\n+        ...\n+\n+    @overload\n+    @staticmethod\n+    def decreasing_map(x: Union[BoolIn, BoolVR], fn: BoolFn) -> BoolVR:\n+        ...\n+\n+    @staticmethod\n+    def decreasing_map(x: Union[AllIn, AllVR], fn: AllFn) -> AllVR:\n         \"\"\"Decreasing: x <= y => f(x) >= f(y).\"\"\"\n-        x = cls.wrap(x)\n-        return ValueRanges(fn(x.upper), fn(x.lower))\n+        x = ValueRanges.wrap(x)\n+        # consistently either Expr or Bool, but we don't know it here\n+        return ValueRanges(fn(x.upper), fn(x.lower))  # type: ignore[arg-type]\n \n-    @classmethod\n-    def monotone_map(cls, x, fn):\n+    @staticmethod\n+    def monotone_map(x: Union[ExprIn, ExprVR], fn: ExprFn) -> ExprVR:\n         \"\"\"It's increasing or decreasing.\"\"\"\n-        x = cls.wrap(x)\n+        x = ValueRanges.wrap(x)\n         l = fn(x.lower)\n         u = fn(x.upper)\n         return ValueRanges(min(l, u), max(l, u))\n \n-    @classmethod\n-    def convex_min_zero_map(cls, x, fn):\n+    @staticmethod\n+    def convex_min_zero_map(x: Union[ExprIn, ExprVR], fn: ExprFn) -> ExprVR:\n         \"\"\"Fn is convex and has a minimum at 0.\"\"\"\n         x = ValueRanges.wrap(x)\n         if 0 in x:\n             return ValueRanges(0, max(fn(x.lower), fn(x.upper)))\n         else:\n-            return cls.monotone_map(x, fn)\n+            return ValueRanges.monotone_map(x, fn)\n \n-    @classmethod\n-    def coordinatewise_increasing_map(cls, x, y, fn):\n+    @overload\n+    @staticmethod\n+    def coordinatewise_increasing_map(x: Union[ExprIn, ExprVR], y: Union[ExprIn, ExprVR], fn: ExprFn2) -> ExprVR:\n+        ...\n+\n+    @overload\n+    @staticmethod\n+    def coordinatewise_increasing_map(x: Union[BoolIn, BoolVR], y: Union[BoolIn, BoolVR], fn: BoolFn2) -> BoolVR:\n+        ...\n+\n+    @staticmethod\n+    def coordinatewise_increasing_map(x: Union[AllIn, AllVR], y: Union[AllIn, AllVR], fn: AllFn2) -> AllVR:\n         \"\"\"\n         It's increasing on each coordinate.\n \n@@ -190,10 +260,10 @@ class ValueRanges(Generic[_T]):\n         For every 1 <= i <= n and x_i <= y_i we have that\n         f(x1, .., xn) <= f(x1, , yi, ..., xn)\n         \"\"\"\n-        x, y = cls.wrap(x), cls.wrap(y)\n+        x, y = ValueRanges.wrap(x), ValueRanges.wrap(y)\n         return ValueRanges(\n-            fn(x.lower, y.lower),\n-            fn(x.upper, y.upper),\n+            fn(x.lower, y.lower),  # type: ignore[arg-type]\n+            fn(x.upper, y.upper),  # type: ignore[arg-type]\n         )\n \n     @classmethod\n@@ -450,7 +520,7 @@ class SymPyValueRangeAnalysis:\n         b = ValueRanges.wrap(b)\n \n         # Performs upcasting first\n-        def fn_(x, y):\n+        def fn_(x: sympy.Expr, y: sympy.Expr) -> sympy.Expr:\n             # Poorman's version of upcasting in Sympy\n             # Inf is not a float...\n             if x.is_Integer and y.is_Integer:"
        }
    ]
},
{
    "Id": 498,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/10c646295d3512237cfb3ab44aa21dfcc9832441",
    "date": "2023-09-27T07:29:46+00:00",
    "message": "When doing typed typecheck, also check signature with symint removed (#109727)\n\nSee the test case for what we didn't catch (SymInt vs const SymInt&\nmismatch.)\n\nIt's necessary to test for both, because we will fall back to the\nnon-SymInt signature if there is no SymInt unboxed kernel available.\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109727\nApproved by: https://github.com/zou3519",
    "label": "YES",
    "changes": [
        {
            "name": "fbgemm.txt",
            "path": ".github/ci_commit_pins/fbgemm.txt",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 1,
                    "new_start": 1,
                    "new_length": 1,
                    "hunk": "@@ -1,1 +1,1 @@\n-1b2746f642cc2c99fe9d1a0c34359c0de45341c2\n+0346155d7f15fbe8be72687e665078edbe1ca5aa\n"
                }
            ],
            "whole_deleted": "-1b2746f642cc2c99fe9d1a0c34359c0de45341c2\n",
            "whole_added": "+0346155d7f15fbe8be72687e665078edbe1ca5aa\n",
            "whole_hunk": "@@ -1,1 +1,1 @@\n-1b2746f642cc2c99fe9d1a0c34359c0de45341c2\n+0346155d7f15fbe8be72687e665078edbe1ca5aa\n"
        },
        {
            "name": "KernelFunction.h",
            "path": "aten/src/ATen/core/boxing/KernelFunction.h",
            "patches": [
                {
                    "old_start": 18,
                    "old_length": 10,
                    "new_start": 18,
                    "new_length": 10,
                    "hunk": "@@ -18,10 +18,10 @@ class KernelFunction;\n template <typename T>\n using has_symint =\n   guts::disjunction<\n-    std::is_same<c10::SymInt, std::decay_t<T>>,\n-    std::is_same<c10::SymIntArrayRef, std::decay_t<T>>,\n-    std::is_same<at::OptionalSymIntArrayRef, std::decay_t<T>>,\n-    std::is_same<c10::optional<c10::SymInt>, std::decay_t<T>>\n+    std::is_same<c10::SymInt, T>,\n+    std::is_same<c10::SymIntArrayRef, T>,\n+    std::is_same<at::OptionalSymIntArrayRef, T>,\n+    std::is_same<c10::optional<c10::SymInt>, T>\n   >;\n \n template <typename T>\n"
                },
                {
                    "old_start": 65,
                    "old_length": 6,
                    "new_start": 65,
                    "new_length": 14,
                    "hunk": "@@ -65,6 +65,14 @@ using fn_has_symint = typename guts::typelist::true_for_any_type<\n   typename guts::infer_function_traits<T>::type::parameter_types\n >;\n \n+template <typename T>\n+struct fn_remove_symint;\n+\n+template <typename Ret, typename... Args>\n+struct fn_remove_symint<Ret(Args...)> {\n+  using type = Ret(typename remove_symint<Args>::type...);\n+};\n+\n /**\n  * KernelFunction is similar to std::function but stores a kernel function.\n  * You can create a KernelFunction from a boxed or unboxed function/functor/lambda\n"
                }
            ],
            "whole_deleted": "-    std::is_same<c10::SymInt, std::decay_t<T>>,\n-    std::is_same<c10::SymIntArrayRef, std::decay_t<T>>,\n-    std::is_same<at::OptionalSymIntArrayRef, std::decay_t<T>>,\n-    std::is_same<c10::optional<c10::SymInt>, std::decay_t<T>>\n",
            "whole_added": "+    std::is_same<c10::SymInt, T>,\n+    std::is_same<c10::SymIntArrayRef, T>,\n+    std::is_same<at::OptionalSymIntArrayRef, T>,\n+    std::is_same<c10::optional<c10::SymInt>, T>\n+template <typename T>\n+struct fn_remove_symint;\n+\n+template <typename Ret, typename... Args>\n+struct fn_remove_symint<Ret(Args...)> {\n+  using type = Ret(typename remove_symint<Args>::type...);\n+};\n+\n",
            "whole_hunk": "@@ -18,10 +18,10 @@ class KernelFunction;\n template <typename T>\n using has_symint =\n   guts::disjunction<\n-    std::is_same<c10::SymInt, std::decay_t<T>>,\n-    std::is_same<c10::SymIntArrayRef, std::decay_t<T>>,\n-    std::is_same<at::OptionalSymIntArrayRef, std::decay_t<T>>,\n-    std::is_same<c10::optional<c10::SymInt>, std::decay_t<T>>\n+    std::is_same<c10::SymInt, T>,\n+    std::is_same<c10::SymIntArrayRef, T>,\n+    std::is_same<at::OptionalSymIntArrayRef, T>,\n+    std::is_same<c10::optional<c10::SymInt>, T>\n   >;\n \n template <typename T>\n@@ -65,6 +65,14 @@ using fn_has_symint = typename guts::typelist::true_for_any_type<\n   typename guts::infer_function_traits<T>::type::parameter_types\n >;\n \n+template <typename T>\n+struct fn_remove_symint;\n+\n+template <typename Ret, typename... Args>\n+struct fn_remove_symint<Ret(Args...)> {\n+  using type = Ret(typename remove_symint<Args>::type...);\n+};\n+\n /**\n  * KernelFunction is similar to std::function but stores a kernel function.\n  * You can create a KernelFunction from a boxed or unboxed function/functor/lambda\n"
        },
        {
            "name": "make_boxed_from_unboxed_functor.h",
            "path": "aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h",
            "patches": [
                {
                    "old_start": 179,
                    "old_length": 10,
                    "new_start": 179,
                    "new_length": 6,
                    "hunk": "@@ -179,10 +179,6 @@ namespace impl {\n       \"You tried to register a kernel with an unsupported input type: std::array<Scalar, N>. Please use std::array<int64_t, N> instead.\");\n   };\n \n-  // The following specialisations of assert_is_valid_input_type are technically not\n-  // necessary since we would hit the base case and show an error message\n-  // there if they didn't exist, but we can show a better error message\n-  // in some common error scenarios.\n   template<class T, bool AllowDeprecatedTypes>\n   struct assert_is_valid_input_type<T, AllowDeprecatedTypes, std::enable_if_t<std::is_same<float, T>::value>> {\n     // There is no reason to support float when we have double. Keep the API lean.\n"
                },
                {
                    "old_start": 204,
                    "old_length": 6,
                    "new_start": 200,
                    "new_length": 14,
                    "hunk": "@@ -204,6 +200,14 @@ namespace impl {\n     static_assert(guts::false_t<T>::value,\n       \"You tried to register a kernel with an unsupported integral input type. Please use int64_t instead.\");\n   };\n+  template<class T, bool AllowDeprecatedTypes>\n+  struct assert_is_valid_input_type<T, AllowDeprecatedTypes, std::enable_if_t<std::is_same<const c10::SymInt&, T>::value>> {\n+    static_assert(guts::false_t<T>::value,\n+      \"You tried to register a kernel taking c10::SymInt by reference. Please accept it by value instead.\");\n+  };\n+\n+  // TODO: it probably would be good to tighten this up quite a bit more with\n+  // an explicit list for everything\n \n   //\n   // assert_is_valid_output_type\n"
                }
            ],
            "whole_deleted": "-  // The following specialisations of assert_is_valid_input_type are technically not\n-  // necessary since we would hit the base case and show an error message\n-  // there if they didn't exist, but we can show a better error message\n-  // in some common error scenarios.\n",
            "whole_added": "+  template<class T, bool AllowDeprecatedTypes>\n+  struct assert_is_valid_input_type<T, AllowDeprecatedTypes, std::enable_if_t<std::is_same<const c10::SymInt&, T>::value>> {\n+    static_assert(guts::false_t<T>::value,\n+      \"You tried to register a kernel taking c10::SymInt by reference. Please accept it by value instead.\");\n+  };\n+\n+  // TODO: it probably would be good to tighten this up quite a bit more with\n+  // an explicit list for everything\n",
            "whole_hunk": "@@ -179,10 +179,6 @@ namespace impl {\n       \"You tried to register a kernel with an unsupported input type: std::array<Scalar, N>. Please use std::array<int64_t, N> instead.\");\n   };\n \n-  // The following specialisations of assert_is_valid_input_type are technically not\n-  // necessary since we would hit the base case and show an error message\n-  // there if they didn't exist, but we can show a better error message\n-  // in some common error scenarios.\n   template<class T, bool AllowDeprecatedTypes>\n   struct assert_is_valid_input_type<T, AllowDeprecatedTypes, std::enable_if_t<std::is_same<float, T>::value>> {\n     // There is no reason to support float when we have double. Keep the API lean.\n@@ -204,6 +200,14 @@ namespace impl {\n     static_assert(guts::false_t<T>::value,\n       \"You tried to register a kernel with an unsupported integral input type. Please use int64_t instead.\");\n   };\n+  template<class T, bool AllowDeprecatedTypes>\n+  struct assert_is_valid_input_type<T, AllowDeprecatedTypes, std::enable_if_t<std::is_same<const c10::SymInt&, T>::value>> {\n+    static_assert(guts::false_t<T>::value,\n+      \"You tried to register a kernel taking c10::SymInt by reference. Please accept it by value instead.\");\n+  };\n+\n+  // TODO: it probably would be good to tighten this up quite a bit more with\n+  // an explicit list for everything\n \n   //\n   // assert_is_valid_output_type\n"
        },
        {
            "name": "Dispatcher.h",
            "path": "aten/src/ATen/core/dispatch/Dispatcher.h",
            "patches": [
                {
                    "old_start": 438,
                    "old_length": 6,
                    "new_start": 438,
                    "new_length": 9,
                    "hunk": "@@ -438,6 +438,9 @@ public:\n     // will be done by the time a typed() handle is acquired.\n #if !defined C10_MOBILE\n     operatorDef_->op.assertSignatureIsCorrect<FuncType>();\n+    if (fn_has_symint<FuncType>::value) {\n+      operatorDef_->op.assertSignatureIsCorrect<typename fn_remove_symint<FuncType>::type>();\n+    }\n #endif\n     return TypedOperatorHandle<FuncType>(operatorIterator_);\n   }\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    if (fn_has_symint<FuncType>::value) {\n+      operatorDef_->op.assertSignatureIsCorrect<typename fn_remove_symint<FuncType>::type>();\n+    }\n",
            "whole_hunk": "@@ -438,6 +438,9 @@ public:\n     // will be done by the time a typed() handle is acquired.\n #if !defined C10_MOBILE\n     operatorDef_->op.assertSignatureIsCorrect<FuncType>();\n+    if (fn_has_symint<FuncType>::value) {\n+      operatorDef_->op.assertSignatureIsCorrect<typename fn_remove_symint<FuncType>::type>();\n+    }\n #endif\n     return TypedOperatorHandle<FuncType>(operatorIterator_);\n   }\n"
        },
        {
            "name": "op_registration_test.cpp",
            "path": "aten/src/ATen/core/op_registration/op_registration_test.cpp",
            "patches": [
                {
                    "old_start": 2157,
                    "old_length": 6,
                    "new_start": 2157,
                    "new_length": 70,
                    "hunk": "@@ -2157,6 +2157,70 @@ TEST(OperatorRegistrationTest, getRegistrationsForDispatchKey) {\n   ASSERT_TRUE(std::includes(all_ops.begin(), all_ops.end(), cpu_ops.begin(), cpu_ops.end(), cmp_lambda));\n }\n \n+Tensor symint_op(const Tensor& self, int64_t length) {\n+  return self.clone();\n+}\n+\n+TEST(OperatorRegistrationTest, TestSymNonSymCompatibility) {\n+  auto m = MAKE_TORCH_LIBRARY(_test);\n+  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n+  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n+  m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op));\n+\n+  auto opHandle = c10::Dispatcher::singleton().findSchemaOrThrow(\n+      \"_test::symint_op\", \"\");\n+\n+  opHandle.typed<Tensor(const Tensor&, int64_t)>().call(dummyTensor(c10::DispatchKey::CPU), 4);\n+  opHandle.typed<Tensor(const Tensor&, c10::SymInt)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+\n+  expectThrows<c10::Error>([&] {\n+    opHandle.typed<Tensor(const Tensor&, const c10::SymInt&)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+  }, \"Tried to access or call an operator with a wrong signature\");\n+}\n+\n+Tensor symint_op2(const Tensor& self, c10::SymInt length) {\n+  return self.clone();\n+}\n+\n+TEST(OperatorRegistrationTest, TestSymSymCompatibility) {\n+  auto m = MAKE_TORCH_LIBRARY(_test);\n+  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n+  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n+  m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op2));\n+\n+  auto opHandle = c10::Dispatcher::singleton().findSchemaOrThrow(\n+      \"_test::symint_op\", \"\");\n+\n+  opHandle.typed<Tensor(const Tensor&, int64_t)>().call(dummyTensor(c10::DispatchKey::CPU), 4);\n+  opHandle.typed<Tensor(const Tensor&, c10::SymInt)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+  // TODO: We should reject this on principle, but today it accidentally works\n+  // due to going through the boxed calling convention.\n+  //\n+  // First, we attempt to test if const SymInt& has SymInt. It does not,\n+  // because we only accept something as SymInt if it has exactly SymInt in\n+  // its signature. So we check if there is a non-symint kernel. But there is\n+  // no non-SymInt kernel, because we only registered a real SymInt kernel.\n+  // When this occurs, we fall back to the boxed calling convention.  And the\n+  // boxed calling convention can deal with const SymInt& fine, as during\n+  // boxing it will just create a SymInt to push onto the argument stack and\n+  // everything is fine.\n+  opHandle.typed<Tensor(const Tensor&, const c10::SymInt&)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+}\n+\n+Tensor symint_op3(const Tensor& self, const c10::SymInt& length) {\n+  return self.clone();\n+}\n+\n+TEST(OperatorRegistrationTest, TestSymSymRefCompatibility) {\n+  auto m = MAKE_TORCH_LIBRARY(_test);\n+  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n+  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n+\n+  expectThrows<c10::Error>([&] {\n+    m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op3));\n+  }, \"doesn't match the expected function schema\");\n+}\n+\n }\n \n #pragma GCC diagnostic pop"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+Tensor symint_op(const Tensor& self, int64_t length) {\n+  return self.clone();\n+}\n+\n+TEST(OperatorRegistrationTest, TestSymNonSymCompatibility) {\n+  auto m = MAKE_TORCH_LIBRARY(_test);\n+  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n+  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n+  m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op));\n+\n+  auto opHandle = c10::Dispatcher::singleton().findSchemaOrThrow(\n+      \"_test::symint_op\", \"\");\n+\n+  opHandle.typed<Tensor(const Tensor&, int64_t)>().call(dummyTensor(c10::DispatchKey::CPU), 4);\n+  opHandle.typed<Tensor(const Tensor&, c10::SymInt)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+\n+  expectThrows<c10::Error>([&] {\n+    opHandle.typed<Tensor(const Tensor&, const c10::SymInt&)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+  }, \"Tried to access or call an operator with a wrong signature\");\n+}\n+\n+Tensor symint_op2(const Tensor& self, c10::SymInt length) {\n+  return self.clone();\n+}\n+\n+TEST(OperatorRegistrationTest, TestSymSymCompatibility) {\n+  auto m = MAKE_TORCH_LIBRARY(_test);\n+  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n+  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n+  m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op2));\n+\n+  auto opHandle = c10::Dispatcher::singleton().findSchemaOrThrow(\n+      \"_test::symint_op\", \"\");\n+\n+  opHandle.typed<Tensor(const Tensor&, int64_t)>().call(dummyTensor(c10::DispatchKey::CPU), 4);\n+  opHandle.typed<Tensor(const Tensor&, c10::SymInt)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+  // TODO: We should reject this on principle, but today it accidentally works\n+  // due to going through the boxed calling convention.\n+  //\n+  // First, we attempt to test if const SymInt& has SymInt. It does not,\n+  // because we only accept something as SymInt if it has exactly SymInt in\n+  // its signature. So we check if there is a non-symint kernel. But there is\n+  // no non-SymInt kernel, because we only registered a real SymInt kernel.\n+  // When this occurs, we fall back to the boxed calling convention.  And the\n+  // boxed calling convention can deal with const SymInt& fine, as during\n+  // boxing it will just create a SymInt to push onto the argument stack and\n+  // everything is fine.\n+  opHandle.typed<Tensor(const Tensor&, const c10::SymInt&)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+}\n+\n+Tensor symint_op3(const Tensor& self, const c10::SymInt& length) {\n+  return self.clone();\n+}\n+\n+TEST(OperatorRegistrationTest, TestSymSymRefCompatibility) {\n+  auto m = MAKE_TORCH_LIBRARY(_test);\n+  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n+  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n+\n+  expectThrows<c10::Error>([&] {\n+    m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op3));\n+  }, \"doesn't match the expected function schema\");\n+}\n+\n",
            "whole_hunk": "@@ -2157,6 +2157,70 @@ TEST(OperatorRegistrationTest, getRegistrationsForDispatchKey) {\n   ASSERT_TRUE(std::includes(all_ops.begin(), all_ops.end(), cpu_ops.begin(), cpu_ops.end(), cmp_lambda));\n }\n \n+Tensor symint_op(const Tensor& self, int64_t length) {\n+  return self.clone();\n+}\n+\n+TEST(OperatorRegistrationTest, TestSymNonSymCompatibility) {\n+  auto m = MAKE_TORCH_LIBRARY(_test);\n+  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n+  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n+  m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op));\n+\n+  auto opHandle = c10::Dispatcher::singleton().findSchemaOrThrow(\n+      \"_test::symint_op\", \"\");\n+\n+  opHandle.typed<Tensor(const Tensor&, int64_t)>().call(dummyTensor(c10::DispatchKey::CPU), 4);\n+  opHandle.typed<Tensor(const Tensor&, c10::SymInt)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+\n+  expectThrows<c10::Error>([&] {\n+    opHandle.typed<Tensor(const Tensor&, const c10::SymInt&)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+  }, \"Tried to access or call an operator with a wrong signature\");\n+}\n+\n+Tensor symint_op2(const Tensor& self, c10::SymInt length) {\n+  return self.clone();\n+}\n+\n+TEST(OperatorRegistrationTest, TestSymSymCompatibility) {\n+  auto m = MAKE_TORCH_LIBRARY(_test);\n+  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n+  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n+  m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op2));\n+\n+  auto opHandle = c10::Dispatcher::singleton().findSchemaOrThrow(\n+      \"_test::symint_op\", \"\");\n+\n+  opHandle.typed<Tensor(const Tensor&, int64_t)>().call(dummyTensor(c10::DispatchKey::CPU), 4);\n+  opHandle.typed<Tensor(const Tensor&, c10::SymInt)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+  // TODO: We should reject this on principle, but today it accidentally works\n+  // due to going through the boxed calling convention.\n+  //\n+  // First, we attempt to test if const SymInt& has SymInt. It does not,\n+  // because we only accept something as SymInt if it has exactly SymInt in\n+  // its signature. So we check if there is a non-symint kernel. But there is\n+  // no non-SymInt kernel, because we only registered a real SymInt kernel.\n+  // When this occurs, we fall back to the boxed calling convention.  And the\n+  // boxed calling convention can deal with const SymInt& fine, as during\n+  // boxing it will just create a SymInt to push onto the argument stack and\n+  // everything is fine.\n+  opHandle.typed<Tensor(const Tensor&, const c10::SymInt&)>().call(dummyTensor(c10::DispatchKey::CPU), c10::SymInt(4));\n+}\n+\n+Tensor symint_op3(const Tensor& self, const c10::SymInt& length) {\n+  return self.clone();\n+}\n+\n+TEST(OperatorRegistrationTest, TestSymSymRefCompatibility) {\n+  auto m = MAKE_TORCH_LIBRARY(_test);\n+  m.def(\"_test::symint_op(Tensor self, SymInt length) -> Tensor\");\n+  auto m_cpu = MAKE_TORCH_LIBRARY_IMPL(_test, CPU);\n+\n+  expectThrows<c10::Error>([&] {\n+    m_cpu.impl(\"symint_op\", c10::DispatchKey::CPU, TORCH_FN(symint_op3));\n+  }, \"doesn't match the expected function schema\");\n+}\n+\n }\n \n #pragma GCC diagnostic pop"
        }
    ]
},
{
    "Id": 383,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/03ff44c958913dedbf160b2c2f6b9107cfd160c1",
    "date": "2023-12-11T21:06:05+00:00",
    "message": "[c10d] Fix Store check condition in NCCL PG watchdog (#115475)\n\nIn https://github.com/pytorch/pytorch/pull/115449/ somehow after turning on `DUMP_ON_TIMEOUT=1`, some existing tests failed. Upon checking, the failing is because of TCPStore check call within watchdog thread.\n\n1. It's not because of TCPStore creation has not completed, because if I put it sleep for a long time, the test still failed. Rather, it's because we query TCPStore after we shutdown the PG.\n\n2. The reason for that is: The `std::chrono::steady_clock::now()` function in C++ returns a `time_point` object representing the current point in time according to the steady clock. The default unit of this time_point is not directly specified in terms of seconds or nanoseconds; rather, it is dependent on the internal representation of the steady clock, which can vary between implementations. In reality it's actually nanosecs which makes the delta so big that we are checking the store every time when watchdog thread wakes up. To make things even worse, `terminateProcessGroup_` might be turned to be `True` before the next check for the outmost while but before TCPStore check, so watchdog gets stuck because we are checking a TCPStore which is already deleted. And main thread is still waiting for watchdog to join.\n\nThe solution here is:\n1. Add back `std::chrono::duration_cast` to ensure the delta is indeed mil_sec, so that the timeout check logic is working as expected.\n2. Check `terminateProcessGroup_` as well so that, we don't do any dump when main thread has already mark the process exited.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115475\nApproved by: https://github.com/wconstab",
    "label": "YES",
    "changes": [
        {
            "name": "ProcessGroupNCCL.cpp",
            "path": "torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp",
            "patches": [
                {
                    "old_start": 1362,
                    "old_length": 8,
                    "new_start": 1362,
                    "new_length": 13,
                    "hunk": "@@ -1362,8 +1362,13 @@ void ProcessGroupNCCL::workCleanupLoop() {\n     if (dumpOnTimeout_) {\n       auto currentTime = std::chrono::steady_clock::now();\n       auto timeSinceLastWorkListUpdate =\n-          (currentTime - lastWorkListUpdateTime_).count();\n-      auto timeSinceLastPollStore = (currentTime - lastTimePollStore).count();\n+          std::chrono::duration_cast<std::chrono::milliseconds>(\n+              (currentTime - lastWorkListUpdateTime_))\n+              .count();\n+      auto timeSinceLastPollStore =\n+          std::chrono::duration_cast<std::chrono::milliseconds>(\n+              (currentTime - lastTimePollStore))\n+              .count();\n       if (timeSinceLastWorkListUpdate >= kWatchdogThreadSleepMillis &&\n           timeSinceLastPollStore >= heartbeatTimeoutInSec_ * 1000) {\n         lastTimePollStore = currentTime;"
                }
            ],
            "whole_deleted": "-          (currentTime - lastWorkListUpdateTime_).count();\n-      auto timeSinceLastPollStore = (currentTime - lastTimePollStore).count();\n",
            "whole_added": "+          std::chrono::duration_cast<std::chrono::milliseconds>(\n+              (currentTime - lastWorkListUpdateTime_))\n+              .count();\n+      auto timeSinceLastPollStore =\n+          std::chrono::duration_cast<std::chrono::milliseconds>(\n+              (currentTime - lastTimePollStore))\n+              .count();\n",
            "whole_hunk": "@@ -1362,8 +1362,13 @@ void ProcessGroupNCCL::workCleanupLoop() {\n     if (dumpOnTimeout_) {\n       auto currentTime = std::chrono::steady_clock::now();\n       auto timeSinceLastWorkListUpdate =\n-          (currentTime - lastWorkListUpdateTime_).count();\n-      auto timeSinceLastPollStore = (currentTime - lastTimePollStore).count();\n+          std::chrono::duration_cast<std::chrono::milliseconds>(\n+              (currentTime - lastWorkListUpdateTime_))\n+              .count();\n+      auto timeSinceLastPollStore =\n+          std::chrono::duration_cast<std::chrono::milliseconds>(\n+              (currentTime - lastTimePollStore))\n+              .count();\n       if (timeSinceLastWorkListUpdate >= kWatchdogThreadSleepMillis &&\n           timeSinceLastPollStore >= heartbeatTimeoutInSec_ * 1000) {\n         lastTimePollStore = currentTime;"
        }
    ]
},
{
    "Id": 274,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/02a410ee12e6f7318c2abb463edfaff456a0bdb4",
    "date": "2024-03-01T04:47:13+00:00",
    "message": "Enable TORCH_TRACE by default in all Tupperware like environments (#120915)\n\nSummary:\nThis is a reimplemented version of the FB specific code in https://www.internalfb.com/diff/D54230697\n\nThe new strategy is that we unconditionally install an FB handler to trace_log logger (and always set level to DEBUG). When the first log message is emitted, we check the JK/filesystem to see if we should actually do logging. If we decide we don't do logging, we remove the handler from trace_log and are done.\n\nbuild_only[github-export-checks,executorch,pytorch_benchmark,pytorch_quantization,pytorch_distributed,pytorch_distributed_gpu,pytorch_dynamo_inductor,pytorch_functorch,pytorch_fx2trt,pytorch_diff_train_tests_ads,glow_fb_pytorch_tests,training_platform,training_platform_compatibility,training_toolkit_applications,training_toolkit_examples,training_toolkit_model_optimization,dper3_pytorch,xplat_caffe2,pytorch_dev,android-pytorch-instrumentation-tests,smartpytorchgithub_first_try_merge,frl-target-determinator,f6-buck,training_platform_for_github,sigmoid_cpu,sigmoid_gpu,aiplatform_modelprocessing_for_github,accelerators_workloads_models_slimdsnn,ae_aotinductor_benchmark_test,aps_,aps_deterministic_ne_tests,dper_lib_silvertorch,torchrec,torchrec_fb,deeplearning_aot_inductor]\n\nTest Plan:\nsandcastle\n\n```\nbuck2 test 'fbcode//mode/dev-nosan' fbcode//torchrec/inference/tests:test_single_gpu_executor -- --exact 'torchrec/inference/tests:test_single_gpu_executor - TorchDeployGPUTest.NestedModelSingleGPU'\nbuck2 test 'fbcode//mode/dev-nosan' fbcode//dper_lib/silvertorch/modules/dynamic_stats/tests:accumulators_test -- --exact 'dper_lib/silvertorch/modules/dynamic_stats/tests:accumulators_test - test_global_fixed_interval_accumulator (dper_lib.silvertorch.modules.dynamic_stats.tests.accumulators_test.GlobalFixedIntervalUnivalentAcculumatorTest)'\n```\n\nAlso running a test flow with/without JK enabled\n\nDifferential Revision: D54275086\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/120915\nApproved by: https://github.com/yanboliang",
    "label": "NO",
    "changes": [
        {
            "name": "_internal.py",
            "path": "torch/_logging/_internal.py",
            "patches": [
                {
                    "old_start": 6,
                    "old_length": 6,
                    "new_start": 6,
                    "new_length": 7,
                    "hunk": "@@ -6,6 +6,7 @@ import logging\n import os\n import os.path\n import re\n+import tempfile\n from dataclasses import dataclass, field\n from importlib import __import__\n from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union\n"
                },
                {
                    "old_start": 844,
                    "old_length": 7,
                    "new_start": 845,
                    "new_length": 6,
                    "hunk": "@@ -844,7 +845,6 @@ def _reset_logs():\n         log.setLevel(logging.NOTSET)\n         log.propagate = True\n \n-    trace_log.setLevel(logging.WARNING)\n     trace_log.propagate = False\n     _clear_handlers(trace_log)\n \n"
                },
                {
                    "old_start": 911,
                    "old_length": 19,
                    "new_start": 911,
                    "new_length": 26,
                    "hunk": "@@ -911,19 +911,26 @@ def _init_logs(log_file_name=None):\n     handler: Optional[logging.Handler] = None\n     if trace_file_name is not None:\n         handler = logging.FileHandler(trace_file_name)\n-    if handler is not None:\n-        trace_log.setLevel(logging.DEBUG)\n-        trace_log_handler = _track_handler(handler)\n-        trace_log_handler.setFormatter(TorchLogsFormatter(trace=True))\n-        trace_log.addHandler(trace_log_handler)\n-\n-\n-class FreshFileHandler(logging.StreamHandler):\n+    else:\n+        # This handler may remove itself if we are not actually in an FB\n+        # environment.  This allows us to defer actually initializing it until\n+        # we actually need to log anything.  This is important because JK\n+        # initializes a C++ singleton, which will pork our process if we\n+        # subsequently fork.\n+        handler = LazyFbTraceHandler()\n+    # This log is ALWAYS at debug level.  We will additionally test if there\n+    # are any handlers before deciding to actually call logging on this.  Do\n+    # not manually call\n+    trace_log.setLevel(logging.DEBUG)\n+    trace_log_handler = _track_handler(handler)\n+    trace_log_handler.setFormatter(TorchLogsFormatter(trace=True))\n+    trace_log.addHandler(trace_log_handler)\n+\n+\n+class LazyFbTraceHandler(logging.StreamHandler):\n     \"\"\"Like FileHandler, but the file is allocated lazily only upon the first log message\"\"\"\n \n-    def __init__(self, filename_cb):\n-        self.filename_cb = filename_cb\n-        self.filename = None\n+    def __init__(self):\n         # This is implemented in the same way that delay is implemented on\n         # FileHandler\n         logging.Handler.__init__(self)\n"
                },
                {
                    "old_start": 954,
                    "old_length": 8,
                    "new_start": 961,
                    "new_length": 50,
                    "hunk": "@@ -954,8 +961,50 @@ class FreshFileHandler(logging.StreamHandler):\n \n     def emit(self, record):\n         if self.stream is None:\n+            # TODO: more robust is_fbcode test\n+            import torch.version\n+\n+            TRACE_LOG_DIR = \"/logs\"\n             open_func = self._builtin_open\n-            self.stream = open_func(self.filename_cb(), \"w\")\n+\n+            ok = False\n+            import torch.version as torch_version\n+\n+            if hasattr(torch_version, \"git_version\"):\n+                log.info(\"LazyFbTraceHandler: disabled because not fbcode\")\n+            elif not torch._utils_internal.justknobs_check(\"pytorch/trace:enable\"):\n+                log.info(\n+                    \"LazyFbTraceHandler: disabled because justknobs_check('pytorch/trace:enable') returned False\"\n+                )\n+            elif not os.path.exists(TRACE_LOG_DIR):\n+                log.info(\n+                    \"LazyFbTraceHandler: disabled because %s does not exist\",\n+                    TRACE_LOG_DIR,\n+                )\n+            elif not os.access(TRACE_LOG_DIR, os.W_OK):\n+                log.info(\n+                    \"LazyFbTraceHandler: disabled because %s is not writeable\",\n+                    TRACE_LOG_DIR,\n+                )\n+            else:\n+                ok = True\n+\n+            if ok:\n+                ranksuffix = \"\"\n+                if dist.is_available() and dist.is_initialized():\n+                    ranksuffix = f\"rank_{dist.get_rank()}_\"\n+                self.stream = tempfile.NamedTemporaryFile(\n+                    mode=\"w+\",\n+                    suffix=\".log\",\n+                    prefix=f\"dedicated_log_torch_trace_{ranksuffix}\",\n+                    dir=TRACE_LOG_DIR,\n+                    delete=False,\n+                )\n+                log.info(\"LazyFbTraceHandler: logging to %s\", self.stream.name)\n+            else:\n+                # We go poof, remove and no-op\n+                trace_log.removeHandler(self)\n+                return\n         if self.stream:\n             super().emit(record)\n \n"
                },
                {
                    "old_start": 1004,
                    "old_length": 7,
                    "new_start": 1053,
                    "new_length": 9,
                    "hunk": "@@ -1004,7 +1053,9 @@ def trace_structured(\n     assert callable(\n         payload_fn\n     ), f\"payload_fn should be callable, but got {type(payload_fn)}\"\n-    if trace_log.isEnabledFor(logging.DEBUG):\n+    # trace_log never propagates and is ALWAYS DEBUG, so also check that there\n+    # are handlers instead of checking the log level\n+    if trace_log.handlers:\n         record: Dict[str, object] = {}\n         record[name] = metadata_fn()\n         if not suppress_context:\n"
                }
            ],
            "whole_deleted": "-    trace_log.setLevel(logging.WARNING)\n-    if handler is not None:\n-        trace_log.setLevel(logging.DEBUG)\n-        trace_log_handler = _track_handler(handler)\n-        trace_log_handler.setFormatter(TorchLogsFormatter(trace=True))\n-        trace_log.addHandler(trace_log_handler)\n-\n-\n-class FreshFileHandler(logging.StreamHandler):\n-    def __init__(self, filename_cb):\n-        self.filename_cb = filename_cb\n-        self.filename = None\n-            self.stream = open_func(self.filename_cb(), \"w\")\n-    if trace_log.isEnabledFor(logging.DEBUG):\n",
            "whole_added": "+import tempfile\n+    else:\n+        # This handler may remove itself if we are not actually in an FB\n+        # environment.  This allows us to defer actually initializing it until\n+        # we actually need to log anything.  This is important because JK\n+        # initializes a C++ singleton, which will pork our process if we\n+        # subsequently fork.\n+        handler = LazyFbTraceHandler()\n+    # This log is ALWAYS at debug level.  We will additionally test if there\n+    # are any handlers before deciding to actually call logging on this.  Do\n+    # not manually call\n+    trace_log.setLevel(logging.DEBUG)\n+    trace_log_handler = _track_handler(handler)\n+    trace_log_handler.setFormatter(TorchLogsFormatter(trace=True))\n+    trace_log.addHandler(trace_log_handler)\n+\n+\n+class LazyFbTraceHandler(logging.StreamHandler):\n+    def __init__(self):\n+            # TODO: more robust is_fbcode test\n+            import torch.version\n+\n+            TRACE_LOG_DIR = \"/logs\"\n+\n+            ok = False\n+            import torch.version as torch_version\n+\n+            if hasattr(torch_version, \"git_version\"):\n+                log.info(\"LazyFbTraceHandler: disabled because not fbcode\")\n+            elif not torch._utils_internal.justknobs_check(\"pytorch/trace:enable\"):\n+                log.info(\n+                    \"LazyFbTraceHandler: disabled because justknobs_check('pytorch/trace:enable') returned False\"\n+                )\n+            elif not os.path.exists(TRACE_LOG_DIR):\n+                log.info(\n+                    \"LazyFbTraceHandler: disabled because %s does not exist\",\n+                    TRACE_LOG_DIR,\n+                )\n+            elif not os.access(TRACE_LOG_DIR, os.W_OK):\n+                log.info(\n+                    \"LazyFbTraceHandler: disabled because %s is not writeable\",\n+                    TRACE_LOG_DIR,\n+                )\n+            else:\n+                ok = True\n+\n+            if ok:\n+                ranksuffix = \"\"\n+                if dist.is_available() and dist.is_initialized():\n+                    ranksuffix = f\"rank_{dist.get_rank()}_\"\n+                self.stream = tempfile.NamedTemporaryFile(\n+                    mode=\"w+\",\n+                    suffix=\".log\",\n+                    prefix=f\"dedicated_log_torch_trace_{ranksuffix}\",\n+                    dir=TRACE_LOG_DIR,\n+                    delete=False,\n+                )\n+                log.info(\"LazyFbTraceHandler: logging to %s\", self.stream.name)\n+            else:\n+                # We go poof, remove and no-op\n+                trace_log.removeHandler(self)\n+                return\n+    # trace_log never propagates and is ALWAYS DEBUG, so also check that there\n+    # are handlers instead of checking the log level\n+    if trace_log.handlers:\n",
            "whole_hunk": "@@ -6,6 +6,7 @@ import logging\n import os\n import os.path\n import re\n+import tempfile\n from dataclasses import dataclass, field\n from importlib import __import__\n from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union\n@@ -844,7 +845,6 @@ def _reset_logs():\n         log.setLevel(logging.NOTSET)\n         log.propagate = True\n \n-    trace_log.setLevel(logging.WARNING)\n     trace_log.propagate = False\n     _clear_handlers(trace_log)\n \n@@ -911,19 +911,26 @@ def _init_logs(log_file_name=None):\n     handler: Optional[logging.Handler] = None\n     if trace_file_name is not None:\n         handler = logging.FileHandler(trace_file_name)\n-    if handler is not None:\n-        trace_log.setLevel(logging.DEBUG)\n-        trace_log_handler = _track_handler(handler)\n-        trace_log_handler.setFormatter(TorchLogsFormatter(trace=True))\n-        trace_log.addHandler(trace_log_handler)\n-\n-\n-class FreshFileHandler(logging.StreamHandler):\n+    else:\n+        # This handler may remove itself if we are not actually in an FB\n+        # environment.  This allows us to defer actually initializing it until\n+        # we actually need to log anything.  This is important because JK\n+        # initializes a C++ singleton, which will pork our process if we\n+        # subsequently fork.\n+        handler = LazyFbTraceHandler()\n+    # This log is ALWAYS at debug level.  We will additionally test if there\n+    # are any handlers before deciding to actually call logging on this.  Do\n+    # not manually call\n+    trace_log.setLevel(logging.DEBUG)\n+    trace_log_handler = _track_handler(handler)\n+    trace_log_handler.setFormatter(TorchLogsFormatter(trace=True))\n+    trace_log.addHandler(trace_log_handler)\n+\n+\n+class LazyFbTraceHandler(logging.StreamHandler):\n     \"\"\"Like FileHandler, but the file is allocated lazily only upon the first log message\"\"\"\n \n-    def __init__(self, filename_cb):\n-        self.filename_cb = filename_cb\n-        self.filename = None\n+    def __init__(self):\n         # This is implemented in the same way that delay is implemented on\n         # FileHandler\n         logging.Handler.__init__(self)\n@@ -954,8 +961,50 @@ class FreshFileHandler(logging.StreamHandler):\n \n     def emit(self, record):\n         if self.stream is None:\n+            # TODO: more robust is_fbcode test\n+            import torch.version\n+\n+            TRACE_LOG_DIR = \"/logs\"\n             open_func = self._builtin_open\n-            self.stream = open_func(self.filename_cb(), \"w\")\n+\n+            ok = False\n+            import torch.version as torch_version\n+\n+            if hasattr(torch_version, \"git_version\"):\n+                log.info(\"LazyFbTraceHandler: disabled because not fbcode\")\n+            elif not torch._utils_internal.justknobs_check(\"pytorch/trace:enable\"):\n+                log.info(\n+                    \"LazyFbTraceHandler: disabled because justknobs_check('pytorch/trace:enable') returned False\"\n+                )\n+            elif not os.path.exists(TRACE_LOG_DIR):\n+                log.info(\n+                    \"LazyFbTraceHandler: disabled because %s does not exist\",\n+                    TRACE_LOG_DIR,\n+                )\n+            elif not os.access(TRACE_LOG_DIR, os.W_OK):\n+                log.info(\n+                    \"LazyFbTraceHandler: disabled because %s is not writeable\",\n+                    TRACE_LOG_DIR,\n+                )\n+            else:\n+                ok = True\n+\n+            if ok:\n+                ranksuffix = \"\"\n+                if dist.is_available() and dist.is_initialized():\n+                    ranksuffix = f\"rank_{dist.get_rank()}_\"\n+                self.stream = tempfile.NamedTemporaryFile(\n+                    mode=\"w+\",\n+                    suffix=\".log\",\n+                    prefix=f\"dedicated_log_torch_trace_{ranksuffix}\",\n+                    dir=TRACE_LOG_DIR,\n+                    delete=False,\n+                )\n+                log.info(\"LazyFbTraceHandler: logging to %s\", self.stream.name)\n+            else:\n+                # We go poof, remove and no-op\n+                trace_log.removeHandler(self)\n+                return\n         if self.stream:\n             super().emit(record)\n \n@@ -1004,7 +1053,9 @@ def trace_structured(\n     assert callable(\n         payload_fn\n     ), f\"payload_fn should be callable, but got {type(payload_fn)}\"\n-    if trace_log.isEnabledFor(logging.DEBUG):\n+    # trace_log never propagates and is ALWAYS DEBUG, so also check that there\n+    # are handlers instead of checking the log level\n+    if trace_log.handlers:\n         record: Dict[str, object] = {}\n         record[name] = metadata_fn()\n         if not suppress_context:\n"
        },
        {
            "name": "_utils_internal.py",
            "path": "torch/_utils_internal.py",
            "patches": [
                {
                    "old_start": 95,
                    "old_length": 6,
                    "new_start": 95,
                    "new_length": 32,
                    "hunk": "@@ -95,6 +95,32 @@ def log_export_usage(**kwargs):\n     pass\n \n \n+def justknobs_check(name: str) -> bool:\n+    \"\"\"\n+    This function can be used to killswitch functionality in FB prod,\n+    where you can toggle this value to False in JK without having to\n+    do a code push.  In OSS, we always have everything turned on all\n+    the time, because downstream users can simply choose to not update\n+    PyTorch.  (If more fine-grained enable/disable is needed, we could\n+    potentially have a map we lookup name in to toggle behavior.  But\n+    the point is that it's all tied to source code in OSS, since there's\n+    no live server to query.)\n+\n+    This is the bare minimum functionality I needed to do some killswitches.\n+    We have a more detailed plan at\n+    https://docs.google.com/document/d/1Ukerh9_42SeGh89J-tGtecpHBPwGlkQ043pddkKb3PU/edit\n+    In particular, in some circumstances it may be necessary to read in\n+    a knob once at process start, and then use it consistently for the\n+    rest of the process.  Future functionality will codify these patterns\n+    into a better high level API.\n+\n+    WARNING: Do NOT call this function at module import time, JK is not\n+    fork safe and you will break anyone who forks the process and then\n+    hits JK again.\n+    \"\"\"\n+    return True\n+\n+\n @functools.lru_cache(None)\n def max_clock_rate():\n     from triton.testing import nvsmi"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+def justknobs_check(name: str) -> bool:\n+    \"\"\"\n+    This function can be used to killswitch functionality in FB prod,\n+    where you can toggle this value to False in JK without having to\n+    do a code push.  In OSS, we always have everything turned on all\n+    the time, because downstream users can simply choose to not update\n+    PyTorch.  (If more fine-grained enable/disable is needed, we could\n+    potentially have a map we lookup name in to toggle behavior.  But\n+    the point is that it's all tied to source code in OSS, since there's\n+    no live server to query.)\n+\n+    This is the bare minimum functionality I needed to do some killswitches.\n+    We have a more detailed plan at\n+    https://docs.google.com/document/d/1Ukerh9_42SeGh89J-tGtecpHBPwGlkQ043pddkKb3PU/edit\n+    In particular, in some circumstances it may be necessary to read in\n+    a knob once at process start, and then use it consistently for the\n+    rest of the process.  Future functionality will codify these patterns\n+    into a better high level API.\n+\n+    WARNING: Do NOT call this function at module import time, JK is not\n+    fork safe and you will break anyone who forks the process and then\n+    hits JK again.\n+    \"\"\"\n+    return True\n+\n+\n",
            "whole_hunk": "@@ -95,6 +95,32 @@ def log_export_usage(**kwargs):\n     pass\n \n \n+def justknobs_check(name: str) -> bool:\n+    \"\"\"\n+    This function can be used to killswitch functionality in FB prod,\n+    where you can toggle this value to False in JK without having to\n+    do a code push.  In OSS, we always have everything turned on all\n+    the time, because downstream users can simply choose to not update\n+    PyTorch.  (If more fine-grained enable/disable is needed, we could\n+    potentially have a map we lookup name in to toggle behavior.  But\n+    the point is that it's all tied to source code in OSS, since there's\n+    no live server to query.)\n+\n+    This is the bare minimum functionality I needed to do some killswitches.\n+    We have a more detailed plan at\n+    https://docs.google.com/document/d/1Ukerh9_42SeGh89J-tGtecpHBPwGlkQ043pddkKb3PU/edit\n+    In particular, in some circumstances it may be necessary to read in\n+    a knob once at process start, and then use it consistently for the\n+    rest of the process.  Future functionality will codify these patterns\n+    into a better high level API.\n+\n+    WARNING: Do NOT call this function at module import time, JK is not\n+    fork safe and you will break anyone who forks the process and then\n+    hits JK again.\n+    \"\"\"\n+    return True\n+\n+\n @functools.lru_cache(None)\n def max_clock_rate():\n     from triton.testing import nvsmi"
        }
    ]
},
{
    "Id": 312,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/3f1f057adfcd4cef67fff9605a894cb075c02881",
    "date": "2024-02-02T05:29:49+00:00",
    "message": "Remove parent device mesh check (#118620)\n\nRemoves raising error if a device_mesh has a parent.\n\nThe comment says that HSDP + TP is not supported, but I'm able to do 2D parallelism + HSDP fine. The only issues are:\n- this check\n- https://github.com/pytorch/pytorch/pull/118618\n- a series of PRs related to checkpointing with 3D meshes that I will open\nWe currently monkeypatch for the above which I am slowly upstreaming.\n\nI imagine torch will have a better, native integration eventually, but this check seems too aggressive in the meantime given DTensor now lets users do some things themselves (which is amazing \ud83c\udf89)!\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/118620\nApproved by: https://github.com/wz337, https://github.com/wanchaol",
    "label": "NO",
    "changes": [
        {
            "name": "_init_utils.py",
            "path": "torch/distributed/fsdp/_init_utils.py",
            "patches": [
                {
                    "old_start": 204,
                    "old_length": 12,
                    "new_start": 204,
                    "new_length": 6,
                    "hunk": "@@ -204,12 +204,6 @@ def _is_valid_hybrid_shard_pg_type(process_group: Any) -> bool:\n \n @no_type_check\n def _is_valid_hybrid_shard_device_mesh(device_mesh: DeviceMesh) -> bool:\n-    parent_mesh = _mesh_resources.get_parent_mesh(device_mesh)\n-    if parent_mesh is not None:\n-        raise RuntimeError(\n-            f\"Found device_mesh {device_mesh} passed in has a parent device_mesh {parent_mesh}.\",\n-            \"Hybrid sharding + TP is not supported yet.\",\n-        )\n     return isinstance(device_mesh, DeviceMesh) and device_mesh.ndim == 2\n \n "
                }
            ],
            "whole_deleted": "-    parent_mesh = _mesh_resources.get_parent_mesh(device_mesh)\n-    if parent_mesh is not None:\n-        raise RuntimeError(\n-            f\"Found device_mesh {device_mesh} passed in has a parent device_mesh {parent_mesh}.\",\n-            \"Hybrid sharding + TP is not supported yet.\",\n-        )\n",
            "whole_added": "",
            "whole_hunk": "@@ -204,12 +204,6 @@ def _is_valid_hybrid_shard_pg_type(process_group: Any) -> bool:\n \n @no_type_check\n def _is_valid_hybrid_shard_device_mesh(device_mesh: DeviceMesh) -> bool:\n-    parent_mesh = _mesh_resources.get_parent_mesh(device_mesh)\n-    if parent_mesh is not None:\n-        raise RuntimeError(\n-            f\"Found device_mesh {device_mesh} passed in has a parent device_mesh {parent_mesh}.\",\n-            \"Hybrid sharding + TP is not supported yet.\",\n-        )\n     return isinstance(device_mesh, DeviceMesh) and device_mesh.ndim == 2\n \n "
        }
    ]
},
{
    "Id": 367,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/491292229782e06b1d99684fc591fabae5445cdd",
    "date": "2023-12-23T08:38:26+00:00",
    "message": "Fake Tensor refactors part 1 (#116344)\n\nThese are mostly small performance optimizations to move constant list construction into global scope and replace O(n) `x in list` checks with O(1) `x in dict` checks.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116344\nApproved by: https://github.com/yanboliang",
    "label": "NO",
    "changes": [
        {
            "name": "fake_tensor.py",
            "path": "torch/_subclasses/fake_tensor.py",
            "patches": [
                {
                    "old_start": 88,
                    "old_length": 7,
                    "new_start": 88,
                    "new_length": 11,
                    "hunk": "@@ -88,7 +88,11 @@ class UnsupportedOperatorException(RuntimeError):\n     func: OpOverload\n \n \n-_device_not_kwarg_ops = (\n+def ordered_set(*items):\n+    return {k: True for k in items}\n+\n+\n+_device_not_kwarg_ops = ordered_set(\n     aten._resize_output_.default,\n     aten._nested_tensor_from_tensor_list.default,\n     aten._nested_tensor_from_tensor_list.out,\n"
                },
                {
                    "old_start": 121,
                    "old_length": 7,
                    "new_start": 125,
                    "new_length": 7,
                    "hunk": "@@ -121,7 +125,7 @@ def contains_tensor_types(type):\n     )\n \n \n-_like_tensor_constructors = (\n+_like_tensor_constructors = ordered_set(\n     aten.empty_like.default,\n     aten.empty_like.out,\n     aten.full_like.default,\n"
                },
                {
                    "old_start": 627,
                    "old_length": 9,
                    "new_start": 631,
                    "new_length": 9,
                    "hunk": "@@ -627,9 +631,9 @@ def masked_select(fake_mode, func, self, mask):\n         has_free_symbols,\n     )\n \n-    if not has_free_symbols(arg.numel()):\n-        if arg.numel() >= 2:\n-            maxval = int(arg.numel())\n+    if not has_free_symbols(self.numel()):\n+        if self.numel() >= 2:\n+            maxval = int(self.numel())\n \n     _constrain_range_for_size(nnz, max=maxval)\n \n"
                },
                {
                    "old_start": 666,
                    "old_length": 8,
                    "new_start": 670,
                    "new_length": 11,
                    "hunk": "@@ -666,8 +670,11 @@ def run_and_return_new_tensor_of_input_device(fake_mode, func, args, kwargs):\n     return FakeTensor(fake_mode, out, out_device)\n \n \n+_is_builtin_namespaces = ordered_set(\"aten\", \"prims\", \"prim\")\n+\n+\n def is_builtin(op):\n-    return op.namespace in (\"aten\", \"prims\", \"prim\")\n+    return op.namespace in _is_builtin_namespaces\n \n \n def has_meta(func):\n"
                },
                {
                    "old_start": 1465,
                    "old_length": 41,
                    "new_start": 1472,
                    "new_length": 22,
                    "hunk": "@@ -1465,41 +1472,22 @@ class FakeTensorMode(TorchDispatchMode):\n                 torch._C._set_dispatch_mode(maybe_prev_fake_mode)\n \n     def dispatch(self, func, types, args=(), kwargs=None):\n-        kwargs = kwargs if kwargs else {}\n+        kwargs = kwargs or {}\n         log.debug(\"%s %s %s\", func, args, kwargs)\n \n-        if func == torch.ops.prim.device.default:\n-            # NB: Don't use is_our_fake, just serve the fake information\n-            # as is.  Notice we don't use 'self'; we use args[0].fake_mode\n-            # because they may not be the same.  It would also be possible\n-            # to return NotImplemented here, in which case the FakeTensor\n-            # handler on args[0] would handle it, but we're being nice and\n-            # short-circuiting quickly.\n-            assert len(args) == 1 and isinstance(args[0], FakeTensor)\n-            if args[0].fake_mode.in_kernel_invocation:\n-                return torch.device(\"meta\")\n-            else:\n-                return args[0].fake_device\n-        elif func is torch.ops.aten.size.default:\n-            return tuple(int(s) for s in args[0].size())\n-        elif func is torch.ops.aten.stride.default:\n-            return tuple(int(s) for s in args[0].stride())\n-        elif func is torch.ops.aten.storage_offset.default:\n-            return int(args[0].storage_offset())\n+        if func in _DISPATCH_META_HANDLERS:\n+            return _DISPATCH_META_HANDLERS[func](args)\n \n         if log.getEffectiveLevel() <= logging.DEBUG:\n             log.debug(\n                 \"%sFakeTensorMode.__torch_dispatch__: %s\", \" \" * RECURSION_COUNT, func\n             )\n+            # NOTE: incr is intentionally unused for a RAII pattern\n             incr = IncrementRecursionCount()\n \n         # Some attribute queries that can be serviced directly\n         # See Note [is_coalesced is dispatched]\n-        if func in {\n-            torch.ops.aten.is_coalesced.default,\n-            torch.ops.aten.dense_dim.default,\n-            torch.ops.aten.sparse_dim.default,\n-        }:\n+        if func in _DISPATCH_HANDLE_DIRECLTY:\n             # NB: no_dispatch is ok here too, this func is very simple\n             with in_kernel_invocation_manager(self):\n                 return func(*args, **kwargs)\n"
                },
                {
                    "old_start": 1700,
                    "old_length": 36,
                    "new_start": 1688,
                    "new_length": 6,
                    "hunk": "@@ -1700,36 +1688,6 @@ class FakeTensorMode(TorchDispatchMode):\n                 if op_impl_out != NotImplemented:\n                     return op_impl_out\n \n-        def can_run_unsafe_fallback(func: OpOverload):\n-            if not self.allow_fallback_kernels:\n-                return False\n-            # It's OK to try the fallback for built-in ops (e.g. aten, prims)\n-            # because we control and test these but the fallback leads to unexpected behavior\n-            # in user-defined custom ops\n-            #\n-            # WARNING: DO NOT add any additional namespaces/operators here if they refer to operators\n-            # outside of the pytorch/pytorch library! Any pre-existing things here\n-            # are either in the pytorch/pytorch library or have been grandfathered in.\n-            # The fallback does not always work and MAY CRASH and emit unreadable error messages\n-            # so it should not be allowed by default.\n-            allowed_namespaces = {\n-                \"debugprims\",\n-                \"prims\",\n-                \"aten\",\n-                \"xla\",\n-                \"vision\",\n-                \"torchtext\",\n-                \"torchaudio\",\n-                \"quantized\",\n-            }\n-            grandfathered_ops_FIXME = {\n-                \"fbgemm::gmm\",\n-            }\n-            return (\n-                func.namespace in allowed_namespaces\n-                or func.name() in grandfathered_ops_FIXME\n-            )\n-\n         def maybe_run_unsafe_fallback(error=None):\n             # We infer the meta of a custom ops that return None to just\n             # return None. custom ops are not allowed to mutate metadata\n"
                },
                {
                    "old_start": 1737,
                    "old_length": 7,
                    "new_start": 1695,
                    "new_length": 7,
                    "hunk": "@@ -1737,7 +1695,7 @@ class FakeTensorMode(TorchDispatchMode):\n             if can_generate_trivial_abstract_impl(func):\n                 return None\n             # no meta kernel registered, fallback to kernel for the device\n-            if has_symbolic_sizes or not can_run_unsafe_fallback(func):\n+            if has_symbolic_sizes or not self.can_run_unsafe_fallback(func):\n                 raise UnsupportedOperatorException(func)\n             if error is None:\n                 error = UnsupportedOperatorException(func)\n"
                },
                {
                    "old_start": 1761,
                    "old_length": 6,
                    "new_start": 1719,
                    "new_length": 33,
                    "hunk": "@@ -1761,6 +1719,33 @@ class FakeTensorMode(TorchDispatchMode):\n             r, func, flat_args, device=kwargs.get(\"device\")\n         )\n \n+    # WARNING: DO NOT add any additional namespaces/operators here if they refer to operators\n+    # outside of the pytorch/pytorch library! Any pre-existing things here\n+    # are either in the pytorch/pytorch library or have been grandfathered in.\n+    # The fallback does not always work and MAY CRASH and emit unreadable error messages\n+    # so it should not be allowed by default.\n+    _can_run_unsafe_fallback_allowed_namespaces = ordered_set(\n+        \"debugprims\",\n+        \"prims\",\n+        \"aten\",\n+        \"xla\",\n+        \"vision\",\n+        \"torchtext\",\n+        \"torchaudio\",\n+        \"quantized\",\n+    )\n+\n+    def can_run_unsafe_fallback(self, func: OpOverload):\n+        if not self.allow_fallback_kernels:\n+            return False\n+        # It's OK to try the fallback for built-in ops (e.g. aten, prims)\n+        # because we control and test these but the fallback leads to unexpected behavior\n+        # in user-defined custom ops\n+        return (\n+            func.namespace in self._can_run_unsafe_fallback_allowed_namespaces\n+            or func.name() == \"fbgemm::gmm\"\n+        )\n+\n     # [subclass inputs]\n     # Suppose we enable fake tensor mode.  This means that fake tensor\n     # mode will run first.  But what if we do an operation that\n"
                },
                {
                    "old_start": 1862,
                    "old_length": 26,
                    "new_start": 1847,
                    "new_length": 26,
                    "hunk": "@@ -1862,26 +1847,26 @@ class FakeTensorMode(TorchDispatchMode):\n \n         return tree_map(wrap, r)\n \n+    _cpp_meta_supports_symint = ordered_set(\n+        aten.empty.memory_format,\n+        aten.empty_strided.default,\n+        aten.as_strided_scatter.default,\n+        aten.as_strided.default,\n+        aten.as_strided_.default,\n+        aten.zeros.default,\n+        aten.detach.default,\n+        aten.view_as_real.default,\n+        aten.view_as_complex.default,\n+        aten.set_.source_Storage_storage_offset,\n+        aten._sparse_coo_tensor_with_dims_and_tensors.default,\n+    )\n+\n     def cpp_meta_supports_symint(self, func):\n         if torch.Tag.view_copy in func.tags:\n             return True\n-        return func in [\n-            aten.empty.memory_format,\n-            aten.empty_strided.default,\n-            aten.as_strided_scatter.default,\n-            aten.as_strided.default,\n-            aten.as_strided_.default,\n-            aten.zeros.default,\n-            aten.detach.default,\n-            aten.view_as_real.default,\n-            aten.view_as_complex.default,\n-            aten.set_.source_Storage_storage_offset,\n-            aten._sparse_coo_tensor_with_dims_and_tensors.default,\n-        ]\n+        return func in self._cpp_meta_supports_symint\n \n-    @property\n-    def lift_fns(self):\n-        return (aten.lift_fresh.default, aten.lift_fresh_copy.default)\n+    lift_fns = ordered_set(aten.lift_fresh.default, aten.lift_fresh_copy.default)\n \n     def may_turn_const(self, t):\n         return (\n"
                },
                {
                    "old_start": 2049,
                    "old_length": 3,
                    "new_start": 2034,
                    "new_length": 31,
                    "hunk": "@@ -2049,3 +2034,31 @@ class FakeCopyMode(TorchFunctionMode):\n         else:\n             with torch._C.DisableTorchFunctionSubclass():\n                 return func(*args, **kwargs)\n+\n+\n+def _device_handler(args):\n+    # NB: Don't use is_our_fake, just serve the fake information\n+    # as is.  Notice we don't use 'self'; we use args[0].fake_mode\n+    # because they may not be the same.  It would also be possible\n+    # to return NotImplemented here, in which case the FakeTensor\n+    # handler on args[0] would handle it, but we're being nice and\n+    # short-circuiting quickly.\n+    assert len(args) == 1 and isinstance(args[0], FakeTensor)\n+    if args[0].fake_mode.in_kernel_invocation:\n+        return torch.device(\"meta\")\n+    else:\n+        return args[0].fake_device\n+\n+\n+_DISPATCH_META_HANDLERS = {\n+    torch.ops.prim.device.default: _device_handler,\n+    torch.ops.aten.size.default: lambda args: tuple(int(s) for s in args[0].size()),\n+    torch.ops.aten.stride.default: lambda args: tuple(int(s) for s in args[0].stride()),\n+    torch.ops.aten.storage_offset.default: lambda args: int(args[0].storage_offset()),\n+}\n+\n+_DISPATCH_HANDLE_DIRECLTY = ordered_set(\n+    torch.ops.aten.is_coalesced.default,\n+    torch.ops.aten.dense_dim.default,\n+    torch.ops.aten.sparse_dim.default,\n+)"
                }
            ],
            "whole_deleted": "-_device_not_kwarg_ops = (\n-_like_tensor_constructors = (\n-    if not has_free_symbols(arg.numel()):\n-        if arg.numel() >= 2:\n-            maxval = int(arg.numel())\n-    return op.namespace in (\"aten\", \"prims\", \"prim\")\n-        kwargs = kwargs if kwargs else {}\n-        if func == torch.ops.prim.device.default:\n-            # NB: Don't use is_our_fake, just serve the fake information\n-            # as is.  Notice we don't use 'self'; we use args[0].fake_mode\n-            # because they may not be the same.  It would also be possible\n-            # to return NotImplemented here, in which case the FakeTensor\n-            # handler on args[0] would handle it, but we're being nice and\n-            # short-circuiting quickly.\n-            assert len(args) == 1 and isinstance(args[0], FakeTensor)\n-            if args[0].fake_mode.in_kernel_invocation:\n-                return torch.device(\"meta\")\n-            else:\n-                return args[0].fake_device\n-        elif func is torch.ops.aten.size.default:\n-            return tuple(int(s) for s in args[0].size())\n-        elif func is torch.ops.aten.stride.default:\n-            return tuple(int(s) for s in args[0].stride())\n-        elif func is torch.ops.aten.storage_offset.default:\n-            return int(args[0].storage_offset())\n-        if func in {\n-            torch.ops.aten.is_coalesced.default,\n-            torch.ops.aten.dense_dim.default,\n-            torch.ops.aten.sparse_dim.default,\n-        }:\n-        def can_run_unsafe_fallback(func: OpOverload):\n-            if not self.allow_fallback_kernels:\n-                return False\n-            # It's OK to try the fallback for built-in ops (e.g. aten, prims)\n-            # because we control and test these but the fallback leads to unexpected behavior\n-            # in user-defined custom ops\n-            #\n-            # WARNING: DO NOT add any additional namespaces/operators here if they refer to operators\n-            # outside of the pytorch/pytorch library! Any pre-existing things here\n-            # are either in the pytorch/pytorch library or have been grandfathered in.\n-            # The fallback does not always work and MAY CRASH and emit unreadable error messages\n-            # so it should not be allowed by default.\n-            allowed_namespaces = {\n-                \"debugprims\",\n-                \"prims\",\n-                \"aten\",\n-                \"xla\",\n-                \"vision\",\n-                \"torchtext\",\n-                \"torchaudio\",\n-                \"quantized\",\n-            }\n-            grandfathered_ops_FIXME = {\n-                \"fbgemm::gmm\",\n-            }\n-            return (\n-                func.namespace in allowed_namespaces\n-                or func.name() in grandfathered_ops_FIXME\n-            )\n-\n-            if has_symbolic_sizes or not can_run_unsafe_fallback(func):\n-        return func in [\n-            aten.empty.memory_format,\n-            aten.empty_strided.default,\n-            aten.as_strided_scatter.default,\n-            aten.as_strided.default,\n-            aten.as_strided_.default,\n-            aten.zeros.default,\n-            aten.detach.default,\n-            aten.view_as_real.default,\n-            aten.view_as_complex.default,\n-            aten.set_.source_Storage_storage_offset,\n-            aten._sparse_coo_tensor_with_dims_and_tensors.default,\n-        ]\n-    @property\n-    def lift_fns(self):\n-        return (aten.lift_fresh.default, aten.lift_fresh_copy.default)\n",
            "whole_added": "+def ordered_set(*items):\n+    return {k: True for k in items}\n+\n+\n+_device_not_kwarg_ops = ordered_set(\n+_like_tensor_constructors = ordered_set(\n+    if not has_free_symbols(self.numel()):\n+        if self.numel() >= 2:\n+            maxval = int(self.numel())\n+_is_builtin_namespaces = ordered_set(\"aten\", \"prims\", \"prim\")\n+\n+\n+    return op.namespace in _is_builtin_namespaces\n+        kwargs = kwargs or {}\n+        if func in _DISPATCH_META_HANDLERS:\n+            return _DISPATCH_META_HANDLERS[func](args)\n+            # NOTE: incr is intentionally unused for a RAII pattern\n+        if func in _DISPATCH_HANDLE_DIRECLTY:\n+            if has_symbolic_sizes or not self.can_run_unsafe_fallback(func):\n+    # WARNING: DO NOT add any additional namespaces/operators here if they refer to operators\n+    # outside of the pytorch/pytorch library! Any pre-existing things here\n+    # are either in the pytorch/pytorch library or have been grandfathered in.\n+    # The fallback does not always work and MAY CRASH and emit unreadable error messages\n+    # so it should not be allowed by default.\n+    _can_run_unsafe_fallback_allowed_namespaces = ordered_set(\n+        \"debugprims\",\n+        \"prims\",\n+        \"aten\",\n+        \"xla\",\n+        \"vision\",\n+        \"torchtext\",\n+        \"torchaudio\",\n+        \"quantized\",\n+    )\n+\n+    def can_run_unsafe_fallback(self, func: OpOverload):\n+        if not self.allow_fallback_kernels:\n+            return False\n+        # It's OK to try the fallback for built-in ops (e.g. aten, prims)\n+        # because we control and test these but the fallback leads to unexpected behavior\n+        # in user-defined custom ops\n+        return (\n+            func.namespace in self._can_run_unsafe_fallback_allowed_namespaces\n+            or func.name() == \"fbgemm::gmm\"\n+        )\n+\n+    _cpp_meta_supports_symint = ordered_set(\n+        aten.empty.memory_format,\n+        aten.empty_strided.default,\n+        aten.as_strided_scatter.default,\n+        aten.as_strided.default,\n+        aten.as_strided_.default,\n+        aten.zeros.default,\n+        aten.detach.default,\n+        aten.view_as_real.default,\n+        aten.view_as_complex.default,\n+        aten.set_.source_Storage_storage_offset,\n+        aten._sparse_coo_tensor_with_dims_and_tensors.default,\n+    )\n+\n+        return func in self._cpp_meta_supports_symint\n+    lift_fns = ordered_set(aten.lift_fresh.default, aten.lift_fresh_copy.default)\n+\n+\n+def _device_handler(args):\n+    # NB: Don't use is_our_fake, just serve the fake information\n+    # as is.  Notice we don't use 'self'; we use args[0].fake_mode\n+    # because they may not be the same.  It would also be possible\n+    # to return NotImplemented here, in which case the FakeTensor\n+    # handler on args[0] would handle it, but we're being nice and\n+    # short-circuiting quickly.\n+    assert len(args) == 1 and isinstance(args[0], FakeTensor)\n+    if args[0].fake_mode.in_kernel_invocation:\n+        return torch.device(\"meta\")\n+    else:\n+        return args[0].fake_device\n+\n+\n+_DISPATCH_META_HANDLERS = {\n+    torch.ops.prim.device.default: _device_handler,\n+    torch.ops.aten.size.default: lambda args: tuple(int(s) for s in args[0].size()),\n+    torch.ops.aten.stride.default: lambda args: tuple(int(s) for s in args[0].stride()),\n+    torch.ops.aten.storage_offset.default: lambda args: int(args[0].storage_offset()),\n+}\n+\n+_DISPATCH_HANDLE_DIRECLTY = ordered_set(\n+    torch.ops.aten.is_coalesced.default,\n+    torch.ops.aten.dense_dim.default,\n+    torch.ops.aten.sparse_dim.default,\n+)\n",
            "whole_hunk": "@@ -88,7 +88,11 @@ class UnsupportedOperatorException(RuntimeError):\n     func: OpOverload\n \n \n-_device_not_kwarg_ops = (\n+def ordered_set(*items):\n+    return {k: True for k in items}\n+\n+\n+_device_not_kwarg_ops = ordered_set(\n     aten._resize_output_.default,\n     aten._nested_tensor_from_tensor_list.default,\n     aten._nested_tensor_from_tensor_list.out,\n@@ -121,7 +125,7 @@ def contains_tensor_types(type):\n     )\n \n \n-_like_tensor_constructors = (\n+_like_tensor_constructors = ordered_set(\n     aten.empty_like.default,\n     aten.empty_like.out,\n     aten.full_like.default,\n@@ -627,9 +631,9 @@ def masked_select(fake_mode, func, self, mask):\n         has_free_symbols,\n     )\n \n-    if not has_free_symbols(arg.numel()):\n-        if arg.numel() >= 2:\n-            maxval = int(arg.numel())\n+    if not has_free_symbols(self.numel()):\n+        if self.numel() >= 2:\n+            maxval = int(self.numel())\n \n     _constrain_range_for_size(nnz, max=maxval)\n \n@@ -666,8 +670,11 @@ def run_and_return_new_tensor_of_input_device(fake_mode, func, args, kwargs):\n     return FakeTensor(fake_mode, out, out_device)\n \n \n+_is_builtin_namespaces = ordered_set(\"aten\", \"prims\", \"prim\")\n+\n+\n def is_builtin(op):\n-    return op.namespace in (\"aten\", \"prims\", \"prim\")\n+    return op.namespace in _is_builtin_namespaces\n \n \n def has_meta(func):\n@@ -1465,41 +1472,22 @@ class FakeTensorMode(TorchDispatchMode):\n                 torch._C._set_dispatch_mode(maybe_prev_fake_mode)\n \n     def dispatch(self, func, types, args=(), kwargs=None):\n-        kwargs = kwargs if kwargs else {}\n+        kwargs = kwargs or {}\n         log.debug(\"%s %s %s\", func, args, kwargs)\n \n-        if func == torch.ops.prim.device.default:\n-            # NB: Don't use is_our_fake, just serve the fake information\n-            # as is.  Notice we don't use 'self'; we use args[0].fake_mode\n-            # because they may not be the same.  It would also be possible\n-            # to return NotImplemented here, in which case the FakeTensor\n-            # handler on args[0] would handle it, but we're being nice and\n-            # short-circuiting quickly.\n-            assert len(args) == 1 and isinstance(args[0], FakeTensor)\n-            if args[0].fake_mode.in_kernel_invocation:\n-                return torch.device(\"meta\")\n-            else:\n-                return args[0].fake_device\n-        elif func is torch.ops.aten.size.default:\n-            return tuple(int(s) for s in args[0].size())\n-        elif func is torch.ops.aten.stride.default:\n-            return tuple(int(s) for s in args[0].stride())\n-        elif func is torch.ops.aten.storage_offset.default:\n-            return int(args[0].storage_offset())\n+        if func in _DISPATCH_META_HANDLERS:\n+            return _DISPATCH_META_HANDLERS[func](args)\n \n         if log.getEffectiveLevel() <= logging.DEBUG:\n             log.debug(\n                 \"%sFakeTensorMode.__torch_dispatch__: %s\", \" \" * RECURSION_COUNT, func\n             )\n+            # NOTE: incr is intentionally unused for a RAII pattern\n             incr = IncrementRecursionCount()\n \n         # Some attribute queries that can be serviced directly\n         # See Note [is_coalesced is dispatched]\n-        if func in {\n-            torch.ops.aten.is_coalesced.default,\n-            torch.ops.aten.dense_dim.default,\n-            torch.ops.aten.sparse_dim.default,\n-        }:\n+        if func in _DISPATCH_HANDLE_DIRECLTY:\n             # NB: no_dispatch is ok here too, this func is very simple\n             with in_kernel_invocation_manager(self):\n                 return func(*args, **kwargs)\n@@ -1700,36 +1688,6 @@ class FakeTensorMode(TorchDispatchMode):\n                 if op_impl_out != NotImplemented:\n                     return op_impl_out\n \n-        def can_run_unsafe_fallback(func: OpOverload):\n-            if not self.allow_fallback_kernels:\n-                return False\n-            # It's OK to try the fallback for built-in ops (e.g. aten, prims)\n-            # because we control and test these but the fallback leads to unexpected behavior\n-            # in user-defined custom ops\n-            #\n-            # WARNING: DO NOT add any additional namespaces/operators here if they refer to operators\n-            # outside of the pytorch/pytorch library! Any pre-existing things here\n-            # are either in the pytorch/pytorch library or have been grandfathered in.\n-            # The fallback does not always work and MAY CRASH and emit unreadable error messages\n-            # so it should not be allowed by default.\n-            allowed_namespaces = {\n-                \"debugprims\",\n-                \"prims\",\n-                \"aten\",\n-                \"xla\",\n-                \"vision\",\n-                \"torchtext\",\n-                \"torchaudio\",\n-                \"quantized\",\n-            }\n-            grandfathered_ops_FIXME = {\n-                \"fbgemm::gmm\",\n-            }\n-            return (\n-                func.namespace in allowed_namespaces\n-                or func.name() in grandfathered_ops_FIXME\n-            )\n-\n         def maybe_run_unsafe_fallback(error=None):\n             # We infer the meta of a custom ops that return None to just\n             # return None. custom ops are not allowed to mutate metadata\n@@ -1737,7 +1695,7 @@ class FakeTensorMode(TorchDispatchMode):\n             if can_generate_trivial_abstract_impl(func):\n                 return None\n             # no meta kernel registered, fallback to kernel for the device\n-            if has_symbolic_sizes or not can_run_unsafe_fallback(func):\n+            if has_symbolic_sizes or not self.can_run_unsafe_fallback(func):\n                 raise UnsupportedOperatorException(func)\n             if error is None:\n                 error = UnsupportedOperatorException(func)\n@@ -1761,6 +1719,33 @@ class FakeTensorMode(TorchDispatchMode):\n             r, func, flat_args, device=kwargs.get(\"device\")\n         )\n \n+    # WARNING: DO NOT add any additional namespaces/operators here if they refer to operators\n+    # outside of the pytorch/pytorch library! Any pre-existing things here\n+    # are either in the pytorch/pytorch library or have been grandfathered in.\n+    # The fallback does not always work and MAY CRASH and emit unreadable error messages\n+    # so it should not be allowed by default.\n+    _can_run_unsafe_fallback_allowed_namespaces = ordered_set(\n+        \"debugprims\",\n+        \"prims\",\n+        \"aten\",\n+        \"xla\",\n+        \"vision\",\n+        \"torchtext\",\n+        \"torchaudio\",\n+        \"quantized\",\n+    )\n+\n+    def can_run_unsafe_fallback(self, func: OpOverload):\n+        if not self.allow_fallback_kernels:\n+            return False\n+        # It's OK to try the fallback for built-in ops (e.g. aten, prims)\n+        # because we control and test these but the fallback leads to unexpected behavior\n+        # in user-defined custom ops\n+        return (\n+            func.namespace in self._can_run_unsafe_fallback_allowed_namespaces\n+            or func.name() == \"fbgemm::gmm\"\n+        )\n+\n     # [subclass inputs]\n     # Suppose we enable fake tensor mode.  This means that fake tensor\n     # mode will run first.  But what if we do an operation that\n@@ -1862,26 +1847,26 @@ class FakeTensorMode(TorchDispatchMode):\n \n         return tree_map(wrap, r)\n \n+    _cpp_meta_supports_symint = ordered_set(\n+        aten.empty.memory_format,\n+        aten.empty_strided.default,\n+        aten.as_strided_scatter.default,\n+        aten.as_strided.default,\n+        aten.as_strided_.default,\n+        aten.zeros.default,\n+        aten.detach.default,\n+        aten.view_as_real.default,\n+        aten.view_as_complex.default,\n+        aten.set_.source_Storage_storage_offset,\n+        aten._sparse_coo_tensor_with_dims_and_tensors.default,\n+    )\n+\n     def cpp_meta_supports_symint(self, func):\n         if torch.Tag.view_copy in func.tags:\n             return True\n-        return func in [\n-            aten.empty.memory_format,\n-            aten.empty_strided.default,\n-            aten.as_strided_scatter.default,\n-            aten.as_strided.default,\n-            aten.as_strided_.default,\n-            aten.zeros.default,\n-            aten.detach.default,\n-            aten.view_as_real.default,\n-            aten.view_as_complex.default,\n-            aten.set_.source_Storage_storage_offset,\n-            aten._sparse_coo_tensor_with_dims_and_tensors.default,\n-        ]\n+        return func in self._cpp_meta_supports_symint\n \n-    @property\n-    def lift_fns(self):\n-        return (aten.lift_fresh.default, aten.lift_fresh_copy.default)\n+    lift_fns = ordered_set(aten.lift_fresh.default, aten.lift_fresh_copy.default)\n \n     def may_turn_const(self, t):\n         return (\n@@ -2049,3 +2034,31 @@ class FakeCopyMode(TorchFunctionMode):\n         else:\n             with torch._C.DisableTorchFunctionSubclass():\n                 return func(*args, **kwargs)\n+\n+\n+def _device_handler(args):\n+    # NB: Don't use is_our_fake, just serve the fake information\n+    # as is.  Notice we don't use 'self'; we use args[0].fake_mode\n+    # because they may not be the same.  It would also be possible\n+    # to return NotImplemented here, in which case the FakeTensor\n+    # handler on args[0] would handle it, but we're being nice and\n+    # short-circuiting quickly.\n+    assert len(args) == 1 and isinstance(args[0], FakeTensor)\n+    if args[0].fake_mode.in_kernel_invocation:\n+        return torch.device(\"meta\")\n+    else:\n+        return args[0].fake_device\n+\n+\n+_DISPATCH_META_HANDLERS = {\n+    torch.ops.prim.device.default: _device_handler,\n+    torch.ops.aten.size.default: lambda args: tuple(int(s) for s in args[0].size()),\n+    torch.ops.aten.stride.default: lambda args: tuple(int(s) for s in args[0].stride()),\n+    torch.ops.aten.storage_offset.default: lambda args: int(args[0].storage_offset()),\n+}\n+\n+_DISPATCH_HANDLE_DIRECLTY = ordered_set(\n+    torch.ops.aten.is_coalesced.default,\n+    torch.ops.aten.dense_dim.default,\n+    torch.ops.aten.sparse_dim.default,\n+)"
        }
    ]
},
{
    "Id": 321,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/993e4f3911856be3a93746f6ed6a13f25de6ff65",
    "date": "2024-01-27T03:49:00+00:00",
    "message": "[c10d] relax the nccl error check for nonblocking mode (#118254)\n\nresolve https://github.com/pytorch/pytorch/issues/117749\n\nSummary:\nThis is the first step to enable NCCL nonblocking mode.\n\nIn NCCL nonblocking mode,  ncclInProgress is an expected return value\nwhen checking communicators. Without this relaxation, watchdog thread\nwould throw NCCL errors during work checking while it is expected.\n\nTest Plan:\nSet nonblocking mode in unit tests, and make sure all existing NCCL\ntests pass\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/118254\nApproved by: https://github.com/kwen2501",
    "label": "NO",
    "changes": [
        {
            "name": "test_c10d_nccl.py",
            "path": "test/distributed/test_c10d_nccl.py",
            "patches": [
                {
                    "old_start": 231,
                    "old_length": 6,
                    "new_start": 231,
                    "new_length": 9,
                    "hunk": "@@ -231,6 +231,9 @@ class ProcessGroupNCCLTest(MultiProcessTestCase):\n         # that use TORCH_NCCL_BLOCKING_WAIT will test it as expected.\n         os.environ[\"TORCH_NCCL_ASYNC_ERROR_HANDLING\"] = \"1\"\n         # self.num_gpus = torch.cuda.device_count()\n+        # To test NONBLOCKING NCCL calls\n+        # os.environ[\"TORCH_NCCL_USE_COMM_NONBLOCKING\"] = \"1\"\n+        # os.environ[\"TORCH_NCCL_NONBLOCKING_TIMEOUT\"] = \"10\"\n         self._spawn_processes()\n \n     def tearDown(self):\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        # To test NONBLOCKING NCCL calls\n+        # os.environ[\"TORCH_NCCL_USE_COMM_NONBLOCKING\"] = \"1\"\n+        # os.environ[\"TORCH_NCCL_NONBLOCKING_TIMEOUT\"] = \"10\"\n",
            "whole_hunk": "@@ -231,6 +231,9 @@ class ProcessGroupNCCLTest(MultiProcessTestCase):\n         # that use TORCH_NCCL_BLOCKING_WAIT will test it as expected.\n         os.environ[\"TORCH_NCCL_ASYNC_ERROR_HANDLING\"] = \"1\"\n         # self.num_gpus = torch.cuda.device_count()\n+        # To test NONBLOCKING NCCL calls\n+        # os.environ[\"TORCH_NCCL_USE_COMM_NONBLOCKING\"] = \"1\"\n+        # os.environ[\"TORCH_NCCL_NONBLOCKING_TIMEOUT\"] = \"10\"\n         self._spawn_processes()\n \n     def tearDown(self):\n"
        },
        {
            "name": "ProcessGroupNCCL.cpp",
            "path": "torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp",
            "patches": [
                {
                    "old_start": 1702,
                    "old_length": 7,
                    "new_start": 1702,
                    "new_length": 10,
                    "hunk": "@@ -1702,7 +1702,10 @@ std::exception_ptr ProcessGroupNCCL::checkForNCCLErrorsInternal(\n               *commFailureReason)));\n     }\n     ncclResult_t ncclAsyncErr = ncclComm->checkForNcclError();\n-    if (ncclAsyncErr != ncclSuccess) {\n+    // When nonblocking mode is enabled by TORCH_NCCL_USE_COMM_NONBLOCKING,\n+    // ncclInProgress could be returned when there are pending NCCL calls.\n+    // In this case, no exception should be thrown\n+    if (ncclAsyncErr != ncclSuccess && ncclAsyncErr != ncclInProgress) {\n       return std::make_exception_ptr(C10_BUILD_ERROR(\n           DistBackendError,\n           \"NCCL error: \" + ncclGetErrorWithVersion(ncclAsyncErr) + \"\\n\" +"
                }
            ],
            "whole_deleted": "-    if (ncclAsyncErr != ncclSuccess) {\n",
            "whole_added": "+    // When nonblocking mode is enabled by TORCH_NCCL_USE_COMM_NONBLOCKING,\n+    // ncclInProgress could be returned when there are pending NCCL calls.\n+    // In this case, no exception should be thrown\n+    if (ncclAsyncErr != ncclSuccess && ncclAsyncErr != ncclInProgress) {\n",
            "whole_hunk": "@@ -1702,7 +1702,10 @@ std::exception_ptr ProcessGroupNCCL::checkForNCCLErrorsInternal(\n               *commFailureReason)));\n     }\n     ncclResult_t ncclAsyncErr = ncclComm->checkForNcclError();\n-    if (ncclAsyncErr != ncclSuccess) {\n+    // When nonblocking mode is enabled by TORCH_NCCL_USE_COMM_NONBLOCKING,\n+    // ncclInProgress could be returned when there are pending NCCL calls.\n+    // In this case, no exception should be thrown\n+    if (ncclAsyncErr != ncclSuccess && ncclAsyncErr != ncclInProgress) {\n       return std::make_exception_ptr(C10_BUILD_ERROR(\n           DistBackendError,\n           \"NCCL error: \" + ncclGetErrorWithVersion(ncclAsyncErr) + \"\\n\" +"
        }
    ]
},
{
    "Id": 19,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/741c1710e81bcc4034440d87ebbd23ec4c89cd81",
    "date": "2024-07-12T18:02:09+00:00",
    "message": "[cond] inlining into one of the branches when pred is a python constant (#130493)\n\nReland https://github.com/pytorch/pytorch/pull/128709.\n\nWhen the input predicate is a python constant, we specialize into one of the branches and warn users that torch.cond is not preserving the dynamism. The previous behavior is that we baked in True/False in the cond operator. This can be confusing. In this PR, we change it to be specializing into one of the branches when the inputs are constants.\n\nWe additionally change the naming of cond operator to default one without overriding its name. This allows better testing on de-serialized graph.\n\nTest Plan:\nThe predicate in some existing tests is the result of a shape comparison. When no dynamic shape is involved, the predicate is a python bool. To fix them, we either change the predicate to be some data-dependent tensor or change the test to check cond is specialized as one of the branches,\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130493\nApproved by: https://github.com/BoyuanFeng",
    "label": "NO",
    "changes": [
        {
            "name": "test_export.py",
            "path": "test/dynamo/test_export.py",
            "patches": [
                {
                    "old_start": 1912,
                    "old_length": 13,
                    "new_start": 1912,
                    "new_length": 10,
                    "hunk": "@@ -1912,13 +1912,10 @@ def forward(self, l_x_):\n             ):\n                 # True branch and false branch return tensors of different shape\n                 torch._dynamo.export(mod)(torch.randn(3, 2))\n-            with self.assertRaisesRegex(\n-                torch._dynamo.exc.UncapturedHigherOrderOpError,\n-                \"Cond doesn't work unless it is captured completely with torch.compile\",\n-            ):\n-                # True branch and false branch return tensors of different shape\n-                test_x = torch.randn(3, 2)\n-                mod(test_x)\n+\n+            # We specialize into one of the branches since predicate is a python boolean.\n+            test_x = torch.randn(3, 2)\n+            mod(test_x)\n \n     def test_export_with_map_cond(self):\n         from functorch.experimental.control_flow import cond, map\n"
                }
            ],
            "whole_deleted": "-            with self.assertRaisesRegex(\n-                torch._dynamo.exc.UncapturedHigherOrderOpError,\n-                \"Cond doesn't work unless it is captured completely with torch.compile\",\n-            ):\n-                # True branch and false branch return tensors of different shape\n-                test_x = torch.randn(3, 2)\n-                mod(test_x)\n",
            "whole_added": "+\n+            # We specialize into one of the branches since predicate is a python boolean.\n+            test_x = torch.randn(3, 2)\n+            mod(test_x)\n",
            "whole_hunk": "@@ -1912,13 +1912,10 @@ def forward(self, l_x_):\n             ):\n                 # True branch and false branch return tensors of different shape\n                 torch._dynamo.export(mod)(torch.randn(3, 2))\n-            with self.assertRaisesRegex(\n-                torch._dynamo.exc.UncapturedHigherOrderOpError,\n-                \"Cond doesn't work unless it is captured completely with torch.compile\",\n-            ):\n-                # True branch and false branch return tensors of different shape\n-                test_x = torch.randn(3, 2)\n-                mod(test_x)\n+\n+            # We specialize into one of the branches since predicate is a python boolean.\n+            test_x = torch.randn(3, 2)\n+            mod(test_x)\n \n     def test_export_with_map_cond(self):\n         from functorch.experimental.control_flow import cond, map\n"
        },
        {
            "name": "test_higher_order_ops.py",
            "path": "test/dynamo/test_higher_order_ops.py",
            "patches": [
                {
                    "old_start": 1406,
                    "old_length": 7,
                    "new_start": 1406,
                    "new_length": 7,
                    "hunk": "@@ -1406,7 +1406,7 @@ def forward(self, child, const_unused):\n                 def false_fn(x):\n                     return (x - 1).sum()\n \n-                return control_flow.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n+                return control_flow.cond(x.sum() > 4, true_fn, false_fn, [x])\n \n         mod_for_compile = torch.compile(Foo(), backend=cnt, dynamic=True)\n         mod_for_eager = Foo()\n"
                },
                {
                    "old_start": 6145,
                    "old_length": 12,
                    "new_start": 6145,
                    "new_length": 16,
                    "hunk": "@@ -6145,12 +6145,16 @@ class ActivationCheckpointingTests(torch._dynamo.test_case.TestCase):\n             return cond_op(pred=pred, true_fn=true_fn, false_fn=false_fn, operands=[x])\n \n         cnt = CompileCounter()\n-        opt_test = torch.compile(test, backend=cnt)\n+        opt_test = torch.compile(test, backend=cnt, fullgraph=True)\n         inp = torch.ones(3, 3)\n-        self.assertTrue(torch.allclose(test(True, inp), opt_test(True, inp)))\n+        true_pred = torch.Tensor([True])\n+        false_pred = torch.Tensor([False])\n+        self.assertTrue(torch.allclose(test(true_pred, inp), opt_test(true_pred, inp)))\n+        self.assertEqual(cnt.frame_count, 1)\n+        self.assertTrue(\n+            torch.allclose(test(false_pred, inp), opt_test(false_pred, inp))\n+        )\n         self.assertEqual(cnt.frame_count, 1)\n-        self.assertTrue(torch.allclose(test(False, inp), opt_test(False, inp)))\n-        self.assertEqual(cnt.frame_count, 2)\n \n     def test_cond_with_invalid_kwargs(self):\n         from torch._higher_order_ops.cond import cond_op\n"
                }
            ],
            "whole_deleted": "-                return control_flow.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n-        opt_test = torch.compile(test, backend=cnt)\n-        self.assertTrue(torch.allclose(test(True, inp), opt_test(True, inp)))\n-        self.assertTrue(torch.allclose(test(False, inp), opt_test(False, inp)))\n-        self.assertEqual(cnt.frame_count, 2)\n",
            "whole_added": "+                return control_flow.cond(x.sum() > 4, true_fn, false_fn, [x])\n+        opt_test = torch.compile(test, backend=cnt, fullgraph=True)\n+        true_pred = torch.Tensor([True])\n+        false_pred = torch.Tensor([False])\n+        self.assertTrue(torch.allclose(test(true_pred, inp), opt_test(true_pred, inp)))\n+        self.assertEqual(cnt.frame_count, 1)\n+        self.assertTrue(\n+            torch.allclose(test(false_pred, inp), opt_test(false_pred, inp))\n+        )\n",
            "whole_hunk": "@@ -1406,7 +1406,7 @@ def forward(self, child, const_unused):\n                 def false_fn(x):\n                     return (x - 1).sum()\n \n-                return control_flow.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n+                return control_flow.cond(x.sum() > 4, true_fn, false_fn, [x])\n \n         mod_for_compile = torch.compile(Foo(), backend=cnt, dynamic=True)\n         mod_for_eager = Foo()\n@@ -6145,12 +6145,16 @@ class ActivationCheckpointingTests(torch._dynamo.test_case.TestCase):\n             return cond_op(pred=pred, true_fn=true_fn, false_fn=false_fn, operands=[x])\n \n         cnt = CompileCounter()\n-        opt_test = torch.compile(test, backend=cnt)\n+        opt_test = torch.compile(test, backend=cnt, fullgraph=True)\n         inp = torch.ones(3, 3)\n-        self.assertTrue(torch.allclose(test(True, inp), opt_test(True, inp)))\n+        true_pred = torch.Tensor([True])\n+        false_pred = torch.Tensor([False])\n+        self.assertTrue(torch.allclose(test(true_pred, inp), opt_test(true_pred, inp)))\n+        self.assertEqual(cnt.frame_count, 1)\n+        self.assertTrue(\n+            torch.allclose(test(false_pred, inp), opt_test(false_pred, inp))\n+        )\n         self.assertEqual(cnt.frame_count, 1)\n-        self.assertTrue(torch.allclose(test(False, inp), opt_test(False, inp)))\n-        self.assertEqual(cnt.frame_count, 2)\n \n     def test_cond_with_invalid_kwargs(self):\n         from torch._higher_order_ops.cond import cond_op\n"
        },
        {
            "name": "test_export.py",
            "path": "test/export/test_export.py",
            "patches": [
                {
                    "old_start": 804,
                    "old_length": 7,
                    "new_start": 804,
                    "new_length": 7,
                    "hunk": "@@ -804,7 +804,7 @@ def forward(self, p_linear_weight, p_linear_bias, x):\n                 return x.sin()\n \n             def forward(self, x):\n-                return cond(x.shape[0] <= 2, self.subm.forward, self.bar, [x])\n+                return cond(x.sum() <= 2, self.subm.forward, self.bar, [x])\n \n         example_inputs = (torch.randn(1, 3, 3, 3),)\n         m = CondBranchClassMethod()\n"
                },
                {
                    "old_start": 3616,
                    "old_length": 7,
                    "new_start": 3616,
                    "new_length": 7,
                    "hunk": "@@ -3616,7 +3616,7 @@ def forward(self, x):\n         ):\n             torch.export.export(exported_v2.module(), (torch.randn(2, 2),))\n \n-    def test_export_cond(self):\n+    def test_export_cond_symbool_pred(self):\n         class A(torch.nn.Module):\n             def __init__(self):\n                 super().__init__()\n"
                },
                {
                    "old_start": 3639,
                    "old_length": 10,
                    "new_start": 3639,
                    "new_length": 20,
                    "hunk": "@@ -3639,10 +3639,20 @@ def forward(self, x):\n \n                 return cond(x.shape[0] > 4, true_fn, false_fn, [x])\n \n+        dim0 = torch.export.Dim(\"dim0\", min=3)\n         inp = torch.ones(6, 4)\n-        ep = export(\n-            Foo(),\n-            (inp,),\n+        ep = export(Foo(), (inp,), dynamic_shapes={\"x\": {0: dim0}})\n+        self.assertExpectedInline(\n+            ep.graph_module.code.strip(),\n+            \"\"\"\\\n+def forward(self, b_a_buffer, x):\n+    sym_size_int_1 = torch.ops.aten.sym_size.int(x, 0)\n+    gt = sym_size_int_1 > 4;  sym_size_int_1 = None\n+    true_graph_0 = self.true_graph_0\n+    false_graph_0 = self.false_graph_0\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [x, b_a_buffer]);  gt = true_graph_0 = false_graph_0 = x = b_a_buffer = None\n+    getitem = cond[0];  cond = None\n+    return (getitem,)\"\"\",\n         )\n         self.assertTrue(\n             torch.allclose(ep.module()(torch.ones(6, 4)), Foo()(torch.ones(6, 4)))\n"
                },
                {
                    "old_start": 4988,
                    "old_length": 7,
                    "new_start": 4998,
                    "new_length": 7,
                    "hunk": "@@ -4988,7 +4998,7 @@ graph():\n                 def false_fn(x):\n                     return self.linear(x).sin()\n \n-                return torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n+                return torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n \n         class CondExport(torch.nn.Module):\n             def __init__(self):\n"
                },
                {
                    "old_start": 5005,
                    "old_length": 10,
                    "new_start": 5015,
                    "new_length": 12,
                    "hunk": "@@ -5005,10 +5015,12 @@ graph():\n             \"\"\"\\\n def forward(self, p_bar_linear_weight, p_bar_linear_bias, x):\n     cos = torch.ops.aten.cos.default(x)\n+    sum_1 = torch.ops.aten.sum.default(x)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [p_bar_linear_bias, p_bar_linear_weight, x]);  true_graph_0 = false_graph_0 = p_bar_linear_bias = p_bar_linear_weight = x = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [p_bar_linear_bias, p_bar_linear_weight, x]);  gt = true_graph_0 = false_graph_0 = p_bar_linear_bias = p_bar_linear_weight = x = None\n+    getitem = cond[0];  cond = None\n     add = torch.ops.aten.add.Tensor(cos, getitem);  cos = getitem = None\n     return (add,)\"\"\",\n         )\n"
                },
                {
                    "old_start": 5103,
                    "old_length": 8,
                    "new_start": 5115,
                    "new_length": 8,
                    "hunk": "@@ -5103,8 +5115,8 @@ def forward(self, p_bar_linear_weight, p_bar_linear_bias, x):\n def forward(self, b_pred, b_t, x, y):\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(b_pred, true_graph_0, false_graph_0, [b_t, x, y]);  b_pred = true_graph_0 = false_graph_0 = b_t = x = y = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(b_pred, true_graph_0, false_graph_0, [b_t, x, y]);  b_pred = true_graph_0 = false_graph_0 = b_t = x = y = None\n+    getitem = cond[0];  cond = None\n     return (getitem,)\"\"\",\n         )  # noqa: B950\n \n"
                }
            ],
            "whole_deleted": "-                return cond(x.shape[0] <= 2, self.subm.forward, self.bar, [x])\n-    def test_export_cond(self):\n-        ep = export(\n-            Foo(),\n-            (inp,),\n-                return torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [p_bar_linear_bias, p_bar_linear_weight, x]);  true_graph_0 = false_graph_0 = p_bar_linear_bias = p_bar_linear_weight = x = None\n-    getitem = conditional[0];  conditional = None\n-    conditional = torch.ops.higher_order.cond(b_pred, true_graph_0, false_graph_0, [b_t, x, y]);  b_pred = true_graph_0 = false_graph_0 = b_t = x = y = None\n-    getitem = conditional[0];  conditional = None\n",
            "whole_added": "+                return cond(x.sum() <= 2, self.subm.forward, self.bar, [x])\n+    def test_export_cond_symbool_pred(self):\n+        dim0 = torch.export.Dim(\"dim0\", min=3)\n+        ep = export(Foo(), (inp,), dynamic_shapes={\"x\": {0: dim0}})\n+        self.assertExpectedInline(\n+            ep.graph_module.code.strip(),\n+            \"\"\"\\\n+def forward(self, b_a_buffer, x):\n+    sym_size_int_1 = torch.ops.aten.sym_size.int(x, 0)\n+    gt = sym_size_int_1 > 4;  sym_size_int_1 = None\n+    true_graph_0 = self.true_graph_0\n+    false_graph_0 = self.false_graph_0\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [x, b_a_buffer]);  gt = true_graph_0 = false_graph_0 = x = b_a_buffer = None\n+    getitem = cond[0];  cond = None\n+    return (getitem,)\"\"\",\n+                return torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n+    sum_1 = torch.ops.aten.sum.default(x)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [p_bar_linear_bias, p_bar_linear_weight, x]);  gt = true_graph_0 = false_graph_0 = p_bar_linear_bias = p_bar_linear_weight = x = None\n+    getitem = cond[0];  cond = None\n+    cond = torch.ops.higher_order.cond(b_pred, true_graph_0, false_graph_0, [b_t, x, y]);  b_pred = true_graph_0 = false_graph_0 = b_t = x = y = None\n+    getitem = cond[0];  cond = None\n",
            "whole_hunk": "@@ -804,7 +804,7 @@ def forward(self, p_linear_weight, p_linear_bias, x):\n                 return x.sin()\n \n             def forward(self, x):\n-                return cond(x.shape[0] <= 2, self.subm.forward, self.bar, [x])\n+                return cond(x.sum() <= 2, self.subm.forward, self.bar, [x])\n \n         example_inputs = (torch.randn(1, 3, 3, 3),)\n         m = CondBranchClassMethod()\n@@ -3616,7 +3616,7 @@ def forward(self, x):\n         ):\n             torch.export.export(exported_v2.module(), (torch.randn(2, 2),))\n \n-    def test_export_cond(self):\n+    def test_export_cond_symbool_pred(self):\n         class A(torch.nn.Module):\n             def __init__(self):\n                 super().__init__()\n@@ -3639,10 +3639,20 @@ def forward(self, x):\n \n                 return cond(x.shape[0] > 4, true_fn, false_fn, [x])\n \n+        dim0 = torch.export.Dim(\"dim0\", min=3)\n         inp = torch.ones(6, 4)\n-        ep = export(\n-            Foo(),\n-            (inp,),\n+        ep = export(Foo(), (inp,), dynamic_shapes={\"x\": {0: dim0}})\n+        self.assertExpectedInline(\n+            ep.graph_module.code.strip(),\n+            \"\"\"\\\n+def forward(self, b_a_buffer, x):\n+    sym_size_int_1 = torch.ops.aten.sym_size.int(x, 0)\n+    gt = sym_size_int_1 > 4;  sym_size_int_1 = None\n+    true_graph_0 = self.true_graph_0\n+    false_graph_0 = self.false_graph_0\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [x, b_a_buffer]);  gt = true_graph_0 = false_graph_0 = x = b_a_buffer = None\n+    getitem = cond[0];  cond = None\n+    return (getitem,)\"\"\",\n         )\n         self.assertTrue(\n             torch.allclose(ep.module()(torch.ones(6, 4)), Foo()(torch.ones(6, 4)))\n@@ -4988,7 +4998,7 @@ graph():\n                 def false_fn(x):\n                     return self.linear(x).sin()\n \n-                return torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n+                return torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n \n         class CondExport(torch.nn.Module):\n             def __init__(self):\n@@ -5005,10 +5015,12 @@ graph():\n             \"\"\"\\\n def forward(self, p_bar_linear_weight, p_bar_linear_bias, x):\n     cos = torch.ops.aten.cos.default(x)\n+    sum_1 = torch.ops.aten.sum.default(x)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [p_bar_linear_bias, p_bar_linear_weight, x]);  true_graph_0 = false_graph_0 = p_bar_linear_bias = p_bar_linear_weight = x = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [p_bar_linear_bias, p_bar_linear_weight, x]);  gt = true_graph_0 = false_graph_0 = p_bar_linear_bias = p_bar_linear_weight = x = None\n+    getitem = cond[0];  cond = None\n     add = torch.ops.aten.add.Tensor(cos, getitem);  cos = getitem = None\n     return (add,)\"\"\",\n         )\n@@ -5103,8 +5115,8 @@ def forward(self, p_bar_linear_weight, p_bar_linear_bias, x):\n def forward(self, b_pred, b_t, x, y):\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(b_pred, true_graph_0, false_graph_0, [b_t, x, y]);  b_pred = true_graph_0 = false_graph_0 = b_t = x = y = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(b_pred, true_graph_0, false_graph_0, [b_t, x, y]);  b_pred = true_graph_0 = false_graph_0 = b_t = x = y = None\n+    getitem = cond[0];  cond = None\n     return (getitem,)\"\"\",\n         )  # noqa: B950\n \n"
        },
        {
            "name": "test_verifier.py",
            "path": "test/export/test_verifier.py",
            "patches": [
                {
                    "old_start": 68,
                    "old_length": 7,
                    "new_start": 68,
                    "new_length": 7,
                    "hunk": "@@ -68,7 +68,7 @@ class TestVerifier(TestCase):\n                 def false_fn(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n                     return x - y\n \n-                return control_flow.cond(x.shape[0] > 2, true_fn, false_fn, [x, y])\n+                return control_flow.cond(x.sum() > 2, true_fn, false_fn, [x, y])\n \n         f = Foo()\n \n"
                },
                {
                    "old_start": 87,
                    "old_length": 7,
                    "new_start": 87,
                    "new_length": 7,
                    "hunk": "@@ -87,7 +87,7 @@ class TestVerifier(TestCase):\n                 def false_fn(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n                     return x - y\n \n-                return control_flow.cond(x.shape[0] > 2, true_fn, false_fn, [x, y])\n+                return control_flow.cond(x.sum() > 2, true_fn, false_fn, [x, y])\n \n         f = Foo()\n \n"
                }
            ],
            "whole_deleted": "-                return control_flow.cond(x.shape[0] > 2, true_fn, false_fn, [x, y])\n-                return control_flow.cond(x.shape[0] > 2, true_fn, false_fn, [x, y])\n",
            "whole_added": "+                return control_flow.cond(x.sum() > 2, true_fn, false_fn, [x, y])\n+                return control_flow.cond(x.sum() > 2, true_fn, false_fn, [x, y])\n",
            "whole_hunk": "@@ -68,7 +68,7 @@ class TestVerifier(TestCase):\n                 def false_fn(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n                     return x - y\n \n-                return control_flow.cond(x.shape[0] > 2, true_fn, false_fn, [x, y])\n+                return control_flow.cond(x.sum() > 2, true_fn, false_fn, [x, y])\n \n         f = Foo()\n \n@@ -87,7 +87,7 @@ class TestVerifier(TestCase):\n                 def false_fn(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n                     return x - y\n \n-                return control_flow.cond(x.shape[0] > 2, true_fn, false_fn, [x, y])\n+                return control_flow.cond(x.sum() > 2, true_fn, false_fn, [x, y])\n \n         f = Foo()\n \n"
        },
        {
            "name": "test_aotdispatch.py",
            "path": "test/functorch/test_aotdispatch.py",
            "patches": [
                {
                    "old_start": 4237,
                    "old_length": 7,
                    "new_start": 4237,
                    "new_length": 7,
                    "hunk": "@@ -4237,7 +4237,7 @@ def forward(self, arg0_1, arg1_1):\n                         return x.cos()\n \n                     return torch.cond(\n-                        y.cos().shape[0] > 5, true_true_fn, true_false_fn, [y.cos()]\n+                        y.cos().sum() > 5, true_true_fn, true_false_fn, [y.cos()]\n                     )\n \n                 def false_fn(x):\n"
                },
                {
                    "old_start": 4245,
                    "old_length": 7,
                    "new_start": 4245,
                    "new_length": 7,
                    "hunk": "@@ -4245,7 +4245,7 @@ def forward(self, arg0_1, arg1_1):\n                     z.add_(6)\n                     return z.sin()\n \n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n                 return (a + 3, a + 4)\n \n         inp = torch.randn(2, 2)\n"
                },
                {
                    "old_start": 4254,
                    "old_length": 10,
                    "new_start": 4254,
                    "new_length": 12,
                    "hunk": "@@ -4254,10 +4254,12 @@ def forward(self, arg0_1, arg1_1):\n             str(gm.code).strip(),\n             \"\"\"\\\n def forward(self, arg0_1):\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1]);  true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n     add = torch.ops.aten.add.Tensor(getitem, 3)\n     add_1 = torch.ops.aten.add.Tensor(getitem, 4);  getitem = None\n     return (add, add_1)\"\"\",  # noqa: B950\n"
                },
                {
                    "old_start": 4270,
                    "old_length": 11,
                    "new_start": 4272,
                    "new_length": 13,
                    "hunk": "@@ -4270,11 +4272,13 @@ def forward(self, arg0_1):\n     sin = torch.ops.aten.sin.default(arg0_1);  arg0_1 = None\n     add = torch.ops.aten.add.Tensor(sin, 5);  sin = None\n     cos = torch.ops.aten.cos.default(add)\n+    sum_1 = torch.ops.aten.sum.default(cos);  cos = None\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 5);  sum_1 = None\n     cos_1 = torch.ops.aten.cos.default(add);  add = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [cos_1]);  true_graph_0 = false_graph_0 = cos_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [cos_1]);  gt = true_graph_0 = false_graph_0 = cos_1 = None\n+    getitem = cond[0];  cond = None\n     return (getitem,)\"\"\",  # noqa: B950\n         )\n \n"
                },
                {
                    "old_start": 4317,
                    "old_length": 7,
                    "new_start": 4321,
                    "new_length": 7,
                    "hunk": "@@ -4317,7 +4321,7 @@ def forward(self, arg0_1):\n                         + control_flow.map(f, z, r).sum()\n                     )\n \n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x, y])\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x, y])\n                 return (a + 3, a + 4)\n \n         inps = [torch.randn(2, 2), torch.ones(2)]\n"
                },
                {
                    "old_start": 4326,
                    "old_length": 10,
                    "new_start": 4330,
                    "new_length": 12,
                    "hunk": "@@ -4326,10 +4330,12 @@ def forward(self, arg0_1):\n             str(gm.code).strip(),\n             \"\"\"\\\n def forward(self, arg0_1, arg1_1):\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1, arg1_1]);  true_graph_0 = false_graph_0 = arg0_1 = arg1_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1, arg1_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = arg1_1 = None\n+    getitem = cond[0];  cond = None\n     add = torch.ops.aten.add.Tensor(getitem, 3)\n     add_1 = torch.ops.aten.add.Tensor(getitem, 4);  getitem = None\n     return (add, add_1)\"\"\",  # noqa: B950\n"
                },
                {
                    "old_start": 4434,
                    "old_length": 7,
                    "new_start": 4440,
                    "new_length": 7,
                    "hunk": "@@ -4434,7 +4440,7 @@ def forward(self, arg0_1, arg1_1):\n                     z.add_(6)\n                     return z.sin()\n \n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n                 return (a + 3, a + 4)\n \n         inp = torch.randn(2, 2)\n"
                },
                {
                    "old_start": 4443,
                    "old_length": 10,
                    "new_start": 4449,
                    "new_length": 12,
                    "hunk": "@@ -4443,10 +4449,12 @@ def forward(self, arg0_1, arg1_1):\n             str(gm.code).strip(),\n             \"\"\"\\\n def forward(self, arg0_1):\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1]);  true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n     add = torch.ops.aten.add.Tensor(getitem, 3)\n     add_1 = torch.ops.aten.add.Tensor(getitem, 4);  getitem = None\n     return (add, add_1)\"\"\",  # noqa: B950\n"
                },
                {
                    "old_start": 4867,
                    "old_length": 7,
                    "new_start": 4875,
                    "new_length": 7,
                    "hunk": "@@ -4867,7 +4875,7 @@ def forward(self, arg0_1, arg1_1, arg2_1):\n                     y.add_(6)\n                     return x.sin()\n \n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n                 return (a + 3, a + 4)\n \n         inp = torch.randn(3, 4)\n"
                },
                {
                    "old_start": 4876,
                    "old_length": 10,
                    "new_start": 4884,
                    "new_length": 12,
                    "hunk": "@@ -4876,10 +4884,12 @@ def forward(self, arg0_1, arg1_1, arg2_1):\n             gm.code.strip(),\n             \"\"\"\\\n def forward(self, arg0_1):\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1]);  true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n     add = torch.ops.aten.add.Tensor(getitem, 3)\n     add_1 = torch.ops.aten.add.Tensor(getitem, 4);  getitem = None\n     return (add, add_1)\"\"\",  # noqa: B950\n"
                }
            ],
            "whole_deleted": "-                        y.cos().shape[0] > 5, true_true_fn, true_false_fn, [y.cos()]\n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1]);  true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [cos_1]);  true_graph_0 = false_graph_0 = cos_1 = None\n-    getitem = conditional[0];  conditional = None\n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x, y])\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1, arg1_1]);  true_graph_0 = false_graph_0 = arg0_1 = arg1_1 = None\n-    getitem = conditional[0];  conditional = None\n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1]);  true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1]);  true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n",
            "whole_added": "+                        y.cos().sum() > 5, true_true_fn, true_false_fn, [y.cos()]\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n+    sum_1 = torch.ops.aten.sum.default(cos);  cos = None\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 5);  sum_1 = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [cos_1]);  gt = true_graph_0 = false_graph_0 = cos_1 = None\n+    getitem = cond[0];  cond = None\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x, y])\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1, arg1_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = arg1_1 = None\n+    getitem = cond[0];  cond = None\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n",
            "whole_hunk": "@@ -4237,7 +4237,7 @@ def forward(self, arg0_1, arg1_1):\n                         return x.cos()\n \n                     return torch.cond(\n-                        y.cos().shape[0] > 5, true_true_fn, true_false_fn, [y.cos()]\n+                        y.cos().sum() > 5, true_true_fn, true_false_fn, [y.cos()]\n                     )\n \n                 def false_fn(x):\n@@ -4245,7 +4245,7 @@ def forward(self, arg0_1, arg1_1):\n                     z.add_(6)\n                     return z.sin()\n \n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n                 return (a + 3, a + 4)\n \n         inp = torch.randn(2, 2)\n@@ -4254,10 +4254,12 @@ def forward(self, arg0_1, arg1_1):\n             str(gm.code).strip(),\n             \"\"\"\\\n def forward(self, arg0_1):\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1]);  true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n     add = torch.ops.aten.add.Tensor(getitem, 3)\n     add_1 = torch.ops.aten.add.Tensor(getitem, 4);  getitem = None\n     return (add, add_1)\"\"\",  # noqa: B950\n@@ -4270,11 +4272,13 @@ def forward(self, arg0_1):\n     sin = torch.ops.aten.sin.default(arg0_1);  arg0_1 = None\n     add = torch.ops.aten.add.Tensor(sin, 5);  sin = None\n     cos = torch.ops.aten.cos.default(add)\n+    sum_1 = torch.ops.aten.sum.default(cos);  cos = None\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 5);  sum_1 = None\n     cos_1 = torch.ops.aten.cos.default(add);  add = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [cos_1]);  true_graph_0 = false_graph_0 = cos_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [cos_1]);  gt = true_graph_0 = false_graph_0 = cos_1 = None\n+    getitem = cond[0];  cond = None\n     return (getitem,)\"\"\",  # noqa: B950\n         )\n \n@@ -4317,7 +4321,7 @@ def forward(self, arg0_1):\n                         + control_flow.map(f, z, r).sum()\n                     )\n \n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x, y])\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x, y])\n                 return (a + 3, a + 4)\n \n         inps = [torch.randn(2, 2), torch.ones(2)]\n@@ -4326,10 +4330,12 @@ def forward(self, arg0_1):\n             str(gm.code).strip(),\n             \"\"\"\\\n def forward(self, arg0_1, arg1_1):\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1, arg1_1]);  true_graph_0 = false_graph_0 = arg0_1 = arg1_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1, arg1_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = arg1_1 = None\n+    getitem = cond[0];  cond = None\n     add = torch.ops.aten.add.Tensor(getitem, 3)\n     add_1 = torch.ops.aten.add.Tensor(getitem, 4);  getitem = None\n     return (add, add_1)\"\"\",  # noqa: B950\n@@ -4434,7 +4440,7 @@ def forward(self, arg0_1, arg1_1):\n                     z.add_(6)\n                     return z.sin()\n \n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n                 return (a + 3, a + 4)\n \n         inp = torch.randn(2, 2)\n@@ -4443,10 +4449,12 @@ def forward(self, arg0_1, arg1_1):\n             str(gm.code).strip(),\n             \"\"\"\\\n def forward(self, arg0_1):\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1]);  true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n     add = torch.ops.aten.add.Tensor(getitem, 3)\n     add_1 = torch.ops.aten.add.Tensor(getitem, 4);  getitem = None\n     return (add, add_1)\"\"\",  # noqa: B950\n@@ -4867,7 +4875,7 @@ def forward(self, arg0_1, arg1_1, arg2_1):\n                     y.add_(6)\n                     return x.sin()\n \n-                a = torch.cond(x.shape[0] > 4, true_fn, false_fn, [x])\n+                a = torch.cond(x.sum() > 4, true_fn, false_fn, [x])\n                 return (a + 3, a + 4)\n \n         inp = torch.randn(3, 4)\n@@ -4876,10 +4884,12 @@ def forward(self, arg0_1, arg1_1, arg2_1):\n             gm.code.strip(),\n             \"\"\"\\\n def forward(self, arg0_1):\n+    sum_1 = torch.ops.aten.sum.default(arg0_1)\n+    gt = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [arg0_1]);  true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n     add = torch.ops.aten.add.Tensor(getitem, 3)\n     add_1 = torch.ops.aten.add.Tensor(getitem, 4);  getitem = None\n     return (add, add_1)\"\"\",  # noqa: B950\n"
        },
        {
            "name": "test_control_flow.py",
            "path": "test/functorch/test_control_flow.py",
            "patches": [
                {
                    "old_start": 877,
                    "old_length": 7,
                    "new_start": 877,
                    "new_length": 7,
                    "hunk": "@@ -877,7 +877,7 @@ def forward(self, arg0_1, arg1_1, arg2_1, arg3_1):\n             f(x, torch.tensor(True), torch.tensor(True)),\n         )\n \n-    def test_cond_functionalized_hah(self):\n+    def test_cond_functionalized(self):\n         def true_fn(x):\n             y = x.sin()\n             y.add_(4)\n"
                },
                {
                    "old_start": 894,
                    "old_length": 7,
                    "new_start": 894,
                    "new_length": 9,
                    "hunk": "@@ -894,7 +894,9 @@ def forward(self, arg0_1, arg1_1, arg2_1, arg3_1):\n         functional_f = torch.func.functionalize(f)\n         self.assertEqual(functional_f(*example_inputs), f(*example_inputs))\n \n-        graph_module = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        graph_module = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+            *example_inputs\n+        )\n         self.assertEqual(graph_module(*example_inputs), f(*example_inputs))\n \n         all_ops_in_true_branch = []\n"
                },
                {
                    "old_start": 904,
                    "old_length": 9,
                    "new_start": 906,
                    "new_length": 6,
                    "hunk": "@@ -904,9 +906,6 @@ def forward(self, arg0_1, arg1_1, arg2_1, arg3_1):\n \n         self.assertFalse(any(op._schema.is_mutable for op in all_ops_in_true_branch))\n \n-        graph_module = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n-            *example_inputs\n-        )\n         self.assertEqual(graph_module(*example_inputs), f(*example_inputs))\n \n     def test_cond_accepts_torch_function_as_inputs(self):\n"
                },
                {
                    "old_start": 925,
                    "old_length": 8,
                    "new_start": 924,
                    "new_length": 8,
                    "hunk": "@@ -925,8 +924,8 @@ def forward(self, a_1, b_1):\n     gt = torch.ops.aten.gt.Scalar(sum_1, 0);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [a_1, b_1]);  gt = true_graph_0 = false_graph_0 = a_1 = b_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [a_1, b_1]);  gt = true_graph_0 = false_graph_0 = a_1 = b_1 = None\n+    getitem = cond[0];  cond = None\n     return getitem\"\"\",  # noqa: B950\n         )\n         self.assertExpectedInline(\n"
                },
                {
                    "old_start": 973,
                    "old_length": 9,
                    "new_start": 972,
                    "new_length": 9,
                    "hunk": "@@ -973,9 +972,9 @@ def forward(self, arg0_1, arg1_1):\n             z = torch.add(y, y)\n             return z\n \n-        symbolic_traced_graph = self._check_tracing(f, (torch.ones(4), True))[\n-            \"symbolic\"\n-        ]\n+        symbolic_traced_graph = self._check_tracing(\n+            f, (torch.ones(4), torch.Tensor([True]))\n+        )[\"symbolic\"]\n         graph_shape_env = symbolic_traced_graph.shape_env\n \n         def _node_shape_env_iter(gm):\n"
                },
                {
                    "old_start": 1021,
                    "old_length": 15,
                    "new_start": 1020,
                    "new_length": 14,
                    "hunk": "@@ -1021,15 +1020,14 @@ def forward(self, arg0_1, arg1_1):\n         functional_f = torch.func.functionalize(f)\n         self.assertEqual(functional_f(*example_inputs), f(*example_inputs))\n \n-        graph_module = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        graph_module = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+            *example_inputs\n+        )\n         self.assertEqual(graph_module(*example_inputs), f(*example_inputs))\n \n         gm_true_true_branch = graph_module.true_graph_0.true_graph_0\n \n-        graph_module1 = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n-            *example_inputs\n-        )\n-        self.assertEqual(graph_module1(*example_inputs), f(*example_inputs))\n+        self.assertEqual(graph_module(*example_inputs), f(*example_inputs))\n \n         all_ops = []\n         for node in gm_true_true_branch.graph.nodes:\n"
                },
                {
                    "old_start": 1057,
                    "old_length": 8,
                    "new_start": 1055,
                    "new_length": 7,
                    "hunk": "@@ -1057,8 +1055,7 @@ def forward(self, arg0_1, arg1_1):\n         self.assertEqual(graph_module(*example_inputs), f(*example_inputs))\n \n     # https://github.com/pytorch/pytorch/issues/126988\n-    @xfailIfTorchDynamo\n-    def test_cond_functionalized_input_mutation_on_true_branch(self):\n+    def test_cond_functionalized_input_mutation_on_true_brancte(self):\n         def true_fn(x):\n             view_x = x.view(x.shape)\n             view_x.add_(1)\n"
                },
                {
                    "old_start": 1072,
                    "old_length": 19,
                    "new_start": 1069,
                    "new_length": 33,
                    "hunk": "@@ -1072,19 +1069,33 @@ def forward(self, arg0_1, arg1_1):\n             return cond(pred, true_fn, false_fn, [x])\n \n         example_inputs = (torch.ones(4, 5),)\n-        functional_f = torch.func.functionalize(f)\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-        ):\n-            functional_f(*example_inputs)\n+        # torch.cond inlines into one of the branches because the predicate\n+        # is a constant.\n+        gm = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        self.assertExpectedInline(\n+            gm.code.strip(),\n+            \"\"\"\\\n+def forward(self, x_1):\n+    view = torch.ops.aten.view.default(x_1, [4, 5])\n+    add = torch.ops.aten.add.Tensor(view, 1);  view = None\n+    view_1 = torch.ops.aten.view.default(add, [4, 5]);  add = None\n+    view_2 = torch.ops.aten.view.default(view_1, [4, 5])\n+    sin = torch.ops.aten.sin.default(view_2);  view_2 = None\n+    sum_1 = torch.ops.aten.sum.default(sin);  sin = None\n+    copy_ = torch.ops.aten.copy_.default(x_1, view_1);  x_1 = view_1 = None\n+    return sum_1\"\"\",\n+        )\n \n+        # torch.cond triggers the check of the branches because the predicate\n+        # is a SymBool.\n         with self.assertRaisesRegex(\n             UnsupportedAliasMutationException, \"One of torch.cond branch\"\n         ):\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n \n     # https://github.com/pytorch/pytorch/issues/126988\n-    @xfailIfTorchDynamo\n     def test_cond_functionalized_input_mutation_on_false_branch(self):\n         def true_fn(x):\n             return x.sin().sum()\n"
                },
                {
                    "old_start": 1099,
                    "old_length": 19,
                    "new_start": 1110,
                    "new_length": 33,
                    "hunk": "@@ -1099,19 +1110,33 @@ def forward(self, arg0_1, arg1_1):\n             return cond(pred, true_fn, false_fn, [x])\n \n         example_inputs = (torch.ones(5, 5),)\n-        functional_f = torch.func.functionalize(f)\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-        ):\n-            functional_f(*example_inputs)\n+        gm = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        # torch.cond inlines into one of the branches because the predicate\n+        # is a constant.\n+        self.assertExpectedInline(\n+            gm.code.strip(),\n+            \"\"\"\\\n+def forward(self, x_1):\n+    view = torch.ops.aten.view.default(x_1, [5, 5])\n+    add = torch.ops.aten.add.Tensor(view, 1);  view = None\n+    view_1 = torch.ops.aten.view.default(add, [5, 5]);  add = None\n+    view_2 = torch.ops.aten.view.default(view_1, [5, 5])\n+    cos = torch.ops.aten.cos.default(view_2);  view_2 = None\n+    sum_1 = torch.ops.aten.sum.default(cos);  cos = None\n+    copy_ = torch.ops.aten.copy_.default(x_1, view_1);  x_1 = view_1 = None\n+    return sum_1\"\"\",\n+        )\n \n+        # torch.cond triggers the check of the branches because the predicate\n+        # is a SymBool.\n         with self.assertRaisesRegex(\n             UnsupportedAliasMutationException, \"One of torch.cond branch\"\n         ):\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n \n     # https://github.com/pytorch/pytorch/issues/126988\n-    @xfailIfTorchDynamo\n     def test_cond_functionalized_output_alias_input(self):\n         def true_fn(x):\n             return x\n"
                },
                {
                    "old_start": 1125,
                    "old_length": 22,
                    "new_start": 1150,
                    "new_length": 27,
                    "hunk": "@@ -1125,22 +1150,27 @@ def forward(self, arg0_1, arg1_1):\n             return cond(pred, true_fn, false_fn, [x])\n \n         example_inputs = (torch.ones(5, 5),)\n-        functional_f = torch.func.functionalize(f)\n-\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException,\n-            \"One of torch.cond branch might be aliasing\",\n-        ):\n-            functional_f(*example_inputs)\n+        gm = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        # torch.cond inlines into one of the branches because the predicate\n+        # is a constant.\n+        self.assertExpectedInline(\n+            gm.code.strip(),\n+            \"\"\"\\\n+def forward(self, x_1):\n+    view = torch.ops.aten.view.default(x_1, [5, 5]);  x_1 = None\n+    return view\"\"\",\n+        )\n \n+        # torch.cond triggers the check of the branches because the predicate\n+        # is a SymBool.\n         with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException,\n-            \"One of torch.cond branch might be aliasing\",\n+            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n         ):\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n \n     # https://github.com/pytorch/pytorch/issues/126988\n-    @xfailIfTorchDynamo\n     def test_cond_functionalized_nested_input_mutation(self):\n         def true_true_fn(x):\n             x.add_(4)\n"
                },
                {
                    "old_start": 1161,
                    "old_length": 19,
                    "new_start": 1191,
                    "new_length": 14,
                    "hunk": "@@ -1161,19 +1191,14 @@ def forward(self, arg0_1, arg1_1):\n             return cond(pred, true_fn, false_fn, [x])\n \n         example_inputs = (torch.ones(4, 5),)\n-        functional_f = torch.func.functionalize(f)\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-        ):\n-            functional_f(*example_inputs)\n-\n         with self.assertRaisesRegex(\n             UnsupportedAliasMutationException, \"One of torch.cond branch\"\n         ):\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n \n     # https://github.com/pytorch/pytorch/issues/126988\n-    @xfailIfTorchDynamo\n     def test_cond_functionalized_nested_input_mutation_with_aot_func(self):\n         def true_true_fn(x):\n             x.add_(4)\n"
                },
                {
                    "old_start": 1197,
                    "old_length": 15,
                    "new_start": 1222,
                    "new_length": 12,
                    "hunk": "@@ -1197,15 +1222,12 @@ def forward(self, arg0_1, arg1_1):\n         try:\n             example_input_func = to_fun_old(example_input)\n             torch._enable_functionalization(reapply_views=False)\n-            with self.assertRaisesRegex(\n-                UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-            ):\n-                f(example_input_func)\n+            f(example_input_func)\n \n             with self.assertRaisesRegex(\n                 UnsupportedAliasMutationException, \"One of torch.cond branch\"\n             ):\n-                make_fx(f)(example_input_func)\n+                make_fx(f, tracing_mode=\"symbolic\")(example_input_func)\n         finally:\n             torch._disable_functionalization()\n \n"
                },
                {
                    "old_start": 1223,
                    "old_length": 7,
                    "new_start": 1245,
                    "new_length": 7,
                    "hunk": "@@ -1223,7 +1245,7 @@ def forward(self, arg0_1, arg1_1):\n         with self.assertRaisesRegex(\n             UnsupportedAliasMutationException, \"One of torch.cond branch\"\n         ):\n-            make_fx(f_wrapper(f))(example_input_func)\n+            make_fx(f_wrapper(f), tracing_mode=\"symbolic\")(example_input_func)\n \n     # https://github.com/pytorch/pytorch/issues/126988\n     @xfailIfTorchDynamo\n"
                },
                {
                    "old_start": 1236,
                    "old_length": 7,
                    "new_start": 1258,
                    "new_length": 7,
                    "hunk": "@@ -1236,7 +1258,7 @@ def forward(self, arg0_1, arg1_1):\n             return view_x\n \n         def f(x):\n-            pred = x.shape[0] == 4\n+            pred = x.sum() > 0\n             return cond(pred, true_fn, false_fn, [x])\n \n         example_input = torch.ones(5, 5)\n"
                },
                {
                    "old_start": 1278,
                    "old_length": 7,
                    "new_start": 1300,
                    "new_length": 7,
                    "hunk": "@@ -1278,7 +1300,7 @@ def forward(self, arg0_1, arg1_1):\n             UnsupportedAliasMutationException,\n             \"One of torch.cond branch might be aliasing\",\n         ):\n-            make_fx(f_wrapper(f))(example_input)\n+            make_fx(f_wrapper(f), tracing_mode=\"symbolic\")(example_input)\n \n     def test_cond_functionalized_aot_func_check_functional(self):\n         def true_fn(x):\n"
                },
                {
                    "old_start": 1316,
                    "old_length": 7,
                    "new_start": 1338,
                    "new_length": 7,
                    "hunk": "@@ -1316,7 +1338,7 @@ def forward(self, arg0_1, arg1_1):\n \n             return wrapper\n \n-        result_gm = make_fx(f_wrapper(f))(example_input)\n+        result_gm = make_fx(f_wrapper(f), tracing_mode=\"symbolic\")(example_input)\n         for node in result_gm.true_graph_0.graph.nodes:\n             if node.op == \"call_function\":\n                 self.assertTrue(not node.target._schema.is_mutable)\n"
                },
                {
                    "old_start": 1382,
                    "old_length": 12,
                    "new_start": 1404,
                    "new_length": 12,
                    "hunk": "@@ -1382,12 +1404,12 @@ def forward(self, arg0_1, arg1_1):\n def forward(self, x_1, pred_1, pred2_1):\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n+    getitem = cond[0];  cond = None\n     true_graph_1 = self.true_graph_1\n     false_graph_1 = self.false_graph_1\n-    conditional_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n-    getitem_1 = conditional_1[0];  conditional_1 = None\n+    cond_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n+    getitem_1 = cond_1[0];  cond_1 = None\n     add = torch.ops.aten.add.Tensor(getitem, getitem_1);  getitem = getitem_1 = None\n     return add\"\"\",  # noqa: B950\n         )\n"
                },
                {
                    "old_start": 1555,
                    "old_length": 12,
                    "new_start": 1577,
                    "new_length": 12,
                    "hunk": "@@ -1555,12 +1577,12 @@ def forward(self, arg0_1):\n def forward(self, x_1, pred_1, pred2_1):\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n+    getitem = cond[0];  cond = None\n     true_graph_1 = self.true_graph_1\n     false_graph_1 = self.false_graph_1\n-    conditional_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n-    getitem_1 = conditional_1[0];  conditional_1 = None\n+    cond_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n+    getitem_1 = cond_1[0];  cond_1 = None\n     add = torch.ops.aten.add.Tensor(getitem, getitem_1);  getitem = getitem_1 = None\n     return add\"\"\",  # noqa: B950\n         )\n"
                },
                {
                    "old_start": 1891,
                    "old_length": 7,
                    "new_start": 1913,
                    "new_length": 7,
                    "hunk": "@@ -1891,7 +1913,7 @@ def forward(self, arg0_1):\n         ):\n             functional_f(*example_inputs)\n \n-    def test_cond_autograd_fail(self):\n+    def test_cond_autograd_succeed_when_pred_is_constant(self):\n         def true_fn(x):\n             return x.cos()\n \n"
                },
                {
                    "old_start": 1901,
                    "old_length": 6,
                    "new_start": 1923,
                    "new_length": 27,
                    "hunk": "@@ -1901,6 +1923,27 @@ def forward(self, arg0_1):\n         def f(x, y):\n             return control_flow.cond(x.shape[0] > 4, true_fn, false_fn, [y])\n \n+        example_inputs = (\n+            torch.ones(3, 2, 4, requires_grad=True),\n+            torch.ones(4, requires_grad=True),\n+        )\n+        # Due to x.shape[0] can be statically evaluated to be False, we can evaluate\n+        # the backward.\n+        f(*example_inputs).sum().backward()\n+\n+        # Ensure no error is thrown when not running backward\n+        f(*example_inputs)\n+\n+    def test_cond_autograd_fail(self):\n+        def true_fn(x):\n+            return x.cos()\n+\n+        def false_fn(x):\n+            return x.sin()\n+\n+        def f(x, y):\n+            return control_flow.cond(x.sum() > 4, true_fn, false_fn, [y])\n+\n         example_inputs = (\n             torch.ones(3, 2, 4, requires_grad=True),\n             torch.ones(4, requires_grad=True),\n"
                },
                {
                    "old_start": 2029,
                    "old_length": 8,
                    "new_start": 2072,
                    "new_length": 8,
                    "hunk": "@@ -2029,8 +2072,8 @@ def forward(self, x_1):\n     eq = sym_size_int == 4;  sym_size_int = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n+    getitem = cond[0];  cond = None\n     return getitem\"\"\",  # noqa: B950\n         )\n \n"
                },
                {
                    "old_start": 2102,
                    "old_length": 18,
                    "new_start": 2145,
                    "new_length": 20,
                    "hunk": "@@ -2102,18 +2145,20 @@ def forward(self, x_1):\n         # expected branches takes [x, a, b] as input\n         inp = torch.randn(2, 3)\n \n-        gm = make_fx(foo)(inp)\n+        gm = make_fx(foo, tracing_mode=\"symbolic\", _allow_non_fake_inputs=True)(inp)\n \n         self.assertExpectedInline(\n             gm.code.strip(),\n             \"\"\"\\\n def forward(self, x_1):\n+    sym_size_int = torch.ops.aten.sym_size.int(x_1, 0)\n+    eq = sym_size_int == 4;  sym_size_int = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n     _tensor_constant0 = self._tensor_constant0\n     _tensor_constant1 = self._tensor_constant1\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [x_1, _tensor_constant0, _tensor_constant1]);  true_graph_0 = false_graph_0 = x_1 = _tensor_constant0 = _tensor_constant1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1, _tensor_constant0, _tensor_constant1]);  eq = true_graph_0 = false_graph_0 = x_1 = _tensor_constant0 = _tensor_constant1 = None\n+    getitem = cond[0];  cond = None\n     return getitem\"\"\",  # noqa: B950\n         )\n         self.assertExpectedInline(\n"
                },
                {
                    "old_start": 2263,
                    "old_length": 8,
                    "new_start": 2308,
                    "new_length": 8,
                    "hunk": "@@ -2263,8 +2308,8 @@ def forward(self, pred_1, x_1):\n def forward(self, arg0_1, arg1_1):\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(arg1_1, true_graph_0, false_graph_0, [arg0_1]);  arg1_1 = true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(arg1_1, true_graph_0, false_graph_0, [arg0_1]);  arg1_1 = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n     return [getitem]\"\"\",  # noqa: B950\n         )\n \n"
                },
                {
                    "old_start": 2305,
                    "old_length": 7,
                    "new_start": 2350,
                    "new_length": 7,
                    "hunk": "@@ -2305,7 +2350,7 @@ def forward(self, arg0_1, arg1_1):\n         counters.clear()\n \n         def foo(x, true_fn, false_fn):\n-            return cond(x.shape[0] == 4, true_fn, false_fn, (x,))\n+            return cond(x.sum() < 0, true_fn, false_fn, (x,))\n \n         inp = torch.ones(3, 4)\n         exp_out = inp.sin()\n"
                },
                {
                    "old_start": 2347,
                    "old_length": 8,
                    "new_start": 2392,
                    "new_length": 8,
                    "hunk": "@@ -2347,8 +2392,8 @@ def forward(self, x_1):\n     eq = sym_size_int == 4;  sym_size_int = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n+    getitem = cond[0];  cond = None\n     return getitem\"\"\",  # noqa: B950\n             )\n \n"
                }
            ],
            "whole_deleted": "-    def test_cond_functionalized_hah(self):\n-        graph_module = make_fx(torch.func.functionalize(f))(*example_inputs)\n-        graph_module = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n-            *example_inputs\n-        )\n-    conditional = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [a_1, b_1]);  gt = true_graph_0 = false_graph_0 = a_1 = b_1 = None\n-    getitem = conditional[0];  conditional = None\n-        symbolic_traced_graph = self._check_tracing(f, (torch.ones(4), True))[\n-            \"symbolic\"\n-        ]\n-        graph_module = make_fx(torch.func.functionalize(f))(*example_inputs)\n-        graph_module1 = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n-            *example_inputs\n-        )\n-        self.assertEqual(graph_module1(*example_inputs), f(*example_inputs))\n-    @xfailIfTorchDynamo\n-    def test_cond_functionalized_input_mutation_on_true_branch(self):\n-        functional_f = torch.func.functionalize(f)\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-        ):\n-            functional_f(*example_inputs)\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n-    @xfailIfTorchDynamo\n-        functional_f = torch.func.functionalize(f)\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-        ):\n-            functional_f(*example_inputs)\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n-    @xfailIfTorchDynamo\n-        functional_f = torch.func.functionalize(f)\n-\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException,\n-            \"One of torch.cond branch might be aliasing\",\n-        ):\n-            functional_f(*example_inputs)\n-            UnsupportedAliasMutationException,\n-            \"One of torch.cond branch might be aliasing\",\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n-    @xfailIfTorchDynamo\n-        functional_f = torch.func.functionalize(f)\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-        ):\n-            functional_f(*example_inputs)\n-\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n-    @xfailIfTorchDynamo\n-            with self.assertRaisesRegex(\n-                UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-            ):\n-                f(example_input_func)\n-                make_fx(f)(example_input_func)\n-            make_fx(f_wrapper(f))(example_input_func)\n-            pred = x.shape[0] == 4\n-            make_fx(f_wrapper(f))(example_input)\n-        result_gm = make_fx(f_wrapper(f))(example_input)\n-    conditional = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n-    getitem = conditional[0];  conditional = None\n-    conditional_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n-    getitem_1 = conditional_1[0];  conditional_1 = None\n-    conditional = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n-    getitem = conditional[0];  conditional = None\n-    conditional_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n-    getitem_1 = conditional_1[0];  conditional_1 = None\n-    def test_cond_autograd_fail(self):\n-    conditional = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n-    getitem = conditional[0];  conditional = None\n-        gm = make_fx(foo)(inp)\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [x_1, _tensor_constant0, _tensor_constant1]);  true_graph_0 = false_graph_0 = x_1 = _tensor_constant0 = _tensor_constant1 = None\n-    getitem = conditional[0];  conditional = None\n-    conditional = torch.ops.higher_order.cond(arg1_1, true_graph_0, false_graph_0, [arg0_1]);  arg1_1 = true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n-            return cond(x.shape[0] == 4, true_fn, false_fn, (x,))\n-    conditional = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n-    getitem = conditional[0];  conditional = None\n",
            "whole_added": "+    def test_cond_functionalized(self):\n+        graph_module = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+            *example_inputs\n+        )\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [a_1, b_1]);  gt = true_graph_0 = false_graph_0 = a_1 = b_1 = None\n+    getitem = cond[0];  cond = None\n+        symbolic_traced_graph = self._check_tracing(\n+            f, (torch.ones(4), torch.Tensor([True]))\n+        )[\"symbolic\"]\n+        graph_module = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+            *example_inputs\n+        )\n+        self.assertEqual(graph_module(*example_inputs), f(*example_inputs))\n+    def test_cond_functionalized_input_mutation_on_true_brancte(self):\n+        # torch.cond inlines into one of the branches because the predicate\n+        # is a constant.\n+        gm = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        self.assertExpectedInline(\n+            gm.code.strip(),\n+            \"\"\"\\\n+def forward(self, x_1):\n+    view = torch.ops.aten.view.default(x_1, [4, 5])\n+    add = torch.ops.aten.add.Tensor(view, 1);  view = None\n+    view_1 = torch.ops.aten.view.default(add, [4, 5]);  add = None\n+    view_2 = torch.ops.aten.view.default(view_1, [4, 5])\n+    sin = torch.ops.aten.sin.default(view_2);  view_2 = None\n+    sum_1 = torch.ops.aten.sum.default(sin);  sin = None\n+    copy_ = torch.ops.aten.copy_.default(x_1, view_1);  x_1 = view_1 = None\n+    return sum_1\"\"\",\n+        )\n+        # torch.cond triggers the check of the branches because the predicate\n+        # is a SymBool.\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n+        gm = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        # torch.cond inlines into one of the branches because the predicate\n+        # is a constant.\n+        self.assertExpectedInline(\n+            gm.code.strip(),\n+            \"\"\"\\\n+def forward(self, x_1):\n+    view = torch.ops.aten.view.default(x_1, [5, 5])\n+    add = torch.ops.aten.add.Tensor(view, 1);  view = None\n+    view_1 = torch.ops.aten.view.default(add, [5, 5]);  add = None\n+    view_2 = torch.ops.aten.view.default(view_1, [5, 5])\n+    cos = torch.ops.aten.cos.default(view_2);  view_2 = None\n+    sum_1 = torch.ops.aten.sum.default(cos);  cos = None\n+    copy_ = torch.ops.aten.copy_.default(x_1, view_1);  x_1 = view_1 = None\n+    return sum_1\"\"\",\n+        )\n+        # torch.cond triggers the check of the branches because the predicate\n+        # is a SymBool.\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n+        gm = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        # torch.cond inlines into one of the branches because the predicate\n+        # is a constant.\n+        self.assertExpectedInline(\n+            gm.code.strip(),\n+            \"\"\"\\\n+def forward(self, x_1):\n+    view = torch.ops.aten.view.default(x_1, [5, 5]);  x_1 = None\n+    return view\"\"\",\n+        )\n+        # torch.cond triggers the check of the branches because the predicate\n+        # is a SymBool.\n+            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n+            f(example_input_func)\n+                make_fx(f, tracing_mode=\"symbolic\")(example_input_func)\n+            make_fx(f_wrapper(f), tracing_mode=\"symbolic\")(example_input_func)\n+            pred = x.sum() > 0\n+            make_fx(f_wrapper(f), tracing_mode=\"symbolic\")(example_input)\n+        result_gm = make_fx(f_wrapper(f), tracing_mode=\"symbolic\")(example_input)\n+    cond = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n+    getitem = cond[0];  cond = None\n+    cond_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n+    getitem_1 = cond_1[0];  cond_1 = None\n+    cond = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n+    getitem = cond[0];  cond = None\n+    cond_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n+    getitem_1 = cond_1[0];  cond_1 = None\n+    def test_cond_autograd_succeed_when_pred_is_constant(self):\n+        example_inputs = (\n+            torch.ones(3, 2, 4, requires_grad=True),\n+            torch.ones(4, requires_grad=True),\n+        )\n+        # Due to x.shape[0] can be statically evaluated to be False, we can evaluate\n+        # the backward.\n+        f(*example_inputs).sum().backward()\n+\n+        # Ensure no error is thrown when not running backward\n+        f(*example_inputs)\n+\n+    def test_cond_autograd_fail(self):\n+        def true_fn(x):\n+            return x.cos()\n+\n+        def false_fn(x):\n+            return x.sin()\n+\n+        def f(x, y):\n+            return control_flow.cond(x.sum() > 4, true_fn, false_fn, [y])\n+\n+    cond = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n+    getitem = cond[0];  cond = None\n+        gm = make_fx(foo, tracing_mode=\"symbolic\", _allow_non_fake_inputs=True)(inp)\n+    sym_size_int = torch.ops.aten.sym_size.int(x_1, 0)\n+    eq = sym_size_int == 4;  sym_size_int = None\n+    cond = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1, _tensor_constant0, _tensor_constant1]);  eq = true_graph_0 = false_graph_0 = x_1 = _tensor_constant0 = _tensor_constant1 = None\n+    getitem = cond[0];  cond = None\n+    cond = torch.ops.higher_order.cond(arg1_1, true_graph_0, false_graph_0, [arg0_1]);  arg1_1 = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n+            return cond(x.sum() < 0, true_fn, false_fn, (x,))\n+    cond = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n+    getitem = cond[0];  cond = None\n",
            "whole_hunk": "@@ -877,7 +877,7 @@ def forward(self, arg0_1, arg1_1, arg2_1, arg3_1):\n             f(x, torch.tensor(True), torch.tensor(True)),\n         )\n \n-    def test_cond_functionalized_hah(self):\n+    def test_cond_functionalized(self):\n         def true_fn(x):\n             y = x.sin()\n             y.add_(4)\n@@ -894,7 +894,9 @@ def forward(self, arg0_1, arg1_1, arg2_1, arg3_1):\n         functional_f = torch.func.functionalize(f)\n         self.assertEqual(functional_f(*example_inputs), f(*example_inputs))\n \n-        graph_module = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        graph_module = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+            *example_inputs\n+        )\n         self.assertEqual(graph_module(*example_inputs), f(*example_inputs))\n \n         all_ops_in_true_branch = []\n@@ -904,9 +906,6 @@ def forward(self, arg0_1, arg1_1, arg2_1, arg3_1):\n \n         self.assertFalse(any(op._schema.is_mutable for op in all_ops_in_true_branch))\n \n-        graph_module = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n-            *example_inputs\n-        )\n         self.assertEqual(graph_module(*example_inputs), f(*example_inputs))\n \n     def test_cond_accepts_torch_function_as_inputs(self):\n@@ -925,8 +924,8 @@ def forward(self, a_1, b_1):\n     gt = torch.ops.aten.gt.Scalar(sum_1, 0);  sum_1 = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [a_1, b_1]);  gt = true_graph_0 = false_graph_0 = a_1 = b_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [a_1, b_1]);  gt = true_graph_0 = false_graph_0 = a_1 = b_1 = None\n+    getitem = cond[0];  cond = None\n     return getitem\"\"\",  # noqa: B950\n         )\n         self.assertExpectedInline(\n@@ -973,9 +972,9 @@ def forward(self, arg0_1, arg1_1):\n             z = torch.add(y, y)\n             return z\n \n-        symbolic_traced_graph = self._check_tracing(f, (torch.ones(4), True))[\n-            \"symbolic\"\n-        ]\n+        symbolic_traced_graph = self._check_tracing(\n+            f, (torch.ones(4), torch.Tensor([True]))\n+        )[\"symbolic\"]\n         graph_shape_env = symbolic_traced_graph.shape_env\n \n         def _node_shape_env_iter(gm):\n@@ -1021,15 +1020,14 @@ def forward(self, arg0_1, arg1_1):\n         functional_f = torch.func.functionalize(f)\n         self.assertEqual(functional_f(*example_inputs), f(*example_inputs))\n \n-        graph_module = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        graph_module = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+            *example_inputs\n+        )\n         self.assertEqual(graph_module(*example_inputs), f(*example_inputs))\n \n         gm_true_true_branch = graph_module.true_graph_0.true_graph_0\n \n-        graph_module1 = make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n-            *example_inputs\n-        )\n-        self.assertEqual(graph_module1(*example_inputs), f(*example_inputs))\n+        self.assertEqual(graph_module(*example_inputs), f(*example_inputs))\n \n         all_ops = []\n         for node in gm_true_true_branch.graph.nodes:\n@@ -1057,8 +1055,7 @@ def forward(self, arg0_1, arg1_1):\n         self.assertEqual(graph_module(*example_inputs), f(*example_inputs))\n \n     # https://github.com/pytorch/pytorch/issues/126988\n-    @xfailIfTorchDynamo\n-    def test_cond_functionalized_input_mutation_on_true_branch(self):\n+    def test_cond_functionalized_input_mutation_on_true_brancte(self):\n         def true_fn(x):\n             view_x = x.view(x.shape)\n             view_x.add_(1)\n@@ -1072,19 +1069,33 @@ def forward(self, arg0_1, arg1_1):\n             return cond(pred, true_fn, false_fn, [x])\n \n         example_inputs = (torch.ones(4, 5),)\n-        functional_f = torch.func.functionalize(f)\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-        ):\n-            functional_f(*example_inputs)\n+        # torch.cond inlines into one of the branches because the predicate\n+        # is a constant.\n+        gm = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        self.assertExpectedInline(\n+            gm.code.strip(),\n+            \"\"\"\\\n+def forward(self, x_1):\n+    view = torch.ops.aten.view.default(x_1, [4, 5])\n+    add = torch.ops.aten.add.Tensor(view, 1);  view = None\n+    view_1 = torch.ops.aten.view.default(add, [4, 5]);  add = None\n+    view_2 = torch.ops.aten.view.default(view_1, [4, 5])\n+    sin = torch.ops.aten.sin.default(view_2);  view_2 = None\n+    sum_1 = torch.ops.aten.sum.default(sin);  sin = None\n+    copy_ = torch.ops.aten.copy_.default(x_1, view_1);  x_1 = view_1 = None\n+    return sum_1\"\"\",\n+        )\n \n+        # torch.cond triggers the check of the branches because the predicate\n+        # is a SymBool.\n         with self.assertRaisesRegex(\n             UnsupportedAliasMutationException, \"One of torch.cond branch\"\n         ):\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n \n     # https://github.com/pytorch/pytorch/issues/126988\n-    @xfailIfTorchDynamo\n     def test_cond_functionalized_input_mutation_on_false_branch(self):\n         def true_fn(x):\n             return x.sin().sum()\n@@ -1099,19 +1110,33 @@ def forward(self, arg0_1, arg1_1):\n             return cond(pred, true_fn, false_fn, [x])\n \n         example_inputs = (torch.ones(5, 5),)\n-        functional_f = torch.func.functionalize(f)\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-        ):\n-            functional_f(*example_inputs)\n+        gm = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        # torch.cond inlines into one of the branches because the predicate\n+        # is a constant.\n+        self.assertExpectedInline(\n+            gm.code.strip(),\n+            \"\"\"\\\n+def forward(self, x_1):\n+    view = torch.ops.aten.view.default(x_1, [5, 5])\n+    add = torch.ops.aten.add.Tensor(view, 1);  view = None\n+    view_1 = torch.ops.aten.view.default(add, [5, 5]);  add = None\n+    view_2 = torch.ops.aten.view.default(view_1, [5, 5])\n+    cos = torch.ops.aten.cos.default(view_2);  view_2 = None\n+    sum_1 = torch.ops.aten.sum.default(cos);  cos = None\n+    copy_ = torch.ops.aten.copy_.default(x_1, view_1);  x_1 = view_1 = None\n+    return sum_1\"\"\",\n+        )\n \n+        # torch.cond triggers the check of the branches because the predicate\n+        # is a SymBool.\n         with self.assertRaisesRegex(\n             UnsupportedAliasMutationException, \"One of torch.cond branch\"\n         ):\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n \n     # https://github.com/pytorch/pytorch/issues/126988\n-    @xfailIfTorchDynamo\n     def test_cond_functionalized_output_alias_input(self):\n         def true_fn(x):\n             return x\n@@ -1125,22 +1150,27 @@ def forward(self, arg0_1, arg1_1):\n             return cond(pred, true_fn, false_fn, [x])\n \n         example_inputs = (torch.ones(5, 5),)\n-        functional_f = torch.func.functionalize(f)\n-\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException,\n-            \"One of torch.cond branch might be aliasing\",\n-        ):\n-            functional_f(*example_inputs)\n+        gm = make_fx(torch.func.functionalize(f))(*example_inputs)\n+        # torch.cond inlines into one of the branches because the predicate\n+        # is a constant.\n+        self.assertExpectedInline(\n+            gm.code.strip(),\n+            \"\"\"\\\n+def forward(self, x_1):\n+    view = torch.ops.aten.view.default(x_1, [5, 5]);  x_1 = None\n+    return view\"\"\",\n+        )\n \n+        # torch.cond triggers the check of the branches because the predicate\n+        # is a SymBool.\n         with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException,\n-            \"One of torch.cond branch might be aliasing\",\n+            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n         ):\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n \n     # https://github.com/pytorch/pytorch/issues/126988\n-    @xfailIfTorchDynamo\n     def test_cond_functionalized_nested_input_mutation(self):\n         def true_true_fn(x):\n             x.add_(4)\n@@ -1161,19 +1191,14 @@ def forward(self, arg0_1, arg1_1):\n             return cond(pred, true_fn, false_fn, [x])\n \n         example_inputs = (torch.ones(4, 5),)\n-        functional_f = torch.func.functionalize(f)\n-        with self.assertRaisesRegex(\n-            UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-        ):\n-            functional_f(*example_inputs)\n-\n         with self.assertRaisesRegex(\n             UnsupportedAliasMutationException, \"One of torch.cond branch\"\n         ):\n-            make_fx(torch.func.functionalize(f))(*example_inputs)\n+            make_fx(torch.func.functionalize(f), tracing_mode=\"symbolic\")(\n+                *example_inputs\n+            )\n \n     # https://github.com/pytorch/pytorch/issues/126988\n-    @xfailIfTorchDynamo\n     def test_cond_functionalized_nested_input_mutation_with_aot_func(self):\n         def true_true_fn(x):\n             x.add_(4)\n@@ -1197,15 +1222,12 @@ def forward(self, arg0_1, arg1_1):\n         try:\n             example_input_func = to_fun_old(example_input)\n             torch._enable_functionalization(reapply_views=False)\n-            with self.assertRaisesRegex(\n-                UnsupportedAliasMutationException, \"One of torch.cond branch\"\n-            ):\n-                f(example_input_func)\n+            f(example_input_func)\n \n             with self.assertRaisesRegex(\n                 UnsupportedAliasMutationException, \"One of torch.cond branch\"\n             ):\n-                make_fx(f)(example_input_func)\n+                make_fx(f, tracing_mode=\"symbolic\")(example_input_func)\n         finally:\n             torch._disable_functionalization()\n \n@@ -1223,7 +1245,7 @@ def forward(self, arg0_1, arg1_1):\n         with self.assertRaisesRegex(\n             UnsupportedAliasMutationException, \"One of torch.cond branch\"\n         ):\n-            make_fx(f_wrapper(f))(example_input_func)\n+            make_fx(f_wrapper(f), tracing_mode=\"symbolic\")(example_input_func)\n \n     # https://github.com/pytorch/pytorch/issues/126988\n     @xfailIfTorchDynamo\n@@ -1236,7 +1258,7 @@ def forward(self, arg0_1, arg1_1):\n             return view_x\n \n         def f(x):\n-            pred = x.shape[0] == 4\n+            pred = x.sum() > 0\n             return cond(pred, true_fn, false_fn, [x])\n \n         example_input = torch.ones(5, 5)\n@@ -1278,7 +1300,7 @@ def forward(self, arg0_1, arg1_1):\n             UnsupportedAliasMutationException,\n             \"One of torch.cond branch might be aliasing\",\n         ):\n-            make_fx(f_wrapper(f))(example_input)\n+            make_fx(f_wrapper(f), tracing_mode=\"symbolic\")(example_input)\n \n     def test_cond_functionalized_aot_func_check_functional(self):\n         def true_fn(x):\n@@ -1316,7 +1338,7 @@ def forward(self, arg0_1, arg1_1):\n \n             return wrapper\n \n-        result_gm = make_fx(f_wrapper(f))(example_input)\n+        result_gm = make_fx(f_wrapper(f), tracing_mode=\"symbolic\")(example_input)\n         for node in result_gm.true_graph_0.graph.nodes:\n             if node.op == \"call_function\":\n                 self.assertTrue(not node.target._schema.is_mutable)\n@@ -1382,12 +1404,12 @@ def forward(self, arg0_1, arg1_1):\n def forward(self, x_1, pred_1, pred2_1):\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n+    getitem = cond[0];  cond = None\n     true_graph_1 = self.true_graph_1\n     false_graph_1 = self.false_graph_1\n-    conditional_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n-    getitem_1 = conditional_1[0];  conditional_1 = None\n+    cond_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n+    getitem_1 = cond_1[0];  cond_1 = None\n     add = torch.ops.aten.add.Tensor(getitem, getitem_1);  getitem = getitem_1 = None\n     return add\"\"\",  # noqa: B950\n         )\n@@ -1555,12 +1577,12 @@ def forward(self, arg0_1):\n def forward(self, x_1, pred_1, pred2_1):\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(pred_1, true_graph_0, false_graph_0, [x_1]);  pred_1 = true_graph_0 = false_graph_0 = None\n+    getitem = cond[0];  cond = None\n     true_graph_1 = self.true_graph_1\n     false_graph_1 = self.false_graph_1\n-    conditional_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n-    getitem_1 = conditional_1[0];  conditional_1 = None\n+    cond_1 = torch.ops.higher_order.cond(pred2_1, true_graph_1, false_graph_1, [x_1]);  pred2_1 = true_graph_1 = false_graph_1 = x_1 = None\n+    getitem_1 = cond_1[0];  cond_1 = None\n     add = torch.ops.aten.add.Tensor(getitem, getitem_1);  getitem = getitem_1 = None\n     return add\"\"\",  # noqa: B950\n         )\n@@ -1891,7 +1913,7 @@ def forward(self, arg0_1):\n         ):\n             functional_f(*example_inputs)\n \n-    def test_cond_autograd_fail(self):\n+    def test_cond_autograd_succeed_when_pred_is_constant(self):\n         def true_fn(x):\n             return x.cos()\n \n@@ -1901,6 +1923,27 @@ def forward(self, arg0_1):\n         def f(x, y):\n             return control_flow.cond(x.shape[0] > 4, true_fn, false_fn, [y])\n \n+        example_inputs = (\n+            torch.ones(3, 2, 4, requires_grad=True),\n+            torch.ones(4, requires_grad=True),\n+        )\n+        # Due to x.shape[0] can be statically evaluated to be False, we can evaluate\n+        # the backward.\n+        f(*example_inputs).sum().backward()\n+\n+        # Ensure no error is thrown when not running backward\n+        f(*example_inputs)\n+\n+    def test_cond_autograd_fail(self):\n+        def true_fn(x):\n+            return x.cos()\n+\n+        def false_fn(x):\n+            return x.sin()\n+\n+        def f(x, y):\n+            return control_flow.cond(x.sum() > 4, true_fn, false_fn, [y])\n+\n         example_inputs = (\n             torch.ones(3, 2, 4, requires_grad=True),\n             torch.ones(4, requires_grad=True),\n@@ -2029,8 +2072,8 @@ def forward(self, x_1):\n     eq = sym_size_int == 4;  sym_size_int = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n+    getitem = cond[0];  cond = None\n     return getitem\"\"\",  # noqa: B950\n         )\n \n@@ -2102,18 +2145,20 @@ def forward(self, x_1):\n         # expected branches takes [x, a, b] as input\n         inp = torch.randn(2, 3)\n \n-        gm = make_fx(foo)(inp)\n+        gm = make_fx(foo, tracing_mode=\"symbolic\", _allow_non_fake_inputs=True)(inp)\n \n         self.assertExpectedInline(\n             gm.code.strip(),\n             \"\"\"\\\n def forward(self, x_1):\n+    sym_size_int = torch.ops.aten.sym_size.int(x_1, 0)\n+    eq = sym_size_int == 4;  sym_size_int = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n     _tensor_constant0 = self._tensor_constant0\n     _tensor_constant1 = self._tensor_constant1\n-    conditional = torch.ops.higher_order.cond(False, true_graph_0, false_graph_0, [x_1, _tensor_constant0, _tensor_constant1]);  true_graph_0 = false_graph_0 = x_1 = _tensor_constant0 = _tensor_constant1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1, _tensor_constant0, _tensor_constant1]);  eq = true_graph_0 = false_graph_0 = x_1 = _tensor_constant0 = _tensor_constant1 = None\n+    getitem = cond[0];  cond = None\n     return getitem\"\"\",  # noqa: B950\n         )\n         self.assertExpectedInline(\n@@ -2263,8 +2308,8 @@ def forward(self, pred_1, x_1):\n def forward(self, arg0_1, arg1_1):\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(arg1_1, true_graph_0, false_graph_0, [arg0_1]);  arg1_1 = true_graph_0 = false_graph_0 = arg0_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(arg1_1, true_graph_0, false_graph_0, [arg0_1]);  arg1_1 = true_graph_0 = false_graph_0 = arg0_1 = None\n+    getitem = cond[0];  cond = None\n     return [getitem]\"\"\",  # noqa: B950\n         )\n \n@@ -2305,7 +2350,7 @@ def forward(self, arg0_1, arg1_1):\n         counters.clear()\n \n         def foo(x, true_fn, false_fn):\n-            return cond(x.shape[0] == 4, true_fn, false_fn, (x,))\n+            return cond(x.sum() < 0, true_fn, false_fn, (x,))\n \n         inp = torch.ones(3, 4)\n         exp_out = inp.sin()\n@@ -2347,8 +2392,8 @@ def forward(self, x_1):\n     eq = sym_size_int == 4;  sym_size_int = None\n     true_graph_0 = self.true_graph_0\n     false_graph_0 = self.false_graph_0\n-    conditional = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n-    getitem = conditional[0];  conditional = None\n+    cond = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None\n+    getitem = cond[0];  cond = None\n     return getitem\"\"\",  # noqa: B950\n             )\n \n"
        },
        {
            "name": "higher_order_ops.py",
            "path": "torch/_dynamo/variables/higher_order_ops.py",
            "patches": [
                {
                    "old_start": 632,
                    "old_length": 6,
                    "new_start": 632,
                    "new_length": 18,
                    "hunk": "@@ -632,6 +632,18 @@ class CondHigherOrderVariable(TorchHigherOrderOperatorVariable):\n                 f\"Expected 4 arguments but got {len(args)}.\\n\"\n                 f\"Usage: cond(pred, true_fn, false_fn, operands)\",\n             )\n+\n+        # Specialize into one of the branches since pred is constant\n+        if type(args[0]) is ConstantVariable:\n+            log.warning(\n+                \"Pred is a Python constant. When used with torch.cond, it executes only one of the branches.\"\n+                \" If you want torch.cond to perserve two branches, please make the predicate a boolean tensor or a SymBool.\"\n+            )\n+            if args[0].as_python_constant():\n+                return args[1].call_function(tx, args[3].unpack_var_sequence(tx), {})\n+            else:\n+                return args[2].call_function(tx, args[3].unpack_var_sequence(tx), {})\n+\n         # predicate\n         if type(args[0]) not in (ConstantVariable, TensorVariable, SymNodeVariable):\n             unimplemented(\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+\n+        # Specialize into one of the branches since pred is constant\n+        if type(args[0]) is ConstantVariable:\n+            log.warning(\n+                \"Pred is a Python constant. When used with torch.cond, it executes only one of the branches.\"\n+                \" If you want torch.cond to perserve two branches, please make the predicate a boolean tensor or a SymBool.\"\n+            )\n+            if args[0].as_python_constant():\n+                return args[1].call_function(tx, args[3].unpack_var_sequence(tx), {})\n+            else:\n+                return args[2].call_function(tx, args[3].unpack_var_sequence(tx), {})\n+\n",
            "whole_hunk": "@@ -632,6 +632,18 @@ class CondHigherOrderVariable(TorchHigherOrderOperatorVariable):\n                 f\"Expected 4 arguments but got {len(args)}.\\n\"\n                 f\"Usage: cond(pred, true_fn, false_fn, operands)\",\n             )\n+\n+        # Specialize into one of the branches since pred is constant\n+        if type(args[0]) is ConstantVariable:\n+            log.warning(\n+                \"Pred is a Python constant. When used with torch.cond, it executes only one of the branches.\"\n+                \" If you want torch.cond to perserve two branches, please make the predicate a boolean tensor or a SymBool.\"\n+            )\n+            if args[0].as_python_constant():\n+                return args[1].call_function(tx, args[3].unpack_var_sequence(tx), {})\n+            else:\n+                return args[2].call_function(tx, args[3].unpack_var_sequence(tx), {})\n+\n         # predicate\n         if type(args[0]) not in (ConstantVariable, TensorVariable, SymNodeVariable):\n             unimplemented(\n"
        },
        {
            "name": "cond.py",
            "path": "torch/_higher_order_ops/cond.py",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 6,
                    "new_start": 1,
                    "new_length": 8,
                    "hunk": "@@ -1,6 +1,8 @@\n # mypy: allow-untyped-defs\n import contextlib\n \n+import logging\n+\n import torch\n import torch._subclasses.functional_tensor\n import torch.utils._pytree as pytree\n"
                },
                {
                    "old_start": 32,
                    "old_length": 6,
                    "new_start": 34,
                    "new_length": 8,
                    "hunk": "@@ -32,6 +34,8 @@ from torch.fx.experimental.proxy_tensor import (\n from torch.fx.passes.shape_prop import _extract_tensor_metadata\n from torch.utils._python_dispatch import _get_current_dispatch_mode\n \n+log = logging.getLogger(__name__)\n+\n \n @exposed_in(\"torch\")\n def cond(pred, true_fn, false_fn, operands):\n"
                },
                {
                    "old_start": 107,
                    "old_length": 6,
                    "new_start": 110,
                    "new_length": 16,
                    "hunk": "@@ -107,6 +110,16 @@ def cond(pred, true_fn, false_fn, operands):\n     if torch.compiler.is_dynamo_compiling():\n         return cond_op(pred, true_fn, false_fn, operands)\n \n+    if isinstance(pred, (bool, int, float)):\n+        log.warning(\n+            \"Pred is a Python constant. When used with torch.cond, it executes only one of the branches.\"\n+            \" If you want torch.cond to perserve two branches, please make the predicate a boolean tensor or a SymBool.\"\n+        )\n+        if pred:\n+            return true_fn(*operands)\n+        else:\n+            return false_fn(*operands)\n+\n     def _validate_input(pred, true_fn, false_fn, operands):\n         if not isinstance(pred, (bool, torch.Tensor, torch.SymBool)):\n             raise RuntimeError(f\"Expected pred to be bool or tensor, but got {pred}.\")\n"
                },
                {
                    "old_start": 200,
                    "old_length": 7,
                    "new_start": 213,
                    "new_length": 7,
                    "hunk": "@@ -200,7 +213,7 @@ def trace_cond(proxy_mode, func_overload, pred, true_fn, false_fn, operands):\n     proxy_args = pytree.tree_map(proxy_mode.tracer.unwrap_proxy, args)\n \n     out_proxy = proxy_mode.tracer.create_proxy(\n-        \"call_function\", func_overload, proxy_args, {}, name=\"conditional\"\n+        \"call_function\", func_overload, proxy_args, {}\n     )\n \n     # At this point, we're *guaranteed* that whether an output came from the"
                }
            ],
            "whole_deleted": "-        \"call_function\", func_overload, proxy_args, {}, name=\"conditional\"\n",
            "whole_added": "+import logging\n+\n+log = logging.getLogger(__name__)\n+\n+    if isinstance(pred, (bool, int, float)):\n+        log.warning(\n+            \"Pred is a Python constant. When used with torch.cond, it executes only one of the branches.\"\n+            \" If you want torch.cond to perserve two branches, please make the predicate a boolean tensor or a SymBool.\"\n+        )\n+        if pred:\n+            return true_fn(*operands)\n+        else:\n+            return false_fn(*operands)\n+\n+        \"call_function\", func_overload, proxy_args, {}\n",
            "whole_hunk": "@@ -1,6 +1,8 @@\n # mypy: allow-untyped-defs\n import contextlib\n \n+import logging\n+\n import torch\n import torch._subclasses.functional_tensor\n import torch.utils._pytree as pytree\n@@ -32,6 +34,8 @@ from torch.fx.experimental.proxy_tensor import (\n from torch.fx.passes.shape_prop import _extract_tensor_metadata\n from torch.utils._python_dispatch import _get_current_dispatch_mode\n \n+log = logging.getLogger(__name__)\n+\n \n @exposed_in(\"torch\")\n def cond(pred, true_fn, false_fn, operands):\n@@ -107,6 +110,16 @@ def cond(pred, true_fn, false_fn, operands):\n     if torch.compiler.is_dynamo_compiling():\n         return cond_op(pred, true_fn, false_fn, operands)\n \n+    if isinstance(pred, (bool, int, float)):\n+        log.warning(\n+            \"Pred is a Python constant. When used with torch.cond, it executes only one of the branches.\"\n+            \" If you want torch.cond to perserve two branches, please make the predicate a boolean tensor or a SymBool.\"\n+        )\n+        if pred:\n+            return true_fn(*operands)\n+        else:\n+            return false_fn(*operands)\n+\n     def _validate_input(pred, true_fn, false_fn, operands):\n         if not isinstance(pred, (bool, torch.Tensor, torch.SymBool)):\n             raise RuntimeError(f\"Expected pred to be bool or tensor, but got {pred}.\")\n@@ -200,7 +213,7 @@ def trace_cond(proxy_mode, func_overload, pred, true_fn, false_fn, operands):\n     proxy_args = pytree.tree_map(proxy_mode.tracer.unwrap_proxy, args)\n \n     out_proxy = proxy_mode.tracer.create_proxy(\n-        \"call_function\", func_overload, proxy_args, {}, name=\"conditional\"\n+        \"call_function\", func_overload, proxy_args, {}\n     )\n \n     # At this point, we're *guaranteed* that whether an output came from the"
        }
    ]
},
{
    "Id": 380,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/75d3bbaaa23a5b3febb0264df0742b6b6e4c916e",
    "date": "2023-12-13T18:44:43+00:00",
    "message": "Fix cudagraph check message (#115664)\n\nThis error message is printed when CUDAGraph trees are used with multiple device indices.\n\nHowever, the message seems to say the opposite.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115664\nApproved by: https://github.com/soulitzer",
    "label": "YES",
    "changes": [
        {
            "name": "compile_fx.py",
            "path": "torch/_inductor/compile_fx.py",
            "patches": [
                {
                    "old_start": 369,
                    "old_length": 7,
                    "new_start": 369,
                    "new_length": 7,
                    "hunk": "@@ -369,7 +369,7 @@ def compile_fx_inner(\n                     len(compiled_graph.device_idxs) == 1\n                     or not config.triton.cudagraph_trees\n                 ),\n-                \"multiple device indices without cudagraph_trees\",\n+                \"multiple device indices with cudagraph_trees\",\n             ),\n         ]\n         cudagraph_fail_reasons = [s for b, s in cudagraph_tests if not b]"
                }
            ],
            "whole_deleted": "-                \"multiple device indices without cudagraph_trees\",\n",
            "whole_added": "+                \"multiple device indices with cudagraph_trees\",\n",
            "whole_hunk": "@@ -369,7 +369,7 @@ def compile_fx_inner(\n                     len(compiled_graph.device_idxs) == 1\n                     or not config.triton.cudagraph_trees\n                 ),\n-                \"multiple device indices without cudagraph_trees\",\n+                \"multiple device indices with cudagraph_trees\",\n             ),\n         ]\n         cudagraph_fail_reasons = [s for b, s in cudagraph_tests if not b]"
        }
    ]
},
{
    "Id": 56,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/fff633f087fa5fb8ac69038fadbe88ccc54e614a",
    "date": "2024-06-30T14:00:09+00:00",
    "message": "[CI] Enable AOT inductor FP32 accuracy test for CPU (#129040)\n\nThis PR enabled AOT inductor backend FP32 accuracy check for CPU in CI workflow, which could catch AOT inductor issue at early stage.\n\n**Test Time cost:**\n| Suite       \t| Precision \t| Time cost \t|\n|-------------\t|-----------\t|-----------\t|\n| Huggingface \t| FP32      \t|   1h12m   \t|\n| Timm models \t| FP32      \t|   1h32m   \t|\n|  Torchbench \t| FP32      \t|   1h40m   \t|\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/129040\nApproved by: https://github.com/chuanqi129, https://github.com/desertfire, https://github.com/malfet",
    "label": "NO",
    "changes": [
        {
            "name": "test.sh",
            "path": ".ci/pytorch/test.sh",
            "patches": [
                {
                    "old_start": 406,
                    "old_length": 7,
                    "new_start": 406,
                    "new_length": 7,
                    "hunk": "@@ -406,7 +406,7 @@ if [[ \"${TEST_CONFIG}\" == *dynamic* ]]; then\n   DYNAMO_BENCHMARK_FLAGS+=(--dynamic-shapes --dynamic-batch-only)\n fi\n \n-if [[ \"${TEST_CONFIG}\" == *cpu_inductor* ]]; then\n+if [[ \"${TEST_CONFIG}\" == *cpu_inductor* || \"${TEST_CONFIG}\" == *cpu_aot_inductor* ]]; then\n   DYNAMO_BENCHMARK_FLAGS+=(--device cpu)\n else\n   DYNAMO_BENCHMARK_FLAGS+=(--device cuda)\n"
                },
                {
                    "old_start": 531,
                    "old_length": 9,
                    "new_start": 531,
                    "new_length": 10,
                    "hunk": "@@ -531,9 +531,10 @@ test_single_dynamo_benchmark() {\n     test_perf_for_dashboard \"$suite\" \\\n       \"${DYNAMO_BENCHMARK_FLAGS[@]}\" \"$@\" \"${partition_flags[@]}\"\n   else\n-    if [[ \"${TEST_CONFIG}\" == *aot_inductor* ]]; then\n+    if [[ \"${TEST_CONFIG}\" == *aot_inductor* && \"${TEST_CONFIG}\" != *cpu_aot_inductor* ]]; then\n       # Test AOTInductor with the ABI-compatible mode on CI\n       # This can be removed once the ABI-compatible mode becomes default.\n+      # For CPU device, we perfer non ABI-compatible mode on CI when testing AOTInductor.\n       export TORCHINDUCTOR_ABI_COMPATIBLE=1\n     fi\n     python \"benchmarks/dynamo/$suite.py\" \\\n"
                },
                {
                    "old_start": 574,
                    "old_length": 7,
                    "new_start": 575,
                    "new_length": 7,
                    "hunk": "@@ -574,7 +575,7 @@ test_dynamo_benchmark() {\n   elif [[ \"${TEST_CONFIG}\" == *perf* ]]; then\n     test_single_dynamo_benchmark \"dashboard\" \"$suite\" \"$shard_id\" \"$@\"\n   else\n-    if [[ \"${TEST_CONFIG}\" == *cpu_inductor* ]]; then\n+    if [[ \"${TEST_CONFIG}\" == *cpu_inductor* || \"${TEST_CONFIG}\" == *cpu_aot_inductor* ]]; then\n       local dt=\"float32\"\n       if [[ \"${TEST_CONFIG}\" == *amp* ]]; then\n         dt=\"amp\"\n"
                },
                {
                    "old_start": 1267,
                    "old_length": 7,
                    "new_start": 1268,
                    "new_length": 7,
                    "hunk": "@@ -1267,7 +1268,7 @@ elif [[ \"${TEST_CONFIG}\" == *timm* ]]; then\n   id=$((SHARD_NUMBER-1))\n   test_dynamo_benchmark timm_models \"$id\"\n elif [[ \"${TEST_CONFIG}\" == *torchbench* ]]; then\n-  if [[ \"${TEST_CONFIG}\" == *cpu_inductor* ]]; then\n+  if [[ \"${TEST_CONFIG}\" == *cpu_inductor* || \"${TEST_CONFIG}\" == *cpu_aot_inductor* ]]; then\n     install_torchaudio cpu\n   else\n     install_torchaudio cuda\n"
                },
                {
                    "old_start": 1293,
                    "old_length": 7,
                    "new_start": 1294,
                    "new_length": 7,
                    "hunk": "@@ -1293,7 +1294,7 @@ elif [[ \"${TEST_CONFIG}\" == *torchbench* ]]; then\n     checkout_install_torchbench\n     # Do this after checkout_install_torchbench to ensure we clobber any\n     # nightlies that torchbench may pull in\n-    if [[ \"${TEST_CONFIG}\" != *cpu_inductor* ]]; then\n+    if [[ \"${TEST_CONFIG}\" != *cpu_inductor* && \"${TEST_CONFIG}\" != *cpu_aot_inductor* ]]; then\n       install_torchrec_and_fbgemm\n     fi\n     PYTHONPATH=$(pwd)/torchbench test_dynamo_benchmark torchbench \"$id\"\n"
                }
            ],
            "whole_deleted": "-if [[ \"${TEST_CONFIG}\" == *cpu_inductor* ]]; then\n-    if [[ \"${TEST_CONFIG}\" == *aot_inductor* ]]; then\n-    if [[ \"${TEST_CONFIG}\" == *cpu_inductor* ]]; then\n-  if [[ \"${TEST_CONFIG}\" == *cpu_inductor* ]]; then\n-    if [[ \"${TEST_CONFIG}\" != *cpu_inductor* ]]; then\n",
            "whole_added": "+if [[ \"${TEST_CONFIG}\" == *cpu_inductor* || \"${TEST_CONFIG}\" == *cpu_aot_inductor* ]]; then\n+    if [[ \"${TEST_CONFIG}\" == *aot_inductor* && \"${TEST_CONFIG}\" != *cpu_aot_inductor* ]]; then\n+      # For CPU device, we perfer non ABI-compatible mode on CI when testing AOTInductor.\n+    if [[ \"${TEST_CONFIG}\" == *cpu_inductor* || \"${TEST_CONFIG}\" == *cpu_aot_inductor* ]]; then\n+  if [[ \"${TEST_CONFIG}\" == *cpu_inductor* || \"${TEST_CONFIG}\" == *cpu_aot_inductor* ]]; then\n+    if [[ \"${TEST_CONFIG}\" != *cpu_inductor* && \"${TEST_CONFIG}\" != *cpu_aot_inductor* ]]; then\n",
            "whole_hunk": "@@ -406,7 +406,7 @@ if [[ \"${TEST_CONFIG}\" == *dynamic* ]]; then\n   DYNAMO_BENCHMARK_FLAGS+=(--dynamic-shapes --dynamic-batch-only)\n fi\n \n-if [[ \"${TEST_CONFIG}\" == *cpu_inductor* ]]; then\n+if [[ \"${TEST_CONFIG}\" == *cpu_inductor* || \"${TEST_CONFIG}\" == *cpu_aot_inductor* ]]; then\n   DYNAMO_BENCHMARK_FLAGS+=(--device cpu)\n else\n   DYNAMO_BENCHMARK_FLAGS+=(--device cuda)\n@@ -531,9 +531,10 @@ test_single_dynamo_benchmark() {\n     test_perf_for_dashboard \"$suite\" \\\n       \"${DYNAMO_BENCHMARK_FLAGS[@]}\" \"$@\" \"${partition_flags[@]}\"\n   else\n-    if [[ \"${TEST_CONFIG}\" == *aot_inductor* ]]; then\n+    if [[ \"${TEST_CONFIG}\" == *aot_inductor* && \"${TEST_CONFIG}\" != *cpu_aot_inductor* ]]; then\n       # Test AOTInductor with the ABI-compatible mode on CI\n       # This can be removed once the ABI-compatible mode becomes default.\n+      # For CPU device, we perfer non ABI-compatible mode on CI when testing AOTInductor.\n       export TORCHINDUCTOR_ABI_COMPATIBLE=1\n     fi\n     python \"benchmarks/dynamo/$suite.py\" \\\n@@ -574,7 +575,7 @@ test_dynamo_benchmark() {\n   elif [[ \"${TEST_CONFIG}\" == *perf* ]]; then\n     test_single_dynamo_benchmark \"dashboard\" \"$suite\" \"$shard_id\" \"$@\"\n   else\n-    if [[ \"${TEST_CONFIG}\" == *cpu_inductor* ]]; then\n+    if [[ \"${TEST_CONFIG}\" == *cpu_inductor* || \"${TEST_CONFIG}\" == *cpu_aot_inductor* ]]; then\n       local dt=\"float32\"\n       if [[ \"${TEST_CONFIG}\" == *amp* ]]; then\n         dt=\"amp\"\n@@ -1267,7 +1268,7 @@ elif [[ \"${TEST_CONFIG}\" == *timm* ]]; then\n   id=$((SHARD_NUMBER-1))\n   test_dynamo_benchmark timm_models \"$id\"\n elif [[ \"${TEST_CONFIG}\" == *torchbench* ]]; then\n-  if [[ \"${TEST_CONFIG}\" == *cpu_inductor* ]]; then\n+  if [[ \"${TEST_CONFIG}\" == *cpu_inductor* || \"${TEST_CONFIG}\" == *cpu_aot_inductor* ]]; then\n     install_torchaudio cpu\n   else\n     install_torchaudio cuda\n@@ -1293,7 +1294,7 @@ elif [[ \"${TEST_CONFIG}\" == *torchbench* ]]; then\n     checkout_install_torchbench\n     # Do this after checkout_install_torchbench to ensure we clobber any\n     # nightlies that torchbench may pull in\n-    if [[ \"${TEST_CONFIG}\" != *cpu_inductor* ]]; then\n+    if [[ \"${TEST_CONFIG}\" != *cpu_inductor* && \"${TEST_CONFIG}\" != *cpu_aot_inductor* ]]; then\n       install_torchrec_and_fbgemm\n     fi\n     PYTHONPATH=$(pwd)/torchbench test_dynamo_benchmark torchbench \"$id\"\n"
        },
        {
            "name": "inductor.yml",
            "path": ".github/workflows/inductor.yml",
            "patches": [
                {
                    "old_start": 182,
                    "old_length": 6,
                    "new_start": 182,
                    "new_length": 11,
                    "hunk": "@@ -182,6 +182,11 @@ jobs:\n           { config: \"dynamic_cpu_inductor_timm\", shard: 2, num_shards: 2, runner: \"linux.12xlarge\" },\n           { config: \"dynamic_cpu_inductor_torchbench\", shard: 1, num_shards: 2, runner: \"linux.12xlarge\" },\n           { config: \"dynamic_cpu_inductor_torchbench\", shard: 2, num_shards: 2, runner: \"linux.12xlarge\" },\n+          { config: \"cpu_aot_inductor_huggingface_freezing\", shard: 1, num_shards: 1, runner: \"linux.12xlarge\" },\n+          { config: \"cpu_aot_inductor_timm_freezing\", shard: 1, num_shards: 2, runner: \"linux.12xlarge\" },\n+          { config: \"cpu_aot_inductor_timm_freezing\", shard: 2, num_shards: 2, runner: \"linux.12xlarge\" },\n+          { config: \"cpu_aot_inductor_torchbench_freezing\", shard: 1, num_shards: 2, runner: \"linux.12xlarge\" },\n+          { config: \"cpu_aot_inductor_torchbench_freezing\", shard: 2, num_shards: 2, runner: \"linux.12xlarge\" },\n           { config: \"inductor_torchbench_cpu_smoketest_perf\", shard: 1, num_shards: 1, runner: \"linux.24xl.spr-metal\" },\n         ]}\n     secrets:\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+          { config: \"cpu_aot_inductor_huggingface_freezing\", shard: 1, num_shards: 1, runner: \"linux.12xlarge\" },\n+          { config: \"cpu_aot_inductor_timm_freezing\", shard: 1, num_shards: 2, runner: \"linux.12xlarge\" },\n+          { config: \"cpu_aot_inductor_timm_freezing\", shard: 2, num_shards: 2, runner: \"linux.12xlarge\" },\n+          { config: \"cpu_aot_inductor_torchbench_freezing\", shard: 1, num_shards: 2, runner: \"linux.12xlarge\" },\n+          { config: \"cpu_aot_inductor_torchbench_freezing\", shard: 2, num_shards: 2, runner: \"linux.12xlarge\" },\n",
            "whole_hunk": "@@ -182,6 +182,11 @@ jobs:\n           { config: \"dynamic_cpu_inductor_timm\", shard: 2, num_shards: 2, runner: \"linux.12xlarge\" },\n           { config: \"dynamic_cpu_inductor_torchbench\", shard: 1, num_shards: 2, runner: \"linux.12xlarge\" },\n           { config: \"dynamic_cpu_inductor_torchbench\", shard: 2, num_shards: 2, runner: \"linux.12xlarge\" },\n+          { config: \"cpu_aot_inductor_huggingface_freezing\", shard: 1, num_shards: 1, runner: \"linux.12xlarge\" },\n+          { config: \"cpu_aot_inductor_timm_freezing\", shard: 1, num_shards: 2, runner: \"linux.12xlarge\" },\n+          { config: \"cpu_aot_inductor_timm_freezing\", shard: 2, num_shards: 2, runner: \"linux.12xlarge\" },\n+          { config: \"cpu_aot_inductor_torchbench_freezing\", shard: 1, num_shards: 2, runner: \"linux.12xlarge\" },\n+          { config: \"cpu_aot_inductor_torchbench_freezing\", shard: 2, num_shards: 2, runner: \"linux.12xlarge\" },\n           { config: \"inductor_torchbench_cpu_smoketest_perf\", shard: 1, num_shards: 1, runner: \"linux.24xl.spr-metal\" },\n         ]}\n     secrets:\n"
        },
        {
            "name": "cpu_aot_inductor_huggingface_freezing_inference.csv",
            "path": "benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_huggingface_freezing_inference.csv",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 185,
                    "hunk": "@@ -0,0 +1,185 @@\n+name,accuracy,graph_breaks\n+\n+\n+\n+AlbertForMaskedLM,pass,0\n+\n+\n+\n+AlbertForQuestionAnswering,pass,0\n+\n+\n+\n+AllenaiLongformerBase,fail_to_run,0\n+\n+\n+\n+BartForCausalLM,pass,0\n+\n+\n+\n+BartForConditionalGeneration,pass,0\n+\n+\n+\n+BertForMaskedLM,pass,0\n+\n+\n+\n+BertForQuestionAnswering,pass,0\n+\n+\n+\n+BlenderbotForCausalLM,pass_due_to_skip,0\n+\n+\n+\n+BlenderbotSmallForCausalLM,pass,0\n+\n+\n+\n+BlenderbotSmallForConditionalGeneration,pass,0\n+\n+\n+\n+CamemBert,pass,0\n+\n+\n+\n+DebertaForMaskedLM,pass,0\n+\n+\n+\n+DebertaForQuestionAnswering,pass,0\n+\n+\n+\n+DebertaV2ForMaskedLM,pass_due_to_skip,0\n+\n+\n+\n+DebertaV2ForQuestionAnswering,pass,0\n+\n+\n+\n+DistilBertForMaskedLM,pass,0\n+\n+\n+\n+DistilBertForQuestionAnswering,pass,0\n+\n+\n+\n+DistillGPT2,pass,0\n+\n+\n+\n+ElectraForCausalLM,pass,0\n+\n+\n+\n+ElectraForQuestionAnswering,pass,0\n+\n+\n+\n+GPT2ForSequenceClassification,pass,0\n+\n+\n+\n+GoogleFnet,pass,0\n+\n+\n+\n+LayoutLMForMaskedLM,pass,0\n+\n+\n+\n+LayoutLMForSequenceClassification,pass,0\n+\n+\n+\n+M2M100ForConditionalGeneration,pass,0\n+\n+\n+\n+MBartForCausalLM,pass,0\n+\n+\n+\n+MBartForConditionalGeneration,pass,0\n+\n+\n+\n+MT5ForConditionalGeneration,pass,0\n+\n+\n+\n+MegatronBertForCausalLM,pass,0\n+\n+\n+\n+MegatronBertForQuestionAnswering,pass,0\n+\n+\n+\n+MobileBertForMaskedLM,pass,0\n+\n+\n+\n+MobileBertForQuestionAnswering,pass,0\n+\n+\n+\n+OPTForCausalLM,pass,0\n+\n+\n+\n+PLBartForCausalLM,pass,0\n+\n+\n+\n+PLBartForConditionalGeneration,pass,0\n+\n+\n+\n+PegasusForCausalLM,pass,0\n+\n+\n+\n+PegasusForConditionalGeneration,pass,0\n+\n+\n+\n+RobertaForCausalLM,pass,0\n+\n+\n+\n+RobertaForQuestionAnswering,pass,0\n+\n+\n+\n+Speech2Text2ForCausalLM,pass,0\n+\n+\n+\n+T5ForConditionalGeneration,pass,0\n+\n+\n+\n+T5Small,pass,0\n+\n+\n+\n+TrOCRForCausalLM,pass,0\n+\n+\n+\n+XGLMForCausalLM,pass,0\n+\n+\n+\n+XLNetLMHeadModel,pass,0\n+\n+\n+\n+YituTechConvBert,pass,0\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+name,accuracy,graph_breaks\n+\n+\n+\n+AlbertForMaskedLM,pass,0\n+\n+\n+\n+AlbertForQuestionAnswering,pass,0\n+\n+\n+\n+AllenaiLongformerBase,fail_to_run,0\n+\n+\n+\n+BartForCausalLM,pass,0\n+\n+\n+\n+BartForConditionalGeneration,pass,0\n+\n+\n+\n+BertForMaskedLM,pass,0\n+\n+\n+\n+BertForQuestionAnswering,pass,0\n+\n+\n+\n+BlenderbotForCausalLM,pass_due_to_skip,0\n+\n+\n+\n+BlenderbotSmallForCausalLM,pass,0\n+\n+\n+\n+BlenderbotSmallForConditionalGeneration,pass,0\n+\n+\n+\n+CamemBert,pass,0\n+\n+\n+\n+DebertaForMaskedLM,pass,0\n+\n+\n+\n+DebertaForQuestionAnswering,pass,0\n+\n+\n+\n+DebertaV2ForMaskedLM,pass_due_to_skip,0\n+\n+\n+\n+DebertaV2ForQuestionAnswering,pass,0\n+\n+\n+\n+DistilBertForMaskedLM,pass,0\n+\n+\n+\n+DistilBertForQuestionAnswering,pass,0\n+\n+\n+\n+DistillGPT2,pass,0\n+\n+\n+\n+ElectraForCausalLM,pass,0\n+\n+\n+\n+ElectraForQuestionAnswering,pass,0\n+\n+\n+\n+GPT2ForSequenceClassification,pass,0\n+\n+\n+\n+GoogleFnet,pass,0\n+\n+\n+\n+LayoutLMForMaskedLM,pass,0\n+\n+\n+\n+LayoutLMForSequenceClassification,pass,0\n+\n+\n+\n+M2M100ForConditionalGeneration,pass,0\n+\n+\n+\n+MBartForCausalLM,pass,0\n+\n+\n+\n+MBartForConditionalGeneration,pass,0\n+\n+\n+\n+MT5ForConditionalGeneration,pass,0\n+\n+\n+\n+MegatronBertForCausalLM,pass,0\n+\n+\n+\n+MegatronBertForQuestionAnswering,pass,0\n+\n+\n+\n+MobileBertForMaskedLM,pass,0\n+\n+\n+\n+MobileBertForQuestionAnswering,pass,0\n+\n+\n+\n+OPTForCausalLM,pass,0\n+\n+\n+\n+PLBartForCausalLM,pass,0\n+\n+\n+\n+PLBartForConditionalGeneration,pass,0\n+\n+\n+\n+PegasusForCausalLM,pass,0\n+\n+\n+\n+PegasusForConditionalGeneration,pass,0\n+\n+\n+\n+RobertaForCausalLM,pass,0\n+\n+\n+\n+RobertaForQuestionAnswering,pass,0\n+\n+\n+\n+Speech2Text2ForCausalLM,pass,0\n+\n+\n+\n+T5ForConditionalGeneration,pass,0\n+\n+\n+\n+T5Small,pass,0\n+\n+\n+\n+TrOCRForCausalLM,pass,0\n+\n+\n+\n+XGLMForCausalLM,pass,0\n+\n+\n+\n+XLNetLMHeadModel,pass,0\n+\n+\n+\n+YituTechConvBert,pass,0\n",
            "whole_hunk": "@@ -0,0 +1,185 @@\n+name,accuracy,graph_breaks\n+\n+\n+\n+AlbertForMaskedLM,pass,0\n+\n+\n+\n+AlbertForQuestionAnswering,pass,0\n+\n+\n+\n+AllenaiLongformerBase,fail_to_run,0\n+\n+\n+\n+BartForCausalLM,pass,0\n+\n+\n+\n+BartForConditionalGeneration,pass,0\n+\n+\n+\n+BertForMaskedLM,pass,0\n+\n+\n+\n+BertForQuestionAnswering,pass,0\n+\n+\n+\n+BlenderbotForCausalLM,pass_due_to_skip,0\n+\n+\n+\n+BlenderbotSmallForCausalLM,pass,0\n+\n+\n+\n+BlenderbotSmallForConditionalGeneration,pass,0\n+\n+\n+\n+CamemBert,pass,0\n+\n+\n+\n+DebertaForMaskedLM,pass,0\n+\n+\n+\n+DebertaForQuestionAnswering,pass,0\n+\n+\n+\n+DebertaV2ForMaskedLM,pass_due_to_skip,0\n+\n+\n+\n+DebertaV2ForQuestionAnswering,pass,0\n+\n+\n+\n+DistilBertForMaskedLM,pass,0\n+\n+\n+\n+DistilBertForQuestionAnswering,pass,0\n+\n+\n+\n+DistillGPT2,pass,0\n+\n+\n+\n+ElectraForCausalLM,pass,0\n+\n+\n+\n+ElectraForQuestionAnswering,pass,0\n+\n+\n+\n+GPT2ForSequenceClassification,pass,0\n+\n+\n+\n+GoogleFnet,pass,0\n+\n+\n+\n+LayoutLMForMaskedLM,pass,0\n+\n+\n+\n+LayoutLMForSequenceClassification,pass,0\n+\n+\n+\n+M2M100ForConditionalGeneration,pass,0\n+\n+\n+\n+MBartForCausalLM,pass,0\n+\n+\n+\n+MBartForConditionalGeneration,pass,0\n+\n+\n+\n+MT5ForConditionalGeneration,pass,0\n+\n+\n+\n+MegatronBertForCausalLM,pass,0\n+\n+\n+\n+MegatronBertForQuestionAnswering,pass,0\n+\n+\n+\n+MobileBertForMaskedLM,pass,0\n+\n+\n+\n+MobileBertForQuestionAnswering,pass,0\n+\n+\n+\n+OPTForCausalLM,pass,0\n+\n+\n+\n+PLBartForCausalLM,pass,0\n+\n+\n+\n+PLBartForConditionalGeneration,pass,0\n+\n+\n+\n+PegasusForCausalLM,pass,0\n+\n+\n+\n+PegasusForConditionalGeneration,pass,0\n+\n+\n+\n+RobertaForCausalLM,pass,0\n+\n+\n+\n+RobertaForQuestionAnswering,pass,0\n+\n+\n+\n+Speech2Text2ForCausalLM,pass,0\n+\n+\n+\n+T5ForConditionalGeneration,pass,0\n+\n+\n+\n+T5Small,pass,0\n+\n+\n+\n+TrOCRForCausalLM,pass,0\n+\n+\n+\n+XGLMForCausalLM,pass,0\n+\n+\n+\n+XLNetLMHeadModel,pass,0\n+\n+\n+\n+YituTechConvBert,pass,0\n"
        },
        {
            "name": "cpu_aot_inductor_timm_freezing_inference.csv",
            "path": "benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_timm_freezing_inference.csv",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 245,
                    "hunk": "@@ -0,0 +1,245 @@\n+name,accuracy,graph_breaks\n+\n+\n+\n+adv_inception_v3,pass,0\n+\n+\n+\n+beit_base_patch16_224,pass,0\n+\n+\n+\n+botnet26t_256,pass,0\n+\n+\n+\n+cait_m36_384,pass,0\n+\n+\n+\n+coat_lite_mini,pass,0\n+\n+\n+\n+convit_base,fail_to_run,0\n+\n+\n+\n+convmixer_768_32,pass,0\n+\n+\n+\n+convnext_base,pass,0\n+\n+\n+\n+crossvit_9_240,pass,0\n+\n+\n+\n+cspdarknet53,pass,0\n+\n+\n+\n+deit_base_distilled_patch16_224,fail_to_run,0\n+\n+\n+\n+dla102,pass,0\n+\n+\n+\n+dm_nfnet_f0,pass,0\n+\n+\n+\n+dpn107,pass,0\n+\n+\n+\n+eca_botnext26ts_256,pass,0\n+\n+\n+\n+eca_halonext26ts,pass,0\n+\n+\n+\n+ese_vovnet19b_dw,pass,0\n+\n+\n+\n+fbnetc_100,pass,0\n+\n+\n+\n+fbnetv3_b,pass,0\n+\n+\n+\n+gernet_l,pass,0\n+\n+\n+\n+ghostnet_100,pass,0\n+\n+\n+\n+gluon_inception_v3,pass,0\n+\n+\n+\n+gmixer_24_224,pass,0\n+\n+\n+\n+gmlp_s16_224,pass,0\n+\n+\n+\n+hrnet_w18,pass,0\n+\n+\n+\n+inception_v3,pass,0\n+\n+\n+\n+jx_nest_base,pass,0\n+\n+\n+\n+lcnet_050,pass,0\n+\n+\n+\n+levit_128,fail_to_run,0\n+\n+\n+\n+mixer_b16_224,pass,0\n+\n+\n+\n+mixnet_l,pass,0\n+\n+\n+\n+mnasnet_100,pass,0\n+\n+\n+\n+mobilenetv2_100,pass,0\n+\n+\n+\n+mobilenetv3_large_100,pass,0\n+\n+\n+\n+mobilevit_s,pass,0\n+\n+\n+\n+nfnet_l0,pass,0\n+\n+\n+\n+pit_b_224,pass,0\n+\n+\n+\n+pnasnet5large,pass,0\n+\n+\n+\n+poolformer_m36,pass,0\n+\n+\n+\n+regnety_002,pass,0\n+\n+\n+\n+repvgg_a2,pass,0\n+\n+\n+\n+res2net101_26w_4s,pass,0\n+\n+\n+\n+res2net50_14w_8s,pass,0\n+\n+\n+\n+res2next50,pass,0\n+\n+\n+\n+resmlp_12_224,pass,0\n+\n+\n+\n+resnest101e,pass,0\n+\n+\n+\n+rexnet_100,pass,0\n+\n+\n+\n+sebotnet33ts_256,pass,0\n+\n+\n+\n+selecsls42b,pass,0\n+\n+\n+\n+spnasnet_100,pass,0\n+\n+\n+\n+swin_base_patch4_window7_224,pass,0\n+\n+\n+\n+swsl_resnext101_32x16d,pass,0\n+\n+\n+\n+tf_efficientnet_b0,pass,0\n+\n+\n+\n+tf_mixnet_l,pass,0\n+\n+\n+\n+tinynet_a,pass,0\n+\n+\n+\n+tnt_s_patch16_224,pass,0\n+\n+\n+\n+twins_pcpvt_base,pass,0\n+\n+\n+\n+visformer_small,pass,0\n+\n+\n+\n+vit_base_patch16_224,pass,0\n+\n+\n+\n+volo_d1_224,fail_to_run,0\n+\n+\n+\n+xcit_large_24_p8_224,pass,0\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+name,accuracy,graph_breaks\n+\n+\n+\n+adv_inception_v3,pass,0\n+\n+\n+\n+beit_base_patch16_224,pass,0\n+\n+\n+\n+botnet26t_256,pass,0\n+\n+\n+\n+cait_m36_384,pass,0\n+\n+\n+\n+coat_lite_mini,pass,0\n+\n+\n+\n+convit_base,fail_to_run,0\n+\n+\n+\n+convmixer_768_32,pass,0\n+\n+\n+\n+convnext_base,pass,0\n+\n+\n+\n+crossvit_9_240,pass,0\n+\n+\n+\n+cspdarknet53,pass,0\n+\n+\n+\n+deit_base_distilled_patch16_224,fail_to_run,0\n+\n+\n+\n+dla102,pass,0\n+\n+\n+\n+dm_nfnet_f0,pass,0\n+\n+\n+\n+dpn107,pass,0\n+\n+\n+\n+eca_botnext26ts_256,pass,0\n+\n+\n+\n+eca_halonext26ts,pass,0\n+\n+\n+\n+ese_vovnet19b_dw,pass,0\n+\n+\n+\n+fbnetc_100,pass,0\n+\n+\n+\n+fbnetv3_b,pass,0\n+\n+\n+\n+gernet_l,pass,0\n+\n+\n+\n+ghostnet_100,pass,0\n+\n+\n+\n+gluon_inception_v3,pass,0\n+\n+\n+\n+gmixer_24_224,pass,0\n+\n+\n+\n+gmlp_s16_224,pass,0\n+\n+\n+\n+hrnet_w18,pass,0\n+\n+\n+\n+inception_v3,pass,0\n+\n+\n+\n+jx_nest_base,pass,0\n+\n+\n+\n+lcnet_050,pass,0\n+\n+\n+\n+levit_128,fail_to_run,0\n+\n+\n+\n+mixer_b16_224,pass,0\n+\n+\n+\n+mixnet_l,pass,0\n+\n+\n+\n+mnasnet_100,pass,0\n+\n+\n+\n+mobilenetv2_100,pass,0\n+\n+\n+\n+mobilenetv3_large_100,pass,0\n+\n+\n+\n+mobilevit_s,pass,0\n+\n+\n+\n+nfnet_l0,pass,0\n+\n+\n+\n+pit_b_224,pass,0\n+\n+\n+\n+pnasnet5large,pass,0\n+\n+\n+\n+poolformer_m36,pass,0\n+\n+\n+\n+regnety_002,pass,0\n+\n+\n+\n+repvgg_a2,pass,0\n+\n+\n+\n+res2net101_26w_4s,pass,0\n+\n+\n+\n+res2net50_14w_8s,pass,0\n+\n+\n+\n+res2next50,pass,0\n+\n+\n+\n+resmlp_12_224,pass,0\n+\n+\n+\n+resnest101e,pass,0\n+\n+\n+\n+rexnet_100,pass,0\n+\n+\n+\n+sebotnet33ts_256,pass,0\n+\n+\n+\n+selecsls42b,pass,0\n+\n+\n+\n+spnasnet_100,pass,0\n+\n+\n+\n+swin_base_patch4_window7_224,pass,0\n+\n+\n+\n+swsl_resnext101_32x16d,pass,0\n+\n+\n+\n+tf_efficientnet_b0,pass,0\n+\n+\n+\n+tf_mixnet_l,pass,0\n+\n+\n+\n+tinynet_a,pass,0\n+\n+\n+\n+tnt_s_patch16_224,pass,0\n+\n+\n+\n+twins_pcpvt_base,pass,0\n+\n+\n+\n+visformer_small,pass,0\n+\n+\n+\n+vit_base_patch16_224,pass,0\n+\n+\n+\n+volo_d1_224,fail_to_run,0\n+\n+\n+\n+xcit_large_24_p8_224,pass,0\n",
            "whole_hunk": "@@ -0,0 +1,245 @@\n+name,accuracy,graph_breaks\n+\n+\n+\n+adv_inception_v3,pass,0\n+\n+\n+\n+beit_base_patch16_224,pass,0\n+\n+\n+\n+botnet26t_256,pass,0\n+\n+\n+\n+cait_m36_384,pass,0\n+\n+\n+\n+coat_lite_mini,pass,0\n+\n+\n+\n+convit_base,fail_to_run,0\n+\n+\n+\n+convmixer_768_32,pass,0\n+\n+\n+\n+convnext_base,pass,0\n+\n+\n+\n+crossvit_9_240,pass,0\n+\n+\n+\n+cspdarknet53,pass,0\n+\n+\n+\n+deit_base_distilled_patch16_224,fail_to_run,0\n+\n+\n+\n+dla102,pass,0\n+\n+\n+\n+dm_nfnet_f0,pass,0\n+\n+\n+\n+dpn107,pass,0\n+\n+\n+\n+eca_botnext26ts_256,pass,0\n+\n+\n+\n+eca_halonext26ts,pass,0\n+\n+\n+\n+ese_vovnet19b_dw,pass,0\n+\n+\n+\n+fbnetc_100,pass,0\n+\n+\n+\n+fbnetv3_b,pass,0\n+\n+\n+\n+gernet_l,pass,0\n+\n+\n+\n+ghostnet_100,pass,0\n+\n+\n+\n+gluon_inception_v3,pass,0\n+\n+\n+\n+gmixer_24_224,pass,0\n+\n+\n+\n+gmlp_s16_224,pass,0\n+\n+\n+\n+hrnet_w18,pass,0\n+\n+\n+\n+inception_v3,pass,0\n+\n+\n+\n+jx_nest_base,pass,0\n+\n+\n+\n+lcnet_050,pass,0\n+\n+\n+\n+levit_128,fail_to_run,0\n+\n+\n+\n+mixer_b16_224,pass,0\n+\n+\n+\n+mixnet_l,pass,0\n+\n+\n+\n+mnasnet_100,pass,0\n+\n+\n+\n+mobilenetv2_100,pass,0\n+\n+\n+\n+mobilenetv3_large_100,pass,0\n+\n+\n+\n+mobilevit_s,pass,0\n+\n+\n+\n+nfnet_l0,pass,0\n+\n+\n+\n+pit_b_224,pass,0\n+\n+\n+\n+pnasnet5large,pass,0\n+\n+\n+\n+poolformer_m36,pass,0\n+\n+\n+\n+regnety_002,pass,0\n+\n+\n+\n+repvgg_a2,pass,0\n+\n+\n+\n+res2net101_26w_4s,pass,0\n+\n+\n+\n+res2net50_14w_8s,pass,0\n+\n+\n+\n+res2next50,pass,0\n+\n+\n+\n+resmlp_12_224,pass,0\n+\n+\n+\n+resnest101e,pass,0\n+\n+\n+\n+rexnet_100,pass,0\n+\n+\n+\n+sebotnet33ts_256,pass,0\n+\n+\n+\n+selecsls42b,pass,0\n+\n+\n+\n+spnasnet_100,pass,0\n+\n+\n+\n+swin_base_patch4_window7_224,pass,0\n+\n+\n+\n+swsl_resnext101_32x16d,pass,0\n+\n+\n+\n+tf_efficientnet_b0,pass,0\n+\n+\n+\n+tf_mixnet_l,pass,0\n+\n+\n+\n+tinynet_a,pass,0\n+\n+\n+\n+tnt_s_patch16_224,pass,0\n+\n+\n+\n+twins_pcpvt_base,pass,0\n+\n+\n+\n+visformer_small,pass,0\n+\n+\n+\n+vit_base_patch16_224,pass,0\n+\n+\n+\n+volo_d1_224,fail_to_run,0\n+\n+\n+\n+xcit_large_24_p8_224,pass,0\n"
        },
        {
            "name": "cpu_aot_inductor_torchbench_freezing_inference.csv",
            "path": "benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_torchbench_freezing_inference.csv",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 361,
                    "hunk": "@@ -0,0 +1,361 @@\n+name,accuracy,graph_breaks\n+\n+\n+\n+torchrec_dlrm,eager_fail_to_run,0\n+\n+\n+\n+BERT_pytorch,fail_to_run,0\n+\n+\n+\n+Background_Matting,pass_due_to_skip,0\n+\n+\n+\n+DALLE2_pytorch,fail_to_run,0\n+\n+\n+\n+LearningToPaint,pass,0\n+\n+\n+\n+Super_SloMo,pass,0\n+\n+\n+\n+alexnet,pass,0\n+\n+\n+\n+basic_gnn_edgecnn,pass,0\n+\n+\n+\n+basic_gnn_gcn,pass,0\n+\n+\n+\n+basic_gnn_gin,pass,0\n+\n+\n+\n+basic_gnn_sage,fail_to_run,0\n+\n+\n+\n+dcgan,pass,0\n+\n+\n+\n+demucs,pass,0\n+\n+\n+\n+densenet121,pass,0\n+\n+\n+\n+detectron2_fasterrcnn_r_101_c4,fail_to_run,0\n+\n+\n+\n+detectron2_fasterrcnn_r_101_dc5,fail_to_run,0\n+\n+\n+\n+detectron2_fasterrcnn_r_101_fpn,fail_to_run,0\n+\n+\n+\n+detectron2_fasterrcnn_r_50_c4,fail_to_run,0\n+\n+\n+\n+detectron2_fasterrcnn_r_50_dc5,fail_to_run,0\n+\n+\n+\n+detectron2_fasterrcnn_r_50_fpn,fail_to_run,0\n+\n+\n+\n+detectron2_maskrcnn_r_101_c4,fail_to_run,0\n+\n+\n+\n+detectron2_maskrcnn_r_101_fpn,fail_to_run,0\n+\n+\n+\n+detectron2_maskrcnn_r_50_c4,fail_to_run,0\n+\n+\n+\n+detectron2_maskrcnn_r_50_fpn,fail_to_run,0\n+\n+\n+\n+dlrm,fail_to_run,0\n+\n+\n+\n+doctr_det_predictor,fail_to_run,0\n+\n+\n+\n+doctr_reco_predictor,fail_to_run,0\n+\n+\n+\n+drq,fail_to_run,0\n+\n+\n+\n+functorch_dp_cifar10,pass,0\n+\n+\n+\n+functorch_maml_omniglot,pass,0\n+\n+\n+\n+hf_Albert,pass,0\n+\n+\n+\n+hf_Bart,pass,0\n+\n+\n+\n+hf_Bert,pass,0\n+\n+\n+\n+hf_Bert_large,pass,0\n+\n+\n+\n+hf_BigBird,fail_to_run,0\n+\n+\n+\n+hf_DistilBert,pass,0\n+\n+\n+\n+hf_GPT2,pass,0\n+\n+\n+\n+hf_GPT2_large,pass_due_to_skip,0\n+\n+\n+\n+hf_T5,pass,0\n+\n+\n+\n+hf_T5_base,pass,0\n+\n+\n+\n+hf_T5_large,pass_due_to_skip,0\n+\n+\n+\n+hf_Whisper,pass,0\n+\n+\n+\n+hf_distil_whisper,pass,0\n+\n+\n+\n+lennard_jones,pass,0\n+\n+\n+\n+llama,fail_to_run,0\n+\n+\n+\n+llama_v2_7b_16h,model_fail_to_load,0\n+\n+\n+\n+llava,model_fail_to_load,0\n+\n+\n+\n+maml,pass_due_to_skip,0\n+\n+\n+\n+maml_omniglot,pass,0\n+\n+\n+\n+mnasnet1_0,pass,0\n+\n+\n+\n+mobilenet_v2,pass,0\n+\n+\n+\n+mobilenet_v2_quantized_qat,fail_to_run,0\n+\n+\n+\n+mobilenet_v3_large,pass,0\n+\n+\n+\n+moco,fail_to_run,0\n+\n+\n+\n+moondream,pass,0\n+\n+\n+\n+nanogpt,pass,0\n+\n+\n+\n+nvidia_deeprecommender,pass,0\n+\n+\n+\n+phlippe_densenet,pass,0\n+\n+\n+\n+phlippe_resnet,pass,0\n+\n+\n+\n+pyhpc_equation_of_state,pass,0\n+\n+\n+\n+pyhpc_isoneutral_mixing,pass,0\n+\n+\n+\n+pyhpc_turbulent_kinetic_energy,pass,0\n+\n+\n+\n+pytorch_CycleGAN_and_pix2pix,pass,0\n+\n+\n+\n+pytorch_stargan,pass,0\n+\n+\n+\n+pytorch_unet,pass,0\n+\n+\n+\n+resnet152,pass,0\n+\n+\n+\n+resnet18,pass,0\n+\n+\n+\n+resnet50,pass,0\n+\n+\n+\n+resnet50_quantized_qat,fail_to_run,0\n+\n+\n+\n+resnext50_32x4d,pass,0\n+\n+\n+\n+sam,fail_to_run,0\n+\n+\n+\n+sam_fast,fail_to_run,0\n+\n+\n+\n+shufflenet_v2_x1_0,pass,0\n+\n+\n+\n+soft_actor_critic,fail_to_run,0\n+\n+\n+\n+squeezenet1_1,pass,0\n+\n+\n+\n+stable_diffusion_text_encoder,pass,0\n+\n+\n+\n+stable_diffusion_unet,pass_due_to_skip,0\n+\n+\n+\n+timm_efficientdet,model_fail_to_load,0\n+\n+\n+\n+timm_efficientnet,pass,0\n+\n+\n+\n+timm_nfnet,pass,0\n+\n+\n+\n+timm_regnet,pass,0\n+\n+\n+\n+timm_resnest,pass,0\n+\n+\n+\n+timm_vision_transformer,pass,0\n+\n+\n+\n+timm_vision_transformer_large,pass_due_to_skip,0\n+\n+\n+\n+timm_vovnet,pass,0\n+\n+\n+\n+torch_multimodal_clip,pass,0\n+\n+\n+\n+tts_angular,fail_to_run,0\n+\n+\n+\n+vgg16,pass,0\n+\n+\n+\n+vision_maskrcnn,fail_to_run,0\n+\n+\n+\n+yolov3,pass,0"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+name,accuracy,graph_breaks\n+\n+\n+\n+torchrec_dlrm,eager_fail_to_run,0\n+\n+\n+\n+BERT_pytorch,fail_to_run,0\n+\n+\n+\n+Background_Matting,pass_due_to_skip,0\n+\n+\n+\n+DALLE2_pytorch,fail_to_run,0\n+\n+\n+\n+LearningToPaint,pass,0\n+\n+\n+\n+Super_SloMo,pass,0\n+\n+\n+\n+alexnet,pass,0\n+\n+\n+\n+basic_gnn_edgecnn,pass,0\n+\n+\n+\n+basic_gnn_gcn,pass,0\n+\n+\n+\n+basic_gnn_gin,pass,0\n+\n+\n+\n+basic_gnn_sage,fail_to_run,0\n+\n+\n+\n+dcgan,pass,0\n+\n+\n+\n+demucs,pass,0\n+\n+\n+\n+densenet121,pass,0\n+\n+\n+\n+detectron2_fasterrcnn_r_101_c4,fail_to_run,0\n+\n+\n+\n+detectron2_fasterrcnn_r_101_dc5,fail_to_run,0\n+\n+\n+\n+detectron2_fasterrcnn_r_101_fpn,fail_to_run,0\n+\n+\n+\n+detectron2_fasterrcnn_r_50_c4,fail_to_run,0\n+\n+\n+\n+detectron2_fasterrcnn_r_50_dc5,fail_to_run,0\n+\n+\n+\n+detectron2_fasterrcnn_r_50_fpn,fail_to_run,0\n+\n+\n+\n+detectron2_maskrcnn_r_101_c4,fail_to_run,0\n+\n+\n+\n+detectron2_maskrcnn_r_101_fpn,fail_to_run,0\n+\n+\n+\n+detectron2_maskrcnn_r_50_c4,fail_to_run,0\n+\n+\n+\n+detectron2_maskrcnn_r_50_fpn,fail_to_run,0\n+\n+\n+\n+dlrm,fail_to_run,0\n+\n+\n+\n+doctr_det_predictor,fail_to_run,0\n+\n+\n+\n+doctr_reco_predictor,fail_to_run,0\n+\n+\n+\n+drq,fail_to_run,0\n+\n+\n+\n+functorch_dp_cifar10,pass,0\n+\n+\n+\n+functorch_maml_omniglot,pass,0\n+\n+\n+\n+hf_Albert,pass,0\n+\n+\n+\n+hf_Bart,pass,0\n+\n+\n+\n+hf_Bert,pass,0\n+\n+\n+\n+hf_Bert_large,pass,0\n+\n+\n+\n+hf_BigBird,fail_to_run,0\n+\n+\n+\n+hf_DistilBert,pass,0\n+\n+\n+\n+hf_GPT2,pass,0\n+\n+\n+\n+hf_GPT2_large,pass_due_to_skip,0\n+\n+\n+\n+hf_T5,pass,0\n+\n+\n+\n+hf_T5_base,pass,0\n+\n+\n+\n+hf_T5_large,pass_due_to_skip,0\n+\n+\n+\n+hf_Whisper,pass,0\n+\n+\n+\n+hf_distil_whisper,pass,0\n+\n+\n+\n+lennard_jones,pass,0\n+\n+\n+\n+llama,fail_to_run,0\n+\n+\n+\n+llama_v2_7b_16h,model_fail_to_load,0\n+\n+\n+\n+llava,model_fail_to_load,0\n+\n+\n+\n+maml,pass_due_to_skip,0\n+\n+\n+\n+maml_omniglot,pass,0\n+\n+\n+\n+mnasnet1_0,pass,0\n+\n+\n+\n+mobilenet_v2,pass,0\n+\n+\n+\n+mobilenet_v2_quantized_qat,fail_to_run,0\n+\n+\n+\n+mobilenet_v3_large,pass,0\n+\n+\n+\n+moco,fail_to_run,0\n+\n+\n+\n+moondream,pass,0\n+\n+\n+\n+nanogpt,pass,0\n+\n+\n+\n+nvidia_deeprecommender,pass,0\n+\n+\n+\n+phlippe_densenet,pass,0\n+\n+\n+\n+phlippe_resnet,pass,0\n+\n+\n+\n+pyhpc_equation_of_state,pass,0\n+\n+\n+\n+pyhpc_isoneutral_mixing,pass,0\n+\n+\n+\n+pyhpc_turbulent_kinetic_energy,pass,0\n+\n+\n+\n+pytorch_CycleGAN_and_pix2pix,pass,0\n+\n+\n+\n+pytorch_stargan,pass,0\n+\n+\n+\n+pytorch_unet,pass,0\n+\n+\n+\n+resnet152,pass,0\n+\n+\n+\n+resnet18,pass,0\n+\n+\n+\n+resnet50,pass,0\n+\n+\n+\n+resnet50_quantized_qat,fail_to_run,0\n+\n+\n+\n+resnext50_32x4d,pass,0\n+\n+\n+\n+sam,fail_to_run,0\n+\n+\n+\n+sam_fast,fail_to_run,0\n+\n+\n+\n+shufflenet_v2_x1_0,pass,0\n+\n+\n+\n+soft_actor_critic,fail_to_run,0\n+\n+\n+\n+squeezenet1_1,pass,0\n+\n+\n+\n+stable_diffusion_text_encoder,pass,0\n+\n+\n+\n+stable_diffusion_unet,pass_due_to_skip,0\n+\n+\n+\n+timm_efficientdet,model_fail_to_load,0\n+\n+\n+\n+timm_efficientnet,pass,0\n+\n+\n+\n+timm_nfnet,pass,0\n+\n+\n+\n+timm_regnet,pass,0\n+\n+\n+\n+timm_resnest,pass,0\n+\n+\n+\n+timm_vision_transformer,pass,0\n+\n+\n+\n+timm_vision_transformer_large,pass_due_to_skip,0\n+\n+\n+\n+timm_vovnet,pass,0\n+\n+\n+\n+torch_multimodal_clip,pass,0\n+\n+\n+\n+tts_angular,fail_to_run,0\n+\n+\n+\n+vgg16,pass,0\n+\n+\n+\n+vision_maskrcnn,fail_to_run,0\n+\n+\n+\n+yolov3,pass,0\n",
            "whole_hunk": "@@ -0,0 +1,361 @@\n+name,accuracy,graph_breaks\n+\n+\n+\n+torchrec_dlrm,eager_fail_to_run,0\n+\n+\n+\n+BERT_pytorch,fail_to_run,0\n+\n+\n+\n+Background_Matting,pass_due_to_skip,0\n+\n+\n+\n+DALLE2_pytorch,fail_to_run,0\n+\n+\n+\n+LearningToPaint,pass,0\n+\n+\n+\n+Super_SloMo,pass,0\n+\n+\n+\n+alexnet,pass,0\n+\n+\n+\n+basic_gnn_edgecnn,pass,0\n+\n+\n+\n+basic_gnn_gcn,pass,0\n+\n+\n+\n+basic_gnn_gin,pass,0\n+\n+\n+\n+basic_gnn_sage,fail_to_run,0\n+\n+\n+\n+dcgan,pass,0\n+\n+\n+\n+demucs,pass,0\n+\n+\n+\n+densenet121,pass,0\n+\n+\n+\n+detectron2_fasterrcnn_r_101_c4,fail_to_run,0\n+\n+\n+\n+detectron2_fasterrcnn_r_101_dc5,fail_to_run,0\n+\n+\n+\n+detectron2_fasterrcnn_r_101_fpn,fail_to_run,0\n+\n+\n+\n+detectron2_fasterrcnn_r_50_c4,fail_to_run,0\n+\n+\n+\n+detectron2_fasterrcnn_r_50_dc5,fail_to_run,0\n+\n+\n+\n+detectron2_fasterrcnn_r_50_fpn,fail_to_run,0\n+\n+\n+\n+detectron2_maskrcnn_r_101_c4,fail_to_run,0\n+\n+\n+\n+detectron2_maskrcnn_r_101_fpn,fail_to_run,0\n+\n+\n+\n+detectron2_maskrcnn_r_50_c4,fail_to_run,0\n+\n+\n+\n+detectron2_maskrcnn_r_50_fpn,fail_to_run,0\n+\n+\n+\n+dlrm,fail_to_run,0\n+\n+\n+\n+doctr_det_predictor,fail_to_run,0\n+\n+\n+\n+doctr_reco_predictor,fail_to_run,0\n+\n+\n+\n+drq,fail_to_run,0\n+\n+\n+\n+functorch_dp_cifar10,pass,0\n+\n+\n+\n+functorch_maml_omniglot,pass,0\n+\n+\n+\n+hf_Albert,pass,0\n+\n+\n+\n+hf_Bart,pass,0\n+\n+\n+\n+hf_Bert,pass,0\n+\n+\n+\n+hf_Bert_large,pass,0\n+\n+\n+\n+hf_BigBird,fail_to_run,0\n+\n+\n+\n+hf_DistilBert,pass,0\n+\n+\n+\n+hf_GPT2,pass,0\n+\n+\n+\n+hf_GPT2_large,pass_due_to_skip,0\n+\n+\n+\n+hf_T5,pass,0\n+\n+\n+\n+hf_T5_base,pass,0\n+\n+\n+\n+hf_T5_large,pass_due_to_skip,0\n+\n+\n+\n+hf_Whisper,pass,0\n+\n+\n+\n+hf_distil_whisper,pass,0\n+\n+\n+\n+lennard_jones,pass,0\n+\n+\n+\n+llama,fail_to_run,0\n+\n+\n+\n+llama_v2_7b_16h,model_fail_to_load,0\n+\n+\n+\n+llava,model_fail_to_load,0\n+\n+\n+\n+maml,pass_due_to_skip,0\n+\n+\n+\n+maml_omniglot,pass,0\n+\n+\n+\n+mnasnet1_0,pass,0\n+\n+\n+\n+mobilenet_v2,pass,0\n+\n+\n+\n+mobilenet_v2_quantized_qat,fail_to_run,0\n+\n+\n+\n+mobilenet_v3_large,pass,0\n+\n+\n+\n+moco,fail_to_run,0\n+\n+\n+\n+moondream,pass,0\n+\n+\n+\n+nanogpt,pass,0\n+\n+\n+\n+nvidia_deeprecommender,pass,0\n+\n+\n+\n+phlippe_densenet,pass,0\n+\n+\n+\n+phlippe_resnet,pass,0\n+\n+\n+\n+pyhpc_equation_of_state,pass,0\n+\n+\n+\n+pyhpc_isoneutral_mixing,pass,0\n+\n+\n+\n+pyhpc_turbulent_kinetic_energy,pass,0\n+\n+\n+\n+pytorch_CycleGAN_and_pix2pix,pass,0\n+\n+\n+\n+pytorch_stargan,pass,0\n+\n+\n+\n+pytorch_unet,pass,0\n+\n+\n+\n+resnet152,pass,0\n+\n+\n+\n+resnet18,pass,0\n+\n+\n+\n+resnet50,pass,0\n+\n+\n+\n+resnet50_quantized_qat,fail_to_run,0\n+\n+\n+\n+resnext50_32x4d,pass,0\n+\n+\n+\n+sam,fail_to_run,0\n+\n+\n+\n+sam_fast,fail_to_run,0\n+\n+\n+\n+shufflenet_v2_x1_0,pass,0\n+\n+\n+\n+soft_actor_critic,fail_to_run,0\n+\n+\n+\n+squeezenet1_1,pass,0\n+\n+\n+\n+stable_diffusion_text_encoder,pass,0\n+\n+\n+\n+stable_diffusion_unet,pass_due_to_skip,0\n+\n+\n+\n+timm_efficientdet,model_fail_to_load,0\n+\n+\n+\n+timm_efficientnet,pass,0\n+\n+\n+\n+timm_nfnet,pass,0\n+\n+\n+\n+timm_regnet,pass,0\n+\n+\n+\n+timm_resnest,pass,0\n+\n+\n+\n+timm_vision_transformer,pass,0\n+\n+\n+\n+timm_vision_transformer_large,pass_due_to_skip,0\n+\n+\n+\n+timm_vovnet,pass,0\n+\n+\n+\n+torch_multimodal_clip,pass,0\n+\n+\n+\n+tts_angular,fail_to_run,0\n+\n+\n+\n+vgg16,pass,0\n+\n+\n+\n+vision_maskrcnn,fail_to_run,0\n+\n+\n+\n+yolov3,pass,0"
        }
    ]
},
{
    "Id": 483,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/07f0f383fa23e63eca164036ab58ab983e9437eb",
    "date": "2023-10-12T02:14:38+00:00",
    "message": "update tensor-like to check instance for torch function impl (#111087)\n\ntensor like should check the instance for a torch function impl, not the type\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111087\nApproved by: https://github.com/ezyang",
    "label": "NO",
    "changes": [
        {
            "name": "overrides.py",
            "path": "torch/overrides.py",
            "patches": [
                {
                    "old_start": 1845,
                    "old_length": 7,
                    "new_start": 1845,
                    "new_length": 7,
                    "hunk": "@@ -1845,7 +1845,7 @@ def is_tensor_like(inp):\n     >>> is_tensor_like(TensorLike())\n     True\n     \"\"\"\n-    return type(inp) is torch.Tensor or hasattr(type(inp), \"__torch_function__\")\n+    return type(inp) is torch.Tensor or hasattr(inp, \"__torch_function__\")\n \n class TorchFunctionMode:\n     \"\"\""
                }
            ],
            "whole_deleted": "-    return type(inp) is torch.Tensor or hasattr(type(inp), \"__torch_function__\")\n",
            "whole_added": "+    return type(inp) is torch.Tensor or hasattr(inp, \"__torch_function__\")\n",
            "whole_hunk": "@@ -1845,7 +1845,7 @@ def is_tensor_like(inp):\n     >>> is_tensor_like(TensorLike())\n     True\n     \"\"\"\n-    return type(inp) is torch.Tensor or hasattr(type(inp), \"__torch_function__\")\n+    return type(inp) is torch.Tensor or hasattr(inp, \"__torch_function__\")\n \n class TorchFunctionMode:\n     \"\"\""
        }
    ]
},
{
    "Id": 281,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/e29eb39e04a9318abae5be131ec361690443fc7d",
    "date": "2024-02-23T20:31:21+00:00",
    "message": "[EZ] Fix typo in gcc version detection (#120489)\n\nIt should be `FATAL_ERROR` rather than `FATAL`\n\nI wish cmakelint would have detected it\n\nAlso, downgrade this check to 9.3, as all our binary builds are using 9.3 at the moment (will update in a followup PR)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/120489\nApproved by: https://github.com/DanilBaibak, https://github.com/Skylion007",
    "label": "YES",
    "changes": [
        {
            "name": "CMakeLists.txt",
            "path": "CMakeLists.txt",
            "patches": [
                {
                    "old_start": 43,
                    "old_length": 9,
                    "new_start": 43,
                    "new_length": 9,
                    "hunk": "@@ -43,9 +43,9 @@ set(CMAKE_C_STANDARD   11 CACHE STRING \"The C standard whose features are reques\n # ---[ Utils\n include(cmake/public/utils.cmake)\n \n-# --- [ Check that minimal gcc version is 9.4+\n-if(CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 9.4)\n-  message(FATAL \"GCC-9.4 or newer is required to compile PyTorch, but found ${CMAKE_CXX_COMPILER_VERSION}\")\n+# --- [ Check that minimal gcc version is 9.3+\n+if(CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 9.3)\n+  message(FATAL_ERROR \"GCC-9.3 or newer is required to compile PyTorch, but found ${CMAKE_CXX_COMPILER_VERSION}\")\n endif()\n \n # This define is needed to preserve behavior given anticpated changes to cccl/thrust"
                }
            ],
            "whole_deleted": "-# --- [ Check that minimal gcc version is 9.4+\n-if(CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 9.4)\n-  message(FATAL \"GCC-9.4 or newer is required to compile PyTorch, but found ${CMAKE_CXX_COMPILER_VERSION}\")\n",
            "whole_added": "+# --- [ Check that minimal gcc version is 9.3+\n+if(CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 9.3)\n+  message(FATAL_ERROR \"GCC-9.3 or newer is required to compile PyTorch, but found ${CMAKE_CXX_COMPILER_VERSION}\")\n",
            "whole_hunk": "@@ -43,9 +43,9 @@ set(CMAKE_C_STANDARD   11 CACHE STRING \"The C standard whose features are reques\n # ---[ Utils\n include(cmake/public/utils.cmake)\n \n-# --- [ Check that minimal gcc version is 9.4+\n-if(CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 9.4)\n-  message(FATAL \"GCC-9.4 or newer is required to compile PyTorch, but found ${CMAKE_CXX_COMPILER_VERSION}\")\n+# --- [ Check that minimal gcc version is 9.3+\n+if(CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 9.3)\n+  message(FATAL_ERROR \"GCC-9.3 or newer is required to compile PyTorch, but found ${CMAKE_CXX_COMPILER_VERSION}\")\n endif()\n \n # This define is needed to preserve behavior given anticpated changes to cccl/thrust"
        }
    ]
},
{
    "Id": 394,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/7979ba7b4327d1fabbeb06bafb2f1d2a2269f8b9",
    "date": "2023-12-03T23:05:02+00:00",
    "message": "[inductor] Add dropout type check to match eager (#115040)\n\nFixes #98970\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115040\nApproved by: https://github.com/oulgen",
    "label": "YES",
    "changes": [
        {
            "name": "test_cpu_repro.py",
            "path": "test/inductor/test_cpu_repro.py",
            "patches": [
                {
                    "old_start": 2423,
                    "old_length": 6,
                    "new_start": 2423,
                    "new_length": 21,
                    "hunk": "@@ -2423,6 +2423,21 @@ class CPUReproTests(TestCase):\n             self.assertFalse(\"= as_strided(\" in code)\n             self.assertEqual(run(*v), mod(*v))\n \n+    def test_invalid_dropout_args(self):\n+        class MyModel(torch.nn.Module):\n+            def forward(self, x):\n+                x = x * 2\n+                x = torch.nn.functional.dropout(x, p=0.5)\n+                x = torch.relu(x)\n+                return x\n+\n+        example_inputs = torch.tensor([[1, 2, 3], [4, 5, 6]])\n+\n+        func = MyModel()\n+        jit_func = torch.compile(func)\n+        self.assertRaises(RuntimeError, lambda: func(example_inputs))\n+        self.assertRaises(RuntimeError, lambda: jit_func(example_inputs))\n+\n     @config.patch(inplace_buffers=True)\n     def test_in_out_buffer(self):\n         def fn(x, y):\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    def test_invalid_dropout_args(self):\n+        class MyModel(torch.nn.Module):\n+            def forward(self, x):\n+                x = x * 2\n+                x = torch.nn.functional.dropout(x, p=0.5)\n+                x = torch.relu(x)\n+                return x\n+\n+        example_inputs = torch.tensor([[1, 2, 3], [4, 5, 6]])\n+\n+        func = MyModel()\n+        jit_func = torch.compile(func)\n+        self.assertRaises(RuntimeError, lambda: func(example_inputs))\n+        self.assertRaises(RuntimeError, lambda: jit_func(example_inputs))\n+\n",
            "whole_hunk": "@@ -2423,6 +2423,21 @@ class CPUReproTests(TestCase):\n             self.assertFalse(\"= as_strided(\" in code)\n             self.assertEqual(run(*v), mod(*v))\n \n+    def test_invalid_dropout_args(self):\n+        class MyModel(torch.nn.Module):\n+            def forward(self, x):\n+                x = x * 2\n+                x = torch.nn.functional.dropout(x, p=0.5)\n+                x = torch.relu(x)\n+                return x\n+\n+        example_inputs = torch.tensor([[1, 2, 3], [4, 5, 6]])\n+\n+        func = MyModel()\n+        jit_func = torch.compile(func)\n+        self.assertRaises(RuntimeError, lambda: func(example_inputs))\n+        self.assertRaises(RuntimeError, lambda: jit_func(example_inputs))\n+\n     @config.patch(inplace_buffers=True)\n     def test_in_out_buffer(self):\n         def fn(x, y):\n"
        },
        {
            "name": "decompositions.py",
            "path": "torch/_decomp/decompositions.py",
            "patches": [
                {
                    "old_start": 1095,
                    "old_length": 6,
                    "new_start": 1095,
                    "new_length": 10,
                    "hunk": "@@ -1095,6 +1095,10 @@ def native_dropout(input: Tensor, p: float, train: Optional[bool]):\n     if train and p != 0:\n         if p == 1:\n             return (torch.zeros_like(input), torch.zeros_like(input, dtype=torch.bool))\n+        if not input.dtype.is_floating_point:\n+            raise RuntimeError(\n+                \"result type Float can't be cast to the desired output type Long\"\n+            )\n         bool_mask = torch.rand_like(input) > p\n         res = bool_mask * input * float(1.0 / (1.0 - p))\n         return (res, bool_mask)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        if not input.dtype.is_floating_point:\n+            raise RuntimeError(\n+                \"result type Float can't be cast to the desired output type Long\"\n+            )\n",
            "whole_hunk": "@@ -1095,6 +1095,10 @@ def native_dropout(input: Tensor, p: float, train: Optional[bool]):\n     if train and p != 0:\n         if p == 1:\n             return (torch.zeros_like(input), torch.zeros_like(input, dtype=torch.bool))\n+        if not input.dtype.is_floating_point:\n+            raise RuntimeError(\n+                \"result type Float can't be cast to the desired output type Long\"\n+            )\n         bool_mask = torch.rand_like(input) > p\n         res = bool_mask * input * float(1.0 / (1.0 - p))\n         return (res, bool_mask)"
        }
    ]
},
{
    "Id": 470,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/fdc29f58c6c1b4f1a7215573d18551e63071f1a0",
    "date": "2023-10-20T19:20:43+00:00",
    "message": "[TP] Refactor style to make it work with torch.compile (#111625)\n\nWe are refactoring parallel style to solve the following things:\n1. To further simplifying code logic to make more readable for users.\n2. To remove tuple check so that we can work with dynamo for now. Ideally dynamo needs to support this case and we will fix it in parallel.\n3. Add tests for newly added parallel style in UT and torch compile test so that we can capture regression due to code change.\n4. Move placements early return check into DTensor since it is by passed by dynamo.\n5. Remove PairwiseParallelStyle from unit tests to use the new Col/Rowwise parallel style.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111625\nApproved by: https://github.com/wanchaol",
    "label": "NO",
    "changes": [
        {
            "name": "test_dtensor_compile.py",
            "path": "test/distributed/_tensor/test_dtensor_compile.py",
            "patches": [
                {
                    "old_start": 12,
                    "old_length": 13,
                    "new_start": 12,
                    "new_length": 17,
                    "hunk": "@@ -12,13 +12,17 @@ from torch.distributed._tensor import DeviceMesh, DTensor, Replicate, Shard\n from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n from torch.distributed.tensor.parallel import (\n     ColwiseParallel,\n-    PairwiseParallel,\n     parallelize_module,\n+    PrepareModuleInput,\n     RowwiseParallel,\n )\n from torch.distributed.tensor.parallel.fsdp import enable_2d_with_fsdp\n from torch.testing._internal.common_distributed import skip_if_lt_x_gpu\n-from torch.testing._internal.common_utils import run_tests\n+from torch.testing._internal.common_utils import (\n+    instantiate_parametrized_tests,\n+    parametrize,\n+    run_tests,\n+)\n from torch.testing._internal.distributed._tensor.common_dtensor import (\n     DTensorTestBase,\n     MLPModule,\n"
                },
                {
                    "old_start": 182,
                    "old_length": 20,
                    "new_start": 186,
                    "new_length": 40,
                    "hunk": "@@ -182,20 +186,40 @@ class TestDTensorCompileE2E(DTensorTestBase):\n         return 4\n \n     @with_comms\n-    def test_tp_compile_fullgraph(self):\n+    @parametrize(\"is_seq_parallel\", [True, False])\n+    def test_tp_compile_fullgraph(self, is_seq_parallel):\n         mesh = DeviceMesh(self.device_type, torch.arange(self.world_size))\n \n         model = SimpleModel(self.device_type)\n+        module_prepare_input = (\n+            PrepareModuleInput()\n+            if is_seq_parallel\n+            else PrepareModuleInput(input_layouts=Replicate())\n+        )\n+        no_input_prepare_colwise_style = ColwiseParallel(input_layouts=None)\n+        colwise_style = (\n+            ColwiseParallel(input_layouts=Shard(0))\n+            if is_seq_parallel\n+            else ColwiseParallel()\n+        )\n+        rowwise_style = (\n+            RowwiseParallel(output_layouts=Shard(0))\n+            if is_seq_parallel\n+            else RowwiseParallel()\n+        )\n         model = parallelize_module(\n             model,\n             mesh,\n             parallelize_plan={\n-                \"mlp_0.net1\": ColwiseParallel(),\n-                \"mlp_0.net2\": RowwiseParallel(),\n-                \"mlp_1.net1\": ColwiseParallel(),\n-                \"mlp_1.net2\": RowwiseParallel(),\n+                \"mlp_0\": module_prepare_input,\n+                \"mlp_0.net1\": no_input_prepare_colwise_style,\n+                \"mlp_0.net2\": rowwise_style,\n+                \"mlp_1.net1\": colwise_style,\n+                \"mlp_1.net2\": rowwise_style,\n             },\n         )\n+        rng_seed = self.rank if is_seq_parallel else 0\n+        torch.manual_seed(rng_seed)\n         inp = torch.rand(20, 10, device=self.device_type)\n         out = model(inp)\n         compiled_mod = torch.compile(model, backend=\"aot_eager\", fullgraph=True)\n"
                },
                {
                    "old_start": 219,
                    "old_length": 15,
                    "new_start": 243,
                    "new_length": 19,
                    "hunk": "@@ -219,15 +243,19 @@ class TestDTensorCompileE2E(DTensorTestBase):\n         fsdp_pg = twod_mesh.get_dim_groups()[0]\n \n         inp = torch.rand(20, 10, device=self.device_type)\n-        tp_model = parallelize_module(\n-            model, twod_mesh, PairwiseParallel(), tp_mesh_dim=1\n-        )\n+        parallelize_plan = {\n+            \"mlp_0.net1\": ColwiseParallel(),\n+            \"mlp_0.net2\": RowwiseParallel(),\n+            \"mlp_1.net1\": ColwiseParallel(),\n+            \"mlp_1.net2\": RowwiseParallel(),\n+        }\n+        tp_model = parallelize_module(model, twod_mesh, parallelize_plan, tp_mesh_dim=1)\n         eager_2d = FSDP(\n             tp_model, process_group=fsdp_pg, device_id=self.rank, use_orig_params=True\n         )\n         out = eager_2d(inp)\n         tp_model2 = parallelize_module(\n-            model_copy, twod_mesh, PairwiseParallel(), tp_mesh_dim=1\n+            model_copy, twod_mesh, parallelize_plan, tp_mesh_dim=1\n         )\n         fsdp_2d = FSDP(\n             tp_model2,\n"
                },
                {
                    "old_start": 276,
                    "old_length": 5,
                    "new_start": 304,
                    "new_length": 7,
                    "hunk": "@@ -276,5 +304,7 @@ class TestDTensorCompileE2E(DTensorTestBase):\n         self.assertEqual(y_ref.grad, y.grad)\n \n \n+instantiate_parametrized_tests(TestDTensorCompileE2E)\n+\n if __name__ == \"__main__\":\n     run_tests()\n"
                }
            ],
            "whole_deleted": "-    PairwiseParallel,\n-from torch.testing._internal.common_utils import run_tests\n-    def test_tp_compile_fullgraph(self):\n-                \"mlp_0.net1\": ColwiseParallel(),\n-                \"mlp_0.net2\": RowwiseParallel(),\n-                \"mlp_1.net1\": ColwiseParallel(),\n-                \"mlp_1.net2\": RowwiseParallel(),\n-        tp_model = parallelize_module(\n-            model, twod_mesh, PairwiseParallel(), tp_mesh_dim=1\n-        )\n-            model_copy, twod_mesh, PairwiseParallel(), tp_mesh_dim=1\n",
            "whole_added": "+    PrepareModuleInput,\n+from torch.testing._internal.common_utils import (\n+    instantiate_parametrized_tests,\n+    parametrize,\n+    run_tests,\n+)\n+    @parametrize(\"is_seq_parallel\", [True, False])\n+    def test_tp_compile_fullgraph(self, is_seq_parallel):\n+        module_prepare_input = (\n+            PrepareModuleInput()\n+            if is_seq_parallel\n+            else PrepareModuleInput(input_layouts=Replicate())\n+        )\n+        no_input_prepare_colwise_style = ColwiseParallel(input_layouts=None)\n+        colwise_style = (\n+            ColwiseParallel(input_layouts=Shard(0))\n+            if is_seq_parallel\n+            else ColwiseParallel()\n+        )\n+        rowwise_style = (\n+            RowwiseParallel(output_layouts=Shard(0))\n+            if is_seq_parallel\n+            else RowwiseParallel()\n+        )\n+                \"mlp_0\": module_prepare_input,\n+                \"mlp_0.net1\": no_input_prepare_colwise_style,\n+                \"mlp_0.net2\": rowwise_style,\n+                \"mlp_1.net1\": colwise_style,\n+                \"mlp_1.net2\": rowwise_style,\n+        rng_seed = self.rank if is_seq_parallel else 0\n+        torch.manual_seed(rng_seed)\n+        parallelize_plan = {\n+            \"mlp_0.net1\": ColwiseParallel(),\n+            \"mlp_0.net2\": RowwiseParallel(),\n+            \"mlp_1.net1\": ColwiseParallel(),\n+            \"mlp_1.net2\": RowwiseParallel(),\n+        }\n+        tp_model = parallelize_module(model, twod_mesh, parallelize_plan, tp_mesh_dim=1)\n+            model_copy, twod_mesh, parallelize_plan, tp_mesh_dim=1\n+instantiate_parametrized_tests(TestDTensorCompileE2E)\n+\n",
            "whole_hunk": "@@ -12,13 +12,17 @@ from torch.distributed._tensor import DeviceMesh, DTensor, Replicate, Shard\n from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n from torch.distributed.tensor.parallel import (\n     ColwiseParallel,\n-    PairwiseParallel,\n     parallelize_module,\n+    PrepareModuleInput,\n     RowwiseParallel,\n )\n from torch.distributed.tensor.parallel.fsdp import enable_2d_with_fsdp\n from torch.testing._internal.common_distributed import skip_if_lt_x_gpu\n-from torch.testing._internal.common_utils import run_tests\n+from torch.testing._internal.common_utils import (\n+    instantiate_parametrized_tests,\n+    parametrize,\n+    run_tests,\n+)\n from torch.testing._internal.distributed._tensor.common_dtensor import (\n     DTensorTestBase,\n     MLPModule,\n@@ -182,20 +186,40 @@ class TestDTensorCompileE2E(DTensorTestBase):\n         return 4\n \n     @with_comms\n-    def test_tp_compile_fullgraph(self):\n+    @parametrize(\"is_seq_parallel\", [True, False])\n+    def test_tp_compile_fullgraph(self, is_seq_parallel):\n         mesh = DeviceMesh(self.device_type, torch.arange(self.world_size))\n \n         model = SimpleModel(self.device_type)\n+        module_prepare_input = (\n+            PrepareModuleInput()\n+            if is_seq_parallel\n+            else PrepareModuleInput(input_layouts=Replicate())\n+        )\n+        no_input_prepare_colwise_style = ColwiseParallel(input_layouts=None)\n+        colwise_style = (\n+            ColwiseParallel(input_layouts=Shard(0))\n+            if is_seq_parallel\n+            else ColwiseParallel()\n+        )\n+        rowwise_style = (\n+            RowwiseParallel(output_layouts=Shard(0))\n+            if is_seq_parallel\n+            else RowwiseParallel()\n+        )\n         model = parallelize_module(\n             model,\n             mesh,\n             parallelize_plan={\n-                \"mlp_0.net1\": ColwiseParallel(),\n-                \"mlp_0.net2\": RowwiseParallel(),\n-                \"mlp_1.net1\": ColwiseParallel(),\n-                \"mlp_1.net2\": RowwiseParallel(),\n+                \"mlp_0\": module_prepare_input,\n+                \"mlp_0.net1\": no_input_prepare_colwise_style,\n+                \"mlp_0.net2\": rowwise_style,\n+                \"mlp_1.net1\": colwise_style,\n+                \"mlp_1.net2\": rowwise_style,\n             },\n         )\n+        rng_seed = self.rank if is_seq_parallel else 0\n+        torch.manual_seed(rng_seed)\n         inp = torch.rand(20, 10, device=self.device_type)\n         out = model(inp)\n         compiled_mod = torch.compile(model, backend=\"aot_eager\", fullgraph=True)\n@@ -219,15 +243,19 @@ class TestDTensorCompileE2E(DTensorTestBase):\n         fsdp_pg = twod_mesh.get_dim_groups()[0]\n \n         inp = torch.rand(20, 10, device=self.device_type)\n-        tp_model = parallelize_module(\n-            model, twod_mesh, PairwiseParallel(), tp_mesh_dim=1\n-        )\n+        parallelize_plan = {\n+            \"mlp_0.net1\": ColwiseParallel(),\n+            \"mlp_0.net2\": RowwiseParallel(),\n+            \"mlp_1.net1\": ColwiseParallel(),\n+            \"mlp_1.net2\": RowwiseParallel(),\n+        }\n+        tp_model = parallelize_module(model, twod_mesh, parallelize_plan, tp_mesh_dim=1)\n         eager_2d = FSDP(\n             tp_model, process_group=fsdp_pg, device_id=self.rank, use_orig_params=True\n         )\n         out = eager_2d(inp)\n         tp_model2 = parallelize_module(\n-            model_copy, twod_mesh, PairwiseParallel(), tp_mesh_dim=1\n+            model_copy, twod_mesh, parallelize_plan, tp_mesh_dim=1\n         )\n         fsdp_2d = FSDP(\n             tp_model2,\n@@ -276,5 +304,7 @@ class TestDTensorCompileE2E(DTensorTestBase):\n         self.assertEqual(y_ref.grad, y.grad)\n \n \n+instantiate_parametrized_tests(TestDTensorCompileE2E)\n+\n if __name__ == \"__main__\":\n     run_tests()\n"
        },
        {
            "name": "test_tp_style.py",
            "path": "test/distributed/tensor/parallel/test_tp_style.py",
            "patches": [
                {
                    "old_start": 13,
                    "old_length": 6,
                    "new_start": 13,
                    "new_length": 8,
                    "hunk": "@@ -13,6 +13,8 @@ from torch.distributed.tensor.parallel.style import (\n     make_output_reshard_tensor,\n     make_output_shard_1d,\n     make_output_tensor,\n+    PrepareModuleInput,\n+    PrepareModuleOutput,\n     RowwiseParallel,\n )\n from torch.testing._internal.common_utils import run_tests\n"
                },
                {
                    "old_start": 41,
                    "old_length": 13,
                    "new_start": 43,
                    "new_length": 16,
                    "hunk": "@@ -41,13 +43,16 @@ class TensorParallelStyleTest(DTensorTestBase):\n         device_mesh = DeviceMesh(self.device_type, list(range(self.world_size)))\n         # test 1: replicate local tensor\n         dtensor = func(input_local_tensor, device_mesh)\n-        self.assertEqual(expected_local_tensor, dtensor.to_local())\n+        result = dtensor[0] if isinstance(dtensor, tuple) else dtensor\n+        self.assertEqual(expected_local_tensor, result.to_local())\n         # test 2: replicate DTensor\n         dtensor = func(dtensor)\n-        self.assertEqual(expected_local_tensor, dtensor.to_local())\n+        result = dtensor[0] if isinstance(dtensor, tuple) else dtensor\n+        self.assertEqual(expected_local_tensor, result.to_local())\n         # test 3: replicate DTensor with DeviceMesh passed\n         dtensor = func(dtensor, device_mesh)\n-        self.assertEqual(expected_local_tensor, dtensor.to_local())\n+        result = dtensor[0] if isinstance(dtensor, tuple) else dtensor\n+        self.assertEqual(expected_local_tensor, result.to_local())\n \n     @with_comms\n     def test_make_input_replicate_1d(self):\n"
                },
                {
                    "old_start": 187,
                    "old_length": 9,
                    "new_start": 192,
                    "new_length": 9,
                    "hunk": "@@ -187,9 +192,9 @@ class TensorParallelStyleTest(DTensorTestBase):\n         dtensor = distribute_tensor(tensor, device_mesh, [Shard(0)])\n         output = [dtensor]\n         with self.assertRaisesRegex(\n-            AssertionError,\n-            \"Expect output of Tensor Parallel to be a DTensor, but found\"\n-            f\" {type(output)}.\",\n+            RuntimeError,\n+            \"Tensor parallel module expects DTensor or tensor\"\n+            f\" when layout specified but received {type(output)}!\",\n         ):\n             func(output, device_mesh)\n \n"
                },
                {
                    "old_start": 204,
                    "old_length": 9,
                    "new_start": 209,
                    "new_length": 10,
                    "hunk": "@@ -204,9 +209,10 @@ class TensorParallelStyleTest(DTensorTestBase):\n         tensor = torch.rand(8, 16, device=self.device_type)\n         rs = RowwiseParallel()\n         self._1d_input_func_check(\n-            tensor,\n+            [tensor],\n             tensor,\n             rs._prepare_input,\n+            error_msgs=\"No device mesh is currently active\",\n         )\n         # TODO: change output test\n         output, dtensor, device_mesh = self._test_prepare_output(\n"
                },
                {
                    "old_start": 229,
                    "old_length": 15,
                    "new_start": 235,
                    "new_length": 41,
                    "hunk": "@@ -229,15 +235,41 @@ class TensorParallelStyleTest(DTensorTestBase):\n         tensor = torch.rand(8, 16, device=self.device_type)\n         cs = ColwiseParallel()\n         self._1d_input_func_check(\n-            tensor,\n+            [tensor],\n             tensor,\n             cs._prepare_input,\n+            error_msgs=\"No device mesh is currently active\",\n         )\n         output, dtensor, device_mesh = self._test_prepare_output(\n             cs._prepare_output, [Shard(-1)]\n         )\n         self.assertEqual(output, dtensor.to_local())\n \n+    @with_comms\n+    def test_prepare_module_input(self):\n+        tensor = torch.rand(8, 16, device=self.device_type)\n+        gathered_tensors = [\n+            torch.empty_like(tensor) for _ in range(self.world_size)\n+        ]\n+        dist.all_gather(gathered_tensors, tensor)\n+        gathered_tensors = torch.cat(gathered_tensors, dim=0).contiguous()\n+        prepare_hook = PrepareModuleInput(input_layouts=[Shard(0)], output_layouts=[Replicate()])\n+        self._1d_input_func_check(\n+            [tensor],\n+            gathered_tensors,\n+            prepare_hook._prepare_input,\n+            error_msgs=\"No device mesh is currently active\",\n+        )\n+\n+    @with_comms\n+    def test_prepare_module_output(self):\n+        tensor = torch.rand(8, 16, device=self.device_type)\n+        prepare_hook = PrepareModuleOutput(input_layouts=[Replicate()], output_layouts=[Shard(0)])\n+        output, dtensor, device_mesh = self._test_prepare_output(\n+            prepare_hook._prepare_output, [Replicate()]\n+        )\n+        self.assertEqual(output, dtensor.redistribute(device_mesh, [Shard(0)]).to_local())\n+\n \n if __name__ == \"__main__\":\n     run_tests()\n"
                }
            ],
            "whole_deleted": "-        self.assertEqual(expected_local_tensor, dtensor.to_local())\n-        self.assertEqual(expected_local_tensor, dtensor.to_local())\n-        self.assertEqual(expected_local_tensor, dtensor.to_local())\n-            AssertionError,\n-            \"Expect output of Tensor Parallel to be a DTensor, but found\"\n-            f\" {type(output)}.\",\n-            tensor,\n-            tensor,\n",
            "whole_added": "+    PrepareModuleInput,\n+    PrepareModuleOutput,\n+        result = dtensor[0] if isinstance(dtensor, tuple) else dtensor\n+        self.assertEqual(expected_local_tensor, result.to_local())\n+        result = dtensor[0] if isinstance(dtensor, tuple) else dtensor\n+        self.assertEqual(expected_local_tensor, result.to_local())\n+        result = dtensor[0] if isinstance(dtensor, tuple) else dtensor\n+        self.assertEqual(expected_local_tensor, result.to_local())\n+            RuntimeError,\n+            \"Tensor parallel module expects DTensor or tensor\"\n+            f\" when layout specified but received {type(output)}!\",\n+            [tensor],\n+            error_msgs=\"No device mesh is currently active\",\n+            [tensor],\n+            error_msgs=\"No device mesh is currently active\",\n+    @with_comms\n+    def test_prepare_module_input(self):\n+        tensor = torch.rand(8, 16, device=self.device_type)\n+        gathered_tensors = [\n+            torch.empty_like(tensor) for _ in range(self.world_size)\n+        ]\n+        dist.all_gather(gathered_tensors, tensor)\n+        gathered_tensors = torch.cat(gathered_tensors, dim=0).contiguous()\n+        prepare_hook = PrepareModuleInput(input_layouts=[Shard(0)], output_layouts=[Replicate()])\n+        self._1d_input_func_check(\n+            [tensor],\n+            gathered_tensors,\n+            prepare_hook._prepare_input,\n+            error_msgs=\"No device mesh is currently active\",\n+        )\n+\n+    @with_comms\n+    def test_prepare_module_output(self):\n+        tensor = torch.rand(8, 16, device=self.device_type)\n+        prepare_hook = PrepareModuleOutput(input_layouts=[Replicate()], output_layouts=[Shard(0)])\n+        output, dtensor, device_mesh = self._test_prepare_output(\n+            prepare_hook._prepare_output, [Replicate()]\n+        )\n+        self.assertEqual(output, dtensor.redistribute(device_mesh, [Shard(0)]).to_local())\n+\n",
            "whole_hunk": "@@ -13,6 +13,8 @@ from torch.distributed.tensor.parallel.style import (\n     make_output_reshard_tensor,\n     make_output_shard_1d,\n     make_output_tensor,\n+    PrepareModuleInput,\n+    PrepareModuleOutput,\n     RowwiseParallel,\n )\n from torch.testing._internal.common_utils import run_tests\n@@ -41,13 +43,16 @@ class TensorParallelStyleTest(DTensorTestBase):\n         device_mesh = DeviceMesh(self.device_type, list(range(self.world_size)))\n         # test 1: replicate local tensor\n         dtensor = func(input_local_tensor, device_mesh)\n-        self.assertEqual(expected_local_tensor, dtensor.to_local())\n+        result = dtensor[0] if isinstance(dtensor, tuple) else dtensor\n+        self.assertEqual(expected_local_tensor, result.to_local())\n         # test 2: replicate DTensor\n         dtensor = func(dtensor)\n-        self.assertEqual(expected_local_tensor, dtensor.to_local())\n+        result = dtensor[0] if isinstance(dtensor, tuple) else dtensor\n+        self.assertEqual(expected_local_tensor, result.to_local())\n         # test 3: replicate DTensor with DeviceMesh passed\n         dtensor = func(dtensor, device_mesh)\n-        self.assertEqual(expected_local_tensor, dtensor.to_local())\n+        result = dtensor[0] if isinstance(dtensor, tuple) else dtensor\n+        self.assertEqual(expected_local_tensor, result.to_local())\n \n     @with_comms\n     def test_make_input_replicate_1d(self):\n@@ -187,9 +192,9 @@ class TensorParallelStyleTest(DTensorTestBase):\n         dtensor = distribute_tensor(tensor, device_mesh, [Shard(0)])\n         output = [dtensor]\n         with self.assertRaisesRegex(\n-            AssertionError,\n-            \"Expect output of Tensor Parallel to be a DTensor, but found\"\n-            f\" {type(output)}.\",\n+            RuntimeError,\n+            \"Tensor parallel module expects DTensor or tensor\"\n+            f\" when layout specified but received {type(output)}!\",\n         ):\n             func(output, device_mesh)\n \n@@ -204,9 +209,10 @@ class TensorParallelStyleTest(DTensorTestBase):\n         tensor = torch.rand(8, 16, device=self.device_type)\n         rs = RowwiseParallel()\n         self._1d_input_func_check(\n-            tensor,\n+            [tensor],\n             tensor,\n             rs._prepare_input,\n+            error_msgs=\"No device mesh is currently active\",\n         )\n         # TODO: change output test\n         output, dtensor, device_mesh = self._test_prepare_output(\n@@ -229,15 +235,41 @@ class TensorParallelStyleTest(DTensorTestBase):\n         tensor = torch.rand(8, 16, device=self.device_type)\n         cs = ColwiseParallel()\n         self._1d_input_func_check(\n-            tensor,\n+            [tensor],\n             tensor,\n             cs._prepare_input,\n+            error_msgs=\"No device mesh is currently active\",\n         )\n         output, dtensor, device_mesh = self._test_prepare_output(\n             cs._prepare_output, [Shard(-1)]\n         )\n         self.assertEqual(output, dtensor.to_local())\n \n+    @with_comms\n+    def test_prepare_module_input(self):\n+        tensor = torch.rand(8, 16, device=self.device_type)\n+        gathered_tensors = [\n+            torch.empty_like(tensor) for _ in range(self.world_size)\n+        ]\n+        dist.all_gather(gathered_tensors, tensor)\n+        gathered_tensors = torch.cat(gathered_tensors, dim=0).contiguous()\n+        prepare_hook = PrepareModuleInput(input_layouts=[Shard(0)], output_layouts=[Replicate()])\n+        self._1d_input_func_check(\n+            [tensor],\n+            gathered_tensors,\n+            prepare_hook._prepare_input,\n+            error_msgs=\"No device mesh is currently active\",\n+        )\n+\n+    @with_comms\n+    def test_prepare_module_output(self):\n+        tensor = torch.rand(8, 16, device=self.device_type)\n+        prepare_hook = PrepareModuleOutput(input_layouts=[Replicate()], output_layouts=[Shard(0)])\n+        output, dtensor, device_mesh = self._test_prepare_output(\n+            prepare_hook._prepare_output, [Replicate()]\n+        )\n+        self.assertEqual(output, dtensor.redistribute(device_mesh, [Shard(0)]).to_local())\n+\n \n if __name__ == \"__main__\":\n     run_tests()\n"
        },
        {
            "name": "api.py",
            "path": "torch/distributed/_tensor/api.py",
            "patches": [
                {
                    "old_start": 394,
                    "old_length": 6,
                    "new_start": 394,
                    "new_length": 10,
                    "hunk": "@@ -394,6 +394,10 @@ class DTensor(torch.Tensor):  # pyre-ignore[13]: pyre is bad at __new__\n         if placements is None:\n             raise RuntimeError(\"placements is needed for redistribute!\")\n \n+        # Early return the original DTensor if the placements are the same.\n+        if self._spec.placements == placements:\n+            return self\n+\n         for placement in placements:\n             if placement.is_partial():\n                 raise RuntimeError(\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        # Early return the original DTensor if the placements are the same.\n+        if self._spec.placements == placements:\n+            return self\n+\n",
            "whole_hunk": "@@ -394,6 +394,10 @@ class DTensor(torch.Tensor):  # pyre-ignore[13]: pyre is bad at __new__\n         if placements is None:\n             raise RuntimeError(\"placements is needed for redistribute!\")\n \n+        # Early return the original DTensor if the placements are the same.\n+        if self._spec.placements == placements:\n+            return self\n+\n         for placement in placements:\n             if placement.is_partial():\n                 raise RuntimeError(\n"
        },
        {
            "name": "style.py",
            "path": "torch/distributed/tensor/parallel/style.py",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 11,
                    "new_start": 1,
                    "new_length": 10,
                    "hunk": "@@ -1,11 +1,10 @@\n # Copyright (c) Meta Platforms, Inc. and affiliates\n import functools\n from abc import ABC, abstractmethod\n-from typing import Any, Callable, Optional, Tuple, Union\n+from typing import Any, Optional, Tuple, Union\n \n import torch\n from torch.distributed._tensor import DeviceMesh, DTensor, Replicate, Shard\n-from torch.distributed._tensor.placement_types import Placement\n from torch.distributed.tensor.parallel._utils import (\n     _deprecate_warnings,\n     _prepare_input_validate,\n"
                },
                {
                    "old_start": 281,
                    "old_length": 162,
                    "new_start": 280,
                    "new_length": 36,
                    "hunk": "@@ -281,162 +280,36 @@ def make_output_reshard_tensor(\n     return make_output_shard_1d(output, device_mesh).to_local()  # type: ignore[call-arg, attr-defined, misc]\n \n \n-def _needs_redistribute(\n-    dst_placements: Tuple[Placement, ...], dtensor: DTensor\n-) -> bool:\n-    \"\"\"\n-    Check DTensor placements to decide whether the DTensor redistribute\n-    is needed to be called or not. If not, we can directly early return\n-    and save CPU overhead.\n-    \"\"\"\n-    return dtensor._spec.placements == dst_placements\n-\n-\n-def _get_prepare_input(\n-    input_layouts: LayoutsType, output_layouts: LayoutsType\n-) -> Callable[[Any], Any]:\n-    \"\"\"\n-    Get the prepare input function for this parallel style.\n-    \"\"\"\n-\n-    def _redistribute_per_both_layouts(t, input_layout, output_layout, device_mesh):\n-        dst_placements = (output_layout,)\n-        if isinstance(t, DTensor):\n-            return (\n-                t\n-                if _needs_redistribute(dst_placements, t)\n-                else t.redistribute(device_mesh, dst_placements)\n-            )\n-        elif isinstance(t, torch.Tensor):\n-            dtensor = DTensor.from_local(\n-                t, device_mesh, [input_layout], run_check=False\n-            )\n-            return (\n-                dtensor\n-                if _needs_redistribute(dst_placements, dtensor)\n-                else dtensor.redistribute(device_mesh, dst_placements)\n+def _redistribute_per_both_layouts(t, input_layout, output_layout, device_mesh):\n+    dst_placements = (output_layout,)\n+    if isinstance(t, DTensor):\n+        return t.redistribute(device_mesh, dst_placements)\n+    elif isinstance(t, torch.Tensor):\n+        dtensor = DTensor.from_local(t, device_mesh, (input_layout,), run_check=False)\n+        return dtensor.redistribute(device_mesh, dst_placements)\n+    else:\n+        if input_layout is not None:\n+            raise RuntimeError(\n+                \"Tensor parallel module expects DTensor or tensor\"\n+                f\" when layout specified but received {type(t)}!\"\n             )\n         else:\n-            if input_layout is not None:\n-                raise RuntimeError(\n-                    \"Tensor parallel module expects DTensor or tensor\"\n-                    f\" when layout specified but received {type(t)}!\"\n-                )\n-            else:\n-                return t\n+            return t\n \n-    def make_input_redistribute_1d(\n-        input_layouts: LayoutsType,\n-        output_layouts: LayoutsType,\n-        inputs: Tuple[Any, ...],\n-        device_mesh: Optional[DeviceMesh] = None,\n-    ) -> Optional[Any]:\n-        \"\"\"\n-        Redistribute input tensor over an 1-D device mesh. This function will be used in ParallelStyle.\n-\n-        Args:\n-            input (Union[:class:`torch.Tensor`, :class:`DTensor`]):\n-                This input tensor will be replicated over the 1-D :class:`DeviceMesh`.\n-            device_mesh (:class:`DeviceMesh`, optional):\n-                The 1-D device mesh where ``input`` will be replicated.\n-                If no :class:`DeviceMesh` is passed and ``input`` is a :class:`DTensor`,\n-                ``input.device_mesh`` will be used.\n-                If :class:`DeviceMesh` is not 1-D, an exception will be thrown.\n-                Default: ``None``\n-\n-        Returns:\n-            A :class:`DTensor` replicated over ``device_mesh``.\n-        \"\"\"\n-        # Early return to save CPU overhead when there is only one input.\n-        if not isinstance(inputs, tuple):\n-            return _redistribute_per_both_layouts(\n-                inputs, input_layouts, output_layouts, device_mesh\n-            )\n \n-        if not isinstance(input_layouts, tuple):\n-            input_layouts = (input_layouts,)  # type: ignore[assignment]\n-            output_layouts = (output_layouts,)  # type: ignore[assignment]\n-        results = []\n-        for input, input_layout, output_layout in zip(\n-            inputs, input_layouts, output_layouts  # type: ignore[arg-type]\n-        ):\n-            results.append(\n-                _redistribute_per_both_layouts(\n-                    input, input_layout, output_layout, device_mesh\n-                )\n-            )\n-        return tuple(results)\n-\n-    return functools.partial(make_input_redistribute_1d, input_layouts, output_layouts)\n-\n-\n-def _get_prepare_output(\n-    output_layouts: LayoutsType, use_local_output: bool\n-) -> Callable[[Any], Any]:\n-    \"\"\"\n-    Get the prepare input function for this parallel style.\n-    \"\"\"\n-\n-    def _redistribute_per_layout(t, layout, device_mesh, use_local_output):\n-        dst_placements = (layout,)\n-        if isinstance(t, DTensor):\n-            dtensor = (\n-                t\n-                if _needs_redistribute(dst_placements, t)\n-                else t.redistribute(device_mesh, dst_placements)\n+def _redistribute_per_layout(layout, use_local_output, t, device_mesh):\n+    dst_placements = (layout,)\n+    if isinstance(t, DTensor):\n+        dtensor = t.redistribute(device_mesh, dst_placements)\n+        return dtensor.to_local() if use_local_output else dtensor\n+    else:\n+        if layout is not None:\n+            raise RuntimeError(\n+                \"Tensor parallel module expects DTensor or tensor\"\n+                f\" when layout specified but received {type(t)}!\"\n             )\n-            return dtensor.to_local() if use_local_output else dtensor\n         else:\n-            if layout is not None:\n-                raise RuntimeError(\n-                    \"Tensor parallel module expects DTensor or tensor\"\n-                    f\" when layout specified but received {type(t)}!\"\n-                )\n-            else:\n-                return t\n-\n-    def make_output_redistribute_1d(\n-        output_layouts: LayoutsType,\n-        use_local_output: bool,\n-        outputs: Tuple[Any, ...],\n-        device_mesh: Optional[DeviceMesh] = None,\n-    ) -> Optional[Any]:\n-        \"\"\"\n-        Redistribute input tensor over an 1-D device mesh. This function will be used in ParallelStyle.\n-\n-        Args:\n-            input (Union[:class:`torch.Tensor`, :class:`DTensor`]):\n-                This input tensor will be replicated over the 1-D :class:`DeviceMesh`.\n-            device_mesh (:class:`DeviceMesh`, optional):\n-                The 1-D device mesh where ``input`` will be replicated.\n-                If no :class:`DeviceMesh` is passed and ``input`` is a :class:`DTensor`,\n-                ``input.device_mesh`` will be used.\n-                If :class:`DeviceMesh` is not 1-D, an exception will be thrown.\n-                Default: ``None``\n-\n-        Returns:\n-            A :class:`DTensor` replicated over ``device_mesh``.\n-        \"\"\"\n-        # Early return to save CPU overhead when there is only one output.\n-        if not isinstance(outputs, tuple):\n-            return _redistribute_per_layout(\n-                outputs, output_layouts, device_mesh, use_local_output\n-            )\n-\n-        if not isinstance(output_layouts, tuple):\n-            output_layouts = (output_layouts,)  # type: ignore[assignment]\n-        results = []\n-        for output, output_layout in zip(outputs, output_layouts):  # type: ignore[arg-type]\n-            results.append(\n-                _redistribute_per_layout(\n-                    output, output_layout, device_mesh, use_local_output\n-                )\n-            )\n-        return tuple(results)\n-\n-    return functools.partial(\n-        make_output_redistribute_1d, output_layouts, use_local_output\n-    )\n+            return t\n \n \n class RowwiseParallel(ParallelStyle):\n"
                },
                {
                    "old_start": 494,
                    "old_length": 24,
                    "new_start": 367,
                    "new_length": 22,
                    "hunk": "@@ -494,24 +367,22 @@ class RowwiseParallel(ParallelStyle):\n                 \"RowwiseParallel only supports single input/output.\"\n             )\n \n+        prepare_input_fn = None\n         if _prepare_input is not None:\n             prepare_input_fn = _prepare_input\n-        if input_layouts == Shard(-1):\n-            prepare_input_fn = make_input_shard_1d_last_dim\n-        else:\n-            prepare_input_fn = _get_prepare_input(\n+        elif input_layouts is not None:\n+            prepare_input_fn = functools.partial(\n+                RowwiseParallel._prepare_input_fn,\n                 input_layouts,\n                 Shard(-1),\n             )\n \n+        prepare_output_fn = None\n         if _prepare_output is not None:\n             prepare_output_fn = _prepare_output\n-        elif output_layouts == Replicate():\n-            prepare_output_fn = make_output_tensor\n-        else:\n-            prepare_output_fn = _get_prepare_output(\n-                output_layouts,\n-                use_local_output,\n+        elif output_layouts is not None:\n+            prepare_output_fn = functools.partial(\n+                _redistribute_per_layout, output_layouts, use_local_output\n             )\n \n         super().__init__(\n"
                },
                {
                    "old_start": 522,
                    "old_length": 6,
                    "new_start": 393,
                    "new_length": 14,
                    "hunk": "@@ -522,6 +393,14 @@ class RowwiseParallel(ParallelStyle):\n             _prepare_output=prepare_output_fn,\n         )\n \n+    @staticmethod\n+    def _prepare_input_fn(input_layouts, output_layouts, inputs, device_mesh=None):\n+        return (\n+            _redistribute_per_both_layouts(\n+                inputs[0], input_layouts, output_layouts, device_mesh\n+            ),\n+        )\n+\n \n class ColwiseParallel(ParallelStyle):\n     \"\"\"\n"
                },
                {
                    "old_start": 573,
                    "old_length": 32,
                    "new_start": 452,
                    "new_length": 27,
                    "hunk": "@@ -573,32 +452,27 @@ class ColwiseParallel(ParallelStyle):\n         output_layouts=Shard(-1),\n         use_local_output=True,\n     ) -> None:\n-        \"\"\"\n-\n-        \"\"\"\n         if isinstance(input_layouts, tuple) or isinstance(output_layouts, tuple):\n             raise NotImplementedError(\n                 \"ColwiseParallel only supports single input/output.\"\n             )\n \n+        prepare_input_fn = None\n         if _prepare_input is not None:\n             prepare_input_fn = _prepare_input\n-        if input_layouts == Replicate():\n-            prepare_input_fn = make_input_replicate_1d\n-        else:\n-            prepare_input_fn = _get_prepare_input(\n+        elif input_layouts is not None:\n+            prepare_input_fn = functools.partial(\n+                ColwiseParallel._prepare_input_fn,\n                 input_layouts,\n                 Replicate(),\n             )\n \n+        prepare_output_fn = None\n         if _prepare_output is not None:\n             prepare_output_fn = _prepare_output\n-        elif output_layouts == Shard(-1):\n-            prepare_output_fn = make_sharded_output_tensor\n-        else:\n-            prepare_output_fn = _get_prepare_output(\n-                output_layouts,\n-                use_local_output,\n+        elif output_layouts is not None:\n+            prepare_output_fn = functools.partial(\n+                _redistribute_per_layout, output_layouts, use_local_output\n             )\n \n         super().__init__(\n"
                },
                {
                    "old_start": 609,
                    "old_length": 6,
                    "new_start": 483,
                    "new_length": 13,
                    "hunk": "@@ -609,6 +483,13 @@ class ColwiseParallel(ParallelStyle):\n             _prepare_output=prepare_output_fn,\n         )\n \n+    @staticmethod\n+    def _prepare_input_fn(input_layouts, output_layouts, inputs, device_mesh=None):\n+        return (\n+            _redistribute_per_both_layouts(\n+                inputs[0], input_layouts, output_layouts, device_mesh\n+            ),\n+        )\n \n class PrepareModuleInput(ParallelStyle):\n     \"\"\"\n"
                },
                {
                    "old_start": 620,
                    "old_length": 6,
                    "new_start": 501,
                    "new_length": 22,
                    "hunk": "@@ -620,6 +501,22 @@ class PrepareModuleInput(ParallelStyle):\n \n     When the input is not a :class:`torch.Tensor` or :class:`DTensor`, if no layout is\n     specified, it will be a no-op. Otherwise, it will throw an error.\n+\n+    Example::\n+        >>> # xdoctest: +SKIP(failing)\n+        >>> from torch.distributed.tensor.parallel import parallelize_module, PrepareModuleInput\n+        >>> ...\n+        >>> parallelize_plan = {\n+        >>>     \"attn\": PrepareModuleInput(),   # The input of attn will be converted to Sharded DTensor\n+        >>>                                     # and and redistributed to Replicated DTensor.\n+        >>>     ...\n+        >>> }\n+        >>> parallelize_module(\n+        >>>     module=block, # this can be a submodule or module\n+        >>>     ...,\n+        >>>     parallelize_plan=parallelize_plan,\n+        >>> )\n+        >>> ...\n     \"\"\"\n \n     def __init__(\n"
                },
                {
                    "old_start": 639,
                    "old_length": 34,
                    "new_start": 536,
                    "new_length": 47,
                    "hunk": "@@ -639,34 +536,47 @@ class PrepareModuleInput(ParallelStyle):\n \n         Returns:\n             None.\n-\n-        Example::\n-            >>> # xdoctest: +SKIP(failing)\n-            >>> from torch.distributed.tensor.parallel import parallelize_module, PrepareModuleInput\n-            >>> ...\n-            >>> parallelize_plan = {\n-            >>>     \"attn\": PrepareModuleInput(),   # The input of attn will be converted to Sharded DTensor\n-            >>>                                     # and and redistributed to Replicated DTensor.\n-            >>>     ...\n-            >>> }\n-            >>> parallelize_module(\n-            >>>     module=block, # this can be a submodule or module\n-            >>>     ...,\n-            >>>     parallelize_plan=parallelize_plan,\n-            >>> )\n-            >>> ...\n         \"\"\"\n+        if not isinstance(input_layouts, (list, tuple)):\n+            input_layouts = (input_layouts,)  # type: ignore[assignment]\n+\n+        if not isinstance(output_layouts, (list, tuple)):\n+            output_layouts = (output_layouts,)  # type: ignore[assignment]\n+\n         super().__init__(\n             input_layouts=input_layouts,\n             output_layouts=output_layouts,\n             use_local_output=use_local_output,\n-            _prepare_input=_get_prepare_input(\n+            _prepare_input=functools.partial(\n+                PrepareModuleInput._make_input_redistribute_1d,\n                 input_layouts,\n                 output_layouts,\n             ),\n             _prepare_output=None,\n         )\n \n+    @staticmethod\n+    def _make_input_redistribute_1d(\n+        input_layouts: LayoutsType,\n+        output_layouts: LayoutsType,\n+        inputs: Tuple[Any, ...],\n+        device_mesh: Optional[DeviceMesh] = None,\n+    ) -> Optional[Any]:\n+        \"\"\"\n+        Redistribute inputs over a device mesh.\n+        \"\"\"\n+        # Always assume layouts are tuples.\n+        results = []\n+        for input, input_layout, output_layout in zip(\n+            inputs, input_layouts, output_layouts  # type: ignore[arg-type]\n+        ):\n+            results.append(\n+                _redistribute_per_both_layouts(\n+                    input, input_layout, output_layout, device_mesh\n+                )\n+            )\n+        return tuple(results)\n+\n \n class PrepareModuleOutput(ParallelStyle):\n     \"\"\"\n"
                },
                {
                    "old_start": 702,
                    "old_length": 10,
                    "new_start": 612,
                    "new_length": 32,
                    "hunk": "@@ -702,10 +612,32 @@ class PrepareModuleOutput(ParallelStyle):\n         output_layouts: LayoutsType = Shard(0),\n         use_local_output: bool = True,\n     ) -> None:\n+        \"\"\"\n+        Args:\n+            input_layouts (Union[Placement, Tuple[Placement, ...]]):\n+                The layout of output DTensor(s).\n+            output_layouts (Union[Placement, Tuple[Placement, ...]]):\n+                The layout of output DTensor(s)/tensor(s) which output DTensor(s) will be redistributed to.\n+            use_local_output (bool):\n+                Whether to convert the DTensor to local :class:`torch.Tensor`.\n+\n+        Returns:\n+            None.\n+        \"\"\"\n+        if isinstance(output_layouts, (list, tuple)):\n+            if len(output_layouts) != 1:\n+                raise NotImplementedError(\n+                    \"Only support single output redistribute now.\"\n+                )\n+            else:\n+                output_layouts = output_layouts[0]\n+\n         super().__init__(\n             input_layouts=input_layouts,\n             output_layouts=output_layouts,\n             use_local_output=use_local_output,\n             _prepare_input=None,\n-            _prepare_output=_get_prepare_output(output_layouts, use_local_output),\n+            _prepare_output=functools.partial(\n+                _redistribute_per_layout, output_layouts, use_local_output\n+            ),\n         )"
                }
            ],
            "whole_deleted": "-from typing import Any, Callable, Optional, Tuple, Union\n-from torch.distributed._tensor.placement_types import Placement\n-def _needs_redistribute(\n-    dst_placements: Tuple[Placement, ...], dtensor: DTensor\n-) -> bool:\n-    \"\"\"\n-    Check DTensor placements to decide whether the DTensor redistribute\n-    is needed to be called or not. If not, we can directly early return\n-    and save CPU overhead.\n-    \"\"\"\n-    return dtensor._spec.placements == dst_placements\n-\n-\n-def _get_prepare_input(\n-    input_layouts: LayoutsType, output_layouts: LayoutsType\n-) -> Callable[[Any], Any]:\n-    \"\"\"\n-    Get the prepare input function for this parallel style.\n-    \"\"\"\n-\n-    def _redistribute_per_both_layouts(t, input_layout, output_layout, device_mesh):\n-        dst_placements = (output_layout,)\n-        if isinstance(t, DTensor):\n-            return (\n-                t\n-                if _needs_redistribute(dst_placements, t)\n-                else t.redistribute(device_mesh, dst_placements)\n-            )\n-        elif isinstance(t, torch.Tensor):\n-            dtensor = DTensor.from_local(\n-                t, device_mesh, [input_layout], run_check=False\n-            )\n-            return (\n-                dtensor\n-                if _needs_redistribute(dst_placements, dtensor)\n-                else dtensor.redistribute(device_mesh, dst_placements)\n-            if input_layout is not None:\n-                raise RuntimeError(\n-                    \"Tensor parallel module expects DTensor or tensor\"\n-                    f\" when layout specified but received {type(t)}!\"\n-                )\n-            else:\n-                return t\n-    def make_input_redistribute_1d(\n-        input_layouts: LayoutsType,\n-        output_layouts: LayoutsType,\n-        inputs: Tuple[Any, ...],\n-        device_mesh: Optional[DeviceMesh] = None,\n-    ) -> Optional[Any]:\n-        \"\"\"\n-        Redistribute input tensor over an 1-D device mesh. This function will be used in ParallelStyle.\n-\n-        Args:\n-            input (Union[:class:`torch.Tensor`, :class:`DTensor`]):\n-                This input tensor will be replicated over the 1-D :class:`DeviceMesh`.\n-            device_mesh (:class:`DeviceMesh`, optional):\n-                The 1-D device mesh where ``input`` will be replicated.\n-                If no :class:`DeviceMesh` is passed and ``input`` is a :class:`DTensor`,\n-                ``input.device_mesh`` will be used.\n-                If :class:`DeviceMesh` is not 1-D, an exception will be thrown.\n-                Default: ``None``\n-\n-        Returns:\n-            A :class:`DTensor` replicated over ``device_mesh``.\n-        \"\"\"\n-        # Early return to save CPU overhead when there is only one input.\n-        if not isinstance(inputs, tuple):\n-            return _redistribute_per_both_layouts(\n-                inputs, input_layouts, output_layouts, device_mesh\n-            )\n-        if not isinstance(input_layouts, tuple):\n-            input_layouts = (input_layouts,)  # type: ignore[assignment]\n-            output_layouts = (output_layouts,)  # type: ignore[assignment]\n-        results = []\n-        for input, input_layout, output_layout in zip(\n-            inputs, input_layouts, output_layouts  # type: ignore[arg-type]\n-        ):\n-            results.append(\n-                _redistribute_per_both_layouts(\n-                    input, input_layout, output_layout, device_mesh\n-                )\n-            )\n-        return tuple(results)\n-\n-    return functools.partial(make_input_redistribute_1d, input_layouts, output_layouts)\n-\n-\n-def _get_prepare_output(\n-    output_layouts: LayoutsType, use_local_output: bool\n-) -> Callable[[Any], Any]:\n-    \"\"\"\n-    Get the prepare input function for this parallel style.\n-    \"\"\"\n-\n-    def _redistribute_per_layout(t, layout, device_mesh, use_local_output):\n-        dst_placements = (layout,)\n-        if isinstance(t, DTensor):\n-            dtensor = (\n-                t\n-                if _needs_redistribute(dst_placements, t)\n-                else t.redistribute(device_mesh, dst_placements)\n-            return dtensor.to_local() if use_local_output else dtensor\n-            if layout is not None:\n-                raise RuntimeError(\n-                    \"Tensor parallel module expects DTensor or tensor\"\n-                    f\" when layout specified but received {type(t)}!\"\n-                )\n-            else:\n-                return t\n-\n-    def make_output_redistribute_1d(\n-        output_layouts: LayoutsType,\n-        use_local_output: bool,\n-        outputs: Tuple[Any, ...],\n-        device_mesh: Optional[DeviceMesh] = None,\n-    ) -> Optional[Any]:\n-        \"\"\"\n-        Redistribute input tensor over an 1-D device mesh. This function will be used in ParallelStyle.\n-\n-        Args:\n-            input (Union[:class:`torch.Tensor`, :class:`DTensor`]):\n-                This input tensor will be replicated over the 1-D :class:`DeviceMesh`.\n-            device_mesh (:class:`DeviceMesh`, optional):\n-                The 1-D device mesh where ``input`` will be replicated.\n-                If no :class:`DeviceMesh` is passed and ``input`` is a :class:`DTensor`,\n-                ``input.device_mesh`` will be used.\n-                If :class:`DeviceMesh` is not 1-D, an exception will be thrown.\n-                Default: ``None``\n-\n-        Returns:\n-            A :class:`DTensor` replicated over ``device_mesh``.\n-        \"\"\"\n-        # Early return to save CPU overhead when there is only one output.\n-        if not isinstance(outputs, tuple):\n-            return _redistribute_per_layout(\n-                outputs, output_layouts, device_mesh, use_local_output\n-            )\n-\n-        if not isinstance(output_layouts, tuple):\n-            output_layouts = (output_layouts,)  # type: ignore[assignment]\n-        results = []\n-        for output, output_layout in zip(outputs, output_layouts):  # type: ignore[arg-type]\n-            results.append(\n-                _redistribute_per_layout(\n-                    output, output_layout, device_mesh, use_local_output\n-                )\n-            )\n-        return tuple(results)\n-\n-    return functools.partial(\n-        make_output_redistribute_1d, output_layouts, use_local_output\n-    )\n-        if input_layouts == Shard(-1):\n-            prepare_input_fn = make_input_shard_1d_last_dim\n-        else:\n-            prepare_input_fn = _get_prepare_input(\n-        elif output_layouts == Replicate():\n-            prepare_output_fn = make_output_tensor\n-        else:\n-            prepare_output_fn = _get_prepare_output(\n-                output_layouts,\n-                use_local_output,\n-        \"\"\"\n-\n-        \"\"\"\n-        if input_layouts == Replicate():\n-            prepare_input_fn = make_input_replicate_1d\n-        else:\n-            prepare_input_fn = _get_prepare_input(\n-        elif output_layouts == Shard(-1):\n-            prepare_output_fn = make_sharded_output_tensor\n-        else:\n-            prepare_output_fn = _get_prepare_output(\n-                output_layouts,\n-                use_local_output,\n-\n-        Example::\n-            >>> # xdoctest: +SKIP(failing)\n-            >>> from torch.distributed.tensor.parallel import parallelize_module, PrepareModuleInput\n-            >>> ...\n-            >>> parallelize_plan = {\n-            >>>     \"attn\": PrepareModuleInput(),   # The input of attn will be converted to Sharded DTensor\n-            >>>                                     # and and redistributed to Replicated DTensor.\n-            >>>     ...\n-            >>> }\n-            >>> parallelize_module(\n-            >>>     module=block, # this can be a submodule or module\n-            >>>     ...,\n-            >>>     parallelize_plan=parallelize_plan,\n-            >>> )\n-            >>> ...\n-            _prepare_input=_get_prepare_input(\n-            _prepare_output=_get_prepare_output(output_layouts, use_local_output),\n",
            "whole_added": "+from typing import Any, Optional, Tuple, Union\n+def _redistribute_per_both_layouts(t, input_layout, output_layout, device_mesh):\n+    dst_placements = (output_layout,)\n+    if isinstance(t, DTensor):\n+        return t.redistribute(device_mesh, dst_placements)\n+    elif isinstance(t, torch.Tensor):\n+        dtensor = DTensor.from_local(t, device_mesh, (input_layout,), run_check=False)\n+        return dtensor.redistribute(device_mesh, dst_placements)\n+    else:\n+        if input_layout is not None:\n+            raise RuntimeError(\n+                \"Tensor parallel module expects DTensor or tensor\"\n+                f\" when layout specified but received {type(t)}!\"\n+            return t\n+def _redistribute_per_layout(layout, use_local_output, t, device_mesh):\n+    dst_placements = (layout,)\n+    if isinstance(t, DTensor):\n+        dtensor = t.redistribute(device_mesh, dst_placements)\n+        return dtensor.to_local() if use_local_output else dtensor\n+    else:\n+        if layout is not None:\n+            raise RuntimeError(\n+                \"Tensor parallel module expects DTensor or tensor\"\n+                f\" when layout specified but received {type(t)}!\"\n+            return t\n+        prepare_input_fn = None\n+        elif input_layouts is not None:\n+            prepare_input_fn = functools.partial(\n+                RowwiseParallel._prepare_input_fn,\n+        prepare_output_fn = None\n+        elif output_layouts is not None:\n+            prepare_output_fn = functools.partial(\n+                _redistribute_per_layout, output_layouts, use_local_output\n+    @staticmethod\n+    def _prepare_input_fn(input_layouts, output_layouts, inputs, device_mesh=None):\n+        return (\n+            _redistribute_per_both_layouts(\n+                inputs[0], input_layouts, output_layouts, device_mesh\n+            ),\n+        )\n+\n+        prepare_input_fn = None\n+        elif input_layouts is not None:\n+            prepare_input_fn = functools.partial(\n+                ColwiseParallel._prepare_input_fn,\n+        prepare_output_fn = None\n+        elif output_layouts is not None:\n+            prepare_output_fn = functools.partial(\n+                _redistribute_per_layout, output_layouts, use_local_output\n+    @staticmethod\n+    def _prepare_input_fn(input_layouts, output_layouts, inputs, device_mesh=None):\n+        return (\n+            _redistribute_per_both_layouts(\n+                inputs[0], input_layouts, output_layouts, device_mesh\n+            ),\n+        )\n+\n+    Example::\n+        >>> # xdoctest: +SKIP(failing)\n+        >>> from torch.distributed.tensor.parallel import parallelize_module, PrepareModuleInput\n+        >>> ...\n+        >>> parallelize_plan = {\n+        >>>     \"attn\": PrepareModuleInput(),   # The input of attn will be converted to Sharded DTensor\n+        >>>                                     # and and redistributed to Replicated DTensor.\n+        >>>     ...\n+        >>> }\n+        >>> parallelize_module(\n+        >>>     module=block, # this can be a submodule or module\n+        >>>     ...,\n+        >>>     parallelize_plan=parallelize_plan,\n+        >>> )\n+        >>> ...\n+        if not isinstance(input_layouts, (list, tuple)):\n+            input_layouts = (input_layouts,)  # type: ignore[assignment]\n+\n+        if not isinstance(output_layouts, (list, tuple)):\n+            output_layouts = (output_layouts,)  # type: ignore[assignment]\n+\n+            _prepare_input=functools.partial(\n+                PrepareModuleInput._make_input_redistribute_1d,\n+    @staticmethod\n+    def _make_input_redistribute_1d(\n+        input_layouts: LayoutsType,\n+        output_layouts: LayoutsType,\n+        inputs: Tuple[Any, ...],\n+        device_mesh: Optional[DeviceMesh] = None,\n+    ) -> Optional[Any]:\n+        \"\"\"\n+        Redistribute inputs over a device mesh.\n+        \"\"\"\n+        # Always assume layouts are tuples.\n+        results = []\n+        for input, input_layout, output_layout in zip(\n+            inputs, input_layouts, output_layouts  # type: ignore[arg-type]\n+        ):\n+            results.append(\n+                _redistribute_per_both_layouts(\n+                    input, input_layout, output_layout, device_mesh\n+                )\n+            )\n+        return tuple(results)\n+\n+        \"\"\"\n+        Args:\n+            input_layouts (Union[Placement, Tuple[Placement, ...]]):\n+                The layout of output DTensor(s).\n+            output_layouts (Union[Placement, Tuple[Placement, ...]]):\n+                The layout of output DTensor(s)/tensor(s) which output DTensor(s) will be redistributed to.\n+            use_local_output (bool):\n+                Whether to convert the DTensor to local :class:`torch.Tensor`.\n+\n+        Returns:\n+            None.\n+        \"\"\"\n+        if isinstance(output_layouts, (list, tuple)):\n+            if len(output_layouts) != 1:\n+                raise NotImplementedError(\n+                    \"Only support single output redistribute now.\"\n+                )\n+            else:\n+                output_layouts = output_layouts[0]\n+\n+            _prepare_output=functools.partial(\n+                _redistribute_per_layout, output_layouts, use_local_output\n+            ),\n",
            "whole_hunk": "@@ -1,11 +1,10 @@\n # Copyright (c) Meta Platforms, Inc. and affiliates\n import functools\n from abc import ABC, abstractmethod\n-from typing import Any, Callable, Optional, Tuple, Union\n+from typing import Any, Optional, Tuple, Union\n \n import torch\n from torch.distributed._tensor import DeviceMesh, DTensor, Replicate, Shard\n-from torch.distributed._tensor.placement_types import Placement\n from torch.distributed.tensor.parallel._utils import (\n     _deprecate_warnings,\n     _prepare_input_validate,\n@@ -281,162 +280,36 @@ def make_output_reshard_tensor(\n     return make_output_shard_1d(output, device_mesh).to_local()  # type: ignore[call-arg, attr-defined, misc]\n \n \n-def _needs_redistribute(\n-    dst_placements: Tuple[Placement, ...], dtensor: DTensor\n-) -> bool:\n-    \"\"\"\n-    Check DTensor placements to decide whether the DTensor redistribute\n-    is needed to be called or not. If not, we can directly early return\n-    and save CPU overhead.\n-    \"\"\"\n-    return dtensor._spec.placements == dst_placements\n-\n-\n-def _get_prepare_input(\n-    input_layouts: LayoutsType, output_layouts: LayoutsType\n-) -> Callable[[Any], Any]:\n-    \"\"\"\n-    Get the prepare input function for this parallel style.\n-    \"\"\"\n-\n-    def _redistribute_per_both_layouts(t, input_layout, output_layout, device_mesh):\n-        dst_placements = (output_layout,)\n-        if isinstance(t, DTensor):\n-            return (\n-                t\n-                if _needs_redistribute(dst_placements, t)\n-                else t.redistribute(device_mesh, dst_placements)\n-            )\n-        elif isinstance(t, torch.Tensor):\n-            dtensor = DTensor.from_local(\n-                t, device_mesh, [input_layout], run_check=False\n-            )\n-            return (\n-                dtensor\n-                if _needs_redistribute(dst_placements, dtensor)\n-                else dtensor.redistribute(device_mesh, dst_placements)\n+def _redistribute_per_both_layouts(t, input_layout, output_layout, device_mesh):\n+    dst_placements = (output_layout,)\n+    if isinstance(t, DTensor):\n+        return t.redistribute(device_mesh, dst_placements)\n+    elif isinstance(t, torch.Tensor):\n+        dtensor = DTensor.from_local(t, device_mesh, (input_layout,), run_check=False)\n+        return dtensor.redistribute(device_mesh, dst_placements)\n+    else:\n+        if input_layout is not None:\n+            raise RuntimeError(\n+                \"Tensor parallel module expects DTensor or tensor\"\n+                f\" when layout specified but received {type(t)}!\"\n             )\n         else:\n-            if input_layout is not None:\n-                raise RuntimeError(\n-                    \"Tensor parallel module expects DTensor or tensor\"\n-                    f\" when layout specified but received {type(t)}!\"\n-                )\n-            else:\n-                return t\n+            return t\n \n-    def make_input_redistribute_1d(\n-        input_layouts: LayoutsType,\n-        output_layouts: LayoutsType,\n-        inputs: Tuple[Any, ...],\n-        device_mesh: Optional[DeviceMesh] = None,\n-    ) -> Optional[Any]:\n-        \"\"\"\n-        Redistribute input tensor over an 1-D device mesh. This function will be used in ParallelStyle.\n-\n-        Args:\n-            input (Union[:class:`torch.Tensor`, :class:`DTensor`]):\n-                This input tensor will be replicated over the 1-D :class:`DeviceMesh`.\n-            device_mesh (:class:`DeviceMesh`, optional):\n-                The 1-D device mesh where ``input`` will be replicated.\n-                If no :class:`DeviceMesh` is passed and ``input`` is a :class:`DTensor`,\n-                ``input.device_mesh`` will be used.\n-                If :class:`DeviceMesh` is not 1-D, an exception will be thrown.\n-                Default: ``None``\n-\n-        Returns:\n-            A :class:`DTensor` replicated over ``device_mesh``.\n-        \"\"\"\n-        # Early return to save CPU overhead when there is only one input.\n-        if not isinstance(inputs, tuple):\n-            return _redistribute_per_both_layouts(\n-                inputs, input_layouts, output_layouts, device_mesh\n-            )\n \n-        if not isinstance(input_layouts, tuple):\n-            input_layouts = (input_layouts,)  # type: ignore[assignment]\n-            output_layouts = (output_layouts,)  # type: ignore[assignment]\n-        results = []\n-        for input, input_layout, output_layout in zip(\n-            inputs, input_layouts, output_layouts  # type: ignore[arg-type]\n-        ):\n-            results.append(\n-                _redistribute_per_both_layouts(\n-                    input, input_layout, output_layout, device_mesh\n-                )\n-            )\n-        return tuple(results)\n-\n-    return functools.partial(make_input_redistribute_1d, input_layouts, output_layouts)\n-\n-\n-def _get_prepare_output(\n-    output_layouts: LayoutsType, use_local_output: bool\n-) -> Callable[[Any], Any]:\n-    \"\"\"\n-    Get the prepare input function for this parallel style.\n-    \"\"\"\n-\n-    def _redistribute_per_layout(t, layout, device_mesh, use_local_output):\n-        dst_placements = (layout,)\n-        if isinstance(t, DTensor):\n-            dtensor = (\n-                t\n-                if _needs_redistribute(dst_placements, t)\n-                else t.redistribute(device_mesh, dst_placements)\n+def _redistribute_per_layout(layout, use_local_output, t, device_mesh):\n+    dst_placements = (layout,)\n+    if isinstance(t, DTensor):\n+        dtensor = t.redistribute(device_mesh, dst_placements)\n+        return dtensor.to_local() if use_local_output else dtensor\n+    else:\n+        if layout is not None:\n+            raise RuntimeError(\n+                \"Tensor parallel module expects DTensor or tensor\"\n+                f\" when layout specified but received {type(t)}!\"\n             )\n-            return dtensor.to_local() if use_local_output else dtensor\n         else:\n-            if layout is not None:\n-                raise RuntimeError(\n-                    \"Tensor parallel module expects DTensor or tensor\"\n-                    f\" when layout specified but received {type(t)}!\"\n-                )\n-            else:\n-                return t\n-\n-    def make_output_redistribute_1d(\n-        output_layouts: LayoutsType,\n-        use_local_output: bool,\n-        outputs: Tuple[Any, ...],\n-        device_mesh: Optional[DeviceMesh] = None,\n-    ) -> Optional[Any]:\n-        \"\"\"\n-        Redistribute input tensor over an 1-D device mesh. This function will be used in ParallelStyle.\n-\n-        Args:\n-            input (Union[:class:`torch.Tensor`, :class:`DTensor`]):\n-                This input tensor will be replicated over the 1-D :class:`DeviceMesh`.\n-            device_mesh (:class:`DeviceMesh`, optional):\n-                The 1-D device mesh where ``input`` will be replicated.\n-                If no :class:`DeviceMesh` is passed and ``input`` is a :class:`DTensor`,\n-                ``input.device_mesh`` will be used.\n-                If :class:`DeviceMesh` is not 1-D, an exception will be thrown.\n-                Default: ``None``\n-\n-        Returns:\n-            A :class:`DTensor` replicated over ``device_mesh``.\n-        \"\"\"\n-        # Early return to save CPU overhead when there is only one output.\n-        if not isinstance(outputs, tuple):\n-            return _redistribute_per_layout(\n-                outputs, output_layouts, device_mesh, use_local_output\n-            )\n-\n-        if not isinstance(output_layouts, tuple):\n-            output_layouts = (output_layouts,)  # type: ignore[assignment]\n-        results = []\n-        for output, output_layout in zip(outputs, output_layouts):  # type: ignore[arg-type]\n-            results.append(\n-                _redistribute_per_layout(\n-                    output, output_layout, device_mesh, use_local_output\n-                )\n-            )\n-        return tuple(results)\n-\n-    return functools.partial(\n-        make_output_redistribute_1d, output_layouts, use_local_output\n-    )\n+            return t\n \n \n class RowwiseParallel(ParallelStyle):\n@@ -494,24 +367,22 @@ class RowwiseParallel(ParallelStyle):\n                 \"RowwiseParallel only supports single input/output.\"\n             )\n \n+        prepare_input_fn = None\n         if _prepare_input is not None:\n             prepare_input_fn = _prepare_input\n-        if input_layouts == Shard(-1):\n-            prepare_input_fn = make_input_shard_1d_last_dim\n-        else:\n-            prepare_input_fn = _get_prepare_input(\n+        elif input_layouts is not None:\n+            prepare_input_fn = functools.partial(\n+                RowwiseParallel._prepare_input_fn,\n                 input_layouts,\n                 Shard(-1),\n             )\n \n+        prepare_output_fn = None\n         if _prepare_output is not None:\n             prepare_output_fn = _prepare_output\n-        elif output_layouts == Replicate():\n-            prepare_output_fn = make_output_tensor\n-        else:\n-            prepare_output_fn = _get_prepare_output(\n-                output_layouts,\n-                use_local_output,\n+        elif output_layouts is not None:\n+            prepare_output_fn = functools.partial(\n+                _redistribute_per_layout, output_layouts, use_local_output\n             )\n \n         super().__init__(\n@@ -522,6 +393,14 @@ class RowwiseParallel(ParallelStyle):\n             _prepare_output=prepare_output_fn,\n         )\n \n+    @staticmethod\n+    def _prepare_input_fn(input_layouts, output_layouts, inputs, device_mesh=None):\n+        return (\n+            _redistribute_per_both_layouts(\n+                inputs[0], input_layouts, output_layouts, device_mesh\n+            ),\n+        )\n+\n \n class ColwiseParallel(ParallelStyle):\n     \"\"\"\n@@ -573,32 +452,27 @@ class ColwiseParallel(ParallelStyle):\n         output_layouts=Shard(-1),\n         use_local_output=True,\n     ) -> None:\n-        \"\"\"\n-\n-        \"\"\"\n         if isinstance(input_layouts, tuple) or isinstance(output_layouts, tuple):\n             raise NotImplementedError(\n                 \"ColwiseParallel only supports single input/output.\"\n             )\n \n+        prepare_input_fn = None\n         if _prepare_input is not None:\n             prepare_input_fn = _prepare_input\n-        if input_layouts == Replicate():\n-            prepare_input_fn = make_input_replicate_1d\n-        else:\n-            prepare_input_fn = _get_prepare_input(\n+        elif input_layouts is not None:\n+            prepare_input_fn = functools.partial(\n+                ColwiseParallel._prepare_input_fn,\n                 input_layouts,\n                 Replicate(),\n             )\n \n+        prepare_output_fn = None\n         if _prepare_output is not None:\n             prepare_output_fn = _prepare_output\n-        elif output_layouts == Shard(-1):\n-            prepare_output_fn = make_sharded_output_tensor\n-        else:\n-            prepare_output_fn = _get_prepare_output(\n-                output_layouts,\n-                use_local_output,\n+        elif output_layouts is not None:\n+            prepare_output_fn = functools.partial(\n+                _redistribute_per_layout, output_layouts, use_local_output\n             )\n \n         super().__init__(\n@@ -609,6 +483,13 @@ class ColwiseParallel(ParallelStyle):\n             _prepare_output=prepare_output_fn,\n         )\n \n+    @staticmethod\n+    def _prepare_input_fn(input_layouts, output_layouts, inputs, device_mesh=None):\n+        return (\n+            _redistribute_per_both_layouts(\n+                inputs[0], input_layouts, output_layouts, device_mesh\n+            ),\n+        )\n \n class PrepareModuleInput(ParallelStyle):\n     \"\"\"\n@@ -620,6 +501,22 @@ class PrepareModuleInput(ParallelStyle):\n \n     When the input is not a :class:`torch.Tensor` or :class:`DTensor`, if no layout is\n     specified, it will be a no-op. Otherwise, it will throw an error.\n+\n+    Example::\n+        >>> # xdoctest: +SKIP(failing)\n+        >>> from torch.distributed.tensor.parallel import parallelize_module, PrepareModuleInput\n+        >>> ...\n+        >>> parallelize_plan = {\n+        >>>     \"attn\": PrepareModuleInput(),   # The input of attn will be converted to Sharded DTensor\n+        >>>                                     # and and redistributed to Replicated DTensor.\n+        >>>     ...\n+        >>> }\n+        >>> parallelize_module(\n+        >>>     module=block, # this can be a submodule or module\n+        >>>     ...,\n+        >>>     parallelize_plan=parallelize_plan,\n+        >>> )\n+        >>> ...\n     \"\"\"\n \n     def __init__(\n@@ -639,34 +536,47 @@ class PrepareModuleInput(ParallelStyle):\n \n         Returns:\n             None.\n-\n-        Example::\n-            >>> # xdoctest: +SKIP(failing)\n-            >>> from torch.distributed.tensor.parallel import parallelize_module, PrepareModuleInput\n-            >>> ...\n-            >>> parallelize_plan = {\n-            >>>     \"attn\": PrepareModuleInput(),   # The input of attn will be converted to Sharded DTensor\n-            >>>                                     # and and redistributed to Replicated DTensor.\n-            >>>     ...\n-            >>> }\n-            >>> parallelize_module(\n-            >>>     module=block, # this can be a submodule or module\n-            >>>     ...,\n-            >>>     parallelize_plan=parallelize_plan,\n-            >>> )\n-            >>> ...\n         \"\"\"\n+        if not isinstance(input_layouts, (list, tuple)):\n+            input_layouts = (input_layouts,)  # type: ignore[assignment]\n+\n+        if not isinstance(output_layouts, (list, tuple)):\n+            output_layouts = (output_layouts,)  # type: ignore[assignment]\n+\n         super().__init__(\n             input_layouts=input_layouts,\n             output_layouts=output_layouts,\n             use_local_output=use_local_output,\n-            _prepare_input=_get_prepare_input(\n+            _prepare_input=functools.partial(\n+                PrepareModuleInput._make_input_redistribute_1d,\n                 input_layouts,\n                 output_layouts,\n             ),\n             _prepare_output=None,\n         )\n \n+    @staticmethod\n+    def _make_input_redistribute_1d(\n+        input_layouts: LayoutsType,\n+        output_layouts: LayoutsType,\n+        inputs: Tuple[Any, ...],\n+        device_mesh: Optional[DeviceMesh] = None,\n+    ) -> Optional[Any]:\n+        \"\"\"\n+        Redistribute inputs over a device mesh.\n+        \"\"\"\n+        # Always assume layouts are tuples.\n+        results = []\n+        for input, input_layout, output_layout in zip(\n+            inputs, input_layouts, output_layouts  # type: ignore[arg-type]\n+        ):\n+            results.append(\n+                _redistribute_per_both_layouts(\n+                    input, input_layout, output_layout, device_mesh\n+                )\n+            )\n+        return tuple(results)\n+\n \n class PrepareModuleOutput(ParallelStyle):\n     \"\"\"\n@@ -702,10 +612,32 @@ class PrepareModuleOutput(ParallelStyle):\n         output_layouts: LayoutsType = Shard(0),\n         use_local_output: bool = True,\n     ) -> None:\n+        \"\"\"\n+        Args:\n+            input_layouts (Union[Placement, Tuple[Placement, ...]]):\n+                The layout of output DTensor(s).\n+            output_layouts (Union[Placement, Tuple[Placement, ...]]):\n+                The layout of output DTensor(s)/tensor(s) which output DTensor(s) will be redistributed to.\n+            use_local_output (bool):\n+                Whether to convert the DTensor to local :class:`torch.Tensor`.\n+\n+        Returns:\n+            None.\n+        \"\"\"\n+        if isinstance(output_layouts, (list, tuple)):\n+            if len(output_layouts) != 1:\n+                raise NotImplementedError(\n+                    \"Only support single output redistribute now.\"\n+                )\n+            else:\n+                output_layouts = output_layouts[0]\n+\n         super().__init__(\n             input_layouts=input_layouts,\n             output_layouts=output_layouts,\n             use_local_output=use_local_output,\n             _prepare_input=None,\n-            _prepare_output=_get_prepare_output(output_layouts, use_local_output),\n+            _prepare_output=functools.partial(\n+                _redistribute_per_layout, output_layouts, use_local_output\n+            ),\n         )"
        }
    ]
},
{
    "Id": 257,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/b1657beac174811acb03024327b8d6e972e9eca9",
    "date": "2024-03-07T23:26:03+00:00",
    "message": "feat: Add min, max ranges to mark_dynamic API (#119737)\n\nFixes https://github.com/pytorch/pytorch/issues/115137\n\nThis PR adds:\n\n- mark_dynamic API will accept `min`, `max` values to create a bounded constraint on the dim.\n- test case in test_misc.py which checks if `ConstraintViolationError` is triggered if `torch.compile` gets a input dimension out of bounds.\n\nCo-authored-by: Edward Z. Yang <ezyang@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/119737\nApproved by: https://github.com/ezyang, https://github.com/jansel",
    "label": "NO",
    "changes": [
        {
            "name": "torch.compiler_dynamic_shapes.rst",
            "path": "docs/source/torch.compiler_dynamic_shapes.rst",
            "patches": [
                {
                    "old_start": 32,
                    "old_length": 7,
                    "new_start": 32,
                    "new_length": 8,
                    "hunk": "@@ -32,7 +32,8 @@ The default dynamic behavior in PyTorch 2.1 is:\n   when guards are added and why.\n \n - If you know ahead of time something will be dynamic, you can skip the first\n-  recompile with ``torch._dynamo.mark_dynamic(tensor, dim)``.\n+  recompile with ``torch._dynamo.mark_dynamic(tensor, dim)``. If you know ahead of time\n+  the ``min`` and ``max`` value this dimension can take, you can specify ``torch._dynamo.mark_dynamic(tensor, dim, min=min, max=max)``\n \n - If you say ``torch.compile(dynamic=False)``, we will turn off automatic\n   dynamic shapes on recompiles and always recompile for each distinct size.\n"
                }
            ],
            "whole_deleted": "-  recompile with ``torch._dynamo.mark_dynamic(tensor, dim)``.\n",
            "whole_added": "+  recompile with ``torch._dynamo.mark_dynamic(tensor, dim)``. If you know ahead of time\n+  the ``min`` and ``max`` value this dimension can take, you can specify ``torch._dynamo.mark_dynamic(tensor, dim, min=min, max=max)``\n",
            "whole_hunk": "@@ -32,7 +32,8 @@ The default dynamic behavior in PyTorch 2.1 is:\n   when guards are added and why.\n \n - If you know ahead of time something will be dynamic, you can skip the first\n-  recompile with ``torch._dynamo.mark_dynamic(tensor, dim)``.\n+  recompile with ``torch._dynamo.mark_dynamic(tensor, dim)``. If you know ahead of time\n+  the ``min`` and ``max`` value this dimension can take, you can specify ``torch._dynamo.mark_dynamic(tensor, dim, min=min, max=max)``\n \n - If you say ``torch.compile(dynamic=False)``, we will turn off automatic\n   dynamic shapes on recompiles and always recompile for each distinct size.\n"
        },
        {
            "name": "test_misc.py",
            "path": "test/dynamo/test_misc.py",
            "patches": [
                {
                    "old_start": 7123,
                    "old_length": 6,
                    "new_start": 7123,
                    "new_length": 20,
                    "hunk": "@@ -7123,6 +7123,20 @@ def fn():\n         with self.assertRaises(ConstraintViolationError):\n             torch._dynamo.optimize(\"eager\")(my_dyn_fn)(y)\n \n+    # Translation validation changes the exception type, don't run with it\n+    @torch.fx.experimental._config.patch(translation_validation=False)\n+    def test_mark_dynamic_with_ranges(self):\n+        y = torch.randn([8, 3, 3])\n+\n+        def my_dyn_fn(x):\n+            if x.shape[0] == 3:\n+                return x.sin()\n+            return x.cos()\n+\n+        torch._dynamo.mark_dynamic(y, 0, min=2, max=5)\n+        with self.assertRaises(ConstraintViolationError):\n+            torch._dynamo.optimize(\"eager\")(my_dyn_fn)(y)\n+\n     def test_mark_static(self):\n         counter = CompileCounter()\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    # Translation validation changes the exception type, don't run with it\n+    @torch.fx.experimental._config.patch(translation_validation=False)\n+    def test_mark_dynamic_with_ranges(self):\n+        y = torch.randn([8, 3, 3])\n+\n+        def my_dyn_fn(x):\n+            if x.shape[0] == 3:\n+                return x.sin()\n+            return x.cos()\n+\n+        torch._dynamo.mark_dynamic(y, 0, min=2, max=5)\n+        with self.assertRaises(ConstraintViolationError):\n+            torch._dynamo.optimize(\"eager\")(my_dyn_fn)(y)\n+\n",
            "whole_hunk": "@@ -7123,6 +7123,20 @@ def fn():\n         with self.assertRaises(ConstraintViolationError):\n             torch._dynamo.optimize(\"eager\")(my_dyn_fn)(y)\n \n+    # Translation validation changes the exception type, don't run with it\n+    @torch.fx.experimental._config.patch(translation_validation=False)\n+    def test_mark_dynamic_with_ranges(self):\n+        y = torch.randn([8, 3, 3])\n+\n+        def my_dyn_fn(x):\n+            if x.shape[0] == 3:\n+                return x.sin()\n+            return x.cos()\n+\n+        torch._dynamo.mark_dynamic(y, 0, min=2, max=5)\n+        with self.assertRaises(ConstraintViolationError):\n+            torch._dynamo.optimize(\"eager\")(my_dyn_fn)(y)\n+\n     def test_mark_static(self):\n         counter = CompileCounter()\n \n"
        },
        {
            "name": "decorators.py",
            "path": "torch/_dynamo/decorators.py",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 3,
                    "new_start": 1,
                    "new_length": 4,
                    "hunk": "@@ -1,3 +1,4 @@\n+from dataclasses import dataclass\n from typing import TYPE_CHECKING\n \n import torch\n"
                },
                {
                    "old_start": 168,
                    "old_length": 20,
                    "new_start": 169,
                    "new_length": 33,
                    "hunk": "@@ -168,20 +169,33 @@ def forbid_in_graph(fn):\n # Helper function to flatten a tensor subclass and apply a function to\n # all inner tensors that match the outer dim. Used to reduce duplication\n # across the various marking APIs.\n-def _apply_func_to_inner_tensors_of_same_dim(func, t, *args):\n+def _apply_func_to_inner_tensors_of_same_dim(func, t, *args, **kwargs):\n     assert is_traceable_wrapper_subclass(t)\n \n     attrs, ctx = t.__tensor_flatten__()\n     for attr in attrs:\n         inner = getattr(t, attr)\n         if inner.dim() == t.dim():\n-            func(inner, *args)\n+            func(inner, *args, **kwargs)\n+\n+\n+@dataclass(frozen=True)\n+class _DimRange:\n+    \"\"\"\n+    This represents an dimension of a tensor and the corresponding\n+    min and max values it can take.  Don't create this\n+    class directly; instead, use :func:`mark_dynamic`.\n+    \"\"\"\n+\n+    dim: int\n+    min: int\n+    max: int\n \n \n @forbid_in_graph\n-def mark_dynamic(t, index):\n+def mark_dynamic(t, index, *, min=None, max=None):\n     \"\"\"\n-    Mark a tensor as having a dynamic dim.\n+    Mark a tensor as having a dynamic dim and set corresponding min and max range for the dim.\n \n     [Note - on the state of mark_dynamic]\n \n"
                },
                {
                    "old_start": 206,
                    "old_length": 18,
                    "new_start": 220,
                    "new_length": 22,
                    "hunk": "@@ -206,18 +220,22 @@ def mark_dynamic(t, index):\n     if is_traceable_wrapper_subclass(t):\n         # default behavior: mirror mark_dynamic() on all inner tensors with same dim as t\n         # TODO: Make this configurable via a supported public API\n-        _apply_func_to_inner_tensors_of_same_dim(mark_dynamic, t, index)\n+        _apply_func_to_inner_tensors_of_same_dim(\n+            mark_dynamic, t, index, min=min, max=max\n+        )\n \n     if isinstance(index, int):\n         if not hasattr(t, \"_dynamo_dynamic_indices\"):\n             t._dynamo_dynamic_indices = set()\n+            t._dynamo_dynamic_range = set()\n         # TODO(voz): Should we bounds check?\n         t._dynamo_dynamic_indices.add(index)\n+        t._dynamo_dynamic_range.add(_DimRange(index, min, max))\n         return\n \n     assert isinstance(index, (list, tuple))\n     for i in index:\n-        mark_dynamic(t, i)\n+        mark_dynamic(t, i, min=min, max=max)\n \n \n @forbid_in_graph\n"
                }
            ],
            "whole_deleted": "-def _apply_func_to_inner_tensors_of_same_dim(func, t, *args):\n-            func(inner, *args)\n-def mark_dynamic(t, index):\n-    Mark a tensor as having a dynamic dim.\n-        _apply_func_to_inner_tensors_of_same_dim(mark_dynamic, t, index)\n-        mark_dynamic(t, i)\n",
            "whole_added": "+from dataclasses import dataclass\n+def _apply_func_to_inner_tensors_of_same_dim(func, t, *args, **kwargs):\n+            func(inner, *args, **kwargs)\n+\n+\n+@dataclass(frozen=True)\n+class _DimRange:\n+    \"\"\"\n+    This represents an dimension of a tensor and the corresponding\n+    min and max values it can take.  Don't create this\n+    class directly; instead, use :func:`mark_dynamic`.\n+    \"\"\"\n+\n+    dim: int\n+    min: int\n+    max: int\n+def mark_dynamic(t, index, *, min=None, max=None):\n+    Mark a tensor as having a dynamic dim and set corresponding min and max range for the dim.\n+        _apply_func_to_inner_tensors_of_same_dim(\n+            mark_dynamic, t, index, min=min, max=max\n+        )\n+            t._dynamo_dynamic_range = set()\n+        t._dynamo_dynamic_range.add(_DimRange(index, min, max))\n+        mark_dynamic(t, i, min=min, max=max)\n",
            "whole_hunk": "@@ -1,3 +1,4 @@\n+from dataclasses import dataclass\n from typing import TYPE_CHECKING\n \n import torch\n@@ -168,20 +169,33 @@ def forbid_in_graph(fn):\n # Helper function to flatten a tensor subclass and apply a function to\n # all inner tensors that match the outer dim. Used to reduce duplication\n # across the various marking APIs.\n-def _apply_func_to_inner_tensors_of_same_dim(func, t, *args):\n+def _apply_func_to_inner_tensors_of_same_dim(func, t, *args, **kwargs):\n     assert is_traceable_wrapper_subclass(t)\n \n     attrs, ctx = t.__tensor_flatten__()\n     for attr in attrs:\n         inner = getattr(t, attr)\n         if inner.dim() == t.dim():\n-            func(inner, *args)\n+            func(inner, *args, **kwargs)\n+\n+\n+@dataclass(frozen=True)\n+class _DimRange:\n+    \"\"\"\n+    This represents an dimension of a tensor and the corresponding\n+    min and max values it can take.  Don't create this\n+    class directly; instead, use :func:`mark_dynamic`.\n+    \"\"\"\n+\n+    dim: int\n+    min: int\n+    max: int\n \n \n @forbid_in_graph\n-def mark_dynamic(t, index):\n+def mark_dynamic(t, index, *, min=None, max=None):\n     \"\"\"\n-    Mark a tensor as having a dynamic dim.\n+    Mark a tensor as having a dynamic dim and set corresponding min and max range for the dim.\n \n     [Note - on the state of mark_dynamic]\n \n@@ -206,18 +220,22 @@ def mark_dynamic(t, index):\n     if is_traceable_wrapper_subclass(t):\n         # default behavior: mirror mark_dynamic() on all inner tensors with same dim as t\n         # TODO: Make this configurable via a supported public API\n-        _apply_func_to_inner_tensors_of_same_dim(mark_dynamic, t, index)\n+        _apply_func_to_inner_tensors_of_same_dim(\n+            mark_dynamic, t, index, min=min, max=max\n+        )\n \n     if isinstance(index, int):\n         if not hasattr(t, \"_dynamo_dynamic_indices\"):\n             t._dynamo_dynamic_indices = set()\n+            t._dynamo_dynamic_range = set()\n         # TODO(voz): Should we bounds check?\n         t._dynamo_dynamic_indices.add(index)\n+        t._dynamo_dynamic_range.add(_DimRange(index, min, max))\n         return\n \n     assert isinstance(index, (list, tuple))\n     for i in index:\n-        mark_dynamic(t, i)\n+        mark_dynamic(t, i, min=min, max=max)\n \n \n @forbid_in_graph\n"
        },
        {
            "name": "builder.py",
            "path": "torch/_dynamo/variables/builder.py",
            "patches": [
                {
                    "old_start": 15,
                    "old_length": 6,
                    "new_start": 15,
                    "new_length": 8,
                    "hunk": "@@ -15,6 +15,8 @@ import sys\n import types\n from typing import List, NamedTuple, Optional, Union\n \n+from torch.utils._sympy.value_ranges import ValueRanges\n+\n try:\n     import numpy as np\n except ModuleNotFoundError:\n"
                },
                {
                    "old_start": 1779,
                    "old_length": 7,
                    "new_start": 1781,
                    "new_length": 24,
                    "hunk": "@@ -1779,7 +1781,24 @@ def _automatic_dynamic(\n         constraint = dim2constraint.get(i)\n         if constraint is None:\n             if marked_dynamic and not config.allow_ignore_mark_dynamic:\n-                constraint_dim = RelaxedUnspecConstraint(warn_only=False)\n+                if hasattr(e, \"_dynamo_dynamic_range\"):\n+                    dim_range = [\n+                        dr for dr in e._dynamo_dynamic_range if dr.dim == i\n+                    ].pop()\n+                    if dim_range.min is None and dim_range.max is None:\n+                        constraint_dim = RelaxedUnspecConstraint(warn_only=False)\n+                    else:\n+                        from torch.fx.experimental.symbolic_shapes import (\n+                            StrictMinMaxConstraint,\n+                        )\n+\n+                        constraint_dim = StrictMinMaxConstraint(\n+                            vr=ValueRanges(lower=dim_range.min, upper=dim_range.max),\n+                            warn_only=False,\n+                        )\n+                else:\n+                    constraint_dim = RelaxedUnspecConstraint(warn_only=False)\n+\n             elif not marked_static and automatic_dynamic:\n                 constraint_dim = RelaxedUnspecConstraint(warn_only=True)\n             else:"
                }
            ],
            "whole_deleted": "-                constraint_dim = RelaxedUnspecConstraint(warn_only=False)\n",
            "whole_added": "+from torch.utils._sympy.value_ranges import ValueRanges\n+\n+                if hasattr(e, \"_dynamo_dynamic_range\"):\n+                    dim_range = [\n+                        dr for dr in e._dynamo_dynamic_range if dr.dim == i\n+                    ].pop()\n+                    if dim_range.min is None and dim_range.max is None:\n+                        constraint_dim = RelaxedUnspecConstraint(warn_only=False)\n+                    else:\n+                        from torch.fx.experimental.symbolic_shapes import (\n+                            StrictMinMaxConstraint,\n+                        )\n+\n+                        constraint_dim = StrictMinMaxConstraint(\n+                            vr=ValueRanges(lower=dim_range.min, upper=dim_range.max),\n+                            warn_only=False,\n+                        )\n+                else:\n+                    constraint_dim = RelaxedUnspecConstraint(warn_only=False)\n+\n",
            "whole_hunk": "@@ -15,6 +15,8 @@ import sys\n import types\n from typing import List, NamedTuple, Optional, Union\n \n+from torch.utils._sympy.value_ranges import ValueRanges\n+\n try:\n     import numpy as np\n except ModuleNotFoundError:\n@@ -1779,7 +1781,24 @@ def _automatic_dynamic(\n         constraint = dim2constraint.get(i)\n         if constraint is None:\n             if marked_dynamic and not config.allow_ignore_mark_dynamic:\n-                constraint_dim = RelaxedUnspecConstraint(warn_only=False)\n+                if hasattr(e, \"_dynamo_dynamic_range\"):\n+                    dim_range = [\n+                        dr for dr in e._dynamo_dynamic_range if dr.dim == i\n+                    ].pop()\n+                    if dim_range.min is None and dim_range.max is None:\n+                        constraint_dim = RelaxedUnspecConstraint(warn_only=False)\n+                    else:\n+                        from torch.fx.experimental.symbolic_shapes import (\n+                            StrictMinMaxConstraint,\n+                        )\n+\n+                        constraint_dim = StrictMinMaxConstraint(\n+                            vr=ValueRanges(lower=dim_range.min, upper=dim_range.max),\n+                            warn_only=False,\n+                        )\n+                else:\n+                    constraint_dim = RelaxedUnspecConstraint(warn_only=False)\n+\n             elif not marked_static and automatic_dynamic:\n                 constraint_dim = RelaxedUnspecConstraint(warn_only=True)\n             else:"
        }
    ]
},
{
    "Id": 487,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/6b4c686b9a33a1503a4a4133f9067dd31e0822f7",
    "date": "2023-10-08T04:06:44+00:00",
    "message": "[aotindutor] Forward fix a performance regression (#110800)\n\nSummary: Forward fix a performance regression caused by https://github.com/pytorch/pytorch/pull/110510. When a model is run once, all those kernel pointers are initialized and removing the if-nullptr check will cause those loadKernel be unnecessarily executed again when we rerun the foward function. Another way to do this is to codegen loadKernel in the initializer, which I may do in a later PR.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110800\nApproved by: https://github.com/jansel",
    "label": "YES",
    "changes": [
        {
            "name": "wrapper.py",
            "path": "torch/_inductor/codegen/wrapper.py",
            "patches": [
                {
                    "old_start": 2021,
                    "old_length": 13,
                    "new_start": 2021,
                    "new_length": 17,
                    "hunk": "@@ -2021,13 +2021,17 @@ class CudaWrapperCodeGen(CppWrapperCodeGen):\n         self, name: str, mangled_name: str, cubin_path: str, shared_mem: int\n     ):\n         if V.graph.aot_mode:\n+            self.writeline(f\"if (kernels.{name} == nullptr) {{\")\n             self.writeline(\n-                f\"\"\"kernels.{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem}, this->cubin_dir_);\"\"\"\n+                f\"\"\"    kernels.{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem}, this->cubin_dir_);\"\"\"\n             )\n+            self.writeline(\"}\")\n         else:\n+            self.writeline(f\"if ({name} == nullptr) {{\")\n             self.writeline(\n-                f\"\"\"{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem});\"\"\"\n+                f\"\"\"    {name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem});\"\"\"\n             )\n+            self.writeline(\"}\")\n \n     def generate_args_decl(self, call_args):\n         dynamic_symbols = V.graph.sizevars.free_symbols()"
                }
            ],
            "whole_deleted": "-                f\"\"\"kernels.{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem}, this->cubin_dir_);\"\"\"\n-                f\"\"\"{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem});\"\"\"\n",
            "whole_added": "+            self.writeline(f\"if (kernels.{name} == nullptr) {{\")\n+                f\"\"\"    kernels.{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem}, this->cubin_dir_);\"\"\"\n+            self.writeline(\"}\")\n+            self.writeline(f\"if ({name} == nullptr) {{\")\n+                f\"\"\"    {name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem});\"\"\"\n+            self.writeline(\"}\")\n",
            "whole_hunk": "@@ -2021,13 +2021,17 @@ class CudaWrapperCodeGen(CppWrapperCodeGen):\n         self, name: str, mangled_name: str, cubin_path: str, shared_mem: int\n     ):\n         if V.graph.aot_mode:\n+            self.writeline(f\"if (kernels.{name} == nullptr) {{\")\n             self.writeline(\n-                f\"\"\"kernels.{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem}, this->cubin_dir_);\"\"\"\n+                f\"\"\"    kernels.{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem}, this->cubin_dir_);\"\"\"\n             )\n+            self.writeline(\"}\")\n         else:\n+            self.writeline(f\"if ({name} == nullptr) {{\")\n             self.writeline(\n-                f\"\"\"{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem});\"\"\"\n+                f\"\"\"    {name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem});\"\"\"\n             )\n+            self.writeline(\"}\")\n \n     def generate_args_decl(self, call_args):\n         dynamic_symbols = V.graph.sizevars.free_symbols()"
        }
    ]
},
{
    "Id": 192,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/e0e2d897ed519e65b097fa811abb5db04e6576b2",
    "date": "2024-04-24T12:18:33+00:00",
    "message": "Handle Tensor returns in PropagateUnbackedSymInts (#124297)\n\nThis subsumes https://github.com/pytorch/pytorch/pull/124069\n\nIn the original PR, my idea was that when we run PropagateUnbackedSymInts, we check that the sizes before and after are exactly the same. This ended up turning up lots of bugs that I didn't feel like fixing. Separately, Ivan let me know that this pass was quite expensive in terms of compile time, since we spent a lot of time thinking about the equalities.\n\nTo kill two birds with one stone, we now only check for equality precisely when an unbacked SymInt was bound (thanks to the previous PR in this stack, we now have this information). Specifically, we look to see if `meta[\"unbacked_bindings\"]` is set on the old node, and if it is, we assert the old value is equal to the new value from the repropagation. Note that the pytree key is used to actually extract the new value from the example value, as it may be nested inside an, e.g., tensor size.\n\nWe do something a bit naughty at the end: we use `defer_runtime_assert` to actually teach ShapeEnv about the equality. This is implementationally equivalent to what we used to do, but we're going to change this later soon.\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/124297\nApproved by: https://github.com/lezcano\nghstack dependencies: #124290",
    "label": "YES",
    "changes": [
        {
            "name": "fx.experimental.rst",
            "path": "docs/source/fx.experimental.rst",
            "patches": [
                {
                    "old_start": 46,
                    "old_length": 3,
                    "new_start": 46,
                    "new_length": 4,
                    "hunk": "@@ -46,3 +46,4 @@ torch.fx.experimental.symbolic_shapes\n     lru_cache\n     check_consistent\n     compute_unbacked_bindings\n+    rebind_unbacked\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    rebind_unbacked\n",
            "whole_hunk": "@@ -46,3 +46,4 @@ torch.fx.experimental.symbolic_shapes\n     lru_cache\n     check_consistent\n     compute_unbacked_bindings\n+    rebind_unbacked\n"
        },
        {
            "name": "test_misc.py",
            "path": "test/dynamo/test_misc.py",
            "patches": [
                {
                    "old_start": 8514,
                    "old_length": 6,
                    "new_start": 8514,
                    "new_length": 16,
                    "hunk": "@@ -8514,6 +8514,16 @@ def ___make_guard_fn():\n             RuntimeError, lambda: fn(torch.randn(2, 3), torch.tensor([1]))\n         )\n \n+    @torch._dynamo.config.patch(\n+        capture_scalar_outputs=True, capture_dynamic_output_shape_ops=True\n+    )\n+    def test_aot_autograd_propagate_unbacked_symints_shape(self):\n+        @torch.compile(backend=\"aot_eager\")\n+        def f(x):\n+            return torch.nonzero(x)\n+\n+        f(torch.tensor([1, 0, 3, 2, 0]))\n+\n     def test_simple_set_usage(self):\n         def foo(x, y):\n             setty = {x, y}\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    @torch._dynamo.config.patch(\n+        capture_scalar_outputs=True, capture_dynamic_output_shape_ops=True\n+    )\n+    def test_aot_autograd_propagate_unbacked_symints_shape(self):\n+        @torch.compile(backend=\"aot_eager\")\n+        def f(x):\n+            return torch.nonzero(x)\n+\n+        f(torch.tensor([1, 0, 3, 2, 0]))\n+\n",
            "whole_hunk": "@@ -8514,6 +8514,16 @@ def ___make_guard_fn():\n             RuntimeError, lambda: fn(torch.randn(2, 3), torch.tensor([1]))\n         )\n \n+    @torch._dynamo.config.patch(\n+        capture_scalar_outputs=True, capture_dynamic_output_shape_ops=True\n+    )\n+    def test_aot_autograd_propagate_unbacked_symints_shape(self):\n+        @torch.compile(backend=\"aot_eager\")\n+        def f(x):\n+            return torch.nonzero(x)\n+\n+        f(torch.tensor([1, 0, 3, 2, 0]))\n+\n     def test_simple_set_usage(self):\n         def foo(x, y):\n             setty = {x, y}\n"
        },
        {
            "name": "test_torchinductor_dynamic_shapes.py",
            "path": "test/inductor/test_torchinductor_dynamic_shapes.py",
            "patches": [
                {
                    "old_start": 218,
                    "old_length": 6,
                    "new_start": 218,
                    "new_length": 15,
                    "hunk": "@@ -218,6 +218,15 @@ class TestInductorDynamic(TestCase):\n         opt_r = opt_f(x, b)\n         self.assertEqual(r, opt_r)\n \n+    @torch._dynamo.config.patch(capture_dynamic_output_shape_ops=True)\n+    def test_nonzero_no_realloc(self, device):\n+        @torch.compile(fullgraph=True, dynamic=True)\n+        def f(x, y):\n+            z = x.nonzero()\n+            return torch.split(z, [y.size(0)])\n+\n+        f(torch.tensor([1, 0, 1, 1, 0, 1, 0]), torch.randn(4))\n+\n     @torch._dynamo.config.patch(capture_scalar_outputs=True)\n     def test_item_nobreak(self, device):\n         @torch.compile(fullgraph=True)\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    @torch._dynamo.config.patch(capture_dynamic_output_shape_ops=True)\n+    def test_nonzero_no_realloc(self, device):\n+        @torch.compile(fullgraph=True, dynamic=True)\n+        def f(x, y):\n+            z = x.nonzero()\n+            return torch.split(z, [y.size(0)])\n+\n+        f(torch.tensor([1, 0, 1, 1, 0, 1, 0]), torch.randn(4))\n+\n",
            "whole_hunk": "@@ -218,6 +218,15 @@ class TestInductorDynamic(TestCase):\n         opt_r = opt_f(x, b)\n         self.assertEqual(r, opt_r)\n \n+    @torch._dynamo.config.patch(capture_dynamic_output_shape_ops=True)\n+    def test_nonzero_no_realloc(self, device):\n+        @torch.compile(fullgraph=True, dynamic=True)\n+        def f(x, y):\n+            z = x.nonzero()\n+            return torch.split(z, [y.size(0)])\n+\n+        f(torch.tensor([1, 0, 1, 1, 0, 1, 0]), torch.randn(4))\n+\n     @torch._dynamo.config.patch(capture_scalar_outputs=True)\n     def test_item_nobreak(self, device):\n         @torch.compile(fullgraph=True)\n"
        },
        {
            "name": "traced_function_transforms.py",
            "path": "torch/_functorch/_aot_autograd/traced_function_transforms.py",
            "patches": [
                {
                    "old_start": 23,
                    "old_length": 7,
                    "new_start": 23,
                    "new_length": 11,
                    "hunk": "@@ -23,7 +23,11 @@ from torch import Tensor\n from torch._decomp.decompositions_for_rng import PhiloxStateTracker\n from torch._guards import detect_fake_mode\n from torch._prims_common import CUDARngStateHelper\n-from torch.fx.experimental.symbolic_shapes import definitely_false, sym_eq\n+from torch.fx.experimental.symbolic_shapes import (\n+    definitely_false,\n+    rebind_unbacked,\n+    sym_eq,\n+)\n from torch.nn.utils import stateless\n \n from .. import config\n"
                },
                {
                    "old_start": 675,
                    "old_length": 16,
                    "new_start": 679,
                    "new_length": 8,
                    "hunk": "@@ -675,16 +679,8 @@ def aot_dispatch_subclass(\n \n class PropagateUnbackedSymInts(torch.fx.Interpreter):\n     def run_node(self, n: torch.fx.Node):\n-        import sympy\n-\n         result = super().run_node(n)\n-        # TODO: handle Tensor returns\n-        if \"example_value\" in n.meta:\n-            if isinstance(result, torch.SymInt) and isinstance(\n-                result.node.expr, sympy.Symbol\n-            ):\n-                torch._check(result == n.meta[\"example_value\"])\n-\n+        rebind_unbacked(detect_fake_mode().shape_env, n, result)\n         return result\n \n \n"
                }
            ],
            "whole_deleted": "-from torch.fx.experimental.symbolic_shapes import definitely_false, sym_eq\n-        import sympy\n-\n-        # TODO: handle Tensor returns\n-        if \"example_value\" in n.meta:\n-            if isinstance(result, torch.SymInt) and isinstance(\n-                result.node.expr, sympy.Symbol\n-            ):\n-                torch._check(result == n.meta[\"example_value\"])\n-\n",
            "whole_added": "+from torch.fx.experimental.symbolic_shapes import (\n+    definitely_false,\n+    rebind_unbacked,\n+    sym_eq,\n+)\n+        rebind_unbacked(detect_fake_mode().shape_env, n, result)\n",
            "whole_hunk": "@@ -23,7 +23,11 @@ from torch import Tensor\n from torch._decomp.decompositions_for_rng import PhiloxStateTracker\n from torch._guards import detect_fake_mode\n from torch._prims_common import CUDARngStateHelper\n-from torch.fx.experimental.symbolic_shapes import definitely_false, sym_eq\n+from torch.fx.experimental.symbolic_shapes import (\n+    definitely_false,\n+    rebind_unbacked,\n+    sym_eq,\n+)\n from torch.nn.utils import stateless\n \n from .. import config\n@@ -675,16 +679,8 @@ def aot_dispatch_subclass(\n \n class PropagateUnbackedSymInts(torch.fx.Interpreter):\n     def run_node(self, n: torch.fx.Node):\n-        import sympy\n-\n         result = super().run_node(n)\n-        # TODO: handle Tensor returns\n-        if \"example_value\" in n.meta:\n-            if isinstance(result, torch.SymInt) and isinstance(\n-                result.node.expr, sympy.Symbol\n-            ):\n-                torch._check(result == n.meta[\"example_value\"])\n-\n+        rebind_unbacked(detect_fake_mode().shape_env, n, result)\n         return result\n \n \n"
        },
        {
            "name": "symbolic_shapes.py",
            "path": "torch/fx/experimental/symbolic_shapes.py",
            "patches": [
                {
                    "old_start": 97,
                    "old_length": 6,
                    "new_start": 97,
                    "new_length": 7,
                    "hunk": "@@ -97,6 +97,7 @@ __all__ = [\n     \"StatefulSymbolicContext\", \"SubclassSymbolicContext\", \"statically_known_true\",\n     \"guard_size_oblivious\", \"check_consistent\",\n     \"compute_unbacked_bindings\", \"ConvertIntKey\",\n+    \"rebind_unbacked\",\n ]\n \n # FX node metadata keys for symbolic shape FX graph.\n"
                },
                {
                    "old_start": 265,
                    "old_length": 6,
                    "new_start": 266,
                    "new_length": 29,
                    "hunk": "@@ -265,6 +266,29 @@ def check_consistent(new, old) -> None:\n         assert isinstance(old, scalar_types) and not isinstance(old, bool), f\"{old} != {new}\"\n         torch._check(old == new, lambda: f\"{old} != {new} (old != new)\")\n \n+def rebind_unbacked(shape_env, n: torch.fx.Node, result):\n+    \"\"\"\n+    Suppose we are retracing a pre-existing FX graph that previously had\n+    fake tensor propagation (and therefore unbacked SymInts).  When we retrace,\n+    we re-propagate fake tensors, which results in new unbacked SymInts.\n+    When this happens, we need to tell the shape environment about the equivalence\n+    of the old and new unbacked SymInts.  Pass us the old torch.fx.Node (which\n+    has the old binding information) and the new result (which we can extract the\n+    new unbacked SymInts out from).\n+    \"\"\"\n+    if bindings := n.meta.get(\"unbacked_bindings\"):\n+        for raw_u0, path in bindings.items():\n+            u1 = pytree.key_get(result, path)\n+            # We should never have bindings for raw bools; instead they should\n+            # have been converted to ints via ConvertIntKey\n+            assert type(u1) is not bool\n+            if isinstance(u1, (int, float)):\n+                raw_u1 = sympy.sympify(u1)\n+            else:\n+                raw_u1 = u1.node.expr\n+            # TODO: replace with rename unbacked to\n+            shape_env.defer_runtime_assert(sympy.Eq(raw_u0, raw_u1), \"\")\n+\n def canonicalize_bool_expr(expr: SympyBoolean) -> SympyBoolean:\n     r\"\"\" Canonicalize a boolean expression by transforming it into a lt / le\n     inequality and moving all the non-constant terms to the rhs.\n"
                },
                {
                    "old_start": 406,
                    "old_length": 11,
                    "new_start": 430,
                    "new_length": 11,
                    "hunk": "@@ -406,11 +430,11 @@ def find_symbol_binding_fx_nodes(graph):\n @dataclass(frozen=True)\n class ConvertIntKey:\n     def __str__(self) -> str:\n-        return \".__int__()\"\n+        return \".cast_symbool_to_symint_guardless()\"\n \n     def get(self, b: bool) -> int:\n         \"\"\"Get the int value from bool\"\"\"\n-        return int(b)\n+        return cast_symbool_to_symint_guardless(b)\n \n \n @dataclass(frozen=True)\n"
                },
                {
                    "old_start": 1229,
                    "old_length": 7,
                    "new_start": 1253,
                    "new_length": 7,
                    "hunk": "@@ -1229,7 +1253,7 @@ def _eval_is_non_overlapping_and_dense(sizes, strides):\n \n def cast_symbool_to_symint_guardless(symbool: torch.SymBool) -> torch.SymInt:\n     int_sym = sympy.Piecewise((1, symbool.node.expr), (0, True))\n-    return symbool.node.shape_env.create_symintnode(int_sym, hint=int(symbool.node.require_hint()))\n+    return symbool.node.shape_env.create_symintnode(int_sym, hint=int(symbool.node.require_hint()) if has_hint(symbool) else None)\n \n SYMPY_INTERP = {\n     'Abs': operator.abs,"
                }
            ],
            "whole_deleted": "-        return \".__int__()\"\n-        return int(b)\n-    return symbool.node.shape_env.create_symintnode(int_sym, hint=int(symbool.node.require_hint()))\n",
            "whole_added": "+    \"rebind_unbacked\",\n+def rebind_unbacked(shape_env, n: torch.fx.Node, result):\n+    \"\"\"\n+    Suppose we are retracing a pre-existing FX graph that previously had\n+    fake tensor propagation (and therefore unbacked SymInts).  When we retrace,\n+    we re-propagate fake tensors, which results in new unbacked SymInts.\n+    When this happens, we need to tell the shape environment about the equivalence\n+    of the old and new unbacked SymInts.  Pass us the old torch.fx.Node (which\n+    has the old binding information) and the new result (which we can extract the\n+    new unbacked SymInts out from).\n+    \"\"\"\n+    if bindings := n.meta.get(\"unbacked_bindings\"):\n+        for raw_u0, path in bindings.items():\n+            u1 = pytree.key_get(result, path)\n+            # We should never have bindings for raw bools; instead they should\n+            # have been converted to ints via ConvertIntKey\n+            assert type(u1) is not bool\n+            if isinstance(u1, (int, float)):\n+                raw_u1 = sympy.sympify(u1)\n+            else:\n+                raw_u1 = u1.node.expr\n+            # TODO: replace with rename unbacked to\n+            shape_env.defer_runtime_assert(sympy.Eq(raw_u0, raw_u1), \"\")\n+\n+        return \".cast_symbool_to_symint_guardless()\"\n+        return cast_symbool_to_symint_guardless(b)\n+    return symbool.node.shape_env.create_symintnode(int_sym, hint=int(symbool.node.require_hint()) if has_hint(symbool) else None)\n",
            "whole_hunk": "@@ -97,6 +97,7 @@ __all__ = [\n     \"StatefulSymbolicContext\", \"SubclassSymbolicContext\", \"statically_known_true\",\n     \"guard_size_oblivious\", \"check_consistent\",\n     \"compute_unbacked_bindings\", \"ConvertIntKey\",\n+    \"rebind_unbacked\",\n ]\n \n # FX node metadata keys for symbolic shape FX graph.\n@@ -265,6 +266,29 @@ def check_consistent(new, old) -> None:\n         assert isinstance(old, scalar_types) and not isinstance(old, bool), f\"{old} != {new}\"\n         torch._check(old == new, lambda: f\"{old} != {new} (old != new)\")\n \n+def rebind_unbacked(shape_env, n: torch.fx.Node, result):\n+    \"\"\"\n+    Suppose we are retracing a pre-existing FX graph that previously had\n+    fake tensor propagation (and therefore unbacked SymInts).  When we retrace,\n+    we re-propagate fake tensors, which results in new unbacked SymInts.\n+    When this happens, we need to tell the shape environment about the equivalence\n+    of the old and new unbacked SymInts.  Pass us the old torch.fx.Node (which\n+    has the old binding information) and the new result (which we can extract the\n+    new unbacked SymInts out from).\n+    \"\"\"\n+    if bindings := n.meta.get(\"unbacked_bindings\"):\n+        for raw_u0, path in bindings.items():\n+            u1 = pytree.key_get(result, path)\n+            # We should never have bindings for raw bools; instead they should\n+            # have been converted to ints via ConvertIntKey\n+            assert type(u1) is not bool\n+            if isinstance(u1, (int, float)):\n+                raw_u1 = sympy.sympify(u1)\n+            else:\n+                raw_u1 = u1.node.expr\n+            # TODO: replace with rename unbacked to\n+            shape_env.defer_runtime_assert(sympy.Eq(raw_u0, raw_u1), \"\")\n+\n def canonicalize_bool_expr(expr: SympyBoolean) -> SympyBoolean:\n     r\"\"\" Canonicalize a boolean expression by transforming it into a lt / le\n     inequality and moving all the non-constant terms to the rhs.\n@@ -406,11 +430,11 @@ def find_symbol_binding_fx_nodes(graph):\n @dataclass(frozen=True)\n class ConvertIntKey:\n     def __str__(self) -> str:\n-        return \".__int__()\"\n+        return \".cast_symbool_to_symint_guardless()\"\n \n     def get(self, b: bool) -> int:\n         \"\"\"Get the int value from bool\"\"\"\n-        return int(b)\n+        return cast_symbool_to_symint_guardless(b)\n \n \n @dataclass(frozen=True)\n@@ -1229,7 +1253,7 @@ def _eval_is_non_overlapping_and_dense(sizes, strides):\n \n def cast_symbool_to_symint_guardless(symbool: torch.SymBool) -> torch.SymInt:\n     int_sym = sympy.Piecewise((1, symbool.node.expr), (0, True))\n-    return symbool.node.shape_env.create_symintnode(int_sym, hint=int(symbool.node.require_hint()))\n+    return symbool.node.shape_env.create_symintnode(int_sym, hint=int(symbool.node.require_hint()) if has_hint(symbool) else None)\n \n SYMPY_INTERP = {\n     'Abs': operator.abs,"
        }
    ]
},
{
    "Id": 25,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/2a51ccc77e26de66c35e3308aab4412986f1b0c7",
    "date": "2024-07-11T13:02:31+00:00",
    "message": "When translation validation is enabled, assert that hint is consistent (#130478)\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130478\nApproved by: https://github.com/lezcano",
    "label": "NO",
    "changes": [
        {
            "name": "sym_node.py",
            "path": "torch/fx/experimental/sym_node.py",
            "patches": [
                {
                    "old_start": 107,
                    "old_length": 18,
                    "new_start": 108,
                    "new_length": 30,
                    "hunk": "@@ -107,18 +108,30 @@ class SymNode:\n         # unbacked symint that a hint was now possible, but as we added more\n         # potential refinements to unbacked symints this got harder to keep\n         # in sync, so we've deleted it for now.)\n-        if hint is not None:\n-            assert type(hint) is pytype or type(hint) is _to_symtype(pytype), (\n-                \"Cannot create SymNode of type \"\n-                f\"{pytype} with incompatible hint of type {type(hint)}\"\n-            )\n-        else:\n+\n+        def compute_hint():\n             # This occasionally gets exercised by, e.g.,\n             # convert_shape_to_symint.  It's just a nicety so you don't HAVE\n             # to have a correct hint on hand when making a SymNode.\n             hint = self.shape_env._maybe_evaluate_static(self.expr, compute_hint=True)\n             if hint is not None:\n                 hint = self.pytype(hint) if not isinstance(hint, SymTypes) else hint\n+            return hint\n+\n+        if hint is not None:\n+            assert type(hint) is pytype or type(hint) is _to_symtype(pytype), (\n+                \"Cannot create SymNode of type \"\n+                f\"{pytype} with incompatible hint of type {type(hint)}\"\n+            )\n+            if self.shape_env._translation_validation_enabled:\n+                # This is technically not TV, but this assert is expensive so\n+                # let's only do it when we're already doing expensive things\n+                computed_hint = compute_hint()\n+                assert (\n+                    hint == computed_hint\n+                ), f\"{hint} != {computed_hint} (for {self.expr})\"\n+        else:\n+            hint = compute_hint()\n         self._hint = hint\n         self.constant: Optional[Union[int, float, bool]] = constant\n "
                }
            ],
            "whole_deleted": "-        if hint is not None:\n-            assert type(hint) is pytype or type(hint) is _to_symtype(pytype), (\n-                \"Cannot create SymNode of type \"\n-                f\"{pytype} with incompatible hint of type {type(hint)}\"\n-            )\n-        else:\n",
            "whole_added": "+\n+        def compute_hint():\n+            return hint\n+\n+        if hint is not None:\n+            assert type(hint) is pytype or type(hint) is _to_symtype(pytype), (\n+                \"Cannot create SymNode of type \"\n+                f\"{pytype} with incompatible hint of type {type(hint)}\"\n+            )\n+            if self.shape_env._translation_validation_enabled:\n+                # This is technically not TV, but this assert is expensive so\n+                # let's only do it when we're already doing expensive things\n+                computed_hint = compute_hint()\n+                assert (\n+                    hint == computed_hint\n+                ), f\"{hint} != {computed_hint} (for {self.expr})\"\n+        else:\n+            hint = compute_hint()\n",
            "whole_hunk": "@@ -107,18 +108,30 @@ class SymNode:\n         # unbacked symint that a hint was now possible, but as we added more\n         # potential refinements to unbacked symints this got harder to keep\n         # in sync, so we've deleted it for now.)\n-        if hint is not None:\n-            assert type(hint) is pytype or type(hint) is _to_symtype(pytype), (\n-                \"Cannot create SymNode of type \"\n-                f\"{pytype} with incompatible hint of type {type(hint)}\"\n-            )\n-        else:\n+\n+        def compute_hint():\n             # This occasionally gets exercised by, e.g.,\n             # convert_shape_to_symint.  It's just a nicety so you don't HAVE\n             # to have a correct hint on hand when making a SymNode.\n             hint = self.shape_env._maybe_evaluate_static(self.expr, compute_hint=True)\n             if hint is not None:\n                 hint = self.pytype(hint) if not isinstance(hint, SymTypes) else hint\n+            return hint\n+\n+        if hint is not None:\n+            assert type(hint) is pytype or type(hint) is _to_symtype(pytype), (\n+                \"Cannot create SymNode of type \"\n+                f\"{pytype} with incompatible hint of type {type(hint)}\"\n+            )\n+            if self.shape_env._translation_validation_enabled:\n+                # This is technically not TV, but this assert is expensive so\n+                # let's only do it when we're already doing expensive things\n+                computed_hint = compute_hint()\n+                assert (\n+                    hint == computed_hint\n+                ), f\"{hint} != {computed_hint} (for {self.expr})\"\n+        else:\n+            hint = compute_hint()\n         self._hint = hint\n         self.constant: Optional[Union[int, float, bool]] = constant\n "
        }
    ]
},
{
    "Id": 290,
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/b51e0246b7f119770c47183b230c553f15ab4fbb",
    "date": "2024-02-13T23:50:40+00:00",
    "message": "sccache version update (#119554)\n\nFixes #37928\n\n`sccache` is updated to the newer version (`v0.7.4`) to fix non-cacheable calls `multiple input files`  for `CUDA` builds.\n\nThis should make `Cache hits (CUDA)`  work as expected and improve the speed dramatically.\n\n---\n\nAdditional information:\n\n- Modified `install_sccache.bat` check structure due to GitHub Action error `Process completed with exit code 255.`\n    - Error is occurring when freshly downloaded `sccache` is being called with `--show-stats` or `--start-server` arguments within the script\n    - Now, it is checking file's existence and killing/deleting executable before the download\n\n- Removed `sccache-cl` since it is no longer needed with newer versions of `sccache`\n\n---\n\n`win-vs2019-cpu-py3 / build` - `16m 27s`\n\n![image](https://github.com/pytorch/pytorch/assets/148207261/b5628e6c-64bb-4293-9d07-480f56df44f1)\n\n`win-vs2019-cuda11.8-py3 / build` - `17m 4s` **(previously ~45 mins - 1h30mins)**\n\n![image](https://github.com/pytorch/pytorch/assets/148207261/e4ab01cb-0f56-41e8-984f-110e643b9c09)\n\nNow `Cache Hits (CUDA)` hits all `304` object and the error `Non-cacheable reasons` is fixed.\n\n![image](https://github.com/pytorch/pytorch/assets/148207261/c8c25d2e-3fc1-4edb-8982-99c1f490cb54)\n\n---\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/119554\nApproved by: https://github.com/malfet",
    "label": "YES",
    "changes": [
        {
            "name": "build_pytorch.bat",
            "path": ".ci/pytorch/win-test-helpers/build_pytorch.bat",
            "patches": [
                {
                    "old_start": 88,
                    "old_length": 8,
                    "new_start": 88,
                    "new_length": 8,
                    "hunk": "@@ -88,8 +88,8 @@ set SCCACHE_IGNORE_SERVER_IO_ERROR=1\n sccache --stop-server\n sccache --start-server\n sccache --zero-stats\n-set CC=sccache-cl\n-set CXX=sccache-cl\n+set CMAKE_C_COMPILER_LAUNCHER=sccache\n+set CMAKE_CXX_COMPILER_LAUNCHER=sccache\n \n set CMAKE_GENERATOR=Ninja\n \n"
                }
            ],
            "whole_deleted": "-set CC=sccache-cl\n-set CXX=sccache-cl\n",
            "whole_added": "+set CMAKE_C_COMPILER_LAUNCHER=sccache\n+set CMAKE_CXX_COMPILER_LAUNCHER=sccache\n",
            "whole_hunk": "@@ -88,8 +88,8 @@ set SCCACHE_IGNORE_SERVER_IO_ERROR=1\n sccache --stop-server\n sccache --start-server\n sccache --zero-stats\n-set CC=sccache-cl\n-set CXX=sccache-cl\n+set CMAKE_C_COMPILER_LAUNCHER=sccache\n+set CMAKE_CXX_COMPILER_LAUNCHER=sccache\n \n set CMAKE_GENERATOR=Ninja\n \n"
        },
        {
            "name": "install_sccache.bat",
            "path": ".ci/pytorch/win-test-helpers/installation-helpers/install_sccache.bat",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 18,
                    "new_start": 1,
                    "new_length": 13,
                    "hunk": "@@ -1,18 +1,13 @@\n mkdir %TMP_DIR_WIN%\\bin\n \n if \"%REBUILD%\"==\"\" (\n-  :check_sccache\n-  %TMP_DIR_WIN%\\bin\\sccache.exe --show-stats || (\n+  IF EXIST %TMP_DIR_WIN%\\bin\\sccache.exe (\n     taskkill /im sccache.exe /f /t || ver > nul\n     del %TMP_DIR_WIN%\\bin\\sccache.exe || ver > nul\n-    del %TMP_DIR_WIN%\\bin\\sccache-cl.exe || ver > nul\n-    if \"%BUILD_ENVIRONMENT%\"==\"\" (\n-      curl --retry 3 --retry-all-errors -k https://s3.amazonaws.com/ossci-windows/sccache.exe --output %TMP_DIR_WIN%\\bin\\sccache.exe\n-      curl --retry 3 --retry-all-errors -k https://s3.amazonaws.com/ossci-windows/sccache-cl.exe --output %TMP_DIR_WIN%\\bin\\sccache-cl.exe\n-    ) else (\n-      aws s3 cp s3://ossci-windows/sccache.exe %TMP_DIR_WIN%\\bin\\sccache.exe\n-      aws s3 cp s3://ossci-windows/sccache-cl.exe %TMP_DIR_WIN%\\bin\\sccache-cl.exe\n-    )\n-    goto :check_sccache\n+  )\n+  if \"%BUILD_ENVIRONMENT%\"==\"\" (\n+    curl --retry 3 --retry-all-errors -k https://s3.amazonaws.com/ossci-windows/sccache-v0.7.4.exe --output %TMP_DIR_WIN%\\bin\\sccache.exe\n+  ) else (\n+    aws s3 cp s3://ossci-windows/sccache-v0.7.4.exe %TMP_DIR_WIN%\\bin\\sccache.exe\n   )\n )\n\\ No newline at end of file\n"
                }
            ],
            "whole_deleted": "-  :check_sccache\n-  %TMP_DIR_WIN%\\bin\\sccache.exe --show-stats || (\n-    del %TMP_DIR_WIN%\\bin\\sccache-cl.exe || ver > nul\n-    if \"%BUILD_ENVIRONMENT%\"==\"\" (\n-      curl --retry 3 --retry-all-errors -k https://s3.amazonaws.com/ossci-windows/sccache.exe --output %TMP_DIR_WIN%\\bin\\sccache.exe\n-      curl --retry 3 --retry-all-errors -k https://s3.amazonaws.com/ossci-windows/sccache-cl.exe --output %TMP_DIR_WIN%\\bin\\sccache-cl.exe\n-    ) else (\n-      aws s3 cp s3://ossci-windows/sccache.exe %TMP_DIR_WIN%\\bin\\sccache.exe\n-      aws s3 cp s3://ossci-windows/sccache-cl.exe %TMP_DIR_WIN%\\bin\\sccache-cl.exe\n-    )\n-    goto :check_sccache\n",
            "whole_added": "+  IF EXIST %TMP_DIR_WIN%\\bin\\sccache.exe (\n+  )\n+  if \"%BUILD_ENVIRONMENT%\"==\"\" (\n+    curl --retry 3 --retry-all-errors -k https://s3.amazonaws.com/ossci-windows/sccache-v0.7.4.exe --output %TMP_DIR_WIN%\\bin\\sccache.exe\n+  ) else (\n+    aws s3 cp s3://ossci-windows/sccache-v0.7.4.exe %TMP_DIR_WIN%\\bin\\sccache.exe\n",
            "whole_hunk": "@@ -1,18 +1,13 @@\n mkdir %TMP_DIR_WIN%\\bin\n \n if \"%REBUILD%\"==\"\" (\n-  :check_sccache\n-  %TMP_DIR_WIN%\\bin\\sccache.exe --show-stats || (\n+  IF EXIST %TMP_DIR_WIN%\\bin\\sccache.exe (\n     taskkill /im sccache.exe /f /t || ver > nul\n     del %TMP_DIR_WIN%\\bin\\sccache.exe || ver > nul\n-    del %TMP_DIR_WIN%\\bin\\sccache-cl.exe || ver > nul\n-    if \"%BUILD_ENVIRONMENT%\"==\"\" (\n-      curl --retry 3 --retry-all-errors -k https://s3.amazonaws.com/ossci-windows/sccache.exe --output %TMP_DIR_WIN%\\bin\\sccache.exe\n-      curl --retry 3 --retry-all-errors -k https://s3.amazonaws.com/ossci-windows/sccache-cl.exe --output %TMP_DIR_WIN%\\bin\\sccache-cl.exe\n-    ) else (\n-      aws s3 cp s3://ossci-windows/sccache.exe %TMP_DIR_WIN%\\bin\\sccache.exe\n-      aws s3 cp s3://ossci-windows/sccache-cl.exe %TMP_DIR_WIN%\\bin\\sccache-cl.exe\n-    )\n-    goto :check_sccache\n+  )\n+  if \"%BUILD_ENVIRONMENT%\"==\"\" (\n+    curl --retry 3 --retry-all-errors -k https://s3.amazonaws.com/ossci-windows/sccache-v0.7.4.exe --output %TMP_DIR_WIN%\\bin\\sccache.exe\n+  ) else (\n+    aws s3 cp s3://ossci-windows/sccache-v0.7.4.exe %TMP_DIR_WIN%\\bin\\sccache.exe\n   )\n )\n\\ No newline at end of file\n"
        },
        {
            "name": "_win-build.yml",
            "path": ".github/workflows/_win-build.yml",
            "patches": [
                {
                    "old_start": 128,
                    "old_length": 6,
                    "new_start": 128,
                    "new_length": 7,
                    "hunk": "@@ -128,6 +128,7 @@ jobs:\n           PYTHON_VERSION: \"3.8\"\n           SCCACHE_BUCKET: \"ossci-compiler-cache\"\n           SCCACHE_S3_KEY_PREFIX: ${{ github.workflow }}\n+          SCCACHE_REGION: us-east-1\n           VC_PRODUCT: \"BuildTools\"\n           VC_VERSION: \"\"\n           VC_YEAR: \"2019\""
                }
            ],
            "whole_deleted": "",
            "whole_added": "+          SCCACHE_REGION: us-east-1\n",
            "whole_hunk": "@@ -128,6 +128,7 @@ jobs:\n           PYTHON_VERSION: \"3.8\"\n           SCCACHE_BUCKET: \"ossci-compiler-cache\"\n           SCCACHE_S3_KEY_PREFIX: ${{ github.workflow }}\n+          SCCACHE_REGION: us-east-1\n           VC_PRODUCT: \"BuildTools\"\n           VC_VERSION: \"\"\n           VC_YEAR: \"2019\""
        }
    ]
}]